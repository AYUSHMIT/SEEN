{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/models/epoch_26.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/models/epoch_26.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/data/test_data.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c91771180b4fa0b29724206cf290e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9efc746a14948a7b16f69eb6cf6a2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 80))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2K0lEQVR4nO3dd3yV9dn48c+VvXcCSAKBBAgbJMhSloq4beuizjqorbu1y7aO2j5PrbXVp4tSB46KvzpR24qiIMhQwg4bwgorCzLJvn5/nJMQyIacnJOc6/165ZXkvu9zn+swznW+6/qKqmKMMcZ7+bg7AGOMMe5licAYY7ycJQJjjPFylgiMMcbLWSIwxhgv5+fuANorLi5Ok5OT3R2GcaXt2x3fBw1ybxzGdCNr1qzJU9X4ps51uUSQnJxMRkaGu8MwrjR1quP7kiXujMKYbkVE9jV3zrqGjDHGy1kiMMYYL2eJwBhjvJwlAmOM8XKWCIwxxsu5LBGIyEsikiMimS1cM1VE1ovIZhH5wlWxGGOMaZ4rWwTzgJnNnRSRKOCvwFWqOhS4zoWxGGOMaYbLEoGqLgUKWrjk28C7qrrfeX2Oq2IB2H6kmGcWbuN4WaUrn8YYY7ocd44RDASiRWSJiKwRkVubu1BEZotIhohk5ObmntGT7ckr5S+Ld5N97MSZxmuMMd2SOxOBHzAGuBy4BPiliAxs6kJVnauq6aqaHh/f5ArpVsWHBwCQV1JxZtEaY0w35c4SE9lAnqqWAqUishQYCexwxZPFhQUCkF9iXUPGGNOQO1sEC4ALRMRPREKAccBWVz1ZrDMRWIvAGGNO5bIWgYjMB6YCcSKSDTwO+AOo6hxV3SoiHwMbgVrgBVVtdqrp2QoN8CXI34f8UmsRGGNMQy5LBKo6qw3XPAM846oYGhIRYkMDySu2FoExxjTkVSuL48IDybMWgTHGnMK7EkFogLUIjDHmNN6VCMICbbDYGGNO41WJIDYsgILSSmpr1d2hGGOMx/CqRBAXFkh1rVJ4osrdoRhjjMfwrkQQ7lxUVmrdQ8YYU8e7EkGoo8xEbrHNHDLGmDrelQisRWCMMY14VSKIdbYIbAqpMcac5FWJIDokAB+BPCs8Z4wx9bwqEfj4CDGhgdY1ZIwxDXhVIgCICwuwwWJjjGnA6xJBfLi1CIwxpiGvSwSxoQFWZsIYYxrwukQQFxZou5QZY0wDXpcIYsMCKausoayy2t2hGGOMR/C6RBAXVreWwFoFxhgDXpkInHsX24CxMcYA3pwIbHWxMcYALkwEIvKSiOSISJMb0ovIVBEpFJH1zq/HXBVLQ3Hhjq4h28TeGGMcXLZ5PTAP+DPwagvXLFPVK1wYQyMxVm/IGGNO4bIWgaouBQpcdf8zFejnS0SQn7UIjDHGyd1jBBNEZIOI/FdEhnbWk8aFBZJri8qMMQZwbddQa9YCfVW1REQuA94HBjR1oYjMBmYD9OnT56yfOC4s0LqGjDHGyW0tAlUtUtUS58//AfxFJK6Za+eqarqqpsfHx5/1c8eGBVjXkDHGOLktEYhITxER58/nOWPJ74znjgsLtHpDxhjj5LKuIRGZD0wF4kQkG3gc8AdQ1TnAtcD3RKQaOAHcqKrqqngaigsL5HhZFVU1tfj7unuYxBhj3MtliUBVZ7Vy/s84ppd2ulhnmYmC0kp6RAS5IwRjjPEYXvlxuH51sXUPGWOMtyYC56IyK0dtjDHemgis3pAxxtTxykRQN0ZgW1YaY4yXJoKwQD8C/Xysa8gYY/DSRCAitpbAGGOcvDIRgGPA2FoExhjj1YkgkHxrERhjjPcmgtiwAOsaMsYYvDgROFoEldTWdkpVC2OM8VhemwhiwwKprlWKyqvcHYoxxriV1yaCk6uLrXvIGOPd3LkxjVvVrS6+/eXVhAT44iNCoL8vv7lmGMN6R7o5OmOM6TxemwhG94nihvQkiiuqqK2FWlU+3XqUz7bmWCIwxngVr00EIQF+PH3tiFOOnf/05+zOLXFTRMYY4x5eO0bQlJT4MHblWCIwxngXSwQNpCaEkZVXYlNKjTFexRJBAynxYZRX1XKo8IS7QzHGmE5jiaCB1IQwAOseMsZ4FUsEDaTEhwKwO7fUzZEYY0zncVkiEJGXRCRHRDJbuW6siNSIyLWuiqWtYkIDiArxtxaBMcaruLJFMA+Y2dIFIuILPA0sdGEcbSYipMaH2RRSY4xXcVkiUNWlQEErl90PvAPkuCqO9kqJDyPLEoExxou4bYxARHoD3wDmtOHa2SKSISIZubm5Lo0rNSGMvJJKjpfZpjXGGO/gzsHi54CfqGpNaxeq6lxVTVfV9Pj4eJcGlZJQN2BsrQJjjHdwZ4mJdOBNEQGIAy4TkWpVfd+NMZEaHw44ppCO6RvjzlCMMaZTuC0RqGq/up9FZB7wkbuTAEDv6GAC/HxsCqkxxmu4LBGIyHxgKhAnItnA44A/gKq2Oi7gLr4+Qv+4UHbbFFJjjJdwWSJQ1VntuPZ2V8VxJlISwsg8WOjuMIwxplPYyuImpMSHcaCgjPKqVsexjTGmy7NE0ITUhDBqFfbm2ziBMab7s0TQhPqaQzmWCIwx3Z8lgib0jwtDxKqQGmO8gyWCJgQH+NI7KtgWlRljvIIlgmakWPE5Y4yXsETQjNQERyKwbSuNMd2dJYJm2LaVxhhvYYmgGbZtpTHGW1giaEbdFFJLBMaY7s4SQTNiQgMICfDlcGG5u0MxxhiXskTQDBEhITyQnOIKd4dijDEuZYmgBfHhgeQWW4vAGNO9WSJogSMRWIvAGNO9WSJoQXyYJQJjTPdniaAFCRFBFJVXWzlqY0y31moiEJGBIvKZiGQ6fx8hIr9wfWjuFx8WCGCtAmNMt9aWFsE/gJ8BVQCquhG40ZVBeYr4cGciKLFEYIzpvtqSCEJU9evTjlW7IhhPU58IrEVgjOnG2pII8kQkBVAAEbkWONzag0TkJRHJqetSauL81SKyUUTWi0iGiJzfrsg7QYIlAmOMF2jL5vX3AnOBNBE5COwBbmrD4+YBfwZebeb8Z8AHqqoiMgL4F5DWhvt2mpjQAEQsERhjurcWE4GI+ALfU9WLRCQU8FHV4rbcWFWXikhyC+cbFvEJxdni8CR+vj7EhgbY6mJjTLfWYiJQ1RoRGeP8ucM38BWRbwD/CyQAl3f0/TtCnK0lMMZ0c23pGlonIh8AbwH1yUBV3z3bJ1fV94D3RGQy8BRwUVPXichsYDZAnz59zvZp2yU+PNBmDRljurW2JIIYIB+Y3uCYAmedCOpv5uhGShGROFXNa+L8XBzjFKSnp3dqF1JCeBBZufmd+ZTGGNOpWk0EqvodVzyxiKQCu52DxecCATgSjkepqzekqoiIu8MxxpgO12oiEJFE4E/AJBwtgS+BB1U1u5XHzQemAnEikg08DvgDqOoc4FvArSJSBZwAblBVjxswjg8PpLKmlsITVUSFBLg7HGOM6XBt6Rp6GXgDuM75+83OYxe39CBVndXK+aeBp9vw/G7VcFGZJQJjTHfUlgVl8ar6sqpWO7/mAfEujstjWL0hY0x319aVxTeLiK/z62Y8sC/fVRIirN6QMaZ7a0siuAO4HjiCo7TEtc5jXsHqDRljuru2zBraD1zVCbF4pPBAPwL9fGx1sTGm22rLfgSviEhUg9+jReQll0blQUTEtqw0xnRrbekaGqGqx+t+UdVjwGiXReSBEiwRGGO6sbYkAh8Ria77RURiaNu0027DWgTGmO6sLW/ozwIrRORt5+/XAb9xXUieJz48kNV7j7k7DGOMcYm2DBa/KiIZnKw19E1V3eLasDxLfFgQBaWVVFbXEuDXlkaUMcZ0Hc2+q4lIiIjUlYTYAnyKo0SER20e0xnqppDml1r3kDGm+2np4+3HQDLUF4hbCfQH7hWR37o+NM9hW1YaY7qzlhJBtKrudP58GzBfVe8HLsVDN5FxFVtUZozpzlpKBA0rgU7H0TWEqlYCta4MytNYIjDGdGctDRZvFJHfAweBVOATgIaLy7xFnLPwnK0uNsZ0Ry21CO4G8nCME8xQ1TLn8SHA710cl0cJ8PMhOsTfWgTGmG6p2RaBqp4AGg0Kq+oKYIUrg/JEtqjMGNNd2aT4NmpqE/tlO3P5ZPMRN0VkjDEdw6tKRZyN+LBA1u4/Xv97YVkV3399LcUV1Vw7JpFfXT2UkAD74zTGdD3WImij+PBAcorLqdtW+cXleyiuqGbWeX14Z202V/7pS7YeLnJzlMYY034trSyOE5HHReQBEQkTkb+JSKaILHAuMPMqCeFBlFfVUlJRTeGJKl5evoeZQ3vyv98czj/vGkdxeTVX/2U5b2UccHeoxhjTLi21CN4AAoEBwNdAFo7dyT4CXmjtxiLykojkiEhmM+dvEpGNzq8VIjKy/eF3noZrCV5evofi8mruv9CRDyemxPHfBy9gTJ9oHn1vEwcKylq6lTHGeJSWEkEPVX0UeAAIU9VnVHWbqv4DiGrDvecBM1s4vweYoqojgKeAuW0L2T3qEkFWbikvfbmHi4f0YOg5kfXnY8MC+cMNI/ER4dlPtrsrTGOMabeWEkENgDo6xfNOO9fqymJVXQoUtHB+hXOTG4BVQGJr93SnukTw7Kc7KCqv5sELBzS6pldkMHec34/31x8i82BhZ4dojDFnpKVE0F9EPhCRDxv8XPd7vw6O407gv82dFJHZIpIhIhm5ubkd/NRtE+9cXbz1cBEXDU5gWO/IJq+7Z0oKUSH+/G6htQqMMV1DS/Mdr27w8+kriTtsZbGITMORCM5v7hpVnYuz6yg9PV2bu86VokL88fcVqmqUBy8c2Ox1kcH+3DctlV//eyvLd+UxKTWuE6M0xpj2a2ll8Rd1P4tIvPNYh34cF5EROAaeL1XV/I68d0cTEfrGhpIcG8rwxKZbA3VuHt+Xl5fv5bf/3caCeyfh4yOdFKUxxrRfS9NHxTl9NA/YBuwQkVwReawjnlhE+gDvAreo6o6OuKervXH3OP5v1qhWrwvy9+UHFw9k08FC/r3psOsDM8Z0OWWV1e4OoV5LYwQP4eiuGauqsaoaDYwDJonIw63dWETm49jMZpCIZIvInSJyj4jc47zkMSAW+KuIrHduh+nREsKD2rx6+JrRvUnrGc7vFm6jvKrGxZEZY7qS5bvyGPnkJ2Tllrg7FKDlRHArMEtV99QdUNUs4GbnuRap6ixV7aWq/qqaqKovquocVZ3jPH+Xqkar6ijnV/rZvhhP4usjPHbFEA4UnOD/PtvZ+gOMMV3Wqyv3cv/8dVRWt22rlo82HqKqRlm+6/QJmc2rqK6hqsY1W8G0lAj8VbVRlM5xAn+XRNPNTEyN47oxifx9aRZbDln5CWO6I1VlzpLdfLjhEI8tyKwvQ9PS9Yu3OYZbV+891uK1dZbvyuPS55bx8vI9rV98BlpKBJVneM408PPLBxMd4s9P391ITa1bJjwZY1xo86EiDhWWM/ScCN5cfYBXV+5r8foth4s4UlROSIAvGXubXWoFQE5ROQ/MX8dNL3xFjSqDe0V0ZOj1WkoEI0WkqImvYmC4S6LphqJCAnj8yqFszC50WTY3xrjPJ1uO4iMw7zvncdHgBH710RZWtNDls3hbDgDfmZTMocJyDh4/0eR1b3y1nwuf/YKPM4/w4IUDWPjQZC4YEO+S19BsIlBVX1WNaOIrXFWta6gdrhjRi+lpCTz7yQ6rQ2RMN/PplqOM6RtNfHggf7xhFP3jQvn+G2vZn9/0//XPt+UwIjGSS4f1AmiyVbDzaDGPvreJYb0jWfjwZB6+eCBB/r4uew1WhroTiAhPXTMMH4FfvN96H6Ix5sx9svkId72ymtIK10/PPFBQxtbDRVw8pAcA4UH+vHBbOqow+7UMqk8b3C0orWTdgeNMHZRAWs9wQgN8yWhinODjzCOIwPM3jqJfXKjLX4clgk7SOyqY+y8cwBc7ctmdW+rucIzpljIPFvLAm+tYtDWHl75sviu2qLyKnKJy8koqKCitpLi86oyeb9HWowBcPKRn/bG+saE8/a0RbDtSzNtrsk+5/osdOajC9LQE/Hx9OLdvNBn7mkgEm48wOimKhIigM4qrvSwRdKLLnE3BFbvbPmXMGNM2eSUVfPe1NUSHBDApNZa5S7M4Vtp4XsvSHbmkP7WI8/7nM9J/vYhzn/qU4U98wj2vreFwYdP99VU1tdQ2Mdnj0y1HGZAQ1uhT+yVDe3BunyieW7TzlHVEn2/LJS4sgBHOWmVj+kaz7UgRRQ0S0YGCMjYfKmLmsJ50FksEnahPbAhJMcF8udMSgTEdqbK6lu+/vpa8kgrm3pLO41cOpaSymr99sfuU6wrLqvjR2xvoExvCb74xjKeuHsqTVw3lnikpLNmRw0XPfsELy7KorqlFVfl6TwE/emsDI5/8hHvfWHtKt25hWRVf7Smo7xZqSET40SVpHCkq5zXnLKLqmlq+2J7DlIEJ9WVnxibHoArrGmyDu9C5D/olQzsvEdgmu51sUkoc/950mJpaxddqEBnTIX710Wa+3lvA8zeOqq8F9s3RicxbsZfvTEqmV2QwAI99kEl+SSUv3ja2UQXhm8b14bEFmfz631t5e002J6pq2JdfRmiAL6OSovhv5hFeXr6XO853FF/+fPtRamq1yUQAMCEllskD4/nrkl3ceF4SWw8XU1RezfS0hPprRiVF4esjZOwtYMpAx4ygTzYfJa1nOH1jXT82UMdaBJ1sUmocxeXVbLL9CozpEO+syeb1Vfv57pT+XD2qd/3xhy4aAArPL3Ks7P/3xsMsWH+IBy4c0GQZ+aSYEF66fSxzbj6X6lqlV2QQz143ktW/uIh/3jWOi4f04H//u5UNB44Djm6hhPBARiZGNRvbjy8ZxLGyKv6xbA+fb8vBz0e4YODJisShgX4M6RXBaufModziClbvK+jU1gBYIuh0E1NiAdq1tNwY07Sqmlr+8OkORiZF8eNL0k45lxQTwk3j+/CvjAOs3J3PL97fxMikKL4/NaXZ+4kIM4f1YtEPpvDm7Al8a0wiIQF+iAjPXDuChPAg7n1jLbnFFXyxPZcLB/dosbrwsN6RXD68Fy8sy+I/mw6TnhxNRNCps+/H9I1m/YHjVNXUsmjrUVTp1PEBsETQ6WLDAhncK8ISgekS1u0/xhc73LMZVFt8uOEQB4+f4P5pqU12td47LZVgf19uefEryiprePa6kfj5ntnbXlRIAH/69miOFJZzw99XUlpZw4xmuoUa+sGMgVRU17K/oOyUbqE6Y5NjKK+qZfOhIhZuPkKfmBDSeoafUYxnyhKBG5yfGkvGvmNWldR4vMcWbOaB+esazYfvTHvySjlSWN7oeG2t8rcluxnUI7zJN1iAuLBA7rqgP9W1yk8vTSM1IeysYjm3TzQ/mZlGVl4pIQG+THC28FuSEh/Gtec6duJtKs705GjAseJ4+a48Zg7riUjnjh/aYLEbTEyN4x/L9pCx9xjnD7AdzIxnyi2uqB/LWrPvGOP6t/6m15F25RTz3KKd/HvTYXpGBPHh/ecT59wyFuCzbTnszCnhuRtGtdg9c9/0VMb3j2Vcv5gOieuuC/qx7UgxcWEBbV7t+8srhzBzeE9SExp/0u8REURSTDAvfrmHqhrlkqGttzI6mrUI3OC85Bj8fYUvrXvIeLCGXUKfOevjuJqqsvVwEQ+9uY6L/7iUz7flcOv4vhSUVnLfG2vrWyaqyl+X7CIxOpgrRvRq8Z7+vj5MSIntsJ0CRYRnrx/Jzy4b3ObHhAX6MW1Q060WgLF9YyipqCY+PJDRSdEdEWa7WIvADUID/RidFG3jBMajLdmeQ1xYIGk9w1m09SiPtuGNL/tYGX/8dCc/mDGQ3lHBbXqezIOFLNuZR8beAtbsP8bxsiqC/X2ZPbk/sy/oT2xYICMSo/jhWxv43cLtPHrZYFZlFbBu/3GeunroGff5e5IxydG8u+4gM4a0PPjsKpYI3GRSahzPfbaD42WVRIUEuDscY05RXVPLsp15XDykB8N7R/L4B5vJyi2hf3zLfexvZWTzztpsvt6bz/y7x5MYHdLstWWV1fz2v9vqyzb3jw9lxpAepCfHMD0t4ZRuoG+NSWRD9nHmLs1iRGIk/2/1AeLCArguPaljXrCbTR4QT3SIP98ak+iW57dE4CaTUmP54yJYuTufS4e33LQ1prNtyD5O4Ykqpg6KZ1RSFI9/sJnPtua0mghWZuXTOyqYwrIqbpy7ivl3jycppnEyWLPvGD/813r25pdxx6R+3DsthdgGb/xN+cXlQ9h8qIgf/msDFdW1/HjmIJdW5OxMSTEhrHtshtuev+u3qbqokUlRhAb4stzqDhkPtHhbLj4CF6TGkxgdUt891JLyqhrW7z/OZcN78s+7xlNcXs2Nc1fVl14/VlrJil15PPXRFq6bs4KqGmX+3eN57MohrSYBgAA/H/5607mEB/kTHujHzeP7dshrNdYicBt/Xx/G949l+a58d4diTCNLduQwpm80kSGOxU8XDk5gzhdZFJZV1R873dp9x6isqWVCSizDEyP5513juOmFr7jmL8sJ9PPhUIMpoDekJ/GLKwYTHtS+rU16RATx3vcnUlRe1WhhljlzLmsRiMhLIpIjIpnNnE8TkZUiUiEij7gqDk82MTWOPXmlbD5k5SaM58gpLifzYBFTG8xyuXBwD2pqlSU7mp89tDIrH18fYWyyY5rmsN6OZDDknAjG9ovhZ5em8fqd41j7y4t5+toR7U4CdZJiQhh6TuMSEebMubJraB4ws4XzBcADwO9dGINHu2rkOSSEB/Ld19aQV1Lh7nCMAWDpDkd3ZV0RNIBRiVHEhQWwaGsLiWB3PsN6R57yBj+sdySv3TmO528czXenpHD+gDhiQm1yhKdxWSJQ1aU43uybO5+jqquBM9sRohuIDw/khdvS6+uo20pj4wkWb88hPjyQoeec3Cjdx0eYnpbAku05VDWxyrisspoN2ceZ0MmLzkzH6BKDxSIyW0QyRCQjN9dz656ciRGJUfzh+lGs2XeMn76z0baxNB2qsKyK3OIKSiqqqWliY5XTVdfUsmxHLlMHxjcqc3Dh4B4Ul1ezek/jz3cZe49RVaNtKrlgPE+XGCxW1bnAXID09PRu90552fBePDJjIL//ZAepCWHcN32Au0My3cCBgjKmP7uEqpqT/2VCAnx57Ioh3HhenyYfs/7AcYrKq08ZH6hzwYA4Avx8WLQ1h4mpp5ZGWZmVj5+PkN6381fFmrPXJRKBN7h3Wiq7c0v5/Sc7iA8P5IaxTf9HNaatPs48QlWN8rNL0xCBE5W1fLEjhyc+3My4/rFNboq+ZHsuvj7SZA2skAA/JqbEsnDzEX56aRoBfic7FFbuzmdEYiShgfaW0hV1ia4hbyAi/PZbw5k8MJ6fvLOJf361z90hmS5u4eYjDO4VwXenpDB7cgoPXjSAv908hgBfH3701oZGXUUHj5/grTUHHNNGg5ue0XPbhGQOHj/B35ac3AKypMKx0ZJ1C3Vdrpw+Oh9YCQwSkWwRuVNE7hGRe5zne4pINvAD4BfOayJaumd3F+jny9xbxjA9LYGfv5fJKyv2ujsk00XlFlewZv+xRpUse0QE8diVQ8nYd+yUf1+5xRXc8oKjZv/jVw5p9r7T0hK4cuQ5/HnxTnYcLQZg9d4CamqVCf2tkm5X5cpZQ7NUtZeq+qtqoqq+qKpzVHWO8/wR5/EIVY1y/lzkqni6iiB/X+bcPIaLh/Tg8Q8288KyLHeHZLqgup2umtry8Fvn9mZ6WgK/W7iNPXmlFJ6o4taXvuZQ4Qlevn1sq3P0H79yCGGBfvzknY3U1Cqrdufj7yuMsfGBLsu6hjxQ3VL6S4f15Nf/3sp/Nh12d0imi1m4+QhJMcFN7nQlIvzPN4bj7+wiumPeanblFPP3W9JJT269Zn9cWCCPXTmEdfuP88qKvazMymd0UjTBAd2j7o83skTgofx9ffjTrNEMSAjj/z7badNKTZsVl1exYlc+lwxpfqernpFBPHbFEDL2HWPd/mM8f+PoUxaQteaaUb2ZOiieZxZuJ/NgIeNtfKBLs0Tgwfx8fbh7cn+2HSlm2U4rTmfaZvH2XCprarmklQ3Qrx2TyAPTU/nzt8/lsnZWwBURfvON4fgI1Cq2kKyLs0Tg4a4e5ShD8Q8bKzBttHDzEeLCAji3T8t99iLCD2YMancSqNM7KpgnrhpKWs9wRveJOqN7GM9gicDDBfr5cvukZJbtzLPidF3cA/PXubybr7yqhiXbcrh4SA98O2Gnq+vSk/j4ocndZl8Ab2WJoAu4aVxfQgN8+cdSaxV0VeVVNXyw4RB/+HQHT3ywmdo2lHs4Eyt251FaWcOMJmYLGdMcSwRdQGSwPzee14cPNx7m0PET7g7HnIE9eaUAjEiM5JWV+/jR2xvrN2Jvq8yDhfzy/Uz+sngX/954mMyDhZRWVJ9yzcLMo4QFOlYAG9NWth68i/jOpGTmrdjLS1/u4RdXNL/gx3imukTwP98Yzmdbc/jjoh2UVVbz3I2jCPRrvVslK7eEW178itKKGiobJBARGNQjnPTkaMYmx7Bo61GmpSW06Z7G1LFE0EUkRodwxYhezP96P/dfOKDZEgDGM2XllgCODdqH9R5AWJAfT320hXW/W8K4/jGkJ8dwXnIMAxLC8Dmtbz+vpILbX16NiPDJw5OJDw9kX34Z+/JL2X60mDX7jvH+ukO8vmo/QKPVxMa0xhJBF3L3Bf1ZsP4Qr6zYywMXWoXSriQrt5RekUGEBDj+y915fj+SooNZsP4QK3fns2D9IQCSYoK5b1oq3zw3EX9fH8oqq7nzlQxyisuZf/d4kp2F4oacE8GQcyK41Dnjp6ZW2XakiL15ZcxsZdqoMaezRNCFDOsdycyhPfnL4l1cM6o3fWJDGl1TUFqJqrZpM3DTebLyShtV+5wxtCczhvZEVTlQcIKv9uTz2qp9/OSdTfx58S7um5bKp1ty2Jh9nL/fPIbRLUwH9fURhp4TaVs4mjNig8VdzBNXDcXf14dfLMhsNA3x0PETzPjjUma/tsZN0ZmmqCpZuSX0j29c9hkc8/n7xIZwXXoSC+6dxEu3pxMdEsBP3tnEoq1HeeLKoTYLyLiUtQi6mJ6RQTwyYyBPfLiFDzce5qqR5wCOrQLveiWDvJIK8koqyCkuJyE8yM3Rdh+qyobsQmpqlUA/HwL8fIgM9qdHROt/xgWllRSVV9MvLqzVa0WE6Wk9mDYogcXbczheVsU3z03siJdgTLMsEXRBt0xI5t11B/nVh1uYMiCe8CA/Hv5/69l2pIifzEzj6Y+3sWR7LtenJ7k71G7jqY+28tLyPY2Ov33PhFYLtWU5Zww11yJoSl1CMKYzWNdQF+Tr46geWVBawdMLt/Hsp9tZuPkoP798CPdM6U+vyCAWb8txd5jdxmdbj/LS8j1cn57IvO+MZe4tY/jzt0cTHuTHP7/a3+rj9+Q6EkFKG1oExriDtQi6qGG9I7ljUj9e+NLxKXXWeUncMSkZEWHqoAQ+3HCIyuraU7YTNO13pLCcR97awOBeEfzq6mGnlFJYuTuft9dk88RVQ1uczrs7r4QAXx96Rwd3RsjGtJu9S3RhD188kOTYECamxPLkVcPqSw5PT0ugpKKajL0Fbo6wa6upVR76f+sor6rlz98e3aiezo1j+1BRXcsHGw61eJ+s3FL6xoZ0Su0fY86EJYIuLDTQj48fmszrd4475ZP/pNRYAvx8+Ny6h5qkqry8fA9/WbyrxQJwf1m8i1VZBfzq6qGkxDfu1hnWO4LBvSL41+oDLT7fniamjhrjSSwRdHFB/r6NVqKGBPgxvn8sn2+3RHA6VeXpj7fz5IdbeGbhdn65ILPJAnCfbzvKc4t2cPWoc7h2TNOzdkSEG9IT2XSwsNnKsNU1tezLL6V/E4nEGE/hys3rXxKRHBHJbOa8iMj/icguEdkoIue6KhZvNH1QPFm5pezLL3V3KB6jtlZ58sMtzPliNzeP78N3p/Tn9VX7efS9TfXJoKK6hl9/tIU75mUwsEc4v75mWLO7fAFcM7o3AX4+zbYKDh4/QVWNtmvGkDGdzZUtgnnAzBbOXwoMcH7NBv7mwli8Tt3UQ2/oHlJVnlu0o76eT1NqapVH39vEvBV7uev8fjx19TB+OjONB6an8ubqAzzy9ga2HSni6j8v54Uv93DrhL68f+8kwoNarukUFRLAzKE9eX/9Icqrahqdz3LOGOpvXUPGg7ksEajqUqCl0cqrgVfVYRUQJSJntlWSaaRPbAgp8aFekQgOFJzguUU762dQNeXJDzfz5uoD3D89lZ9fPhgRqd+h6wcXD+TdtQeZ+dwycosrePG29EYzhFpyw9gkCk9UsXDzkUbndtcXm7OuIeO53Dl9tDfQsD2d7Tx2+PQLRWQ2jlYDffr06ZTguoPpaQm8smIfpRXVhAb6UVxexbOf7CD7WBn/uDW9xS6Ps7H1cBERwf70juqc6ZJ1b7ZLtuWgqo1eV0lFNW9+fYAb0pP44YxBjR7/wIUDiAjyY/2B4/z88iHEh7evTtOE/rEkxQTzr4wDXD2q9ynn9uSVEhnsT3SIVYs1nsudg8VNvQs1OYVDVeeqarqqpsfHx7s4rO5jWloClTW1LN+Vx+fbjjLjj0uZt2Ivi7bmsDOn+W6Us1FWWc2Nc1fx+ILNLrl/U3Y5X8uhwnJ2HG38ur5wbub+zXN7NzpX5/ZJ/XjuxtHtTgIAPj7CdWOSWL4rn/35Zaecy8otpX98qMuSrjEdwZ2JIBtoWAMhEWh5QrZpl/S+MYQF+vHoe5u4Y14G4UF+/GnWaABW7MpzyXMuWH+IwhNVrD9wzKV78za0O7eEYGc3TlNdYZ9uOUJMaABj+ra8mfvZuC49ET8fYe6y3acc35NXSn9bUWw8nDsTwQfArc7ZQ+OBQlVt1C1kzlyAnw8XDU6g8EQVD180kI/uv4ArR55DUkwwK3bnd/jzqSqvrNgLQF5JJdnHOmdbzV05JQzvHcngXhEsPm3KbFVNLZ9vy2F6WgJ+vq77594rMpgbxibx5tcHOFDgaBWUVlRzpKjcZgwZj+fK6aPzgZXAIBHJFpE7ReQeEbnHecl/gCxgF/AP4PuuisWb/eYbw1n+0+k8eNGA+kVnE/vHsSorn5oO3kB99d5jbDtSzM3jHeM4G7KPd+j9m7M7t4SUhDCmDYpnzb5jFJ6oqj/39Z4CisqruXiI6wu43T99AL4+wvOf7QRObk9pM4aMp3PlrKFZqtpLVf1VNVFVX1TVOao6x3leVfVeVU1R1eGqmuGqWLxZaKBfo3LUE1NjKSqvZsuhog59rldW7CUiyI8fz0wj0M+H9fuPd+j9m1JQWsmxsipS4kOZnpZATa3y5c6T3V6fbjlKkL8Pkwe4fmypZ2QQt4zvy7trs9mVU9Kg6qh1DRnPZiuLvdCE/rEArNjdceMERwrL+XjzEW4Ym0REkD/Dekd2SougbqA4NSGMUUlRRAb713cPqSqfbjnK+anxBAd0zmbu90xNIcjft35dgwj0bWInOWM8iSUCL5QQEURqQlibxglqa5VVWflszD5OYVlVs9e98dU+alW5ZXwyACMTo9h0sJCqmtqOCrtJdVNHU+LD8PP1YfLAeJZsz6W2VtlyuIiDx08woxO6herEhQVyx6R+fLTxMAs3H6V3VHCb1yMY4y5WhtpLTUyJ5e012S2Wqi6rrOaRtzbwn00nF0pFhfjTNzaUa0adw6zz+hDk70tFdQ1vfL2f6YMS6vdRHtUnipeW72HH0WKX7qO7K6eEIH+f+jUL0wbF8+GGQ2QeKuSzrTmIwPTBCS57/qbcfUF/Xlm5l62Hi7hgQFynPrcxZ8JaBF5qYkosZZU1bGym++bg8RNcN2clH2ce4UeXDOLvt4zh0cvSuHx4L1BHzZ6pzyzhtVX7WLD+EHklldw6Mbn+8aMSowDYcKBxMbYDBWWs2XesQ17H7twS+seF1RfemzIwHhFYvC2XT7ccJb1vNHFh7V8bcDYiQ/z57uT+AE1WLTXG01iLwEuN6xeLCKzYnd9oq8U1+wr47mtrqKiq5cXbxzJt0KmfqFWVlbvz+cOnO/jl+46agv3iQrkg9eSn36SYYGJCA1h/4BjfHnfqavAfv72RNfuPsfChyWddnnlXTgmj+5xcHxAbFsjIxCjeWnOA7GMnePSytLO6/5m6fVI/Fm/PZcogWwBpPJ+1CLxUdGgAQ3pFNBowXrojl1lzvyIs0I/37p3YKAmAo/zyxNQ43rpnAq/ecR5TBsbzo0sGnVIOW0QYmRjZqEWwJ6+UlVn5VFbX8tiCzLNadHaisoaDx0+Qetqn7mmDEurXMFw8pOcZ3/9shAX68c73mv7zM8bTWCLwYhNTYlm773h91czduSXc+8Za+seH8v69k0hNCG/x8SLC5IHxvHLHeVw2vHG9wJFJUezIKaakorr+2Jur9+PrI9w3LZVlO/P4cOOZryHck1eKKqQknNqqmJ7mePMdkBBmG8IY0waWCLzYxJQ4KmtqHYuwyqq4+5UMAnx9eOG2dKJCAs76/qOSolCFTdmOVkFldS1vZ2RzYVoCD188kBGJkTz10RaKypufjdSSXbknp442NPScCNJ6hnNdetMbyhhjTmWJwIuN7ReDr4+wdGcu981fy4FjZcy5ZQyJ0R0z732kc8B4/YHjgGNxV35pJbPG9cHXR/jNNcPJL6ng2YXbz+j+u3NK8BFIjj31U7+Pj/DxQ5OZPTnlbMI3xmtYIvBiYYF+jEyM5IVle1i2M4/fXDOcsacNHJ+N6NAAkmND2OBMBG+u3k/vqOD6Vb7DEyO5dUIyr67a1+zspZbsyi0hKSbE5ukbc5YsEXi5iSlx1NQqd0zqx/Vjk1p/QDuNTIpiQ/Zx9ueXsWxnHtenJ+HbYFD5BzMGEhcWyM/fa//A8e6cEpueaUwHsETg5W6d0JfHrhjismmWo5KiOFxYzvOf7cRH4Pqxp/bbRwT586MZg9h0sJCV7aiIWlOrZOWVkmKVPY05a5YIvFxCRBB3nN/PZSWaRyZFAfDO2mympyXQK7LxrmVXjTqHqBB/Xlu1r833PXjsBJXVtY0Gio0x7WeJwLjUkF4R+Ps6uoJuHNv0NqNB/r7ckJ7EJ1uOcriwbXsY7MotBmzlrjEdwRKBcakgf1+G9IqgZ0QQU1tYZXvTuL7UqjL/6wONzqkq+/JLTxlD2J3jKPFsicCYs2eJwLjc09eO4IXb0lvsfuoTG8LUgfHM/3o/p++X87cvdjPlmSX8ckEm1c5qprtzS4gNDSA69OzXOxjj7SwRGJdL6xnBsN6tVyC9ZUJfcosrOFZWWX9sVVY+v1+4neTYEF5ftZ97Xl9DWWU1u3Icu5IZY86eJQLjMaYMTCApJpgjheUA5BZXcP/8dSTHhvLRAxfwq6uH8vm2HGbNXcWOo8XWLWRMB7FEYDyGr49w07i+FJdXUVpZw4NvrqO4vIq/3nwuYYF+3DohmTk3j2H70WKKyqtt6qgxHcQSgfEo16cnISJsO1LEit35PHX1MNJ6RtSfnzG0J/PvHs/4/jH1xeWMMWfHpYlARGaKyHYR2SUiP23ifLSIvCciG0XkaxEZ5sp4jOeLCQ0gNiyAqupark9P5Lr0xqudR/eJ5s3ZE2xTeGM6iMsSgYj4An8BLgWGALNEZMhplz0KrFfVEcCtwPOuisd0HUnRIZwTFcyTV9nnAmM6gytbBOcBu1Q1S1UrgTeBq0+7ZgjwGYCqbgOSRaTzdho3HinQz4c+MSEEB1gxOWM6gysTQW+g4eqgbOexhjYA3wQQkfOAvkCjIvIiMltEMkQkIzc310XhGmOMd3JlIpAmjp1eXvK3QLSIrAfuB9YB1Y0epDpXVdNVNT0+3vaANcaYjuTKzeuzgYYjfYnAoYYXqGoR8B0AERFgj/PLGGNMJ3Fli2A1MEBE+olIAHAj8EHDC0QkynkO4C5gqTM5GGOM6SQuaxGoarWI3AcsBHyBl1R1s4jc4zw/BxgMvCoiNcAW4E5XxWOMMaZpruwaQlX/A/zntGNzGvy8EhjgyhiMMca0zFYWG2OMl7NEYIwxXk7au2G4u4lILtD2PQ1dLw7Ic3cQrbAYO0ZXiBG6RpwWY8doT4x9VbXJ+fddLhF4GhHJUNV0d8fREouxY3SFGKFrxGkxdoyOitG6howxxstZIjDGGC9nieDszXV3AG1gMXaMrhAjdI04LcaO0SEx2hiBMcZ4OWsRGGOMl7NEYIwxXs4SQTuIyEsikiMimQ2OxYjIpyKy0/k92o3xJYnIYhHZKiKbReRBT4vRGU+Qc2vSDc44n/TQOH1FZJ2IfOSJ8Tlj2isim0RkvYhkeGKczuKSb4vINue/zQmeFKOIDHL++dV9FYnIQ54UozPOh53/XzJFZL7z/1GHxGiJoH3mATNPO/ZT4DNVHYBjt7VGezN3omrgh6o6GBgP3OvcHtSTYgSoAKar6khgFDBTRMbjeXE+CGxt8LunxVdnmqqOajCf3NPifB74WFXTgJE4/kw9JkZV3e788xsFjAHKgPc8KUYR6Q08AKSr6jAchTxv7LAYVdW+2vEFJAOZDX7fDvRy/twL2O7uGBvEtgC42MNjDAHWAuM8KU4c+2d8BkwHPvLUv2tgLxB32jGPiROIwLHHiHhqjKfFNQNY7mkxcnLHxxgcxUI/csbaITFai+Ds9VDVwwDO7wlujgcAEUkGRgNf4YExOrtd1gM5wKeq6mlxPgf8GKhtcMyT4qujwCciskZEZjuPeVKc/YFc4GVnN9sLIhLqYTE2dCMw3/mzx8SoqgeB3wP7gcNAoap+0lExWiLohkQkDHgHeEg9dKMfVa1RR1M8EThPRIa5OaR6InIFkKOqa9wdSxtMUtVzgUtxdAVOdndAp/EDzgX+pqqjgVLc31XVJOcmWVcBb7k7ltM5+/6vBvoB5wChInJzR93fEsHZOyoivQCc33PcGYyI+ONIAv9U1Xedhz0qxoZU9TiwBMfYi6fEOQm4SkT2Am8C00XkdQ+Kr56qHnJ+z8HRr30enhVnNpDtbPEBvI0jMXhSjHUuBdaq6lHn754U40XAHlXNVdUq4F1gYkfFaIng7H0A3Ob8+TYc/fJuISICvAhsVdU/NDjlMTECiEi8iEQ5fw7G8Y98Gx4Sp6r+TFUTVTUZR1fB56p6s6fEV0dEQkUkvO5nHH3GmXhQnKp6BDggIoOchy7EsRuhx8TYwCxOdguBZ8W4HxgvIiHO/+cX4hh075gY3T0405W+cPwjOQxU4fikcycQi2NQcafze4wb4zsfR5/xRmC98+syT4rRGecIYJ0zzkzgMedxj4rTGdNUTg4We1R8OPrfNzi/NgM/99A4RwEZzr/v94FoD4wxBMgHIhsc87QYn8TxgSkTeA0I7KgYrcSEMcZ4OesaMsYYL2eJwBhjvJwlAmOM8XKWCIwxxstZIjDGGC9nicB4HBFREXm2we+PiMgTHXTveSJybUfcq5Xnuc5ZaXOxK+MSkWQR+Xb7IzTmJEsExhNVAN8UkTh3B9KQiPi24/I7ge+r6jRXxeOUDLQrEbTzdRgvYInAeKJqHHuxPnz6idM/OYtIifP7VBH5QkT+JSI7ROS3InKTOPY92CQiKQ1uc5GILHNed4Xz8b4i8oyIrBaRjSLy3Qb3XSwibwCbmohnlvP+mSLytPPYYzgW980RkWeaeMyPnY/ZICK/beL83rokKCLpIrLE+fOUBjXz1zlXFf8WuMB57OG2vg7nquR/O2PIFJEb2vIXY7onP3cHYEwz/gJsFJHfteMxI4HBQAGQBbygqueJY4Oe+4GHnNclA1OAFGCxiKQCt+Ko6DhWRAKB5SLyifP684Bhqrqn4ZOJyDnA0zhq2B/DUQX0GlX9lYhMBx5R1YzTHnMpcA0wTlXLRCSmHa/vEeBeVV3uLCxYjqOA2yOqWpfQZrfldYjIt4BDqnq583GR7YjDdDPWIjAeSR1VU1/FsRlHW61W1cOqWgHsBureADfhePOv8y9VrVXVnTgSRhqOOj23iqM09lc4lu4PcF7/9elJwGkssEQdhcCqgX8CrVX/vAh4WVXLnK+zoB2vbznwBxF5AIhyPufp2vo6NuFoGT0tIheoamE74jDdjCUC48mew9HXHtrgWDXOf7fO4lsBDc5VNPi5tsHvtZza+j29rooCAtyvzp2qVLWfOuq9g6N0clOkja/j9Me0Vtel/jUCQfVBqv4WuAsIBlaJSFoz92/1dajqDhwtmU3A/zq7s4yXskRgPJbz0/K/cCSDOntxvIGBoz67/xnc+joR8XGOG/THscvTQuB74ijjjYgMdFb0bMlXwBQRiXMOwM4CvmjlMZ8Ad4hIiPN5muoa2svJ1/ituoMikqKqm1T1aRxF3NKAYiC8wWPb9Dqc3Vplqvo6jg1Pzm0lbtON2RiB8XTPAvc1+P0fwAIR+RpHtcXmPq23ZDuON+wewD2qWi4iL+DoPlrrbGnk4ujLb5aqHhaRnwGLcXwS/4+qtlgGWFU/FpFRQIaIVAL/AR497bIngRdF5FEcyabOQyIyDajBUcr5vzhaO9UisgHHntrPt/F1DAeeEZFaHNV0v9dS3KZ7s+qjxhjj5axryBhjvJwlAmOM8XKWCIwxxstZIjDGGC9nicAYY7ycJQJjjPFylgiMMcbL/X8VDrOSFOVuzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9787611491647366\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.2774474382030967\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.9271408839779005\n",
      "layer 5: 0.8344267955801106\n",
      "layer 6: 0.587189226519337\n",
      "layer 7: 0.43266574585635365\n",
      "layer 8: 0.25496374309392267\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 3.433 | Reg loss: 0.019 | Tree loss: 3.433 | Accuracy: 0.047000 | 3.93 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 3.426 | Reg loss: 0.018 | Tree loss: 3.426 | Accuracy: 0.071000 | 4.06 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 3.421 | Reg loss: 0.018 | Tree loss: 3.421 | Accuracy: 0.094000 | 4.134 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 3.412 | Reg loss: 0.018 | Tree loss: 3.412 | Accuracy: 0.109000 | 4.185 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 3.404 | Reg loss: 0.018 | Tree loss: 3.404 | Accuracy: 0.145000 | 4.18 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 3.402 | Reg loss: 0.018 | Tree loss: 3.402 | Accuracy: 0.139000 | 4.12 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 3.394 | Reg loss: 0.017 | Tree loss: 3.394 | Accuracy: 0.147000 | 4.16 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 3.386 | Reg loss: 0.017 | Tree loss: 3.386 | Accuracy: 0.157000 | 4.182 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 3.384 | Reg loss: 0.017 | Tree loss: 3.384 | Accuracy: 0.138000 | 4.166 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 3.373 | Reg loss: 0.017 | Tree loss: 3.373 | Accuracy: 0.152500 | 4.133 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 3.358 | Reg loss: 0.017 | Tree loss: 3.358 | Accuracy: 0.180887 | 4.121 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 3.419 | Reg loss: 0.016 | Tree loss: 3.419 | Accuracy: 0.046000 | 4.116 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 3.416 | Reg loss: 0.016 | Tree loss: 3.416 | Accuracy: 0.083500 | 4.063 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 3.408 | Reg loss: 0.016 | Tree loss: 3.408 | Accuracy: 0.095000 | 4.051 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 3.400 | Reg loss: 0.016 | Tree loss: 3.400 | Accuracy: 0.123500 | 4.004 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 3.398 | Reg loss: 0.016 | Tree loss: 3.398 | Accuracy: 0.146000 | 3.997 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 3.388 | Reg loss: 0.016 | Tree loss: 3.388 | Accuracy: 0.160000 | 4.008 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 3.384 | Reg loss: 0.016 | Tree loss: 3.384 | Accuracy: 0.151000 | 4.007 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 3.372 | Reg loss: 0.017 | Tree loss: 3.372 | Accuracy: 0.173000 | 4.057 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 3.364 | Reg loss: 0.017 | Tree loss: 3.364 | Accuracy: 0.170000 | 4.059 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 3.360 | Reg loss: 0.017 | Tree loss: 3.360 | Accuracy: 0.163000 | 4.096 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 3.345 | Reg loss: 0.017 | Tree loss: 3.345 | Accuracy: 0.201365 | 4.084 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 3.413 | Reg loss: 0.016 | Tree loss: 3.413 | Accuracy: 0.089500 | 4.1 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 3.406 | Reg loss: 0.016 | Tree loss: 3.406 | Accuracy: 0.095000 | 4.081 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 3.399 | Reg loss: 0.016 | Tree loss: 3.399 | Accuracy: 0.115000 | 4.099 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 3.390 | Reg loss: 0.016 | Tree loss: 3.390 | Accuracy: 0.153000 | 4.087 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 3.384 | Reg loss: 0.016 | Tree loss: 3.384 | Accuracy: 0.175000 | 4.014 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 3.378 | Reg loss: 0.016 | Tree loss: 3.378 | Accuracy: 0.164500 | 3.989 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 3.369 | Reg loss: 0.016 | Tree loss: 3.369 | Accuracy: 0.173500 | 4.021 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 3.363 | Reg loss: 0.016 | Tree loss: 3.363 | Accuracy: 0.154000 | 4.041 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 3.354 | Reg loss: 0.017 | Tree loss: 3.354 | Accuracy: 0.158500 | 4.049 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 3.348 | Reg loss: 0.017 | Tree loss: 3.348 | Accuracy: 0.152000 | 4.06 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 3.324 | Reg loss: 0.017 | Tree loss: 3.324 | Accuracy: 0.184300 | 4.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 3.411 | Reg loss: 0.015 | Tree loss: 3.411 | Accuracy: 0.092000 | 4.079 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 3.404 | Reg loss: 0.016 | Tree loss: 3.404 | Accuracy: 0.117500 | 4.067 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 3.394 | Reg loss: 0.016 | Tree loss: 3.394 | Accuracy: 0.160000 | 4.082 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 3.387 | Reg loss: 0.016 | Tree loss: 3.387 | Accuracy: 0.172000 | 4.104 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 3.369 | Reg loss: 0.016 | Tree loss: 3.369 | Accuracy: 0.190500 | 4.101 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 3.363 | Reg loss: 0.016 | Tree loss: 3.363 | Accuracy: 0.169000 | 4.125 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 3.348 | Reg loss: 0.016 | Tree loss: 3.348 | Accuracy: 0.171500 | 4.132 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 3.341 | Reg loss: 0.016 | Tree loss: 3.341 | Accuracy: 0.166000 | 4.156 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 3.334 | Reg loss: 0.017 | Tree loss: 3.334 | Accuracy: 0.157000 | 4.153 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 3.318 | Reg loss: 0.017 | Tree loss: 3.318 | Accuracy: 0.162500 | 4.145 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 3.311 | Reg loss: 0.017 | Tree loss: 3.311 | Accuracy: 0.156997 | 4.143 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 3.407 | Reg loss: 0.016 | Tree loss: 3.407 | Accuracy: 0.100000 | 4.143 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 3.396 | Reg loss: 0.016 | Tree loss: 3.396 | Accuracy: 0.130000 | 4.121 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 3.387 | Reg loss: 0.016 | Tree loss: 3.387 | Accuracy: 0.154000 | 4.123 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 3.372 | Reg loss: 0.016 | Tree loss: 3.372 | Accuracy: 0.190500 | 4.122 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 3.355 | Reg loss: 0.016 | Tree loss: 3.355 | Accuracy: 0.182500 | 4.119 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 3.340 | Reg loss: 0.016 | Tree loss: 3.340 | Accuracy: 0.165000 | 4.116 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 3.325 | Reg loss: 0.016 | Tree loss: 3.325 | Accuracy: 0.171500 | 4.11 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 3.310 | Reg loss: 0.017 | Tree loss: 3.310 | Accuracy: 0.169500 | 4.105 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 3.306 | Reg loss: 0.017 | Tree loss: 3.306 | Accuracy: 0.149500 | 4.107 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 3.300 | Reg loss: 0.017 | Tree loss: 3.300 | Accuracy: 0.138000 | 4.109 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 3.286 | Reg loss: 0.017 | Tree loss: 3.286 | Accuracy: 0.143345 | 4.111 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 3.400 | Reg loss: 0.016 | Tree loss: 3.400 | Accuracy: 0.131000 | 4.118 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 3.390 | Reg loss: 0.016 | Tree loss: 3.390 | Accuracy: 0.135000 | 4.111 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 3.373 | Reg loss: 0.016 | Tree loss: 3.373 | Accuracy: 0.190500 | 4.091 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 3.355 | Reg loss: 0.016 | Tree loss: 3.355 | Accuracy: 0.189500 | 4.078 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 3.340 | Reg loss: 0.016 | Tree loss: 3.340 | Accuracy: 0.171000 | 4.062 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 3.323 | Reg loss: 0.016 | Tree loss: 3.323 | Accuracy: 0.162500 | 4.052 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 3.310 | Reg loss: 0.017 | Tree loss: 3.310 | Accuracy: 0.145000 | 4.049 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 3.286 | Reg loss: 0.017 | Tree loss: 3.286 | Accuracy: 0.163500 | 4.032 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 3.273 | Reg loss: 0.017 | Tree loss: 3.273 | Accuracy: 0.152000 | 4.028 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 3.261 | Reg loss: 0.017 | Tree loss: 3.261 | Accuracy: 0.145500 | 4.031 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 3.246 | Reg loss: 0.018 | Tree loss: 3.246 | Accuracy: 0.170648 | 4.022 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 3.391 | Reg loss: 0.016 | Tree loss: 3.391 | Accuracy: 0.145500 | 4.034 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 3.379 | Reg loss: 0.016 | Tree loss: 3.379 | Accuracy: 0.175000 | 4.024 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 3.363 | Reg loss: 0.016 | Tree loss: 3.363 | Accuracy: 0.199500 | 4.024 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 3.339 | Reg loss: 0.016 | Tree loss: 3.339 | Accuracy: 0.190000 | 4.03 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 3.322 | Reg loss: 0.017 | Tree loss: 3.322 | Accuracy: 0.178000 | 4.025 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 3.295 | Reg loss: 0.017 | Tree loss: 3.295 | Accuracy: 0.165000 | 4.029 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 3.271 | Reg loss: 0.017 | Tree loss: 3.271 | Accuracy: 0.156000 | 4.031 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 3.257 | Reg loss: 0.017 | Tree loss: 3.257 | Accuracy: 0.162000 | 4.028 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 3.240 | Reg loss: 0.018 | Tree loss: 3.240 | Accuracy: 0.162500 | 4.032 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 3.241 | Reg loss: 0.018 | Tree loss: 3.241 | Accuracy: 0.142000 | 4.038 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 3.202 | Reg loss: 0.018 | Tree loss: 3.202 | Accuracy: 0.153584 | 4.04 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 3.382 | Reg loss: 0.017 | Tree loss: 3.382 | Accuracy: 0.161000 | 4.043 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 3.366 | Reg loss: 0.017 | Tree loss: 3.366 | Accuracy: 0.207500 | 4.043 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 3.336 | Reg loss: 0.017 | Tree loss: 3.336 | Accuracy: 0.215000 | 4.036 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 3.316 | Reg loss: 0.017 | Tree loss: 3.316 | Accuracy: 0.188500 | 4.037 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 3.290 | Reg loss: 0.017 | Tree loss: 3.290 | Accuracy: 0.154500 | 4.037 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 3.261 | Reg loss: 0.017 | Tree loss: 3.261 | Accuracy: 0.170000 | 4.039 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 3.239 | Reg loss: 0.017 | Tree loss: 3.239 | Accuracy: 0.159500 | 4.039 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 3.224 | Reg loss: 0.018 | Tree loss: 3.224 | Accuracy: 0.143000 | 4.04 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 3.210 | Reg loss: 0.018 | Tree loss: 3.210 | Accuracy: 0.155500 | 4.049 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 3.198 | Reg loss: 0.018 | Tree loss: 3.198 | Accuracy: 0.146500 | 4.057 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 3.183 | Reg loss: 0.019 | Tree loss: 3.183 | Accuracy: 0.139932 | 4.044 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 3.366 | Reg loss: 0.017 | Tree loss: 3.366 | Accuracy: 0.194000 | 4.052 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 3.345 | Reg loss: 0.017 | Tree loss: 3.345 | Accuracy: 0.202000 | 4.051 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 3.325 | Reg loss: 0.017 | Tree loss: 3.325 | Accuracy: 0.193500 | 4.052 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 3.294 | Reg loss: 0.017 | Tree loss: 3.294 | Accuracy: 0.185500 | 4.054 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 3.254 | Reg loss: 0.017 | Tree loss: 3.254 | Accuracy: 0.188000 | 4.057 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 3.224 | Reg loss: 0.018 | Tree loss: 3.224 | Accuracy: 0.171000 | 4.063 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 3.192 | Reg loss: 0.018 | Tree loss: 3.192 | Accuracy: 0.184500 | 4.067 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 3.186 | Reg loss: 0.018 | Tree loss: 3.186 | Accuracy: 0.144500 | 4.078 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 3.170 | Reg loss: 0.019 | Tree loss: 3.170 | Accuracy: 0.156500 | 4.087 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 3.160 | Reg loss: 0.019 | Tree loss: 3.160 | Accuracy: 0.154500 | 4.089 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 3.157 | Reg loss: 0.019 | Tree loss: 3.157 | Accuracy: 0.143345 | 4.097 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 3.352 | Reg loss: 0.018 | Tree loss: 3.352 | Accuracy: 0.204500 | 4.117 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 3.324 | Reg loss: 0.018 | Tree loss: 3.324 | Accuracy: 0.204500 | 4.117 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 3.289 | Reg loss: 0.018 | Tree loss: 3.289 | Accuracy: 0.214000 | 4.118 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 3.255 | Reg loss: 0.018 | Tree loss: 3.255 | Accuracy: 0.186000 | 4.116 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 3.225 | Reg loss: 0.018 | Tree loss: 3.225 | Accuracy: 0.181000 | 4.114 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 3.190 | Reg loss: 0.018 | Tree loss: 3.190 | Accuracy: 0.169500 | 4.112 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 3.170 | Reg loss: 0.019 | Tree loss: 3.170 | Accuracy: 0.158000 | 4.115 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 3.157 | Reg loss: 0.019 | Tree loss: 3.157 | Accuracy: 0.144500 | 4.117 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 3.118 | Reg loss: 0.019 | Tree loss: 3.118 | Accuracy: 0.176000 | 4.127 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 3.106 | Reg loss: 0.020 | Tree loss: 3.106 | Accuracy: 0.161500 | 4.13 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 3.107 | Reg loss: 0.020 | Tree loss: 3.107 | Accuracy: 0.156997 | 4.136 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 3.336 | Reg loss: 0.018 | Tree loss: 3.336 | Accuracy: 0.188000 | 4.151 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 3.293 | Reg loss: 0.018 | Tree loss: 3.293 | Accuracy: 0.225500 | 4.153 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 3.254 | Reg loss: 0.018 | Tree loss: 3.254 | Accuracy: 0.215000 | 4.152 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 3.215 | Reg loss: 0.019 | Tree loss: 3.215 | Accuracy: 0.202000 | 4.148 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 3.179 | Reg loss: 0.019 | Tree loss: 3.179 | Accuracy: 0.175500 | 4.146 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 3.154 | Reg loss: 0.019 | Tree loss: 3.154 | Accuracy: 0.179000 | 4.146 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 3.131 | Reg loss: 0.019 | Tree loss: 3.131 | Accuracy: 0.152500 | 4.145 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 3.106 | Reg loss: 0.020 | Tree loss: 3.106 | Accuracy: 0.159000 | 4.141 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 3.087 | Reg loss: 0.020 | Tree loss: 3.087 | Accuracy: 0.159500 | 4.125 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 3.073 | Reg loss: 0.020 | Tree loss: 3.073 | Accuracy: 0.153000 | 4.126 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 3.053 | Reg loss: 0.020 | Tree loss: 3.053 | Accuracy: 0.174061 | 4.128 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 3.304 | Reg loss: 0.019 | Tree loss: 3.304 | Accuracy: 0.190500 | 4.13 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 3.270 | Reg loss: 0.019 | Tree loss: 3.270 | Accuracy: 0.199000 | 4.125 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 3.232 | Reg loss: 0.019 | Tree loss: 3.232 | Accuracy: 0.198000 | 4.118 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 3.187 | Reg loss: 0.019 | Tree loss: 3.187 | Accuracy: 0.216500 | 4.109 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 3.121 | Reg loss: 0.019 | Tree loss: 3.121 | Accuracy: 0.211500 | 4.112 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 3.104 | Reg loss: 0.020 | Tree loss: 3.104 | Accuracy: 0.194000 | 4.113 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 3.078 | Reg loss: 0.020 | Tree loss: 3.078 | Accuracy: 0.176000 | 4.116 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 3.046 | Reg loss: 0.020 | Tree loss: 3.046 | Accuracy: 0.186500 | 4.117 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 3.037 | Reg loss: 0.021 | Tree loss: 3.037 | Accuracy: 0.187000 | 4.12 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 3.031 | Reg loss: 0.021 | Tree loss: 3.031 | Accuracy: 0.162000 | 4.123 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 3.010 | Reg loss: 0.021 | Tree loss: 3.010 | Accuracy: 0.156997 | 4.124 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 3.280 | Reg loss: 0.020 | Tree loss: 3.280 | Accuracy: 0.198000 | 4.133 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 3.236 | Reg loss: 0.020 | Tree loss: 3.236 | Accuracy: 0.238500 | 4.124 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 3.197 | Reg loss: 0.020 | Tree loss: 3.197 | Accuracy: 0.219500 | 4.125 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 3.140 | Reg loss: 0.020 | Tree loss: 3.140 | Accuracy: 0.246500 | 4.129 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 3.088 | Reg loss: 0.020 | Tree loss: 3.088 | Accuracy: 0.225500 | 4.136 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 3.053 | Reg loss: 0.020 | Tree loss: 3.053 | Accuracy: 0.230000 | 4.137 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 3.029 | Reg loss: 0.021 | Tree loss: 3.029 | Accuracy: 0.209500 | 4.142 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 2.997 | Reg loss: 0.021 | Tree loss: 2.997 | Accuracy: 0.226500 | 4.146 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 2.982 | Reg loss: 0.021 | Tree loss: 2.982 | Accuracy: 0.200500 | 4.156 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 2.976 | Reg loss: 0.021 | Tree loss: 2.976 | Accuracy: 0.194000 | 4.162 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 2.989 | Reg loss: 0.022 | Tree loss: 2.989 | Accuracy: 0.201365 | 4.149 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 3.253 | Reg loss: 0.020 | Tree loss: 3.253 | Accuracy: 0.218000 | 4.156 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 3.210 | Reg loss: 0.020 | Tree loss: 3.210 | Accuracy: 0.221500 | 4.154 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 3.143 | Reg loss: 0.020 | Tree loss: 3.143 | Accuracy: 0.258000 | 4.15 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 3.108 | Reg loss: 0.020 | Tree loss: 3.108 | Accuracy: 0.230000 | 4.151 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 3.047 | Reg loss: 0.021 | Tree loss: 3.047 | Accuracy: 0.222000 | 4.154 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 3.020 | Reg loss: 0.021 | Tree loss: 3.020 | Accuracy: 0.203500 | 4.156 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 2.973 | Reg loss: 0.021 | Tree loss: 2.973 | Accuracy: 0.204500 | 4.16 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 2.953 | Reg loss: 0.021 | Tree loss: 2.953 | Accuracy: 0.201000 | 4.16 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 2.930 | Reg loss: 0.022 | Tree loss: 2.930 | Accuracy: 0.209000 | 4.154 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 2.893 | Reg loss: 0.022 | Tree loss: 2.893 | Accuracy: 0.227000 | 4.155 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 2.906 | Reg loss: 0.022 | Tree loss: 2.906 | Accuracy: 0.249147 | 4.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 3.231 | Reg loss: 0.021 | Tree loss: 3.231 | Accuracy: 0.202000 | 4.159 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 3.179 | Reg loss: 0.021 | Tree loss: 3.179 | Accuracy: 0.232500 | 4.156 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 3.115 | Reg loss: 0.021 | Tree loss: 3.115 | Accuracy: 0.232500 | 4.159 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 3.057 | Reg loss: 0.021 | Tree loss: 3.057 | Accuracy: 0.231000 | 4.161 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 3.004 | Reg loss: 0.021 | Tree loss: 3.004 | Accuracy: 0.244000 | 4.16 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 2.970 | Reg loss: 0.021 | Tree loss: 2.970 | Accuracy: 0.216500 | 4.163 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 2.912 | Reg loss: 0.022 | Tree loss: 2.912 | Accuracy: 0.221000 | 4.16 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 2.912 | Reg loss: 0.022 | Tree loss: 2.912 | Accuracy: 0.219500 | 4.162 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 2.876 | Reg loss: 0.022 | Tree loss: 2.876 | Accuracy: 0.227500 | 4.162 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 2.868 | Reg loss: 0.022 | Tree loss: 2.868 | Accuracy: 0.228000 | 4.159 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 2.894 | Reg loss: 0.022 | Tree loss: 2.894 | Accuracy: 0.204778 | 4.158 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 3.191 | Reg loss: 0.021 | Tree loss: 3.191 | Accuracy: 0.222000 | 4.162 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 3.140 | Reg loss: 0.021 | Tree loss: 3.140 | Accuracy: 0.235500 | 4.161 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 3.086 | Reg loss: 0.021 | Tree loss: 3.086 | Accuracy: 0.231000 | 4.161 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 3.010 | Reg loss: 0.021 | Tree loss: 3.010 | Accuracy: 0.238500 | 4.161 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 2.988 | Reg loss: 0.022 | Tree loss: 2.988 | Accuracy: 0.223000 | 4.162 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 2.928 | Reg loss: 0.022 | Tree loss: 2.928 | Accuracy: 0.214000 | 4.159 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 2.887 | Reg loss: 0.022 | Tree loss: 2.887 | Accuracy: 0.238000 | 4.159 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 2.847 | Reg loss: 0.022 | Tree loss: 2.847 | Accuracy: 0.230500 | 4.16 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 2.845 | Reg loss: 0.022 | Tree loss: 2.845 | Accuracy: 0.243500 | 4.161 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 2.819 | Reg loss: 0.023 | Tree loss: 2.819 | Accuracy: 0.240500 | 4.163 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 2.812 | Reg loss: 0.023 | Tree loss: 2.812 | Accuracy: 0.262799 | 4.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 3.168 | Reg loss: 0.022 | Tree loss: 3.168 | Accuracy: 0.196000 | 4.17 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 3.115 | Reg loss: 0.022 | Tree loss: 3.115 | Accuracy: 0.213500 | 4.17 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 3.041 | Reg loss: 0.022 | Tree loss: 3.041 | Accuracy: 0.242000 | 4.168 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 2.982 | Reg loss: 0.022 | Tree loss: 2.982 | Accuracy: 0.238500 | 4.163 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 2.938 | Reg loss: 0.022 | Tree loss: 2.938 | Accuracy: 0.232000 | 4.163 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 2.876 | Reg loss: 0.022 | Tree loss: 2.876 | Accuracy: 0.239500 | 4.161 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 2.843 | Reg loss: 0.022 | Tree loss: 2.843 | Accuracy: 0.232000 | 4.163 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 2.832 | Reg loss: 0.023 | Tree loss: 2.832 | Accuracy: 0.225000 | 4.162 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 2.802 | Reg loss: 0.023 | Tree loss: 2.802 | Accuracy: 0.224500 | 4.16 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 2.791 | Reg loss: 0.023 | Tree loss: 2.791 | Accuracy: 0.244500 | 4.162 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 2.804 | Reg loss: 0.023 | Tree loss: 2.804 | Accuracy: 0.255973 | 4.161 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 3.138 | Reg loss: 0.022 | Tree loss: 3.138 | Accuracy: 0.214500 | 4.162 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 3.081 | Reg loss: 0.022 | Tree loss: 3.081 | Accuracy: 0.227000 | 4.163 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 3.009 | Reg loss: 0.022 | Tree loss: 3.009 | Accuracy: 0.236000 | 4.162 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 2.962 | Reg loss: 0.022 | Tree loss: 2.962 | Accuracy: 0.234000 | 4.163 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 2.888 | Reg loss: 0.022 | Tree loss: 2.888 | Accuracy: 0.239000 | 4.166 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 2.861 | Reg loss: 0.023 | Tree loss: 2.861 | Accuracy: 0.221000 | 4.167 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 2.811 | Reg loss: 0.023 | Tree loss: 2.811 | Accuracy: 0.243500 | 4.167 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 2.773 | Reg loss: 0.023 | Tree loss: 2.773 | Accuracy: 0.248000 | 4.166 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 2.777 | Reg loss: 0.023 | Tree loss: 2.777 | Accuracy: 0.232000 | 4.165 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 2.754 | Reg loss: 0.023 | Tree loss: 2.754 | Accuracy: 0.244500 | 4.165 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 2.715 | Reg loss: 0.024 | Tree loss: 2.715 | Accuracy: 0.269625 | 4.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 3.107 | Reg loss: 0.023 | Tree loss: 3.107 | Accuracy: 0.208500 | 4.152 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 3.040 | Reg loss: 0.023 | Tree loss: 3.040 | Accuracy: 0.226000 | 4.151 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 2.980 | Reg loss: 0.023 | Tree loss: 2.980 | Accuracy: 0.243000 | 4.149 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 2.906 | Reg loss: 0.023 | Tree loss: 2.906 | Accuracy: 0.249500 | 4.15 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 2.864 | Reg loss: 0.023 | Tree loss: 2.864 | Accuracy: 0.222000 | 4.153 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 2.798 | Reg loss: 0.023 | Tree loss: 2.798 | Accuracy: 0.248000 | 4.155 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 2.786 | Reg loss: 0.023 | Tree loss: 2.786 | Accuracy: 0.226500 | 4.157 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 2.760 | Reg loss: 0.023 | Tree loss: 2.760 | Accuracy: 0.238500 | 4.159 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 2.736 | Reg loss: 0.024 | Tree loss: 2.736 | Accuracy: 0.246500 | 4.159 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 2.722 | Reg loss: 0.024 | Tree loss: 2.722 | Accuracy: 0.253000 | 4.162 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 2.733 | Reg loss: 0.024 | Tree loss: 2.733 | Accuracy: 0.245734 | 4.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 3.058 | Reg loss: 0.023 | Tree loss: 3.058 | Accuracy: 0.219500 | 4.165 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 3.005 | Reg loss: 0.023 | Tree loss: 3.005 | Accuracy: 0.228500 | 4.164 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 2.953 | Reg loss: 0.023 | Tree loss: 2.953 | Accuracy: 0.217000 | 4.162 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 2.874 | Reg loss: 0.023 | Tree loss: 2.874 | Accuracy: 0.244500 | 4.16 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 2.829 | Reg loss: 0.023 | Tree loss: 2.829 | Accuracy: 0.238000 | 4.155 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 2.787 | Reg loss: 0.024 | Tree loss: 2.787 | Accuracy: 0.245000 | 4.152 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 2.735 | Reg loss: 0.024 | Tree loss: 2.735 | Accuracy: 0.252000 | 4.149 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 2.714 | Reg loss: 0.024 | Tree loss: 2.714 | Accuracy: 0.222000 | 4.149 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 2.684 | Reg loss: 0.024 | Tree loss: 2.684 | Accuracy: 0.259000 | 4.144 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 2.675 | Reg loss: 0.024 | Tree loss: 2.675 | Accuracy: 0.266000 | 4.138 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 2.691 | Reg loss: 0.024 | Tree loss: 2.691 | Accuracy: 0.259386 | 4.134 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 3.011 | Reg loss: 0.023 | Tree loss: 3.011 | Accuracy: 0.231500 | 4.141 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 2.967 | Reg loss: 0.024 | Tree loss: 2.967 | Accuracy: 0.210000 | 4.139 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 2.891 | Reg loss: 0.024 | Tree loss: 2.891 | Accuracy: 0.238000 | 4.136 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 2.836 | Reg loss: 0.024 | Tree loss: 2.836 | Accuracy: 0.239000 | 4.126 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 2.794 | Reg loss: 0.024 | Tree loss: 2.794 | Accuracy: 0.250000 | 4.118 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 2.757 | Reg loss: 0.024 | Tree loss: 2.757 | Accuracy: 0.252000 | 4.115 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 2.712 | Reg loss: 0.024 | Tree loss: 2.712 | Accuracy: 0.248500 | 4.115 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 2.690 | Reg loss: 0.024 | Tree loss: 2.690 | Accuracy: 0.255500 | 4.115 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 2.680 | Reg loss: 0.024 | Tree loss: 2.680 | Accuracy: 0.263000 | 4.115 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 2.644 | Reg loss: 0.025 | Tree loss: 2.644 | Accuracy: 0.260500 | 4.116 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 2.654 | Reg loss: 0.025 | Tree loss: 2.654 | Accuracy: 0.266212 | 4.115 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 2.996 | Reg loss: 0.024 | Tree loss: 2.996 | Accuracy: 0.217500 | 4.115 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 2.923 | Reg loss: 0.024 | Tree loss: 2.923 | Accuracy: 0.222500 | 4.113 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 2.874 | Reg loss: 0.024 | Tree loss: 2.874 | Accuracy: 0.238000 | 4.107 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 2.805 | Reg loss: 0.024 | Tree loss: 2.805 | Accuracy: 0.245000 | 4.101 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 2.746 | Reg loss: 0.024 | Tree loss: 2.746 | Accuracy: 0.257500 | 4.102 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 2.697 | Reg loss: 0.024 | Tree loss: 2.697 | Accuracy: 0.258000 | 4.103 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 2.678 | Reg loss: 0.024 | Tree loss: 2.678 | Accuracy: 0.264000 | 4.105 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 2.659 | Reg loss: 0.025 | Tree loss: 2.659 | Accuracy: 0.249500 | 4.108 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 2.632 | Reg loss: 0.025 | Tree loss: 2.632 | Accuracy: 0.242500 | 4.107 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 2.631 | Reg loss: 0.025 | Tree loss: 2.631 | Accuracy: 0.239500 | 4.108 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 2.576 | Reg loss: 0.025 | Tree loss: 2.576 | Accuracy: 0.242321 | 4.105 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 2.963 | Reg loss: 0.024 | Tree loss: 2.963 | Accuracy: 0.201500 | 4.103 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 2.884 | Reg loss: 0.024 | Tree loss: 2.884 | Accuracy: 0.238000 | 4.104 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 2.817 | Reg loss: 0.024 | Tree loss: 2.817 | Accuracy: 0.239500 | 4.103 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 2.763 | Reg loss: 0.024 | Tree loss: 2.763 | Accuracy: 0.256500 | 4.104 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 2.735 | Reg loss: 0.025 | Tree loss: 2.735 | Accuracy: 0.258000 | 4.104 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 2.679 | Reg loss: 0.025 | Tree loss: 2.679 | Accuracy: 0.260000 | 4.101 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 2.629 | Reg loss: 0.025 | Tree loss: 2.629 | Accuracy: 0.264000 | 4.1 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 2.613 | Reg loss: 0.025 | Tree loss: 2.613 | Accuracy: 0.246500 | 4.1 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 2.599 | Reg loss: 0.025 | Tree loss: 2.599 | Accuracy: 0.259000 | 4.1 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 2.606 | Reg loss: 0.025 | Tree loss: 2.606 | Accuracy: 0.250000 | 4.097 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 2.595 | Reg loss: 0.025 | Tree loss: 2.595 | Accuracy: 0.269625 | 4.095 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 2.913 | Reg loss: 0.025 | Tree loss: 2.913 | Accuracy: 0.225500 | 4.099 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 2.850 | Reg loss: 0.025 | Tree loss: 2.850 | Accuracy: 0.233500 | 4.099 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 2.792 | Reg loss: 0.025 | Tree loss: 2.792 | Accuracy: 0.239500 | 4.095 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 2.747 | Reg loss: 0.025 | Tree loss: 2.747 | Accuracy: 0.250500 | 4.094 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 2.685 | Reg loss: 0.025 | Tree loss: 2.685 | Accuracy: 0.266000 | 4.092 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 2.653 | Reg loss: 0.025 | Tree loss: 2.653 | Accuracy: 0.255500 | 4.09 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 2.614 | Reg loss: 0.025 | Tree loss: 2.614 | Accuracy: 0.255000 | 4.091 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 2.576 | Reg loss: 0.025 | Tree loss: 2.576 | Accuracy: 0.253000 | 4.089 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 2.559 | Reg loss: 0.025 | Tree loss: 2.559 | Accuracy: 0.252500 | 4.088 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 2.555 | Reg loss: 0.025 | Tree loss: 2.555 | Accuracy: 0.244000 | 4.088 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 2.493 | Reg loss: 0.026 | Tree loss: 2.493 | Accuracy: 0.266212 | 4.088 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 2.891 | Reg loss: 0.025 | Tree loss: 2.891 | Accuracy: 0.225000 | 4.088 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 2.818 | Reg loss: 0.025 | Tree loss: 2.818 | Accuracy: 0.245000 | 4.084 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 2.754 | Reg loss: 0.025 | Tree loss: 2.754 | Accuracy: 0.226000 | 4.083 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 2.691 | Reg loss: 0.025 | Tree loss: 2.691 | Accuracy: 0.254500 | 4.082 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 2.662 | Reg loss: 0.025 | Tree loss: 2.662 | Accuracy: 0.274000 | 4.081 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 2.609 | Reg loss: 0.025 | Tree loss: 2.609 | Accuracy: 0.264000 | 4.081 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 2.563 | Reg loss: 0.025 | Tree loss: 2.563 | Accuracy: 0.274500 | 4.08 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 2.569 | Reg loss: 0.025 | Tree loss: 2.569 | Accuracy: 0.259500 | 4.08 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 2.524 | Reg loss: 0.026 | Tree loss: 2.524 | Accuracy: 0.269000 | 4.079 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 2.507 | Reg loss: 0.026 | Tree loss: 2.507 | Accuracy: 0.266500 | 4.08 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 2.537 | Reg loss: 0.026 | Tree loss: 2.537 | Accuracy: 0.225256 | 4.08 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 2.839 | Reg loss: 0.025 | Tree loss: 2.839 | Accuracy: 0.232000 | 4.083 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 2.792 | Reg loss: 0.025 | Tree loss: 2.792 | Accuracy: 0.235500 | 4.081 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 2.714 | Reg loss: 0.025 | Tree loss: 2.714 | Accuracy: 0.237000 | 4.078 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 2.655 | Reg loss: 0.025 | Tree loss: 2.655 | Accuracy: 0.258500 | 4.078 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 2.618 | Reg loss: 0.025 | Tree loss: 2.618 | Accuracy: 0.269500 | 4.076 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 2.589 | Reg loss: 0.026 | Tree loss: 2.589 | Accuracy: 0.260000 | 4.076 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 2.541 | Reg loss: 0.026 | Tree loss: 2.541 | Accuracy: 0.259000 | 4.077 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 2.522 | Reg loss: 0.026 | Tree loss: 2.522 | Accuracy: 0.258500 | 4.078 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 2.502 | Reg loss: 0.026 | Tree loss: 2.502 | Accuracy: 0.263500 | 4.079 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 2.482 | Reg loss: 0.026 | Tree loss: 2.482 | Accuracy: 0.267000 | 4.076 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 2.425 | Reg loss: 0.026 | Tree loss: 2.425 | Accuracy: 0.300341 | 4.073 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 2.809 | Reg loss: 0.026 | Tree loss: 2.809 | Accuracy: 0.241000 | 4.073 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 2.768 | Reg loss: 0.026 | Tree loss: 2.768 | Accuracy: 0.210500 | 4.072 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 2.679 | Reg loss: 0.026 | Tree loss: 2.679 | Accuracy: 0.246000 | 4.069 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 2.615 | Reg loss: 0.026 | Tree loss: 2.615 | Accuracy: 0.265500 | 4.069 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 2.585 | Reg loss: 0.026 | Tree loss: 2.585 | Accuracy: 0.284500 | 4.067 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 2.543 | Reg loss: 0.026 | Tree loss: 2.543 | Accuracy: 0.272000 | 4.067 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 2.521 | Reg loss: 0.026 | Tree loss: 2.521 | Accuracy: 0.264000 | 4.064 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 2.474 | Reg loss: 0.026 | Tree loss: 2.474 | Accuracy: 0.283500 | 4.061 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 2.460 | Reg loss: 0.026 | Tree loss: 2.460 | Accuracy: 0.272000 | 4.059 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 2.471 | Reg loss: 0.026 | Tree loss: 2.471 | Accuracy: 0.256000 | 4.058 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 2.472 | Reg loss: 0.026 | Tree loss: 2.472 | Accuracy: 0.232082 | 4.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 2.765 | Reg loss: 0.026 | Tree loss: 2.765 | Accuracy: 0.237500 | 4.058 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 2.702 | Reg loss: 0.026 | Tree loss: 2.702 | Accuracy: 0.242000 | 4.055 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 2.649 | Reg loss: 0.026 | Tree loss: 2.649 | Accuracy: 0.246500 | 4.053 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 2.596 | Reg loss: 0.026 | Tree loss: 2.596 | Accuracy: 0.268500 | 4.054 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 2.545 | Reg loss: 0.026 | Tree loss: 2.545 | Accuracy: 0.281000 | 4.053 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 2.518 | Reg loss: 0.026 | Tree loss: 2.518 | Accuracy: 0.274500 | 4.048 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 2.463 | Reg loss: 0.026 | Tree loss: 2.463 | Accuracy: 0.282500 | 4.049 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 2.463 | Reg loss: 0.026 | Tree loss: 2.463 | Accuracy: 0.274000 | 4.046 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 2.470 | Reg loss: 0.026 | Tree loss: 2.470 | Accuracy: 0.254000 | 4.046 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 2.435 | Reg loss: 0.026 | Tree loss: 2.435 | Accuracy: 0.279000 | 4.047 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 2.435 | Reg loss: 0.027 | Tree loss: 2.435 | Accuracy: 0.221843 | 4.045 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 2.710 | Reg loss: 0.026 | Tree loss: 2.710 | Accuracy: 0.255500 | 4.049 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 2.690 | Reg loss: 0.026 | Tree loss: 2.690 | Accuracy: 0.233000 | 4.047 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 2.623 | Reg loss: 0.026 | Tree loss: 2.623 | Accuracy: 0.254000 | 4.043 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 2.560 | Reg loss: 0.026 | Tree loss: 2.560 | Accuracy: 0.271500 | 4.04 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 2.523 | Reg loss: 0.026 | Tree loss: 2.523 | Accuracy: 0.274500 | 4.039 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 2.492 | Reg loss: 0.026 | Tree loss: 2.492 | Accuracy: 0.289000 | 4.039 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 2.475 | Reg loss: 0.026 | Tree loss: 2.475 | Accuracy: 0.262000 | 4.038 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 2.444 | Reg loss: 0.026 | Tree loss: 2.444 | Accuracy: 0.268000 | 4.037 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 2.398 | Reg loss: 0.027 | Tree loss: 2.398 | Accuracy: 0.276500 | 4.037 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 2.400 | Reg loss: 0.027 | Tree loss: 2.400 | Accuracy: 0.277500 | 4.036 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 2.350 | Reg loss: 0.027 | Tree loss: 2.350 | Accuracy: 0.269625 | 4.034 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 2.692 | Reg loss: 0.026 | Tree loss: 2.692 | Accuracy: 0.239000 | 4.036 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 2.642 | Reg loss: 0.026 | Tree loss: 2.642 | Accuracy: 0.240500 | 4.036 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 2.596 | Reg loss: 0.026 | Tree loss: 2.596 | Accuracy: 0.250500 | 4.035 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 2.541 | Reg loss: 0.026 | Tree loss: 2.541 | Accuracy: 0.284000 | 4.034 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 2.496 | Reg loss: 0.026 | Tree loss: 2.496 | Accuracy: 0.290000 | 4.031 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 2.454 | Reg loss: 0.026 | Tree loss: 2.454 | Accuracy: 0.272000 | 4.031 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 2.421 | Reg loss: 0.027 | Tree loss: 2.421 | Accuracy: 0.291500 | 4.031 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 2.391 | Reg loss: 0.027 | Tree loss: 2.391 | Accuracy: 0.279500 | 4.029 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 2.395 | Reg loss: 0.027 | Tree loss: 2.395 | Accuracy: 0.279000 | 4.028 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 2.393 | Reg loss: 0.027 | Tree loss: 2.393 | Accuracy: 0.280000 | 4.026 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 2.395 | Reg loss: 0.027 | Tree loss: 2.395 | Accuracy: 0.266212 | 4.025 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 2.666 | Reg loss: 0.026 | Tree loss: 2.666 | Accuracy: 0.242500 | 4.028 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 2.611 | Reg loss: 0.026 | Tree loss: 2.611 | Accuracy: 0.241000 | 4.027 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 2.557 | Reg loss: 0.027 | Tree loss: 2.557 | Accuracy: 0.269500 | 4.027 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 2.499 | Reg loss: 0.027 | Tree loss: 2.499 | Accuracy: 0.294000 | 4.026 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 2.452 | Reg loss: 0.027 | Tree loss: 2.452 | Accuracy: 0.287500 | 4.025 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 2.424 | Reg loss: 0.027 | Tree loss: 2.424 | Accuracy: 0.284000 | 4.023 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 2.414 | Reg loss: 0.027 | Tree loss: 2.414 | Accuracy: 0.279500 | 4.022 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 2.391 | Reg loss: 0.027 | Tree loss: 2.391 | Accuracy: 0.263500 | 4.021 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 2.391 | Reg loss: 0.027 | Tree loss: 2.391 | Accuracy: 0.277000 | 4.021 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 2.350 | Reg loss: 0.027 | Tree loss: 2.350 | Accuracy: 0.278500 | 4.021 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 2.354 | Reg loss: 0.027 | Tree loss: 2.354 | Accuracy: 0.320819 | 4.02 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 2.627 | Reg loss: 0.027 | Tree loss: 2.627 | Accuracy: 0.243000 | 4.023 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 2.573 | Reg loss: 0.027 | Tree loss: 2.573 | Accuracy: 0.243500 | 4.021 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 2.550 | Reg loss: 0.027 | Tree loss: 2.550 | Accuracy: 0.267000 | 4.02 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 2.489 | Reg loss: 0.027 | Tree loss: 2.489 | Accuracy: 0.278000 | 4.021 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 2.450 | Reg loss: 0.027 | Tree loss: 2.450 | Accuracy: 0.295000 | 4.021 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 2.390 | Reg loss: 0.027 | Tree loss: 2.390 | Accuracy: 0.291500 | 4.021 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 2.373 | Reg loss: 0.027 | Tree loss: 2.373 | Accuracy: 0.286500 | 4.021 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 2.379 | Reg loss: 0.027 | Tree loss: 2.379 | Accuracy: 0.280500 | 4.02 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 2.338 | Reg loss: 0.027 | Tree loss: 2.338 | Accuracy: 0.291500 | 4.019 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 2.329 | Reg loss: 0.027 | Tree loss: 2.329 | Accuracy: 0.286500 | 4.018 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 2.284 | Reg loss: 0.027 | Tree loss: 2.284 | Accuracy: 0.279863 | 4.017 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 2.589 | Reg loss: 0.027 | Tree loss: 2.589 | Accuracy: 0.255500 | 4.018 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 2.555 | Reg loss: 0.027 | Tree loss: 2.555 | Accuracy: 0.255500 | 4.017 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 2.502 | Reg loss: 0.027 | Tree loss: 2.502 | Accuracy: 0.278000 | 4.016 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 2.451 | Reg loss: 0.027 | Tree loss: 2.451 | Accuracy: 0.286500 | 4.014 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 2.423 | Reg loss: 0.027 | Tree loss: 2.423 | Accuracy: 0.277000 | 4.013 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 2.385 | Reg loss: 0.027 | Tree loss: 2.385 | Accuracy: 0.286000 | 4.013 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 2.358 | Reg loss: 0.027 | Tree loss: 2.358 | Accuracy: 0.278000 | 4.012 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 2.349 | Reg loss: 0.027 | Tree loss: 2.349 | Accuracy: 0.279000 | 4.013 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 2.329 | Reg loss: 0.027 | Tree loss: 2.329 | Accuracy: 0.288500 | 4.014 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 2.307 | Reg loss: 0.027 | Tree loss: 2.307 | Accuracy: 0.297000 | 4.012 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 2.332 | Reg loss: 0.028 | Tree loss: 2.332 | Accuracy: 0.283276 | 4.014 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 2.568 | Reg loss: 0.027 | Tree loss: 2.568 | Accuracy: 0.254500 | 4.017 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 2.517 | Reg loss: 0.027 | Tree loss: 2.517 | Accuracy: 0.269000 | 4.015 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 2.494 | Reg loss: 0.027 | Tree loss: 2.494 | Accuracy: 0.274000 | 4.013 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 2.426 | Reg loss: 0.027 | Tree loss: 2.426 | Accuracy: 0.298500 | 4.011 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 2.411 | Reg loss: 0.027 | Tree loss: 2.411 | Accuracy: 0.272500 | 4.007 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 2.368 | Reg loss: 0.027 | Tree loss: 2.368 | Accuracy: 0.291000 | 4.006 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 2.350 | Reg loss: 0.027 | Tree loss: 2.350 | Accuracy: 0.278000 | 4.006 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 2.306 | Reg loss: 0.027 | Tree loss: 2.306 | Accuracy: 0.284500 | 4.005 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 2.292 | Reg loss: 0.028 | Tree loss: 2.292 | Accuracy: 0.297000 | 4.004 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 2.274 | Reg loss: 0.028 | Tree loss: 2.274 | Accuracy: 0.303000 | 4.005 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 2.301 | Reg loss: 0.028 | Tree loss: 2.301 | Accuracy: 0.252560 | 4.004 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 2.541 | Reg loss: 0.027 | Tree loss: 2.541 | Accuracy: 0.259500 | 4.006 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 2.482 | Reg loss: 0.027 | Tree loss: 2.482 | Accuracy: 0.271500 | 4.004 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 2.462 | Reg loss: 0.027 | Tree loss: 2.462 | Accuracy: 0.279500 | 4.003 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 2.410 | Reg loss: 0.027 | Tree loss: 2.410 | Accuracy: 0.285000 | 4.003 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 2.378 | Reg loss: 0.027 | Tree loss: 2.378 | Accuracy: 0.291500 | 4.001 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 2.335 | Reg loss: 0.028 | Tree loss: 2.335 | Accuracy: 0.288500 | 4.0 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 2.327 | Reg loss: 0.028 | Tree loss: 2.327 | Accuracy: 0.280500 | 3.999 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 2.286 | Reg loss: 0.028 | Tree loss: 2.286 | Accuracy: 0.302500 | 3.998 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 2.287 | Reg loss: 0.028 | Tree loss: 2.287 | Accuracy: 0.301500 | 3.996 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 2.275 | Reg loss: 0.028 | Tree loss: 2.275 | Accuracy: 0.268500 | 3.995 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 2.237 | Reg loss: 0.028 | Tree loss: 2.237 | Accuracy: 0.293515 | 3.994 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 2.511 | Reg loss: 0.027 | Tree loss: 2.511 | Accuracy: 0.260000 | 3.997 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 2.472 | Reg loss: 0.028 | Tree loss: 2.472 | Accuracy: 0.276500 | 3.997 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 2.471 | Reg loss: 0.028 | Tree loss: 2.471 | Accuracy: 0.268000 | 3.997 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 2.384 | Reg loss: 0.028 | Tree loss: 2.384 | Accuracy: 0.290000 | 3.997 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 2.360 | Reg loss: 0.028 | Tree loss: 2.360 | Accuracy: 0.303500 | 3.998 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 2.318 | Reg loss: 0.028 | Tree loss: 2.318 | Accuracy: 0.291000 | 3.996 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 2.293 | Reg loss: 0.028 | Tree loss: 2.293 | Accuracy: 0.271000 | 3.995 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 2.276 | Reg loss: 0.028 | Tree loss: 2.276 | Accuracy: 0.295000 | 3.995 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 2.254 | Reg loss: 0.028 | Tree loss: 2.254 | Accuracy: 0.313000 | 3.994 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 2.246 | Reg loss: 0.028 | Tree loss: 2.246 | Accuracy: 0.286000 | 3.995 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 2.234 | Reg loss: 0.028 | Tree loss: 2.234 | Accuracy: 0.313993 | 3.996 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 2.457 | Reg loss: 0.028 | Tree loss: 2.457 | Accuracy: 0.277500 | 3.999 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 2.447 | Reg loss: 0.028 | Tree loss: 2.447 | Accuracy: 0.276000 | 4.0 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 2.404 | Reg loss: 0.028 | Tree loss: 2.404 | Accuracy: 0.287000 | 3.999 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 2.372 | Reg loss: 0.028 | Tree loss: 2.372 | Accuracy: 0.289500 | 3.998 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 2.346 | Reg loss: 0.028 | Tree loss: 2.346 | Accuracy: 0.292000 | 3.998 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 2.292 | Reg loss: 0.028 | Tree loss: 2.292 | Accuracy: 0.290500 | 3.998 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 2.309 | Reg loss: 0.028 | Tree loss: 2.309 | Accuracy: 0.260500 | 3.998 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 2.283 | Reg loss: 0.028 | Tree loss: 2.283 | Accuracy: 0.292000 | 3.997 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 2.255 | Reg loss: 0.028 | Tree loss: 2.255 | Accuracy: 0.292500 | 3.997 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 2.245 | Reg loss: 0.028 | Tree loss: 2.245 | Accuracy: 0.284000 | 3.998 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 2.148 | Reg loss: 0.028 | Tree loss: 2.148 | Accuracy: 0.317406 | 3.998 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 2.466 | Reg loss: 0.028 | Tree loss: 2.466 | Accuracy: 0.275500 | 4.001 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 2.418 | Reg loss: 0.028 | Tree loss: 2.418 | Accuracy: 0.288000 | 4.0 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 2.394 | Reg loss: 0.028 | Tree loss: 2.394 | Accuracy: 0.276500 | 4.0 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 2.352 | Reg loss: 0.028 | Tree loss: 2.352 | Accuracy: 0.294500 | 3.999 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 2.317 | Reg loss: 0.028 | Tree loss: 2.317 | Accuracy: 0.292000 | 4.0 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 2.288 | Reg loss: 0.028 | Tree loss: 2.288 | Accuracy: 0.278000 | 3.999 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 2.268 | Reg loss: 0.028 | Tree loss: 2.268 | Accuracy: 0.301000 | 3.998 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 2.263 | Reg loss: 0.028 | Tree loss: 2.263 | Accuracy: 0.272500 | 3.998 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 2.233 | Reg loss: 0.028 | Tree loss: 2.233 | Accuracy: 0.279500 | 3.998 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 2.216 | Reg loss: 0.028 | Tree loss: 2.216 | Accuracy: 0.294000 | 3.998 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 2.232 | Reg loss: 0.028 | Tree loss: 2.232 | Accuracy: 0.293515 | 3.999 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 2.442 | Reg loss: 0.028 | Tree loss: 2.442 | Accuracy: 0.282500 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 2.404 | Reg loss: 0.028 | Tree loss: 2.404 | Accuracy: 0.272000 | 3.999 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 2.377 | Reg loss: 0.028 | Tree loss: 2.377 | Accuracy: 0.282500 | 3.999 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 2.350 | Reg loss: 0.028 | Tree loss: 2.350 | Accuracy: 0.280500 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 2.316 | Reg loss: 0.028 | Tree loss: 2.316 | Accuracy: 0.285000 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 2.269 | Reg loss: 0.028 | Tree loss: 2.269 | Accuracy: 0.292000 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 2.264 | Reg loss: 0.028 | Tree loss: 2.264 | Accuracy: 0.279500 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 2.242 | Reg loss: 0.028 | Tree loss: 2.242 | Accuracy: 0.281000 | 3.999 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 2.216 | Reg loss: 0.028 | Tree loss: 2.216 | Accuracy: 0.302000 | 4.0 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 2.191 | Reg loss: 0.029 | Tree loss: 2.191 | Accuracy: 0.313500 | 4.001 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 2.178 | Reg loss: 0.029 | Tree loss: 2.178 | Accuracy: 0.273038 | 3.997 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 2.424 | Reg loss: 0.028 | Tree loss: 2.424 | Accuracy: 0.282000 | 3.992 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 2.377 | Reg loss: 0.028 | Tree loss: 2.377 | Accuracy: 0.294500 | 3.991 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 2.331 | Reg loss: 0.028 | Tree loss: 2.331 | Accuracy: 0.301500 | 3.987 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 2.320 | Reg loss: 0.028 | Tree loss: 2.320 | Accuracy: 0.286000 | 3.988 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 2.301 | Reg loss: 0.028 | Tree loss: 2.301 | Accuracy: 0.287000 | 3.989 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 2.275 | Reg loss: 0.028 | Tree loss: 2.275 | Accuracy: 0.284000 | 3.99 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 2.239 | Reg loss: 0.029 | Tree loss: 2.239 | Accuracy: 0.286500 | 3.988 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 2.232 | Reg loss: 0.029 | Tree loss: 2.232 | Accuracy: 0.286000 | 3.987 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 2.214 | Reg loss: 0.029 | Tree loss: 2.214 | Accuracy: 0.277500 | 3.987 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 2.197 | Reg loss: 0.029 | Tree loss: 2.197 | Accuracy: 0.291000 | 3.986 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 2.181 | Reg loss: 0.029 | Tree loss: 2.181 | Accuracy: 0.293515 | 3.985 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 2.428 | Reg loss: 0.028 | Tree loss: 2.428 | Accuracy: 0.258000 | 3.985 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 2.389 | Reg loss: 0.028 | Tree loss: 2.389 | Accuracy: 0.278500 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.285000 | 3.983 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 2.281 | Reg loss: 0.029 | Tree loss: 2.281 | Accuracy: 0.294000 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 2.263 | Reg loss: 0.029 | Tree loss: 2.263 | Accuracy: 0.300500 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 2.240 | Reg loss: 0.029 | Tree loss: 2.240 | Accuracy: 0.278500 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 2.216 | Reg loss: 0.029 | Tree loss: 2.216 | Accuracy: 0.296000 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 2.194 | Reg loss: 0.029 | Tree loss: 2.194 | Accuracy: 0.296500 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 2.212 | Reg loss: 0.029 | Tree loss: 2.212 | Accuracy: 0.281000 | 3.984 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 2.183 | Reg loss: 0.029 | Tree loss: 2.183 | Accuracy: 0.290000 | 3.985 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 2.191 | Reg loss: 0.029 | Tree loss: 2.191 | Accuracy: 0.290102 | 3.986 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 2.383 | Reg loss: 0.029 | Tree loss: 2.383 | Accuracy: 0.283000 | 3.985 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 2.367 | Reg loss: 0.029 | Tree loss: 2.367 | Accuracy: 0.289000 | 3.984 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 2.296 | Reg loss: 0.029 | Tree loss: 2.296 | Accuracy: 0.306000 | 3.982 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 2.287 | Reg loss: 0.029 | Tree loss: 2.287 | Accuracy: 0.298000 | 3.982 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 2.268 | Reg loss: 0.029 | Tree loss: 2.268 | Accuracy: 0.277000 | 3.981 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 2.233 | Reg loss: 0.029 | Tree loss: 2.233 | Accuracy: 0.285500 | 3.98 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 2.241 | Reg loss: 0.029 | Tree loss: 2.241 | Accuracy: 0.269000 | 3.98 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 2.185 | Reg loss: 0.029 | Tree loss: 2.185 | Accuracy: 0.297500 | 3.979 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 2.186 | Reg loss: 0.029 | Tree loss: 2.186 | Accuracy: 0.291500 | 3.979 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 2.179 | Reg loss: 0.029 | Tree loss: 2.179 | Accuracy: 0.298500 | 3.981 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 2.149 | Reg loss: 0.029 | Tree loss: 2.149 | Accuracy: 0.276451 | 3.982 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 2.378 | Reg loss: 0.029 | Tree loss: 2.378 | Accuracy: 0.284500 | 3.985 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.283500 | 3.985 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 2.308 | Reg loss: 0.029 | Tree loss: 2.308 | Accuracy: 0.281000 | 3.986 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 2.273 | Reg loss: 0.029 | Tree loss: 2.273 | Accuracy: 0.289500 | 3.986 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 2.249 | Reg loss: 0.029 | Tree loss: 2.249 | Accuracy: 0.282500 | 3.984 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 2.224 | Reg loss: 0.029 | Tree loss: 2.224 | Accuracy: 0.300000 | 3.983 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 2.202 | Reg loss: 0.029 | Tree loss: 2.202 | Accuracy: 0.277500 | 3.982 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 2.194 | Reg loss: 0.029 | Tree loss: 2.194 | Accuracy: 0.281500 | 3.978 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 2.151 | Reg loss: 0.029 | Tree loss: 2.151 | Accuracy: 0.300500 | 3.977 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 2.167 | Reg loss: 0.029 | Tree loss: 2.167 | Accuracy: 0.285000 | 3.974 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 2.168 | Reg loss: 0.029 | Tree loss: 2.168 | Accuracy: 0.324232 | 3.97 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.276500 | 3.972 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 2.289 | Reg loss: 0.029 | Tree loss: 2.289 | Accuracy: 0.294500 | 3.971 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 2.295 | Reg loss: 0.029 | Tree loss: 2.295 | Accuracy: 0.281500 | 3.97 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 2.269 | Reg loss: 0.029 | Tree loss: 2.269 | Accuracy: 0.285500 | 3.969 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 2.242 | Reg loss: 0.029 | Tree loss: 2.242 | Accuracy: 0.286000 | 3.968 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 2.205 | Reg loss: 0.029 | Tree loss: 2.205 | Accuracy: 0.293500 | 3.967 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 2.215 | Reg loss: 0.029 | Tree loss: 2.215 | Accuracy: 0.275500 | 3.966 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 2.186 | Reg loss: 0.029 | Tree loss: 2.186 | Accuracy: 0.286000 | 3.965 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 2.147 | Reg loss: 0.029 | Tree loss: 2.147 | Accuracy: 0.302500 | 3.963 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.288500 | 3.963 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 2.197 | Reg loss: 0.030 | Tree loss: 2.197 | Accuracy: 0.283276 | 3.962 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.297000 | 3.963 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.283500 | 3.962 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 2.284 | Reg loss: 0.029 | Tree loss: 2.284 | Accuracy: 0.282000 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 2.242 | Reg loss: 0.029 | Tree loss: 2.242 | Accuracy: 0.302500 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 2.208 | Reg loss: 0.029 | Tree loss: 2.208 | Accuracy: 0.297500 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 2.202 | Reg loss: 0.029 | Tree loss: 2.202 | Accuracy: 0.280000 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 2.195 | Reg loss: 0.029 | Tree loss: 2.195 | Accuracy: 0.279000 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.297500 | 3.961 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 2.168 | Reg loss: 0.030 | Tree loss: 2.168 | Accuracy: 0.286000 | 3.958 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.287000 | 3.957 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 2.092 | Reg loss: 0.030 | Tree loss: 2.092 | Accuracy: 0.331058 | 3.955 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 2.307 | Reg loss: 0.029 | Tree loss: 2.307 | Accuracy: 0.299500 | 3.955 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 2.310 | Reg loss: 0.029 | Tree loss: 2.310 | Accuracy: 0.281500 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 2.278 | Reg loss: 0.029 | Tree loss: 2.278 | Accuracy: 0.284000 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.297000 | 3.953 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.289500 | 3.953 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.280500 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.285000 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.278500 | 3.955 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 2.148 | Reg loss: 0.030 | Tree loss: 2.148 | Accuracy: 0.288000 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 2.144 | Reg loss: 0.030 | Tree loss: 2.144 | Accuracy: 0.288000 | 3.954 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 2.105 | Reg loss: 0.030 | Tree loss: 2.105 | Accuracy: 0.252560 | 3.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 2.325 | Reg loss: 0.030 | Tree loss: 2.325 | Accuracy: 0.290000 | 3.953 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 2.284 | Reg loss: 0.030 | Tree loss: 2.284 | Accuracy: 0.288500 | 3.952 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 2.258 | Reg loss: 0.030 | Tree loss: 2.258 | Accuracy: 0.285500 | 3.951 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 2.244 | Reg loss: 0.030 | Tree loss: 2.244 | Accuracy: 0.295000 | 3.95 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.282000 | 3.951 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.316500 | 3.951 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.302500 | 3.952 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 2.134 | Reg loss: 0.030 | Tree loss: 2.134 | Accuracy: 0.299500 | 3.952 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 2.135 | Reg loss: 0.030 | Tree loss: 2.135 | Accuracy: 0.307500 | 3.951 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 2.099 | Reg loss: 0.030 | Tree loss: 2.099 | Accuracy: 0.320500 | 3.952 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 2.092 | Reg loss: 0.030 | Tree loss: 2.092 | Accuracy: 0.351536 | 3.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 2.308 | Reg loss: 0.030 | Tree loss: 2.308 | Accuracy: 0.298500 | 3.952 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 2.273 | Reg loss: 0.030 | Tree loss: 2.273 | Accuracy: 0.282000 | 3.951 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.313000 | 3.95 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 2.231 | Reg loss: 0.030 | Tree loss: 2.231 | Accuracy: 0.292000 | 3.949 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 2.196 | Reg loss: 0.030 | Tree loss: 2.196 | Accuracy: 0.300000 | 3.95 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.294500 | 3.95 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 2.147 | Reg loss: 0.030 | Tree loss: 2.147 | Accuracy: 0.293000 | 3.95 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.278000 | 3.95 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 2.138 | Reg loss: 0.030 | Tree loss: 2.138 | Accuracy: 0.291000 | 3.951 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 2.124 | Reg loss: 0.030 | Tree loss: 2.124 | Accuracy: 0.297000 | 3.951 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 2.119 | Reg loss: 0.030 | Tree loss: 2.119 | Accuracy: 0.279863 | 3.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 2.282 | Reg loss: 0.030 | Tree loss: 2.282 | Accuracy: 0.286000 | 3.953 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 2.267 | Reg loss: 0.030 | Tree loss: 2.267 | Accuracy: 0.287500 | 3.95 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.294500 | 3.949 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.302000 | 3.95 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 2.218 | Reg loss: 0.030 | Tree loss: 2.218 | Accuracy: 0.272000 | 3.95 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.321000 | 3.949 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 2.146 | Reg loss: 0.030 | Tree loss: 2.146 | Accuracy: 0.295000 | 3.948 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 2.146 | Reg loss: 0.030 | Tree loss: 2.146 | Accuracy: 0.287000 | 3.947 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 2.113 | Reg loss: 0.030 | Tree loss: 2.113 | Accuracy: 0.304500 | 3.947 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 2.093 | Reg loss: 0.030 | Tree loss: 2.093 | Accuracy: 0.303500 | 3.947 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 2.054 | Reg loss: 0.030 | Tree loss: 2.054 | Accuracy: 0.310580 | 3.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 2.286 | Reg loss: 0.030 | Tree loss: 2.286 | Accuracy: 0.273000 | 3.947 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.295000 | 3.946 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.297000 | 3.945 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.284500 | 3.945 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.292500 | 3.944 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.292500 | 3.944 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 2.121 | Reg loss: 0.030 | Tree loss: 2.121 | Accuracy: 0.307500 | 3.944 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 2.119 | Reg loss: 0.031 | Tree loss: 2.119 | Accuracy: 0.282500 | 3.944 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 2.113 | Reg loss: 0.031 | Tree loss: 2.113 | Accuracy: 0.284000 | 3.943 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 2.114 | Reg loss: 0.031 | Tree loss: 2.114 | Accuracy: 0.280500 | 3.942 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 2.085 | Reg loss: 0.031 | Tree loss: 2.085 | Accuracy: 0.307167 | 3.942 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 2.282 | Reg loss: 0.030 | Tree loss: 2.282 | Accuracy: 0.274500 | 3.943 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 2.262 | Reg loss: 0.030 | Tree loss: 2.262 | Accuracy: 0.273500 | 3.942 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.314500 | 3.942 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.300500 | 3.942 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 2.163 | Reg loss: 0.031 | Tree loss: 2.163 | Accuracy: 0.287500 | 3.943 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 2.132 | Reg loss: 0.031 | Tree loss: 2.132 | Accuracy: 0.292500 | 3.944 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 2.145 | Reg loss: 0.031 | Tree loss: 2.145 | Accuracy: 0.297000 | 3.945 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 2.121 | Reg loss: 0.031 | Tree loss: 2.121 | Accuracy: 0.298500 | 3.944 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 2.092 | Reg loss: 0.031 | Tree loss: 2.092 | Accuracy: 0.301500 | 3.941 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 2.091 | Reg loss: 0.031 | Tree loss: 2.091 | Accuracy: 0.305000 | 3.939 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 2.073 | Reg loss: 0.031 | Tree loss: 2.073 | Accuracy: 0.303754 | 3.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.278000 | 3.937 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.292500 | 3.937 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 2.194 | Reg loss: 0.031 | Tree loss: 2.194 | Accuracy: 0.316500 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 2.163 | Reg loss: 0.031 | Tree loss: 2.163 | Accuracy: 0.307000 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 2.156 | Reg loss: 0.031 | Tree loss: 2.156 | Accuracy: 0.300000 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 2.128 | Reg loss: 0.031 | Tree loss: 2.128 | Accuracy: 0.305500 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 2.132 | Reg loss: 0.031 | Tree loss: 2.132 | Accuracy: 0.283000 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 2.115 | Reg loss: 0.031 | Tree loss: 2.115 | Accuracy: 0.308000 | 3.935 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 2.111 | Reg loss: 0.031 | Tree loss: 2.111 | Accuracy: 0.304000 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 2.114 | Reg loss: 0.031 | Tree loss: 2.114 | Accuracy: 0.287000 | 3.936 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 2.059 | Reg loss: 0.031 | Tree loss: 2.059 | Accuracy: 0.327645 | 3.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 2.270 | Reg loss: 0.031 | Tree loss: 2.270 | Accuracy: 0.285500 | 3.938 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.303000 | 3.937 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.287000 | 3.935 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 2.166 | Reg loss: 0.031 | Tree loss: 2.166 | Accuracy: 0.316500 | 3.936 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 2.143 | Reg loss: 0.031 | Tree loss: 2.143 | Accuracy: 0.299500 | 3.936 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 2.138 | Reg loss: 0.031 | Tree loss: 2.138 | Accuracy: 0.298500 | 3.936 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 2.106 | Reg loss: 0.031 | Tree loss: 2.106 | Accuracy: 0.290500 | 3.936 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 2.102 | Reg loss: 0.031 | Tree loss: 2.102 | Accuracy: 0.298000 | 3.937 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 2.070 | Reg loss: 0.031 | Tree loss: 2.070 | Accuracy: 0.327500 | 3.937 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 2.082 | Reg loss: 0.031 | Tree loss: 2.082 | Accuracy: 0.323000 | 3.934 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 2.086 | Reg loss: 0.031 | Tree loss: 2.086 | Accuracy: 0.310580 | 3.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 2.242 | Reg loss: 0.031 | Tree loss: 2.242 | Accuracy: 0.292500 | 3.934 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 2.185 | Reg loss: 0.031 | Tree loss: 2.185 | Accuracy: 0.298500 | 3.932 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.282000 | 3.932 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 2.172 | Reg loss: 0.031 | Tree loss: 2.172 | Accuracy: 0.303500 | 3.932 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 2.145 | Reg loss: 0.031 | Tree loss: 2.145 | Accuracy: 0.295500 | 3.933 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 2.125 | Reg loss: 0.031 | Tree loss: 2.125 | Accuracy: 0.311500 | 3.933 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 2.091 | Reg loss: 0.031 | Tree loss: 2.091 | Accuracy: 0.313500 | 3.933 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 2.094 | Reg loss: 0.031 | Tree loss: 2.094 | Accuracy: 0.302500 | 3.932 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 2.082 | Reg loss: 0.031 | Tree loss: 2.082 | Accuracy: 0.310500 | 3.933 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 2.071 | Reg loss: 0.031 | Tree loss: 2.071 | Accuracy: 0.325500 | 3.934 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 2.053 | Reg loss: 0.031 | Tree loss: 2.053 | Accuracy: 0.337884 | 3.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.305000 | 3.935 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 2.210 | Reg loss: 0.031 | Tree loss: 2.210 | Accuracy: 0.292500 | 3.934 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 2.185 | Reg loss: 0.031 | Tree loss: 2.185 | Accuracy: 0.295500 | 3.933 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 2.159 | Reg loss: 0.031 | Tree loss: 2.159 | Accuracy: 0.283000 | 3.932 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 2.116 | Reg loss: 0.031 | Tree loss: 2.116 | Accuracy: 0.313000 | 3.931 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 2.102 | Reg loss: 0.031 | Tree loss: 2.102 | Accuracy: 0.313000 | 3.932 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 2.100 | Reg loss: 0.031 | Tree loss: 2.100 | Accuracy: 0.307500 | 3.931 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 2.096 | Reg loss: 0.031 | Tree loss: 2.096 | Accuracy: 0.289500 | 3.932 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 2.074 | Reg loss: 0.032 | Tree loss: 2.074 | Accuracy: 0.300000 | 3.932 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 2.098 | Reg loss: 0.032 | Tree loss: 2.098 | Accuracy: 0.301500 | 3.932 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 2.031 | Reg loss: 0.032 | Tree loss: 2.031 | Accuracy: 0.341297 | 3.933 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.280000 | 3.935 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 2.187 | Reg loss: 0.031 | Tree loss: 2.187 | Accuracy: 0.288000 | 3.935 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 2.166 | Reg loss: 0.031 | Tree loss: 2.166 | Accuracy: 0.313500 | 3.935 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 2.142 | Reg loss: 0.031 | Tree loss: 2.142 | Accuracy: 0.298500 | 3.935 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 2.148 | Reg loss: 0.032 | Tree loss: 2.148 | Accuracy: 0.301000 | 3.936 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 2.115 | Reg loss: 0.032 | Tree loss: 2.115 | Accuracy: 0.299000 | 3.936 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 2.095 | Reg loss: 0.032 | Tree loss: 2.095 | Accuracy: 0.317000 | 3.935 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 2.052 | Reg loss: 0.032 | Tree loss: 2.052 | Accuracy: 0.336000 | 3.934 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 2.063 | Reg loss: 0.032 | Tree loss: 2.063 | Accuracy: 0.320000 | 3.933 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 2.081 | Reg loss: 0.032 | Tree loss: 2.081 | Accuracy: 0.315000 | 3.934 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 2.098 | Reg loss: 0.032 | Tree loss: 2.098 | Accuracy: 0.310580 | 3.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 2.198 | Reg loss: 0.032 | Tree loss: 2.198 | Accuracy: 0.292500 | 3.933 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 2.188 | Reg loss: 0.032 | Tree loss: 2.188 | Accuracy: 0.275500 | 3.933 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 2.167 | Reg loss: 0.032 | Tree loss: 2.167 | Accuracy: 0.290000 | 3.933 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 2.130 | Reg loss: 0.032 | Tree loss: 2.130 | Accuracy: 0.303500 | 3.934 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 2.127 | Reg loss: 0.032 | Tree loss: 2.127 | Accuracy: 0.298000 | 3.934 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 2.109 | Reg loss: 0.032 | Tree loss: 2.109 | Accuracy: 0.310500 | 3.935 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 2.084 | Reg loss: 0.032 | Tree loss: 2.084 | Accuracy: 0.317500 | 3.936 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 2.074 | Reg loss: 0.032 | Tree loss: 2.074 | Accuracy: 0.327500 | 3.937 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 2.084 | Reg loss: 0.032 | Tree loss: 2.084 | Accuracy: 0.315500 | 3.937 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 2.049 | Reg loss: 0.032 | Tree loss: 2.049 | Accuracy: 0.311000 | 3.937 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 2.036 | Reg loss: 0.032 | Tree loss: 2.036 | Accuracy: 0.327645 | 3.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 2.176 | Reg loss: 0.032 | Tree loss: 2.176 | Accuracy: 0.301000 | 3.939 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 2.181 | Reg loss: 0.032 | Tree loss: 2.181 | Accuracy: 0.284500 | 3.939 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 2.156 | Reg loss: 0.032 | Tree loss: 2.156 | Accuracy: 0.292500 | 3.938 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 2.141 | Reg loss: 0.032 | Tree loss: 2.141 | Accuracy: 0.303000 | 3.937 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 2.117 | Reg loss: 0.032 | Tree loss: 2.117 | Accuracy: 0.298000 | 3.937 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 2.093 | Reg loss: 0.032 | Tree loss: 2.093 | Accuracy: 0.314000 | 3.937 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 2.084 | Reg loss: 0.032 | Tree loss: 2.084 | Accuracy: 0.307500 | 3.937 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 2.085 | Reg loss: 0.032 | Tree loss: 2.085 | Accuracy: 0.316500 | 3.937 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 2.060 | Reg loss: 0.032 | Tree loss: 2.060 | Accuracy: 0.320500 | 3.935 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 2.050 | Reg loss: 0.032 | Tree loss: 2.050 | Accuracy: 0.335000 | 3.933 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.996 | Reg loss: 0.032 | Tree loss: 1.996 | Accuracy: 0.358362 | 3.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 2.190 | Reg loss: 0.032 | Tree loss: 2.190 | Accuracy: 0.286000 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 2.172 | Reg loss: 0.032 | Tree loss: 2.172 | Accuracy: 0.290000 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 2.139 | Reg loss: 0.032 | Tree loss: 2.139 | Accuracy: 0.301500 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 2.126 | Reg loss: 0.032 | Tree loss: 2.126 | Accuracy: 0.296000 | 3.936 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 2.120 | Reg loss: 0.032 | Tree loss: 2.120 | Accuracy: 0.296000 | 3.936 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 2.097 | Reg loss: 0.032 | Tree loss: 2.097 | Accuracy: 0.313000 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 2.062 | Reg loss: 0.032 | Tree loss: 2.062 | Accuracy: 0.319500 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 2.086 | Reg loss: 0.032 | Tree loss: 2.086 | Accuracy: 0.318000 | 3.935 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 2.036 | Reg loss: 0.032 | Tree loss: 2.036 | Accuracy: 0.334500 | 3.934 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 2.051 | Reg loss: 0.032 | Tree loss: 2.051 | Accuracy: 0.312000 | 3.934 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 2.030 | Reg loss: 0.032 | Tree loss: 2.030 | Accuracy: 0.395904 | 3.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 2.190 | Reg loss: 0.032 | Tree loss: 2.190 | Accuracy: 0.290000 | 3.936 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 2.189 | Reg loss: 0.032 | Tree loss: 2.189 | Accuracy: 0.274500 | 3.935 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 2.132 | Reg loss: 0.032 | Tree loss: 2.132 | Accuracy: 0.305500 | 3.935 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 2.110 | Reg loss: 0.032 | Tree loss: 2.110 | Accuracy: 0.299500 | 3.935 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 2.108 | Reg loss: 0.032 | Tree loss: 2.108 | Accuracy: 0.286000 | 3.934 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 2.084 | Reg loss: 0.032 | Tree loss: 2.084 | Accuracy: 0.302000 | 3.935 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 2.065 | Reg loss: 0.032 | Tree loss: 2.065 | Accuracy: 0.317000 | 3.936 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 2.071 | Reg loss: 0.032 | Tree loss: 2.071 | Accuracy: 0.315000 | 3.937 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 2.026 | Reg loss: 0.032 | Tree loss: 2.026 | Accuracy: 0.321000 | 3.938 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 2.049 | Reg loss: 0.032 | Tree loss: 2.049 | Accuracy: 0.318000 | 3.938 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 2.043 | Reg loss: 0.033 | Tree loss: 2.043 | Accuracy: 0.348123 | 3.937 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 2.205 | Reg loss: 0.032 | Tree loss: 2.205 | Accuracy: 0.281000 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 2.156 | Reg loss: 0.032 | Tree loss: 2.156 | Accuracy: 0.305500 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 2.158 | Reg loss: 0.032 | Tree loss: 2.158 | Accuracy: 0.284000 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 2.118 | Reg loss: 0.032 | Tree loss: 2.118 | Accuracy: 0.288000 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 2.114 | Reg loss: 0.032 | Tree loss: 2.114 | Accuracy: 0.288000 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 2.055 | Reg loss: 0.032 | Tree loss: 2.055 | Accuracy: 0.314500 | 3.938 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 2.068 | Reg loss: 0.033 | Tree loss: 2.068 | Accuracy: 0.294000 | 3.937 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 2.015 | Reg loss: 0.033 | Tree loss: 2.015 | Accuracy: 0.322500 | 3.937 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 2.059 | Reg loss: 0.033 | Tree loss: 2.059 | Accuracy: 0.324000 | 3.937 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 2.036 | Reg loss: 0.033 | Tree loss: 2.036 | Accuracy: 0.322000 | 3.937 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.999 | Reg loss: 0.033 | Tree loss: 1.999 | Accuracy: 0.327645 | 3.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 2.186 | Reg loss: 0.032 | Tree loss: 2.186 | Accuracy: 0.276500 | 3.938 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 2.148 | Reg loss: 0.033 | Tree loss: 2.148 | Accuracy: 0.292500 | 3.937 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 2.121 | Reg loss: 0.033 | Tree loss: 2.121 | Accuracy: 0.296500 | 3.937 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 2.111 | Reg loss: 0.033 | Tree loss: 2.111 | Accuracy: 0.310500 | 3.936 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 2.096 | Reg loss: 0.033 | Tree loss: 2.096 | Accuracy: 0.300500 | 3.935 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 2.079 | Reg loss: 0.033 | Tree loss: 2.079 | Accuracy: 0.311000 | 3.936 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 2.064 | Reg loss: 0.033 | Tree loss: 2.064 | Accuracy: 0.329500 | 3.936 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 2.049 | Reg loss: 0.033 | Tree loss: 2.049 | Accuracy: 0.328500 | 3.937 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 2.013 | Reg loss: 0.033 | Tree loss: 2.013 | Accuracy: 0.330500 | 3.936 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 2.026 | Reg loss: 0.033 | Tree loss: 2.026 | Accuracy: 0.350500 | 3.936 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 2.049 | Reg loss: 0.033 | Tree loss: 2.049 | Accuracy: 0.317406 | 3.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 2.179 | Reg loss: 0.033 | Tree loss: 2.179 | Accuracy: 0.279000 | 3.936 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 2.158 | Reg loss: 0.033 | Tree loss: 2.158 | Accuracy: 0.285500 | 3.935 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 2.108 | Reg loss: 0.033 | Tree loss: 2.108 | Accuracy: 0.309500 | 3.932 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 2.084 | Reg loss: 0.033 | Tree loss: 2.084 | Accuracy: 0.317000 | 3.929 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 2.085 | Reg loss: 0.033 | Tree loss: 2.085 | Accuracy: 0.295000 | 3.927 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 2.088 | Reg loss: 0.033 | Tree loss: 2.088 | Accuracy: 0.305500 | 3.926 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 2.054 | Reg loss: 0.033 | Tree loss: 2.054 | Accuracy: 0.308500 | 3.926 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 2.018 | Reg loss: 0.033 | Tree loss: 2.018 | Accuracy: 0.344000 | 3.926 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 2.052 | Reg loss: 0.033 | Tree loss: 2.052 | Accuracy: 0.333500 | 3.926 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 2.032 | Reg loss: 0.033 | Tree loss: 2.032 | Accuracy: 0.351500 | 3.925 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.992 | Reg loss: 0.033 | Tree loss: 1.992 | Accuracy: 0.365188 | 3.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 2.152 | Reg loss: 0.033 | Tree loss: 2.152 | Accuracy: 0.291500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 2.133 | Reg loss: 0.033 | Tree loss: 2.133 | Accuracy: 0.300500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 2.145 | Reg loss: 0.033 | Tree loss: 2.145 | Accuracy: 0.289000 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 2.101 | Reg loss: 0.033 | Tree loss: 2.101 | Accuracy: 0.303500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 2.076 | Reg loss: 0.033 | Tree loss: 2.076 | Accuracy: 0.297000 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 2.077 | Reg loss: 0.033 | Tree loss: 2.077 | Accuracy: 0.301500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 2.033 | Reg loss: 0.033 | Tree loss: 2.033 | Accuracy: 0.323500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 2.052 | Reg loss: 0.033 | Tree loss: 2.052 | Accuracy: 0.316500 | 3.926 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 2.013 | Reg loss: 0.033 | Tree loss: 2.013 | Accuracy: 0.343000 | 3.927 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 2.017 | Reg loss: 0.033 | Tree loss: 2.017 | Accuracy: 0.357500 | 3.927 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 2.055 | Reg loss: 0.033 | Tree loss: 2.055 | Accuracy: 0.351536 | 3.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 2.154 | Reg loss: 0.033 | Tree loss: 2.154 | Accuracy: 0.292000 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 2.135 | Reg loss: 0.033 | Tree loss: 2.135 | Accuracy: 0.310000 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 2.121 | Reg loss: 0.033 | Tree loss: 2.121 | Accuracy: 0.294000 | 3.924 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 2.078 | Reg loss: 0.033 | Tree loss: 2.078 | Accuracy: 0.303000 | 3.924 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 2.074 | Reg loss: 0.033 | Tree loss: 2.074 | Accuracy: 0.300000 | 3.925 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 2.056 | Reg loss: 0.033 | Tree loss: 2.056 | Accuracy: 0.316000 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 2.051 | Reg loss: 0.033 | Tree loss: 2.051 | Accuracy: 0.310500 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 2.027 | Reg loss: 0.033 | Tree loss: 2.027 | Accuracy: 0.321000 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 2.041 | Reg loss: 0.033 | Tree loss: 2.041 | Accuracy: 0.329000 | 3.926 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 2.007 | Reg loss: 0.033 | Tree loss: 2.007 | Accuracy: 0.345500 | 3.924 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 2.026 | Reg loss: 0.033 | Tree loss: 2.026 | Accuracy: 0.361775 | 3.923 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 2.162 | Reg loss: 0.033 | Tree loss: 2.162 | Accuracy: 0.278000 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 2.150 | Reg loss: 0.033 | Tree loss: 2.150 | Accuracy: 0.286500 | 3.922 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 2.109 | Reg loss: 0.033 | Tree loss: 2.109 | Accuracy: 0.299500 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 2.090 | Reg loss: 0.033 | Tree loss: 2.090 | Accuracy: 0.287000 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 2.064 | Reg loss: 0.033 | Tree loss: 2.064 | Accuracy: 0.307000 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 2.048 | Reg loss: 0.033 | Tree loss: 2.048 | Accuracy: 0.337000 | 3.922 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 2.039 | Reg loss: 0.033 | Tree loss: 2.039 | Accuracy: 0.327500 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 2.030 | Reg loss: 0.033 | Tree loss: 2.030 | Accuracy: 0.323500 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 2.004 | Reg loss: 0.033 | Tree loss: 2.004 | Accuracy: 0.343500 | 3.924 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.998 | Reg loss: 0.034 | Tree loss: 1.998 | Accuracy: 0.361000 | 3.923 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 1.988 | Reg loss: 0.034 | Tree loss: 1.988 | Accuracy: 0.382253 | 3.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 2.145 | Reg loss: 0.033 | Tree loss: 2.145 | Accuracy: 0.292000 | 3.922 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 2.124 | Reg loss: 0.033 | Tree loss: 2.124 | Accuracy: 0.296000 | 3.921 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 2.114 | Reg loss: 0.033 | Tree loss: 2.114 | Accuracy: 0.297000 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 2.084 | Reg loss: 0.033 | Tree loss: 2.084 | Accuracy: 0.301500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 2.075 | Reg loss: 0.033 | Tree loss: 2.075 | Accuracy: 0.314500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 2.036 | Reg loss: 0.034 | Tree loss: 2.036 | Accuracy: 0.310500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 2.032 | Reg loss: 0.034 | Tree loss: 2.032 | Accuracy: 0.319500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 2.032 | Reg loss: 0.034 | Tree loss: 2.032 | Accuracy: 0.331500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 2.010 | Reg loss: 0.034 | Tree loss: 2.010 | Accuracy: 0.342500 | 3.92 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.989 | Reg loss: 0.034 | Tree loss: 1.989 | Accuracy: 0.349000 | 3.919 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 2.046 | Reg loss: 0.034 | Tree loss: 2.046 | Accuracy: 0.344710 | 3.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 2.159 | Reg loss: 0.034 | Tree loss: 2.159 | Accuracy: 0.289000 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 2.108 | Reg loss: 0.034 | Tree loss: 2.108 | Accuracy: 0.293500 | 3.92 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 2.084 | Reg loss: 0.034 | Tree loss: 2.084 | Accuracy: 0.303000 | 3.92 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 2.086 | Reg loss: 0.034 | Tree loss: 2.086 | Accuracy: 0.296500 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 2.048 | Reg loss: 0.034 | Tree loss: 2.048 | Accuracy: 0.325500 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 2.030 | Reg loss: 0.034 | Tree loss: 2.030 | Accuracy: 0.334000 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 2.041 | Reg loss: 0.034 | Tree loss: 2.041 | Accuracy: 0.298500 | 3.92 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 2.006 | Reg loss: 0.034 | Tree loss: 2.006 | Accuracy: 0.323500 | 3.92 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 2.032 | Reg loss: 0.034 | Tree loss: 2.032 | Accuracy: 0.344500 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 2.007 | Reg loss: 0.034 | Tree loss: 2.007 | Accuracy: 0.330000 | 3.921 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.991 | Reg loss: 0.034 | Tree loss: 1.991 | Accuracy: 0.348123 | 3.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 2.151 | Reg loss: 0.034 | Tree loss: 2.151 | Accuracy: 0.284000 | 3.921 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 2.095 | Reg loss: 0.034 | Tree loss: 2.095 | Accuracy: 0.291500 | 3.921 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 2.092 | Reg loss: 0.034 | Tree loss: 2.092 | Accuracy: 0.306000 | 3.921 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 2.062 | Reg loss: 0.034 | Tree loss: 2.062 | Accuracy: 0.318000 | 3.921 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 2.052 | Reg loss: 0.034 | Tree loss: 2.052 | Accuracy: 0.307000 | 3.92 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 2.038 | Reg loss: 0.034 | Tree loss: 2.038 | Accuracy: 0.299500 | 3.919 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 2.026 | Reg loss: 0.034 | Tree loss: 2.026 | Accuracy: 0.311000 | 3.92 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 2.008 | Reg loss: 0.034 | Tree loss: 2.008 | Accuracy: 0.339000 | 3.92 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 2.017 | Reg loss: 0.034 | Tree loss: 2.017 | Accuracy: 0.334000 | 3.92 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 2.009 | Reg loss: 0.034 | Tree loss: 2.009 | Accuracy: 0.342000 | 3.92 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 2.032 | Reg loss: 0.034 | Tree loss: 2.032 | Accuracy: 0.344710 | 3.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 2.136 | Reg loss: 0.034 | Tree loss: 2.136 | Accuracy: 0.290000 | 3.92 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 2.088 | Reg loss: 0.034 | Tree loss: 2.088 | Accuracy: 0.289000 | 3.92 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 2.100 | Reg loss: 0.034 | Tree loss: 2.100 | Accuracy: 0.288500 | 3.919 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 2.077 | Reg loss: 0.034 | Tree loss: 2.077 | Accuracy: 0.285000 | 3.919 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 2.068 | Reg loss: 0.034 | Tree loss: 2.068 | Accuracy: 0.299000 | 3.92 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 2.049 | Reg loss: 0.034 | Tree loss: 2.049 | Accuracy: 0.308000 | 3.92 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 2.029 | Reg loss: 0.034 | Tree loss: 2.029 | Accuracy: 0.331500 | 3.92 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.985 | Reg loss: 0.034 | Tree loss: 1.985 | Accuracy: 0.338000 | 3.919 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.998 | Reg loss: 0.034 | Tree loss: 1.998 | Accuracy: 0.361500 | 3.919 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.982 | Reg loss: 0.034 | Tree loss: 1.982 | Accuracy: 0.359000 | 3.919 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 1.991 | Reg loss: 0.034 | Tree loss: 1.991 | Accuracy: 0.324232 | 3.92 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 2.152 | Reg loss: 0.034 | Tree loss: 2.152 | Accuracy: 0.290000 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 2.109 | Reg loss: 0.034 | Tree loss: 2.109 | Accuracy: 0.283500 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 2.087 | Reg loss: 0.034 | Tree loss: 2.087 | Accuracy: 0.299000 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 2.076 | Reg loss: 0.034 | Tree loss: 2.076 | Accuracy: 0.299500 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 2.051 | Reg loss: 0.034 | Tree loss: 2.051 | Accuracy: 0.307500 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 2.012 | Reg loss: 0.034 | Tree loss: 2.012 | Accuracy: 0.326500 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 2.018 | Reg loss: 0.034 | Tree loss: 2.018 | Accuracy: 0.326000 | 3.921 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 2.001 | Reg loss: 0.034 | Tree loss: 2.001 | Accuracy: 0.331500 | 3.92 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.975 | Reg loss: 0.034 | Tree loss: 1.975 | Accuracy: 0.345000 | 3.92 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 2.006 | Reg loss: 0.034 | Tree loss: 2.006 | Accuracy: 0.337500 | 3.92 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.881 | Reg loss: 0.034 | Tree loss: 1.881 | Accuracy: 0.412969 | 3.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 2.132 | Reg loss: 0.034 | Tree loss: 2.132 | Accuracy: 0.293000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 2.111 | Reg loss: 0.034 | Tree loss: 2.111 | Accuracy: 0.290500 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 2.078 | Reg loss: 0.034 | Tree loss: 2.078 | Accuracy: 0.296000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 2.069 | Reg loss: 0.034 | Tree loss: 2.069 | Accuracy: 0.295000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 2.039 | Reg loss: 0.034 | Tree loss: 2.039 | Accuracy: 0.298000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.998 | Reg loss: 0.034 | Tree loss: 1.998 | Accuracy: 0.342000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 2.025 | Reg loss: 0.034 | Tree loss: 2.025 | Accuracy: 0.320500 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.998 | Reg loss: 0.034 | Tree loss: 1.998 | Accuracy: 0.332000 | 3.921 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 2.001 | Reg loss: 0.034 | Tree loss: 2.001 | Accuracy: 0.338500 | 3.92 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.969 | Reg loss: 0.034 | Tree loss: 1.969 | Accuracy: 0.360000 | 3.919 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 2.011 | Reg loss: 0.035 | Tree loss: 2.011 | Accuracy: 0.310580 | 3.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 2.132 | Reg loss: 0.034 | Tree loss: 2.132 | Accuracy: 0.296500 | 3.918 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 2.117 | Reg loss: 0.034 | Tree loss: 2.117 | Accuracy: 0.284000 | 3.915 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 2.078 | Reg loss: 0.034 | Tree loss: 2.078 | Accuracy: 0.286500 | 3.913 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 2.045 | Reg loss: 0.034 | Tree loss: 2.045 | Accuracy: 0.314000 | 3.911 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 2.040 | Reg loss: 0.034 | Tree loss: 2.040 | Accuracy: 0.307500 | 3.909 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 2.033 | Reg loss: 0.034 | Tree loss: 2.033 | Accuracy: 0.294500 | 3.907 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 2.011 | Reg loss: 0.035 | Tree loss: 2.011 | Accuracy: 0.320500 | 3.904 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 1.990 | Reg loss: 0.035 | Tree loss: 1.990 | Accuracy: 0.339500 | 3.902 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.976 | Reg loss: 0.035 | Tree loss: 1.976 | Accuracy: 0.360500 | 3.9 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 1.966 | Reg loss: 0.035 | Tree loss: 1.966 | Accuracy: 0.375500 | 3.898 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.953 | Reg loss: 0.035 | Tree loss: 1.953 | Accuracy: 0.385666 | 3.896 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 2.153 | Reg loss: 0.034 | Tree loss: 2.153 | Accuracy: 0.274500 | 3.895 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 2.085 | Reg loss: 0.034 | Tree loss: 2.085 | Accuracy: 0.304000 | 3.892 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 2.059 | Reg loss: 0.035 | Tree loss: 2.059 | Accuracy: 0.294000 | 3.89 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 2.056 | Reg loss: 0.035 | Tree loss: 2.056 | Accuracy: 0.300000 | 3.887 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.994 | Reg loss: 0.035 | Tree loss: 1.994 | Accuracy: 0.334500 | 3.886 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 2.014 | Reg loss: 0.035 | Tree loss: 2.014 | Accuracy: 0.330500 | 3.884 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 2.008 | Reg loss: 0.035 | Tree loss: 2.008 | Accuracy: 0.322000 | 3.882 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 1.993 | Reg loss: 0.035 | Tree loss: 1.993 | Accuracy: 0.355500 | 3.88 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 1.996 | Reg loss: 0.035 | Tree loss: 1.996 | Accuracy: 0.356000 | 3.879 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 1.989 | Reg loss: 0.035 | Tree loss: 1.989 | Accuracy: 0.369000 | 3.877 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 1.983 | Reg loss: 0.035 | Tree loss: 1.983 | Accuracy: 0.361775 | 3.876 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 2.130 | Reg loss: 0.035 | Tree loss: 2.130 | Accuracy: 0.280000 | 3.875 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 2.079 | Reg loss: 0.035 | Tree loss: 2.079 | Accuracy: 0.292000 | 3.873 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 2.088 | Reg loss: 0.035 | Tree loss: 2.088 | Accuracy: 0.296500 | 3.871 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 2.040 | Reg loss: 0.035 | Tree loss: 2.040 | Accuracy: 0.302000 | 3.87 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 2.004 | Reg loss: 0.035 | Tree loss: 2.004 | Accuracy: 0.331500 | 3.869 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 2.018 | Reg loss: 0.035 | Tree loss: 2.018 | Accuracy: 0.320000 | 3.867 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 1.995 | Reg loss: 0.035 | Tree loss: 1.995 | Accuracy: 0.346000 | 3.866 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 1.980 | Reg loss: 0.035 | Tree loss: 1.980 | Accuracy: 0.335000 | 3.864 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.998 | Reg loss: 0.035 | Tree loss: 1.998 | Accuracy: 0.327000 | 3.862 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.968 | Reg loss: 0.035 | Tree loss: 1.968 | Accuracy: 0.358500 | 3.86 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.941 | Reg loss: 0.035 | Tree loss: 1.941 | Accuracy: 0.389078 | 3.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 2.099 | Reg loss: 0.035 | Tree loss: 2.099 | Accuracy: 0.289000 | 3.856 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 2.101 | Reg loss: 0.035 | Tree loss: 2.101 | Accuracy: 0.303000 | 3.855 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 2.034 | Reg loss: 0.035 | Tree loss: 2.034 | Accuracy: 0.310500 | 3.854 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 2.048 | Reg loss: 0.035 | Tree loss: 2.048 | Accuracy: 0.311500 | 3.852 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 2.012 | Reg loss: 0.035 | Tree loss: 2.012 | Accuracy: 0.326500 | 3.851 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 2.018 | Reg loss: 0.035 | Tree loss: 2.018 | Accuracy: 0.328000 | 3.849 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.996 | Reg loss: 0.035 | Tree loss: 1.996 | Accuracy: 0.344000 | 3.848 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.981 | Reg loss: 0.035 | Tree loss: 1.981 | Accuracy: 0.358000 | 3.846 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 1.988 | Reg loss: 0.035 | Tree loss: 1.988 | Accuracy: 0.360500 | 3.844 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 1.973 | Reg loss: 0.035 | Tree loss: 1.973 | Accuracy: 0.371500 | 3.843 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 1.972 | Reg loss: 0.035 | Tree loss: 1.972 | Accuracy: 0.375427 | 3.841 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 2.120 | Reg loss: 0.035 | Tree loss: 2.120 | Accuracy: 0.289000 | 3.839 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 2.086 | Reg loss: 0.035 | Tree loss: 2.086 | Accuracy: 0.293500 | 3.837 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 2.049 | Reg loss: 0.035 | Tree loss: 2.049 | Accuracy: 0.305500 | 3.835 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 2.023 | Reg loss: 0.035 | Tree loss: 2.023 | Accuracy: 0.311500 | 3.834 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 2.026 | Reg loss: 0.035 | Tree loss: 2.026 | Accuracy: 0.305500 | 3.832 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 1.994 | Reg loss: 0.035 | Tree loss: 1.994 | Accuracy: 0.333500 | 3.831 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 2.002 | Reg loss: 0.035 | Tree loss: 2.002 | Accuracy: 0.345500 | 3.83 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.992 | Reg loss: 0.035 | Tree loss: 1.992 | Accuracy: 0.346500 | 3.828 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 1.963 | Reg loss: 0.035 | Tree loss: 1.963 | Accuracy: 0.370500 | 3.825 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 1.957 | Reg loss: 0.035 | Tree loss: 1.957 | Accuracy: 0.363500 | 3.824 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 2.027 | Reg loss: 0.035 | Tree loss: 2.027 | Accuracy: 0.337884 | 3.822 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 2.084 | Reg loss: 0.035 | Tree loss: 2.084 | Accuracy: 0.296500 | 3.82 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 2.087 | Reg loss: 0.035 | Tree loss: 2.087 | Accuracy: 0.304000 | 3.819 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 2.053 | Reg loss: 0.035 | Tree loss: 2.053 | Accuracy: 0.301000 | 3.817 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 2.037 | Reg loss: 0.035 | Tree loss: 2.037 | Accuracy: 0.299000 | 3.816 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 2.009 | Reg loss: 0.035 | Tree loss: 2.009 | Accuracy: 0.325000 | 3.814 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.994 | Reg loss: 0.035 | Tree loss: 1.994 | Accuracy: 0.332500 | 3.813 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.987 | Reg loss: 0.035 | Tree loss: 1.987 | Accuracy: 0.329000 | 3.812 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 1.991 | Reg loss: 0.035 | Tree loss: 1.991 | Accuracy: 0.355500 | 3.81 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 1.965 | Reg loss: 0.035 | Tree loss: 1.965 | Accuracy: 0.362500 | 3.808 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 1.981 | Reg loss: 0.035 | Tree loss: 1.981 | Accuracy: 0.352500 | 3.807 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.965 | Reg loss: 0.035 | Tree loss: 1.965 | Accuracy: 0.354949 | 3.806 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 2.079 | Reg loss: 0.035 | Tree loss: 2.079 | Accuracy: 0.306500 | 3.805 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 2.084 | Reg loss: 0.035 | Tree loss: 2.084 | Accuracy: 0.298500 | 3.803 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 2.039 | Reg loss: 0.035 | Tree loss: 2.039 | Accuracy: 0.320500 | 3.802 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 2.017 | Reg loss: 0.035 | Tree loss: 2.017 | Accuracy: 0.317000 | 3.801 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 2.011 | Reg loss: 0.035 | Tree loss: 2.011 | Accuracy: 0.318000 | 3.799 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 2.019 | Reg loss: 0.035 | Tree loss: 2.019 | Accuracy: 0.320000 | 3.798 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 1.984 | Reg loss: 0.035 | Tree loss: 1.984 | Accuracy: 0.333500 | 3.796 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 1.973 | Reg loss: 0.035 | Tree loss: 1.973 | Accuracy: 0.379000 | 3.794 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 1.986 | Reg loss: 0.035 | Tree loss: 1.986 | Accuracy: 0.373000 | 3.792 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 1.959 | Reg loss: 0.036 | Tree loss: 1.959 | Accuracy: 0.378500 | 3.791 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.919 | Reg loss: 0.036 | Tree loss: 1.919 | Accuracy: 0.372014 | 3.789 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 2.095 | Reg loss: 0.035 | Tree loss: 2.095 | Accuracy: 0.289000 | 3.788 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 2.084 | Reg loss: 0.035 | Tree loss: 2.084 | Accuracy: 0.295000 | 3.786 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 2.039 | Reg loss: 0.035 | Tree loss: 2.039 | Accuracy: 0.323500 | 3.785 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 2.022 | Reg loss: 0.035 | Tree loss: 2.022 | Accuracy: 0.316500 | 3.783 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 2.013 | Reg loss: 0.035 | Tree loss: 2.013 | Accuracy: 0.315500 | 3.781 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 1.999 | Reg loss: 0.036 | Tree loss: 1.999 | Accuracy: 0.331500 | 3.779 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 1.998 | Reg loss: 0.036 | Tree loss: 1.998 | Accuracy: 0.337500 | 3.777 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 1.958 | Reg loss: 0.036 | Tree loss: 1.958 | Accuracy: 0.365500 | 3.775 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 1.957 | Reg loss: 0.036 | Tree loss: 1.957 | Accuracy: 0.385000 | 3.774 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 1.948 | Reg loss: 0.036 | Tree loss: 1.948 | Accuracy: 0.390500 | 3.772 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 1.931 | Reg loss: 0.036 | Tree loss: 1.931 | Accuracy: 0.402730 | 3.77 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 2.088 | Reg loss: 0.036 | Tree loss: 2.088 | Accuracy: 0.290000 | 3.77 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 2.058 | Reg loss: 0.036 | Tree loss: 2.058 | Accuracy: 0.308000 | 3.768 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 2.040 | Reg loss: 0.036 | Tree loss: 2.040 | Accuracy: 0.303500 | 3.766 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 2.012 | Reg loss: 0.036 | Tree loss: 2.012 | Accuracy: 0.326000 | 3.764 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 2.011 | Reg loss: 0.036 | Tree loss: 2.011 | Accuracy: 0.330000 | 3.762 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 1.988 | Reg loss: 0.036 | Tree loss: 1.988 | Accuracy: 0.343000 | 3.76 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 1.998 | Reg loss: 0.036 | Tree loss: 1.998 | Accuracy: 0.355500 | 3.759 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 1.944 | Reg loss: 0.036 | Tree loss: 1.944 | Accuracy: 0.372000 | 3.757 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 1.969 | Reg loss: 0.036 | Tree loss: 1.969 | Accuracy: 0.390000 | 3.756 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 1.960 | Reg loss: 0.036 | Tree loss: 1.960 | Accuracy: 0.393000 | 3.754 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.395904 | 3.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 2.081 | Reg loss: 0.036 | Tree loss: 2.081 | Accuracy: 0.302000 | 3.752 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 2.087 | Reg loss: 0.036 | Tree loss: 2.087 | Accuracy: 0.305500 | 3.75 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 2.039 | Reg loss: 0.036 | Tree loss: 2.039 | Accuracy: 0.305500 | 3.748 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 2.031 | Reg loss: 0.036 | Tree loss: 2.031 | Accuracy: 0.309500 | 3.746 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 2.021 | Reg loss: 0.036 | Tree loss: 2.021 | Accuracy: 0.319500 | 3.743 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 1.976 | Reg loss: 0.036 | Tree loss: 1.976 | Accuracy: 0.356500 | 3.741 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 1.952 | Reg loss: 0.036 | Tree loss: 1.952 | Accuracy: 0.357500 | 3.739 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 1.975 | Reg loss: 0.036 | Tree loss: 1.975 | Accuracy: 0.349500 | 3.736 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 1.948 | Reg loss: 0.036 | Tree loss: 1.948 | Accuracy: 0.384000 | 3.734 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 1.930 | Reg loss: 0.036 | Tree loss: 1.930 | Accuracy: 0.393000 | 3.732 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 1.959 | Reg loss: 0.036 | Tree loss: 1.959 | Accuracy: 0.423208 | 3.729 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 2.092 | Reg loss: 0.036 | Tree loss: 2.092 | Accuracy: 0.294500 | 3.729 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 2.057 | Reg loss: 0.036 | Tree loss: 2.057 | Accuracy: 0.302500 | 3.727 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 2.024 | Reg loss: 0.036 | Tree loss: 2.024 | Accuracy: 0.321000 | 3.724 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 2.036 | Reg loss: 0.036 | Tree loss: 2.036 | Accuracy: 0.303500 | 3.722 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 2.010 | Reg loss: 0.036 | Tree loss: 2.010 | Accuracy: 0.345500 | 3.719 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 1.981 | Reg loss: 0.036 | Tree loss: 1.981 | Accuracy: 0.353500 | 3.717 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 1.969 | Reg loss: 0.036 | Tree loss: 1.969 | Accuracy: 0.346000 | 3.715 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 1.958 | Reg loss: 0.036 | Tree loss: 1.958 | Accuracy: 0.367000 | 3.713 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 1.931 | Reg loss: 0.036 | Tree loss: 1.931 | Accuracy: 0.380000 | 3.711 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 1.947 | Reg loss: 0.036 | Tree loss: 1.947 | Accuracy: 0.398000 | 3.708 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.883 | Reg loss: 0.036 | Tree loss: 1.883 | Accuracy: 0.419795 | 3.706 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 2.096 | Reg loss: 0.036 | Tree loss: 2.096 | Accuracy: 0.299000 | 3.705 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 2.054 | Reg loss: 0.036 | Tree loss: 2.054 | Accuracy: 0.296000 | 3.702 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 2.017 | Reg loss: 0.036 | Tree loss: 2.017 | Accuracy: 0.334000 | 3.7 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 2.022 | Reg loss: 0.036 | Tree loss: 2.022 | Accuracy: 0.320000 | 3.698 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 1.998 | Reg loss: 0.036 | Tree loss: 1.998 | Accuracy: 0.326500 | 3.695 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 1.959 | Reg loss: 0.036 | Tree loss: 1.959 | Accuracy: 0.374500 | 3.693 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 1.976 | Reg loss: 0.036 | Tree loss: 1.976 | Accuracy: 0.355500 | 3.691 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 1.953 | Reg loss: 0.036 | Tree loss: 1.953 | Accuracy: 0.374000 | 3.689 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 1.937 | Reg loss: 0.036 | Tree loss: 1.937 | Accuracy: 0.385500 | 3.686 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 1.952 | Reg loss: 0.036 | Tree loss: 1.952 | Accuracy: 0.393500 | 3.684 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 1.920 | Reg loss: 0.036 | Tree loss: 1.920 | Accuracy: 0.382253 | 3.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 2.077 | Reg loss: 0.036 | Tree loss: 2.077 | Accuracy: 0.294000 | 3.68 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 2.055 | Reg loss: 0.036 | Tree loss: 2.055 | Accuracy: 0.302500 | 3.678 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 2.021 | Reg loss: 0.036 | Tree loss: 2.021 | Accuracy: 0.324000 | 3.676 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 1.997 | Reg loss: 0.036 | Tree loss: 1.997 | Accuracy: 0.333500 | 3.674 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 1.986 | Reg loss: 0.036 | Tree loss: 1.986 | Accuracy: 0.331500 | 3.671 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 1.980 | Reg loss: 0.036 | Tree loss: 1.980 | Accuracy: 0.361500 | 3.669 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 1.934 | Reg loss: 0.036 | Tree loss: 1.934 | Accuracy: 0.383500 | 3.667 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 1.976 | Reg loss: 0.036 | Tree loss: 1.976 | Accuracy: 0.378500 | 3.665 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 1.935 | Reg loss: 0.036 | Tree loss: 1.935 | Accuracy: 0.386000 | 3.663 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.380000 | 3.66 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 1.916 | Reg loss: 0.036 | Tree loss: 1.916 | Accuracy: 0.365188 | 3.658 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 2.067 | Reg loss: 0.036 | Tree loss: 2.067 | Accuracy: 0.292500 | 3.656 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 2.060 | Reg loss: 0.036 | Tree loss: 2.060 | Accuracy: 0.301000 | 3.654 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 2.054 | Reg loss: 0.036 | Tree loss: 2.054 | Accuracy: 0.273500 | 3.651 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.999 | Reg loss: 0.036 | Tree loss: 1.999 | Accuracy: 0.334000 | 3.649 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 1.985 | Reg loss: 0.036 | Tree loss: 1.985 | Accuracy: 0.365000 | 3.647 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 1.970 | Reg loss: 0.036 | Tree loss: 1.970 | Accuracy: 0.351000 | 3.644 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 1.942 | Reg loss: 0.036 | Tree loss: 1.942 | Accuracy: 0.371000 | 3.643 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 1.969 | Reg loss: 0.036 | Tree loss: 1.969 | Accuracy: 0.370500 | 3.641 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 1.936 | Reg loss: 0.036 | Tree loss: 1.936 | Accuracy: 0.396500 | 3.639 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 1.914 | Reg loss: 0.036 | Tree loss: 1.914 | Accuracy: 0.398500 | 3.636 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 1.955 | Reg loss: 0.036 | Tree loss: 1.955 | Accuracy: 0.365188 | 3.634 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 2.053 | Reg loss: 0.036 | Tree loss: 2.053 | Accuracy: 0.311000 | 3.632 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 2.035 | Reg loss: 0.036 | Tree loss: 2.035 | Accuracy: 0.319500 | 3.63 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 2.045 | Reg loss: 0.036 | Tree loss: 2.045 | Accuracy: 0.297000 | 3.628 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 1.973 | Reg loss: 0.036 | Tree loss: 1.973 | Accuracy: 0.342000 | 3.626 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 1.987 | Reg loss: 0.036 | Tree loss: 1.987 | Accuracy: 0.337500 | 3.624 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 1.981 | Reg loss: 0.036 | Tree loss: 1.981 | Accuracy: 0.353000 | 3.622 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 1.957 | Reg loss: 0.036 | Tree loss: 1.957 | Accuracy: 0.346500 | 3.62 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 1.950 | Reg loss: 0.037 | Tree loss: 1.950 | Accuracy: 0.389000 | 3.618 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 1.940 | Reg loss: 0.037 | Tree loss: 1.940 | Accuracy: 0.401500 | 3.615 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 1.952 | Reg loss: 0.037 | Tree loss: 1.952 | Accuracy: 0.396000 | 3.613 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.399317 | 3.611 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 2.058 | Reg loss: 0.036 | Tree loss: 2.058 | Accuracy: 0.315500 | 3.61 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 2.044 | Reg loss: 0.036 | Tree loss: 2.044 | Accuracy: 0.310500 | 3.608 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 2.021 | Reg loss: 0.036 | Tree loss: 2.021 | Accuracy: 0.325500 | 3.605 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 2.030 | Reg loss: 0.037 | Tree loss: 2.030 | Accuracy: 0.305500 | 3.603 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 1.987 | Reg loss: 0.037 | Tree loss: 1.987 | Accuracy: 0.319500 | 3.601 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 1.963 | Reg loss: 0.037 | Tree loss: 1.963 | Accuracy: 0.361000 | 3.598 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 1.931 | Reg loss: 0.037 | Tree loss: 1.931 | Accuracy: 0.367000 | 3.596 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 1.930 | Reg loss: 0.037 | Tree loss: 1.930 | Accuracy: 0.386000 | 3.594 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.397000 | 3.592 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 1.943 | Reg loss: 0.037 | Tree loss: 1.943 | Accuracy: 0.378000 | 3.59 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 1.926 | Reg loss: 0.037 | Tree loss: 1.926 | Accuracy: 0.419795 | 3.587 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 2.088 | Reg loss: 0.037 | Tree loss: 2.088 | Accuracy: 0.287000 | 3.585 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 2.028 | Reg loss: 0.037 | Tree loss: 2.028 | Accuracy: 0.313000 | 3.583 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 2.028 | Reg loss: 0.037 | Tree loss: 2.028 | Accuracy: 0.319500 | 3.58 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 1.981 | Reg loss: 0.037 | Tree loss: 1.981 | Accuracy: 0.316000 | 3.578 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 1.964 | Reg loss: 0.037 | Tree loss: 1.964 | Accuracy: 0.343000 | 3.575 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 1.959 | Reg loss: 0.037 | Tree loss: 1.959 | Accuracy: 0.358000 | 3.573 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 1.936 | Reg loss: 0.037 | Tree loss: 1.936 | Accuracy: 0.378500 | 3.571 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.381500 | 3.569 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 1.953 | Reg loss: 0.037 | Tree loss: 1.953 | Accuracy: 0.392500 | 3.566 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 1.920 | Reg loss: 0.037 | Tree loss: 1.920 | Accuracy: 0.413000 | 3.564 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 1.971 | Reg loss: 0.037 | Tree loss: 1.971 | Accuracy: 0.412969 | 3.561 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 2.042 | Reg loss: 0.037 | Tree loss: 2.042 | Accuracy: 0.320500 | 3.561 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 2.050 | Reg loss: 0.037 | Tree loss: 2.050 | Accuracy: 0.300500 | 3.558 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 2.027 | Reg loss: 0.037 | Tree loss: 2.027 | Accuracy: 0.320500 | 3.556 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 1.999 | Reg loss: 0.037 | Tree loss: 1.999 | Accuracy: 0.320000 | 3.554 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 1.965 | Reg loss: 0.037 | Tree loss: 1.965 | Accuracy: 0.337500 | 3.551 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 1.988 | Reg loss: 0.037 | Tree loss: 1.988 | Accuracy: 0.355000 | 3.549 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 1.927 | Reg loss: 0.037 | Tree loss: 1.927 | Accuracy: 0.382000 | 3.547 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 1.908 | Reg loss: 0.037 | Tree loss: 1.908 | Accuracy: 0.385000 | 3.544 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 1.926 | Reg loss: 0.037 | Tree loss: 1.926 | Accuracy: 0.387500 | 3.542 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 1.937 | Reg loss: 0.037 | Tree loss: 1.937 | Accuracy: 0.401500 | 3.54 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 1.910 | Reg loss: 0.037 | Tree loss: 1.910 | Accuracy: 0.409556 | 3.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 2.067 | Reg loss: 0.037 | Tree loss: 2.067 | Accuracy: 0.288500 | 3.535 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 2.039 | Reg loss: 0.037 | Tree loss: 2.039 | Accuracy: 0.307500 | 3.533 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 2.009 | Reg loss: 0.037 | Tree loss: 2.009 | Accuracy: 0.329000 | 3.53 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 1.991 | Reg loss: 0.037 | Tree loss: 1.991 | Accuracy: 0.324000 | 3.528 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 1.979 | Reg loss: 0.037 | Tree loss: 1.979 | Accuracy: 0.346000 | 3.526 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 1.954 | Reg loss: 0.037 | Tree loss: 1.954 | Accuracy: 0.346500 | 3.523 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.390000 | 3.521 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 1.926 | Reg loss: 0.037 | Tree loss: 1.926 | Accuracy: 0.387500 | 3.518 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 1.938 | Reg loss: 0.037 | Tree loss: 1.938 | Accuracy: 0.385000 | 3.516 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 1.904 | Reg loss: 0.037 | Tree loss: 1.904 | Accuracy: 0.427500 | 3.514 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 1.877 | Reg loss: 0.037 | Tree loss: 1.877 | Accuracy: 0.453925 | 3.511 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 2.058 | Reg loss: 0.037 | Tree loss: 2.058 | Accuracy: 0.308500 | 3.51 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 2.024 | Reg loss: 0.037 | Tree loss: 2.024 | Accuracy: 0.318500 | 3.508 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.986 | Reg loss: 0.037 | Tree loss: 1.986 | Accuracy: 0.322000 | 3.505 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 1.971 | Reg loss: 0.037 | Tree loss: 1.971 | Accuracy: 0.338000 | 3.503 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 1.966 | Reg loss: 0.037 | Tree loss: 1.966 | Accuracy: 0.341000 | 3.501 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 1.949 | Reg loss: 0.037 | Tree loss: 1.949 | Accuracy: 0.371500 | 3.498 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 1.948 | Reg loss: 0.037 | Tree loss: 1.948 | Accuracy: 0.371000 | 3.496 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 1.950 | Reg loss: 0.037 | Tree loss: 1.950 | Accuracy: 0.380500 | 3.494 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 1.928 | Reg loss: 0.037 | Tree loss: 1.928 | Accuracy: 0.404000 | 3.491 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 1.914 | Reg loss: 0.037 | Tree loss: 1.914 | Accuracy: 0.403500 | 3.489 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 1.940 | Reg loss: 0.037 | Tree loss: 1.940 | Accuracy: 0.406143 | 3.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 2.055 | Reg loss: 0.037 | Tree loss: 2.055 | Accuracy: 0.289500 | 3.485 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 2.005 | Reg loss: 0.037 | Tree loss: 2.005 | Accuracy: 0.318500 | 3.483 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 2.008 | Reg loss: 0.037 | Tree loss: 2.008 | Accuracy: 0.316500 | 3.481 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 2.012 | Reg loss: 0.037 | Tree loss: 2.012 | Accuracy: 0.323500 | 3.478 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 1.954 | Reg loss: 0.037 | Tree loss: 1.954 | Accuracy: 0.353500 | 3.476 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 1.936 | Reg loss: 0.037 | Tree loss: 1.936 | Accuracy: 0.368500 | 3.474 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 1.952 | Reg loss: 0.037 | Tree loss: 1.952 | Accuracy: 0.388000 | 3.471 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 1.900 | Reg loss: 0.037 | Tree loss: 1.900 | Accuracy: 0.428000 | 3.469 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 1.927 | Reg loss: 0.037 | Tree loss: 1.927 | Accuracy: 0.389500 | 3.467 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 1.933 | Reg loss: 0.037 | Tree loss: 1.933 | Accuracy: 0.419500 | 3.464 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 1.865 | Reg loss: 0.037 | Tree loss: 1.865 | Accuracy: 0.440273 | 3.462 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 2.041 | Reg loss: 0.037 | Tree loss: 2.041 | Accuracy: 0.321000 | 3.461 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 2.037 | Reg loss: 0.037 | Tree loss: 2.037 | Accuracy: 0.317500 | 3.459 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 2.000 | Reg loss: 0.037 | Tree loss: 2.000 | Accuracy: 0.338000 | 3.456 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 1.987 | Reg loss: 0.037 | Tree loss: 1.987 | Accuracy: 0.344500 | 3.454 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 1.948 | Reg loss: 0.037 | Tree loss: 1.948 | Accuracy: 0.339000 | 3.452 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 1.933 | Reg loss: 0.037 | Tree loss: 1.933 | Accuracy: 0.360500 | 3.449 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 1.912 | Reg loss: 0.037 | Tree loss: 1.912 | Accuracy: 0.378000 | 3.447 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 1.929 | Reg loss: 0.037 | Tree loss: 1.929 | Accuracy: 0.385000 | 3.445 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 1.931 | Reg loss: 0.037 | Tree loss: 1.931 | Accuracy: 0.405000 | 3.442 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 1.920 | Reg loss: 0.037 | Tree loss: 1.920 | Accuracy: 0.407500 | 3.44 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.447099 | 3.438 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 2.046 | Reg loss: 0.037 | Tree loss: 2.046 | Accuracy: 0.325500 | 3.437 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 2.009 | Reg loss: 0.037 | Tree loss: 2.009 | Accuracy: 0.324000 | 3.434 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 2.008 | Reg loss: 0.037 | Tree loss: 2.008 | Accuracy: 0.323000 | 3.432 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 1.970 | Reg loss: 0.037 | Tree loss: 1.970 | Accuracy: 0.332000 | 3.43 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 1.958 | Reg loss: 0.037 | Tree loss: 1.958 | Accuracy: 0.346000 | 3.427 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 1.923 | Reg loss: 0.037 | Tree loss: 1.923 | Accuracy: 0.379000 | 3.425 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 1.936 | Reg loss: 0.037 | Tree loss: 1.936 | Accuracy: 0.386000 | 3.422 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 1.946 | Reg loss: 0.037 | Tree loss: 1.946 | Accuracy: 0.409500 | 3.42 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 1.907 | Reg loss: 0.038 | Tree loss: 1.907 | Accuracy: 0.431000 | 3.418 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.426000 | 3.415 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 1.918 | Reg loss: 0.038 | Tree loss: 1.918 | Accuracy: 0.426621 | 3.413 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 2.049 | Reg loss: 0.037 | Tree loss: 2.049 | Accuracy: 0.314000 | 3.412 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 2.031 | Reg loss: 0.037 | Tree loss: 2.031 | Accuracy: 0.319000 | 3.409 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 1.995 | Reg loss: 0.037 | Tree loss: 1.995 | Accuracy: 0.315500 | 3.407 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 1.967 | Reg loss: 0.037 | Tree loss: 1.967 | Accuracy: 0.331500 | 3.405 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 1.944 | Reg loss: 0.038 | Tree loss: 1.944 | Accuracy: 0.377000 | 3.403 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 1.948 | Reg loss: 0.038 | Tree loss: 1.948 | Accuracy: 0.369000 | 3.4 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 1.928 | Reg loss: 0.038 | Tree loss: 1.928 | Accuracy: 0.376000 | 3.398 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 1.939 | Reg loss: 0.038 | Tree loss: 1.939 | Accuracy: 0.404500 | 3.396 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.428000 | 3.394 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 1.881 | Reg loss: 0.038 | Tree loss: 1.881 | Accuracy: 0.440500 | 3.392 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 1.900 | Reg loss: 0.038 | Tree loss: 1.900 | Accuracy: 0.423208 | 3.389 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 2.017 | Reg loss: 0.038 | Tree loss: 2.017 | Accuracy: 0.342500 | 3.389 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 2.007 | Reg loss: 0.038 | Tree loss: 2.007 | Accuracy: 0.337000 | 3.387 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 2.017 | Reg loss: 0.038 | Tree loss: 2.017 | Accuracy: 0.315500 | 3.384 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 1.957 | Reg loss: 0.038 | Tree loss: 1.957 | Accuracy: 0.338000 | 3.382 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 1.974 | Reg loss: 0.038 | Tree loss: 1.974 | Accuracy: 0.349000 | 3.38 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 1.927 | Reg loss: 0.038 | Tree loss: 1.927 | Accuracy: 0.368500 | 3.378 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 1.914 | Reg loss: 0.038 | Tree loss: 1.914 | Accuracy: 0.392000 | 3.376 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 1.917 | Reg loss: 0.038 | Tree loss: 1.917 | Accuracy: 0.398000 | 3.374 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.406000 | 3.371 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 1.903 | Reg loss: 0.038 | Tree loss: 1.903 | Accuracy: 0.425000 | 3.369 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 1.896 | Reg loss: 0.038 | Tree loss: 1.896 | Accuracy: 0.402730 | 3.367 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 2.020 | Reg loss: 0.038 | Tree loss: 2.020 | Accuracy: 0.332500 | 3.366 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 1.996 | Reg loss: 0.038 | Tree loss: 1.996 | Accuracy: 0.326000 | 3.364 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.322000 | 3.362 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 1.976 | Reg loss: 0.038 | Tree loss: 1.976 | Accuracy: 0.332500 | 3.36 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 1.955 | Reg loss: 0.038 | Tree loss: 1.955 | Accuracy: 0.364500 | 3.358 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.379000 | 3.355 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.396000 | 3.353 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 1.942 | Reg loss: 0.038 | Tree loss: 1.942 | Accuracy: 0.391500 | 3.351 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 1.902 | Reg loss: 0.038 | Tree loss: 1.902 | Accuracy: 0.429500 | 3.349 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 1.888 | Reg loss: 0.038 | Tree loss: 1.888 | Accuracy: 0.425000 | 3.347 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 1.909 | Reg loss: 0.038 | Tree loss: 1.909 | Accuracy: 0.453925 | 3.345 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 2.053 | Reg loss: 0.038 | Tree loss: 2.053 | Accuracy: 0.312500 | 3.344 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 2.007 | Reg loss: 0.038 | Tree loss: 2.007 | Accuracy: 0.332000 | 3.342 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 1.981 | Reg loss: 0.038 | Tree loss: 1.981 | Accuracy: 0.323500 | 3.34 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 1.991 | Reg loss: 0.038 | Tree loss: 1.991 | Accuracy: 0.331000 | 3.338 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 1.932 | Reg loss: 0.038 | Tree loss: 1.932 | Accuracy: 0.359000 | 3.336 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.382000 | 3.334 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 1.930 | Reg loss: 0.038 | Tree loss: 1.930 | Accuracy: 0.391500 | 3.332 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 1.894 | Reg loss: 0.038 | Tree loss: 1.894 | Accuracy: 0.420000 | 3.33 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 1.888 | Reg loss: 0.038 | Tree loss: 1.888 | Accuracy: 0.436000 | 3.328 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 1.882 | Reg loss: 0.038 | Tree loss: 1.882 | Accuracy: 0.448000 | 3.325 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 1.889 | Reg loss: 0.038 | Tree loss: 1.889 | Accuracy: 0.470990 | 3.323 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 2.042 | Reg loss: 0.038 | Tree loss: 2.042 | Accuracy: 0.308500 | 3.321 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 2.026 | Reg loss: 0.038 | Tree loss: 2.026 | Accuracy: 0.330500 | 3.319 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 1.989 | Reg loss: 0.038 | Tree loss: 1.989 | Accuracy: 0.324500 | 3.317 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 1.948 | Reg loss: 0.038 | Tree loss: 1.948 | Accuracy: 0.361000 | 3.315 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 1.937 | Reg loss: 0.038 | Tree loss: 1.937 | Accuracy: 0.357500 | 3.313 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 1.924 | Reg loss: 0.038 | Tree loss: 1.924 | Accuracy: 0.382000 | 3.311 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 1.938 | Reg loss: 0.038 | Tree loss: 1.938 | Accuracy: 0.402000 | 3.309 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 1.866 | Reg loss: 0.038 | Tree loss: 1.866 | Accuracy: 0.438500 | 3.307 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 1.901 | Reg loss: 0.038 | Tree loss: 1.901 | Accuracy: 0.428500 | 3.305 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 1.882 | Reg loss: 0.038 | Tree loss: 1.882 | Accuracy: 0.447500 | 3.303 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 1.924 | Reg loss: 0.038 | Tree loss: 1.924 | Accuracy: 0.443686 | 3.301 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 2.009 | Reg loss: 0.038 | Tree loss: 2.009 | Accuracy: 0.325000 | 3.3 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 2.029 | Reg loss: 0.038 | Tree loss: 2.029 | Accuracy: 0.320500 | 3.298 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 1.984 | Reg loss: 0.038 | Tree loss: 1.984 | Accuracy: 0.335000 | 3.296 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 1.955 | Reg loss: 0.038 | Tree loss: 1.955 | Accuracy: 0.353500 | 3.295 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 1.938 | Reg loss: 0.038 | Tree loss: 1.938 | Accuracy: 0.363000 | 3.293 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 1.930 | Reg loss: 0.038 | Tree loss: 1.930 | Accuracy: 0.386000 | 3.291 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.401000 | 3.289 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 1.877 | Reg loss: 0.038 | Tree loss: 1.877 | Accuracy: 0.430000 | 3.287 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 1.897 | Reg loss: 0.038 | Tree loss: 1.897 | Accuracy: 0.444500 | 3.285 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 1.903 | Reg loss: 0.038 | Tree loss: 1.903 | Accuracy: 0.416000 | 3.283 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 1.868 | Reg loss: 0.038 | Tree loss: 1.868 | Accuracy: 0.443686 | 3.281 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 2.039 | Reg loss: 0.038 | Tree loss: 2.039 | Accuracy: 0.306500 | 3.281 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 2.029 | Reg loss: 0.038 | Tree loss: 2.029 | Accuracy: 0.300500 | 3.279 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 1.980 | Reg loss: 0.038 | Tree loss: 1.980 | Accuracy: 0.334500 | 3.278 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 1.955 | Reg loss: 0.038 | Tree loss: 1.955 | Accuracy: 0.356500 | 3.276 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 1.908 | Reg loss: 0.038 | Tree loss: 1.908 | Accuracy: 0.395000 | 3.274 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 1.910 | Reg loss: 0.038 | Tree loss: 1.910 | Accuracy: 0.398500 | 3.272 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 1.896 | Reg loss: 0.038 | Tree loss: 1.896 | Accuracy: 0.403500 | 3.27 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 1.897 | Reg loss: 0.038 | Tree loss: 1.897 | Accuracy: 0.414500 | 3.268 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 1.897 | Reg loss: 0.038 | Tree loss: 1.897 | Accuracy: 0.444500 | 3.266 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 1.885 | Reg loss: 0.038 | Tree loss: 1.885 | Accuracy: 0.447500 | 3.264 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 1.940 | Reg loss: 0.038 | Tree loss: 1.940 | Accuracy: 0.368601 | 3.263 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 2.013 | Reg loss: 0.038 | Tree loss: 2.013 | Accuracy: 0.327000 | 3.262 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 2.008 | Reg loss: 0.038 | Tree loss: 2.008 | Accuracy: 0.340000 | 3.26 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 1.978 | Reg loss: 0.038 | Tree loss: 1.978 | Accuracy: 0.328000 | 3.258 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 1.964 | Reg loss: 0.038 | Tree loss: 1.964 | Accuracy: 0.349500 | 3.257 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 1.926 | Reg loss: 0.038 | Tree loss: 1.926 | Accuracy: 0.390500 | 3.255 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 1.907 | Reg loss: 0.038 | Tree loss: 1.907 | Accuracy: 0.402000 | 3.253 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.399000 | 3.251 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.406500 | 3.249 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 1.890 | Reg loss: 0.038 | Tree loss: 1.890 | Accuracy: 0.439000 | 3.248 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 1.891 | Reg loss: 0.038 | Tree loss: 1.891 | Accuracy: 0.431500 | 3.246 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 1.817 | Reg loss: 0.038 | Tree loss: 1.817 | Accuracy: 0.477816 | 3.244 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.329500 | 3.242 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.997 | Reg loss: 0.038 | Tree loss: 1.997 | Accuracy: 0.318000 | 3.24 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 1.991 | Reg loss: 0.038 | Tree loss: 1.991 | Accuracy: 0.329000 | 3.239 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 1.941 | Reg loss: 0.038 | Tree loss: 1.941 | Accuracy: 0.357500 | 3.237 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 1.935 | Reg loss: 0.038 | Tree loss: 1.935 | Accuracy: 0.351500 | 3.235 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 1.918 | Reg loss: 0.038 | Tree loss: 1.918 | Accuracy: 0.417500 | 3.233 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 1.902 | Reg loss: 0.038 | Tree loss: 1.902 | Accuracy: 0.391000 | 3.231 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 1.905 | Reg loss: 0.038 | Tree loss: 1.905 | Accuracy: 0.413000 | 3.229 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 1.876 | Reg loss: 0.039 | Tree loss: 1.876 | Accuracy: 0.451500 | 3.228 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 1.875 | Reg loss: 0.039 | Tree loss: 1.875 | Accuracy: 0.436500 | 3.226 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 1.897 | Reg loss: 0.039 | Tree loss: 1.897 | Accuracy: 0.423208 | 3.224 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 2.012 | Reg loss: 0.038 | Tree loss: 2.012 | Accuracy: 0.323000 | 3.224 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 1.988 | Reg loss: 0.038 | Tree loss: 1.988 | Accuracy: 0.339500 | 3.222 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 1.964 | Reg loss: 0.038 | Tree loss: 1.964 | Accuracy: 0.337500 | 3.22 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 1.938 | Reg loss: 0.038 | Tree loss: 1.938 | Accuracy: 0.377500 | 3.218 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 1.936 | Reg loss: 0.038 | Tree loss: 1.936 | Accuracy: 0.395000 | 3.216 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 1.917 | Reg loss: 0.039 | Tree loss: 1.917 | Accuracy: 0.406000 | 3.214 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 1.902 | Reg loss: 0.039 | Tree loss: 1.902 | Accuracy: 0.429500 | 3.213 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 1.896 | Reg loss: 0.039 | Tree loss: 1.896 | Accuracy: 0.412000 | 3.211 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 1.885 | Reg loss: 0.039 | Tree loss: 1.885 | Accuracy: 0.424000 | 3.209 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 1.888 | Reg loss: 0.039 | Tree loss: 1.888 | Accuracy: 0.435500 | 3.207 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 1.889 | Reg loss: 0.039 | Tree loss: 1.889 | Accuracy: 0.409556 | 3.205 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 2.015 | Reg loss: 0.039 | Tree loss: 2.015 | Accuracy: 0.328500 | 3.205 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.995 | Reg loss: 0.039 | Tree loss: 1.995 | Accuracy: 0.342500 | 3.203 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 1.983 | Reg loss: 0.039 | Tree loss: 1.983 | Accuracy: 0.316000 | 3.201 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 1.939 | Reg loss: 0.039 | Tree loss: 1.939 | Accuracy: 0.348500 | 3.199 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 1.904 | Reg loss: 0.039 | Tree loss: 1.904 | Accuracy: 0.393000 | 3.197 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 1.924 | Reg loss: 0.039 | Tree loss: 1.924 | Accuracy: 0.383000 | 3.195 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 1.898 | Reg loss: 0.039 | Tree loss: 1.898 | Accuracy: 0.415500 | 3.193 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 1.901 | Reg loss: 0.039 | Tree loss: 1.901 | Accuracy: 0.428000 | 3.191 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 1.871 | Reg loss: 0.039 | Tree loss: 1.871 | Accuracy: 0.439000 | 3.19 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 1.866 | Reg loss: 0.039 | Tree loss: 1.866 | Accuracy: 0.458000 | 3.188 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 1.869 | Reg loss: 0.039 | Tree loss: 1.869 | Accuracy: 0.450512 | 3.186 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 2.018 | Reg loss: 0.039 | Tree loss: 2.018 | Accuracy: 0.343500 | 3.185 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 2.001 | Reg loss: 0.039 | Tree loss: 2.001 | Accuracy: 0.321500 | 3.183 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 1.952 | Reg loss: 0.039 | Tree loss: 1.952 | Accuracy: 0.348500 | 3.181 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 1.944 | Reg loss: 0.039 | Tree loss: 1.944 | Accuracy: 0.363000 | 3.18 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 1.905 | Reg loss: 0.039 | Tree loss: 1.905 | Accuracy: 0.382000 | 3.178 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 1.912 | Reg loss: 0.039 | Tree loss: 1.912 | Accuracy: 0.423500 | 3.176 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 1.891 | Reg loss: 0.039 | Tree loss: 1.891 | Accuracy: 0.412500 | 3.175 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 1.876 | Reg loss: 0.039 | Tree loss: 1.876 | Accuracy: 0.430000 | 3.173 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 1.881 | Reg loss: 0.039 | Tree loss: 1.881 | Accuracy: 0.424000 | 3.171 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 1.903 | Reg loss: 0.039 | Tree loss: 1.903 | Accuracy: 0.445000 | 3.169 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.382253 | 3.168 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 2.021 | Reg loss: 0.039 | Tree loss: 2.021 | Accuracy: 0.324500 | 3.167 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 1.972 | Reg loss: 0.039 | Tree loss: 1.972 | Accuracy: 0.346500 | 3.165 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 1.955 | Reg loss: 0.039 | Tree loss: 1.955 | Accuracy: 0.361000 | 3.164 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 1.928 | Reg loss: 0.039 | Tree loss: 1.928 | Accuracy: 0.358500 | 3.162 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 1.913 | Reg loss: 0.039 | Tree loss: 1.913 | Accuracy: 0.370000 | 3.16 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 1.921 | Reg loss: 0.039 | Tree loss: 1.921 | Accuracy: 0.387000 | 3.158 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 1.896 | Reg loss: 0.039 | Tree loss: 1.896 | Accuracy: 0.416000 | 3.157 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 1.890 | Reg loss: 0.039 | Tree loss: 1.890 | Accuracy: 0.406000 | 3.155 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 1.883 | Reg loss: 0.039 | Tree loss: 1.883 | Accuracy: 0.434500 | 3.153 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 1.877 | Reg loss: 0.039 | Tree loss: 1.877 | Accuracy: 0.447000 | 3.152 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 1.896 | Reg loss: 0.039 | Tree loss: 1.896 | Accuracy: 0.402730 | 3.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 2.026 | Reg loss: 0.039 | Tree loss: 2.026 | Accuracy: 0.328000 | 3.149 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 1.987 | Reg loss: 0.039 | Tree loss: 1.987 | Accuracy: 0.340000 | 3.147 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 1.947 | Reg loss: 0.039 | Tree loss: 1.947 | Accuracy: 0.352500 | 3.145 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 1.917 | Reg loss: 0.039 | Tree loss: 1.917 | Accuracy: 0.362500 | 3.144 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 1.912 | Reg loss: 0.039 | Tree loss: 1.912 | Accuracy: 0.394000 | 3.142 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 1.899 | Reg loss: 0.039 | Tree loss: 1.899 | Accuracy: 0.428500 | 3.14 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 1.888 | Reg loss: 0.039 | Tree loss: 1.888 | Accuracy: 0.427500 | 3.139 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 1.884 | Reg loss: 0.039 | Tree loss: 1.884 | Accuracy: 0.445000 | 3.137 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 1.917 | Reg loss: 0.039 | Tree loss: 1.917 | Accuracy: 0.436500 | 3.135 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 1.859 | Reg loss: 0.039 | Tree loss: 1.859 | Accuracy: 0.450000 | 3.134 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.457338 | 3.132 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 1.976 | Reg loss: 0.039 | Tree loss: 1.976 | Accuracy: 0.342000 | 3.131 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 1.991 | Reg loss: 0.039 | Tree loss: 1.991 | Accuracy: 0.327000 | 3.129 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 1.975 | Reg loss: 0.039 | Tree loss: 1.975 | Accuracy: 0.351000 | 3.128 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 1.933 | Reg loss: 0.039 | Tree loss: 1.933 | Accuracy: 0.351000 | 3.126 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 1.922 | Reg loss: 0.039 | Tree loss: 1.922 | Accuracy: 0.370000 | 3.124 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 1.874 | Reg loss: 0.039 | Tree loss: 1.874 | Accuracy: 0.422000 | 3.123 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 1.891 | Reg loss: 0.039 | Tree loss: 1.891 | Accuracy: 0.413000 | 3.121 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 1.892 | Reg loss: 0.039 | Tree loss: 1.892 | Accuracy: 0.417000 | 3.12 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 1.887 | Reg loss: 0.039 | Tree loss: 1.887 | Accuracy: 0.423000 | 3.118 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 1.877 | Reg loss: 0.039 | Tree loss: 1.877 | Accuracy: 0.433000 | 3.116 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 1.844 | Reg loss: 0.039 | Tree loss: 1.844 | Accuracy: 0.474403 | 3.115 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 2.000 | Reg loss: 0.039 | Tree loss: 2.000 | Accuracy: 0.334500 | 3.114 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 1.989 | Reg loss: 0.039 | Tree loss: 1.989 | Accuracy: 0.339500 | 3.112 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 1.951 | Reg loss: 0.039 | Tree loss: 1.951 | Accuracy: 0.355000 | 3.11 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 1.911 | Reg loss: 0.039 | Tree loss: 1.911 | Accuracy: 0.375500 | 3.109 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 1.907 | Reg loss: 0.039 | Tree loss: 1.907 | Accuracy: 0.391000 | 3.107 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 1.920 | Reg loss: 0.039 | Tree loss: 1.920 | Accuracy: 0.411000 | 3.105 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 1.885 | Reg loss: 0.039 | Tree loss: 1.885 | Accuracy: 0.425000 | 3.104 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 1.886 | Reg loss: 0.039 | Tree loss: 1.886 | Accuracy: 0.414000 | 3.102 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 1.861 | Reg loss: 0.039 | Tree loss: 1.861 | Accuracy: 0.456000 | 3.101 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 1.865 | Reg loss: 0.039 | Tree loss: 1.865 | Accuracy: 0.441500 | 3.099 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 1.864 | Reg loss: 0.039 | Tree loss: 1.864 | Accuracy: 0.491468 | 3.097 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 2.008 | Reg loss: 0.039 | Tree loss: 2.008 | Accuracy: 0.313500 | 3.097 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 1.973 | Reg loss: 0.039 | Tree loss: 1.973 | Accuracy: 0.357000 | 3.095 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 1.941 | Reg loss: 0.039 | Tree loss: 1.941 | Accuracy: 0.363500 | 3.093 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 1.940 | Reg loss: 0.039 | Tree loss: 1.940 | Accuracy: 0.365500 | 3.092 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.380000 | 3.09 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 1.901 | Reg loss: 0.039 | Tree loss: 1.901 | Accuracy: 0.425000 | 3.088 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 1.884 | Reg loss: 0.039 | Tree loss: 1.884 | Accuracy: 0.425500 | 3.087 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 1.863 | Reg loss: 0.039 | Tree loss: 1.863 | Accuracy: 0.432500 | 3.085 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 1.895 | Reg loss: 0.039 | Tree loss: 1.895 | Accuracy: 0.432000 | 3.084 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 1.866 | Reg loss: 0.039 | Tree loss: 1.866 | Accuracy: 0.438000 | 3.082 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 1.867 | Reg loss: 0.039 | Tree loss: 1.867 | Accuracy: 0.447099 | 3.081 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 2.008 | Reg loss: 0.039 | Tree loss: 2.008 | Accuracy: 0.333000 | 3.08 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 1.981 | Reg loss: 0.039 | Tree loss: 1.981 | Accuracy: 0.348000 | 3.078 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 1.948 | Reg loss: 0.039 | Tree loss: 1.948 | Accuracy: 0.348500 | 3.077 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 1.926 | Reg loss: 0.039 | Tree loss: 1.926 | Accuracy: 0.353500 | 3.075 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 1.892 | Reg loss: 0.039 | Tree loss: 1.892 | Accuracy: 0.390000 | 3.074 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.399500 | 3.072 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 1.874 | Reg loss: 0.039 | Tree loss: 1.874 | Accuracy: 0.431000 | 3.07 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 1.855 | Reg loss: 0.039 | Tree loss: 1.855 | Accuracy: 0.443000 | 3.069 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 1.900 | Reg loss: 0.039 | Tree loss: 1.900 | Accuracy: 0.446000 | 3.067 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 1.854 | Reg loss: 0.039 | Tree loss: 1.854 | Accuracy: 0.451000 | 3.066 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 1.933 | Reg loss: 0.039 | Tree loss: 1.933 | Accuracy: 0.406143 | 3.064 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 1.998 | Reg loss: 0.039 | Tree loss: 1.998 | Accuracy: 0.333000 | 3.063 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 1.958 | Reg loss: 0.039 | Tree loss: 1.958 | Accuracy: 0.352000 | 3.062 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 1.940 | Reg loss: 0.039 | Tree loss: 1.940 | Accuracy: 0.337000 | 3.06 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 1.914 | Reg loss: 0.039 | Tree loss: 1.914 | Accuracy: 0.374000 | 3.059 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 1.918 | Reg loss: 0.039 | Tree loss: 1.918 | Accuracy: 0.358500 | 3.057 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 1.884 | Reg loss: 0.039 | Tree loss: 1.884 | Accuracy: 0.407000 | 3.055 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 1.866 | Reg loss: 0.039 | Tree loss: 1.866 | Accuracy: 0.452000 | 3.054 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 1.877 | Reg loss: 0.039 | Tree loss: 1.877 | Accuracy: 0.434500 | 3.052 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 1.903 | Reg loss: 0.039 | Tree loss: 1.903 | Accuracy: 0.428000 | 3.051 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 1.866 | Reg loss: 0.039 | Tree loss: 1.866 | Accuracy: 0.447500 | 3.049 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 1.838 | Reg loss: 0.039 | Tree loss: 1.838 | Accuracy: 0.484642 | 3.048 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.982 | Reg loss: 0.039 | Tree loss: 1.982 | Accuracy: 0.344000 | 3.048 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 1.973 | Reg loss: 0.039 | Tree loss: 1.973 | Accuracy: 0.348500 | 3.046 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 1.959 | Reg loss: 0.039 | Tree loss: 1.959 | Accuracy: 0.347000 | 3.045 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 1.947 | Reg loss: 0.039 | Tree loss: 1.947 | Accuracy: 0.353000 | 3.043 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 1.919 | Reg loss: 0.039 | Tree loss: 1.919 | Accuracy: 0.391500 | 3.041 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 1.873 | Reg loss: 0.039 | Tree loss: 1.873 | Accuracy: 0.416500 | 3.04 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 1.853 | Reg loss: 0.039 | Tree loss: 1.853 | Accuracy: 0.423000 | 3.038 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 1.863 | Reg loss: 0.040 | Tree loss: 1.863 | Accuracy: 0.442500 | 3.037 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 1.870 | Reg loss: 0.040 | Tree loss: 1.870 | Accuracy: 0.429000 | 3.035 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 1.842 | Reg loss: 0.040 | Tree loss: 1.842 | Accuracy: 0.439500 | 3.034 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 1.855 | Reg loss: 0.040 | Tree loss: 1.855 | Accuracy: 0.453925 | 3.032 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 2.007 | Reg loss: 0.039 | Tree loss: 2.007 | Accuracy: 0.335000 | 3.032 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 1.962 | Reg loss: 0.039 | Tree loss: 1.962 | Accuracy: 0.357000 | 3.03 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 1.932 | Reg loss: 0.039 | Tree loss: 1.932 | Accuracy: 0.356500 | 3.029 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 1.914 | Reg loss: 0.039 | Tree loss: 1.914 | Accuracy: 0.371500 | 3.027 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 1.915 | Reg loss: 0.040 | Tree loss: 1.915 | Accuracy: 0.381000 | 3.026 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 1.888 | Reg loss: 0.040 | Tree loss: 1.888 | Accuracy: 0.420000 | 3.025 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 1.872 | Reg loss: 0.040 | Tree loss: 1.872 | Accuracy: 0.421500 | 3.023 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 1.859 | Reg loss: 0.040 | Tree loss: 1.859 | Accuracy: 0.437000 | 3.021 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.453500 | 3.02 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 1.858 | Reg loss: 0.040 | Tree loss: 1.858 | Accuracy: 0.428500 | 3.018 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.450512 | 3.017 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 1.979 | Reg loss: 0.040 | Tree loss: 1.979 | Accuracy: 0.343500 | 3.017 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 1.963 | Reg loss: 0.040 | Tree loss: 1.963 | Accuracy: 0.349000 | 3.015 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 1.948 | Reg loss: 0.040 | Tree loss: 1.948 | Accuracy: 0.343500 | 3.014 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 1.886 | Reg loss: 0.040 | Tree loss: 1.886 | Accuracy: 0.392500 | 3.012 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 1.902 | Reg loss: 0.040 | Tree loss: 1.902 | Accuracy: 0.388000 | 3.011 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.421500 | 3.01 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.418000 | 3.008 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 1.859 | Reg loss: 0.040 | Tree loss: 1.859 | Accuracy: 0.436000 | 3.007 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 1.883 | Reg loss: 0.040 | Tree loss: 1.883 | Accuracy: 0.441500 | 3.005 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 1.866 | Reg loss: 0.040 | Tree loss: 1.866 | Accuracy: 0.442500 | 3.004 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 1.883 | Reg loss: 0.040 | Tree loss: 1.883 | Accuracy: 0.430034 | 3.002 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 1.976 | Reg loss: 0.040 | Tree loss: 1.976 | Accuracy: 0.348000 | 3.002 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 1.958 | Reg loss: 0.040 | Tree loss: 1.958 | Accuracy: 0.348500 | 3.0 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 1.932 | Reg loss: 0.040 | Tree loss: 1.932 | Accuracy: 0.351500 | 2.999 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 1.897 | Reg loss: 0.040 | Tree loss: 1.897 | Accuracy: 0.384000 | 2.997 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 1.902 | Reg loss: 0.040 | Tree loss: 1.902 | Accuracy: 0.396500 | 2.996 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 1.876 | Reg loss: 0.040 | Tree loss: 1.876 | Accuracy: 0.431000 | 2.994 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 1.881 | Reg loss: 0.040 | Tree loss: 1.881 | Accuracy: 0.421500 | 2.993 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 1.872 | Reg loss: 0.040 | Tree loss: 1.872 | Accuracy: 0.426500 | 2.992 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 1.868 | Reg loss: 0.040 | Tree loss: 1.868 | Accuracy: 0.436000 | 2.99 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 1.872 | Reg loss: 0.040 | Tree loss: 1.872 | Accuracy: 0.432000 | 2.989 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 1.844 | Reg loss: 0.040 | Tree loss: 1.844 | Accuracy: 0.494881 | 2.987 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 1.974 | Reg loss: 0.040 | Tree loss: 1.974 | Accuracy: 0.359500 | 2.986 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 1.961 | Reg loss: 0.040 | Tree loss: 1.961 | Accuracy: 0.352000 | 2.985 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 1.926 | Reg loss: 0.040 | Tree loss: 1.926 | Accuracy: 0.380000 | 2.983 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 1.917 | Reg loss: 0.040 | Tree loss: 1.917 | Accuracy: 0.385500 | 2.982 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 1.868 | Reg loss: 0.040 | Tree loss: 1.868 | Accuracy: 0.417500 | 2.98 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 1.867 | Reg loss: 0.040 | Tree loss: 1.867 | Accuracy: 0.424000 | 2.979 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 1.880 | Reg loss: 0.040 | Tree loss: 1.880 | Accuracy: 0.434500 | 2.977 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.453000 | 2.976 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.429000 | 2.974 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 1.878 | Reg loss: 0.040 | Tree loss: 1.878 | Accuracy: 0.436000 | 2.973 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 1.811 | Reg loss: 0.040 | Tree loss: 1.811 | Accuracy: 0.440273 | 2.971 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 1.977 | Reg loss: 0.040 | Tree loss: 1.977 | Accuracy: 0.340000 | 2.971 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 1.957 | Reg loss: 0.040 | Tree loss: 1.957 | Accuracy: 0.343500 | 2.969 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 1.937 | Reg loss: 0.040 | Tree loss: 1.937 | Accuracy: 0.358000 | 2.968 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 1.901 | Reg loss: 0.040 | Tree loss: 1.901 | Accuracy: 0.381000 | 2.966 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 1.898 | Reg loss: 0.040 | Tree loss: 1.898 | Accuracy: 0.389000 | 2.965 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 1.866 | Reg loss: 0.040 | Tree loss: 1.866 | Accuracy: 0.418500 | 2.963 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 1.871 | Reg loss: 0.040 | Tree loss: 1.871 | Accuracy: 0.433000 | 2.962 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.425500 | 2.961 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.439000 | 2.959 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 1.844 | Reg loss: 0.040 | Tree loss: 1.844 | Accuracy: 0.447000 | 2.958 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 1.864 | Reg loss: 0.040 | Tree loss: 1.864 | Accuracy: 0.447099 | 2.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 1.984 | Reg loss: 0.040 | Tree loss: 1.984 | Accuracy: 0.355000 | 2.956 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 1.979 | Reg loss: 0.040 | Tree loss: 1.979 | Accuracy: 0.346500 | 2.954 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 1.953 | Reg loss: 0.040 | Tree loss: 1.953 | Accuracy: 0.354000 | 2.953 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 1.897 | Reg loss: 0.040 | Tree loss: 1.897 | Accuracy: 0.378500 | 2.951 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 1.874 | Reg loss: 0.040 | Tree loss: 1.874 | Accuracy: 0.399500 | 2.95 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 1.854 | Reg loss: 0.040 | Tree loss: 1.854 | Accuracy: 0.428000 | 2.948 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 1.857 | Reg loss: 0.040 | Tree loss: 1.857 | Accuracy: 0.430000 | 2.947 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 1.864 | Reg loss: 0.040 | Tree loss: 1.864 | Accuracy: 0.430500 | 2.945 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 1.858 | Reg loss: 0.040 | Tree loss: 1.858 | Accuracy: 0.437000 | 2.944 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.457500 | 2.942 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 1.843 | Reg loss: 0.040 | Tree loss: 1.843 | Accuracy: 0.457338 | 2.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 1.980 | Reg loss: 0.040 | Tree loss: 1.980 | Accuracy: 0.338000 | 2.94 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 1.949 | Reg loss: 0.040 | Tree loss: 1.949 | Accuracy: 0.365000 | 2.939 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 1.913 | Reg loss: 0.040 | Tree loss: 1.913 | Accuracy: 0.372500 | 2.937 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 1.899 | Reg loss: 0.040 | Tree loss: 1.899 | Accuracy: 0.398000 | 2.936 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 1.868 | Reg loss: 0.040 | Tree loss: 1.868 | Accuracy: 0.413500 | 2.935 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 1.889 | Reg loss: 0.040 | Tree loss: 1.889 | Accuracy: 0.424500 | 2.933 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 1.869 | Reg loss: 0.040 | Tree loss: 1.869 | Accuracy: 0.415500 | 2.932 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 1.874 | Reg loss: 0.040 | Tree loss: 1.874 | Accuracy: 0.432000 | 2.93 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 1.844 | Reg loss: 0.040 | Tree loss: 1.844 | Accuracy: 0.467500 | 2.929 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 1.859 | Reg loss: 0.040 | Tree loss: 1.859 | Accuracy: 0.439000 | 2.928 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 1.826 | Reg loss: 0.040 | Tree loss: 1.826 | Accuracy: 0.501706 | 2.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 1.974 | Reg loss: 0.040 | Tree loss: 1.974 | Accuracy: 0.357000 | 2.926 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 1.940 | Reg loss: 0.040 | Tree loss: 1.940 | Accuracy: 0.351000 | 2.925 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 1.936 | Reg loss: 0.040 | Tree loss: 1.936 | Accuracy: 0.368500 | 2.923 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 1.917 | Reg loss: 0.040 | Tree loss: 1.917 | Accuracy: 0.381500 | 2.922 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 1.878 | Reg loss: 0.040 | Tree loss: 1.878 | Accuracy: 0.403000 | 2.92 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.447500 | 2.919 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.428000 | 2.918 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 1.860 | Reg loss: 0.040 | Tree loss: 1.860 | Accuracy: 0.421000 | 2.916 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 1.849 | Reg loss: 0.040 | Tree loss: 1.849 | Accuracy: 0.444000 | 2.915 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.439000 | 2.914 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 1.803 | Reg loss: 0.040 | Tree loss: 1.803 | Accuracy: 0.430034 | 2.912 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 1.991 | Reg loss: 0.040 | Tree loss: 1.991 | Accuracy: 0.355000 | 2.912 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 1.919 | Reg loss: 0.040 | Tree loss: 1.919 | Accuracy: 0.379000 | 2.911 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 1.935 | Reg loss: 0.040 | Tree loss: 1.935 | Accuracy: 0.372500 | 2.909 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 1.880 | Reg loss: 0.040 | Tree loss: 1.880 | Accuracy: 0.385000 | 2.908 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 1.874 | Reg loss: 0.040 | Tree loss: 1.874 | Accuracy: 0.403000 | 2.906 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.418500 | 2.905 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 1.835 | Reg loss: 0.040 | Tree loss: 1.835 | Accuracy: 0.429000 | 2.904 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 1.867 | Reg loss: 0.040 | Tree loss: 1.867 | Accuracy: 0.437500 | 2.903 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.421000 | 2.901 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 1.845 | Reg loss: 0.040 | Tree loss: 1.845 | Accuracy: 0.447000 | 2.9 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 1.887 | Reg loss: 0.040 | Tree loss: 1.887 | Accuracy: 0.457338 | 2.898 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 1.959 | Reg loss: 0.040 | Tree loss: 1.959 | Accuracy: 0.355500 | 2.898 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 1.952 | Reg loss: 0.040 | Tree loss: 1.952 | Accuracy: 0.364500 | 2.897 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 1.933 | Reg loss: 0.040 | Tree loss: 1.933 | Accuracy: 0.374000 | 2.895 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 1.909 | Reg loss: 0.040 | Tree loss: 1.909 | Accuracy: 0.372000 | 2.894 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 1.875 | Reg loss: 0.040 | Tree loss: 1.875 | Accuracy: 0.403500 | 2.893 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 1.844 | Reg loss: 0.040 | Tree loss: 1.844 | Accuracy: 0.431000 | 2.891 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 1.859 | Reg loss: 0.040 | Tree loss: 1.859 | Accuracy: 0.435000 | 2.89 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 1.856 | Reg loss: 0.040 | Tree loss: 1.856 | Accuracy: 0.418500 | 2.889 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 1.839 | Reg loss: 0.040 | Tree loss: 1.839 | Accuracy: 0.442000 | 2.887 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 1.873 | Reg loss: 0.040 | Tree loss: 1.873 | Accuracy: 0.417500 | 2.886 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 1.817 | Reg loss: 0.040 | Tree loss: 1.817 | Accuracy: 0.460751 | 2.885 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 1.954 | Reg loss: 0.040 | Tree loss: 1.954 | Accuracy: 0.359500 | 2.883 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 1.955 | Reg loss: 0.040 | Tree loss: 1.955 | Accuracy: 0.353500 | 2.882 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 1.956 | Reg loss: 0.040 | Tree loss: 1.956 | Accuracy: 0.361500 | 2.88 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 1.911 | Reg loss: 0.040 | Tree loss: 1.911 | Accuracy: 0.363000 | 2.879 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 1.881 | Reg loss: 0.040 | Tree loss: 1.881 | Accuracy: 0.396500 | 2.878 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 1.856 | Reg loss: 0.040 | Tree loss: 1.856 | Accuracy: 0.419000 | 2.877 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.435500 | 2.875 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 1.850 | Reg loss: 0.040 | Tree loss: 1.850 | Accuracy: 0.448500 | 2.874 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 1.831 | Reg loss: 0.040 | Tree loss: 1.831 | Accuracy: 0.443000 | 2.873 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 1.828 | Reg loss: 0.040 | Tree loss: 1.828 | Accuracy: 0.456500 | 2.871 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.464164 | 2.87 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 1.943 | Reg loss: 0.040 | Tree loss: 1.943 | Accuracy: 0.373000 | 2.869 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 1.951 | Reg loss: 0.040 | Tree loss: 1.951 | Accuracy: 0.360500 | 2.868 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 1.931 | Reg loss: 0.040 | Tree loss: 1.931 | Accuracy: 0.390500 | 2.867 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 1.903 | Reg loss: 0.040 | Tree loss: 1.903 | Accuracy: 0.383500 | 2.865 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 1.883 | Reg loss: 0.040 | Tree loss: 1.883 | Accuracy: 0.399000 | 2.864 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 1.859 | Reg loss: 0.040 | Tree loss: 1.859 | Accuracy: 0.420500 | 2.863 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 1.832 | Reg loss: 0.040 | Tree loss: 1.832 | Accuracy: 0.423500 | 2.861 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 1.845 | Reg loss: 0.041 | Tree loss: 1.845 | Accuracy: 0.437500 | 2.86 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.424500 | 2.859 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 1.859 | Reg loss: 0.041 | Tree loss: 1.859 | Accuracy: 0.437500 | 2.858 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 1.777 | Reg loss: 0.041 | Tree loss: 1.777 | Accuracy: 0.443686 | 2.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 1.945 | Reg loss: 0.040 | Tree loss: 1.945 | Accuracy: 0.378500 | 2.856 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 1.944 | Reg loss: 0.040 | Tree loss: 1.944 | Accuracy: 0.368500 | 2.855 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 1.917 | Reg loss: 0.040 | Tree loss: 1.917 | Accuracy: 0.372000 | 2.853 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 1.921 | Reg loss: 0.040 | Tree loss: 1.921 | Accuracy: 0.369500 | 2.852 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 1.883 | Reg loss: 0.041 | Tree loss: 1.883 | Accuracy: 0.387500 | 2.851 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.412500 | 2.849 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 1.862 | Reg loss: 0.041 | Tree loss: 1.862 | Accuracy: 0.427500 | 2.848 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 1.833 | Reg loss: 0.041 | Tree loss: 1.833 | Accuracy: 0.446500 | 2.847 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.435000 | 2.845 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 1.816 | Reg loss: 0.041 | Tree loss: 1.816 | Accuracy: 0.468500 | 2.844 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 1.840 | Reg loss: 0.041 | Tree loss: 1.840 | Accuracy: 0.498294 | 2.843 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 1.941 | Reg loss: 0.041 | Tree loss: 1.941 | Accuracy: 0.367000 | 2.842 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 1.920 | Reg loss: 0.041 | Tree loss: 1.920 | Accuracy: 0.379500 | 2.841 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 1.939 | Reg loss: 0.041 | Tree loss: 1.939 | Accuracy: 0.361000 | 2.84 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 1.902 | Reg loss: 0.041 | Tree loss: 1.902 | Accuracy: 0.383500 | 2.839 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 1.887 | Reg loss: 0.041 | Tree loss: 1.887 | Accuracy: 0.377500 | 2.837 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 1.849 | Reg loss: 0.041 | Tree loss: 1.849 | Accuracy: 0.431000 | 2.836 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 1.831 | Reg loss: 0.041 | Tree loss: 1.831 | Accuracy: 0.447000 | 2.835 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 1.851 | Reg loss: 0.041 | Tree loss: 1.851 | Accuracy: 0.447500 | 2.834 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 1.861 | Reg loss: 0.041 | Tree loss: 1.861 | Accuracy: 0.435000 | 2.832 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.452000 | 2.831 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.505119 | 2.83 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 1.966 | Reg loss: 0.041 | Tree loss: 1.966 | Accuracy: 0.344500 | 2.83 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 1.926 | Reg loss: 0.041 | Tree loss: 1.926 | Accuracy: 0.367500 | 2.829 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 1.924 | Reg loss: 0.041 | Tree loss: 1.924 | Accuracy: 0.378000 | 2.827 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 1.919 | Reg loss: 0.041 | Tree loss: 1.919 | Accuracy: 0.380000 | 2.826 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 1.866 | Reg loss: 0.041 | Tree loss: 1.866 | Accuracy: 0.418000 | 2.825 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 1.849 | Reg loss: 0.041 | Tree loss: 1.849 | Accuracy: 0.444500 | 2.824 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 1.815 | Reg loss: 0.041 | Tree loss: 1.815 | Accuracy: 0.446000 | 2.822 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.448000 | 2.821 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.443000 | 2.82 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 1.805 | Reg loss: 0.041 | Tree loss: 1.805 | Accuracy: 0.463000 | 2.819 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 1.835 | Reg loss: 0.041 | Tree loss: 1.835 | Accuracy: 0.440273 | 2.817 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 1.941 | Reg loss: 0.041 | Tree loss: 1.941 | Accuracy: 0.370000 | 2.817 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 1.963 | Reg loss: 0.041 | Tree loss: 1.963 | Accuracy: 0.341500 | 2.816 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 1.913 | Reg loss: 0.041 | Tree loss: 1.913 | Accuracy: 0.390000 | 2.815 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 1.898 | Reg loss: 0.041 | Tree loss: 1.898 | Accuracy: 0.356500 | 2.814 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.428000 | 2.813 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.445500 | 2.812 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 1.821 | Reg loss: 0.041 | Tree loss: 1.821 | Accuracy: 0.460500 | 2.81 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 1.855 | Reg loss: 0.041 | Tree loss: 1.855 | Accuracy: 0.432000 | 2.809 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 1.859 | Reg loss: 0.041 | Tree loss: 1.859 | Accuracy: 0.428000 | 2.808 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.446000 | 2.807 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 1.836 | Reg loss: 0.041 | Tree loss: 1.836 | Accuracy: 0.433447 | 2.806 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 1.972 | Reg loss: 0.041 | Tree loss: 1.972 | Accuracy: 0.358000 | 2.805 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 1.943 | Reg loss: 0.041 | Tree loss: 1.943 | Accuracy: 0.349000 | 2.804 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 1.904 | Reg loss: 0.041 | Tree loss: 1.904 | Accuracy: 0.375500 | 2.803 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 1.871 | Reg loss: 0.041 | Tree loss: 1.871 | Accuracy: 0.423000 | 2.802 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 1.878 | Reg loss: 0.041 | Tree loss: 1.878 | Accuracy: 0.415000 | 2.8 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.434500 | 2.799 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 1.825 | Reg loss: 0.041 | Tree loss: 1.825 | Accuracy: 0.437500 | 2.798 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 1.835 | Reg loss: 0.041 | Tree loss: 1.835 | Accuracy: 0.451500 | 2.797 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 1.828 | Reg loss: 0.041 | Tree loss: 1.828 | Accuracy: 0.444500 | 2.796 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.454500 | 2.794 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 1.821 | Reg loss: 0.041 | Tree loss: 1.821 | Accuracy: 0.440273 | 2.793 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 1.949 | Reg loss: 0.041 | Tree loss: 1.949 | Accuracy: 0.353500 | 2.793 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 1.940 | Reg loss: 0.041 | Tree loss: 1.940 | Accuracy: 0.365000 | 2.791 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 1.907 | Reg loss: 0.041 | Tree loss: 1.907 | Accuracy: 0.385500 | 2.79 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 1.882 | Reg loss: 0.041 | Tree loss: 1.882 | Accuracy: 0.395500 | 2.789 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.423000 | 2.788 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 1.825 | Reg loss: 0.041 | Tree loss: 1.825 | Accuracy: 0.450000 | 2.787 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 1.841 | Reg loss: 0.041 | Tree loss: 1.841 | Accuracy: 0.439000 | 2.785 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.432500 | 2.784 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.445500 | 2.783 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 1.837 | Reg loss: 0.041 | Tree loss: 1.837 | Accuracy: 0.448500 | 2.782 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 1.797 | Reg loss: 0.041 | Tree loss: 1.797 | Accuracy: 0.443686 | 2.781 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 1.950 | Reg loss: 0.041 | Tree loss: 1.950 | Accuracy: 0.369000 | 2.78 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 1.923 | Reg loss: 0.041 | Tree loss: 1.923 | Accuracy: 0.367500 | 2.779 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 1.904 | Reg loss: 0.041 | Tree loss: 1.904 | Accuracy: 0.389500 | 2.778 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 1.903 | Reg loss: 0.041 | Tree loss: 1.903 | Accuracy: 0.394000 | 2.777 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 1.872 | Reg loss: 0.041 | Tree loss: 1.872 | Accuracy: 0.409500 | 2.775 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 1.818 | Reg loss: 0.041 | Tree loss: 1.818 | Accuracy: 0.456500 | 2.774 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.433000 | 2.773 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 1.833 | Reg loss: 0.041 | Tree loss: 1.833 | Accuracy: 0.448500 | 2.772 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.458500 | 2.771 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 1.830 | Reg loss: 0.041 | Tree loss: 1.830 | Accuracy: 0.455000 | 2.77 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 1.885 | Reg loss: 0.041 | Tree loss: 1.885 | Accuracy: 0.385666 | 2.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 1.951 | Reg loss: 0.041 | Tree loss: 1.951 | Accuracy: 0.373500 | 2.768 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 1.925 | Reg loss: 0.041 | Tree loss: 1.925 | Accuracy: 0.366500 | 2.767 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 1.929 | Reg loss: 0.041 | Tree loss: 1.929 | Accuracy: 0.368000 | 2.765 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 1.854 | Reg loss: 0.041 | Tree loss: 1.854 | Accuracy: 0.401000 | 2.764 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.409000 | 2.763 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.459000 | 2.762 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.436000 | 2.761 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 1.831 | Reg loss: 0.041 | Tree loss: 1.831 | Accuracy: 0.438000 | 2.76 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 1.841 | Reg loss: 0.041 | Tree loss: 1.841 | Accuracy: 0.435500 | 2.759 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.431500 | 2.757 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 1.783 | Reg loss: 0.041 | Tree loss: 1.783 | Accuracy: 0.460751 | 2.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 1.959 | Reg loss: 0.041 | Tree loss: 1.959 | Accuracy: 0.352500 | 2.756 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 1.926 | Reg loss: 0.041 | Tree loss: 1.926 | Accuracy: 0.378000 | 2.755 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 1.897 | Reg loss: 0.041 | Tree loss: 1.897 | Accuracy: 0.392500 | 2.754 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 1.878 | Reg loss: 0.041 | Tree loss: 1.878 | Accuracy: 0.396000 | 2.753 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 1.836 | Reg loss: 0.041 | Tree loss: 1.836 | Accuracy: 0.429500 | 2.752 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 1.853 | Reg loss: 0.041 | Tree loss: 1.853 | Accuracy: 0.406000 | 2.751 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 1.824 | Reg loss: 0.041 | Tree loss: 1.824 | Accuracy: 0.450000 | 2.749 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 1.842 | Reg loss: 0.041 | Tree loss: 1.842 | Accuracy: 0.454500 | 2.748 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 1.819 | Reg loss: 0.041 | Tree loss: 1.819 | Accuracy: 0.464000 | 2.747 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 1.846 | Reg loss: 0.041 | Tree loss: 1.846 | Accuracy: 0.429500 | 2.746 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 1.794 | Reg loss: 0.041 | Tree loss: 1.794 | Accuracy: 0.457338 | 2.745 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 1.975 | Reg loss: 0.041 | Tree loss: 1.975 | Accuracy: 0.365000 | 2.745 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 1.915 | Reg loss: 0.041 | Tree loss: 1.915 | Accuracy: 0.377000 | 2.744 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.041 | Tree loss: 1.886 | Accuracy: 0.391000 | 2.743 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.396000 | 2.742 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 1.877 | Reg loss: 0.041 | Tree loss: 1.877 | Accuracy: 0.398000 | 2.741 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 1.833 | Reg loss: 0.041 | Tree loss: 1.833 | Accuracy: 0.435500 | 2.74 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 1.831 | Reg loss: 0.041 | Tree loss: 1.831 | Accuracy: 0.454000 | 2.738 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 1.824 | Reg loss: 0.041 | Tree loss: 1.824 | Accuracy: 0.461000 | 2.737 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 1.834 | Reg loss: 0.041 | Tree loss: 1.834 | Accuracy: 0.443500 | 2.736 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 1.820 | Reg loss: 0.041 | Tree loss: 1.820 | Accuracy: 0.453500 | 2.735 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 1.823 | Reg loss: 0.041 | Tree loss: 1.823 | Accuracy: 0.460751 | 2.734 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 1.947 | Reg loss: 0.041 | Tree loss: 1.947 | Accuracy: 0.392000 | 2.734 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 1.920 | Reg loss: 0.041 | Tree loss: 1.920 | Accuracy: 0.374500 | 2.733 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 1.910 | Reg loss: 0.041 | Tree loss: 1.910 | Accuracy: 0.369500 | 2.732 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 1.883 | Reg loss: 0.041 | Tree loss: 1.883 | Accuracy: 0.404000 | 2.731 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 1.866 | Reg loss: 0.041 | Tree loss: 1.866 | Accuracy: 0.394500 | 2.73 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 1.879 | Reg loss: 0.041 | Tree loss: 1.879 | Accuracy: 0.416500 | 2.729 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 1.805 | Reg loss: 0.041 | Tree loss: 1.805 | Accuracy: 0.456000 | 2.728 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 1.807 | Reg loss: 0.041 | Tree loss: 1.807 | Accuracy: 0.470500 | 2.726 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 1.822 | Reg loss: 0.041 | Tree loss: 1.822 | Accuracy: 0.453000 | 2.725 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 1.812 | Reg loss: 0.041 | Tree loss: 1.812 | Accuracy: 0.445000 | 2.724 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 1.783 | Reg loss: 0.041 | Tree loss: 1.783 | Accuracy: 0.433447 | 2.723 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 1.936 | Reg loss: 0.041 | Tree loss: 1.936 | Accuracy: 0.369000 | 2.723 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 1.926 | Reg loss: 0.041 | Tree loss: 1.926 | Accuracy: 0.382000 | 2.722 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 1.910 | Reg loss: 0.041 | Tree loss: 1.910 | Accuracy: 0.400500 | 2.721 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.409000 | 2.72 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 1.880 | Reg loss: 0.041 | Tree loss: 1.880 | Accuracy: 0.386000 | 2.719 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.431000 | 2.718 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 1.816 | Reg loss: 0.041 | Tree loss: 1.816 | Accuracy: 0.459500 | 2.716 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.465500 | 2.715 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 1.825 | Reg loss: 0.041 | Tree loss: 1.825 | Accuracy: 0.440500 | 2.714 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 1.798 | Reg loss: 0.041 | Tree loss: 1.798 | Accuracy: 0.477000 | 2.713 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 1.791 | Reg loss: 0.041 | Tree loss: 1.791 | Accuracy: 0.481229 | 2.712 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 1.919 | Reg loss: 0.041 | Tree loss: 1.919 | Accuracy: 0.384500 | 2.712 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 1.910 | Reg loss: 0.041 | Tree loss: 1.910 | Accuracy: 0.374000 | 2.711 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 1.899 | Reg loss: 0.041 | Tree loss: 1.899 | Accuracy: 0.399500 | 2.71 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.400500 | 2.709 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 1.849 | Reg loss: 0.041 | Tree loss: 1.849 | Accuracy: 0.413500 | 2.707 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.414000 | 2.706 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 1.820 | Reg loss: 0.041 | Tree loss: 1.820 | Accuracy: 0.454000 | 2.705 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 1.839 | Reg loss: 0.041 | Tree loss: 1.839 | Accuracy: 0.440000 | 2.704 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 1.837 | Reg loss: 0.041 | Tree loss: 1.837 | Accuracy: 0.450000 | 2.703 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 1.810 | Reg loss: 0.041 | Tree loss: 1.810 | Accuracy: 0.471500 | 2.702 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.450512 | 2.701 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 1.936 | Reg loss: 0.041 | Tree loss: 1.936 | Accuracy: 0.375000 | 2.701 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 1.912 | Reg loss: 0.041 | Tree loss: 1.912 | Accuracy: 0.370500 | 2.7 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 1.892 | Reg loss: 0.041 | Tree loss: 1.892 | Accuracy: 0.379500 | 2.699 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.398000 | 2.698 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 1.847 | Reg loss: 0.041 | Tree loss: 1.847 | Accuracy: 0.420000 | 2.697 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.417000 | 2.696 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 1.808 | Reg loss: 0.041 | Tree loss: 1.808 | Accuracy: 0.453000 | 2.695 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.453000 | 2.694 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.445000 | 2.693 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.458500 | 2.692 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.457338 | 2.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 1.922 | Reg loss: 0.041 | Tree loss: 1.922 | Accuracy: 0.361500 | 2.69 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 1.928 | Reg loss: 0.041 | Tree loss: 1.928 | Accuracy: 0.359500 | 2.689 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 1.893 | Reg loss: 0.041 | Tree loss: 1.893 | Accuracy: 0.392500 | 2.688 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 1.861 | Reg loss: 0.041 | Tree loss: 1.861 | Accuracy: 0.429000 | 2.687 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 1.821 | Reg loss: 0.041 | Tree loss: 1.821 | Accuracy: 0.435000 | 2.686 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.452000 | 2.685 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.460500 | 2.684 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.450500 | 2.683 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.421500 | 2.682 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 1.822 | Reg loss: 0.042 | Tree loss: 1.822 | Accuracy: 0.450000 | 2.681 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.447099 | 2.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 1.943 | Reg loss: 0.042 | Tree loss: 1.943 | Accuracy: 0.378000 | 2.68 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 1.929 | Reg loss: 0.042 | Tree loss: 1.929 | Accuracy: 0.357500 | 2.679 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 1.898 | Reg loss: 0.042 | Tree loss: 1.898 | Accuracy: 0.384500 | 2.678 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.420000 | 2.677 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.429500 | 2.676 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.434000 | 2.675 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.436000 | 2.674 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.466000 | 2.673 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.446500 | 2.671 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.460500 | 2.67 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.436860 | 2.669 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 1.906 | Reg loss: 0.042 | Tree loss: 1.906 | Accuracy: 0.388000 | 2.669 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 1.926 | Reg loss: 0.042 | Tree loss: 1.926 | Accuracy: 0.374000 | 2.668 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.410000 | 2.667 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.399500 | 2.666 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 1.874 | Reg loss: 0.042 | Tree loss: 1.874 | Accuracy: 0.422000 | 2.665 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.435000 | 2.664 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.449500 | 2.663 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.445000 | 2.662 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.433000 | 2.661 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.458000 | 2.661 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 1.875 | Reg loss: 0.042 | Tree loss: 1.875 | Accuracy: 0.433447 | 2.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 1.934 | Reg loss: 0.042 | Tree loss: 1.934 | Accuracy: 0.364000 | 2.659 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 1.888 | Reg loss: 0.042 | Tree loss: 1.888 | Accuracy: 0.398000 | 2.658 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 1.887 | Reg loss: 0.042 | Tree loss: 1.887 | Accuracy: 0.394000 | 2.657 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.415000 | 2.656 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.437000 | 2.655 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.451000 | 2.654 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.446000 | 2.653 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.440500 | 2.652 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.467500 | 2.651 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.455500 | 2.65 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.457338 | 2.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 1.928 | Reg loss: 0.042 | Tree loss: 1.928 | Accuracy: 0.360000 | 2.65 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 1.905 | Reg loss: 0.042 | Tree loss: 1.905 | Accuracy: 0.380000 | 2.649 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.422000 | 2.648 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 1.877 | Reg loss: 0.042 | Tree loss: 1.877 | Accuracy: 0.396500 | 2.647 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.416500 | 2.646 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.431000 | 2.645 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.437500 | 2.644 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.453000 | 2.643 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 1.817 | Reg loss: 0.042 | Tree loss: 1.817 | Accuracy: 0.450000 | 2.642 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.445000 | 2.641 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.382253 | 2.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 1.952 | Reg loss: 0.042 | Tree loss: 1.952 | Accuracy: 0.365500 | 2.64 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 1.888 | Reg loss: 0.042 | Tree loss: 1.888 | Accuracy: 0.391500 | 2.639 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 1.883 | Reg loss: 0.042 | Tree loss: 1.883 | Accuracy: 0.401000 | 2.638 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 1.871 | Reg loss: 0.042 | Tree loss: 1.871 | Accuracy: 0.412500 | 2.637 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.438500 | 2.636 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.437000 | 2.635 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.455500 | 2.634 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.450000 | 2.633 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.449000 | 2.633 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.452000 | 2.632 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.464164 | 2.631 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 1.927 | Reg loss: 0.042 | Tree loss: 1.927 | Accuracy: 0.375000 | 2.631 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 1.908 | Reg loss: 0.042 | Tree loss: 1.908 | Accuracy: 0.386000 | 2.63 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 1.909 | Reg loss: 0.042 | Tree loss: 1.909 | Accuracy: 0.369000 | 2.629 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.412000 | 2.628 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 1.817 | Reg loss: 0.042 | Tree loss: 1.817 | Accuracy: 0.433500 | 2.627 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 1.839 | Reg loss: 0.042 | Tree loss: 1.839 | Accuracy: 0.416500 | 2.626 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.448000 | 2.626 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.472500 | 2.625 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 1.822 | Reg loss: 0.042 | Tree loss: 1.822 | Accuracy: 0.454000 | 2.624 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 1.820 | Reg loss: 0.042 | Tree loss: 1.820 | Accuracy: 0.441500 | 2.623 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.042 | Tree loss: 1.769 | Accuracy: 0.501706 | 2.622 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 1.912 | Reg loss: 0.042 | Tree loss: 1.912 | Accuracy: 0.385000 | 2.622 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.042 | Tree loss: 1.924 | Accuracy: 0.372500 | 2.621 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 1.882 | Reg loss: 0.042 | Tree loss: 1.882 | Accuracy: 0.398000 | 2.62 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.399500 | 2.619 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.404500 | 2.619 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.445500 | 2.618 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.457500 | 2.617 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.455000 | 2.616 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.464500 | 2.615 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.468000 | 2.614 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.447099 | 2.613 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 1.918 | Reg loss: 0.042 | Tree loss: 1.918 | Accuracy: 0.372000 | 2.614 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.042 | Tree loss: 1.924 | Accuracy: 0.364500 | 2.613 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 1.885 | Reg loss: 0.042 | Tree loss: 1.885 | Accuracy: 0.393000 | 2.612 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.402000 | 2.611 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.436500 | 2.61 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.455500 | 2.609 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.448000 | 2.608 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.476000 | 2.608 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.465000 | 2.607 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.453000 | 2.606 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.419795 | 2.605 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 1.911 | Reg loss: 0.042 | Tree loss: 1.911 | Accuracy: 0.379000 | 2.605 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 1.899 | Reg loss: 0.042 | Tree loss: 1.899 | Accuracy: 0.390000 | 2.604 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 1.901 | Reg loss: 0.042 | Tree loss: 1.901 | Accuracy: 0.369500 | 2.603 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.407500 | 2.603 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.443000 | 2.602 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.442500 | 2.601 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.442000 | 2.6 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.463500 | 2.599 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.472500 | 2.598 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.442000 | 2.598 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.450512 | 2.597 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 1.898 | Reg loss: 0.042 | Tree loss: 1.898 | Accuracy: 0.384500 | 2.596 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.042 | Tree loss: 1.898 | Accuracy: 0.391000 | 2.595 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.419000 | 2.595 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.417500 | 2.594 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.439000 | 2.593 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.444500 | 2.592 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.430000 | 2.591 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.453000 | 2.59 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.459000 | 2.589 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.458500 | 2.588 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 1.855 | Reg loss: 0.042 | Tree loss: 1.855 | Accuracy: 0.450512 | 2.588 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 1.908 | Reg loss: 0.042 | Tree loss: 1.908 | Accuracy: 0.371500 | 2.587 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.042 | Tree loss: 1.898 | Accuracy: 0.378000 | 2.587 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 1.903 | Reg loss: 0.042 | Tree loss: 1.903 | Accuracy: 0.371000 | 2.586 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.411000 | 2.585 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.429000 | 2.584 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 1.811 | Reg loss: 0.042 | Tree loss: 1.811 | Accuracy: 0.435000 | 2.583 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.452500 | 2.582 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 1.795 | Reg loss: 0.042 | Tree loss: 1.795 | Accuracy: 0.478000 | 2.582 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 1.805 | Reg loss: 0.042 | Tree loss: 1.805 | Accuracy: 0.463000 | 2.581 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.445000 | 2.58 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.460751 | 2.579 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.042 | Tree loss: 1.909 | Accuracy: 0.385000 | 2.579 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 1.930 | Reg loss: 0.042 | Tree loss: 1.930 | Accuracy: 0.389500 | 2.578 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 1.877 | Reg loss: 0.042 | Tree loss: 1.877 | Accuracy: 0.388500 | 2.578 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 1.855 | Reg loss: 0.042 | Tree loss: 1.855 | Accuracy: 0.410500 | 2.577 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.436000 | 2.576 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.451500 | 2.575 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.461500 | 2.574 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.449000 | 2.573 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.449500 | 2.572 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.461000 | 2.572 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.457338 | 2.571 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 1.910 | Reg loss: 0.042 | Tree loss: 1.910 | Accuracy: 0.392000 | 2.571 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 1.923 | Reg loss: 0.042 | Tree loss: 1.923 | Accuracy: 0.374000 | 2.57 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 1.874 | Reg loss: 0.042 | Tree loss: 1.874 | Accuracy: 0.389500 | 2.569 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 1.876 | Reg loss: 0.042 | Tree loss: 1.876 | Accuracy: 0.398500 | 2.568 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.433500 | 2.567 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.440500 | 2.566 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.441000 | 2.566 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.460500 | 2.565 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.450500 | 2.564 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.455500 | 2.563 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.532423 | 2.562 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 1.913 | Reg loss: 0.042 | Tree loss: 1.913 | Accuracy: 0.372500 | 2.562 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 1.907 | Reg loss: 0.042 | Tree loss: 1.907 | Accuracy: 0.384500 | 2.561 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.392000 | 2.56 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.405000 | 2.559 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.415500 | 2.558 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.451500 | 2.557 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.450000 | 2.557 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.469500 | 2.556 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.462000 | 2.555 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.448000 | 2.554 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.474403 | 2.553 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.042 | Tree loss: 1.909 | Accuracy: 0.378000 | 2.553 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 1.911 | Reg loss: 0.042 | Tree loss: 1.911 | Accuracy: 0.384000 | 2.552 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.403000 | 2.551 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.403500 | 2.55 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.441000 | 2.55 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.456500 | 2.549 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.453000 | 2.548 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.435500 | 2.547 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.452000 | 2.546 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.455000 | 2.546 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.447099 | 2.545 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.042 | Tree loss: 1.909 | Accuracy: 0.389000 | 2.545 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 1.912 | Reg loss: 0.042 | Tree loss: 1.912 | Accuracy: 0.369500 | 2.544 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.398500 | 2.543 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.428000 | 2.543 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.424500 | 2.542 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.455000 | 2.541 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.467500 | 2.54 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.448500 | 2.539 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.465500 | 2.539 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.457000 | 2.538 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.457338 | 2.537 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 1.935 | Reg loss: 0.042 | Tree loss: 1.935 | Accuracy: 0.371000 | 2.537 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 1.893 | Reg loss: 0.042 | Tree loss: 1.893 | Accuracy: 0.401000 | 2.536 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.398000 | 2.535 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.411500 | 2.535 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.440000 | 2.534 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.463000 | 2.533 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.455500 | 2.532 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.462500 | 2.531 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.447000 | 2.531 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.451500 | 2.53 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.430034 | 2.529 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 1.907 | Reg loss: 0.042 | Tree loss: 1.907 | Accuracy: 0.395000 | 2.529 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 1.892 | Reg loss: 0.042 | Tree loss: 1.892 | Accuracy: 0.383000 | 2.528 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 1.871 | Reg loss: 0.042 | Tree loss: 1.871 | Accuracy: 0.389000 | 2.527 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.421500 | 2.526 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.427500 | 2.526 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.457000 | 2.525 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.470500 | 2.524 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.455000 | 2.523 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.460000 | 2.523 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.469500 | 2.522 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.525597 | 2.521 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 1.899 | Reg loss: 0.043 | Tree loss: 1.899 | Accuracy: 0.394500 | 2.521 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 1.883 | Reg loss: 0.043 | Tree loss: 1.883 | Accuracy: 0.384500 | 2.52 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.415000 | 2.52 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.417500 | 2.519 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.436500 | 2.518 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.465000 | 2.517 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.454500 | 2.517 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.466000 | 2.516 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.441500 | 2.515 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.447000 | 2.514 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.443686 | 2.514 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 1.900 | Reg loss: 0.043 | Tree loss: 1.900 | Accuracy: 0.391000 | 2.514 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 1.886 | Reg loss: 0.043 | Tree loss: 1.886 | Accuracy: 0.383500 | 2.513 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.043 | Tree loss: 1.886 | Accuracy: 0.394500 | 2.512 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.419500 | 2.512 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.434000 | 2.511 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.458000 | 2.51 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.482000 | 2.509 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.450500 | 2.509 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 1.787 | Reg loss: 0.043 | Tree loss: 1.787 | Accuracy: 0.441500 | 2.508 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.444500 | 2.507 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.484642 | 2.506 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 1.906 | Reg loss: 0.043 | Tree loss: 1.906 | Accuracy: 0.389500 | 2.506 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 1.886 | Reg loss: 0.043 | Tree loss: 1.886 | Accuracy: 0.395000 | 2.506 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 1.875 | Reg loss: 0.043 | Tree loss: 1.875 | Accuracy: 0.389500 | 2.505 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.415000 | 2.504 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 1.825 | Reg loss: 0.043 | Tree loss: 1.825 | Accuracy: 0.429500 | 2.503 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.461000 | 2.503 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 1.812 | Reg loss: 0.043 | Tree loss: 1.812 | Accuracy: 0.451000 | 2.502 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.464500 | 2.501 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.448500 | 2.5 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.452000 | 2.5 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.443686 | 2.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 1.925 | Reg loss: 0.043 | Tree loss: 1.925 | Accuracy: 0.380500 | 2.499 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 1.875 | Reg loss: 0.043 | Tree loss: 1.875 | Accuracy: 0.395500 | 2.498 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.043 | Tree loss: 1.850 | Accuracy: 0.414500 | 2.497 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.424000 | 2.497 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 1.840 | Reg loss: 0.043 | Tree loss: 1.840 | Accuracy: 0.427500 | 2.496 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 1.817 | Reg loss: 0.043 | Tree loss: 1.817 | Accuracy: 0.445000 | 2.495 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.448000 | 2.494 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 1.767 | Reg loss: 0.043 | Tree loss: 1.767 | Accuracy: 0.458500 | 2.494 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.465000 | 2.493 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.459000 | 2.492 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.460751 | 2.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 1.916 | Reg loss: 0.043 | Tree loss: 1.916 | Accuracy: 0.393000 | 2.491 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.404000 | 2.49 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 1.869 | Reg loss: 0.043 | Tree loss: 1.869 | Accuracy: 0.395500 | 2.49 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 1.823 | Reg loss: 0.043 | Tree loss: 1.823 | Accuracy: 0.430000 | 2.489 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 1.811 | Reg loss: 0.043 | Tree loss: 1.811 | Accuracy: 0.437000 | 2.488 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.460500 | 2.487 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.469500 | 2.487 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 1.809 | Reg loss: 0.043 | Tree loss: 1.809 | Accuracy: 0.461000 | 2.486 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 1.812 | Reg loss: 0.043 | Tree loss: 1.812 | Accuracy: 0.437500 | 2.485 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.450000 | 2.484 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.470990 | 2.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 1.902 | Reg loss: 0.043 | Tree loss: 1.902 | Accuracy: 0.398000 | 2.483 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 1.890 | Reg loss: 0.043 | Tree loss: 1.890 | Accuracy: 0.376500 | 2.483 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 1.854 | Reg loss: 0.043 | Tree loss: 1.854 | Accuracy: 0.392500 | 2.482 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.424000 | 2.481 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.433500 | 2.48 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.463000 | 2.48 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.469000 | 2.479 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.480500 | 2.478 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.439500 | 2.477 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.449000 | 2.477 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.430034 | 2.476 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 1.898 | Reg loss: 0.043 | Tree loss: 1.898 | Accuracy: 0.393500 | 2.476 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 1.886 | Reg loss: 0.043 | Tree loss: 1.886 | Accuracy: 0.381000 | 2.475 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.424000 | 2.474 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.412500 | 2.474 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.429500 | 2.473 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.461500 | 2.472 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.487000 | 2.471 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.458500 | 2.471 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.463000 | 2.47 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.467500 | 2.469 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.498294 | 2.468 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 1.905 | Reg loss: 0.043 | Tree loss: 1.905 | Accuracy: 0.400000 | 2.468 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 1.880 | Reg loss: 0.043 | Tree loss: 1.880 | Accuracy: 0.384000 | 2.467 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 1.863 | Reg loss: 0.043 | Tree loss: 1.863 | Accuracy: 0.396500 | 2.467 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.442000 | 2.466 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.445000 | 2.465 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.458000 | 2.464 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.438000 | 2.463 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.470000 | 2.463 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.466500 | 2.462 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.447000 | 2.461 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.443686 | 2.46 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 1.914 | Reg loss: 0.043 | Tree loss: 1.914 | Accuracy: 0.379500 | 2.46 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.043 | Tree loss: 1.845 | Accuracy: 0.403500 | 2.459 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 1.852 | Reg loss: 0.043 | Tree loss: 1.852 | Accuracy: 0.412500 | 2.459 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 1.845 | Reg loss: 0.043 | Tree loss: 1.845 | Accuracy: 0.427000 | 2.458 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 1.808 | Reg loss: 0.043 | Tree loss: 1.808 | Accuracy: 0.459000 | 2.457 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.465000 | 2.456 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.457000 | 2.456 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.478000 | 2.455 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.479500 | 2.454 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.441000 | 2.453 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.447099 | 2.453 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 1.881 | Reg loss: 0.043 | Tree loss: 1.881 | Accuracy: 0.401500 | 2.452 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 1.882 | Reg loss: 0.043 | Tree loss: 1.882 | Accuracy: 0.397000 | 2.451 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.400500 | 2.451 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.043 | Tree loss: 1.825 | Accuracy: 0.415500 | 2.45 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.411500 | 2.449 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.465000 | 2.448 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.450000 | 2.448 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.455000 | 2.447 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.453500 | 2.446 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.451500 | 2.445 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.488055 | 2.445 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.043 | Tree loss: 1.882 | Accuracy: 0.405500 | 2.445 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.391000 | 2.444 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.407000 | 2.443 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.424000 | 2.442 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.456500 | 2.442 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.447500 | 2.441 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.458500 | 2.44 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.466000 | 2.439 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.457500 | 2.439 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.466500 | 2.438 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.477816 | 2.437 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 1.897 | Reg loss: 0.043 | Tree loss: 1.897 | Accuracy: 0.407500 | 2.437 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 1.871 | Reg loss: 0.043 | Tree loss: 1.871 | Accuracy: 0.391000 | 2.437 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 1.871 | Reg loss: 0.043 | Tree loss: 1.871 | Accuracy: 0.403500 | 2.436 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.429500 | 2.435 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.437000 | 2.434 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.441500 | 2.434 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.463500 | 2.433 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.453000 | 2.432 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.441000 | 2.431 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.472000 | 2.431 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.484642 | 2.43 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 1.894 | Reg loss: 0.043 | Tree loss: 1.894 | Accuracy: 0.397000 | 2.43 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 1.873 | Reg loss: 0.043 | Tree loss: 1.873 | Accuracy: 0.421000 | 2.429 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 1.867 | Reg loss: 0.043 | Tree loss: 1.867 | Accuracy: 0.399500 | 2.428 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 1.822 | Reg loss: 0.043 | Tree loss: 1.822 | Accuracy: 0.420500 | 2.427 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.460000 | 2.427 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.465000 | 2.426 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 1.770 | Reg loss: 0.043 | Tree loss: 1.770 | Accuracy: 0.465000 | 2.425 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.468000 | 2.425 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.455500 | 2.424 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.457500 | 2.423 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.474403 | 2.423 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 1.897 | Reg loss: 0.043 | Tree loss: 1.897 | Accuracy: 0.384500 | 2.423 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 1.878 | Reg loss: 0.043 | Tree loss: 1.878 | Accuracy: 0.378500 | 2.422 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.405000 | 2.421 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.428000 | 2.421 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 1.817 | Reg loss: 0.043 | Tree loss: 1.817 | Accuracy: 0.442000 | 2.42 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.467500 | 2.419 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 1.808 | Reg loss: 0.043 | Tree loss: 1.808 | Accuracy: 0.451000 | 2.419 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.481500 | 2.418 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.467500 | 2.417 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.463000 | 2.417 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.457338 | 2.416 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 1.911 | Reg loss: 0.043 | Tree loss: 1.911 | Accuracy: 0.381000 | 2.416 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.400500 | 2.415 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 1.867 | Reg loss: 0.043 | Tree loss: 1.867 | Accuracy: 0.398500 | 2.414 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.409000 | 2.414 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.467000 | 2.413 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.443500 | 2.412 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.496500 | 2.412 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.462000 | 2.411 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 1.773 | Reg loss: 0.043 | Tree loss: 1.773 | Accuracy: 0.455000 | 2.41 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.461000 | 2.41 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.477816 | 2.409 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.043 | Tree loss: 1.878 | Accuracy: 0.411000 | 2.409 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 1.896 | Reg loss: 0.043 | Tree loss: 1.896 | Accuracy: 0.396000 | 2.408 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.391000 | 2.408 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.436500 | 2.407 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.423500 | 2.406 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.452500 | 2.406 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.459500 | 2.405 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.452500 | 2.404 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.463500 | 2.404 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.451500 | 2.403 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.481229 | 2.403 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 1.899 | Reg loss: 0.043 | Tree loss: 1.899 | Accuracy: 0.388500 | 2.403 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.410500 | 2.402 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.405000 | 2.401 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.426500 | 2.401 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.434500 | 2.4 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 1.775 | Reg loss: 0.043 | Tree loss: 1.775 | Accuracy: 0.466500 | 2.399 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.496000 | 2.398 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.466000 | 2.398 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.461000 | 2.397 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 1.773 | Reg loss: 0.043 | Tree loss: 1.773 | Accuracy: 0.453500 | 2.396 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.460751 | 2.396 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 1.899 | Reg loss: 0.043 | Tree loss: 1.899 | Accuracy: 0.384000 | 2.396 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.396500 | 2.395 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.417500 | 2.394 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.415500 | 2.394 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.447500 | 2.393 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 1.787 | Reg loss: 0.043 | Tree loss: 1.787 | Accuracy: 0.463500 | 2.392 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.457500 | 2.392 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.483000 | 2.391 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 1.767 | Reg loss: 0.043 | Tree loss: 1.767 | Accuracy: 0.467500 | 2.39 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.469000 | 2.389 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 1.775 | Reg loss: 0.044 | Tree loss: 1.775 | Accuracy: 0.460751 | 2.389 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.043 | Tree loss: 1.882 | Accuracy: 0.394000 | 2.389 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 1.862 | Reg loss: 0.043 | Tree loss: 1.862 | Accuracy: 0.410000 | 2.388 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 1.852 | Reg loss: 0.043 | Tree loss: 1.852 | Accuracy: 0.395500 | 2.387 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.423000 | 2.386 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.458500 | 2.386 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.472000 | 2.385 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.465000 | 2.384 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 1.765 | Reg loss: 0.044 | Tree loss: 1.765 | Accuracy: 0.470000 | 2.384 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.471000 | 2.383 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 1.771 | Reg loss: 0.044 | Tree loss: 1.771 | Accuracy: 0.474500 | 2.382 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.044 | Tree loss: 1.737 | Accuracy: 0.440273 | 2.382 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.043 | Tree loss: 1.878 | Accuracy: 0.383500 | 2.382 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 1.889 | Reg loss: 0.043 | Tree loss: 1.889 | Accuracy: 0.388500 | 2.381 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.043 | Tree loss: 1.850 | Accuracy: 0.406000 | 2.38 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.437000 | 2.38 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 1.817 | Reg loss: 0.043 | Tree loss: 1.817 | Accuracy: 0.435000 | 2.379 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.044 | Tree loss: 1.756 | Accuracy: 0.473500 | 2.379 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 1.777 | Reg loss: 0.044 | Tree loss: 1.777 | Accuracy: 0.468000 | 2.378 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.476500 | 2.377 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 1.783 | Reg loss: 0.044 | Tree loss: 1.783 | Accuracy: 0.487500 | 2.377 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 1.782 | Reg loss: 0.044 | Tree loss: 1.782 | Accuracy: 0.466000 | 2.376 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 1.815 | Reg loss: 0.044 | Tree loss: 1.815 | Accuracy: 0.426621 | 2.375 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 1.923 | Reg loss: 0.043 | Tree loss: 1.923 | Accuracy: 0.381000 | 2.375 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 1.873 | Reg loss: 0.043 | Tree loss: 1.873 | Accuracy: 0.401000 | 2.375 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 1.861 | Reg loss: 0.043 | Tree loss: 1.861 | Accuracy: 0.393500 | 2.374 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 1.817 | Reg loss: 0.044 | Tree loss: 1.817 | Accuracy: 0.433000 | 2.374 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.044 | Tree loss: 1.804 | Accuracy: 0.458500 | 2.373 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 1.778 | Reg loss: 0.044 | Tree loss: 1.778 | Accuracy: 0.469000 | 2.372 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 1.775 | Reg loss: 0.044 | Tree loss: 1.775 | Accuracy: 0.470000 | 2.372 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.449500 | 2.371 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 1.784 | Reg loss: 0.044 | Tree loss: 1.784 | Accuracy: 0.443000 | 2.37 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.044 | Tree loss: 1.732 | Accuracy: 0.475500 | 2.37 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 1.778 | Reg loss: 0.044 | Tree loss: 1.778 | Accuracy: 0.484642 | 2.369 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.044 | Tree loss: 1.866 | Accuracy: 0.419500 | 2.369 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 1.878 | Reg loss: 0.044 | Tree loss: 1.878 | Accuracy: 0.391000 | 2.368 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 1.846 | Reg loss: 0.044 | Tree loss: 1.846 | Accuracy: 0.418500 | 2.367 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 1.819 | Reg loss: 0.044 | Tree loss: 1.819 | Accuracy: 0.410000 | 2.367 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.044 | Tree loss: 1.804 | Accuracy: 0.433000 | 2.366 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 1.796 | Reg loss: 0.044 | Tree loss: 1.796 | Accuracy: 0.444500 | 2.366 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 1.782 | Reg loss: 0.044 | Tree loss: 1.782 | Accuracy: 0.473000 | 2.365 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 1.784 | Reg loss: 0.044 | Tree loss: 1.784 | Accuracy: 0.474500 | 2.364 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.482000 | 2.364 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 1.763 | Reg loss: 0.044 | Tree loss: 1.763 | Accuracy: 0.467500 | 2.363 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.044 | Tree loss: 1.724 | Accuracy: 0.501706 | 2.363 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.044 | Tree loss: 1.865 | Accuracy: 0.403000 | 2.363 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 1.882 | Reg loss: 0.044 | Tree loss: 1.882 | Accuracy: 0.397500 | 2.362 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 1.829 | Reg loss: 0.044 | Tree loss: 1.829 | Accuracy: 0.413000 | 2.361 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 1.818 | Reg loss: 0.044 | Tree loss: 1.818 | Accuracy: 0.422500 | 2.361 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.467500 | 2.36 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 1.769 | Reg loss: 0.044 | Tree loss: 1.769 | Accuracy: 0.474000 | 2.36 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 1.798 | Reg loss: 0.044 | Tree loss: 1.798 | Accuracy: 0.455500 | 2.359 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 1.761 | Reg loss: 0.044 | Tree loss: 1.761 | Accuracy: 0.470500 | 2.358 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 1.784 | Reg loss: 0.044 | Tree loss: 1.784 | Accuracy: 0.477000 | 2.358 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 1.773 | Reg loss: 0.044 | Tree loss: 1.773 | Accuracy: 0.453500 | 2.357 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 1.829 | Reg loss: 0.044 | Tree loss: 1.829 | Accuracy: 0.443686 | 2.356 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.044 | Tree loss: 1.865 | Accuracy: 0.420500 | 2.356 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.044 | Tree loss: 1.834 | Accuracy: 0.406500 | 2.356 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 1.849 | Reg loss: 0.044 | Tree loss: 1.849 | Accuracy: 0.420000 | 2.355 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 1.826 | Reg loss: 0.044 | Tree loss: 1.826 | Accuracy: 0.414000 | 2.355 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 1.809 | Reg loss: 0.044 | Tree loss: 1.809 | Accuracy: 0.433500 | 2.354 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 1.810 | Reg loss: 0.044 | Tree loss: 1.810 | Accuracy: 0.426000 | 2.353 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.472000 | 2.353 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 1.786 | Reg loss: 0.044 | Tree loss: 1.786 | Accuracy: 0.473000 | 2.352 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 1.756 | Reg loss: 0.044 | Tree loss: 1.756 | Accuracy: 0.470500 | 2.352 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 1.780 | Reg loss: 0.044 | Tree loss: 1.780 | Accuracy: 0.466000 | 2.351 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 1.807 | Reg loss: 0.044 | Tree loss: 1.807 | Accuracy: 0.423208 | 2.351 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 1.903 | Reg loss: 0.044 | Tree loss: 1.903 | Accuracy: 0.392500 | 2.351 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 1.880 | Reg loss: 0.044 | Tree loss: 1.880 | Accuracy: 0.394000 | 2.35 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 1.849 | Reg loss: 0.044 | Tree loss: 1.849 | Accuracy: 0.423000 | 2.35 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.044 | Tree loss: 1.798 | Accuracy: 0.423500 | 2.349 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 1.798 | Reg loss: 0.044 | Tree loss: 1.798 | Accuracy: 0.451000 | 2.348 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.472500 | 2.348 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.468500 | 2.347 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 1.768 | Reg loss: 0.044 | Tree loss: 1.768 | Accuracy: 0.478000 | 2.347 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 1.789 | Reg loss: 0.044 | Tree loss: 1.789 | Accuracy: 0.452500 | 2.346 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 1.753 | Reg loss: 0.044 | Tree loss: 1.753 | Accuracy: 0.463500 | 2.345 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.044 | Tree loss: 1.760 | Accuracy: 0.488055 | 2.345 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 1.890 | Reg loss: 0.044 | Tree loss: 1.890 | Accuracy: 0.388500 | 2.345 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.044 | Tree loss: 1.847 | Accuracy: 0.404500 | 2.344 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.410000 | 2.343 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 1.819 | Reg loss: 0.044 | Tree loss: 1.819 | Accuracy: 0.419500 | 2.343 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 1.814 | Reg loss: 0.044 | Tree loss: 1.814 | Accuracy: 0.436500 | 2.342 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 1.786 | Reg loss: 0.044 | Tree loss: 1.786 | Accuracy: 0.459000 | 2.342 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.489500 | 2.341 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.044 | Tree loss: 1.750 | Accuracy: 0.475500 | 2.34 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 1.774 | Reg loss: 0.044 | Tree loss: 1.774 | Accuracy: 0.470000 | 2.34 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 1.774 | Reg loss: 0.044 | Tree loss: 1.774 | Accuracy: 0.474500 | 2.339 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 1.762 | Reg loss: 0.044 | Tree loss: 1.762 | Accuracy: 0.433447 | 2.339 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.044 | Tree loss: 1.882 | Accuracy: 0.384000 | 2.339 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.044 | Tree loss: 1.874 | Accuracy: 0.407500 | 2.338 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 1.841 | Reg loss: 0.044 | Tree loss: 1.841 | Accuracy: 0.402000 | 2.338 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.044 | Tree loss: 1.809 | Accuracy: 0.430000 | 2.337 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.474000 | 2.336 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.459000 | 2.336 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 1.778 | Reg loss: 0.044 | Tree loss: 1.778 | Accuracy: 0.457000 | 2.335 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 1.773 | Reg loss: 0.044 | Tree loss: 1.773 | Accuracy: 0.475000 | 2.335 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 1.778 | Reg loss: 0.044 | Tree loss: 1.778 | Accuracy: 0.459500 | 2.334 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 1.771 | Reg loss: 0.044 | Tree loss: 1.771 | Accuracy: 0.461500 | 2.334 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.470990 | 2.333 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 1.894 | Reg loss: 0.044 | Tree loss: 1.894 | Accuracy: 0.377500 | 2.333 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.044 | Tree loss: 1.856 | Accuracy: 0.412000 | 2.333 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 1.837 | Reg loss: 0.044 | Tree loss: 1.837 | Accuracy: 0.418000 | 2.332 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.044 | Tree loss: 1.805 | Accuracy: 0.429500 | 2.332 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 1.790 | Reg loss: 0.044 | Tree loss: 1.790 | Accuracy: 0.443000 | 2.331 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 1.785 | Reg loss: 0.044 | Tree loss: 1.785 | Accuracy: 0.471000 | 2.33 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 1.795 | Reg loss: 0.044 | Tree loss: 1.795 | Accuracy: 0.452500 | 2.33 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 1.745 | Reg loss: 0.044 | Tree loss: 1.745 | Accuracy: 0.492500 | 2.329 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 1.769 | Reg loss: 0.044 | Tree loss: 1.769 | Accuracy: 0.479500 | 2.329 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.454500 | 2.328 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.044 | Tree loss: 1.728 | Accuracy: 0.470990 | 2.327 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.044 | Tree loss: 1.871 | Accuracy: 0.392000 | 2.328 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 1.855 | Reg loss: 0.044 | Tree loss: 1.855 | Accuracy: 0.414500 | 2.327 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.409000 | 2.326 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.044 | Tree loss: 1.808 | Accuracy: 0.420000 | 2.326 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 1.818 | Reg loss: 0.044 | Tree loss: 1.818 | Accuracy: 0.445000 | 2.325 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 1.788 | Reg loss: 0.044 | Tree loss: 1.788 | Accuracy: 0.455000 | 2.325 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.496000 | 2.324 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 1.754 | Reg loss: 0.044 | Tree loss: 1.754 | Accuracy: 0.480500 | 2.323 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 1.784 | Reg loss: 0.044 | Tree loss: 1.784 | Accuracy: 0.461500 | 2.323 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.044 | Tree loss: 1.749 | Accuracy: 0.464500 | 2.322 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 1.753 | Reg loss: 0.044 | Tree loss: 1.753 | Accuracy: 0.474403 | 2.322 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.044 | Tree loss: 1.865 | Accuracy: 0.416000 | 2.322 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 1.862 | Reg loss: 0.044 | Tree loss: 1.862 | Accuracy: 0.395000 | 2.321 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 1.835 | Reg loss: 0.044 | Tree loss: 1.835 | Accuracy: 0.392500 | 2.321 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 1.807 | Reg loss: 0.044 | Tree loss: 1.807 | Accuracy: 0.435000 | 2.32 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.044 | Tree loss: 1.801 | Accuracy: 0.444500 | 2.32 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.044 | Tree loss: 1.748 | Accuracy: 0.487000 | 2.319 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 1.796 | Reg loss: 0.044 | Tree loss: 1.796 | Accuracy: 0.456500 | 2.318 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 1.774 | Reg loss: 0.044 | Tree loss: 1.774 | Accuracy: 0.459000 | 2.318 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 1.764 | Reg loss: 0.044 | Tree loss: 1.764 | Accuracy: 0.455000 | 2.317 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 1.789 | Reg loss: 0.044 | Tree loss: 1.789 | Accuracy: 0.470500 | 2.317 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 1.745 | Reg loss: 0.044 | Tree loss: 1.745 | Accuracy: 0.508532 | 2.316 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.044 | Tree loss: 1.875 | Accuracy: 0.394000 | 2.316 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.044 | Tree loss: 1.834 | Accuracy: 0.408500 | 2.316 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.044 | Tree loss: 1.850 | Accuracy: 0.402500 | 2.315 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.395500 | 2.314 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.435500 | 2.314 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.473500 | 2.313 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 1.777 | Reg loss: 0.044 | Tree loss: 1.777 | Accuracy: 0.497500 | 2.313 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.044 | Tree loss: 1.757 | Accuracy: 0.480000 | 2.312 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.470500 | 2.312 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 1.752 | Reg loss: 0.044 | Tree loss: 1.752 | Accuracy: 0.487500 | 2.311 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.044 | Tree loss: 1.681 | Accuracy: 0.518771 | 2.31 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.044 | Tree loss: 1.875 | Accuracy: 0.410000 | 2.311 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 1.859 | Reg loss: 0.044 | Tree loss: 1.859 | Accuracy: 0.407000 | 2.31 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 1.855 | Reg loss: 0.044 | Tree loss: 1.855 | Accuracy: 0.395000 | 2.309 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.044 | Tree loss: 1.806 | Accuracy: 0.436000 | 2.309 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 1.798 | Reg loss: 0.044 | Tree loss: 1.798 | Accuracy: 0.437500 | 2.308 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 1.775 | Reg loss: 0.044 | Tree loss: 1.775 | Accuracy: 0.469000 | 2.308 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.469000 | 2.307 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.486500 | 2.307 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.450500 | 2.306 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 1.757 | Reg loss: 0.044 | Tree loss: 1.757 | Accuracy: 0.484000 | 2.306 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.481229 | 2.305 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 1.888 | Reg loss: 0.044 | Tree loss: 1.888 | Accuracy: 0.387500 | 2.305 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 1.857 | Reg loss: 0.044 | Tree loss: 1.857 | Accuracy: 0.399500 | 2.305 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.044 | Tree loss: 1.815 | Accuracy: 0.421500 | 2.304 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.044 | Tree loss: 1.797 | Accuracy: 0.446000 | 2.303 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.452000 | 2.303 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 1.801 | Reg loss: 0.044 | Tree loss: 1.801 | Accuracy: 0.439000 | 2.302 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.044 | Tree loss: 1.771 | Accuracy: 0.484500 | 2.302 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.044 | Tree loss: 1.806 | Accuracy: 0.453000 | 2.301 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 1.749 | Reg loss: 0.044 | Tree loss: 1.749 | Accuracy: 0.480000 | 2.301 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.463500 | 2.3 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 1.751 | Reg loss: 0.044 | Tree loss: 1.751 | Accuracy: 0.474403 | 2.299 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.044 | Tree loss: 1.872 | Accuracy: 0.393500 | 2.3 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.406000 | 2.299 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.044 | Tree loss: 1.817 | Accuracy: 0.435000 | 2.298 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 1.811 | Reg loss: 0.044 | Tree loss: 1.811 | Accuracy: 0.421000 | 2.298 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.454000 | 2.297 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.044 | Tree loss: 1.761 | Accuracy: 0.481000 | 2.297 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 1.796 | Reg loss: 0.044 | Tree loss: 1.796 | Accuracy: 0.461500 | 2.296 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.476500 | 2.296 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 1.782 | Reg loss: 0.044 | Tree loss: 1.782 | Accuracy: 0.468000 | 2.295 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 1.761 | Reg loss: 0.044 | Tree loss: 1.761 | Accuracy: 0.464000 | 2.294 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.044 | Tree loss: 1.757 | Accuracy: 0.477816 | 2.294 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.044 | Tree loss: 1.869 | Accuracy: 0.407500 | 2.294 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 1.869 | Reg loss: 0.044 | Tree loss: 1.869 | Accuracy: 0.386000 | 2.293 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.044 | Tree loss: 1.827 | Accuracy: 0.414500 | 2.293 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.044 | Tree loss: 1.805 | Accuracy: 0.445000 | 2.292 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.044 | Tree loss: 1.801 | Accuracy: 0.452000 | 2.291 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.044 | Tree loss: 1.761 | Accuracy: 0.471500 | 2.291 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.487500 | 2.29 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.464000 | 2.29 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 1.763 | Reg loss: 0.044 | Tree loss: 1.763 | Accuracy: 0.468500 | 2.289 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 1.761 | Reg loss: 0.044 | Tree loss: 1.761 | Accuracy: 0.460500 | 2.288 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 1.780 | Reg loss: 0.044 | Tree loss: 1.780 | Accuracy: 0.474403 | 2.288 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 1.893 | Reg loss: 0.044 | Tree loss: 1.893 | Accuracy: 0.386500 | 2.288 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 1.869 | Reg loss: 0.044 | Tree loss: 1.869 | Accuracy: 0.406000 | 2.287 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 1.822 | Reg loss: 0.044 | Tree loss: 1.822 | Accuracy: 0.421500 | 2.287 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.044 | Tree loss: 1.814 | Accuracy: 0.413000 | 2.286 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 1.781 | Reg loss: 0.044 | Tree loss: 1.781 | Accuracy: 0.448000 | 2.285 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 1.799 | Reg loss: 0.044 | Tree loss: 1.799 | Accuracy: 0.452500 | 2.285 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.044 | Tree loss: 1.771 | Accuracy: 0.490500 | 2.284 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.044 | Tree loss: 1.750 | Accuracy: 0.480500 | 2.284 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.044 | Tree loss: 1.754 | Accuracy: 0.462500 | 2.283 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.044 | Tree loss: 1.735 | Accuracy: 0.480500 | 2.282 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.436860 | 2.282 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.044 | Tree loss: 1.854 | Accuracy: 0.405000 | 2.282 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.044 | Tree loss: 1.844 | Accuracy: 0.397000 | 2.281 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 1.845 | Reg loss: 0.044 | Tree loss: 1.845 | Accuracy: 0.412500 | 2.281 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 1.823 | Reg loss: 0.044 | Tree loss: 1.823 | Accuracy: 0.421000 | 2.28 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.044 | Tree loss: 1.782 | Accuracy: 0.453500 | 2.28 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.457500 | 2.279 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.469500 | 2.278 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 1.747 | Reg loss: 0.044 | Tree loss: 1.747 | Accuracy: 0.479500 | 2.278 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.482500 | 2.277 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 1.768 | Reg loss: 0.044 | Tree loss: 1.768 | Accuracy: 0.473000 | 2.277 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.467577 | 2.276 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.044 | Tree loss: 1.857 | Accuracy: 0.408500 | 2.276 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.044 | Tree loss: 1.863 | Accuracy: 0.397000 | 2.276 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 1.834 | Reg loss: 0.044 | Tree loss: 1.834 | Accuracy: 0.408000 | 2.275 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.044 | Tree loss: 1.797 | Accuracy: 0.427000 | 2.274 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.044 | Tree loss: 1.816 | Accuracy: 0.434000 | 2.274 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.044 | Tree loss: 1.765 | Accuracy: 0.476000 | 2.273 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 1.747 | Reg loss: 0.044 | Tree loss: 1.747 | Accuracy: 0.500500 | 2.273 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.474000 | 2.272 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 1.749 | Reg loss: 0.044 | Tree loss: 1.749 | Accuracy: 0.475000 | 2.272 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 1.773 | Reg loss: 0.044 | Tree loss: 1.773 | Accuracy: 0.457500 | 2.271 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.494881 | 2.27 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.044 | Tree loss: 1.873 | Accuracy: 0.396000 | 2.27 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 1.851 | Reg loss: 0.044 | Tree loss: 1.851 | Accuracy: 0.424500 | 2.27 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 1.840 | Reg loss: 0.044 | Tree loss: 1.840 | Accuracy: 0.398000 | 2.269 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.044 | Tree loss: 1.805 | Accuracy: 0.433000 | 2.269 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.044 | Tree loss: 1.768 | Accuracy: 0.448500 | 2.268 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 1.788 | Reg loss: 0.044 | Tree loss: 1.788 | Accuracy: 0.455000 | 2.267 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.490000 | 2.267 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.473500 | 2.266 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.044 | Tree loss: 1.751 | Accuracy: 0.477000 | 2.266 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 1.742 | Reg loss: 0.044 | Tree loss: 1.742 | Accuracy: 0.474500 | 2.265 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.470990 | 2.265 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.044 | Tree loss: 1.857 | Accuracy: 0.407000 | 2.265 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 1.862 | Reg loss: 0.044 | Tree loss: 1.862 | Accuracy: 0.388500 | 2.264 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.044 | Tree loss: 1.808 | Accuracy: 0.424500 | 2.263 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 1.813 | Reg loss: 0.044 | Tree loss: 1.813 | Accuracy: 0.435000 | 2.263 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.452000 | 2.262 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 1.803 | Reg loss: 0.044 | Tree loss: 1.803 | Accuracy: 0.469500 | 2.262 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 1.770 | Reg loss: 0.044 | Tree loss: 1.770 | Accuracy: 0.490000 | 2.262 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 1.756 | Reg loss: 0.044 | Tree loss: 1.756 | Accuracy: 0.465500 | 2.261 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 1.750 | Reg loss: 0.044 | Tree loss: 1.750 | Accuracy: 0.471000 | 2.261 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.469000 | 2.26 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 1.779 | Reg loss: 0.044 | Tree loss: 1.779 | Accuracy: 0.484642 | 2.26 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.044 | Tree loss: 1.862 | Accuracy: 0.408000 | 2.26 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 1.855 | Reg loss: 0.044 | Tree loss: 1.855 | Accuracy: 0.395000 | 2.259 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.413000 | 2.259 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.044 | Tree loss: 1.782 | Accuracy: 0.424500 | 2.258 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.451500 | 2.258 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 1.787 | Reg loss: 0.044 | Tree loss: 1.787 | Accuracy: 0.468000 | 2.258 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 1.788 | Reg loss: 0.044 | Tree loss: 1.788 | Accuracy: 0.467500 | 2.257 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.044 | Tree loss: 1.757 | Accuracy: 0.474000 | 2.257 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.480500 | 2.256 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 1.769 | Reg loss: 0.044 | Tree loss: 1.769 | Accuracy: 0.452000 | 2.256 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 1.741 | Reg loss: 0.044 | Tree loss: 1.741 | Accuracy: 0.453925 | 2.255 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 1.886 | Reg loss: 0.044 | Tree loss: 1.886 | Accuracy: 0.389000 | 2.255 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.044 | Tree loss: 1.801 | Accuracy: 0.417000 | 2.255 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 1.834 | Reg loss: 0.044 | Tree loss: 1.834 | Accuracy: 0.410500 | 2.254 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.044 | Tree loss: 1.803 | Accuracy: 0.432000 | 2.254 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 1.800 | Reg loss: 0.044 | Tree loss: 1.800 | Accuracy: 0.445000 | 2.253 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 1.774 | Reg loss: 0.044 | Tree loss: 1.774 | Accuracy: 0.475000 | 2.253 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.044 | Tree loss: 1.753 | Accuracy: 0.471000 | 2.252 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 1.796 | Reg loss: 0.044 | Tree loss: 1.796 | Accuracy: 0.440000 | 2.251 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.044 | Tree loss: 1.754 | Accuracy: 0.484000 | 2.251 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 1.743 | Reg loss: 0.044 | Tree loss: 1.743 | Accuracy: 0.486500 | 2.25 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.457338 | 2.25 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.044 | Tree loss: 1.867 | Accuracy: 0.401000 | 2.25 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.399000 | 2.249 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 1.836 | Reg loss: 0.044 | Tree loss: 1.836 | Accuracy: 0.409500 | 2.249 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 1.821 | Reg loss: 0.044 | Tree loss: 1.821 | Accuracy: 0.432500 | 2.248 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.449500 | 2.248 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.044 | Tree loss: 1.758 | Accuracy: 0.485000 | 2.247 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.474000 | 2.246 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.044 | Tree loss: 1.750 | Accuracy: 0.477000 | 2.246 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.044 | Tree loss: 1.729 | Accuracy: 0.492000 | 2.245 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 1.788 | Reg loss: 0.044 | Tree loss: 1.788 | Accuracy: 0.450000 | 2.245 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.044 | Tree loss: 1.760 | Accuracy: 0.460751 | 2.244 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.044 | Tree loss: 1.847 | Accuracy: 0.397500 | 2.244 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 1.853 | Reg loss: 0.044 | Tree loss: 1.853 | Accuracy: 0.397000 | 2.244 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.044 | Tree loss: 1.820 | Accuracy: 0.424000 | 2.243 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.044 | Tree loss: 1.797 | Accuracy: 0.440000 | 2.243 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.044 | Tree loss: 1.776 | Accuracy: 0.455000 | 2.242 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 1.772 | Reg loss: 0.044 | Tree loss: 1.772 | Accuracy: 0.474000 | 2.242 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 1.786 | Reg loss: 0.044 | Tree loss: 1.786 | Accuracy: 0.464500 | 2.241 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 1.758 | Reg loss: 0.044 | Tree loss: 1.758 | Accuracy: 0.476500 | 2.241 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 1.760 | Reg loss: 0.044 | Tree loss: 1.760 | Accuracy: 0.469500 | 2.241 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 1.753 | Reg loss: 0.045 | Tree loss: 1.753 | Accuracy: 0.481500 | 2.24 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.045 | Tree loss: 1.769 | Accuracy: 0.447099 | 2.24 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 1.900 | Reg loss: 0.044 | Tree loss: 1.900 | Accuracy: 0.370500 | 2.24 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.044 | Tree loss: 1.823 | Accuracy: 0.425500 | 2.239 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 1.839 | Reg loss: 0.044 | Tree loss: 1.839 | Accuracy: 0.400500 | 2.239 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 1.819 | Reg loss: 0.044 | Tree loss: 1.819 | Accuracy: 0.423500 | 2.238 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 1.808 | Reg loss: 0.044 | Tree loss: 1.808 | Accuracy: 0.458000 | 2.238 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 1.767 | Reg loss: 0.044 | Tree loss: 1.767 | Accuracy: 0.469000 | 2.237 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 1.759 | Reg loss: 0.044 | Tree loss: 1.759 | Accuracy: 0.497000 | 2.237 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 1.768 | Reg loss: 0.045 | Tree loss: 1.768 | Accuracy: 0.481500 | 2.236 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.045 | Tree loss: 1.716 | Accuracy: 0.512000 | 2.236 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.045 | Tree loss: 1.732 | Accuracy: 0.483000 | 2.235 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.419795 | 2.235 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.044 | Tree loss: 1.865 | Accuracy: 0.406000 | 2.235 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.044 | Tree loss: 1.856 | Accuracy: 0.393500 | 2.235 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.044 | Tree loss: 1.815 | Accuracy: 0.423500 | 2.234 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 1.801 | Reg loss: 0.044 | Tree loss: 1.801 | Accuracy: 0.438500 | 2.234 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.044 | Tree loss: 1.789 | Accuracy: 0.437500 | 2.233 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.044 | Tree loss: 1.756 | Accuracy: 0.481500 | 2.233 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.489500 | 2.232 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 1.773 | Reg loss: 0.045 | Tree loss: 1.773 | Accuracy: 0.473000 | 2.232 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.474000 | 2.231 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.477000 | 2.231 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.045 | Tree loss: 1.769 | Accuracy: 0.484642 | 2.23 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.044 | Tree loss: 1.858 | Accuracy: 0.406500 | 2.23 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.044 | Tree loss: 1.834 | Accuracy: 0.407000 | 2.23 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.044 | Tree loss: 1.802 | Accuracy: 0.421000 | 2.229 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.045 | Tree loss: 1.809 | Accuracy: 0.433500 | 2.229 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 1.802 | Reg loss: 0.045 | Tree loss: 1.802 | Accuracy: 0.466500 | 2.229 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.456500 | 2.228 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.480500 | 2.227 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.045 | Tree loss: 1.757 | Accuracy: 0.480500 | 2.227 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.463500 | 2.226 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 1.774 | Reg loss: 0.045 | Tree loss: 1.774 | Accuracy: 0.472500 | 2.226 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.494881 | 2.225 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.045 | Tree loss: 1.855 | Accuracy: 0.407000 | 2.225 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.045 | Tree loss: 1.858 | Accuracy: 0.407500 | 2.224 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 1.821 | Reg loss: 0.045 | Tree loss: 1.821 | Accuracy: 0.417500 | 2.224 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.045 | Tree loss: 1.814 | Accuracy: 0.418000 | 2.223 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 1.794 | Reg loss: 0.045 | Tree loss: 1.794 | Accuracy: 0.459000 | 2.223 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.482000 | 2.222 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.045 | Tree loss: 1.749 | Accuracy: 0.476500 | 2.222 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.476500 | 2.221 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.479000 | 2.221 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.484000 | 2.22 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.045 | Tree loss: 1.713 | Accuracy: 0.470990 | 2.22 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.045 | Tree loss: 1.850 | Accuracy: 0.403000 | 2.22 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.045 | Tree loss: 1.858 | Accuracy: 0.398000 | 2.219 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.045 | Tree loss: 1.816 | Accuracy: 0.405000 | 2.219 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.045 | Tree loss: 1.783 | Accuracy: 0.427500 | 2.218 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.045 | Tree loss: 1.789 | Accuracy: 0.462500 | 2.218 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 1.783 | Reg loss: 0.045 | Tree loss: 1.783 | Accuracy: 0.462500 | 2.217 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 1.787 | Reg loss: 0.045 | Tree loss: 1.787 | Accuracy: 0.456000 | 2.217 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.504500 | 2.216 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 1.753 | Reg loss: 0.045 | Tree loss: 1.753 | Accuracy: 0.474000 | 2.216 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.045 | Tree loss: 1.729 | Accuracy: 0.497500 | 2.215 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.443686 | 2.215 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.045 | Tree loss: 1.849 | Accuracy: 0.397000 | 2.215 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.045 | Tree loss: 1.856 | Accuracy: 0.414500 | 2.214 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.045 | Tree loss: 1.799 | Accuracy: 0.434500 | 2.214 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.045 | Tree loss: 1.809 | Accuracy: 0.422500 | 2.213 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.045 | Tree loss: 1.796 | Accuracy: 0.449500 | 2.213 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 1.767 | Reg loss: 0.045 | Tree loss: 1.767 | Accuracy: 0.475000 | 2.212 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.479000 | 2.212 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.473000 | 2.212 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.045 | Tree loss: 1.742 | Accuracy: 0.470500 | 2.211 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 1.757 | Reg loss: 0.045 | Tree loss: 1.757 | Accuracy: 0.465500 | 2.211 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 1.801 | Reg loss: 0.045 | Tree loss: 1.801 | Accuracy: 0.488055 | 2.21 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.045 | Tree loss: 1.843 | Accuracy: 0.400000 | 2.21 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.045 | Tree loss: 1.856 | Accuracy: 0.399500 | 2.21 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 1.826 | Reg loss: 0.045 | Tree loss: 1.826 | Accuracy: 0.435500 | 2.209 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 1.811 | Reg loss: 0.045 | Tree loss: 1.811 | Accuracy: 0.432000 | 2.209 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 1.798 | Reg loss: 0.045 | Tree loss: 1.798 | Accuracy: 0.457500 | 2.208 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.045 | Tree loss: 1.754 | Accuracy: 0.479000 | 2.208 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.488500 | 2.208 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.045 | Tree loss: 1.733 | Accuracy: 0.507500 | 2.207 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 1.753 | Reg loss: 0.045 | Tree loss: 1.753 | Accuracy: 0.487000 | 2.207 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 1.753 | Reg loss: 0.045 | Tree loss: 1.753 | Accuracy: 0.474500 | 2.206 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.488055 | 2.206 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 1.881 | Reg loss: 0.045 | Tree loss: 1.881 | Accuracy: 0.394500 | 2.206 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.045 | Tree loss: 1.833 | Accuracy: 0.407500 | 2.205 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.045 | Tree loss: 1.820 | Accuracy: 0.407000 | 2.205 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.045 | Tree loss: 1.808 | Accuracy: 0.416500 | 2.204 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.455500 | 2.204 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 1.773 | Reg loss: 0.045 | Tree loss: 1.773 | Accuracy: 0.472000 | 2.203 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.473500 | 2.203 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.465500 | 2.202 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 1.770 | Reg loss: 0.045 | Tree loss: 1.770 | Accuracy: 0.472500 | 2.202 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.045 | Tree loss: 1.730 | Accuracy: 0.483000 | 2.201 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.045 | Tree loss: 1.717 | Accuracy: 0.494881 | 2.201 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.045 | Tree loss: 1.840 | Accuracy: 0.418000 | 2.201 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.045 | Tree loss: 1.830 | Accuracy: 0.413000 | 2.201 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.045 | Tree loss: 1.815 | Accuracy: 0.421500 | 2.2 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.045 | Tree loss: 1.806 | Accuracy: 0.410000 | 2.2 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.045 | Tree loss: 1.778 | Accuracy: 0.463000 | 2.199 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 1.785 | Reg loss: 0.045 | Tree loss: 1.785 | Accuracy: 0.485000 | 2.199 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.493000 | 2.198 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.045 | Tree loss: 1.757 | Accuracy: 0.484000 | 2.198 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 1.776 | Reg loss: 0.045 | Tree loss: 1.776 | Accuracy: 0.468000 | 2.197 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.045 | Tree loss: 1.738 | Accuracy: 0.497000 | 2.197 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.045 | Tree loss: 1.709 | Accuracy: 0.501706 | 2.196 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.045 | Tree loss: 1.854 | Accuracy: 0.405000 | 2.196 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.045 | Tree loss: 1.847 | Accuracy: 0.401000 | 2.196 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.045 | Tree loss: 1.800 | Accuracy: 0.414000 | 2.196 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 1.811 | Reg loss: 0.045 | Tree loss: 1.811 | Accuracy: 0.408500 | 2.195 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 1.790 | Reg loss: 0.045 | Tree loss: 1.790 | Accuracy: 0.443500 | 2.195 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 1.767 | Reg loss: 0.045 | Tree loss: 1.767 | Accuracy: 0.454500 | 2.194 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.501500 | 2.194 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.471000 | 2.193 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.488500 | 2.193 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.491500 | 2.192 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.045 | Tree loss: 1.746 | Accuracy: 0.447099 | 2.192 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.045 | Tree loss: 1.850 | Accuracy: 0.412500 | 2.192 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.045 | Tree loss: 1.845 | Accuracy: 0.390500 | 2.192 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.045 | Tree loss: 1.823 | Accuracy: 0.423500 | 2.191 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.045 | Tree loss: 1.800 | Accuracy: 0.437500 | 2.191 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.045 | Tree loss: 1.789 | Accuracy: 0.472000 | 2.19 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.477500 | 2.19 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.045 | Tree loss: 1.753 | Accuracy: 0.473500 | 2.19 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.045 | Tree loss: 1.740 | Accuracy: 0.502000 | 2.189 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.476000 | 2.189 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.045 | Tree loss: 1.749 | Accuracy: 0.477000 | 2.188 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.045 | Tree loss: 1.746 | Accuracy: 0.457338 | 2.188 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.045 | Tree loss: 1.827 | Accuracy: 0.424000 | 2.188 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 1.869 | Reg loss: 0.045 | Tree loss: 1.869 | Accuracy: 0.399000 | 2.188 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.045 | Tree loss: 1.827 | Accuracy: 0.407500 | 2.187 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.045 | Tree loss: 1.805 | Accuracy: 0.420500 | 2.187 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.045 | Tree loss: 1.741 | Accuracy: 0.479500 | 2.186 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.476000 | 2.186 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 1.763 | Reg loss: 0.045 | Tree loss: 1.763 | Accuracy: 0.481500 | 2.185 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.472000 | 2.185 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.473500 | 2.185 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 1.783 | Reg loss: 0.045 | Tree loss: 1.783 | Accuracy: 0.447500 | 2.184 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.045 | Tree loss: 1.733 | Accuracy: 0.474403 | 2.184 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.045 | Tree loss: 1.857 | Accuracy: 0.394000 | 2.184 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.045 | Tree loss: 1.852 | Accuracy: 0.393000 | 2.184 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 1.824 | Reg loss: 0.045 | Tree loss: 1.824 | Accuracy: 0.409000 | 2.183 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.045 | Tree loss: 1.816 | Accuracy: 0.433500 | 2.183 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 1.772 | Reg loss: 0.045 | Tree loss: 1.772 | Accuracy: 0.453500 | 2.182 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.494000 | 2.182 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.045 | Tree loss: 1.728 | Accuracy: 0.503000 | 2.181 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.480500 | 2.181 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.045 | Tree loss: 1.740 | Accuracy: 0.498500 | 2.18 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 1.748 | Reg loss: 0.045 | Tree loss: 1.748 | Accuracy: 0.491500 | 2.18 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 1.749 | Reg loss: 0.045 | Tree loss: 1.749 | Accuracy: 0.467577 | 2.179 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.045 | Tree loss: 1.875 | Accuracy: 0.398500 | 2.18 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 1.860 | Reg loss: 0.045 | Tree loss: 1.860 | Accuracy: 0.401000 | 2.179 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.045 | Tree loss: 1.795 | Accuracy: 0.437000 | 2.179 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.045 | Tree loss: 1.785 | Accuracy: 0.435000 | 2.178 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.045 | Tree loss: 1.796 | Accuracy: 0.440000 | 2.178 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 1.778 | Reg loss: 0.045 | Tree loss: 1.778 | Accuracy: 0.452000 | 2.178 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.455000 | 2.177 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.045 | Tree loss: 1.720 | Accuracy: 0.493500 | 2.177 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.045 | Tree loss: 1.727 | Accuracy: 0.480500 | 2.176 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.045 | Tree loss: 1.744 | Accuracy: 0.494500 | 2.176 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.045 | Tree loss: 1.728 | Accuracy: 0.477816 | 2.175 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.045 | Tree loss: 1.847 | Accuracy: 0.413500 | 2.175 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.045 | Tree loss: 1.856 | Accuracy: 0.390500 | 2.175 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.045 | Tree loss: 1.811 | Accuracy: 0.422500 | 2.174 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.045 | Tree loss: 1.780 | Accuracy: 0.442000 | 2.174 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.045 | Tree loss: 1.774 | Accuracy: 0.458500 | 2.173 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 1.775 | Reg loss: 0.045 | Tree loss: 1.775 | Accuracy: 0.483500 | 2.173 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.045 | Tree loss: 1.767 | Accuracy: 0.469500 | 2.173 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.045 | Tree loss: 1.757 | Accuracy: 0.479500 | 2.172 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.045 | Tree loss: 1.739 | Accuracy: 0.486000 | 2.172 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.045 | Tree loss: 1.728 | Accuracy: 0.489000 | 2.171 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 1.805 | Reg loss: 0.045 | Tree loss: 1.805 | Accuracy: 0.419795 | 2.171 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.045 | Tree loss: 1.873 | Accuracy: 0.403000 | 2.171 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.045 | Tree loss: 1.828 | Accuracy: 0.410500 | 2.17 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 1.839 | Reg loss: 0.045 | Tree loss: 1.839 | Accuracy: 0.418500 | 2.17 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.450500 | 2.169 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.045 | Tree loss: 1.770 | Accuracy: 0.477500 | 2.169 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.465500 | 2.168 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 1.777 | Reg loss: 0.045 | Tree loss: 1.777 | Accuracy: 0.467000 | 2.168 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.045 | Tree loss: 1.737 | Accuracy: 0.493500 | 2.167 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.045 | Tree loss: 1.742 | Accuracy: 0.490000 | 2.167 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.462500 | 2.166 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.453925 | 2.166 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.045 | Tree loss: 1.855 | Accuracy: 0.422500 | 2.166 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.045 | Tree loss: 1.854 | Accuracy: 0.403500 | 2.166 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.045 | Tree loss: 1.812 | Accuracy: 0.422000 | 2.165 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.045 | Tree loss: 1.782 | Accuracy: 0.424000 | 2.165 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.045 | Tree loss: 1.796 | Accuracy: 0.427500 | 2.164 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.468000 | 2.164 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.479500 | 2.163 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.045 | Tree loss: 1.721 | Accuracy: 0.474000 | 2.163 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.045 | Tree loss: 1.726 | Accuracy: 0.494000 | 2.162 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 1.779 | Reg loss: 0.045 | Tree loss: 1.779 | Accuracy: 0.465000 | 2.162 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.045 | Tree loss: 1.696 | Accuracy: 0.494881 | 2.162 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.045 | Tree loss: 1.842 | Accuracy: 0.410000 | 2.162 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.045 | Tree loss: 1.844 | Accuracy: 0.419500 | 2.161 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.045 | Tree loss: 1.793 | Accuracy: 0.414000 | 2.161 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.045 | Tree loss: 1.787 | Accuracy: 0.433000 | 2.161 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 1.783 | Reg loss: 0.045 | Tree loss: 1.783 | Accuracy: 0.459000 | 2.16 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.045 | Tree loss: 1.756 | Accuracy: 0.477500 | 2.16 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 1.751 | Reg loss: 0.045 | Tree loss: 1.751 | Accuracy: 0.476000 | 2.159 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.494000 | 2.159 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 1.746 | Reg loss: 0.045 | Tree loss: 1.746 | Accuracy: 0.479500 | 2.159 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 1.776 | Reg loss: 0.045 | Tree loss: 1.776 | Accuracy: 0.467500 | 2.158 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 1.719 | Reg loss: 0.045 | Tree loss: 1.719 | Accuracy: 0.501706 | 2.158 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.045 | Tree loss: 1.852 | Accuracy: 0.411500 | 2.158 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.045 | Tree loss: 1.843 | Accuracy: 0.427000 | 2.158 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 1.838 | Reg loss: 0.045 | Tree loss: 1.838 | Accuracy: 0.410500 | 2.157 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 1.812 | Reg loss: 0.045 | Tree loss: 1.812 | Accuracy: 0.416000 | 2.157 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 1.779 | Reg loss: 0.045 | Tree loss: 1.779 | Accuracy: 0.442000 | 2.156 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.045 | Tree loss: 1.761 | Accuracy: 0.469000 | 2.156 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.045 | Tree loss: 1.742 | Accuracy: 0.482000 | 2.156 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.045 | Tree loss: 1.742 | Accuracy: 0.488500 | 2.155 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.486500 | 2.155 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.469500 | 2.154 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.045 | Tree loss: 1.692 | Accuracy: 0.481229 | 2.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.045 | Tree loss: 1.841 | Accuracy: 0.424000 | 2.154 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.045 | Tree loss: 1.845 | Accuracy: 0.399000 | 2.154 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.045 | Tree loss: 1.820 | Accuracy: 0.416500 | 2.153 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.045 | Tree loss: 1.787 | Accuracy: 0.428500 | 2.153 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.045 | Tree loss: 1.759 | Accuracy: 0.467000 | 2.152 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.045 | Tree loss: 1.761 | Accuracy: 0.448000 | 2.152 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.480000 | 2.152 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 1.754 | Reg loss: 0.045 | Tree loss: 1.754 | Accuracy: 0.493500 | 2.151 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.482500 | 2.151 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.463500 | 2.15 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.045 | Tree loss: 1.726 | Accuracy: 0.491468 | 2.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 1.837 | Reg loss: 0.045 | Tree loss: 1.837 | Accuracy: 0.419000 | 2.15 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.045 | Tree loss: 1.829 | Accuracy: 0.412500 | 2.15 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.045 | Tree loss: 1.814 | Accuracy: 0.411500 | 2.149 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.045 | Tree loss: 1.775 | Accuracy: 0.447000 | 2.149 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.045 | Tree loss: 1.757 | Accuracy: 0.452000 | 2.148 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.477000 | 2.148 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 1.791 | Reg loss: 0.045 | Tree loss: 1.791 | Accuracy: 0.466500 | 2.148 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.045 | Tree loss: 1.723 | Accuracy: 0.507000 | 2.147 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 1.761 | Reg loss: 0.045 | Tree loss: 1.761 | Accuracy: 0.466000 | 2.147 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.472000 | 2.146 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 1.790 | Reg loss: 0.045 | Tree loss: 1.790 | Accuracy: 0.426621 | 2.146 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.045 | Tree loss: 1.846 | Accuracy: 0.404000 | 2.146 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.045 | Tree loss: 1.856 | Accuracy: 0.391000 | 2.146 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.045 | Tree loss: 1.805 | Accuracy: 0.414500 | 2.145 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.045 | Tree loss: 1.800 | Accuracy: 0.442500 | 2.145 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.045 | Tree loss: 1.770 | Accuracy: 0.470000 | 2.144 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 1.768 | Reg loss: 0.045 | Tree loss: 1.768 | Accuracy: 0.470000 | 2.144 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.045 | Tree loss: 1.740 | Accuracy: 0.482500 | 2.144 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 1.770 | Reg loss: 0.045 | Tree loss: 1.770 | Accuracy: 0.467500 | 2.143 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 1.748 | Reg loss: 0.045 | Tree loss: 1.748 | Accuracy: 0.485500 | 2.143 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 1.721 | Reg loss: 0.045 | Tree loss: 1.721 | Accuracy: 0.494000 | 2.143 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.045 | Tree loss: 1.737 | Accuracy: 0.470990 | 2.142 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 1.839 | Reg loss: 0.045 | Tree loss: 1.839 | Accuracy: 0.428000 | 2.142 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.045 | Tree loss: 1.849 | Accuracy: 0.400000 | 2.142 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.045 | Tree loss: 1.795 | Accuracy: 0.428500 | 2.142 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 1.810 | Reg loss: 0.045 | Tree loss: 1.810 | Accuracy: 0.404500 | 2.141 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.045 | Tree loss: 1.761 | Accuracy: 0.445500 | 2.141 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.465500 | 2.14 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.468500 | 2.14 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.045 | Tree loss: 1.739 | Accuracy: 0.500000 | 2.14 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 1.749 | Reg loss: 0.045 | Tree loss: 1.749 | Accuracy: 0.498500 | 2.139 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.471500 | 2.139 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.045 | Tree loss: 1.697 | Accuracy: 0.522184 | 2.139 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 1.837 | Reg loss: 0.045 | Tree loss: 1.837 | Accuracy: 0.424500 | 2.139 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.045 | Tree loss: 1.839 | Accuracy: 0.393500 | 2.139 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.045 | Tree loss: 1.809 | Accuracy: 0.428500 | 2.138 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.045 | Tree loss: 1.771 | Accuracy: 0.460500 | 2.138 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 1.803 | Reg loss: 0.045 | Tree loss: 1.803 | Accuracy: 0.444000 | 2.137 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.045 | Tree loss: 1.733 | Accuracy: 0.472500 | 2.137 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.479000 | 2.137 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.477500 | 2.136 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 1.761 | Reg loss: 0.045 | Tree loss: 1.761 | Accuracy: 0.483500 | 2.136 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 1.745 | Reg loss: 0.045 | Tree loss: 1.745 | Accuracy: 0.481000 | 2.136 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 1.722 | Reg loss: 0.045 | Tree loss: 1.722 | Accuracy: 0.508532 | 2.135 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.045 | Tree loss: 1.844 | Accuracy: 0.411500 | 2.135 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.045 | Tree loss: 1.835 | Accuracy: 0.411000 | 2.135 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 1.840 | Reg loss: 0.045 | Tree loss: 1.840 | Accuracy: 0.398000 | 2.135 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.045 | Tree loss: 1.795 | Accuracy: 0.422500 | 2.134 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 1.781 | Reg loss: 0.045 | Tree loss: 1.781 | Accuracy: 0.456500 | 2.134 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.045 | Tree loss: 1.751 | Accuracy: 0.475000 | 2.133 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.045 | Tree loss: 1.737 | Accuracy: 0.486000 | 2.133 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.045 | Tree loss: 1.720 | Accuracy: 0.487000 | 2.133 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.481000 | 2.132 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.045 | Tree loss: 1.741 | Accuracy: 0.481500 | 2.132 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 1.809 | Reg loss: 0.045 | Tree loss: 1.809 | Accuracy: 0.457338 | 2.131 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.045 | Tree loss: 1.859 | Accuracy: 0.394500 | 2.132 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.045 | Tree loss: 1.815 | Accuracy: 0.422000 | 2.131 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.045 | Tree loss: 1.811 | Accuracy: 0.429500 | 2.131 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.045 | Tree loss: 1.781 | Accuracy: 0.454500 | 2.13 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.482000 | 2.13 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.479500 | 2.13 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.485500 | 2.129 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 1.751 | Reg loss: 0.045 | Tree loss: 1.751 | Accuracy: 0.472500 | 2.129 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 1.741 | Reg loss: 0.045 | Tree loss: 1.741 | Accuracy: 0.482000 | 2.128 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.487000 | 2.128 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 1.756 | Reg loss: 0.045 | Tree loss: 1.756 | Accuracy: 0.498294 | 2.128 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.045 | Tree loss: 1.857 | Accuracy: 0.399000 | 2.128 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.045 | Tree loss: 1.817 | Accuracy: 0.423000 | 2.128 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 1.831 | Reg loss: 0.045 | Tree loss: 1.831 | Accuracy: 0.408000 | 2.127 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.045 | Tree loss: 1.768 | Accuracy: 0.456000 | 2.127 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.045 | Tree loss: 1.769 | Accuracy: 0.466500 | 2.126 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.045 | Tree loss: 1.765 | Accuracy: 0.473500 | 2.126 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.045 | Tree loss: 1.732 | Accuracy: 0.482500 | 2.126 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 1.769 | Reg loss: 0.045 | Tree loss: 1.769 | Accuracy: 0.482000 | 2.125 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.045 | Tree loss: 1.743 | Accuracy: 0.490500 | 2.125 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.495000 | 2.124 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.045 | Tree loss: 1.664 | Accuracy: 0.529010 | 2.124 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.045 | Tree loss: 1.842 | Accuracy: 0.405500 | 2.124 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.045 | Tree loss: 1.817 | Accuracy: 0.409500 | 2.124 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.045 | Tree loss: 1.792 | Accuracy: 0.432000 | 2.123 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.045 | Tree loss: 1.805 | Accuracy: 0.431000 | 2.123 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.045 | Tree loss: 1.763 | Accuracy: 0.449500 | 2.123 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 1.763 | Reg loss: 0.045 | Tree loss: 1.763 | Accuracy: 0.460000 | 2.122 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.045 | Tree loss: 1.744 | Accuracy: 0.465000 | 2.122 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.045 | Tree loss: 1.748 | Accuracy: 0.476000 | 2.122 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 1.745 | Reg loss: 0.045 | Tree loss: 1.745 | Accuracy: 0.483500 | 2.121 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 1.779 | Reg loss: 0.045 | Tree loss: 1.779 | Accuracy: 0.473000 | 2.121 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.045 | Tree loss: 1.727 | Accuracy: 0.494881 | 2.12 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.045 | Tree loss: 1.860 | Accuracy: 0.413000 | 2.121 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.045 | Tree loss: 1.814 | Accuracy: 0.421500 | 2.12 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.045 | Tree loss: 1.795 | Accuracy: 0.443000 | 2.12 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.045 | Tree loss: 1.774 | Accuracy: 0.448500 | 2.119 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 1.777 | Reg loss: 0.045 | Tree loss: 1.777 | Accuracy: 0.478500 | 2.119 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.473500 | 2.119 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.484000 | 2.118 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.045 | Tree loss: 1.748 | Accuracy: 0.492000 | 2.118 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.501000 | 2.118 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 1.750 | Reg loss: 0.045 | Tree loss: 1.750 | Accuracy: 0.473500 | 2.117 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.045 | Tree loss: 1.727 | Accuracy: 0.515358 | 2.117 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.045 | Tree loss: 1.830 | Accuracy: 0.418000 | 2.117 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.045 | Tree loss: 1.832 | Accuracy: 0.422500 | 2.117 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.045 | Tree loss: 1.813 | Accuracy: 0.431500 | 2.116 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.045 | Tree loss: 1.791 | Accuracy: 0.432500 | 2.116 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.045 | Tree loss: 1.767 | Accuracy: 0.453500 | 2.115 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.479500 | 2.115 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 1.756 | Reg loss: 0.045 | Tree loss: 1.756 | Accuracy: 0.473500 | 2.115 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 1.749 | Reg loss: 0.045 | Tree loss: 1.749 | Accuracy: 0.477500 | 2.114 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.045 | Tree loss: 1.738 | Accuracy: 0.491500 | 2.114 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.045 | Tree loss: 1.736 | Accuracy: 0.478500 | 2.114 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.045 | Tree loss: 1.712 | Accuracy: 0.450512 | 2.113 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.045 | Tree loss: 1.840 | Accuracy: 0.410500 | 2.114 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.045 | Tree loss: 1.815 | Accuracy: 0.418000 | 2.113 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.045 | Tree loss: 1.808 | Accuracy: 0.430000 | 2.113 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.045 | Tree loss: 1.778 | Accuracy: 0.434500 | 2.112 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 1.788 | Reg loss: 0.045 | Tree loss: 1.788 | Accuracy: 0.476500 | 2.112 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 1.770 | Reg loss: 0.045 | Tree loss: 1.770 | Accuracy: 0.500500 | 2.112 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.045 | Tree loss: 1.733 | Accuracy: 0.489000 | 2.111 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 1.764 | Reg loss: 0.045 | Tree loss: 1.764 | Accuracy: 0.476000 | 2.111 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.045 | Tree loss: 1.733 | Accuracy: 0.493000 | 2.111 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.045 | Tree loss: 1.735 | Accuracy: 0.471000 | 2.11 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.045 | Tree loss: 1.730 | Accuracy: 0.474403 | 2.11 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.045 | Tree loss: 1.859 | Accuracy: 0.401000 | 2.11 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.045 | Tree loss: 1.806 | Accuracy: 0.446000 | 2.11 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.045 | Tree loss: 1.815 | Accuracy: 0.411000 | 2.109 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 1.796 | Reg loss: 0.045 | Tree loss: 1.796 | Accuracy: 0.426000 | 2.109 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.444000 | 2.109 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.045 | Tree loss: 1.751 | Accuracy: 0.478000 | 2.108 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.045 | Tree loss: 1.754 | Accuracy: 0.464500 | 2.108 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.045 | Tree loss: 1.737 | Accuracy: 0.478500 | 2.108 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.484000 | 2.107 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.045 | Tree loss: 1.741 | Accuracy: 0.486000 | 2.107 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.045 | Tree loss: 1.689 | Accuracy: 0.522184 | 2.106 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.045 | Tree loss: 1.847 | Accuracy: 0.425000 | 2.107 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.045 | Tree loss: 1.840 | Accuracy: 0.404000 | 2.106 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.045 | Tree loss: 1.823 | Accuracy: 0.414500 | 2.106 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.045 | Tree loss: 1.771 | Accuracy: 0.444500 | 2.105 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.045 | Tree loss: 1.751 | Accuracy: 0.477500 | 2.105 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.465500 | 2.105 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 1.748 | Reg loss: 0.045 | Tree loss: 1.748 | Accuracy: 0.485000 | 2.104 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.045 | Tree loss: 1.730 | Accuracy: 0.502000 | 2.104 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 1.745 | Reg loss: 0.045 | Tree loss: 1.745 | Accuracy: 0.492000 | 2.104 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.492500 | 2.103 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.046 | Tree loss: 1.702 | Accuracy: 0.491468 | 2.103 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.045 | Tree loss: 1.855 | Accuracy: 0.411500 | 2.103 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.045 | Tree loss: 1.832 | Accuracy: 0.411500 | 2.103 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.045 | Tree loss: 1.791 | Accuracy: 0.426500 | 2.103 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.045 | Tree loss: 1.785 | Accuracy: 0.430500 | 2.102 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.045 | Tree loss: 1.769 | Accuracy: 0.460000 | 2.102 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 1.760 | Reg loss: 0.045 | Tree loss: 1.760 | Accuracy: 0.469000 | 2.102 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.485000 | 2.101 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.045 | Tree loss: 1.726 | Accuracy: 0.496500 | 2.101 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.499500 | 2.1 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.502500 | 2.1 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.046 | Tree loss: 1.737 | Accuracy: 0.501706 | 2.1 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.045 | Tree loss: 1.836 | Accuracy: 0.436500 | 2.1 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 1.855 | Reg loss: 0.045 | Tree loss: 1.855 | Accuracy: 0.390000 | 2.1 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.045 | Tree loss: 1.808 | Accuracy: 0.415000 | 2.099 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.045 | Tree loss: 1.808 | Accuracy: 0.432000 | 2.099 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.466500 | 2.098 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.045 | Tree loss: 1.752 | Accuracy: 0.463000 | 2.098 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.045 | Tree loss: 1.725 | Accuracy: 0.493000 | 2.098 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.046 | Tree loss: 1.718 | Accuracy: 0.504000 | 2.097 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.046 | Tree loss: 1.751 | Accuracy: 0.487500 | 2.097 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.483500 | 2.097 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.046 | Tree loss: 1.721 | Accuracy: 0.501706 | 2.096 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.045 | Tree loss: 1.850 | Accuracy: 0.406500 | 2.096 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.045 | Tree loss: 1.796 | Accuracy: 0.424000 | 2.096 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 1.828 | Reg loss: 0.045 | Tree loss: 1.828 | Accuracy: 0.421000 | 2.095 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.045 | Tree loss: 1.773 | Accuracy: 0.446000 | 2.095 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.448500 | 2.095 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 1.773 | Reg loss: 0.046 | Tree loss: 1.773 | Accuracy: 0.461000 | 2.094 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.494000 | 2.094 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 1.772 | Reg loss: 0.046 | Tree loss: 1.772 | Accuracy: 0.483500 | 2.094 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.046 | Tree loss: 1.751 | Accuracy: 0.479500 | 2.093 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.046 | Tree loss: 1.716 | Accuracy: 0.500500 | 2.093 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.488055 | 2.093 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.045 | Tree loss: 1.822 | Accuracy: 0.432000 | 2.093 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.045 | Tree loss: 1.826 | Accuracy: 0.410000 | 2.092 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.045 | Tree loss: 1.807 | Accuracy: 0.429500 | 2.092 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.045 | Tree loss: 1.777 | Accuracy: 0.445500 | 2.092 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.460500 | 2.091 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.476500 | 2.091 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.477000 | 2.091 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 1.746 | Reg loss: 0.046 | Tree loss: 1.746 | Accuracy: 0.477000 | 2.09 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 1.736 | Reg loss: 0.046 | Tree loss: 1.736 | Accuracy: 0.484000 | 2.09 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.498500 | 2.09 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.498294 | 2.089 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.045 | Tree loss: 1.852 | Accuracy: 0.416000 | 2.089 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.045 | Tree loss: 1.825 | Accuracy: 0.427000 | 2.089 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.045 | Tree loss: 1.800 | Accuracy: 0.415500 | 2.088 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.046 | Tree loss: 1.795 | Accuracy: 0.429000 | 2.088 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.487500 | 2.088 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.483000 | 2.087 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.481000 | 2.087 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.046 | Tree loss: 1.748 | Accuracy: 0.491500 | 2.087 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 1.756 | Reg loss: 0.046 | Tree loss: 1.756 | Accuracy: 0.489000 | 2.086 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.479500 | 2.086 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.467577 | 2.085 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.046 | Tree loss: 1.825 | Accuracy: 0.433500 | 2.086 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.046 | Tree loss: 1.849 | Accuracy: 0.406000 | 2.085 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.046 | Tree loss: 1.800 | Accuracy: 0.426000 | 2.085 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.437000 | 2.084 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.474500 | 2.084 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.479500 | 2.084 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 1.759 | Reg loss: 0.046 | Tree loss: 1.759 | Accuracy: 0.492000 | 2.083 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 1.762 | Reg loss: 0.046 | Tree loss: 1.762 | Accuracy: 0.470000 | 2.083 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.501500 | 2.082 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.490500 | 2.082 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.046 | Tree loss: 1.707 | Accuracy: 0.498294 | 2.082 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.046 | Tree loss: 1.874 | Accuracy: 0.401000 | 2.082 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.046 | Tree loss: 1.818 | Accuracy: 0.429500 | 2.081 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.046 | Tree loss: 1.805 | Accuracy: 0.423000 | 2.081 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.046 | Tree loss: 1.790 | Accuracy: 0.430000 | 2.081 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.446000 | 2.08 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.484000 | 2.08 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.498000 | 2.079 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.481500 | 2.079 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.046 | Tree loss: 1.723 | Accuracy: 0.480000 | 2.078 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.481500 | 2.078 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.477816 | 2.078 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.046 | Tree loss: 1.827 | Accuracy: 0.408500 | 2.078 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.046 | Tree loss: 1.863 | Accuracy: 0.395500 | 2.077 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.046 | Tree loss: 1.815 | Accuracy: 0.429500 | 2.077 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.046 | Tree loss: 1.773 | Accuracy: 0.432000 | 2.077 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.465500 | 2.076 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 1.767 | Reg loss: 0.046 | Tree loss: 1.767 | Accuracy: 0.464500 | 2.076 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.046 | Tree loss: 1.708 | Accuracy: 0.496500 | 2.075 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.485500 | 2.075 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.483500 | 2.075 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.046 | Tree loss: 1.729 | Accuracy: 0.478000 | 2.074 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.484642 | 2.074 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.046 | Tree loss: 1.866 | Accuracy: 0.410000 | 2.074 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.046 | Tree loss: 1.846 | Accuracy: 0.415500 | 2.073 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.046 | Tree loss: 1.784 | Accuracy: 0.415500 | 2.073 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.046 | Tree loss: 1.765 | Accuracy: 0.463500 | 2.073 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.490000 | 2.072 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.046 | Tree loss: 1.759 | Accuracy: 0.494500 | 2.072 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.046 | Tree loss: 1.743 | Accuracy: 0.483000 | 2.071 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.482500 | 2.071 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.046 | Tree loss: 1.726 | Accuracy: 0.480500 | 2.071 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 1.748 | Reg loss: 0.046 | Tree loss: 1.748 | Accuracy: 0.460500 | 2.07 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.046 | Tree loss: 1.709 | Accuracy: 0.477816 | 2.07 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.046 | Tree loss: 1.823 | Accuracy: 0.413000 | 2.07 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.046 | Tree loss: 1.826 | Accuracy: 0.424500 | 2.07 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.046 | Tree loss: 1.803 | Accuracy: 0.411500 | 2.069 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.046 | Tree loss: 1.798 | Accuracy: 0.429500 | 2.069 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 1.781 | Reg loss: 0.046 | Tree loss: 1.781 | Accuracy: 0.448000 | 2.069 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.458500 | 2.068 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.046 | Tree loss: 1.725 | Accuracy: 0.483500 | 2.068 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.046 | Tree loss: 1.714 | Accuracy: 0.492500 | 2.068 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.486000 | 2.067 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.046 | Tree loss: 1.726 | Accuracy: 0.498000 | 2.067 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 1.805 | Reg loss: 0.046 | Tree loss: 1.805 | Accuracy: 0.447099 | 2.067 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.046 | Tree loss: 1.827 | Accuracy: 0.429000 | 2.067 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.046 | Tree loss: 1.843 | Accuracy: 0.418000 | 2.066 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.046 | Tree loss: 1.809 | Accuracy: 0.424500 | 2.066 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.046 | Tree loss: 1.772 | Accuracy: 0.446000 | 2.066 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.476500 | 2.065 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.470500 | 2.065 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.475500 | 2.065 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.493500 | 2.064 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.477500 | 2.064 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.484000 | 2.063 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.488055 | 2.063 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.046 | Tree loss: 1.849 | Accuracy: 0.401000 | 2.063 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.046 | Tree loss: 1.803 | Accuracy: 0.428000 | 2.062 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.046 | Tree loss: 1.793 | Accuracy: 0.429500 | 2.062 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.046 | Tree loss: 1.770 | Accuracy: 0.451500 | 2.062 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.046 | Tree loss: 1.757 | Accuracy: 0.474000 | 2.061 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.046 | Tree loss: 1.765 | Accuracy: 0.459500 | 2.061 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.497500 | 2.061 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 1.771 | Reg loss: 0.046 | Tree loss: 1.771 | Accuracy: 0.485000 | 2.06 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.046 | Tree loss: 1.709 | Accuracy: 0.497000 | 2.06 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.469500 | 2.06 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 1.828 | Reg loss: 0.046 | Tree loss: 1.828 | Accuracy: 0.419795 | 2.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.046 | Tree loss: 1.819 | Accuracy: 0.409000 | 2.059 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.046 | Tree loss: 1.825 | Accuracy: 0.413500 | 2.059 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.046 | Tree loss: 1.823 | Accuracy: 0.415000 | 2.059 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.046 | Tree loss: 1.765 | Accuracy: 0.445500 | 2.058 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.046 | Tree loss: 1.767 | Accuracy: 0.451500 | 2.058 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 1.772 | Reg loss: 0.046 | Tree loss: 1.772 | Accuracy: 0.465500 | 2.058 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.482500 | 2.057 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.492000 | 2.057 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.478500 | 2.057 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.046 | Tree loss: 1.728 | Accuracy: 0.486000 | 2.056 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.046 | Tree loss: 1.644 | Accuracy: 0.511945 | 2.056 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.046 | Tree loss: 1.835 | Accuracy: 0.427500 | 2.056 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.046 | Tree loss: 1.809 | Accuracy: 0.421500 | 2.056 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.046 | Tree loss: 1.827 | Accuracy: 0.413000 | 2.056 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.046 | Tree loss: 1.785 | Accuracy: 0.445000 | 2.055 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.444000 | 2.055 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.046 | Tree loss: 1.743 | Accuracy: 0.459500 | 2.055 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.492000 | 2.054 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.483000 | 2.054 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.487000 | 2.054 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.483500 | 2.054 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.046 | Tree loss: 1.672 | Accuracy: 0.484642 | 2.053 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.046 | Tree loss: 1.850 | Accuracy: 0.406000 | 2.053 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.046 | Tree loss: 1.848 | Accuracy: 0.392500 | 2.053 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.046 | Tree loss: 1.798 | Accuracy: 0.432000 | 2.053 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.046 | Tree loss: 1.756 | Accuracy: 0.453000 | 2.052 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.466000 | 2.052 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.487000 | 2.052 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.493000 | 2.051 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.046 | Tree loss: 1.720 | Accuracy: 0.502000 | 2.051 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.483000 | 2.051 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 1.759 | Reg loss: 0.046 | Tree loss: 1.759 | Accuracy: 0.483500 | 2.05 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.501706 | 2.05 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.046 | Tree loss: 1.842 | Accuracy: 0.413500 | 2.05 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.046 | Tree loss: 1.810 | Accuracy: 0.436500 | 2.05 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.046 | Tree loss: 1.800 | Accuracy: 0.431500 | 2.049 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.046 | Tree loss: 1.783 | Accuracy: 0.444500 | 2.049 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.463000 | 2.049 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.468000 | 2.048 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.046 | Tree loss: 1.725 | Accuracy: 0.478500 | 2.048 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.477500 | 2.048 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.469000 | 2.047 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.493500 | 2.047 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 1.780 | Reg loss: 0.046 | Tree loss: 1.780 | Accuracy: 0.491468 | 2.047 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 1.839 | Reg loss: 0.046 | Tree loss: 1.839 | Accuracy: 0.417000 | 2.047 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.046 | Tree loss: 1.810 | Accuracy: 0.411000 | 2.047 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.423000 | 2.046 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.046 | Tree loss: 1.797 | Accuracy: 0.444500 | 2.046 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.455500 | 2.045 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.487500 | 2.045 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 1.758 | Reg loss: 0.046 | Tree loss: 1.758 | Accuracy: 0.491000 | 2.045 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.489500 | 2.044 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 1.750 | Reg loss: 0.046 | Tree loss: 1.750 | Accuracy: 0.478000 | 2.044 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.499500 | 2.044 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.046 | Tree loss: 1.696 | Accuracy: 0.518771 | 2.043 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.046 | Tree loss: 1.834 | Accuracy: 0.425000 | 2.044 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.046 | Tree loss: 1.830 | Accuracy: 0.405500 | 2.043 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.046 | Tree loss: 1.809 | Accuracy: 0.425000 | 2.043 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.046 | Tree loss: 1.773 | Accuracy: 0.449500 | 2.043 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.046 | Tree loss: 1.757 | Accuracy: 0.478500 | 2.042 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.477500 | 2.042 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.498500 | 2.042 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.492000 | 2.041 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.486000 | 2.041 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.469500 | 2.041 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.457338 | 2.04 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.046 | Tree loss: 1.856 | Accuracy: 0.439000 | 2.04 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.046 | Tree loss: 1.837 | Accuracy: 0.418000 | 2.04 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.046 | Tree loss: 1.799 | Accuracy: 0.423000 | 2.04 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.046 | Tree loss: 1.786 | Accuracy: 0.420500 | 2.039 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.454500 | 2.039 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.469500 | 2.039 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.469500 | 2.038 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.467500 | 2.038 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.046 | Tree loss: 1.710 | Accuracy: 0.513500 | 2.038 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.046 | Tree loss: 1.707 | Accuracy: 0.508500 | 2.037 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.046 | Tree loss: 1.715 | Accuracy: 0.484642 | 2.037 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.046 | Tree loss: 1.831 | Accuracy: 0.419500 | 2.037 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.046 | Tree loss: 1.799 | Accuracy: 0.430500 | 2.037 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.046 | Tree loss: 1.791 | Accuracy: 0.430000 | 2.036 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.046 | Tree loss: 1.770 | Accuracy: 0.448500 | 2.036 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.046 | Tree loss: 1.767 | Accuracy: 0.462500 | 2.036 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.046 | Tree loss: 1.758 | Accuracy: 0.466500 | 2.035 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.484000 | 2.035 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.046 | Tree loss: 1.723 | Accuracy: 0.474500 | 2.034 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.487000 | 2.034 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.483000 | 2.034 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.470990 | 2.033 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.046 | Tree loss: 1.819 | Accuracy: 0.442000 | 2.034 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.046 | Tree loss: 1.828 | Accuracy: 0.402000 | 2.033 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.046 | Tree loss: 1.795 | Accuracy: 0.444500 | 2.033 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.046 | Tree loss: 1.770 | Accuracy: 0.456000 | 2.033 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 1.785 | Reg loss: 0.046 | Tree loss: 1.785 | Accuracy: 0.444500 | 2.032 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.488500 | 2.032 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.046 | Tree loss: 1.718 | Accuracy: 0.488000 | 2.032 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.483500 | 2.031 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.473000 | 2.031 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.481500 | 2.031 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.046 | Tree loss: 1.710 | Accuracy: 0.501706 | 2.03 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.046 | Tree loss: 1.852 | Accuracy: 0.399000 | 2.031 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.046 | Tree loss: 1.804 | Accuracy: 0.437000 | 2.03 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.046 | Tree loss: 1.794 | Accuracy: 0.420000 | 2.03 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.046 | Tree loss: 1.756 | Accuracy: 0.442000 | 2.03 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 1.779 | Reg loss: 0.046 | Tree loss: 1.779 | Accuracy: 0.441000 | 2.029 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.494500 | 2.029 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.457000 | 2.029 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.046 | Tree loss: 1.729 | Accuracy: 0.501500 | 2.028 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.477000 | 2.028 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.046 | Tree loss: 1.711 | Accuracy: 0.499500 | 2.028 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.474403 | 2.027 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.046 | Tree loss: 1.802 | Accuracy: 0.439000 | 2.028 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.046 | Tree loss: 1.838 | Accuracy: 0.406500 | 2.027 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.046 | Tree loss: 1.794 | Accuracy: 0.440000 | 2.027 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.046 | Tree loss: 1.790 | Accuracy: 0.434000 | 2.027 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.466000 | 2.026 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.471500 | 2.026 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.046 | Tree loss: 1.736 | Accuracy: 0.480000 | 2.026 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.497500 | 2.025 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.046 | Tree loss: 1.729 | Accuracy: 0.509000 | 2.025 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.499500 | 2.025 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 1.779 | Reg loss: 0.046 | Tree loss: 1.779 | Accuracy: 0.505119 | 2.024 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.046 | Tree loss: 1.825 | Accuracy: 0.415500 | 2.025 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.046 | Tree loss: 1.832 | Accuracy: 0.413500 | 2.024 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.046 | Tree loss: 1.802 | Accuracy: 0.436000 | 2.024 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.046 | Tree loss: 1.767 | Accuracy: 0.440000 | 2.024 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.046 | Tree loss: 1.765 | Accuracy: 0.469000 | 2.023 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.046 | Tree loss: 1.744 | Accuracy: 0.486000 | 2.023 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.046 | Tree loss: 1.724 | Accuracy: 0.487000 | 2.023 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.046 | Tree loss: 1.715 | Accuracy: 0.518500 | 2.022 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 1.762 | Reg loss: 0.046 | Tree loss: 1.762 | Accuracy: 0.465500 | 2.022 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.488500 | 2.022 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.518771 | 2.021 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.046 | Tree loss: 1.830 | Accuracy: 0.414000 | 2.022 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.407500 | 2.021 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.046 | Tree loss: 1.796 | Accuracy: 0.438000 | 2.021 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.046 | Tree loss: 1.780 | Accuracy: 0.463000 | 2.021 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.478500 | 2.02 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.490000 | 2.02 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.488500 | 2.02 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.498500 | 2.02 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.477000 | 2.019 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.476000 | 2.019 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.046 | Tree loss: 1.720 | Accuracy: 0.494881 | 2.019 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.046 | Tree loss: 1.834 | Accuracy: 0.409500 | 2.019 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.412000 | 2.018 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.046 | Tree loss: 1.793 | Accuracy: 0.422500 | 2.018 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.046 | Tree loss: 1.764 | Accuracy: 0.454000 | 2.018 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 1.775 | Reg loss: 0.046 | Tree loss: 1.775 | Accuracy: 0.473500 | 2.017 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.477000 | 2.017 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.469500 | 2.017 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.046 | Tree loss: 1.713 | Accuracy: 0.501000 | 2.017 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.496500 | 2.016 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.495000 | 2.016 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.046 | Tree loss: 1.708 | Accuracy: 0.457338 | 2.016 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.046 | Tree loss: 1.845 | Accuracy: 0.399500 | 2.016 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.046 | Tree loss: 1.797 | Accuracy: 0.427000 | 2.015 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.046 | Tree loss: 1.796 | Accuracy: 0.427000 | 2.015 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.046 | Tree loss: 1.756 | Accuracy: 0.461500 | 2.015 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.485500 | 2.014 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.046 | Tree loss: 1.753 | Accuracy: 0.474000 | 2.014 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.468500 | 2.014 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.046 | Tree loss: 1.726 | Accuracy: 0.503000 | 2.013 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.487500 | 2.013 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.494000 | 2.013 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 1.764 | Reg loss: 0.046 | Tree loss: 1.764 | Accuracy: 0.494881 | 2.012 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.046 | Tree loss: 1.848 | Accuracy: 0.396000 | 2.012 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.046 | Tree loss: 1.816 | Accuracy: 0.424500 | 2.012 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.046 | Tree loss: 1.777 | Accuracy: 0.422500 | 2.012 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.457500 | 2.011 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.499000 | 2.011 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.479000 | 2.011 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.496500 | 2.01 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.471000 | 2.01 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 1.765 | Reg loss: 0.046 | Tree loss: 1.765 | Accuracy: 0.478500 | 2.01 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.046 | Tree loss: 1.714 | Accuracy: 0.486000 | 2.009 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.046 | Tree loss: 1.664 | Accuracy: 0.525597 | 2.009 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.046 | Tree loss: 1.834 | Accuracy: 0.425500 | 2.009 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.046 | Tree loss: 1.800 | Accuracy: 0.422500 | 2.009 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.046 | Tree loss: 1.793 | Accuracy: 0.420500 | 2.008 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.046 | Tree loss: 1.792 | Accuracy: 0.426000 | 2.008 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.046 | Tree loss: 1.753 | Accuracy: 0.465500 | 2.008 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.478500 | 2.007 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.496000 | 2.007 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 1.744 | Reg loss: 0.046 | Tree loss: 1.744 | Accuracy: 0.496000 | 2.007 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.046 | Tree loss: 1.717 | Accuracy: 0.488000 | 2.006 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.485000 | 2.006 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.505119 | 2.006 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.046 | Tree loss: 1.815 | Accuracy: 0.424500 | 2.006 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.046 | Tree loss: 1.826 | Accuracy: 0.423500 | 2.006 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.046 | Tree loss: 1.797 | Accuracy: 0.436500 | 2.005 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.446000 | 2.005 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.046 | Tree loss: 1.751 | Accuracy: 0.485000 | 2.005 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.046 | Tree loss: 1.716 | Accuracy: 0.510000 | 2.004 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.046 | Tree loss: 1.753 | Accuracy: 0.478000 | 2.004 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.046 | Tree loss: 1.757 | Accuracy: 0.493000 | 2.004 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.497000 | 2.003 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.046 | Tree loss: 1.717 | Accuracy: 0.490500 | 2.003 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 1.750 | Reg loss: 0.046 | Tree loss: 1.750 | Accuracy: 0.498294 | 2.003 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.046 | Tree loss: 1.831 | Accuracy: 0.419000 | 2.003 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.046 | Tree loss: 1.808 | Accuracy: 0.422000 | 2.002 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.046 | Tree loss: 1.789 | Accuracy: 0.430000 | 2.002 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.425000 | 2.002 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.475500 | 2.001 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.046 | Tree loss: 1.746 | Accuracy: 0.491500 | 2.001 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 1.759 | Reg loss: 0.046 | Tree loss: 1.759 | Accuracy: 0.474500 | 2.001 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.481500 | 2.0 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.046 | Tree loss: 1.721 | Accuracy: 0.505500 | 2.0 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.046 | Tree loss: 1.744 | Accuracy: 0.483500 | 2.0 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.046 | Tree loss: 1.680 | Accuracy: 0.535836 | 1.999 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.046 | Tree loss: 1.826 | Accuracy: 0.433500 | 2.0 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.046 | Tree loss: 1.830 | Accuracy: 0.395000 | 1.999 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.441500 | 1.999 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.046 | Tree loss: 1.771 | Accuracy: 0.443000 | 1.999 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.046 | Tree loss: 1.762 | Accuracy: 0.450500 | 1.998 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.046 | Tree loss: 1.753 | Accuracy: 0.476500 | 1.998 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.046 | Tree loss: 1.728 | Accuracy: 0.492000 | 1.998 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.497000 | 1.997 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.479000 | 1.997 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.485000 | 1.997 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.046 | Tree loss: 1.712 | Accuracy: 0.529010 | 1.996 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.046 | Tree loss: 1.834 | Accuracy: 0.418500 | 1.996 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.046 | Tree loss: 1.815 | Accuracy: 0.434500 | 1.996 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.046 | Tree loss: 1.791 | Accuracy: 0.436000 | 1.996 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.046 | Tree loss: 1.778 | Accuracy: 0.436000 | 1.995 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.457000 | 1.995 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.474500 | 1.995 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.488000 | 1.994 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.486000 | 1.994 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.474000 | 1.994 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.490500 | 1.993 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.474403 | 1.993 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.046 | Tree loss: 1.816 | Accuracy: 0.417500 | 1.993 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.046 | Tree loss: 1.832 | Accuracy: 0.404500 | 1.992 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.046 | Tree loss: 1.785 | Accuracy: 0.427000 | 1.992 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.046 | Tree loss: 1.762 | Accuracy: 0.448000 | 1.992 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.046 | Tree loss: 1.756 | Accuracy: 0.465500 | 1.991 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.481500 | 1.991 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.479000 | 1.991 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.046 | Tree loss: 1.704 | Accuracy: 0.502500 | 1.991 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.484000 | 1.99 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.046 | Tree loss: 1.724 | Accuracy: 0.492500 | 1.99 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.046 | Tree loss: 1.715 | Accuracy: 0.460751 | 1.99 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.046 | Tree loss: 1.831 | Accuracy: 0.414500 | 1.99 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.046 | Tree loss: 1.826 | Accuracy: 0.416000 | 1.99 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.046 | Tree loss: 1.784 | Accuracy: 0.434500 | 1.989 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.445000 | 1.989 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.046 | Tree loss: 1.768 | Accuracy: 0.452500 | 1.989 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.046 | Tree loss: 1.737 | Accuracy: 0.488000 | 1.988 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.481500 | 1.988 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.488500 | 1.988 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.484500 | 1.987 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.046 | Tree loss: 1.710 | Accuracy: 0.502000 | 1.987 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.046 | Tree loss: 1.706 | Accuracy: 0.511945 | 1.987 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.046 | Tree loss: 1.827 | Accuracy: 0.424500 | 1.987 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.046 | Tree loss: 1.805 | Accuracy: 0.416000 | 1.987 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.046 | Tree loss: 1.809 | Accuracy: 0.427000 | 1.986 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.046 | Tree loss: 1.784 | Accuracy: 0.426000 | 1.986 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.470000 | 1.986 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.475500 | 1.986 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.476000 | 1.985 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.046 | Tree loss: 1.711 | Accuracy: 0.489500 | 1.985 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.487500 | 1.985 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.491500 | 1.984 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.491468 | 1.984 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.046 | Tree loss: 1.844 | Accuracy: 0.395500 | 1.984 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.046 | Tree loss: 1.802 | Accuracy: 0.422000 | 1.984 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.046 | Tree loss: 1.815 | Accuracy: 0.417000 | 1.984 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.046 | Tree loss: 1.776 | Accuracy: 0.440000 | 1.983 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.468500 | 1.983 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.046 | Tree loss: 1.759 | Accuracy: 0.469000 | 1.983 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.046 | Tree loss: 1.720 | Accuracy: 0.481500 | 1.982 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.046 | Tree loss: 1.718 | Accuracy: 0.504000 | 1.982 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.500500 | 1.982 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.046 | Tree loss: 1.713 | Accuracy: 0.505000 | 1.981 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.046 | Tree loss: 1.706 | Accuracy: 0.498294 | 1.981 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.046 | Tree loss: 1.833 | Accuracy: 0.405500 | 1.981 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.411000 | 1.981 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.046 | Tree loss: 1.796 | Accuracy: 0.425000 | 1.981 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.046 | Tree loss: 1.795 | Accuracy: 0.420500 | 1.98 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.046 | Tree loss: 1.761 | Accuracy: 0.480500 | 1.98 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.489000 | 1.98 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.046 | Tree loss: 1.718 | Accuracy: 0.497000 | 1.979 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.483500 | 1.979 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.046 | Tree loss: 1.737 | Accuracy: 0.496500 | 1.979 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.046 | Tree loss: 1.729 | Accuracy: 0.503000 | 1.978 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.046 | Tree loss: 1.691 | Accuracy: 0.470990 | 1.978 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 1.818 | Reg loss: 0.046 | Tree loss: 1.818 | Accuracy: 0.423000 | 1.978 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.046 | Tree loss: 1.832 | Accuracy: 0.420500 | 1.978 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.420000 | 1.977 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.046 | Tree loss: 1.784 | Accuracy: 0.428000 | 1.977 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.046 | Tree loss: 1.752 | Accuracy: 0.464500 | 1.977 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.466000 | 1.977 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.046 | Tree loss: 1.726 | Accuracy: 0.496000 | 1.976 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 1.744 | Reg loss: 0.046 | Tree loss: 1.744 | Accuracy: 0.510500 | 1.976 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.046 | Tree loss: 1.712 | Accuracy: 0.500500 | 1.976 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.046 | Tree loss: 1.736 | Accuracy: 0.500500 | 1.975 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 1.659 | Reg loss: 0.046 | Tree loss: 1.659 | Accuracy: 0.532423 | 1.975 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.046 | Tree loss: 1.822 | Accuracy: 0.434000 | 1.975 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.423000 | 1.975 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.437000 | 1.975 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.449500 | 1.975 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.046 | Tree loss: 1.747 | Accuracy: 0.474000 | 1.974 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.046 | Tree loss: 1.754 | Accuracy: 0.477500 | 1.974 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.487500 | 1.974 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.046 | Tree loss: 1.748 | Accuracy: 0.479500 | 1.973 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.046 | Tree loss: 1.717 | Accuracy: 0.499500 | 1.973 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.046 | Tree loss: 1.718 | Accuracy: 0.482000 | 1.973 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.046 | Tree loss: 1.715 | Accuracy: 0.494881 | 1.973 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.046 | Tree loss: 1.799 | Accuracy: 0.433000 | 1.973 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.046 | Tree loss: 1.800 | Accuracy: 0.422500 | 1.973 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.408000 | 1.972 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.046 | Tree loss: 1.786 | Accuracy: 0.435500 | 1.972 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.046 | Tree loss: 1.746 | Accuracy: 0.470000 | 1.972 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.046 | Tree loss: 1.731 | Accuracy: 0.484000 | 1.972 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.496500 | 1.971 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 1.746 | Reg loss: 0.046 | Tree loss: 1.746 | Accuracy: 0.478000 | 1.971 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.497500 | 1.971 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.503500 | 1.97 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.392491 | 1.97 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.046 | Tree loss: 1.846 | Accuracy: 0.411000 | 1.97 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.420000 | 1.97 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.046 | Tree loss: 1.783 | Accuracy: 0.427500 | 1.97 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.451000 | 1.969 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.456000 | 1.969 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 1.760 | Reg loss: 0.046 | Tree loss: 1.760 | Accuracy: 0.473000 | 1.969 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.046 | Tree loss: 1.720 | Accuracy: 0.486000 | 1.969 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.491000 | 1.968 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.046 | Tree loss: 1.743 | Accuracy: 0.493500 | 1.968 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.482000 | 1.968 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.046 | Tree loss: 1.676 | Accuracy: 0.535836 | 1.967 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.046 | Tree loss: 1.825 | Accuracy: 0.405000 | 1.967 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.046 | Tree loss: 1.801 | Accuracy: 0.431500 | 1.967 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.046 | Tree loss: 1.802 | Accuracy: 0.408500 | 1.966 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.439000 | 1.966 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.457000 | 1.966 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.046 | Tree loss: 1.755 | Accuracy: 0.457000 | 1.966 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.046 | Tree loss: 1.742 | Accuracy: 0.500000 | 1.965 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.046 | Tree loss: 1.737 | Accuracy: 0.498000 | 1.965 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.046 | Tree loss: 1.713 | Accuracy: 0.499500 | 1.965 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.046 | Tree loss: 1.712 | Accuracy: 0.494000 | 1.964 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 1.771 | Reg loss: 0.046 | Tree loss: 1.771 | Accuracy: 0.508532 | 1.964 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.046 | Tree loss: 1.847 | Accuracy: 0.416500 | 1.964 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.046 | Tree loss: 1.774 | Accuracy: 0.435500 | 1.964 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.046 | Tree loss: 1.777 | Accuracy: 0.433500 | 1.964 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.046 | Tree loss: 1.757 | Accuracy: 0.462500 | 1.963 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 1.777 | Reg loss: 0.046 | Tree loss: 1.777 | Accuracy: 0.453000 | 1.963 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.046 | Tree loss: 1.758 | Accuracy: 0.480500 | 1.963 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.046 | Tree loss: 1.725 | Accuracy: 0.492000 | 1.962 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.046 | Tree loss: 1.737 | Accuracy: 0.479500 | 1.962 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.046 | Tree loss: 1.717 | Accuracy: 0.496000 | 1.962 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.046 | Tree loss: 1.736 | Accuracy: 0.503000 | 1.962 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.046 | Tree loss: 1.713 | Accuracy: 0.498294 | 1.961 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.046 | Tree loss: 1.843 | Accuracy: 0.423000 | 1.961 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.046 | Tree loss: 1.825 | Accuracy: 0.404000 | 1.961 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.046 | Tree loss: 1.809 | Accuracy: 0.425000 | 1.961 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.463000 | 1.96 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.046 | Tree loss: 1.751 | Accuracy: 0.454500 | 1.96 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.046 | Tree loss: 1.736 | Accuracy: 0.480000 | 1.96 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.483500 | 1.96 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.046 | Tree loss: 1.721 | Accuracy: 0.493000 | 1.959 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.046 | Tree loss: 1.727 | Accuracy: 0.489500 | 1.959 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.508500 | 1.959 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 1.701 | Reg loss: 0.046 | Tree loss: 1.701 | Accuracy: 0.494881 | 1.958 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.046 | Tree loss: 1.824 | Accuracy: 0.420000 | 1.959 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.046 | Tree loss: 1.824 | Accuracy: 0.426000 | 1.958 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.046 | Tree loss: 1.810 | Accuracy: 0.413500 | 1.958 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.046 | Tree loss: 1.761 | Accuracy: 0.443000 | 1.958 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.474000 | 1.957 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.046 | Tree loss: 1.714 | Accuracy: 0.485500 | 1.957 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.487000 | 1.957 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.046 | Tree loss: 1.739 | Accuracy: 0.479000 | 1.956 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.494000 | 1.956 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.496500 | 1.956 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.505119 | 1.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.046 | Tree loss: 1.829 | Accuracy: 0.419000 | 1.956 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 1.782 | Reg loss: 0.046 | Tree loss: 1.782 | Accuracy: 0.428500 | 1.955 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.046 | Tree loss: 1.788 | Accuracy: 0.418000 | 1.955 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.046 | Tree loss: 1.789 | Accuracy: 0.440500 | 1.955 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.046 | Tree loss: 1.744 | Accuracy: 0.461500 | 1.955 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.046 | Tree loss: 1.753 | Accuracy: 0.464000 | 1.954 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.046 | Tree loss: 1.735 | Accuracy: 0.504000 | 1.954 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 1.734 | Reg loss: 0.046 | Tree loss: 1.734 | Accuracy: 0.504500 | 1.954 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.492500 | 1.953 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.492000 | 1.953 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.539249 | 1.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.046 | Tree loss: 1.826 | Accuracy: 0.415500 | 1.953 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.408500 | 1.952 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.046 | Tree loss: 1.800 | Accuracy: 0.430500 | 1.952 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.046 | Tree loss: 1.773 | Accuracy: 0.453500 | 1.952 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.046 | Tree loss: 1.728 | Accuracy: 0.471500 | 1.952 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.046 | Tree loss: 1.749 | Accuracy: 0.486000 | 1.951 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.046 | Tree loss: 1.738 | Accuracy: 0.494500 | 1.951 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.046 | Tree loss: 1.730 | Accuracy: 0.505500 | 1.951 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.510000 | 1.951 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.478000 | 1.95 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 1.795 | Reg loss: 0.047 | Tree loss: 1.795 | Accuracy: 0.491468 | 1.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 1.810 | Reg loss: 0.046 | Tree loss: 1.810 | Accuracy: 0.411500 | 1.95 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.413500 | 1.95 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.046 | Tree loss: 1.803 | Accuracy: 0.419000 | 1.95 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.046 | Tree loss: 1.767 | Accuracy: 0.446500 | 1.949 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.457500 | 1.949 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.046 | Tree loss: 1.726 | Accuracy: 0.474000 | 1.949 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.046 | Tree loss: 1.733 | Accuracy: 0.486000 | 1.949 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.483500 | 1.948 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.506000 | 1.948 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.504000 | 1.948 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.515358 | 1.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.046 | Tree loss: 1.833 | Accuracy: 0.412500 | 1.948 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.046 | Tree loss: 1.805 | Accuracy: 0.417500 | 1.948 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.046 | Tree loss: 1.772 | Accuracy: 0.432500 | 1.947 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.046 | Tree loss: 1.774 | Accuracy: 0.442500 | 1.947 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.465500 | 1.947 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.046 | Tree loss: 1.750 | Accuracy: 0.468000 | 1.947 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.046 | Tree loss: 1.728 | Accuracy: 0.490000 | 1.946 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.497000 | 1.946 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.489500 | 1.946 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.504000 | 1.946 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.488055 | 1.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.046 | Tree loss: 1.854 | Accuracy: 0.410500 | 1.946 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.046 | Tree loss: 1.819 | Accuracy: 0.417500 | 1.945 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.046 | Tree loss: 1.796 | Accuracy: 0.425500 | 1.945 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.046 | Tree loss: 1.763 | Accuracy: 0.444000 | 1.945 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.046 | Tree loss: 1.741 | Accuracy: 0.472000 | 1.945 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.046 | Tree loss: 1.757 | Accuracy: 0.490000 | 1.944 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.047 | Tree loss: 1.696 | Accuracy: 0.498000 | 1.944 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.486500 | 1.944 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.504500 | 1.944 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.047 | Tree loss: 1.710 | Accuracy: 0.481500 | 1.943 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.539249 | 1.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.046 | Tree loss: 1.820 | Accuracy: 0.417000 | 1.943 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.046 | Tree loss: 1.799 | Accuracy: 0.422000 | 1.943 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.429000 | 1.943 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.046 | Tree loss: 1.748 | Accuracy: 0.447000 | 1.943 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.046 | Tree loss: 1.732 | Accuracy: 0.472000 | 1.942 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.466500 | 1.942 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.496000 | 1.942 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.492000 | 1.941 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.047 | Tree loss: 1.742 | Accuracy: 0.493000 | 1.941 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.481000 | 1.941 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.505119 | 1.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.046 | Tree loss: 1.823 | Accuracy: 0.437000 | 1.941 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.046 | Tree loss: 1.802 | Accuracy: 0.427000 | 1.941 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.046 | Tree loss: 1.798 | Accuracy: 0.409500 | 1.94 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.046 | Tree loss: 1.766 | Accuracy: 0.440500 | 1.94 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.469000 | 1.94 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.465500 | 1.94 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.478000 | 1.939 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.472500 | 1.939 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.493500 | 1.939 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.506000 | 1.939 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.047 | Tree loss: 1.702 | Accuracy: 0.518771 | 1.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.046 | Tree loss: 1.815 | Accuracy: 0.415500 | 1.939 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.046 | Tree loss: 1.806 | Accuracy: 0.419500 | 1.938 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.046 | Tree loss: 1.787 | Accuracy: 0.435000 | 1.938 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.047 | Tree loss: 1.775 | Accuracy: 0.464000 | 1.938 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.471000 | 1.937 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.501000 | 1.937 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.469000 | 1.937 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.480000 | 1.937 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.047 | Tree loss: 1.693 | Accuracy: 0.518500 | 1.936 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.508000 | 1.936 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.467577 | 1.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.047 | Tree loss: 1.828 | Accuracy: 0.409000 | 1.936 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.047 | Tree loss: 1.829 | Accuracy: 0.404000 | 1.936 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.446000 | 1.936 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.047 | Tree loss: 1.778 | Accuracy: 0.457500 | 1.935 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 1.777 | Reg loss: 0.047 | Tree loss: 1.777 | Accuracy: 0.451500 | 1.935 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.478000 | 1.935 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.495500 | 1.935 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.480500 | 1.934 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.504500 | 1.934 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.513500 | 1.934 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.474403 | 1.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.047 | Tree loss: 1.843 | Accuracy: 0.409000 | 1.933 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.427000 | 1.933 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.441500 | 1.933 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.047 | Tree loss: 1.766 | Accuracy: 0.449000 | 1.932 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.480000 | 1.932 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.494500 | 1.932 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.502500 | 1.932 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.512000 | 1.931 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.480000 | 1.931 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.488000 | 1.931 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.450512 | 1.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.047 | Tree loss: 1.824 | Accuracy: 0.429000 | 1.931 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.047 | Tree loss: 1.836 | Accuracy: 0.418500 | 1.931 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.047 | Tree loss: 1.784 | Accuracy: 0.447000 | 1.93 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.047 | Tree loss: 1.769 | Accuracy: 0.442000 | 1.93 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.470500 | 1.93 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.472500 | 1.93 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.497500 | 1.929 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.525500 | 1.929 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.503500 | 1.929 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.047 | Tree loss: 1.715 | Accuracy: 0.503000 | 1.929 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.525597 | 1.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 1.818 | Reg loss: 0.047 | Tree loss: 1.818 | Accuracy: 0.397500 | 1.929 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.047 | Tree loss: 1.821 | Accuracy: 0.417000 | 1.928 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.047 | Tree loss: 1.804 | Accuracy: 0.431500 | 1.928 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.047 | Tree loss: 1.762 | Accuracy: 0.448000 | 1.928 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.472000 | 1.928 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.047 | Tree loss: 1.732 | Accuracy: 0.481500 | 1.927 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.498000 | 1.927 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.509500 | 1.927 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.506500 | 1.927 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.483000 | 1.927 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.047 | Tree loss: 1.680 | Accuracy: 0.539249 | 1.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.420500 | 1.927 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.406500 | 1.926 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.047 | Tree loss: 1.803 | Accuracy: 0.427000 | 1.926 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.047 | Tree loss: 1.765 | Accuracy: 0.453000 | 1.926 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.470500 | 1.926 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.473500 | 1.925 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.504500 | 1.925 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.490500 | 1.925 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.513500 | 1.925 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.047 | Tree loss: 1.739 | Accuracy: 0.512500 | 1.925 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.501706 | 1.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.047 | Tree loss: 1.836 | Accuracy: 0.411500 | 1.924 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.047 | Tree loss: 1.806 | Accuracy: 0.419500 | 1.924 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.450500 | 1.924 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.047 | Tree loss: 1.769 | Accuracy: 0.443500 | 1.923 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.047 | Tree loss: 1.766 | Accuracy: 0.453500 | 1.923 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.494000 | 1.923 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.047 | Tree loss: 1.715 | Accuracy: 0.491000 | 1.923 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.477500 | 1.922 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.508000 | 1.922 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.502500 | 1.922 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.481229 | 1.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.047 | Tree loss: 1.835 | Accuracy: 0.413500 | 1.922 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 1.807 | Reg loss: 0.047 | Tree loss: 1.807 | Accuracy: 0.423500 | 1.922 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.047 | Tree loss: 1.784 | Accuracy: 0.431500 | 1.922 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.457000 | 1.921 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.047 | Tree loss: 1.754 | Accuracy: 0.451500 | 1.921 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.482500 | 1.921 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.047 | Tree loss: 1.732 | Accuracy: 0.485000 | 1.921 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.469500 | 1.921 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.505500 | 1.92 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.497500 | 1.92 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.505119 | 1.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.419000 | 1.92 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.047 | Tree loss: 1.804 | Accuracy: 0.419000 | 1.92 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.047 | Tree loss: 1.786 | Accuracy: 0.432500 | 1.92 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.047 | Tree loss: 1.769 | Accuracy: 0.466000 | 1.919 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.047 | Tree loss: 1.753 | Accuracy: 0.482000 | 1.919 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 1.774 | Reg loss: 0.047 | Tree loss: 1.774 | Accuracy: 0.478500 | 1.919 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.503500 | 1.919 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.497500 | 1.919 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.491000 | 1.918 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.484000 | 1.918 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 1.701 | Reg loss: 0.047 | Tree loss: 1.701 | Accuracy: 0.470990 | 1.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.431500 | 1.918 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.047 | Tree loss: 1.810 | Accuracy: 0.431500 | 1.918 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.047 | Tree loss: 1.767 | Accuracy: 0.450500 | 1.918 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.047 | Tree loss: 1.775 | Accuracy: 0.444500 | 1.918 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.466500 | 1.917 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.047 | Tree loss: 1.756 | Accuracy: 0.480500 | 1.917 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.460000 | 1.917 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.047 | Tree loss: 1.706 | Accuracy: 0.516500 | 1.917 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.480500 | 1.917 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.497500 | 1.916 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 1.798 | Reg loss: 0.047 | Tree loss: 1.798 | Accuracy: 0.453925 | 1.916 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.047 | Tree loss: 1.827 | Accuracy: 0.409000 | 1.916 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.047 | Tree loss: 1.831 | Accuracy: 0.411000 | 1.916 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.425000 | 1.916 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.451500 | 1.916 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.466500 | 1.915 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.497500 | 1.915 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.494500 | 1.915 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.501500 | 1.915 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.491000 | 1.915 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.519500 | 1.914 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.470990 | 1.914 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.047 | Tree loss: 1.836 | Accuracy: 0.420500 | 1.915 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.416500 | 1.914 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.443500 | 1.914 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.450500 | 1.914 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.439500 | 1.914 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.457500 | 1.913 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.472500 | 1.913 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.484000 | 1.913 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.506000 | 1.913 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.506000 | 1.913 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 1.790 | Reg loss: 0.047 | Tree loss: 1.790 | Accuracy: 0.488055 | 1.912 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.047 | Tree loss: 1.848 | Accuracy: 0.408500 | 1.913 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 1.795 | Reg loss: 0.047 | Tree loss: 1.795 | Accuracy: 0.417500 | 1.913 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.435500 | 1.912 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.457500 | 1.912 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.472500 | 1.912 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.485500 | 1.912 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.505000 | 1.912 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.506000 | 1.911 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.487500 | 1.911 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.489000 | 1.911 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.470990 | 1.911 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.047 | Tree loss: 1.843 | Accuracy: 0.400500 | 1.911 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.047 | Tree loss: 1.824 | Accuracy: 0.413500 | 1.91 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.047 | Tree loss: 1.784 | Accuracy: 0.436500 | 1.91 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.475500 | 1.91 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.047 | Tree loss: 1.753 | Accuracy: 0.467000 | 1.91 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.499000 | 1.909 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.493500 | 1.909 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.477500 | 1.909 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.047 | Tree loss: 1.697 | Accuracy: 0.522500 | 1.909 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.047 | Tree loss: 1.732 | Accuracy: 0.485500 | 1.909 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.518771 | 1.908 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.047 | Tree loss: 1.829 | Accuracy: 0.409500 | 1.909 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.047 | Tree loss: 1.816 | Accuracy: 0.407500 | 1.908 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.047 | Tree loss: 1.797 | Accuracy: 0.431500 | 1.908 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.456500 | 1.908 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.047 | Tree loss: 1.742 | Accuracy: 0.489000 | 1.908 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.488500 | 1.907 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.496500 | 1.907 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.047 | Tree loss: 1.706 | Accuracy: 0.517500 | 1.907 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.486500 | 1.907 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.494500 | 1.906 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.484642 | 1.906 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.047 | Tree loss: 1.862 | Accuracy: 0.403000 | 1.906 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.047 | Tree loss: 1.808 | Accuracy: 0.422500 | 1.906 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.441000 | 1.906 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.431500 | 1.906 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.458500 | 1.905 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.484500 | 1.905 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.474000 | 1.905 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.492500 | 1.905 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.500500 | 1.905 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.508500 | 1.904 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.491468 | 1.904 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.047 | Tree loss: 1.827 | Accuracy: 0.410000 | 1.904 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.047 | Tree loss: 1.834 | Accuracy: 0.401000 | 1.904 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.047 | Tree loss: 1.798 | Accuracy: 0.420500 | 1.904 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.047 | Tree loss: 1.770 | Accuracy: 0.441000 | 1.903 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.047 | Tree loss: 1.753 | Accuracy: 0.472000 | 1.903 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.047 | Tree loss: 1.739 | Accuracy: 0.479000 | 1.903 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.497500 | 1.903 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.497000 | 1.903 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.047 | Tree loss: 1.681 | Accuracy: 0.512500 | 1.902 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.499500 | 1.902 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.539249 | 1.902 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.424000 | 1.902 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.047 | Tree loss: 1.804 | Accuracy: 0.432000 | 1.902 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.047 | Tree loss: 1.804 | Accuracy: 0.403000 | 1.901 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.442500 | 1.901 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.462000 | 1.901 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.467500 | 1.901 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.047 | Tree loss: 1.754 | Accuracy: 0.484000 | 1.9 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.492500 | 1.9 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.047 | Tree loss: 1.704 | Accuracy: 0.495500 | 1.9 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.047 | Tree loss: 1.710 | Accuracy: 0.524000 | 1.9 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.529010 | 1.899 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.047 | Tree loss: 1.825 | Accuracy: 0.416500 | 1.899 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.047 | Tree loss: 1.803 | Accuracy: 0.429000 | 1.899 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.047 | Tree loss: 1.777 | Accuracy: 0.451500 | 1.899 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.460000 | 1.899 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.489500 | 1.898 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.499500 | 1.898 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.487000 | 1.898 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.496000 | 1.898 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.513000 | 1.897 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.483000 | 1.897 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.440273 | 1.897 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 1.821 | Reg loss: 0.047 | Tree loss: 1.821 | Accuracy: 0.416500 | 1.897 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 1.812 | Reg loss: 0.047 | Tree loss: 1.812 | Accuracy: 0.429000 | 1.897 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.047 | Tree loss: 1.790 | Accuracy: 0.423000 | 1.897 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.444000 | 1.897 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.476500 | 1.896 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.481000 | 1.896 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.476000 | 1.896 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.466000 | 1.896 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.514000 | 1.895 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.492500 | 1.895 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.047 | Tree loss: 1.683 | Accuracy: 0.525597 | 1.895 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.047 | Tree loss: 1.835 | Accuracy: 0.416500 | 1.895 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.446000 | 1.895 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.047 | Tree loss: 1.807 | Accuracy: 0.412000 | 1.895 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.459000 | 1.894 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.476000 | 1.894 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.489500 | 1.894 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.484000 | 1.894 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.504500 | 1.893 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.497000 | 1.893 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.498000 | 1.893 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.047 | Tree loss: 1.629 | Accuracy: 0.590444 | 1.893 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.047 | Tree loss: 1.829 | Accuracy: 0.421500 | 1.893 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.047 | Tree loss: 1.799 | Accuracy: 0.439500 | 1.893 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.047 | Tree loss: 1.792 | Accuracy: 0.440000 | 1.892 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.047 | Tree loss: 1.792 | Accuracy: 0.444500 | 1.892 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.490500 | 1.892 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.470500 | 1.892 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.047 | Tree loss: 1.706 | Accuracy: 0.501500 | 1.892 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.489500 | 1.891 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.496500 | 1.891 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.485500 | 1.891 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.047 | Tree loss: 1.674 | Accuracy: 0.498294 | 1.891 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.047 | Tree loss: 1.814 | Accuracy: 0.419000 | 1.891 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.431500 | 1.891 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.428000 | 1.89 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.047 | Tree loss: 1.774 | Accuracy: 0.437000 | 1.89 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.469000 | 1.89 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.480500 | 1.89 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.481000 | 1.889 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.047 | Tree loss: 1.732 | Accuracy: 0.486000 | 1.889 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.498500 | 1.889 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.524000 | 1.889 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 1.770 | Reg loss: 0.047 | Tree loss: 1.770 | Accuracy: 0.450512 | 1.888 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.047 | Tree loss: 1.820 | Accuracy: 0.411000 | 1.888 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.441000 | 1.888 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.047 | Tree loss: 1.775 | Accuracy: 0.425500 | 1.888 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.047 | Tree loss: 1.766 | Accuracy: 0.448000 | 1.888 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.469000 | 1.888 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.467500 | 1.887 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.491500 | 1.887 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.515000 | 1.887 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.490000 | 1.887 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.496500 | 1.886 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.563140 | 1.886 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.047 | Tree loss: 1.825 | Accuracy: 0.416000 | 1.886 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.426000 | 1.886 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.047 | Tree loss: 1.790 | Accuracy: 0.419500 | 1.886 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.047 | Tree loss: 1.753 | Accuracy: 0.447500 | 1.886 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.457500 | 1.886 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.470500 | 1.885 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.488000 | 1.885 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.498000 | 1.885 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.488500 | 1.885 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.497000 | 1.885 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.556314 | 1.884 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.047 | Tree loss: 1.815 | Accuracy: 0.416000 | 1.885 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.047 | Tree loss: 1.817 | Accuracy: 0.418000 | 1.884 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.047 | Tree loss: 1.795 | Accuracy: 0.444000 | 1.884 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.457000 | 1.884 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.468000 | 1.884 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.492500 | 1.883 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.478500 | 1.883 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.508000 | 1.883 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.494500 | 1.883 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.047 | Tree loss: 1.715 | Accuracy: 0.489000 | 1.882 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.047 | Tree loss: 1.673 | Accuracy: 0.481229 | 1.882 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.047 | Tree loss: 1.843 | Accuracy: 0.405500 | 1.882 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.047 | Tree loss: 1.824 | Accuracy: 0.393500 | 1.882 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.047 | Tree loss: 1.774 | Accuracy: 0.439500 | 1.882 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.454000 | 1.882 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.453500 | 1.882 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.494000 | 1.881 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.472000 | 1.881 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.486500 | 1.881 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.501500 | 1.881 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.501000 | 1.88 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.494881 | 1.88 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.047 | Tree loss: 1.833 | Accuracy: 0.420000 | 1.88 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 1.786 | Reg loss: 0.047 | Tree loss: 1.786 | Accuracy: 0.423000 | 1.88 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.047 | Tree loss: 1.786 | Accuracy: 0.427000 | 1.88 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.442000 | 1.88 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.480000 | 1.879 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.488000 | 1.879 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.495000 | 1.879 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.047 | Tree loss: 1.702 | Accuracy: 0.507000 | 1.879 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.458500 | 1.878 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.047 | Tree loss: 1.691 | Accuracy: 0.502500 | 1.878 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.047 | Tree loss: 1.687 | Accuracy: 0.508532 | 1.878 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.047 | Tree loss: 1.814 | Accuracy: 0.407500 | 1.878 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 1.798 | Reg loss: 0.047 | Tree loss: 1.798 | Accuracy: 0.427000 | 1.878 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.047 | Tree loss: 1.791 | Accuracy: 0.438500 | 1.878 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.444500 | 1.877 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.456500 | 1.877 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.483000 | 1.877 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.495000 | 1.877 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.476000 | 1.876 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.496500 | 1.876 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.505500 | 1.876 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.450512 | 1.876 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.047 | Tree loss: 1.832 | Accuracy: 0.414000 | 1.876 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.047 | Tree loss: 1.822 | Accuracy: 0.416500 | 1.876 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.453500 | 1.875 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 1.812 | Reg loss: 0.047 | Tree loss: 1.812 | Accuracy: 0.429500 | 1.875 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.474500 | 1.875 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.482000 | 1.875 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.489500 | 1.874 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.480500 | 1.874 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.500000 | 1.874 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.503500 | 1.874 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.047 | Tree loss: 1.680 | Accuracy: 0.549488 | 1.873 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 1.810 | Reg loss: 0.047 | Tree loss: 1.810 | Accuracy: 0.415500 | 1.874 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.047 | Tree loss: 1.806 | Accuracy: 0.422000 | 1.873 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.047 | Tree loss: 1.803 | Accuracy: 0.416500 | 1.873 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.456000 | 1.873 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.458500 | 1.873 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.490500 | 1.872 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.509000 | 1.872 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.487500 | 1.872 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.492500 | 1.872 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.511000 | 1.871 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 1.756 | Reg loss: 0.047 | Tree loss: 1.756 | Accuracy: 0.488055 | 1.871 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 1.821 | Reg loss: 0.047 | Tree loss: 1.821 | Accuracy: 0.423000 | 1.871 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 1.798 | Reg loss: 0.047 | Tree loss: 1.798 | Accuracy: 0.429500 | 1.871 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.446500 | 1.871 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.047 | Tree loss: 1.776 | Accuracy: 0.432000 | 1.87 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.456000 | 1.87 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.466500 | 1.87 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.484000 | 1.87 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.498500 | 1.87 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.047 | Tree loss: 1.695 | Accuracy: 0.514000 | 1.869 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.514000 | 1.869 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.467577 | 1.869 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.424000 | 1.869 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.047 | Tree loss: 1.826 | Accuracy: 0.414000 | 1.869 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.435500 | 1.869 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.047 | Tree loss: 1.767 | Accuracy: 0.433500 | 1.868 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.456000 | 1.868 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.471500 | 1.868 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.480500 | 1.868 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 1.765 | Reg loss: 0.047 | Tree loss: 1.765 | Accuracy: 0.458500 | 1.867 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.517500 | 1.867 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.047 | Tree loss: 1.704 | Accuracy: 0.504000 | 1.867 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.481229 | 1.867 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 1.780 | Reg loss: 0.047 | Tree loss: 1.780 | Accuracy: 0.446000 | 1.867 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.047 | Tree loss: 1.805 | Accuracy: 0.416000 | 1.867 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.047 | Tree loss: 1.797 | Accuracy: 0.417500 | 1.866 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.047 | Tree loss: 1.762 | Accuracy: 0.454000 | 1.866 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.446500 | 1.866 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.474500 | 1.866 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.460500 | 1.865 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.512000 | 1.865 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.492000 | 1.865 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.504000 | 1.865 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.047 | Tree loss: 1.686 | Accuracy: 0.529010 | 1.864 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.047 | Tree loss: 1.831 | Accuracy: 0.419000 | 1.865 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.047 | Tree loss: 1.801 | Accuracy: 0.431500 | 1.864 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.047 | Tree loss: 1.776 | Accuracy: 0.432000 | 1.864 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.438500 | 1.864 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.466000 | 1.864 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.460000 | 1.864 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.047 | Tree loss: 1.742 | Accuracy: 0.478500 | 1.863 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.495500 | 1.863 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.047 | Tree loss: 1.704 | Accuracy: 0.494500 | 1.863 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.506000 | 1.863 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.477816 | 1.862 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.047 | Tree loss: 1.816 | Accuracy: 0.425500 | 1.863 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.047 | Tree loss: 1.774 | Accuracy: 0.459000 | 1.862 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.047 | Tree loss: 1.787 | Accuracy: 0.438500 | 1.862 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.445500 | 1.862 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.477000 | 1.862 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.472000 | 1.862 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.499000 | 1.861 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.483500 | 1.861 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.500000 | 1.861 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.492000 | 1.861 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.501706 | 1.861 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.432000 | 1.861 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.047 | Tree loss: 1.814 | Accuracy: 0.423000 | 1.861 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.047 | Tree loss: 1.801 | Accuracy: 0.424500 | 1.86 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.047 | Tree loss: 1.774 | Accuracy: 0.444000 | 1.86 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.047 | Tree loss: 1.765 | Accuracy: 0.457500 | 1.86 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.468000 | 1.86 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.047 | Tree loss: 1.698 | Accuracy: 0.503500 | 1.859 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.494000 | 1.859 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.047 | Tree loss: 1.702 | Accuracy: 0.506000 | 1.859 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.493500 | 1.859 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.047 | Tree loss: 1.739 | Accuracy: 0.525597 | 1.859 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.047 | Tree loss: 1.828 | Accuracy: 0.429000 | 1.859 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.047 | Tree loss: 1.805 | Accuracy: 0.421000 | 1.859 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.457500 | 1.858 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.433500 | 1.858 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.477000 | 1.858 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.511000 | 1.858 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.508000 | 1.857 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.507000 | 1.857 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.492500 | 1.857 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.499000 | 1.857 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.047 | Tree loss: 1.646 | Accuracy: 0.542662 | 1.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.047 | Tree loss: 1.824 | Accuracy: 0.432000 | 1.857 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 1.812 | Reg loss: 0.047 | Tree loss: 1.812 | Accuracy: 0.416000 | 1.857 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.047 | Tree loss: 1.786 | Accuracy: 0.428500 | 1.856 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.047 | Tree loss: 1.780 | Accuracy: 0.436500 | 1.856 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.472000 | 1.856 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.489500 | 1.856 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.478000 | 1.855 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.497000 | 1.855 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.047 | Tree loss: 1.687 | Accuracy: 0.495500 | 1.855 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.486500 | 1.855 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.511945 | 1.854 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.047 | Tree loss: 1.820 | Accuracy: 0.420500 | 1.855 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.047 | Tree loss: 1.792 | Accuracy: 0.428500 | 1.854 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.417000 | 1.854 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.047 | Tree loss: 1.742 | Accuracy: 0.447000 | 1.854 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.453500 | 1.854 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.460000 | 1.854 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.474500 | 1.853 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.488500 | 1.853 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.506000 | 1.853 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.526500 | 1.853 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.474403 | 1.852 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.415500 | 1.852 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 1.778 | Reg loss: 0.047 | Tree loss: 1.778 | Accuracy: 0.438500 | 1.852 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.441500 | 1.852 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.047 | Tree loss: 1.742 | Accuracy: 0.475000 | 1.852 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.047 | Tree loss: 1.762 | Accuracy: 0.475500 | 1.851 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.486500 | 1.851 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.502000 | 1.851 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.504000 | 1.851 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.483000 | 1.851 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.487000 | 1.85 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.505119 | 1.85 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.047 | Tree loss: 1.799 | Accuracy: 0.447500 | 1.85 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.428000 | 1.85 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.434000 | 1.85 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.448500 | 1.849 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.470500 | 1.849 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.464500 | 1.849 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.504500 | 1.849 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.480000 | 1.849 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.503000 | 1.848 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.498500 | 1.848 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.047 | Tree loss: 1.654 | Accuracy: 0.511945 | 1.848 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.047 | Tree loss: 1.815 | Accuracy: 0.414000 | 1.848 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.047 | Tree loss: 1.800 | Accuracy: 0.423000 | 1.848 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.047 | Tree loss: 1.791 | Accuracy: 0.442000 | 1.848 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.442500 | 1.847 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.478000 | 1.847 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.495000 | 1.847 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.486000 | 1.847 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.497500 | 1.846 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.481500 | 1.846 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.499000 | 1.846 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.484642 | 1.846 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.047 | Tree loss: 1.825 | Accuracy: 0.419000 | 1.846 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.047 | Tree loss: 1.797 | Accuracy: 0.429000 | 1.846 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.462500 | 1.846 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.439000 | 1.845 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.481000 | 1.845 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.476000 | 1.845 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.480500 | 1.845 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.475500 | 1.845 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.047 | Tree loss: 1.691 | Accuracy: 0.501000 | 1.844 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.500500 | 1.844 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.047 | Tree loss: 1.700 | Accuracy: 0.494881 | 1.844 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.439500 | 1.844 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.047 | Tree loss: 1.797 | Accuracy: 0.413000 | 1.844 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.047 | Tree loss: 1.807 | Accuracy: 0.423500 | 1.844 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.446500 | 1.843 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.047 | Tree loss: 1.754 | Accuracy: 0.460500 | 1.843 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.486000 | 1.843 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.509500 | 1.843 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.497000 | 1.843 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.488000 | 1.842 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.509000 | 1.842 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.047 | Tree loss: 1.715 | Accuracy: 0.488055 | 1.842 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.047 | Tree loss: 1.808 | Accuracy: 0.426000 | 1.842 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.420000 | 1.842 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.047 | Tree loss: 1.776 | Accuracy: 0.428000 | 1.841 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.460500 | 1.841 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.460000 | 1.841 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.483000 | 1.841 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.501500 | 1.841 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.491500 | 1.84 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.047 | Tree loss: 1.706 | Accuracy: 0.508000 | 1.84 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.501500 | 1.84 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.047 | Tree loss: 1.697 | Accuracy: 0.511945 | 1.84 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.047 | Tree loss: 1.831 | Accuracy: 0.418000 | 1.84 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.047 | Tree loss: 1.818 | Accuracy: 0.415500 | 1.84 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.047 | Tree loss: 1.767 | Accuracy: 0.435500 | 1.84 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.047 | Tree loss: 1.780 | Accuracy: 0.444000 | 1.839 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.479500 | 1.839 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.488000 | 1.839 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.477500 | 1.839 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.501000 | 1.839 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.047 | Tree loss: 1.674 | Accuracy: 0.505000 | 1.838 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.488500 | 1.838 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.518771 | 1.838 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.047 | Tree loss: 1.812 | Accuracy: 0.418500 | 1.838 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.047 | Tree loss: 1.805 | Accuracy: 0.431500 | 1.838 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.428500 | 1.838 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.472000 | 1.838 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.463500 | 1.837 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.483000 | 1.837 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.047 | Tree loss: 1.738 | Accuracy: 0.496000 | 1.837 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.503500 | 1.837 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.047 | Tree loss: 1.700 | Accuracy: 0.502000 | 1.837 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.047 | Tree loss: 1.693 | Accuracy: 0.507000 | 1.836 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.488055 | 1.836 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.047 | Tree loss: 1.836 | Accuracy: 0.421000 | 1.836 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.047 | Tree loss: 1.783 | Accuracy: 0.429500 | 1.836 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.047 | Tree loss: 1.785 | Accuracy: 0.441000 | 1.836 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.459500 | 1.836 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.472000 | 1.836 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.488000 | 1.835 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.047 | Tree loss: 1.693 | Accuracy: 0.515500 | 1.835 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.477500 | 1.835 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.491500 | 1.835 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.501500 | 1.835 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.047 | Tree loss: 1.688 | Accuracy: 0.511945 | 1.834 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.047 | Tree loss: 1.825 | Accuracy: 0.413500 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.431500 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 1.753 | Reg loss: 0.047 | Tree loss: 1.753 | Accuracy: 0.442500 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.455500 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.455500 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.047 | Tree loss: 1.748 | Accuracy: 0.461000 | 1.834 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.487500 | 1.833 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.500500 | 1.833 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.502000 | 1.833 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.047 | Tree loss: 1.696 | Accuracy: 0.516500 | 1.833 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.460751 | 1.833 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.047 | Tree loss: 1.815 | Accuracy: 0.439000 | 1.833 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.047 | Tree loss: 1.810 | Accuracy: 0.423500 | 1.832 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.047 | Tree loss: 1.780 | Accuracy: 0.436500 | 1.832 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.478500 | 1.832 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.451500 | 1.832 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.487000 | 1.832 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.468500 | 1.831 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.047 | Tree loss: 1.707 | Accuracy: 0.502000 | 1.831 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.487000 | 1.831 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.493500 | 1.831 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.047 | Tree loss: 1.674 | Accuracy: 0.535836 | 1.831 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.047 | Tree loss: 1.806 | Accuracy: 0.425500 | 1.831 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.428000 | 1.83 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.047 | Tree loss: 1.796 | Accuracy: 0.430000 | 1.83 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.458000 | 1.83 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.472000 | 1.83 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.494500 | 1.83 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.473000 | 1.829 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.047 | Tree loss: 1.724 | Accuracy: 0.504500 | 1.829 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.047 | Tree loss: 1.698 | Accuracy: 0.507000 | 1.829 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 1.706 | Reg loss: 0.047 | Tree loss: 1.706 | Accuracy: 0.486500 | 1.829 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.481229 | 1.829 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.047 | Tree loss: 1.807 | Accuracy: 0.424500 | 1.829 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.047 | Tree loss: 1.808 | Accuracy: 0.419000 | 1.828 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.047 | Tree loss: 1.782 | Accuracy: 0.427500 | 1.828 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.047 | Tree loss: 1.765 | Accuracy: 0.444500 | 1.828 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.471000 | 1.828 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.047 | Tree loss: 1.716 | Accuracy: 0.490500 | 1.828 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.483000 | 1.827 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.487500 | 1.827 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.506500 | 1.827 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.047 | Tree loss: 1.722 | Accuracy: 0.507000 | 1.827 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.047 | Tree loss: 1.677 | Accuracy: 0.501706 | 1.827 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.047 | Tree loss: 1.824 | Accuracy: 0.407000 | 1.827 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.047 | Tree loss: 1.805 | Accuracy: 0.434500 | 1.826 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.444000 | 1.826 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.462500 | 1.826 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.471500 | 1.826 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.476000 | 1.826 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.512000 | 1.825 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.047 | Tree loss: 1.710 | Accuracy: 0.514500 | 1.825 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.047 | Tree loss: 1.714 | Accuracy: 0.498000 | 1.825 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.477000 | 1.825 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.047 | Tree loss: 1.733 | Accuracy: 0.515358 | 1.824 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.047 | Tree loss: 1.845 | Accuracy: 0.411000 | 1.825 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.047 | Tree loss: 1.808 | Accuracy: 0.421500 | 1.824 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.047 | Tree loss: 1.782 | Accuracy: 0.410500 | 1.824 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.047 | Tree loss: 1.754 | Accuracy: 0.447500 | 1.824 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.047 | Tree loss: 1.765 | Accuracy: 0.443500 | 1.824 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.481000 | 1.824 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.502000 | 1.823 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.512000 | 1.823 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.047 | Tree loss: 1.711 | Accuracy: 0.494500 | 1.823 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.047 | Tree loss: 1.699 | Accuracy: 0.514000 | 1.823 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.047 | Tree loss: 1.682 | Accuracy: 0.474403 | 1.822 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.047 | Tree loss: 1.835 | Accuracy: 0.403500 | 1.823 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.047 | Tree loss: 1.820 | Accuracy: 0.416000 | 1.822 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.047 | Tree loss: 1.800 | Accuracy: 0.441500 | 1.822 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.047 | Tree loss: 1.767 | Accuracy: 0.457500 | 1.822 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.477500 | 1.822 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.483000 | 1.822 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.478500 | 1.821 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.047 | Tree loss: 1.688 | Accuracy: 0.513500 | 1.821 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.047 | Tree loss: 1.725 | Accuracy: 0.498500 | 1.821 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.505500 | 1.821 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.491468 | 1.82 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.047 | Tree loss: 1.796 | Accuracy: 0.422500 | 1.821 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.426000 | 1.82 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.414500 | 1.82 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.047 | Tree loss: 1.763 | Accuracy: 0.453500 | 1.82 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.442000 | 1.82 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.461500 | 1.82 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.047 | Tree loss: 1.732 | Accuracy: 0.488000 | 1.819 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.506000 | 1.819 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.047 | Tree loss: 1.696 | Accuracy: 0.518500 | 1.819 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.047 | Tree loss: 1.697 | Accuracy: 0.514000 | 1.819 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.047 | Tree loss: 1.695 | Accuracy: 0.525597 | 1.819 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 1.809 | Reg loss: 0.047 | Tree loss: 1.809 | Accuracy: 0.421000 | 1.819 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.047 | Tree loss: 1.809 | Accuracy: 0.416500 | 1.818 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 1.771 | Reg loss: 0.047 | Tree loss: 1.771 | Accuracy: 0.459000 | 1.818 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.047 | Tree loss: 1.770 | Accuracy: 0.461000 | 1.818 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.047 | Tree loss: 1.756 | Accuracy: 0.486000 | 1.818 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.510000 | 1.818 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.490000 | 1.817 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.515500 | 1.817 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.490000 | 1.817 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.047 | Tree loss: 1.705 | Accuracy: 0.495500 | 1.817 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.457338 | 1.816 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 1.809 | Reg loss: 0.047 | Tree loss: 1.809 | Accuracy: 0.416500 | 1.817 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.047 | Tree loss: 1.802 | Accuracy: 0.421500 | 1.816 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.047 | Tree loss: 1.791 | Accuracy: 0.434500 | 1.816 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.456000 | 1.816 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.460000 | 1.816 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.472500 | 1.815 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.047 | Tree loss: 1.721 | Accuracy: 0.498500 | 1.815 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.047 | Tree loss: 1.698 | Accuracy: 0.507000 | 1.815 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.504500 | 1.815 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.496500 | 1.815 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.047 | Tree loss: 1.679 | Accuracy: 0.529010 | 1.814 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.047 | Tree loss: 1.806 | Accuracy: 0.438000 | 1.814 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.440500 | 1.814 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.047 | Tree loss: 1.795 | Accuracy: 0.421500 | 1.814 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.458000 | 1.814 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.047 | Tree loss: 1.751 | Accuracy: 0.467000 | 1.814 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.047 | Tree loss: 1.735 | Accuracy: 0.489500 | 1.813 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.494500 | 1.813 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.489000 | 1.813 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.478500 | 1.813 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.047 | Tree loss: 1.720 | Accuracy: 0.506500 | 1.813 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.488055 | 1.812 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.047 | Tree loss: 1.827 | Accuracy: 0.414500 | 1.812 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.433000 | 1.812 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.047 | Tree loss: 1.804 | Accuracy: 0.414000 | 1.812 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.468000 | 1.812 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.047 | Tree loss: 1.750 | Accuracy: 0.462000 | 1.812 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.484000 | 1.811 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.496000 | 1.811 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.492500 | 1.811 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.484500 | 1.811 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.504000 | 1.811 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.470990 | 1.81 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.047 | Tree loss: 1.816 | Accuracy: 0.436000 | 1.811 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.439000 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.449500 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.047 | Tree loss: 1.760 | Accuracy: 0.442000 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.047 | Tree loss: 1.744 | Accuracy: 0.465500 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.460000 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.047 | Tree loss: 1.717 | Accuracy: 0.497500 | 1.81 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.047 | Tree loss: 1.693 | Accuracy: 0.532500 | 1.809 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.508000 | 1.809 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.047 | Tree loss: 1.728 | Accuracy: 0.503500 | 1.809 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.501706 | 1.809 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.047 | Tree loss: 1.845 | Accuracy: 0.409000 | 1.809 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.047 | Tree loss: 1.801 | Accuracy: 0.421000 | 1.809 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.047 | Tree loss: 1.783 | Accuracy: 0.431000 | 1.809 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.047 | Tree loss: 1.747 | Accuracy: 0.468500 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.471000 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.482500 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.047 | Tree loss: 1.694 | Accuracy: 0.527500 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.047 | Tree loss: 1.727 | Accuracy: 0.494500 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.047 | Tree loss: 1.709 | Accuracy: 0.483000 | 1.808 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.506500 | 1.807 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.529010 | 1.807 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.047 | Tree loss: 1.806 | Accuracy: 0.434500 | 1.807 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.047 | Tree loss: 1.831 | Accuracy: 0.416500 | 1.807 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.047 | Tree loss: 1.788 | Accuracy: 0.439000 | 1.807 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.462000 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.047 | Tree loss: 1.739 | Accuracy: 0.476000 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.500500 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.485000 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.047 | Tree loss: 1.708 | Accuracy: 0.518000 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.477000 | 1.806 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.491500 | 1.805 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.048 | Tree loss: 1.660 | Accuracy: 0.477816 | 1.805 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.436500 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.047 | Tree loss: 1.787 | Accuracy: 0.422000 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.047 | Tree loss: 1.794 | Accuracy: 0.418000 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.047 | Tree loss: 1.784 | Accuracy: 0.445000 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 1.713 | Reg loss: 0.047 | Tree loss: 1.713 | Accuracy: 0.475000 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.047 | Tree loss: 1.734 | Accuracy: 0.454000 | 1.805 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.498500 | 1.804 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.047 | Tree loss: 1.696 | Accuracy: 0.507500 | 1.804 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.486500 | 1.804 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.489000 | 1.804 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.467577 | 1.804 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.047 | Tree loss: 1.815 | Accuracy: 0.426000 | 1.804 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.438500 | 1.804 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.459000 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.047 | Tree loss: 1.759 | Accuracy: 0.450500 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.047 | Tree loss: 1.743 | Accuracy: 0.452500 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.047 | Tree loss: 1.737 | Accuracy: 0.473000 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.047 | Tree loss: 1.718 | Accuracy: 0.492500 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.491000 | 1.803 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.497500 | 1.802 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.508500 | 1.802 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.494881 | 1.802 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.420500 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 1.776 | Reg loss: 0.047 | Tree loss: 1.776 | Accuracy: 0.431000 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 1.779 | Reg loss: 0.047 | Tree loss: 1.779 | Accuracy: 0.440000 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.047 | Tree loss: 1.757 | Accuracy: 0.444000 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.479000 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.047 | Tree loss: 1.741 | Accuracy: 0.463000 | 1.802 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.047 | Tree loss: 1.731 | Accuracy: 0.484500 | 1.801 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.047 | Tree loss: 1.719 | Accuracy: 0.493000 | 1.801 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.502500 | 1.801 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.487000 | 1.801 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.048 | Tree loss: 1.657 | Accuracy: 0.525597 | 1.801 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.047 | Tree loss: 1.811 | Accuracy: 0.429000 | 1.801 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.047 | Tree loss: 1.793 | Accuracy: 0.421000 | 1.801 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.047 | Tree loss: 1.791 | Accuracy: 0.435000 | 1.801 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.047 | Tree loss: 1.752 | Accuracy: 0.463000 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.047 | Tree loss: 1.746 | Accuracy: 0.464000 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.047 | Tree loss: 1.715 | Accuracy: 0.486000 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.047 | Tree loss: 1.702 | Accuracy: 0.498000 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.502500 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.493000 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.497500 | 1.8 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.048 | Tree loss: 1.650 | Accuracy: 0.529010 | 1.799 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 1.821 | Reg loss: 0.047 | Tree loss: 1.821 | Accuracy: 0.417500 | 1.8 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.047 | Tree loss: 1.827 | Accuracy: 0.403500 | 1.8 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.431500 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.047 | Tree loss: 1.729 | Accuracy: 0.476000 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.462500 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 1.766 | Reg loss: 0.047 | Tree loss: 1.766 | Accuracy: 0.461000 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.047 | Tree loss: 1.703 | Accuracy: 0.492000 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.499000 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.510500 | 1.799 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.048 | Tree loss: 1.694 | Accuracy: 0.514500 | 1.798 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.508532 | 1.798 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.429000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.047 | Tree loss: 1.810 | Accuracy: 0.418000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.047 | Tree loss: 1.780 | Accuracy: 0.433500 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.047 | Tree loss: 1.740 | Accuracy: 0.455000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.047 | Tree loss: 1.758 | Accuracy: 0.472000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.460000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.047 | Tree loss: 1.712 | Accuracy: 0.478000 | 1.798 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.488500 | 1.797 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.502500 | 1.797 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.512000 | 1.797 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.470990 | 1.797 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.047 | Tree loss: 1.822 | Accuracy: 0.436000 | 1.797 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.047 | Tree loss: 1.781 | Accuracy: 0.437500 | 1.797 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.047 | Tree loss: 1.755 | Accuracy: 0.449500 | 1.797 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.440000 | 1.797 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.047 | Tree loss: 1.723 | Accuracy: 0.484500 | 1.797 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.047 | Tree loss: 1.726 | Accuracy: 0.481000 | 1.796 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.479500 | 1.796 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.488500 | 1.796 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.491500 | 1.796 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.482500 | 1.796 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.048 | Tree loss: 1.660 | Accuracy: 0.552901 | 1.796 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.047 | Tree loss: 1.798 | Accuracy: 0.437500 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.047 | Tree loss: 1.794 | Accuracy: 0.426500 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.047 | Tree loss: 1.756 | Accuracy: 0.450500 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.047 | Tree loss: 1.770 | Accuracy: 0.424500 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.047 | Tree loss: 1.730 | Accuracy: 0.473000 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.047 | Tree loss: 1.761 | Accuracy: 0.472000 | 1.795 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.476000 | 1.794 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.048 | Tree loss: 1.692 | Accuracy: 0.515500 | 1.794 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.480000 | 1.794 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.508000 | 1.794 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.457338 | 1.794 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.047 | Tree loss: 1.800 | Accuracy: 0.434500 | 1.794 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.047 | Tree loss: 1.820 | Accuracy: 0.411500 | 1.794 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.047 | Tree loss: 1.778 | Accuracy: 0.448000 | 1.794 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.047 | Tree loss: 1.745 | Accuracy: 0.447500 | 1.794 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.047 | Tree loss: 1.736 | Accuracy: 0.475500 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.493000 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.481500 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.504000 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.508000 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.506500 | 1.793 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.522184 | 1.792 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 1.801 | Reg loss: 0.047 | Tree loss: 1.801 | Accuracy: 0.425500 | 1.793 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 1.773 | Reg loss: 0.047 | Tree loss: 1.773 | Accuracy: 0.439500 | 1.793 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.047 | Tree loss: 1.756 | Accuracy: 0.450000 | 1.793 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.047 | Tree loss: 1.764 | Accuracy: 0.463000 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.047 | Tree loss: 1.749 | Accuracy: 0.459000 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.491000 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.484500 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.505500 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.481500 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.495000 | 1.792 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.508532 | 1.791 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.047 | Tree loss: 1.808 | Accuracy: 0.433000 | 1.792 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.047 | Tree loss: 1.813 | Accuracy: 0.431000 | 1.792 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.047 | Tree loss: 1.772 | Accuracy: 0.448000 | 1.792 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.047 | Tree loss: 1.769 | Accuracy: 0.447500 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.048 | Tree loss: 1.753 | Accuracy: 0.472000 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.480000 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.496500 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.487000 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.482500 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.048 | Tree loss: 1.687 | Accuracy: 0.508000 | 1.791 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.491468 | 1.79 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.047 | Tree loss: 1.807 | Accuracy: 0.438500 | 1.791 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 1.768 | Reg loss: 0.047 | Tree loss: 1.768 | Accuracy: 0.446000 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.047 | Tree loss: 1.797 | Accuracy: 0.418000 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.456500 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.480500 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.486500 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.493500 | 1.79 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.479000 | 1.789 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.511000 | 1.789 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.472000 | 1.789 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.484642 | 1.789 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.047 | Tree loss: 1.819 | Accuracy: 0.416000 | 1.789 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.047 | Tree loss: 1.789 | Accuracy: 0.428500 | 1.789 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.434000 | 1.789 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.469500 | 1.788 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.478000 | 1.788 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.505000 | 1.788 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.492500 | 1.788 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.509500 | 1.788 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.499500 | 1.787 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.473500 | 1.787 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.549488 | 1.787 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.047 | Tree loss: 1.822 | Accuracy: 0.435000 | 1.787 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.047 | Tree loss: 1.783 | Accuracy: 0.415500 | 1.787 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.452500 | 1.787 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.048 | Tree loss: 1.760 | Accuracy: 0.456500 | 1.787 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.048 | Tree loss: 1.743 | Accuracy: 0.483500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.505500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.048 | Tree loss: 1.743 | Accuracy: 0.488500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.499500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.496500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.492500 | 1.786 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.474403 | 1.785 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.048 | Tree loss: 1.791 | Accuracy: 0.455500 | 1.786 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.425500 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.048 | Tree loss: 1.782 | Accuracy: 0.430500 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.421500 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.466500 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.471000 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.500000 | 1.785 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.501000 | 1.784 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.505500 | 1.784 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.506000 | 1.784 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.515358 | 1.784 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 1.803 | Reg loss: 0.048 | Tree loss: 1.803 | Accuracy: 0.422500 | 1.784 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.433500 | 1.784 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.441500 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.487500 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.452500 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.470500 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.473000 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.503000 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.509000 | 1.783 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.507000 | 1.782 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.467577 | 1.782 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.048 | Tree loss: 1.806 | Accuracy: 0.428000 | 1.783 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.416500 | 1.783 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.450000 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.451500 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.458000 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.482000 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.469500 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.495000 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.508500 | 1.782 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.497500 | 1.781 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 1.591 | Reg loss: 0.048 | Tree loss: 1.591 | Accuracy: 0.587031 | 1.781 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.435500 | 1.782 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.422000 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.466500 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.451000 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.475000 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.480500 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.507000 | 1.781 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.498000 | 1.78 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.503500 | 1.78 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.490000 | 1.78 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.491468 | 1.78 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.048 | Tree loss: 1.796 | Accuracy: 0.432500 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.419500 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.437000 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.449000 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.478000 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.486000 | 1.78 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.508000 | 1.779 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.500500 | 1.779 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.498500 | 1.779 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.486500 | 1.779 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.474403 | 1.779 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.432000 | 1.779 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.048 | Tree loss: 1.810 | Accuracy: 0.405500 | 1.779 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.469000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.048 | Tree loss: 1.753 | Accuracy: 0.458000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.500000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.485000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 1.750 | Reg loss: 0.048 | Tree loss: 1.750 | Accuracy: 0.484000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.498000 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.505500 | 1.778 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.491000 | 1.777 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.464164 | 1.777 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.048 | Tree loss: 1.833 | Accuracy: 0.428500 | 1.778 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.426000 | 1.778 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.441000 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.468000 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.480000 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.467500 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.482000 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.509500 | 1.777 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.500000 | 1.776 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.498000 | 1.776 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.508532 | 1.776 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.048 | Tree loss: 1.822 | Accuracy: 0.422500 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.048 | Tree loss: 1.818 | Accuracy: 0.420000 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.451000 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.444500 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.468500 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.485000 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.505500 | 1.776 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.489500 | 1.775 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.522000 | 1.775 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.503500 | 1.775 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.048 | Tree loss: 1.656 | Accuracy: 0.576792 | 1.775 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 1.794 | Reg loss: 0.048 | Tree loss: 1.794 | Accuracy: 0.448500 | 1.775 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.048 | Tree loss: 1.792 | Accuracy: 0.421500 | 1.775 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.422500 | 1.775 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.048 | Tree loss: 1.764 | Accuracy: 0.440000 | 1.775 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.492000 | 1.775 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.500000 | 1.774 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.483000 | 1.774 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.517000 | 1.774 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.502500 | 1.774 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 1.743 | Reg loss: 0.048 | Tree loss: 1.743 | Accuracy: 0.490000 | 1.774 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.048 | Tree loss: 1.665 | Accuracy: 0.518771 | 1.774 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.048 | Tree loss: 1.817 | Accuracy: 0.418000 | 1.774 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.449000 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.429500 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.467000 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.476500 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.467000 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.492000 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.505000 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.497500 | 1.773 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.506000 | 1.772 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.048 | Tree loss: 1.671 | Accuracy: 0.529010 | 1.772 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.430500 | 1.773 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.430500 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.439000 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.463500 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.489500 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.457000 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.509000 | 1.772 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.501000 | 1.771 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.501500 | 1.771 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.505500 | 1.771 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 1.625 | Reg loss: 0.048 | Tree loss: 1.625 | Accuracy: 0.552901 | 1.771 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.437000 | 1.771 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.410000 | 1.771 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.048 | Tree loss: 1.761 | Accuracy: 0.451000 | 1.771 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.438000 | 1.771 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.479500 | 1.771 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.467500 | 1.77 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.488000 | 1.77 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.481500 | 1.77 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.497500 | 1.77 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.517000 | 1.77 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.048 | Tree loss: 1.673 | Accuracy: 0.522184 | 1.77 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.048 | Tree loss: 1.826 | Accuracy: 0.424000 | 1.77 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.436500 | 1.77 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.048 | Tree loss: 1.790 | Accuracy: 0.449000 | 1.77 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.048 | Tree loss: 1.762 | Accuracy: 0.462500 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.476000 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.500500 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.479500 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.489500 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.048 | Tree loss: 1.680 | Accuracy: 0.527000 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.493500 | 1.769 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.474403 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.432500 | 1.768 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.048 | Tree loss: 1.799 | Accuracy: 0.427500 | 1.768 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.048 | Tree loss: 1.759 | Accuracy: 0.442500 | 1.768 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.428000 | 1.768 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.483000 | 1.768 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.493000 | 1.767 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.505000 | 1.767 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.492000 | 1.767 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.499500 | 1.767 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.498000 | 1.767 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.470990 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.429000 | 1.767 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.451000 | 1.767 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.450000 | 1.767 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.451000 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.048 | Tree loss: 1.758 | Accuracy: 0.459500 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.487500 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.498000 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.514000 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.502500 | 1.766 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.481000 | 1.765 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.048 | Tree loss: 1.629 | Accuracy: 0.505119 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.048 | Tree loss: 1.817 | Accuracy: 0.436000 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.048 | Tree loss: 1.764 | Accuracy: 0.433000 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.454500 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.048 | Tree loss: 1.771 | Accuracy: 0.459000 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.473000 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.484500 | 1.765 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.479000 | 1.764 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.487000 | 1.764 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.522000 | 1.764 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.504500 | 1.764 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 1.599 | Reg loss: 0.048 | Tree loss: 1.599 | Accuracy: 0.566553 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.417000 | 1.764 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.419500 | 1.764 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.446500 | 1.764 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.454000 | 1.764 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.456500 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.500500 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.493500 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.498000 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.503500 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.501500 | 1.763 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.511945 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.048 | Tree loss: 1.806 | Accuracy: 0.437000 | 1.763 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.459500 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.048 | Tree loss: 1.792 | Accuracy: 0.448000 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.048 | Tree loss: 1.760 | Accuracy: 0.489000 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.483500 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.491500 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.481000 | 1.762 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.485500 | 1.761 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.509000 | 1.761 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.495000 | 1.761 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.498294 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.048 | Tree loss: 1.824 | Accuracy: 0.442000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.433000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 1.769 | Reg loss: 0.048 | Tree loss: 1.769 | Accuracy: 0.453000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.451000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.457500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.473000 | 1.76 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.492000 | 1.76 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.509500 | 1.76 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.505500 | 1.76 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.500500 | 1.76 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.477816 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 1.818 | Reg loss: 0.048 | Tree loss: 1.818 | Accuracy: 0.443000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.048 | Tree loss: 1.806 | Accuracy: 0.430000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.432000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.048 | Tree loss: 1.749 | Accuracy: 0.465000 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.465500 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.484000 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.048 | Tree loss: 1.683 | Accuracy: 0.523500 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.504500 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.504500 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.489000 | 1.759 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.048 | Tree loss: 1.640 | Accuracy: 0.542662 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.451500 | 1.759 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.048 | Tree loss: 1.792 | Accuracy: 0.436500 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.437500 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.048 | Tree loss: 1.762 | Accuracy: 0.443000 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.475000 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.483000 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.496000 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.513500 | 1.758 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.506500 | 1.757 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.506500 | 1.757 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 1.771 | Reg loss: 0.048 | Tree loss: 1.771 | Accuracy: 0.484642 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.440500 | 1.757 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.441500 | 1.757 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.460000 | 1.757 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.463500 | 1.757 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.480500 | 1.757 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.481500 | 1.756 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.497500 | 1.756 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.496000 | 1.756 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.493500 | 1.756 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.496000 | 1.756 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.467577 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.439000 | 1.756 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.048 | Tree loss: 1.809 | Accuracy: 0.439500 | 1.756 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.447000 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.467000 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.476500 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.506000 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.486500 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.475000 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.493000 | 1.755 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.048 | Tree loss: 1.676 | Accuracy: 0.499000 | 1.754 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.048 | Tree loss: 1.670 | Accuracy: 0.501706 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.048 | Tree loss: 1.796 | Accuracy: 0.446500 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.435500 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.428000 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.458500 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.487000 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.482500 | 1.754 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.477000 | 1.753 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.048 | Tree loss: 1.730 | Accuracy: 0.486000 | 1.753 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.494000 | 1.753 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.492000 | 1.753 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.508532 | 1.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.048 | Tree loss: 1.811 | Accuracy: 0.426500 | 1.753 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.449000 | 1.753 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.440500 | 1.753 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.048 | Tree loss: 1.759 | Accuracy: 0.454500 | 1.753 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.456000 | 1.753 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.489000 | 1.752 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.502500 | 1.752 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.500500 | 1.752 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.502000 | 1.752 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.503500 | 1.752 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.529010 | 1.752 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.443000 | 1.752 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.444000 | 1.752 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.412500 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.457000 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.469500 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.469500 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.506500 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.493000 | 1.751 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.489500 | 1.75 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.475500 | 1.75 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.546075 | 1.75 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.048 | Tree loss: 1.833 | Accuracy: 0.430000 | 1.75 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.423000 | 1.75 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.429500 | 1.75 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.048 | Tree loss: 1.759 | Accuracy: 0.469500 | 1.75 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.496000 | 1.75 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.509500 | 1.749 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.497500 | 1.749 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.492000 | 1.749 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.494500 | 1.749 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.505000 | 1.749 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.464164 | 1.749 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.048 | Tree loss: 1.828 | Accuracy: 0.415500 | 1.749 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.428500 | 1.749 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.048 | Tree loss: 1.796 | Accuracy: 0.423500 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.449500 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.502500 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.489000 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.510000 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.503000 | 1.748 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.048 | Tree loss: 1.682 | Accuracy: 0.515500 | 1.747 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.510000 | 1.747 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.491468 | 1.747 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.454500 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.429000 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.462000 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.048 | Tree loss: 1.761 | Accuracy: 0.456500 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.487500 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.491500 | 1.747 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.482000 | 1.746 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.491000 | 1.746 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.497000 | 1.746 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.517000 | 1.746 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.048 | Tree loss: 1.656 | Accuracy: 0.508532 | 1.746 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.439000 | 1.746 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.441000 | 1.746 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.435000 | 1.746 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.452500 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.472500 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.480500 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.509000 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.497500 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.509500 | 1.745 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.522000 | 1.744 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.481229 | 1.744 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.427000 | 1.744 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.048 | Tree loss: 1.808 | Accuracy: 0.432000 | 1.744 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.455500 | 1.744 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.454000 | 1.744 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.468500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.511500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.496500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.501500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.505500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.511500 | 1.743 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.048 | Tree loss: 1.692 | Accuracy: 0.494881 | 1.743 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.444000 | 1.743 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.436000 | 1.743 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.048 | Tree loss: 1.782 | Accuracy: 0.435500 | 1.743 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.452000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.475000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.499000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.509000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.504000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.500000 | 1.742 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.488500 | 1.741 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.535836 | 1.741 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.048 | Tree loss: 1.823 | Accuracy: 0.438000 | 1.742 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.452000 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.450000 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.048 | Tree loss: 1.772 | Accuracy: 0.456000 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.472500 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.474500 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.500500 | 1.741 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.505500 | 1.74 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.510000 | 1.74 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.499500 | 1.74 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.048 | Tree loss: 1.679 | Accuracy: 0.515358 | 1.74 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 1.837 | Reg loss: 0.048 | Tree loss: 1.837 | Accuracy: 0.429500 | 1.74 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.048 | Tree loss: 1.809 | Accuracy: 0.426000 | 1.74 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.454000 | 1.74 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.469000 | 1.74 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.461500 | 1.74 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.502500 | 1.739 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.491500 | 1.739 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.496500 | 1.739 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.527000 | 1.739 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.496000 | 1.739 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.481229 | 1.739 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.432000 | 1.739 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.456000 | 1.739 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.429000 | 1.739 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.048 | Tree loss: 1.764 | Accuracy: 0.449000 | 1.739 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.477000 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.492500 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.503000 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.512000 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.504500 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.514000 | 1.738 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.048 | Tree loss: 1.674 | Accuracy: 0.481229 | 1.738 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.048 | Tree loss: 1.823 | Accuracy: 0.429500 | 1.738 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.439000 | 1.738 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.433000 | 1.738 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.449500 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.465500 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.483000 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.504000 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.518000 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.498000 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.048 | Tree loss: 1.685 | Accuracy: 0.510500 | 1.737 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.474403 | 1.737 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.048 | Tree loss: 1.824 | Accuracy: 0.426500 | 1.737 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.445000 | 1.737 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.450000 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.446000 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.488500 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.484500 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.504000 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.518500 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.502000 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.509500 | 1.736 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.477816 | 1.735 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.433000 | 1.736 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 1.790 | Reg loss: 0.048 | Tree loss: 1.790 | Accuracy: 0.448000 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.465500 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.048 | Tree loss: 1.758 | Accuracy: 0.457000 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.485500 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.490500 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.483000 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 1.743 | Reg loss: 0.048 | Tree loss: 1.743 | Accuracy: 0.486000 | 1.735 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.048 | Tree loss: 1.692 | Accuracy: 0.499000 | 1.734 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.048 | Tree loss: 1.674 | Accuracy: 0.521000 | 1.734 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.048 | Tree loss: 1.644 | Accuracy: 0.573379 | 1.734 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.048 | Tree loss: 1.799 | Accuracy: 0.434500 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.409000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.475000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.464000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 1.772 | Reg loss: 0.048 | Tree loss: 1.772 | Accuracy: 0.454000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.048 | Tree loss: 1.749 | Accuracy: 0.458000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.509000 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.499500 | 1.734 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.516500 | 1.733 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.048 | Tree loss: 1.688 | Accuracy: 0.505500 | 1.733 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.048 | Tree loss: 1.668 | Accuracy: 0.580205 | 1.733 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.441500 | 1.733 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 1.812 | Reg loss: 0.048 | Tree loss: 1.812 | Accuracy: 0.424000 | 1.733 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.447000 | 1.733 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.048 | Tree loss: 1.762 | Accuracy: 0.463500 | 1.733 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.488500 | 1.733 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.503000 | 1.732 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.484000 | 1.732 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.048 | Tree loss: 1.685 | Accuracy: 0.517500 | 1.732 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.505000 | 1.732 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.513000 | 1.732 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.494881 | 1.732 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.423000 | 1.732 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.048 | Tree loss: 1.810 | Accuracy: 0.424500 | 1.732 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.459500 | 1.732 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.443000 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.470000 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.475500 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.503000 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.496500 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.507000 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.510000 | 1.731 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.048 | Tree loss: 1.663 | Accuracy: 0.535836 | 1.73 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.430500 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.435500 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.451500 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.463500 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.470000 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.517000 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.473500 | 1.73 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.505000 | 1.729 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.516500 | 1.729 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.493500 | 1.729 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 1.659 | Reg loss: 0.048 | Tree loss: 1.659 | Accuracy: 0.525597 | 1.729 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.448000 | 1.729 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.442500 | 1.729 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.439500 | 1.729 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.048 | Tree loss: 1.764 | Accuracy: 0.459500 | 1.729 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.480000 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.489500 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.512500 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.501000 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.500000 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.494000 | 1.728 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.048 | Tree loss: 1.655 | Accuracy: 0.563140 | 1.727 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.429000 | 1.728 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.048 | Tree loss: 1.797 | Accuracy: 0.440000 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 1.769 | Reg loss: 0.048 | Tree loss: 1.769 | Accuracy: 0.433500 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.449000 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.499000 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.479500 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.496500 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.495500 | 1.727 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.510500 | 1.726 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.525000 | 1.726 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.467577 | 1.726 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.048 | Tree loss: 1.827 | Accuracy: 0.424500 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.431000 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.454500 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.459000 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.485500 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.048 | Tree loss: 1.759 | Accuracy: 0.464500 | 1.726 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.048 | Tree loss: 1.692 | Accuracy: 0.517500 | 1.725 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.048 | Tree loss: 1.687 | Accuracy: 0.517500 | 1.725 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.486500 | 1.725 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.496000 | 1.725 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 1.620 | Reg loss: 0.048 | Tree loss: 1.620 | Accuracy: 0.511945 | 1.725 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.048 | Tree loss: 1.827 | Accuracy: 0.416500 | 1.725 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.048 | Tree loss: 1.796 | Accuracy: 0.447000 | 1.725 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.452500 | 1.725 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.048 | Tree loss: 1.753 | Accuracy: 0.455000 | 1.725 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.471500 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.489500 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.509000 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.495500 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.500000 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.048 | Tree loss: 1.670 | Accuracy: 0.544500 | 1.724 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 1.645 | Reg loss: 0.048 | Tree loss: 1.645 | Accuracy: 0.511945 | 1.723 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.048 | Tree loss: 1.819 | Accuracy: 0.431500 | 1.724 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.438500 | 1.724 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.451500 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.048 | Tree loss: 1.763 | Accuracy: 0.453500 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.479500 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.048 | Tree loss: 1.733 | Accuracy: 0.482000 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.494500 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.526000 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.491000 | 1.723 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.514000 | 1.722 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.048 | Tree loss: 1.666 | Accuracy: 0.522184 | 1.722 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.048 | Tree loss: 1.833 | Accuracy: 0.429000 | 1.722 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.048 | Tree loss: 1.808 | Accuracy: 0.413000 | 1.722 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.463000 | 1.722 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.048 | Tree loss: 1.750 | Accuracy: 0.461500 | 1.722 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.444000 | 1.722 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.487500 | 1.721 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.500500 | 1.721 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.501000 | 1.721 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.512500 | 1.721 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.522000 | 1.721 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 1.648 | Reg loss: 0.048 | Tree loss: 1.648 | Accuracy: 0.511945 | 1.721 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.429500 | 1.721 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.048 | Tree loss: 1.779 | Accuracy: 0.459000 | 1.721 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.456000 | 1.721 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.479000 | 1.721 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.483500 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.490500 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.048 | Tree loss: 1.730 | Accuracy: 0.503000 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.492500 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.494000 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.521500 | 1.72 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.048 | Tree loss: 1.646 | Accuracy: 0.546075 | 1.72 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 1.810 | Reg loss: 0.048 | Tree loss: 1.810 | Accuracy: 0.429000 | 1.72 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.048 | Tree loss: 1.820 | Accuracy: 0.413500 | 1.72 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.444500 | 1.72 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.048 | Tree loss: 1.760 | Accuracy: 0.445500 | 1.72 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.471500 | 1.72 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.495000 | 1.719 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.048 | Tree loss: 1.743 | Accuracy: 0.498000 | 1.719 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.514000 | 1.719 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.504500 | 1.719 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.048 | Tree loss: 1.689 | Accuracy: 0.508500 | 1.719 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.491468 | 1.719 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.441500 | 1.719 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.424000 | 1.719 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.435500 | 1.719 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.441000 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.467500 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.489500 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.481500 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.500000 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.048 | Tree loss: 1.665 | Accuracy: 0.516000 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.509000 | 1.718 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.048 | Tree loss: 1.688 | Accuracy: 0.546075 | 1.718 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.433000 | 1.718 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.048 | Tree loss: 1.809 | Accuracy: 0.423000 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.048 | Tree loss: 1.786 | Accuracy: 0.451000 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.446000 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.501000 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.491000 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.503500 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.511500 | 1.717 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.501500 | 1.716 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.048 | Tree loss: 1.681 | Accuracy: 0.522000 | 1.716 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.048 | Tree loss: 1.687 | Accuracy: 0.501706 | 1.716 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.437000 | 1.716 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.048 | Tree loss: 1.806 | Accuracy: 0.440500 | 1.716 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.463000 | 1.716 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.048 | Tree loss: 1.760 | Accuracy: 0.445500 | 1.716 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.476500 | 1.716 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.501000 | 1.715 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.494500 | 1.715 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.496500 | 1.715 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.518000 | 1.715 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.503000 | 1.715 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.048 | Tree loss: 1.689 | Accuracy: 0.529010 | 1.715 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.048 | Tree loss: 1.832 | Accuracy: 0.434000 | 1.715 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.048 | Tree loss: 1.792 | Accuracy: 0.421000 | 1.715 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.461500 | 1.715 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.450500 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.466500 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.487500 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.478000 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.510000 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.486500 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.499000 | 1.714 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.542662 | 1.713 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.436500 | 1.714 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.446000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.441000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.048 | Tree loss: 1.763 | Accuracy: 0.456000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.472000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.488500 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.515000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.504000 | 1.713 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.497000 | 1.712 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.521500 | 1.712 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.511945 | 1.712 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.439000 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.048 | Tree loss: 1.820 | Accuracy: 0.413500 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.448500 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.438000 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.469500 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 1.694 | Reg loss: 0.048 | Tree loss: 1.694 | Accuracy: 0.511000 | 1.712 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.489000 | 1.711 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.497500 | 1.711 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.506500 | 1.711 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.514500 | 1.711 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.542662 | 1.711 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.424500 | 1.711 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.453000 | 1.711 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.048 | Tree loss: 1.776 | Accuracy: 0.434500 | 1.711 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.447000 | 1.711 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.048 | Tree loss: 1.740 | Accuracy: 0.465500 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.484000 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.503500 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.515000 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.500000 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.500500 | 1.71 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 1.750 | Reg loss: 0.048 | Tree loss: 1.750 | Accuracy: 0.484642 | 1.71 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.048 | Tree loss: 1.791 | Accuracy: 0.439500 | 1.71 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.444500 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.447500 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.473500 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.048 | Tree loss: 1.758 | Accuracy: 0.470500 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.485000 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.492000 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.490000 | 1.709 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.498000 | 1.708 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.516500 | 1.708 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.048 | Tree loss: 1.680 | Accuracy: 0.525597 | 1.708 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.456500 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.447000 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.048 | Tree loss: 1.778 | Accuracy: 0.442000 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.048 | Tree loss: 1.761 | Accuracy: 0.446500 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.445500 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.488500 | 1.708 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.506500 | 1.707 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.532000 | 1.707 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.513500 | 1.707 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.484000 | 1.707 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.048 | Tree loss: 1.679 | Accuracy: 0.508532 | 1.707 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.048 | Tree loss: 1.796 | Accuracy: 0.426000 | 1.707 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.048 | Tree loss: 1.799 | Accuracy: 0.415500 | 1.707 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.447500 | 1.707 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.460000 | 1.707 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.480000 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.514500 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.496000 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.499500 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.498000 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.510500 | 1.706 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 1.623 | Reg loss: 0.048 | Tree loss: 1.623 | Accuracy: 0.552901 | 1.706 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.048 | Tree loss: 1.829 | Accuracy: 0.420000 | 1.706 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.425000 | 1.706 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.448000 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.457500 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.461500 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.496500 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.504000 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.496500 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.516000 | 1.705 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.514000 | 1.704 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.511945 | 1.704 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.431500 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.434500 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.048 | Tree loss: 1.782 | Accuracy: 0.434000 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.469500 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.457500 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.048 | Tree loss: 1.706 | Accuracy: 0.510000 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.510000 | 1.704 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.508000 | 1.703 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.495500 | 1.703 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.048 | Tree loss: 1.687 | Accuracy: 0.521000 | 1.703 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.481229 | 1.703 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.440000 | 1.703 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.432000 | 1.703 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.442500 | 1.703 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.450500 | 1.703 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.450500 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.482000 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.508500 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.478000 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.528500 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.498500 | 1.702 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.488055 | 1.702 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.048 | Tree loss: 1.811 | Accuracy: 0.425500 | 1.702 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.434000 | 1.702 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.446000 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.048 | Tree loss: 1.749 | Accuracy: 0.467000 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.048 | Tree loss: 1.744 | Accuracy: 0.481000 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.510500 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.483500 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.509500 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.482500 | 1.701 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.048 | Tree loss: 1.675 | Accuracy: 0.510500 | 1.7 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.048 | Tree loss: 1.685 | Accuracy: 0.539249 | 1.7 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.422000 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.454000 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.460500 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.435000 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.467000 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.048 | Tree loss: 1.753 | Accuracy: 0.465500 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.491000 | 1.7 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.502000 | 1.699 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.516000 | 1.699 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.048 | Tree loss: 1.687 | Accuracy: 0.530500 | 1.699 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.048 | Tree loss: 1.651 | Accuracy: 0.549488 | 1.699 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.048 | Tree loss: 1.841 | Accuracy: 0.418500 | 1.699 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.048 | Tree loss: 1.772 | Accuracy: 0.437000 | 1.699 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.048 | Tree loss: 1.778 | Accuracy: 0.453000 | 1.699 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.048 | Tree loss: 1.750 | Accuracy: 0.463000 | 1.699 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.464000 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.474500 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.493500 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.493000 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.048 | Tree loss: 1.680 | Accuracy: 0.506500 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.048 | Tree loss: 1.693 | Accuracy: 0.511000 | 1.698 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 1.615 | Reg loss: 0.048 | Tree loss: 1.615 | Accuracy: 0.563140 | 1.698 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.440500 | 1.698 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.048 | Tree loss: 1.794 | Accuracy: 0.445000 | 1.698 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 1.771 | Reg loss: 0.048 | Tree loss: 1.771 | Accuracy: 0.454500 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.443000 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.487000 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.475500 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.487500 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.504500 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.500000 | 1.697 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.506000 | 1.696 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.498294 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.443000 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.432500 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.048 | Tree loss: 1.776 | Accuracy: 0.439000 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.048 | Tree loss: 1.750 | Accuracy: 0.450500 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.480500 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.048 | Tree loss: 1.708 | Accuracy: 0.476500 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.483000 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.495000 | 1.696 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.510500 | 1.695 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.504500 | 1.695 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.048 | Tree loss: 1.686 | Accuracy: 0.549488 | 1.695 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.431500 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.455000 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.433000 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.048 | Tree loss: 1.760 | Accuracy: 0.440500 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.475000 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.491500 | 1.695 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.048 | Tree loss: 1.677 | Accuracy: 0.518500 | 1.694 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.490500 | 1.694 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.491500 | 1.694 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.508500 | 1.694 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.467577 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.433500 | 1.694 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.048 | Tree loss: 1.801 | Accuracy: 0.430500 | 1.694 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.443000 | 1.694 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.048 | Tree loss: 1.758 | Accuracy: 0.454500 | 1.694 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.476000 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.509500 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.484000 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.505500 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.483000 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.498500 | 1.693 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.453925 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.048 | Tree loss: 1.812 | Accuracy: 0.424500 | 1.693 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.048 | Tree loss: 1.810 | Accuracy: 0.428500 | 1.693 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.452000 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.476000 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.495500 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.482000 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.492500 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.496000 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.509500 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.048 | Tree loss: 1.694 | Accuracy: 0.527500 | 1.692 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.488055 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.048 | Tree loss: 1.817 | Accuracy: 0.435500 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 1.769 | Reg loss: 0.048 | Tree loss: 1.769 | Accuracy: 0.445000 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.449500 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.474000 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.481500 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.492500 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.498000 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.485500 | 1.691 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.513000 | 1.69 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.500500 | 1.69 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.048 | Tree loss: 1.649 | Accuracy: 0.576792 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 1.783 | Reg loss: 0.048 | Tree loss: 1.783 | Accuracy: 0.444000 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.432000 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.445500 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.465000 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.490000 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.499000 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.048 | Tree loss: 1.707 | Accuracy: 0.515500 | 1.69 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.486500 | 1.689 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.048 | Tree loss: 1.688 | Accuracy: 0.510500 | 1.689 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.511000 | 1.689 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.048 | Tree loss: 1.653 | Accuracy: 0.498294 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.048 | Tree loss: 1.806 | Accuracy: 0.434000 | 1.689 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.048 | Tree loss: 1.767 | Accuracy: 0.453500 | 1.689 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.450500 | 1.689 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.048 | Tree loss: 1.771 | Accuracy: 0.454000 | 1.689 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.475500 | 1.689 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.048 | Tree loss: 1.717 | Accuracy: 0.499500 | 1.688 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.506000 | 1.688 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.500500 | 1.688 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.048 | Tree loss: 1.691 | Accuracy: 0.506500 | 1.688 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.505500 | 1.688 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 1.606 | Reg loss: 0.048 | Tree loss: 1.606 | Accuracy: 0.539249 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.429000 | 1.688 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.048 | Tree loss: 1.787 | Accuracy: 0.454000 | 1.688 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.048 | Tree loss: 1.741 | Accuracy: 0.463500 | 1.688 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.048 | Tree loss: 1.772 | Accuracy: 0.454000 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.474000 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.508500 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.492000 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.500000 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.048 | Tree loss: 1.701 | Accuracy: 0.509000 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.048 | Tree loss: 1.709 | Accuracy: 0.507500 | 1.687 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.498294 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.048 | Tree loss: 1.800 | Accuracy: 0.435000 | 1.687 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.048 | Tree loss: 1.802 | Accuracy: 0.441000 | 1.687 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 1.769 | Reg loss: 0.048 | Tree loss: 1.769 | Accuracy: 0.436500 | 1.687 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.469500 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.480000 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.503500 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.048 | Tree loss: 1.692 | Accuracy: 0.496000 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.497000 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.509500 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.048 | Tree loss: 1.698 | Accuracy: 0.515000 | 1.686 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.470990 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.048 | Tree loss: 1.808 | Accuracy: 0.443500 | 1.686 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.460500 | 1.686 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.048 | Tree loss: 1.761 | Accuracy: 0.462000 | 1.686 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.458500 | 1.686 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.467500 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.513000 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.494000 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.498500 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.048 | Tree loss: 1.722 | Accuracy: 0.513500 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.048 | Tree loss: 1.716 | Accuracy: 0.499000 | 1.685 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.048 | Tree loss: 1.670 | Accuracy: 0.556314 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.445500 | 1.685 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.455000 | 1.685 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.048 | Tree loss: 1.789 | Accuracy: 0.456500 | 1.685 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.048 | Tree loss: 1.771 | Accuracy: 0.458500 | 1.685 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.472500 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.483000 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.501000 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.500500 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.048 | Tree loss: 1.689 | Accuracy: 0.517500 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.048 | Tree loss: 1.700 | Accuracy: 0.504000 | 1.684 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.505119 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 1.803 | Reg loss: 0.048 | Tree loss: 1.803 | Accuracy: 0.434000 | 1.684 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.048 | Tree loss: 1.779 | Accuracy: 0.436000 | 1.684 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.426500 | 1.684 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.048 | Tree loss: 1.756 | Accuracy: 0.456500 | 1.684 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.466500 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.500500 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.048 | Tree loss: 1.730 | Accuracy: 0.486000 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.525500 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.513000 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.048 | Tree loss: 1.704 | Accuracy: 0.503000 | 1.683 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.048 | Tree loss: 1.651 | Accuracy: 0.549488 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.420500 | 1.683 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 1.776 | Reg loss: 0.048 | Tree loss: 1.776 | Accuracy: 0.440500 | 1.683 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.449500 | 1.683 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.048 | Tree loss: 1.734 | Accuracy: 0.481500 | 1.683 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.048 | Tree loss: 1.746 | Accuracy: 0.471500 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.479000 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.499500 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.492000 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.516000 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.502000 | 1.682 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.048 | Tree loss: 1.677 | Accuracy: 0.515358 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 1.795 | Reg loss: 0.048 | Tree loss: 1.795 | Accuracy: 0.430000 | 1.682 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.439000 | 1.682 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.479000 | 1.682 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.048 | Tree loss: 1.770 | Accuracy: 0.458500 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.483500 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.048 | Tree loss: 1.729 | Accuracy: 0.488500 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.492000 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.486000 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.493000 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.048 | Tree loss: 1.699 | Accuracy: 0.521500 | 1.681 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 1.631 | Reg loss: 0.048 | Tree loss: 1.631 | Accuracy: 0.539249 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.048 | Tree loss: 1.812 | Accuracy: 0.439000 | 1.681 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.442500 | 1.681 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.440500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.048 | Tree loss: 1.759 | Accuracy: 0.456000 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.048 | Tree loss: 1.718 | Accuracy: 0.489500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.469500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.508000 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.048 | Tree loss: 1.710 | Accuracy: 0.501500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.048 | Tree loss: 1.688 | Accuracy: 0.518500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.048 | Tree loss: 1.715 | Accuracy: 0.494500 | 1.68 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.048 | Tree loss: 1.673 | Accuracy: 0.539249 | 1.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.413000 | 1.68 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.048 | Tree loss: 1.814 | Accuracy: 0.426000 | 1.68 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.048 | Tree loss: 1.752 | Accuracy: 0.457000 | 1.68 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.452000 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.483500 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.477000 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.048 | Tree loss: 1.714 | Accuracy: 0.491000 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.048 | Tree loss: 1.689 | Accuracy: 0.514000 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.048 | Tree loss: 1.684 | Accuracy: 0.529000 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.504500 | 1.679 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.474403 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 1.818 | Reg loss: 0.048 | Tree loss: 1.818 | Accuracy: 0.430000 | 1.679 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.440000 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.440000 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.454500 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.456500 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.048 | Tree loss: 1.702 | Accuracy: 0.493500 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.494500 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.048 | Tree loss: 1.690 | Accuracy: 0.502000 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.509000 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.506500 | 1.678 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.048 | Tree loss: 1.642 | Accuracy: 0.556314 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.048 | Tree loss: 1.813 | Accuracy: 0.431000 | 1.678 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 1.798 | Reg loss: 0.048 | Tree loss: 1.798 | Accuracy: 0.442000 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.048 | Tree loss: 1.761 | Accuracy: 0.469500 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.048 | Tree loss: 1.719 | Accuracy: 0.493000 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.485000 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.048 | Tree loss: 1.725 | Accuracy: 0.499500 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.048 | Tree loss: 1.695 | Accuracy: 0.504500 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.048 | Tree loss: 1.705 | Accuracy: 0.502500 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.477000 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.497000 | 1.677 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.049 | Tree loss: 1.712 | Accuracy: 0.515358 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.436000 | 1.677 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.048 | Tree loss: 1.788 | Accuracy: 0.438000 | 1.677 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.439000 | 1.677 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.048 | Tree loss: 1.765 | Accuracy: 0.440500 | 1.677 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.480000 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.492000 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.520000 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.048 | Tree loss: 1.697 | Accuracy: 0.493000 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.048 | Tree loss: 1.689 | Accuracy: 0.519000 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.049 | Tree loss: 1.693 | Accuracy: 0.510500 | 1.676 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.049 | Tree loss: 1.713 | Accuracy: 0.501706 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.048 | Tree loss: 1.823 | Accuracy: 0.441000 | 1.676 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 1.807 | Reg loss: 0.048 | Tree loss: 1.807 | Accuracy: 0.421000 | 1.676 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 1.757 | Reg loss: 0.048 | Tree loss: 1.757 | Accuracy: 0.450000 | 1.676 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.048 | Tree loss: 1.721 | Accuracy: 0.478000 | 1.676 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.048 | Tree loss: 1.730 | Accuracy: 0.470000 | 1.676 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.048 | Tree loss: 1.727 | Accuracy: 0.484000 | 1.675 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.048 | Tree loss: 1.731 | Accuracy: 0.501000 | 1.675 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.048 | Tree loss: 1.703 | Accuracy: 0.501000 | 1.675 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.503500 | 1.675 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.510000 | 1.675 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.049 | Tree loss: 1.690 | Accuracy: 0.494881 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.463500 | 1.675 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.048 | Tree loss: 1.794 | Accuracy: 0.444000 | 1.675 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.048 | Tree loss: 1.777 | Accuracy: 0.448500 | 1.675 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.048 | Tree loss: 1.748 | Accuracy: 0.459000 | 1.675 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.048 | Tree loss: 1.711 | Accuracy: 0.488500 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.048 | Tree loss: 1.739 | Accuracy: 0.477500 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.498500 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.048 | Tree loss: 1.694 | Accuracy: 0.514000 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.501500 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.504000 | 1.674 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.049 | Tree loss: 1.677 | Accuracy: 0.532423 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.446500 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.048 | Tree loss: 1.822 | Accuracy: 0.415500 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 1.773 | Reg loss: 0.048 | Tree loss: 1.773 | Accuracy: 0.438000 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.048 | Tree loss: 1.735 | Accuracy: 0.465000 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.465000 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.048 | Tree loss: 1.696 | Accuracy: 0.507000 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.490000 | 1.674 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.048 | Tree loss: 1.726 | Accuracy: 0.502500 | 1.673 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.510000 | 1.673 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.049 | Tree loss: 1.696 | Accuracy: 0.504000 | 1.673 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.505119 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.434000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.422000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.048 | Tree loss: 1.774 | Accuracy: 0.440500 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.461000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.048 | Tree loss: 1.745 | Accuracy: 0.461000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.489000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.048 | Tree loss: 1.712 | Accuracy: 0.493000 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.049 | Tree loss: 1.714 | Accuracy: 0.492500 | 1.673 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.515000 | 1.672 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.049 | Tree loss: 1.693 | Accuracy: 0.506000 | 1.672 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.049 | Tree loss: 1.651 | Accuracy: 0.515358 | 1.672 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 1.816 | Reg loss: 0.048 | Tree loss: 1.816 | Accuracy: 0.426000 | 1.673 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.048 | Tree loss: 1.794 | Accuracy: 0.439000 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.048 | Tree loss: 1.754 | Accuracy: 0.455000 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 1.737 | Reg loss: 0.048 | Tree loss: 1.737 | Accuracy: 0.477000 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.048 | Tree loss: 1.720 | Accuracy: 0.495000 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.048 | Tree loss: 1.732 | Accuracy: 0.486500 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.048 | Tree loss: 1.736 | Accuracy: 0.478500 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.507500 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.500500 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.509000 | 1.672 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.049 | Tree loss: 1.684 | Accuracy: 0.491468 | 1.672 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.048 | Tree loss: 1.817 | Accuracy: 0.444000 | 1.672 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.048 | Tree loss: 1.780 | Accuracy: 0.446500 | 1.672 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.048 | Tree loss: 1.781 | Accuracy: 0.424500 | 1.672 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.048 | Tree loss: 1.751 | Accuracy: 0.463000 | 1.672 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.048 | Tree loss: 1.742 | Accuracy: 0.454000 | 1.672 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.048 | Tree loss: 1.723 | Accuracy: 0.479000 | 1.671 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.049 | Tree loss: 1.686 | Accuracy: 0.520000 | 1.671 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.495500 | 1.671 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.049 | Tree loss: 1.690 | Accuracy: 0.518500 | 1.671 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.049 | Tree loss: 1.733 | Accuracy: 0.501500 | 1.671 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.049 | Tree loss: 1.666 | Accuracy: 0.559727 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.454000 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.448000 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.048 | Tree loss: 1.792 | Accuracy: 0.438500 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.048 | Tree loss: 1.753 | Accuracy: 0.466000 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 1.713 | Reg loss: 0.048 | Tree loss: 1.713 | Accuracy: 0.494500 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.490500 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.501500 | 1.671 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.504500 | 1.67 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.494500 | 1.67 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.049 | Tree loss: 1.730 | Accuracy: 0.484500 | 1.67 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.491468 | 1.67 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 1.805 | Reg loss: 0.048 | Tree loss: 1.805 | Accuracy: 0.444500 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.048 | Tree loss: 1.779 | Accuracy: 0.455000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.048 | Tree loss: 1.772 | Accuracy: 0.457000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.048 | Tree loss: 1.747 | Accuracy: 0.466000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.048 | Tree loss: 1.728 | Accuracy: 0.472000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.479500 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.482000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.049 | Tree loss: 1.698 | Accuracy: 0.505000 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.049 | Tree loss: 1.715 | Accuracy: 0.494500 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.513500 | 1.67 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.049 | Tree loss: 1.730 | Accuracy: 0.467577 | 1.669 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 1.790 | Reg loss: 0.048 | Tree loss: 1.790 | Accuracy: 0.450500 | 1.67 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.048 | Tree loss: 1.808 | Accuracy: 0.431000 | 1.67 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.048 | Tree loss: 1.784 | Accuracy: 0.433000 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 1.738 | Reg loss: 0.048 | Tree loss: 1.738 | Accuracy: 0.469500 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 1.724 | Reg loss: 0.048 | Tree loss: 1.724 | Accuracy: 0.498000 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.049 | Tree loss: 1.715 | Accuracy: 0.502000 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.486500 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.491000 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.498000 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.049 | Tree loss: 1.691 | Accuracy: 0.508500 | 1.669 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.049 | Tree loss: 1.737 | Accuracy: 0.505119 | 1.669 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.048 | Tree loss: 1.791 | Accuracy: 0.442000 | 1.669 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 1.782 | Reg loss: 0.048 | Tree loss: 1.782 | Accuracy: 0.441500 | 1.669 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.048 | Tree loss: 1.755 | Accuracy: 0.474500 | 1.669 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.048 | Tree loss: 1.766 | Accuracy: 0.466500 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.049 | Tree loss: 1.739 | Accuracy: 0.472000 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.049 | Tree loss: 1.717 | Accuracy: 0.496000 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.049 | Tree loss: 1.727 | Accuracy: 0.488500 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.049 | Tree loss: 1.715 | Accuracy: 0.509000 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.506500 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.049 | Tree loss: 1.699 | Accuracy: 0.506000 | 1.668 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 1.636 | Reg loss: 0.049 | Tree loss: 1.636 | Accuracy: 0.580205 | 1.668 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.048 | Tree loss: 1.815 | Accuracy: 0.427000 | 1.668 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 1.785 | Reg loss: 0.048 | Tree loss: 1.785 | Accuracy: 0.444000 | 1.668 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.048 | Tree loss: 1.730 | Accuracy: 0.474000 | 1.668 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.049 | Tree loss: 1.745 | Accuracy: 0.459000 | 1.668 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 1.727 | Reg loss: 0.049 | Tree loss: 1.727 | Accuracy: 0.470000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.049 | Tree loss: 1.733 | Accuracy: 0.493000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.049 | Tree loss: 1.731 | Accuracy: 0.491000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.049 | Tree loss: 1.710 | Accuracy: 0.501000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.049 | Tree loss: 1.701 | Accuracy: 0.525000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.513000 | 1.667 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 1.736 | Reg loss: 0.049 | Tree loss: 1.736 | Accuracy: 0.484642 | 1.667 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.048 | Tree loss: 1.804 | Accuracy: 0.434500 | 1.667 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.048 | Tree loss: 1.775 | Accuracy: 0.434500 | 1.667 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.049 | Tree loss: 1.785 | Accuracy: 0.441000 | 1.667 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.049 | Tree loss: 1.742 | Accuracy: 0.478000 | 1.667 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.481000 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.483000 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.049 | Tree loss: 1.720 | Accuracy: 0.490000 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.514000 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.514500 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.049 | Tree loss: 1.727 | Accuracy: 0.501000 | 1.666 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.522184 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.048 | Tree loss: 1.799 | Accuracy: 0.444500 | 1.666 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.048 | Tree loss: 1.793 | Accuracy: 0.440000 | 1.666 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.049 | Tree loss: 1.767 | Accuracy: 0.447500 | 1.666 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.049 | Tree loss: 1.757 | Accuracy: 0.466500 | 1.666 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 1.724 | Reg loss: 0.049 | Tree loss: 1.724 | Accuracy: 0.485000 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.049 | Tree loss: 1.740 | Accuracy: 0.475500 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.513000 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.516500 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.049 | Tree loss: 1.701 | Accuracy: 0.503000 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.525000 | 1.665 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.049 | Tree loss: 1.712 | Accuracy: 0.515358 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 1.781 | Reg loss: 0.049 | Tree loss: 1.781 | Accuracy: 0.462000 | 1.665 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.049 | Tree loss: 1.781 | Accuracy: 0.441500 | 1.665 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 1.763 | Reg loss: 0.049 | Tree loss: 1.763 | Accuracy: 0.445000 | 1.665 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.049 | Tree loss: 1.756 | Accuracy: 0.437000 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.049 | Tree loss: 1.730 | Accuracy: 0.467000 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.049 | Tree loss: 1.737 | Accuracy: 0.483000 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.504000 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.049 | Tree loss: 1.720 | Accuracy: 0.502000 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.049 | Tree loss: 1.686 | Accuracy: 0.520500 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.049 | Tree loss: 1.732 | Accuracy: 0.486500 | 1.664 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.481229 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.049 | Tree loss: 1.798 | Accuracy: 0.434500 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 1.769 | Reg loss: 0.049 | Tree loss: 1.769 | Accuracy: 0.460000 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.451500 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.049 | Tree loss: 1.746 | Accuracy: 0.458500 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.049 | Tree loss: 1.739 | Accuracy: 0.462500 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.049 | Tree loss: 1.725 | Accuracy: 0.477000 | 1.664 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.049 | Tree loss: 1.734 | Accuracy: 0.485500 | 1.663 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.049 | Tree loss: 1.724 | Accuracy: 0.492000 | 1.663 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.494000 | 1.663 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.505500 | 1.663 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.049 | Tree loss: 1.667 | Accuracy: 0.529010 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.049 | Tree loss: 1.834 | Accuracy: 0.431500 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.049 | Tree loss: 1.820 | Accuracy: 0.438000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.049 | Tree loss: 1.787 | Accuracy: 0.441000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.452000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.502000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.049 | Tree loss: 1.690 | Accuracy: 0.527000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.526000 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.502500 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.049 | Tree loss: 1.680 | Accuracy: 0.509500 | 1.663 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.049 | Tree loss: 1.693 | Accuracy: 0.499500 | 1.662 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.049 | Tree loss: 1.690 | Accuracy: 0.518771 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 1.794 | Reg loss: 0.049 | Tree loss: 1.794 | Accuracy: 0.443000 | 1.663 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.049 | Tree loss: 1.809 | Accuracy: 0.437500 | 1.663 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.049 | Tree loss: 1.786 | Accuracy: 0.442000 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.049 | Tree loss: 1.757 | Accuracy: 0.449000 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.473500 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.495000 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.493500 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.493500 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.489000 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.049 | Tree loss: 1.693 | Accuracy: 0.526000 | 1.662 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.501706 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.049 | Tree loss: 1.820 | Accuracy: 0.426000 | 1.662 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 1.786 | Reg loss: 0.049 | Tree loss: 1.786 | Accuracy: 0.432000 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.445500 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.049 | Tree loss: 1.755 | Accuracy: 0.466500 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.049 | Tree loss: 1.735 | Accuracy: 0.503500 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.491000 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.049 | Tree loss: 1.725 | Accuracy: 0.504000 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.500500 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.515500 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.049 | Tree loss: 1.675 | Accuracy: 0.510000 | 1.661 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.525597 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.049 | Tree loss: 1.799 | Accuracy: 0.431500 | 1.661 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 1.807 | Reg loss: 0.049 | Tree loss: 1.807 | Accuracy: 0.423500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 1.768 | Reg loss: 0.049 | Tree loss: 1.768 | Accuracy: 0.453500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 1.731 | Reg loss: 0.049 | Tree loss: 1.731 | Accuracy: 0.449000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.493500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.496500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.049 | Tree loss: 1.690 | Accuracy: 0.519000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.049 | Tree loss: 1.696 | Accuracy: 0.516000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.500500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.049 | Tree loss: 1.724 | Accuracy: 0.494500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.049 | Tree loss: 1.695 | Accuracy: 0.501706 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 1.790 | Reg loss: 0.049 | Tree loss: 1.790 | Accuracy: 0.435500 | 1.66 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 1.795 | Reg loss: 0.049 | Tree loss: 1.795 | Accuracy: 0.435000 | 1.66 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.049 | Tree loss: 1.774 | Accuracy: 0.438000 | 1.66 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 1.743 | Reg loss: 0.049 | Tree loss: 1.743 | Accuracy: 0.465000 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.489500 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.485500 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.049 | Tree loss: 1.737 | Accuracy: 0.481000 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.049 | Tree loss: 1.696 | Accuracy: 0.498500 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.498000 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.049 | Tree loss: 1.699 | Accuracy: 0.525500 | 1.659 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 1.643 | Reg loss: 0.049 | Tree loss: 1.643 | Accuracy: 0.532423 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 1.814 | Reg loss: 0.049 | Tree loss: 1.814 | Accuracy: 0.433000 | 1.659 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 1.786 | Reg loss: 0.049 | Tree loss: 1.786 | Accuracy: 0.441500 | 1.659 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.049 | Tree loss: 1.803 | Accuracy: 0.432500 | 1.659 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.465500 | 1.659 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.479500 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.492500 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.049 | Tree loss: 1.691 | Accuracy: 0.503000 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.477000 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.049 | Tree loss: 1.679 | Accuracy: 0.532500 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.511000 | 1.658 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 1.620 | Reg loss: 0.049 | Tree loss: 1.620 | Accuracy: 0.559727 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 1.793 | Reg loss: 0.049 | Tree loss: 1.793 | Accuracy: 0.450000 | 1.658 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.049 | Tree loss: 1.805 | Accuracy: 0.433500 | 1.658 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 1.768 | Reg loss: 0.049 | Tree loss: 1.768 | Accuracy: 0.452000 | 1.658 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.049 | Tree loss: 1.753 | Accuracy: 0.473500 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.049 | Tree loss: 1.742 | Accuracy: 0.488500 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.504500 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.496500 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.049 | Tree loss: 1.688 | Accuracy: 0.517000 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 1.724 | Reg loss: 0.049 | Tree loss: 1.724 | Accuracy: 0.499000 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.049 | Tree loss: 1.710 | Accuracy: 0.491000 | 1.657 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 1.628 | Reg loss: 0.049 | Tree loss: 1.628 | Accuracy: 0.508532 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 1.801 | Reg loss: 0.049 | Tree loss: 1.801 | Accuracy: 0.460500 | 1.657 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.049 | Tree loss: 1.766 | Accuracy: 0.438500 | 1.657 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.442000 | 1.657 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 1.731 | Reg loss: 0.049 | Tree loss: 1.731 | Accuracy: 0.468000 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.049 | Tree loss: 1.746 | Accuracy: 0.461500 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.049 | Tree loss: 1.725 | Accuracy: 0.494000 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.510500 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.049 | Tree loss: 1.722 | Accuracy: 0.509000 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.507000 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.501000 | 1.656 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.049 | Tree loss: 1.695 | Accuracy: 0.515358 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.049 | Tree loss: 1.802 | Accuracy: 0.444500 | 1.656 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 1.785 | Reg loss: 0.049 | Tree loss: 1.785 | Accuracy: 0.448000 | 1.656 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 1.771 | Reg loss: 0.049 | Tree loss: 1.771 | Accuracy: 0.444500 | 1.656 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.049 | Tree loss: 1.746 | Accuracy: 0.451000 | 1.656 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.049 | Tree loss: 1.752 | Accuracy: 0.473500 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.049 | Tree loss: 1.730 | Accuracy: 0.480000 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.522000 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.049 | Tree loss: 1.696 | Accuracy: 0.516000 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.049 | Tree loss: 1.678 | Accuracy: 0.528500 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.514500 | 1.655 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.049 | Tree loss: 1.707 | Accuracy: 0.477816 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.049 | Tree loss: 1.820 | Accuracy: 0.436500 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.049 | Tree loss: 1.783 | Accuracy: 0.441000 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.049 | Tree loss: 1.752 | Accuracy: 0.468000 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.469500 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.469000 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.049 | Tree loss: 1.713 | Accuracy: 0.481000 | 1.655 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.504000 | 1.654 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.508000 | 1.654 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.049 | Tree loss: 1.733 | Accuracy: 0.506500 | 1.654 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.523500 | 1.654 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 1.722 | Reg loss: 0.049 | Tree loss: 1.722 | Accuracy: 0.494881 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.049 | Tree loss: 1.811 | Accuracy: 0.425500 | 1.654 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.049 | Tree loss: 1.759 | Accuracy: 0.460500 | 1.654 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 1.763 | Reg loss: 0.049 | Tree loss: 1.763 | Accuracy: 0.456000 | 1.654 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.049 | Tree loss: 1.734 | Accuracy: 0.466500 | 1.654 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.049 | Tree loss: 1.731 | Accuracy: 0.476500 | 1.654 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.504500 | 1.653 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.049 | Tree loss: 1.714 | Accuracy: 0.516000 | 1.653 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.511000 | 1.653 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.500000 | 1.653 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.049 | Tree loss: 1.731 | Accuracy: 0.496000 | 1.653 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.488055 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.049 | Tree loss: 1.828 | Accuracy: 0.431500 | 1.653 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.049 | Tree loss: 1.800 | Accuracy: 0.445000 | 1.653 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.049 | Tree loss: 1.775 | Accuracy: 0.456500 | 1.653 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.049 | Tree loss: 1.756 | Accuracy: 0.462500 | 1.653 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.049 | Tree loss: 1.744 | Accuracy: 0.494500 | 1.653 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.494500 | 1.652 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.049 | Tree loss: 1.680 | Accuracy: 0.531000 | 1.652 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.516000 | 1.652 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.049 | Tree loss: 1.670 | Accuracy: 0.529500 | 1.652 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.515000 | 1.652 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.049 | Tree loss: 1.727 | Accuracy: 0.501706 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.049 | Tree loss: 1.815 | Accuracy: 0.427000 | 1.652 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 1.790 | Reg loss: 0.049 | Tree loss: 1.790 | Accuracy: 0.446500 | 1.652 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.461000 | 1.652 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 1.733 | Reg loss: 0.049 | Tree loss: 1.733 | Accuracy: 0.476000 | 1.652 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.491000 | 1.652 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.049 | Tree loss: 1.707 | Accuracy: 0.511500 | 1.651 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.497000 | 1.651 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.499500 | 1.651 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.500500 | 1.651 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.049 | Tree loss: 1.699 | Accuracy: 0.518000 | 1.651 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.049 | Tree loss: 1.735 | Accuracy: 0.491468 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.049 | Tree loss: 1.807 | Accuracy: 0.439000 | 1.651 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.049 | Tree loss: 1.799 | Accuracy: 0.436500 | 1.651 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.049 | Tree loss: 1.794 | Accuracy: 0.444000 | 1.651 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.049 | Tree loss: 1.746 | Accuracy: 0.469500 | 1.651 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.049 | Tree loss: 1.707 | Accuracy: 0.505500 | 1.651 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.517500 | 1.65 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.501500 | 1.65 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.508500 | 1.65 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.506500 | 1.65 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.511500 | 1.65 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.049 | Tree loss: 1.685 | Accuracy: 0.529010 | 1.65 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 1.790 | Reg loss: 0.049 | Tree loss: 1.790 | Accuracy: 0.462000 | 1.65 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.049 | Tree loss: 1.804 | Accuracy: 0.441000 | 1.65 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.049 | Tree loss: 1.741 | Accuracy: 0.458000 | 1.65 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.049 | Tree loss: 1.785 | Accuracy: 0.435500 | 1.65 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.049 | Tree loss: 1.738 | Accuracy: 0.478000 | 1.65 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.049 | Tree loss: 1.715 | Accuracy: 0.500500 | 1.649 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.049 | Tree loss: 1.718 | Accuracy: 0.485000 | 1.649 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.511000 | 1.649 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.049 | Tree loss: 1.684 | Accuracy: 0.520000 | 1.649 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.519500 | 1.649 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.049 | Tree loss: 1.738 | Accuracy: 0.491468 | 1.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 1.802 | Reg loss: 0.049 | Tree loss: 1.802 | Accuracy: 0.446000 | 1.649 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.049 | Tree loss: 1.788 | Accuracy: 0.448000 | 1.649 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.049 | Tree loss: 1.767 | Accuracy: 0.434000 | 1.649 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.049 | Tree loss: 1.735 | Accuracy: 0.465500 | 1.649 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.049 | Tree loss: 1.736 | Accuracy: 0.484000 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.049 | Tree loss: 1.737 | Accuracy: 0.483000 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.049 | Tree loss: 1.701 | Accuracy: 0.493000 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.509500 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.530500 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.049 | Tree loss: 1.713 | Accuracy: 0.502500 | 1.648 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.049 | Tree loss: 1.727 | Accuracy: 0.498294 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.049 | Tree loss: 1.815 | Accuracy: 0.440500 | 1.648 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 1.795 | Reg loss: 0.049 | Tree loss: 1.795 | Accuracy: 0.443000 | 1.648 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.049 | Tree loss: 1.790 | Accuracy: 0.446500 | 1.648 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.049 | Tree loss: 1.745 | Accuracy: 0.468000 | 1.648 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.480000 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.483000 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.049 | Tree loss: 1.730 | Accuracy: 0.499000 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.049 | Tree loss: 1.671 | Accuracy: 0.505500 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.049 | Tree loss: 1.722 | Accuracy: 0.496000 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.049 | Tree loss: 1.681 | Accuracy: 0.514500 | 1.647 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.049 | Tree loss: 1.644 | Accuracy: 0.559727 | 1.647 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.049 | Tree loss: 1.823 | Accuracy: 0.447500 | 1.647 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.049 | Tree loss: 1.792 | Accuracy: 0.429500 | 1.647 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.049 | Tree loss: 1.776 | Accuracy: 0.439000 | 1.647 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 1.743 | Reg loss: 0.049 | Tree loss: 1.743 | Accuracy: 0.467000 | 1.647 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.466000 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.510000 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.049 | Tree loss: 1.711 | Accuracy: 0.514500 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.494000 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.501500 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.049 | Tree loss: 1.658 | Accuracy: 0.522500 | 1.646 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.049 | Tree loss: 1.694 | Accuracy: 0.508532 | 1.646 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.049 | Tree loss: 1.812 | Accuracy: 0.419000 | 1.646 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.049 | Tree loss: 1.792 | Accuracy: 0.463000 | 1.646 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.049 | Tree loss: 1.770 | Accuracy: 0.458000 | 1.646 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.049 | Tree loss: 1.759 | Accuracy: 0.462500 | 1.646 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.049 | Tree loss: 1.717 | Accuracy: 0.506000 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.522500 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.049 | Tree loss: 1.716 | Accuracy: 0.514000 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.490500 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.049 | Tree loss: 1.682 | Accuracy: 0.519500 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.049 | Tree loss: 1.682 | Accuracy: 0.501500 | 1.645 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 1.791 | Reg loss: 0.049 | Tree loss: 1.791 | Accuracy: 0.453925 | 1.645 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.049 | Tree loss: 1.807 | Accuracy: 0.452500 | 1.645 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 1.788 | Reg loss: 0.049 | Tree loss: 1.788 | Accuracy: 0.455500 | 1.645 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.049 | Tree loss: 1.791 | Accuracy: 0.440500 | 1.645 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.467500 | 1.645 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.049 | Tree loss: 1.745 | Accuracy: 0.476500 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.488000 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.504500 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.049 | Tree loss: 1.684 | Accuracy: 0.521500 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.049 | Tree loss: 1.685 | Accuracy: 0.505000 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.505500 | 1.644 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.049 | Tree loss: 1.654 | Accuracy: 0.556314 | 1.644 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.049 | Tree loss: 1.828 | Accuracy: 0.424000 | 1.644 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.049 | Tree loss: 1.775 | Accuracy: 0.463500 | 1.644 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.049 | Tree loss: 1.754 | Accuracy: 0.463000 | 1.644 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.049 | Tree loss: 1.768 | Accuracy: 0.436500 | 1.644 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.476000 | 1.644 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.049 | Tree loss: 1.691 | Accuracy: 0.503000 | 1.643 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.509000 | 1.643 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.495000 | 1.643 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.049 | Tree loss: 1.698 | Accuracy: 0.508000 | 1.643 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.049 | Tree loss: 1.720 | Accuracy: 0.505000 | 1.643 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.049 | Tree loss: 1.642 | Accuracy: 0.505119 | 1.643 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 1.809 | Reg loss: 0.049 | Tree loss: 1.809 | Accuracy: 0.445500 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.049 | Tree loss: 1.772 | Accuracy: 0.448500 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.049 | Tree loss: 1.792 | Accuracy: 0.445000 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.049 | Tree loss: 1.740 | Accuracy: 0.480500 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.049 | Tree loss: 1.743 | Accuracy: 0.508000 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.049 | Tree loss: 1.699 | Accuracy: 0.507500 | 1.643 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.506500 | 1.642 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.049 | Tree loss: 1.701 | Accuracy: 0.511500 | 1.642 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.049 | Tree loss: 1.717 | Accuracy: 0.504000 | 1.642 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.495500 | 1.642 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.481229 | 1.642 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.049 | Tree loss: 1.826 | Accuracy: 0.429500 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.049 | Tree loss: 1.815 | Accuracy: 0.439500 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.049 | Tree loss: 1.766 | Accuracy: 0.438000 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.049 | Tree loss: 1.739 | Accuracy: 0.466000 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.049 | Tree loss: 1.739 | Accuracy: 0.473500 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.049 | Tree loss: 1.699 | Accuracy: 0.509000 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.049 | Tree loss: 1.712 | Accuracy: 0.505000 | 1.642 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.049 | Tree loss: 1.694 | Accuracy: 0.510000 | 1.641 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.510000 | 1.641 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.049 | Tree loss: 1.684 | Accuracy: 0.513500 | 1.641 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.049 | Tree loss: 1.651 | Accuracy: 0.529010 | 1.641 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.049 | Tree loss: 1.777 | Accuracy: 0.473500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.049 | Tree loss: 1.787 | Accuracy: 0.429500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 1.764 | Reg loss: 0.049 | Tree loss: 1.764 | Accuracy: 0.433000 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 1.723 | Reg loss: 0.049 | Tree loss: 1.723 | Accuracy: 0.478500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.469500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.498500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.503000 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 1.757 | Reg loss: 0.049 | Tree loss: 1.757 | Accuracy: 0.481000 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.518500 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.049 | Tree loss: 1.702 | Accuracy: 0.515000 | 1.641 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.049 | Tree loss: 1.720 | Accuracy: 0.539249 | 1.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.049 | Tree loss: 1.799 | Accuracy: 0.437500 | 1.641 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 1.791 | Reg loss: 0.049 | Tree loss: 1.791 | Accuracy: 0.448000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.049 | Tree loss: 1.758 | Accuracy: 0.450000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.049 | Tree loss: 1.762 | Accuracy: 0.452000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.049 | Tree loss: 1.741 | Accuracy: 0.487500 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.510500 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.049 | Tree loss: 1.700 | Accuracy: 0.513000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.501000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.049 | Tree loss: 1.706 | Accuracy: 0.508000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.506000 | 1.64 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 1.662 | Reg loss: 0.049 | Tree loss: 1.662 | Accuracy: 0.529010 | 1.639 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 1.809 | Reg loss: 0.049 | Tree loss: 1.809 | Accuracy: 0.446000 | 1.64 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.049 | Tree loss: 1.779 | Accuracy: 0.436500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.049 | Tree loss: 1.782 | Accuracy: 0.450500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 1.720 | Reg loss: 0.049 | Tree loss: 1.720 | Accuracy: 0.474500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.494500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.049 | Tree loss: 1.738 | Accuracy: 0.491000 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.503500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.049 | Tree loss: 1.681 | Accuracy: 0.519500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.516000 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.049 | Tree loss: 1.701 | Accuracy: 0.496500 | 1.639 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.049 | Tree loss: 1.728 | Accuracy: 0.505119 | 1.638 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.049 | Tree loss: 1.808 | Accuracy: 0.446000 | 1.639 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.049 | Tree loss: 1.775 | Accuracy: 0.455500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.049 | Tree loss: 1.801 | Accuracy: 0.417500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.049 | Tree loss: 1.744 | Accuracy: 0.467000 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.469500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.499500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.049 | Tree loss: 1.708 | Accuracy: 0.493500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.049 | Tree loss: 1.722 | Accuracy: 0.500500 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.049 | Tree loss: 1.686 | Accuracy: 0.515000 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.049 | Tree loss: 1.693 | Accuracy: 0.522000 | 1.638 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.049 | Tree loss: 1.734 | Accuracy: 0.518771 | 1.638 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.049 | Tree loss: 1.815 | Accuracy: 0.455000 | 1.638 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.049 | Tree loss: 1.766 | Accuracy: 0.461000 | 1.638 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.049 | Tree loss: 1.752 | Accuracy: 0.464500 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.443000 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.485500 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.049 | Tree loss: 1.725 | Accuracy: 0.491500 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.049 | Tree loss: 1.692 | Accuracy: 0.511500 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.049 | Tree loss: 1.687 | Accuracy: 0.514500 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.049 | Tree loss: 1.719 | Accuracy: 0.488000 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.503000 | 1.637 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.049 | Tree loss: 1.734 | Accuracy: 0.474403 | 1.637 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.049 | Tree loss: 1.770 | Accuracy: 0.458500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.049 | Tree loss: 1.780 | Accuracy: 0.442500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.049 | Tree loss: 1.777 | Accuracy: 0.425000 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.049 | Tree loss: 1.761 | Accuracy: 0.472500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.049 | Tree loss: 1.729 | Accuracy: 0.492000 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.049 | Tree loss: 1.717 | Accuracy: 0.498500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.049 | Tree loss: 1.696 | Accuracy: 0.516500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.049 | Tree loss: 1.709 | Accuracy: 0.514500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.049 | Tree loss: 1.704 | Accuracy: 0.512000 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.049 | Tree loss: 1.713 | Accuracy: 0.494500 | 1.636 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.049 | Tree loss: 1.679 | Accuracy: 0.518771 | 1.636 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 1.803 | Reg loss: 0.049 | Tree loss: 1.803 | Accuracy: 0.446500 | 1.636 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.049 | Tree loss: 1.787 | Accuracy: 0.441000 | 1.636 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.049 | Tree loss: 1.776 | Accuracy: 0.459500 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 1.733 | Reg loss: 0.049 | Tree loss: 1.733 | Accuracy: 0.474000 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.472500 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.049 | Tree loss: 1.726 | Accuracy: 0.486500 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.049 | Tree loss: 1.697 | Accuracy: 0.509500 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.049 | Tree loss: 1.695 | Accuracy: 0.508000 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.511500 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.049 | Tree loss: 1.703 | Accuracy: 0.511000 | 1.635 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.049 | Tree loss: 1.714 | Accuracy: 0.511945 | 1.635 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.049 | Tree loss: 1.800 | Accuracy: 0.442500 | 1.635 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.049 | Tree loss: 1.775 | Accuracy: 0.441000 | 1.635 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.049 | Tree loss: 1.781 | Accuracy: 0.446500 | 1.635 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.049 | Tree loss: 1.772 | Accuracy: 0.434500 | 1.635 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.049 | Tree loss: 1.747 | Accuracy: 0.487500 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.049 | Tree loss: 1.705 | Accuracy: 0.505000 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.530500 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.049 | Tree loss: 1.689 | Accuracy: 0.514000 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.049 | Tree loss: 1.721 | Accuracy: 0.499000 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.049 | Tree loss: 1.685 | Accuracy: 0.515000 | 1.634 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.049 | Tree loss: 1.740 | Accuracy: 0.494881 | 1.634 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEGCAYAAABGnrPVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsd0lEQVR4nO3dd3hUVfoH8O+bXkglhRIgAUJvgdCLSJNiAws27HXVlbUtWHEV15+67uqqu6LY66pYQRER6b0EQXoPIJ2QQHrO74+5M5maTCb3Tsv38zx5mDn3zp33BJh3TrnniFIKREREegnxdQBERBRcmFiIiEhXTCxERKQrJhYiItIVEwsREekqzNcB+FpKSorKzMz0dRhERAFl7dq1x5VSqc6ONfjEkpmZiTVr1vg6DCKigCIi+1wdY1cYERHpiomFiIh0xcRCRES6YmIhIiJdMbEQEZGumFiIiEhXTCxERKQrJhYPrdpzEs//uBXcdoCIyBYTi4fyDpzG67/uwvoDp30dChGRX2Fi8VBafCQAYMLry5A5ZTYOnDzn44iIiPwDE4uH0uKibJ4Pfn4B9hw/66NoiIj8BxOLh8wtFmvnv/grCkvKfRANEZH/YGLxUFqcY2IBgOveWunlSIiI/AsTi4caRTpfGDovvwAnz5Z5ORoiIv/BxOIhEXF5bNwri70YCRGRf2FiqYe8J0c5LT9cUIKDp4u9HA0RkX9gYqmHhOhwl8f2cYYYETVQTCz19MN9g52Wz938h5cjISLyD0ws9dSxabzT8veWu9y1k4goqDGx6ODyXhlOyznOQkQNEROLDp6b0NVp+VPfbvZyJEREvsfEooOwUOe/xp9+P+LlSIiIfI+JhYiIdMXEopPWKbFOy99essfLkRAR+RYTi05GdEp3Wv6373/3ciRERL7FxKKT0BDXS7wQETUkTCw6uTK3hctjZRVVXoyEiMi3mFh0kuVijAXgXfhE1LAwsejog1v6OC2f/NkG7wZCRORDTCw6ap3ayGl5ZZXyciRERL7DxKKj5onRLo+dK6vwYiRERL7DxOIlj8z6zdchEBF5BROLziLDnP9K95w45+VIiIh8g4lFZ21cjLPkHTjt3UCIiHyEiUVnz13mfKVjAFCKg/hEFPyYWHTWLSPR5TFODiOihoCJxYuq2GIhogaAicWLvt1wyNchEBEZjonFix74PM/XIRARGY6JxQD3Dc/2dQhERD4T5usAjCAisQBeB1AG4Fel1EfefP/IcOZrImq4DPsEFJEoEVklInkisllEnqrHtd4WkaMissnJsdEisk1EdorIFK14AoAvlFK3AbjY0/f1VESo61/r6XNlXoyEiMj7jPxqXQpgmFKqO4AeAEaLSD/rE0QkTUTi7MraOrnWuwBG2xeKSCiA1wCMAdAJwNUi0glABoAD2mmV9atG3V3eK8PlsYe+2OjFSIiIvM+wxKJMirSn4dqP/Xzb8wB8IyJRACAitwF4xcm1FgE46eRt+gDYqZTarZQqA/ApgEsA5MOUXAAXdRSRi0RkRkFBQd0q5obEmAiXx+b9fkT39yMi8ieGDgaISKiIbABwFMA8pdRK6+NKqc8B/AjgUxG5FsDNAK6sw1s0R3XLBDAllOYAZgG4TET+A+A7Zy9USn2nlLo9ISGhDm9HRES1MTSxKKUqlVI9YGo99BGRLk7OeR5ACYD/ALjYqpXjDmcbzSul1Fml1E1Kqbu8PXBvNnmE65lhB08XezESIiLv8sr0JaXUaQC/wvk4yWAAXQB8BeDJOl46H4D1ZvMZAPziLsQRHdNdHrv1vTVejISIyLuMnBWWKiKJ2uNoACMAbLU7JwfAmzCNi9wEIFlEnqnD26wGkC0iWSISAeAqAN/qEH69RbhYPh8AzpZy0y8iCl5GtliaAlggIhthSgDzlFLf250TA+AKpdQupVQVgBsA7LO/kIh8AmA5gPYiki8itwCAUqoCwD0A5gLYAuB/SqnNhtWoDsJrmHLMNcOIKJgZdoOkUmojgJxazllq97wcphaM/XlX13CNOQDmeBimYcJDnQ3/mOSf4hgLEQUv3iJukJq6wgDgXBm7w4goODGxGEScTlirtv8ktyomouDExGKQ+OiaexlH/2uxlyIhIvIuJhaDRIaFIj0+0tdhEBF5HROLgcbnuF4zjIgoWDGxGOjhC9r7OgQiIq9jYjFQSEjNA/icGUZEwYiJxYf+u3C3r0MgItIdE4sPlVdW+ToEIiLdMbH40H9+3eXrEIiIdMfEYrA7hrT2dQhERF7FxGKw4TUsn09EFIyYWAzWJyvZ1yEQEXkVE4uPfZfnF/uSERHphonFx6bO+s3XIRAR6YqJxce46RcRBRsmFh+rrGJiIaLgwsTiY2ywEFGwYWLxgqyUWJfHyiqroJhdiCiIMLF4wdd3D0RcpOuNv7YfKfJiNERExmJi8YKE6HBkpzdyefyGt1d5MRoiImMxsXhJZQ29XX+cKfFeIEREBmNi8ZIretW8m2RJeaWXIiEiMhYTi5dc169Vjcdfnr/DS5EQERmLicVPnCwq83UIRES6YGLxE7wDn4iCBROLF716TY7LY5+vzfdiJERExmFi8aKwEPF1CEREhmNi8aLQEP66iSj48ZPOi0L52yaiBoAfdV5UW4vlrcW7vRQJEZFxmFi8qLYxlmdmb/FSJERExmFi8aKclolom+Z6zTAAKCqt8FI0RETGYGLxopiIMMz58+Aaz8k7cNo7wRARGYSJxcsiwmr+lZdXVnkpEiIiYzCx+Jkb31nt6xCIiOqFiYWIiHTFxEJERLpiYvFD3+Yd8nUIREQeY2LxQ99uOOjrEIiIPMbE4ocqq7iEPhEFLiYWH1j/+Mgaj1cyrxBRAGNi8YGk2Igajy/afgx/FJR4KRoiIn0xsfipkS8t9HUIREQeYWLxU4VcM4yIAhQTix87eobdYUQUeJhY/NjUWb+hsKTc12EQEdUJE4sfm7/1KB76fKOvwyAiqhMmFh9Z9/hIrHxkeK3nHSoo9kI0RET6CXPnJBGJBVCslKoSkXYAOgD4QSnFfhoPJdcy5diMN0sSUaBxt8WyCECUiDQHMB/ATQDeNSooqsa8QkSBxt3EIkqpcwAmAPi3Umo8gE7GhUVmWw6f8XUIRER14nZiEZH+AK4FMFsrc6sbjerv5ne5+RcRBQ53E8tkAFMBfKWU2iwirQEsMCwqsvHL1qO+DoGIyG1utTqUUgsBLAQAEQkBcFwp9WcjAyMiosDkVotFRD4WkXhtdtjvALaJyEPGhkbWissqfR0CEZFb3O0K66SUOgPgUgBzALQEMMmooBqS7+8dhEfHdqz1vE9X7/dCNERE9eduYgkXkXCYEss32v0rnAirgy7NEzC0fWqt5y3afgy/5Rd4ISIiovpxN7G8AWAvgFgAi0SkFQDOg9VJaIjUes6Cbcdw0atLuHYYEfk9txKLUuoVpVRzpdRYZbIPwPkGx9ZghIW4v7LO7mNnDYyEiKj+3B28TxCRl0RkjfbzD5haL6SD0NDaWyxml7y21MBIiIjqz92vym8DKARwpfZzBsA7RgXV0IS50RVGRBQo3E0sbZRSTyqldms/TwFobWRgDUldE0vmlNk4wk3AiMhPuZtYikVkkPmJiAwEwPXcdZIcG4HbBmfV6TXjX1uKknLe20JE/sfdxHIngNdEZK+I7AXwKoA7DIuqgRERPDqubmt6HioowaD/46o6ROR/3J0VlqeU6g6gG4BuSqkcAMMMjawBenBUuzqdf7yo1KBIiIg8V6cdJJVSZ7Q78AHgfgPiadDCQrmhJxEFvvp8knEqk848mR22YvcJAyIhIvJcfRILl3TR2ZB2tS/tYu+qGSuw6SCXeiEi/1FjYhGRQhE54+SnEEAzL8XYYLRLj8OvDw6t8+su/PcS/YMhIvJQjfuxKKXivBUImTRJiPJ1CERE9cLR4iDy7/k7OOZCRD7Hfev9jHg4JeJcWQX+MW87AGDvc+N0jIiIqG7YYvEzIR5mlk5PzNU5EiIizzCx+Bk95nA/8c0mHa5CROQZJhY/I572hVl5f/k+HSIhIvIME4ufsU4r9Rkr+XDFPvy46Q8cLuBaoUTkXRy89zM6NFgAAI99Xd0d9vGtfZGeEIWthwvROzMJafGc0kxExmFi8TPmrrDmidG6XfOat1ZaHrdOjcUvDwzV7dpERPbYFeaHZkzqhS/vGgAA+PKu/ujZMlG3ax88xa4xIjIWE4sfGtW5ieUO/F6tkvHAqPa6Xbu0ogplFVW6XY+IyB4TSwDQexnpv32/mbtPEpFhmFgaoA9X7MdL2l36RER6Y2IJBAbsfMPdJ4nIKEwsAUAMyCxnSyvw/cZDKKuowhsLd3FPFyLSDacbB4D0+Ejdrzl38xHM3XzEpmzvc+PwXd4hZDaORdeMBN3fk4gaBiaWANA6tRF+vn8IyioUxr6y2ND3uveT9QC4QjIReY6JJUC0TTN+z7WVVnu5VFYphIYYMLhDREGPYyxkMXHGCsvjNo/M8WEkRBTImFjIpWverE40RwtLcPJsmQ+jIaJAwa6wADOiYzr2nTiLNyb1wrB/LDT0vZbtOoFJM1cis3EsPlhhWoqfYy9EVBtRSvk6Bp/Kzc1Va9as8XUYHjlcUIz+f//Fq+95x5DWuK5fK7RIjvHq+xKRfxGRtUqpXGfH2BUWwCLDQr3+nm8s2o0xLxs7M42IAhsTSwCLjwpD49gIRIV796+xqLQCAFBVpTDk+QWY+MZyr74/Efm3oEwsIhIrIu+JyJsicq2v4zFKWGgI1j4+EpNHtPP6ex8uKEbrR+Zg/8lzWLnnpNffn4j8l2GJRURaiMgCEdkiIptF5L56XOttETkqIpucHBstIttEZKeITNGKJwD4Qil1G4CLPX3fQBHmg/tN7Md2vlibj0kzV+KPghL88Nthr8dDRP7DyBZLBYAHlFIdAfQDcLeIdLI+QUTSRCTOrqytk2u9C2C0faGIhAJ4DcAYAJ0AXK29RwaAA9ppQb8+fIhe+xnXw4Of52HxjuPo9/f5uOujdTjBRS6JGizDEotS6rBSap32uBDAFgDN7U47D8A3IhIFACJyG4BXnFxrEQBn/S19AOxUSu1WSpUB+BTAJQDyYUougIs6ishFIjKjoCDwF19M09YSa5Ma6+NIqpVXVs82PFFUii2Hz/gwGiLyJq/cxyIimQByAKy0LldKfS4iWQA+FZHPAdwMYGQdLt0c1S0TwJRQ+sKUnF4VkXEAvnP2QqXUdwC+y83Nva0O7+eXxnVtitBrBSM7paOwpAIj/7kI7Zs0wtKdJ2p/sUGqlMKJolLM3XwEL83bjuNFpbwHhqiBMDyxiEgjAF8CmKyUcvjaqpR6XkQ+BfAfAG2UUkV1ubyTMqWUOgvgJo8CDkAigjFdmwIAkmIjsOaxESivrMKsdfk4v0Ma+kyf7/WYBjzn/v01R86U4GxpBVqnNjIwIiLyFkNnhYlIOExJ5SOl1CwX5wwG0AXAVwCerONb5ANoYfU8A8AhD0INOuGhIZjYuyXS4qJ8HYqDguJyPPh5Hm5733Rjat9n5xu+igAReY+Rs8IEwEwAW5RSL7k4JwfAmzCNi9wEIFlEnqnD26wGkC0iWSISAeAqAN/WL/Lgc2VuRu0neUFxmWkeRfenfsIXa/Mx7/cjtbyCiAKRkS2WgQAmARgmIhu0n7F258QAuEIptUspVQXgBgD77C8kIp8AWA6gvYjki8gtAKCUqgBwD4C5ME0O+J9SarNxVQpMRuxA6YmOT/zoMIj/0UqHv26LotIKHDxdbHRYRKQzw8ZYlFJLUMtu7UqppXbPy2Fqwdifd3UN15gDgGu818APZiNb2C8H8+hX1bcmbf3jDDo0iYd5/brLXl+GbUcKOehPFGCC8s57cu7Z8V0tj5snRvswEufGv7YMB08XI2vqHDw7Zwu2HSms8Xzz0jJE5F+YWBoA6xbLPyd2x2e39/OrVoxZcXklBmqzyd5cvMfheGWVwvcbD6GqSmHB1qPo8uRcrOJyMkR+h4mlAVFQGJ+Tgb6tGwfUtsOZU2bjmw0H8eGKfbjn4/V46IuNWLHHdI/O2n2nUFxWiamzNqKguLzWa63ffwoHTp4zOmSiBo2JpUFwTCIttf1Ufn1wqJdj8cx9n27AsULTMjFfrsu3TEioUgofrdyHT1YdwBX/XQYA2HDgNKqqnO8zNP71ZRj8/ALvBE3UQDGxNFCvXtMT/72uFzKSbMda/jmxu48iqt1/F+5yeJx/6hyOaeuSbT9ShJW7T+DS15ZixuLdAIC3l+xB7+k/ez9YogaMiaUBsd4sNCE6HKO7NLFZwHLVo8MxPsc/7nlxpsJJK+STVQfwxsLdlucTZ6wAAGzVpjX/7fvfcaywFMt3ncDpc2WW87b9UYjHv97ksmVjz3p8h4hqxsTSANQ0UG99zHyX/ge39DE4IuMt2XkcmVNmW55f/eYK9PjbPMvzm99djQ9W7MPs3w7jq/X5Nq/dfawIO4/ariz0wfK9uOfj9fhire25ROTIK4tQkn9w9l1bnGQdf1iGv76OF5XVeDws1FTHez9ZDwAYn5OBX7YewZIdJ/D2UtOMNPP9M1+uzce2I6ZEc4zbARDViomlAWgUafprjgxz3UBtm1a9AGQwJJba7DvhODPs5nfXOJTtOlaEBz7PcyhXSmH868tw2+DWGNetqSExEgUqdoU1AJNHZOPBUe0wIcd+OxyT/93RH5/d3s/yvGXjGJvjtwzKwuKHzzc0Rl/rOm2uQ9kHK/ZhxW7brQeU1UDVhgOncffH61BZpfDNhoPInDIbt7+/BocL6r4Mzccr92Pw8+6vCE3kz8T6P0pDlJubq9ascfym2tAVFJej+1M/AajuErIes2ioOjWNx5z7BqOqSqH1I6aVhC7vlWEz9jKobQo+vLVvna5r/t2O6JiOf1+dg+iIUP2CroeS8kqIAJFh/hEP+Q8RWauUynV2jC0WciohOhw3DsjEK1fnWMqeubQLJvSsbvXcP7IdbhmU5YvwfOb3w2cw8LlfbLrH7Af0TxebxndKyiuxbv8pVFUpTHxjOSa+sRyZU2bjgxX7MOH1pThw8hzyT9l2yf285Qi+XGd7PaUUFm4/ptuMNKUU3P1C2eHxH9H7GU7Xprphi4Utljozf7ve+9w4lFZUov1jP/o4Iv/TuVk8UhpFYuH2Y3jnpt646Z3VLs+9dVAW3lpSvYTNg6PaoW/rxkiKicDxolIUFJfjjg/W4uo+LXGurAIvXtEd4aEhOFNSDqVMXwJqcra0AlHhoZbVFs57YQEKisux4YlRLl9TcK4cBcXlGPKC6WZSLgRK9mpqsXDwnurFuovk7vPb4LUFu2o4u+HYfKh6e4CakgoAm6QCAC/+tN3m+QMj2wEAPlm1HwBwXb9WCBHBZf8xrTRg/aF/trQCz/+4FX8d0wHHCktx3gu/Wo7t+ftYiIjNxIV1+0/hTHE5hrZPs3nPMS8vwqGCkhrjnrlkD8Z1bYomCZ5tJldVpTD2lcWYPCIbo7uYJkAUl1Xizg/XYtrFnZGVEuvRdf3BHwUliAoPQWJMhK9D8Ql2hVG9/fe6Xhif0xyjO3N2lBH+Mc820UyfvQUPWnXFVVYpHC4oxt7jZ/HO0j14b/k+vPrLTizafszmdRvzCxy6wCa8vgw3vrMa+06cxV+/2IiN+ablcOyTSkVlFXYdK8J2bcXp/FPn8PT3v2PijOV16qJbuvM4vs0zbfJ6rrwSW/8oxJ0frrMcX7LzOBZuP4bps393+5qAaUzQnbXizMorq2xWxz5eVIqps35DaUWly9cUlVZYNqurTb+/z0ffZ/XfEnzv8bN1qqevsMVC9Ta6SxOM7tIESin8fUJXTJ31m69DCmobDpy2ed7mEcftiF7/dRfiomz/e0+fswWD2qZYnpeUV39Imls2n605gBbJjlsqvPjTdssyOiumDreU7ztxDq0fmYPPbu+HjOQY/OnDtXj+8u7Yc7zI0goxu/39NfhJ2zX04u7NbFaw23fiLJonRmPzoQIAwM9bjkIphbNlldh6+AxyM5Nd/DZM7CeaFJdVIio8xOl9WgBw/cxVWL77BJZPHYamCdF4ds4WzFp3EH2ykhxWnygoLseGA6dxw9urEBcVht+mXYCjhSXoM30+Zt6Qi+Ed0wEA834/goykaHRsGg8AKK2oqjFmTwx98VdkJEVjyV+H2ZRXVil8smo/rsxtgQir2wp2HytCaUUVUhpFIjUuUvd4XGFiId2ICK7u0xID26Tg5LkyXPra0tpfRIYpLLHdr2bVnpM22wx0eNz52NiBk47Tpa3Xaev3d8dv4ualdADggn8tAgBEhYdg3l/Ow10frcWJojIctmoF/W/NAeRbrTJ94GQxPl65H28sql6e51xZJe76cC0W7ziOmwdmYXSXJmjfJA4xEaF4b9lejOrUxGFq/JmScpSUV6LP9Pl4dGxH3DakNb7NO4THvvoNax4bafnQXa5NI7/o30ux5rERmLXuIADgL5/l4S+f5Vm6DQHTmNTpc6ZWQmFJBdbvP2V5Pn3OFvRsmYSk2Ajc9r5prNaT8SilFF78aRuaJESjY5O4GhNp/qliHD1TgrR4UxdkSXklXpi7DTOX7MGps2W4d3g2Fu84hhW7T9h0TXtznIyJhXTXsnEMqrQul5bJMdjPZeobpJLyKpcrST/8xUab59fNXIkBbRrblHV+svreoreX7sHbS/cgMSYc5RVVOFtWiWdmb0HL5Bh0bZ5gOa/btJ+QHGsa15g+Zwumz9liOXbNmyvQvkmczczG4y5WUjh9rhzXvLUST17UyZJEzMa/vgw3DsgEAOw+dhY5T8+z+dB+9CvbFvv2I4UY9c9F+McV3fHV+oN48/pcy3TyZTuP45q3VjqN4Z8Tu2N8Tga+Xn8Q3TIS0Dq1+ibmPs/Oxxd39kdaXBRe/GmbpXvxo5X7cefQNpg0c5XD9SqrFPaeOIv5W47g9iFtnL6nXjgrjLPC6mz13pOIjQhDp2bxLs85cqYEfZ+dj/PapWKh1tfPJEP+qKZJJ8mxETh5tublgQDTTcZXvrHcoXzvc+Mc7v9qEh+Fu4e1Rf/WyRjx0qIar3vf8Gy8PH8HwkMFO6aPdXovWfPEaBw8Xd3KnDKmA577YavDefePbIeXtPG6rU+PRlR4/e5NqmlWGBMLE4thFmw9il6ZSeg2zdT//fQlnfH4N5t9HBWR/qZd1AnTvnOccBAXGYZCnbbQdpW8PPX6tT0xtqvnE26YWGrAxGK8DQdO49DpYpw4W4bHv95kKU9pFFHrYpFEZJz6jLvwznvyqR4tEjG2a1OHfSxn/3kwGsc2zHn+RMGMiYW8xn7mZ3p8FK7s3cKm7PM7+wMAwkODf4VlomDFxEJe0zzR8f6I+0e2w1vX52LNYyPw4S19kRRjWp6kZXIM9j43Du3Sq2fCrHlshGU2DhH5LyYW8pqh7dPwvzv625SFh4ZgRKd0pDSKxKDsFMtMFfOU0Ywk030KX941ACmNItEtwzS1dFDbFHRp7npWGhHVrqJS/5s4Ad7HQl7WJ6vmO6gzkmLw/OXdcL62dtU/J/bA4h3H0KtVEgBYFlJMio3AB7f0wQcr9uHjlfux9Y9CYwMnCkIVVQpG7IjAFgv5nStzW1iWn0iIDseF3ZpZjpkTS2VVFUQE1/fPRHZ6nM3rM5Ki8dKV3W3KJvVrZXDURIGnQqetGOwxsZDX3TIoy6FLzF1hWmKpqFQOZQDQqnEMlvx1GCb0tF3v6elLu3j0fkTBzKiuMCYW8rrHL+xUa5eYK5HaGIz1XcOhVonFei7Zuzf1rvFa254ZjcfGdbQpG9o+FXeeZ+xyF0T+4vuNhw25LhMLBZTzslPxlxHt8LdLOlvKQrV5zIkx4Zh5Y3Uysd9jZFw327uMI8NCcUHnJjZlPVsmITvNNBNt2kWd8NGtfXHf8Gxd60DkL1ytlVZfHLyngBISIrhvRLZDGQA8dEF7tLFaqM/eK1fl4Ib+mTbLYtivl6QUMKFnc6THR2Fg28YQEfRokYiX5+/QsRZE/qGSYyxEzoVq/4qdbTh1/8h2lnthQkMEfbKSkR5fvS9Falwk3ru5D24amGm6hlIQEQzKTrEsmx4bGYYf7hvscO2URtXXWTZlmMNxIn9nv7ePXphYKOCFhZj+GTv79vXn4dn46S/n2ZQt+eswbH9mjOX5ee1SLfvGu/r+Zt68yTqZTOxdPUGgWWI0Hh7dHl/9aYBHdSDyhcU7jhtyXSYWCnijOpl28Ovt5oSA8NAQm132AEC0Yf+aFmXdOG0UFj08FA+Pbo93buyNB0e1tzn+p6FtkdMyyaZsQo5p749/XFE9/TnNaie/mwdm1bm1M7R9ap3OJ/I2jrFQwBvQNqXeu+OZJ5bVtNh3fJSpVfOnoW0tZQsfGmrZtKkm1pdd9egIy74aGUnRaJYYjS/v6o/v8g6jQ5M4DGybgpvfXY0dR4tsrjHzhlz0yUpGXFQ4CkvKcdeH67Bkp+M3zrZpjbDT7rV19faNubj5Xa76TZ5hYiFC9QKZVXXcRqJV49gajw9pl4pZ6w+iQ5M4p8fNa5/1apWMXq2qW1ydm8XbJJbWKbGWvdUBIC4qHBd2a2qTWN6+MRfDOqRj86ECjHtlic37fHP3QOw6VoT7/5dXY7zdMhJwUbdmGNYhHSM6puPnLUdqPJ/IGSYWIsAyUK/HHJmf7x+Cf/+yE40iw3BpTnOc3yHNMoZjLySk9lWc1z8+0ulufyH2y0VrMhJjHMq6t0hEt4wEHC4owaU5zXG8sBSVSmHiG8sx4/pc3PTOagDAt/cMsrzmP9f1RHF5pWWjNmszb8jFLe/Ztmi6ZSRgY34BHrqgPV6Yu63WeunpnvPb4tUFO736nuQax1iIUL3opXl15fpomxaHl6/KwfTxXQHAaVJZ8OBQrHpkuMtriFXSSIqNcN7d5iInJcSEO912QERw9/lt0TwxGt1bJKJnyyTsmD7Wsi6bvfDQEEv3n71QJwnxjUm90KFJHC7vVT2poZM26eHbewZafrfW401m399rSmhJMeHYMX2Mw3GzlY8MxzV9W2Ld4yNtyh+8oL2LVzj38/1DLPcrAUCfTM9u2AWAWDe6Qt0xoqPj34P59wIAd5zXWpf3sfbhLX11vybAxEIEwLQ+2fOXdcPNA7O88n5ZKbFIi4+q1zXMH+0D2jTGFb0yMKhtqtUx01EXjRoHNw7IxH+v6+n02Ps398GMSb1sVikI1+Z4D85OwcwbcvHSld3RNCEaP04egnSreoVpCa5KATERpg6S8+wmH7xzU290bBqPvlnJePmqHJctMcC0h8+z47siOTaiTuNql/ZoZvM8NS7KsvcPAEweWfebYL+8awDynhiF9U+MQovk6i0hYjxMNONzMhzKzF8o4qLCMHVMR4fjzux+dmyNx3+4bzCaJURh3eMjMSg7pe6BuoGJhQimb+BX9m6BsFD/+C/hTj4wfwA3iY/CC1d0t53ppl1g+ZThbs06m3ZxZ4zu4nz/8yHtUjGqcxNLq04E6JuVjEn9WuH/LuuG4R3THdZms6+HUgof3toXD4xsh8axEXhkbAfMmNQL6x8fifPbpyE0RPDZHf0xpF0q3OgddIt1iyQm0rbXPzIsBIkx1buXDmiTgp3Tx+COIa3Rs2WiW9fv1SoJCTHhiAgLweKHq3/HcycPsTyePr4LNk4bZfO6RpHORyAUFLY+PdqmTKoP2njGbu076yRbW/dqx6bxWDZ1uOXv0wgcYyHyoh4tEut0/otOuo3M2mgfnD1bJbk8JzEm3On4TH1c0r0ZwkJD3FvY02rsKislFvdqy+PcPsT1emzibjOrFpkpsZYJEOYrTruoEy7p0dzp7yQsNARTx3bEbe/XbzZci+TqMa7IsFCH7sSv7x6AG99ZjfxTxQ6vtY6rQ5M4JMXYfvhPGdNB24soAY99vcnm2NrHRjjtovQFJhYiL9n69Gj3/+Nbpj+7nk7Qo0UiFj98PjKSHHfmnHZRZ0z7brOly0oPnnzeV7dY6va67hkJGJSdgtcW7AIAPHlRJ5RV1LwSb1xkGApLKxze21poiCCplm/q5tf9a2IPTP5sg9Nz3O2GczbLMCYiDM0Tox0Si/2pP04egsKSctMxraymBVIbW92868rP9w+p9Rw9+Ee7n6gBiAoPdfuD3nLDZi3ntUiOcfoN/5q+LbH9mTGGfIOtS454/vJuuKBzOro2T6jTe3xzzyA8dEEHRGvf4G8amIU7all1+pcHh+LHyY5L7wCuk+KXdw2w6boCqrsYI8NCsPXp0biil/NuPnc4+2JQpRQu6t7M8Vztz7io6u/7ltmKdc3Mmt6Z1a3ZcV2bom2a82nvemNiIfJD5vXNmiU4tkZ8pXp1gtrPbZEcjREd09AuPQ5vTMp1WOnAXcumDMOcPztPFvZS4yLRoUn1dtXOkol96L1aJaG93T1GHZqanqfFRyIqPBR3DrVNaO7UZXgH0wwv8+9q3l+qk5dSwLV9W+Llq3rYvCZSu+6yKcMcZr3VNa2Yx0+u6t0SgGll73/ZvZ+R2BVG5IduG9wavVolIbce02D1Zv6gdudDznowuz6SYiNq7bpyx3X9WuGLtfkYYXWTqSv3DsvG4OwUyw2r5vyUHh+JlY+McOv90rSFTs3L12Wnx6FFcjQOnCyGUqaWSKS2J/CgtikY2j4VI7XY4qzGZMyb2LVMdrw3KTo8FMXllU7ff8EDQ1FUVoHURpHYdKgA9w3P1rVbtDZMLER+KCRE/CqpANWLfUb4ycy52ljP8OvQJB5bn3Z9f4y10BCxWQXBLLoOkyDMXVjWYyzmFp+5rGmCaVp2z5aJuHWw83tUosJDMWNSL4c16ABgeMc0lxt1JcSEI0G7b+jJizo7PcdITCxE5JYLOqfjrqFtcMcQ/W/UM8JTF3fGbIN2SKxNiJPJF+Yyc2Lp3iIRs/40AN1qGX8aZbcZnZk5ebVq7Nia8TUmFiJyS1hoCP46uoOvw6hV3hOjICFwuWpAXaVqq1Ff3adljee9ek2OZYwnxNJiqT7urKynk5aIu8Z0aYLv8g5h5g25Hl/DKEwsRBRUEnRYlsdaXFS4W9OLL+xWPdPr+v6t8F3eIZutr6snE+iza+PYrk2xc/oYv7mp15r/RUREFODapsVh/ROj0CShenkb85Rr89I2evDHpAKwxUJE5BXPXdYNk/pnolmi/0whNwoTCxEFre/uGYTyqprv2PeWqPBQ9Kph+Z1gwsRCRAHr09v74aCTNbfMumbU7Y5/0gcTCxEFrH6tG/s6BHLCP0d+iIgoYDGxEBGRrphYiIhIV0wsRESkKyYWIiLSFRMLERHpiomFiIh0xcRCRES6Ek/3Ug4WInIMwD4PX54C4LiO4fibYK4f6xa4grl+gVS3VkqpVGcHGnxiqQ8RWaOU8r/NEHQSzPVj3QJXMNcvWOrGrjAiItIVEwsREemKiaV+Zvg6AIMFc/1Yt8AVzPULirpxjIWIiHTFFgsREemKiYWIiHTFxOIhERktIttEZKeITPF1PO4QkbdF5KiIbLIqSxaReSKyQ/szyerYVK1+20TkAqvyXiLym3bsFRERb9fFnoi0EJEFIrJFRDaLyH1aecDXT0SiRGSViORpdXtKKw/4upmJSKiIrBeR77XnwVS3vVpcG0RkjVYWNPVzSinFnzr+AAgFsAtAawARAPIAdPJ1XG7EPQRATwCbrMqeBzBFezwFwP9pjztp9YoEkKXVN1Q7tgpAfwAC4AcAY/ygbk0B9NQexwHYrtUh4OunxdFIexwOYCWAfsFQN6s63g/gYwDfB9O/Sy2uvQBS7MqCpn7Ofthi8UwfADuVUruVUmUAPgVwiY9jqpVSahGAk3bFlwB4T3v8HoBLrco/VUqVKqX2ANgJoI+INAUQr5Rarkz/2t+3eo3PKKUOK6XWaY8LAWwB0BxBUD9lUqQ9Ddd+FIKgbgAgIhkAxgF4y6o4KOpWg6CuHxOLZ5oDOGD1PF8rC0TpSqnDgOnDGUCaVu6qjs21x/blfkNEMgHkwPTNPijqp3UVbQBwFMA8pVTQ1A3AvwA8DKDKqixY6gaYvgT8JCJrReR2rSyY6ucgzNcBBChnfZvBNm/bVR39uu4i0gjAlwAmK6XO1NANHVD1U0pVAughIokAvhKRLjWcHjB1E5ELARxVSq0VkaHuvMRJmV/WzcpApdQhEUkDME9EttZwbiDWzwFbLJ7JB9DC6nkGgEM+iqW+jmjNbGh/HtXKXdUxX3tsX+5zIhIOU1L5SCk1SysOmvoBgFLqNIBfAYxGcNRtIICLRWQvTF3Kw0TkQwRH3QAASqlD2p9HAXwFU1d60NTPGSYWz6wGkC0iWSISAeAqAN/6OCZPfQvgBu3xDQC+sSq/SkQiRSQLQDaAVVqzvVBE+mmzUq63eo3PaLHMBLBFKfWS1aGAr5+IpGotFYhINIARALYiCOqmlJqqlMpQSmXC9P/oF6XUdQiCugGAiMSKSJz5MYBRADYhSOrnkq9nDwTqD4CxMM082gXgUV/H42bMnwA4DKAcpm9AtwBoDGA+gB3an8lW5z+q1W8brGagAMiF6T/HLgCvQlvBwcd1GwRT18BGABu0n7HBUD8A3QCs1+q2CcATWnnA182unkNRPSssKOoG08zRPO1ns/mzIljq5+qHS7oQEZGu2BVGRES6YmIhIiJdMbEQEZGumFiIiEhXTCxERKQrJhYinYhIkfZnpohco/O1H7F7vkzP6xPpiYmFSH+ZAOqUWEQktJZTbBKLUmpAHWMi8homFiL9PQdgsLb/xl+0BSRfEJHVIrJRRO4AABEZKqY9ZD4G8JtW9rW2WOFm84KFIvIcgGjteh9pZebWkWjX3qTt1THR6tq/isgXIrJVRD7y6/07KKhwEUoi/U0B8KBS6kIA0BJEgVKqt4hEAlgqIj9p5/YB0EWZlkgHgJuVUie1pVtWi8iXSqkpInKPUqqHk/eaAKAHgO4AUrTXLNKO5QDoDNOaUkthWpdrid6VJbLHFguR8UYBuF5b9n4lTMt5ZGvHVlklFQD4s4jkAVgB02KE2ajZIACfKKUqlVJHACwE0Nvq2vlKqSqYlrjJ1KEuRLVii4XIeALgXqXUXJtC0zLxZ+2ejwDQXyl1TkR+BRDlxrVdKbV6XAn+fycvYYuFSH+FMG2PbDYXwF3asv4QkXbaSrf2EgCc0pJKB5i2HzYrN7/eziIAE7VxnFSYtp9epUstiDzEbzBE+tsIoELr0noXwMswdUOt0wbQj8H5trI/ArhTRDbCtLLtCqtjMwBsFJF1Sqlrrcq/gmkf9DyYVnd+WCn1h5aYiHyCqxsTEZGu2BVGRES6YmIhIiJdMbEQEZGumFiIiEhXTCxERKQrJhYiItIVEwsREenq/wEyRLxu3sQpFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eitan.k/miniconda3/envs/rambo/bin/python'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 8.492753623188406\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 138\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "Average comprehensibility: 80.2463768115942\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    sum_comprehensibility += sum([cond.comprehensibility for cond in conds])\n",
    "    \n",
    "print(f\"Average comprehensibility: {sum_comprehensibility / len(leaves)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_tree(tree, factor=1.5)\n",
    "correct = 0\n",
    "tree = tree.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sparseness: {sparseness(tree.inner_nodes.weight)}\")\n",
    "layer = 0\n",
    "sps = []\n",
    "for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "    cur_layer = np.floor(np.log2(i+1))\n",
    "    if cur_layer != layer:\n",
    "        print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "        sps = []\n",
    "        layer = cur_layer\n",
    "    \n",
    "    x_ = tree.inner_nodes.weight[i, :]\n",
    "    sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "    sps.append(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\\n Kurtosis: {kurtosis(weights_layer)}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the accuracy didn't change too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tree_copy = tree_copy.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree_copy.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree_copy.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack = LifoQueue()\n",
    "edge_stack = LifoQueue()\n",
    "stack.put(root)\n",
    "rule_counter = 0\n",
    "root.reset()\n",
    "while not stack.empty():\n",
    "    node = stack.get()\n",
    "    if node.is_leaf():\n",
    "        print(f\"============== Rule {rule_counter} ==============\")\n",
    "        for stack_node, cond in zip(stack.queue, edge_stack.queue[1:]):\n",
    "            print(repr(stack_node.get_condition(attr_names)) + cond)\n",
    "            print()\n",
    "        \n",
    "        rule_counter += 1\n",
    "        edge_stack.get()\n",
    "        continue\n",
    "          \n",
    "    if node.left is not None and not node.left.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.left)\n",
    "        node.left.visited = True\n",
    "        edge_stack.put(' < 0')\n",
    "        continue\n",
    "        \n",
    "    if node.right is not None and not node.right.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.right)\n",
    "        node.right.visited = True\n",
    "        edge_stack.put(' > 0')\n",
    "        continue\n",
    "        \n",
    "    if node is not root:\n",
    "        edge_stack.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
