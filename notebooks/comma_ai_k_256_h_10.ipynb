{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_256/models/epoch_45.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r''  # Insert path of the cpc model\n",
    "dataset_path = r''  # Insert path of the test dataset that was created using the run_cpc.py script\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e631dff5f0804433b1126ee91cd26f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344d6af5f2e343d3b97ac624b83a8a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(2, 30))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEDCAYAAAA7jc+ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU50lEQVR4nO3dbYxc53ne8f8VUmoiRYnbcK24omiqBeNWMSxFXdB2VMhSHAuUX8IESAISroMGclkFVpAXxC2TDzbafnHhomhTy1YImVGMxhQU23KIipZkoEkl21FMUpEsUrIMlmaiDZ2SlmwrcgIodO5+mLPyaLUvZ7mzO2fO/n/AYGee85zZe2bPueaZZ86cTVUhSeqv7xl3AZKk1WXQS1LPGfSS1HMGvST1nEEvST1n0EtSz3U26JPsT3ImybGW/X8+yRNJjif5+GrXJ0mTIl09jj7JdcDzwMeq6rVL9N0G3A38RFV9I8krq+rMWtQpSV3X2RF9VT0IPDvcluSfJrkvydEkDyX5Z82ifwPcVlXfaNY15CWp0dmgX8A+4Jer6l8AvwF8uGn/EeBHknw+ycNJdoytQknqmI3jLqCtJN8P/DjwB0lmm/9B83MjsA24HtgMPJTktVX1zTUuU5I6Z2KCnsG7j29W1dXzLJsBHq6qvwO+muQpBsF/eA3rk6ROmpipm6p6jkGI/xxABq5qFn8auKFp38RgKufkOOqUpK7pbNAnOQD8CfCaJDNJbgbeCdyc5DHgOLCz6X4/8EySJ4A/At5bVc+Mo25J6prOHl4pSRqNzo7oJUmj0ckPYzdt2lRbt24ddxmSNDGOHj369aqamm9ZJ4N+69atHDlyZNxlSNLESPLnCy1z6kaSes6gl6SeW3LqJsl+4O3AmflOLpbkvQwOe5y9v38OTFXVs0lOAX8NfAc4V1XToypcktROmxH9ncCC546pqg9W1dXNN1Z/E/g/VTV8MrIbmuWGvCSNwZJBP99ZJBexGziwoookSSM1sjn6JBcxGPl/cqi5gAea0wrvGdXvkiS1N8rDK98BfH7OtM21VXU6ySuBzyb5cvMO4WWaF4I9AFu2bBlhWZK0vo3yqJtdzJm2qarTzc8zwD3A9oVWrqp9VTVdVdNTU/Me8y9JOg8jCfokPwi8CfjDobaLk1wyex24EWj1/18lSaPT5vDKAwz+ocemJDPA+4ELAKrq9qbbzwAPVNW3h1a9FLin+SchG4GPV9V9oytdks7P1r33AnDqA28bcyVrY8mgr6rdLfrcyeAwzOG2k8BV8/WXJK0dvxkraV2ZHc2vJwa9pHVrvYS+QS9JPWfQS1o31ssIfi6DXpJ6zqCXpJ4z6CWta+thOsegl7QurIdAX4hBL0k9Z9BLUs8Z9JLUcwa9JPWcQS9p3ev7B7UGvaTe63uQL8Wgl6SeM+glqecMeknqOYNeknrOoJeknjPoJannDHpJ6rklgz7J/iRnkhxbYPn1Sb6V5NHm8r6hZTuSPJXkRJK9oyxcktpoewx9n4+1bzOivxPYsUSfh6rq6ubyHwGSbABuA24CrgR2J7lyJcVKkpZvyaCvqgeBZ8/jvrcDJ6rqZFW9ANwF7DyP+9EE6vPoSJo0o5qjf2OSx5J8JsmPNm2XAU8P9Zlp2tRzsyG/de+9Br7UARtHcB+PAK+uqueTvBX4NLANyDx9a6E7SbIH2AOwZcuWEZSlcTDYpe5Z8Yi+qp6rqueb64eAC5JsYjCCv3yo62bg9CL3s6+qpqtqempqaqVlqUMMf2m8Vhz0SX44SZrr25v7fAY4DGxLckWSC4FdwMGV/j5J0vIsOXWT5ABwPbApyQzwfuACgKq6HfhZ4JeSnAP+FthVVQWcS3IrcD+wAdhfVcdX5VGoExy5S920ZNBX1e4lln8I+NACyw4Bh86vNEnSKPjNWI3EUqN5R/saB7e7AYNekhp9fWEw6CWp5wx6rZm+jpakrjPotWIGuNRtBr0k9ZxBL0k9Z9BrTTnNI609g16Ses6g14o4Qpe6z6CX1EsOQr7LoJeknjPoteYcaUlry6CXpJ4z6CWp5wx6nTenYNRHfdyuDXqNRR93JqmrDHpJ6jmDXpJ6zqCXpJ4z6CWp55YM+iT7k5xJcmyB5e9M8qXm8oUkVw0tO5Xk8SSPJjkyysI1XqP4MNUPZLVa3LZeqs2I/k5gxyLLvwq8qapeB/wnYN+c5TdU1dVVNX1+JUqSVmLjUh2q6sEkWxdZ/oWhmw8Dm0dQlyRpREY9R38z8Jmh2wU8kORokj0j/l2SpBaWHNG3leQGBkH/L4ear62q00leCXw2yZer6sEF1t8D7AHYsmXLqMqSpHVvJCP6JK8D7gB2VtUzs+1Vdbr5eQa4B9i+0H1U1b6qmq6q6ampqVGUJUliBEGfZAvwKeBdVfWVofaLk1wyex24EZj3yB1J6pK+HbXT5vDKA8CfAK9JMpPk5iS3JLml6fI+4IeAD885jPJS4HNJHgO+CNxbVfetwmPQGhvlTtC3HUrqojZH3exeYvm7gXfP034SuOrla0iS1pLfjJXUK75LfDmDXpJ6zqCXpJ4z6LUsq/G22Lfa0uoy6CWp5wx6Seo5g16Ses6gl6SeM+gl9YYf7M/PoJeknjPo1dpqjpYcialr+rRNGvSS1HMGvST1nEEvST1n0EtSzxn0ktRzBr2kXujTUTKjZtBLUs8Z9GplLUZLjsik1WHQS1LPGfSS1HMGvST13JJBn2R/kjNJji2wPEl+O8mJJF9Kcs3Qsh1JnmqW7R1l4ZKkdtqM6O8Ediyy/CZgW3PZA3wEIMkG4LZm+ZXA7iRXrqRY9Z8fyEqjt2TQV9WDwLOLdNkJfKwGHgZekeRVwHbgRFWdrKoXgLuavpI0Ug4QFjeKOfrLgKeHbs80bQu1zyvJniRHkhw5e/bsCMrSqLgTSZNtFEGfedpqkfZ5VdW+qpququmpqakRlCVJgtEE/Qxw+dDtzcDpRdolaSL05d3sKIL+IPALzdE3bwC+VVVfAw4D25JckeRCYFfTV5K0hjYu1SHJAeB6YFOSGeD9wAUAVXU7cAh4K3AC+BvgF5tl55LcCtwPbAD2V9XxVXgMkqRFLBn0VbV7ieUFvGeBZYcYvBBIrW3dey+nPvC2cZch9YbfjJU00foyj76aDHotyp1ImnwGvST1nEEvST1n0EtSzxn06iQ/G5BGx6CXpJ4z6CWp5wx6LcjpE3Wd22g7Br0kLaIPLyYGvST1nEEvST1n0EtSzxn0ktRzBr06qw8fgkldYNBLmkgOBNoz6CWp5wx6zcvRktQfBr0k9ZxBL0k9Z9Cr05xC0nzWeruY9O2wVdAn2ZHkqSQnkuydZ/l7kzzaXI4l+U6Sf9QsO5Xk8WbZkVE/AEnS4jYu1SHJBuA24C3ADHA4ycGqemK2T1V9EPhg0/8dwK9V1bNDd3NDVX19pJVLklppM6LfDpyoqpNV9QJwF7Bzkf67gQOjKE6StHJtgv4y4Omh2zNN28skuQjYAXxyqLmAB5IcTbJnoV+SZE+SI0mOnD17tkVZWi2TPh+pfnP7XL42QZ952mqBvu8APj9n2ubaqroGuAl4T5Lr5luxqvZV1XRVTU9NTbUoS5LURpugnwEuH7q9GTi9QN9dzJm2qarTzc8zwD0MpoKk1hzBSSvTJugPA9uSXJHkQgZhfnBupyQ/CLwJ+MOhtouTXDJ7HbgRODaKwiVJ7Sx51E1VnUtyK3A/sAHYX1XHk9zSLL+96fozwANV9e2h1S8F7kky+7s+XlX3jfIBSJIWt2TQA1TVIeDQnLbb59y+E7hzTttJ4KoVVShJDafxzo/fjNVLuCNJ/WPQS1LPGfSS1HMGvSaCU0oat0neBg16SRNhkoN23Ax6Seo5g16Ses6gl6SeM+gldZ7z8ytj0OtFXd+Zul6f1FUGvST1nEEvqdN8J7dyBr0k9ZxBL0k9Z9ALmJy3x5NSp0bDv/doGPSS1HMGvST1nEEvqZOcthkdg16Ses6g18RxpNd/Xf0bd7WupRj0mtiNV1I7rYI+yY4kTyU5kWTvPMuvT/KtJI82l/e1XVeStLo2LtUhyQbgNuAtwAxwOMnBqnpiTteHqurt57muJAG+w1wNbUb024ETVXWyql4A7gJ2trz/lawrSRqBNkF/GfD00O2Zpm2uNyZ5LMlnkvzoMtclyZ4kR5IcOXv2bIuytJ456pPaaxP0maet5tx+BHh1VV0F/A/g08tYd9BYta+qpqtqempqqkVZGgUDU13i9rg62gT9DHD50O3NwOnhDlX1XFU931w/BFyQZFObdSVJq6tN0B8GtiW5IsmFwC7g4HCHJD+cJM317c39PtNmXUkCR/Oracmgr6pzwK3A/cCTwN1VdTzJLUluabr9LHAsyWPAbwO7amDedVfjgWj9MRikdpY8vBJenI45NKft9qHrHwI+1HZdSRrmi/bq8puxkrQMk/iiZNCvY5O4wap/3A5Xn0EvST1n0GuiORqcbP791oZBL0k9Z9BLGgtH82vHoF+n3Mk0Tm5/a8ug18QzNKTFGfSS1pQvzGvPoJe0ZvoS8pP2OAz6dWjSNtI2+viY+sa/0fgY9JLUcwa9pFXnaH68DHpJ6jmDXr3hqLF7tu69179LBxj064w7nbT+GPTqFV/IusO/RXcY9JLUcwa9pJFzNN8tBv06sl52vvXyOLvK5797DHpJI7OeQn6SHmuroE+yI8lTSU4k2TvP8ncm+VJz+UKSq4aWnUryeJJHkxwZZfHSQiZpJ+wLn/Pu2rhUhyQbgNuAtwAzwOEkB6vqiaFuXwXeVFXfSHITsA94/dDyG6rq6yOsW8vkTiitX21G9NuBE1V1sqpeAO4Cdg53qKovVNU3mpsPA5tHW6a0fL64rR2f625bckQPXAY8PXR7hpeO1ue6GfjM0O0CHkhSwO9U1b75VkqyB9gDsGXLlhZlSRo3A34ytBnRZ562mrdjcgODoP/3Q83XVtU1wE3Ae5JcN9+6VbWvqqaranpqaqpFWWprPe+M6/mxS7PajOhngMuHbm8GTs/tlOR1wB3ATVX1zGx7VZ1ufp5Jcg+DqaAHV1K0pPHyBXSytBnRHwa2JbkiyYXALuDgcIckW4BPAe+qqq8MtV+c5JLZ68CNwLFRFS+1YSiNls/nd03Kc7Fk0FfVOeBW4H7gSeDuqjqe5JYktzTd3gf8EPDhOYdRXgp8LsljwBeBe6vqvpE/Ci1oUjbE1ebzMBo+j5OpzdQNVXUIODSn7fah6+8G3j3PeieBq+a2S+Owde+9nPrA28ZdxkQy4Ceb34zVumJgLZ/P2eQz6HvMHXR+Pi/t+Vz1Q6upG6lvZgPMqZyXM9z7xxF9T7mztuPz9FI+H8s3Cc+ZI3qte+t9dD8JQaWVMeilxnDgrYfQN+DXD4O+h9yBV27uc9iX4HfbWJ8MeqmFSQl+g1zzSdW85ycbq+np6TpyxP9Rcj7c0aXxGPeLf5KjVTU93zKPuukRQ17SfAx6Seo5g74nHM1LWohB3wOGvKTFGPQTzpCXuqHL+6JBP8G6vGFJ6g6DfkIZ8pLaMugnzNa99xryUkd1dd806CdIVzciSd3mKRA6znCXJksX/2WlQd9RBrykUTHoO8Jgl/qja6P6VkGfZAfw34ENwB1V9YE5y9MsfyvwN8C/rqpH2qy7HhnqUv91KeyXDPokG4DbgLcAM8DhJAer6omhbjcB25rL64GPAK9vuW6vGOKSZnUl7NuM6LcDJ6rqJECSu4CdwHBY7wQ+VoNzHj+c5BVJXgVsbbHu2BnOklZLF/5zWZugvwx4euj2DINR+1J9Lmu5LgBJ9gB7mpvPJ3mqRW3z2QR8/TzXXU3WtTzWtTzWtTxjqSv/uVW3863t1QstaBP0madt7n8rWahPm3UHjVX7gH0t6llUkiMLnXx/nKxreaxreaxrebpaF6xObW2Cfga4fOj2ZuB0yz4XtlhXkrSK2nwz9jCwLckVSS4EdgEH5/Q5CPxCBt4AfKuqvtZyXUnSKlpyRF9V55LcCtzP4BDJ/VV1PMktzfLbgUMMDq08weDwyl9cbN1VeSTfteLpn1ViXctjXctjXcvT1bpgFWrr5D8HlySNjic1k6SeM+glqed6E/RJdiR5KsmJJHvHXc+sJPuTnElybNy1DEtyeZI/SvJkkuNJfmXcNQEk+d4kX0zyWFPXfxh3TbOSbEjyZ0n+17hrGZbkVJLHkzya5Mi465nVfHHyE0m+3Gxnb+xATa9pnqfZy3NJfnXcdQEk+bVmmz+W5ECS7x3Zffdhjr451cJXGDrVArC7C6daSHId8DyDbw6/dtz1zGq+ufyqqnokySXAUeCnx/2cNedNuriqnk9yAfA54Feq6uFx1gWQ5NeBaeAHqurt465nVpJTwHRVdeqLSUl+D3ioqu5ojrq7qKq+OeayXtTkxl8Cr6+qPx9zLZcx2NavrKq/TXI3cKiq7hzF/fdlRP/iaRqq6gVg9lQLY1dVDwLPjruOuarqa7MnnquqvwaeZPBN5rGqgeebmxc0l7GPRpJsBt4G3DHuWiZBkh8ArgM+ClBVL3Qp5BtvBv7vuEN+yEbg+5JsBC5ihN856kvQL3QKBrWQZCvwY8CfjrkU4MUpkkeBM8Bnq6oLdf034N8Bfz/mOuZTwANJjjanEumCfwKcBX63me66I8nF4y5qjl3AgXEXAVBVfwn8F+AvgK8x+C7SA6O6/74EfetTLeilknw/8EngV6vquXHXA1BV36mqqxl8k3p7krFOeSV5O3Cmqo6Os45FXFtV1zA4i+x7munCcdsIXAN8pKp+DPg20KXPzi4Efgr4g3HXApDkHzKYhbgC+MfAxUn+1ajuvy9B3+Y0DZqjmQP/JPD7VfWpcdczV/NW/4+BHeOthGuBn2rmwu8CfiLJ/xxvSd9VVaebn2eAexhMZY7bDDAz9G7sEwyCvytuAh6pqv837kIaPwl8tarOVtXfAZ8CfnxUd96XoPdUC8vUfOj5UeDJqvqv465nVpKpJK9orn8fgx3gy+Osqap+s6o2V9VWBtvW/66qkY22ViLJxc2H6TRTIzcCYz/Cq6r+Cng6yWuapjfTrdOT76Yj0zaNvwDekOSiZt98M4PPzUaiF/9KcEynWmglyQHgemBTkhng/VX10fFWBQxGqe8CHm/mwwF+q6oOja8kAF4F/F5zRMT3AHdXVacOZ+yYS4F7BtnARuDjVXXfeEt60S8Dv98Mvk7SnBpl3JJcxOAIvX877lpmVdWfJvkE8AhwDvgzRngqhF4cXilJWlhfpm4kSQsw6CWp5wx6Seo5g16Ses6gl6SeM+glqecMeknquf8P3rOXFSIWL60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9919676735820233\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.14984095094592334\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.7417127071823204\n",
      "layer 3: 0.7417127071823204\n",
      "layer 4: 0.6799033149171271\n",
      "layer 5: 0.5562845303867403\n",
      "layer 6: 0.4017610497237569\n",
      "layer 7: 0.25496374309392267\n",
      "layer 8: 0.11202952348066299\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 0.705 | Reg loss: 0.017 | Tree loss: 0.705 | Accuracy: 0.551000 | 6.98 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 0.702 | Reg loss: 0.017 | Tree loss: 0.702 | Accuracy: 0.509000 | 4.138 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 0.697 | Reg loss: 0.017 | Tree loss: 0.697 | Accuracy: 0.531500 | 3.197 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 0.696 | Reg loss: 0.016 | Tree loss: 0.696 | Accuracy: 0.535000 | 2.725 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 0.694 | Reg loss: 0.016 | Tree loss: 0.694 | Accuracy: 0.560500 | 2.447 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 0.691 | Reg loss: 0.015 | Tree loss: 0.691 | Accuracy: 0.565500 | 2.23 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 0.687 | Reg loss: 0.015 | Tree loss: 0.687 | Accuracy: 0.576500 | 2.07 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 0.688 | Reg loss: 0.015 | Tree loss: 0.688 | Accuracy: 0.559000 | 1.978 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 0.687 | Reg loss: 0.015 | Tree loss: 0.687 | Accuracy: 0.568000 | 1.896 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 0.684 | Reg loss: 0.014 | Tree loss: 0.684 | Accuracy: 0.586000 | 1.839 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 0.689 | Reg loss: 0.014 | Tree loss: 0.689 | Accuracy: 0.573379 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 0.700 | Reg loss: 0.013 | Tree loss: 0.700 | Accuracy: 0.592000 | 2.513 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 0.700 | Reg loss: 0.013 | Tree loss: 0.700 | Accuracy: 0.604000 | 2.385 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 0.695 | Reg loss: 0.013 | Tree loss: 0.695 | Accuracy: 0.635000 | 2.288 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 0.690 | Reg loss: 0.013 | Tree loss: 0.690 | Accuracy: 0.629500 | 2.204 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 0.691 | Reg loss: 0.013 | Tree loss: 0.691 | Accuracy: 0.577000 | 2.132 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 0.693 | Reg loss: 0.013 | Tree loss: 0.693 | Accuracy: 0.561500 | 2.071 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 0.688 | Reg loss: 0.013 | Tree loss: 0.688 | Accuracy: 0.566000 | 2.014 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 0.690 | Reg loss: 0.014 | Tree loss: 0.690 | Accuracy: 0.565500 | 1.957 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 0.679 | Reg loss: 0.014 | Tree loss: 0.679 | Accuracy: 0.602500 | 1.902 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 0.685 | Reg loss: 0.014 | Tree loss: 0.685 | Accuracy: 0.581000 | 1.86 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 0.691 | Reg loss: 0.014 | Tree loss: 0.691 | Accuracy: 0.552901 | 1.818 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 0.699 | Reg loss: 0.013 | Tree loss: 0.699 | Accuracy: 0.551500 | 1.943 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 0.701 | Reg loss: 0.013 | Tree loss: 0.701 | Accuracy: 0.548500 | 1.904 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 0.696 | Reg loss: 0.013 | Tree loss: 0.696 | Accuracy: 0.656500 | 1.871 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 0.694 | Reg loss: 0.013 | Tree loss: 0.694 | Accuracy: 0.664500 | 1.839 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 0.689 | Reg loss: 0.013 | Tree loss: 0.689 | Accuracy: 0.618500 | 1.812 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 0.686 | Reg loss: 0.013 | Tree loss: 0.686 | Accuracy: 0.612000 | 1.788 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 0.683 | Reg loss: 0.013 | Tree loss: 0.683 | Accuracy: 0.606000 | 1.771 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 0.683 | Reg loss: 0.013 | Tree loss: 0.683 | Accuracy: 0.587500 | 1.752 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 0.689 | Reg loss: 0.013 | Tree loss: 0.689 | Accuracy: 0.561000 | 1.736 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 0.692 | Reg loss: 0.013 | Tree loss: 0.692 | Accuracy: 0.552000 | 1.723 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 0.687 | Reg loss: 0.013 | Tree loss: 0.687 | Accuracy: 0.552901 | 1.698 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 0.697 | Reg loss: 0.012 | Tree loss: 0.697 | Accuracy: 0.578500 | 2.106 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 0.699 | Reg loss: 0.012 | Tree loss: 0.699 | Accuracy: 0.583500 | 2.08 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 0.694 | Reg loss: 0.012 | Tree loss: 0.694 | Accuracy: 0.656000 | 2.051 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.673500 | 2.018 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 0.690 | Reg loss: 0.012 | Tree loss: 0.690 | Accuracy: 0.637500 | 1.988 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.607500 | 1.964 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.602500 | 1.942 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.578000 | 1.921 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 0.683 | Reg loss: 0.013 | Tree loss: 0.683 | Accuracy: 0.583500 | 1.901 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 0.682 | Reg loss: 0.013 | Tree loss: 0.682 | Accuracy: 0.579500 | 1.881 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 0.680 | Reg loss: 0.013 | Tree loss: 0.680 | Accuracy: 0.607509 | 1.86 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 0.696 | Reg loss: 0.012 | Tree loss: 0.696 | Accuracy: 0.621500 | 1.878 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.665000 | 1.859 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 0.691 | Reg loss: 0.012 | Tree loss: 0.691 | Accuracy: 0.678500 | 1.842 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 0.690 | Reg loss: 0.012 | Tree loss: 0.690 | Accuracy: 0.668500 | 1.826 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.605500 | 1.81 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.577000 | 1.792 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.582500 | 1.774 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.570500 | 1.761 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.578000 | 1.75 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 0.687 | Reg loss: 0.012 | Tree loss: 0.687 | Accuracy: 0.551000 | 1.742 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 0.676 | Reg loss: 0.012 | Tree loss: 0.676 | Accuracy: 0.617747 | 1.727 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 0.695 | Reg loss: 0.012 | Tree loss: 0.695 | Accuracy: 0.650000 | 1.939 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.656000 | 1.924 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.677000 | 1.913 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 0.694 | Reg loss: 0.012 | Tree loss: 0.694 | Accuracy: 0.664500 | 1.903 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.687000 | 1.894 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.633000 | 1.884 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 0.689 | Reg loss: 0.012 | Tree loss: 0.689 | Accuracy: 0.591500 | 1.875 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 0.689 | Reg loss: 0.012 | Tree loss: 0.689 | Accuracy: 0.568000 | 1.866 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.577000 | 1.855 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.591000 | 1.842 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 0.694 | Reg loss: 0.012 | Tree loss: 0.694 | Accuracy: 0.542662 | 1.828 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 0.696 | Reg loss: 0.012 | Tree loss: 0.696 | Accuracy: 0.651500 | 1.933 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.678500 | 1.923 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.680000 | 1.914 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 0.690 | Reg loss: 0.012 | Tree loss: 0.690 | Accuracy: 0.689500 | 1.904 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.703500 | 1.896 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.640000 | 1.888 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.598500 | 1.88 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.591000 | 1.87 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 0.683 | Reg loss: 0.012 | Tree loss: 0.683 | Accuracy: 0.578500 | 1.858 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.565500 | 1.848 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.593857 | 1.838 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 0.694 | Reg loss: 0.012 | Tree loss: 0.694 | Accuracy: 0.659500 | 1.885 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.653000 | 1.874 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 0.691 | Reg loss: 0.012 | Tree loss: 0.691 | Accuracy: 0.680500 | 1.864 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 0.687 | Reg loss: 0.012 | Tree loss: 0.687 | Accuracy: 0.675000 | 1.853 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.622500 | 1.843 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.611500 | 1.834 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.576500 | 1.824 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.578500 | 1.814 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.565500 | 1.805 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.576500 | 1.796 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.569966 | 1.786 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 0.696 | Reg loss: 0.012 | Tree loss: 0.696 | Accuracy: 0.635500 | 1.816 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.653000 | 1.81 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.685000 | 1.805 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.704000 | 1.799 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 0.687 | Reg loss: 0.012 | Tree loss: 0.687 | Accuracy: 0.687000 | 1.794 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 0.687 | Reg loss: 0.012 | Tree loss: 0.687 | Accuracy: 0.636000 | 1.791 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.620500 | 1.785 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.617500 | 1.781 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.587000 | 1.772 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 0.681 | Reg loss: 0.012 | Tree loss: 0.681 | Accuracy: 0.582000 | 1.764 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 0.667 | Reg loss: 0.012 | Tree loss: 0.667 | Accuracy: 0.624573 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.653500 | 1.828 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.723500 | 1.822 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.708000 | 1.814 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.618000 | 1.807 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.594000 | 1.799 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.574000 | 1.792 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.568500 | 1.784 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.571000 | 1.776 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.570500 | 1.769 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.582500 | 1.763 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 0.693 | Reg loss: 0.012 | Tree loss: 0.693 | Accuracy: 0.522184 | 1.755 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.674000 | 1.775 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 0.695 | Reg loss: 0.012 | Tree loss: 0.695 | Accuracy: 0.632000 | 1.769 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 0.691 | Reg loss: 0.012 | Tree loss: 0.691 | Accuracy: 0.658000 | 1.762 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 0.691 | Reg loss: 0.012 | Tree loss: 0.691 | Accuracy: 0.663000 | 1.756 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.677500 | 1.75 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.726500 | 1.744 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.713500 | 1.737 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.643000 | 1.73 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.584000 | 1.724 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 0.678 | Reg loss: 0.012 | Tree loss: 0.678 | Accuracy: 0.579000 | 1.719 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 0.679 | Reg loss: 0.012 | Tree loss: 0.679 | Accuracy: 0.552901 | 1.713 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 0.692 | Reg loss: 0.012 | Tree loss: 0.692 | Accuracy: 0.656500 | 1.826 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 0.689 | Reg loss: 0.012 | Tree loss: 0.689 | Accuracy: 0.703500 | 1.821 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.696500 | 1.816 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.639000 | 1.812 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.598500 | 1.808 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 0.676 | Reg loss: 0.012 | Tree loss: 0.676 | Accuracy: 0.605000 | 1.803 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 0.683 | Reg loss: 0.012 | Tree loss: 0.683 | Accuracy: 0.558500 | 1.799 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.569500 | 1.795 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 0.678 | Reg loss: 0.012 | Tree loss: 0.678 | Accuracy: 0.560000 | 1.791 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 0.672 | Reg loss: 0.012 | Tree loss: 0.672 | Accuracy: 0.573500 | 1.787 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 0.674 | Reg loss: 0.012 | Tree loss: 0.674 | Accuracy: 0.573379 | 1.78 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 0.691 | Reg loss: 0.012 | Tree loss: 0.691 | Accuracy: 0.665500 | 1.882 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 0.689 | Reg loss: 0.012 | Tree loss: 0.689 | Accuracy: 0.684500 | 1.874 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 0.687 | Reg loss: 0.012 | Tree loss: 0.687 | Accuracy: 0.676500 | 1.867 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 0.685 | Reg loss: 0.012 | Tree loss: 0.685 | Accuracy: 0.713500 | 1.861 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.721500 | 1.855 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.673500 | 1.85 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 0.675 | Reg loss: 0.012 | Tree loss: 0.675 | Accuracy: 0.614500 | 1.844 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 0.672 | Reg loss: 0.012 | Tree loss: 0.672 | Accuracy: 0.589500 | 1.839 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 0.673 | Reg loss: 0.012 | Tree loss: 0.673 | Accuracy: 0.566500 | 1.832 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 0.670 | Reg loss: 0.012 | Tree loss: 0.670 | Accuracy: 0.573500 | 1.825 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 0.672 | Reg loss: 0.012 | Tree loss: 0.672 | Accuracy: 0.556314 | 1.819 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 0.689 | Reg loss: 0.012 | Tree loss: 0.689 | Accuracy: 0.665500 | 1.844 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.680000 | 1.84 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.708500 | 1.836 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.704000 | 1.832 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 0.678 | Reg loss: 0.012 | Tree loss: 0.678 | Accuracy: 0.675500 | 1.829 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 0.674 | Reg loss: 0.012 | Tree loss: 0.674 | Accuracy: 0.620500 | 1.825 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 0.669 | Reg loss: 0.012 | Tree loss: 0.669 | Accuracy: 0.601000 | 1.822 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 0.669 | Reg loss: 0.012 | Tree loss: 0.669 | Accuracy: 0.590000 | 1.819 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 0.667 | Reg loss: 0.012 | Tree loss: 0.667 | Accuracy: 0.577000 | 1.815 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 0.666 | Reg loss: 0.012 | Tree loss: 0.666 | Accuracy: 0.579500 | 1.809 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 0.670 | Reg loss: 0.012 | Tree loss: 0.670 | Accuracy: 0.573379 | 1.804 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 0.688 | Reg loss: 0.012 | Tree loss: 0.688 | Accuracy: 0.658000 | 1.851 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 0.684 | Reg loss: 0.012 | Tree loss: 0.684 | Accuracy: 0.692500 | 1.848 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 0.683 | Reg loss: 0.012 | Tree loss: 0.683 | Accuracy: 0.712500 | 1.845 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 0.676 | Reg loss: 0.012 | Tree loss: 0.676 | Accuracy: 0.715000 | 1.842 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 0.674 | Reg loss: 0.012 | Tree loss: 0.674 | Accuracy: 0.644000 | 1.838 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 0.668 | Reg loss: 0.012 | Tree loss: 0.668 | Accuracy: 0.624000 | 1.835 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 0.662 | Reg loss: 0.012 | Tree loss: 0.662 | Accuracy: 0.601500 | 1.832 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 0.660 | Reg loss: 0.012 | Tree loss: 0.660 | Accuracy: 0.599000 | 1.829 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 0.672 | Reg loss: 0.012 | Tree loss: 0.672 | Accuracy: 0.553000 | 1.823 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 0.666 | Reg loss: 0.012 | Tree loss: 0.666 | Accuracy: 0.566500 | 1.818 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 0.654 | Reg loss: 0.012 | Tree loss: 0.654 | Accuracy: 0.627986 | 1.812 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 0.686 | Reg loss: 0.012 | Tree loss: 0.686 | Accuracy: 0.672000 | 1.835 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 0.683 | Reg loss: 0.012 | Tree loss: 0.683 | Accuracy: 0.705500 | 1.83 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 0.680 | Reg loss: 0.012 | Tree loss: 0.680 | Accuracy: 0.709500 | 1.826 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 0.669 | Reg loss: 0.012 | Tree loss: 0.669 | Accuracy: 0.717500 | 1.821 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 0.669 | Reg loss: 0.012 | Tree loss: 0.669 | Accuracy: 0.639500 | 1.817 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 0.664 | Reg loss: 0.012 | Tree loss: 0.664 | Accuracy: 0.602000 | 1.812 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 0.661 | Reg loss: 0.012 | Tree loss: 0.661 | Accuracy: 0.601500 | 1.807 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 0.663 | Reg loss: 0.012 | Tree loss: 0.663 | Accuracy: 0.572500 | 1.802 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 0.662 | Reg loss: 0.012 | Tree loss: 0.662 | Accuracy: 0.575000 | 1.798 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 0.654 | Reg loss: 0.012 | Tree loss: 0.654 | Accuracy: 0.594000 | 1.793 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.648464 | 1.788 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 0.682 | Reg loss: 0.012 | Tree loss: 0.682 | Accuracy: 0.697500 | 1.802 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 0.678 | Reg loss: 0.012 | Tree loss: 0.678 | Accuracy: 0.718500 | 1.799 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.717500 | 1.796 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 0.665 | Reg loss: 0.012 | Tree loss: 0.665 | Accuracy: 0.718500 | 1.793 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 0.660 | Reg loss: 0.012 | Tree loss: 0.660 | Accuracy: 0.655000 | 1.79 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 0.661 | Reg loss: 0.012 | Tree loss: 0.661 | Accuracy: 0.617500 | 1.788 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 0.659 | Reg loss: 0.012 | Tree loss: 0.659 | Accuracy: 0.584500 | 1.785 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 0.655 | Reg loss: 0.012 | Tree loss: 0.655 | Accuracy: 0.595500 | 1.782 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 0.664 | Reg loss: 0.012 | Tree loss: 0.664 | Accuracy: 0.565500 | 1.779 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 0.656 | Reg loss: 0.013 | Tree loss: 0.656 | Accuracy: 0.597500 | 1.775 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 0.647 | Reg loss: 0.013 | Tree loss: 0.647 | Accuracy: 0.668942 | 1.77 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 0.679 | Reg loss: 0.012 | Tree loss: 0.679 | Accuracy: 0.693500 | 1.808 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 0.677 | Reg loss: 0.012 | Tree loss: 0.677 | Accuracy: 0.710000 | 1.806 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 0.674 | Reg loss: 0.012 | Tree loss: 0.674 | Accuracy: 0.722500 | 1.803 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 0.665 | Reg loss: 0.012 | Tree loss: 0.665 | Accuracy: 0.715500 | 1.801 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 0.661 | Reg loss: 0.012 | Tree loss: 0.661 | Accuracy: 0.667000 | 1.798 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 0.656 | Reg loss: 0.012 | Tree loss: 0.656 | Accuracy: 0.633500 | 1.795 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 0.655 | Reg loss: 0.012 | Tree loss: 0.655 | Accuracy: 0.611000 | 1.793 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 0.656 | Reg loss: 0.012 | Tree loss: 0.656 | Accuracy: 0.580000 | 1.79 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 0.649 | Reg loss: 0.013 | Tree loss: 0.649 | Accuracy: 0.598500 | 1.786 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 0.642 | Reg loss: 0.013 | Tree loss: 0.642 | Accuracy: 0.644000 | 1.782 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 0.635 | Reg loss: 0.013 | Tree loss: 0.635 | Accuracy: 0.662116 | 1.777 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 0.672 | Reg loss: 0.012 | Tree loss: 0.672 | Accuracy: 0.732500 | 1.812 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 0.670 | Reg loss: 0.012 | Tree loss: 0.670 | Accuracy: 0.713000 | 1.809 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 0.661 | Reg loss: 0.012 | Tree loss: 0.661 | Accuracy: 0.714000 | 1.805 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 0.657 | Reg loss: 0.012 | Tree loss: 0.657 | Accuracy: 0.677000 | 1.802 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 0.656 | Reg loss: 0.012 | Tree loss: 0.656 | Accuracy: 0.635000 | 1.798 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 0.652 | Reg loss: 0.012 | Tree loss: 0.652 | Accuracy: 0.603000 | 1.794 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 0.653 | Reg loss: 0.013 | Tree loss: 0.653 | Accuracy: 0.602500 | 1.791 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 0.648 | Reg loss: 0.013 | Tree loss: 0.648 | Accuracy: 0.621000 | 1.787 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 0.650 | Reg loss: 0.013 | Tree loss: 0.650 | Accuracy: 0.614000 | 1.783 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.659500 | 1.78 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 0.636 | Reg loss: 0.013 | Tree loss: 0.636 | Accuracy: 0.682594 | 1.777 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 0.670 | Reg loss: 0.012 | Tree loss: 0.670 | Accuracy: 0.729000 | 1.788 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 0.667 | Reg loss: 0.012 | Tree loss: 0.667 | Accuracy: 0.728500 | 1.785 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 0.660 | Reg loss: 0.012 | Tree loss: 0.660 | Accuracy: 0.706000 | 1.782 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 0.656 | Reg loss: 0.012 | Tree loss: 0.656 | Accuracy: 0.673500 | 1.779 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 0.654 | Reg loss: 0.012 | Tree loss: 0.654 | Accuracy: 0.624500 | 1.777 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.634000 | 1.774 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 0.659 | Reg loss: 0.013 | Tree loss: 0.659 | Accuracy: 0.583000 | 1.771 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 0.635 | Reg loss: 0.013 | Tree loss: 0.635 | Accuracy: 0.652000 | 1.769 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 0.628 | Reg loss: 0.013 | Tree loss: 0.628 | Accuracy: 0.663500 | 1.768 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 0.631 | Reg loss: 0.013 | Tree loss: 0.631 | Accuracy: 0.684000 | 1.767 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 0.631 | Reg loss: 0.013 | Tree loss: 0.631 | Accuracy: 0.699659 | 1.765 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 0.662 | Reg loss: 0.012 | Tree loss: 0.662 | Accuracy: 0.734500 | 1.783 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 0.664 | Reg loss: 0.012 | Tree loss: 0.664 | Accuracy: 0.707500 | 1.781 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 0.651 | Reg loss: 0.013 | Tree loss: 0.651 | Accuracy: 0.713500 | 1.779 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.669500 | 1.777 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 0.654 | Reg loss: 0.013 | Tree loss: 0.654 | Accuracy: 0.617000 | 1.775 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 0.642 | Reg loss: 0.013 | Tree loss: 0.642 | Accuracy: 0.640000 | 1.773 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 0.641 | Reg loss: 0.013 | Tree loss: 0.641 | Accuracy: 0.631500 | 1.771 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 0.629 | Reg loss: 0.013 | Tree loss: 0.629 | Accuracy: 0.661500 | 1.768 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 0.631 | Reg loss: 0.013 | Tree loss: 0.631 | Accuracy: 0.669000 | 1.765 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 0.626 | Reg loss: 0.013 | Tree loss: 0.626 | Accuracy: 0.685500 | 1.763 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 0.618 | Reg loss: 0.013 | Tree loss: 0.618 | Accuracy: 0.754266 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 0.660 | Reg loss: 0.013 | Tree loss: 0.660 | Accuracy: 0.729000 | 1.787 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 0.655 | Reg loss: 0.013 | Tree loss: 0.655 | Accuracy: 0.700500 | 1.786 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 0.649 | Reg loss: 0.013 | Tree loss: 0.649 | Accuracy: 0.698500 | 1.783 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.652000 | 1.781 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 0.632 | Reg loss: 0.013 | Tree loss: 0.632 | Accuracy: 0.654500 | 1.778 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 0.636 | Reg loss: 0.013 | Tree loss: 0.636 | Accuracy: 0.636500 | 1.776 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 0.630 | Reg loss: 0.013 | Tree loss: 0.630 | Accuracy: 0.642000 | 1.772 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 0.627 | Reg loss: 0.013 | Tree loss: 0.627 | Accuracy: 0.647500 | 1.77 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 0.621 | Reg loss: 0.013 | Tree loss: 0.621 | Accuracy: 0.682500 | 1.768 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 0.614 | Reg loss: 0.013 | Tree loss: 0.614 | Accuracy: 0.703000 | 1.765 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 0.611 | Reg loss: 0.014 | Tree loss: 0.611 | Accuracy: 0.757679 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 0.655 | Reg loss: 0.013 | Tree loss: 0.655 | Accuracy: 0.719500 | 1.768 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 0.648 | Reg loss: 0.013 | Tree loss: 0.648 | Accuracy: 0.712500 | 1.766 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 0.637 | Reg loss: 0.013 | Tree loss: 0.637 | Accuracy: 0.706500 | 1.763 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 0.631 | Reg loss: 0.013 | Tree loss: 0.631 | Accuracy: 0.681500 | 1.76 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 0.634 | Reg loss: 0.013 | Tree loss: 0.634 | Accuracy: 0.639500 | 1.758 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 0.630 | Reg loss: 0.013 | Tree loss: 0.630 | Accuracy: 0.629000 | 1.754 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 0.642 | Reg loss: 0.013 | Tree loss: 0.642 | Accuracy: 0.616000 | 1.751 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 0.624 | Reg loss: 0.013 | Tree loss: 0.624 | Accuracy: 0.665500 | 1.749 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 0.605 | Reg loss: 0.014 | Tree loss: 0.605 | Accuracy: 0.740500 | 1.747 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 0.605 | Reg loss: 0.014 | Tree loss: 0.605 | Accuracy: 0.753000 | 1.745 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 0.618 | Reg loss: 0.014 | Tree loss: 0.618 | Accuracy: 0.726962 | 1.742 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 0.652 | Reg loss: 0.013 | Tree loss: 0.652 | Accuracy: 0.705500 | 1.764 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 0.634 | Reg loss: 0.013 | Tree loss: 0.634 | Accuracy: 0.717500 | 1.762 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 0.635 | Reg loss: 0.013 | Tree loss: 0.635 | Accuracy: 0.669000 | 1.76 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 0.628 | Reg loss: 0.013 | Tree loss: 0.628 | Accuracy: 0.653000 | 1.758 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 0.634 | Reg loss: 0.013 | Tree loss: 0.634 | Accuracy: 0.632000 | 1.757 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 0.635 | Reg loss: 0.013 | Tree loss: 0.635 | Accuracy: 0.619000 | 1.755 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 0.620 | Reg loss: 0.014 | Tree loss: 0.620 | Accuracy: 0.641500 | 1.753 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 0.604 | Reg loss: 0.014 | Tree loss: 0.604 | Accuracy: 0.715500 | 1.75 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 0.610 | Reg loss: 0.014 | Tree loss: 0.610 | Accuracy: 0.739500 | 1.748 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 0.604 | Reg loss: 0.014 | Tree loss: 0.604 | Accuracy: 0.762500 | 1.746 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 0.600 | Reg loss: 0.014 | Tree loss: 0.600 | Accuracy: 0.778157 | 1.744 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 0.646 | Reg loss: 0.013 | Tree loss: 0.646 | Accuracy: 0.696500 | 1.765 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 0.636 | Reg loss: 0.013 | Tree loss: 0.636 | Accuracy: 0.728500 | 1.764 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 0.619 | Reg loss: 0.014 | Tree loss: 0.619 | Accuracy: 0.733000 | 1.762 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 0.621 | Reg loss: 0.014 | Tree loss: 0.621 | Accuracy: 0.671000 | 1.76 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 0.625 | Reg loss: 0.014 | Tree loss: 0.625 | Accuracy: 0.639000 | 1.759 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 0.618 | Reg loss: 0.014 | Tree loss: 0.618 | Accuracy: 0.643000 | 1.757 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 0.610 | Reg loss: 0.014 | Tree loss: 0.610 | Accuracy: 0.666500 | 1.755 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 0.600 | Reg loss: 0.014 | Tree loss: 0.600 | Accuracy: 0.703000 | 1.753 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 0.589 | Reg loss: 0.014 | Tree loss: 0.589 | Accuracy: 0.779000 | 1.751 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 0.592 | Reg loss: 0.014 | Tree loss: 0.592 | Accuracy: 0.783500 | 1.749 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 0.603 | Reg loss: 0.014 | Tree loss: 0.603 | Accuracy: 0.757679 | 1.747 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 0.638 | Reg loss: 0.014 | Tree loss: 0.638 | Accuracy: 0.711500 | 1.76 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 0.640 | Reg loss: 0.014 | Tree loss: 0.640 | Accuracy: 0.684000 | 1.758 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 0.626 | Reg loss: 0.014 | Tree loss: 0.626 | Accuracy: 0.727500 | 1.755 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 0.616 | Reg loss: 0.014 | Tree loss: 0.616 | Accuracy: 0.711500 | 1.753 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 0.611 | Reg loss: 0.014 | Tree loss: 0.611 | Accuracy: 0.688500 | 1.751 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 0.620 | Reg loss: 0.014 | Tree loss: 0.620 | Accuracy: 0.637000 | 1.749 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 0.588 | Reg loss: 0.014 | Tree loss: 0.588 | Accuracy: 0.690500 | 1.746 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 0.595 | Reg loss: 0.014 | Tree loss: 0.595 | Accuracy: 0.699500 | 1.743 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 0.582 | Reg loss: 0.014 | Tree loss: 0.582 | Accuracy: 0.748000 | 1.741 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 0.578 | Reg loss: 0.014 | Tree loss: 0.578 | Accuracy: 0.803500 | 1.739 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 0.575 | Reg loss: 0.015 | Tree loss: 0.575 | Accuracy: 0.846416 | 1.737 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 0.634 | Reg loss: 0.014 | Tree loss: 0.634 | Accuracy: 0.694500 | 1.744 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 0.623 | Reg loss: 0.014 | Tree loss: 0.623 | Accuracy: 0.707500 | 1.742 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 0.615 | Reg loss: 0.014 | Tree loss: 0.615 | Accuracy: 0.737500 | 1.741 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 0.610 | Reg loss: 0.014 | Tree loss: 0.610 | Accuracy: 0.718500 | 1.739 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 0.611 | Reg loss: 0.014 | Tree loss: 0.611 | Accuracy: 0.667000 | 1.738 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 0.602 | Reg loss: 0.014 | Tree loss: 0.602 | Accuracy: 0.670500 | 1.737 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 0.591 | Reg loss: 0.014 | Tree loss: 0.591 | Accuracy: 0.694500 | 1.736 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 0.577 | Reg loss: 0.014 | Tree loss: 0.577 | Accuracy: 0.745500 | 1.734 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 0.581 | Reg loss: 0.015 | Tree loss: 0.581 | Accuracy: 0.799000 | 1.732 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 0.574 | Reg loss: 0.015 | Tree loss: 0.574 | Accuracy: 0.821500 | 1.73 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 0.567 | Reg loss: 0.015 | Tree loss: 0.567 | Accuracy: 0.825939 | 1.727 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 0.631 | Reg loss: 0.014 | Tree loss: 0.631 | Accuracy: 0.674500 | 1.751 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 0.620 | Reg loss: 0.014 | Tree loss: 0.620 | Accuracy: 0.715000 | 1.749 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 0.602 | Reg loss: 0.014 | Tree loss: 0.602 | Accuracy: 0.722000 | 1.748 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 0.603 | Reg loss: 0.014 | Tree loss: 0.603 | Accuracy: 0.681500 | 1.747 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 0.600 | Reg loss: 0.014 | Tree loss: 0.600 | Accuracy: 0.661500 | 1.745 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 0.595 | Reg loss: 0.015 | Tree loss: 0.595 | Accuracy: 0.663000 | 1.744 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 0.580 | Reg loss: 0.015 | Tree loss: 0.580 | Accuracy: 0.722000 | 1.742 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 0.575 | Reg loss: 0.015 | Tree loss: 0.575 | Accuracy: 0.761000 | 1.741 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 0.571 | Reg loss: 0.015 | Tree loss: 0.571 | Accuracy: 0.812500 | 1.738 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 0.576 | Reg loss: 0.015 | Tree loss: 0.576 | Accuracy: 0.820000 | 1.737 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 0.554 | Reg loss: 0.015 | Tree loss: 0.554 | Accuracy: 0.866894 | 1.734 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 0.625 | Reg loss: 0.015 | Tree loss: 0.625 | Accuracy: 0.668000 | 1.745 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 0.612 | Reg loss: 0.015 | Tree loss: 0.612 | Accuracy: 0.717500 | 1.742 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 0.598 | Reg loss: 0.015 | Tree loss: 0.598 | Accuracy: 0.714500 | 1.74 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 0.602 | Reg loss: 0.015 | Tree loss: 0.602 | Accuracy: 0.674500 | 1.738 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 0.604 | Reg loss: 0.015 | Tree loss: 0.604 | Accuracy: 0.659000 | 1.736 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 0.588 | Reg loss: 0.015 | Tree loss: 0.588 | Accuracy: 0.697500 | 1.734 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 0.568 | Reg loss: 0.015 | Tree loss: 0.568 | Accuracy: 0.772000 | 1.732 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 0.571 | Reg loss: 0.015 | Tree loss: 0.571 | Accuracy: 0.796000 | 1.729 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 0.571 | Reg loss: 0.015 | Tree loss: 0.571 | Accuracy: 0.797500 | 1.727 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 0.564 | Reg loss: 0.015 | Tree loss: 0.564 | Accuracy: 0.804500 | 1.725 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 0.550 | Reg loss: 0.015 | Tree loss: 0.550 | Accuracy: 0.819113 | 1.723 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 0.619 | Reg loss: 0.015 | Tree loss: 0.619 | Accuracy: 0.686000 | 1.729 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 0.602 | Reg loss: 0.015 | Tree loss: 0.602 | Accuracy: 0.720000 | 1.728 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 0.600 | Reg loss: 0.015 | Tree loss: 0.600 | Accuracy: 0.711000 | 1.727 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 0.592 | Reg loss: 0.015 | Tree loss: 0.592 | Accuracy: 0.689000 | 1.726 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 0.599 | Reg loss: 0.015 | Tree loss: 0.599 | Accuracy: 0.654000 | 1.724 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 0.596 | Reg loss: 0.015 | Tree loss: 0.596 | Accuracy: 0.643500 | 1.723 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 0.573 | Reg loss: 0.015 | Tree loss: 0.573 | Accuracy: 0.757000 | 1.722 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 0.559 | Reg loss: 0.015 | Tree loss: 0.559 | Accuracy: 0.818000 | 1.721 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 0.561 | Reg loss: 0.015 | Tree loss: 0.561 | Accuracy: 0.821000 | 1.72 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 0.561 | Reg loss: 0.015 | Tree loss: 0.561 | Accuracy: 0.819500 | 1.718 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 0.554 | Reg loss: 0.015 | Tree loss: 0.554 | Accuracy: 0.822526 | 1.717 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 0.623 | Reg loss: 0.015 | Tree loss: 0.623 | Accuracy: 0.674500 | 1.737 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 0.602 | Reg loss: 0.015 | Tree loss: 0.602 | Accuracy: 0.713500 | 1.736 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 0.585 | Reg loss: 0.015 | Tree loss: 0.585 | Accuracy: 0.725500 | 1.734 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 0.580 | Reg loss: 0.015 | Tree loss: 0.580 | Accuracy: 0.701000 | 1.734 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 0.590 | Reg loss: 0.015 | Tree loss: 0.590 | Accuracy: 0.660000 | 1.733 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 0.587 | Reg loss: 0.015 | Tree loss: 0.587 | Accuracy: 0.676000 | 1.732 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 0.572 | Reg loss: 0.015 | Tree loss: 0.572 | Accuracy: 0.743500 | 1.732 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 0.547 | Reg loss: 0.015 | Tree loss: 0.547 | Accuracy: 0.805500 | 1.73 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 0.553 | Reg loss: 0.015 | Tree loss: 0.553 | Accuracy: 0.820500 | 1.729 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 0.564 | Reg loss: 0.016 | Tree loss: 0.564 | Accuracy: 0.805500 | 1.729 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 0.555 | Reg loss: 0.016 | Tree loss: 0.555 | Accuracy: 0.808874 | 1.727 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 0.620 | Reg loss: 0.015 | Tree loss: 0.620 | Accuracy: 0.685500 | 1.743 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 0.605 | Reg loss: 0.015 | Tree loss: 0.605 | Accuracy: 0.694500 | 1.742 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 0.592 | Reg loss: 0.015 | Tree loss: 0.592 | Accuracy: 0.718000 | 1.74 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 0.588 | Reg loss: 0.015 | Tree loss: 0.588 | Accuracy: 0.694500 | 1.738 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 0.577 | Reg loss: 0.015 | Tree loss: 0.577 | Accuracy: 0.689000 | 1.737 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 0.580 | Reg loss: 0.015 | Tree loss: 0.580 | Accuracy: 0.667000 | 1.735 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 0.560 | Reg loss: 0.015 | Tree loss: 0.560 | Accuracy: 0.746000 | 1.734 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 0.547 | Reg loss: 0.016 | Tree loss: 0.547 | Accuracy: 0.789500 | 1.732 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 0.546 | Reg loss: 0.016 | Tree loss: 0.546 | Accuracy: 0.829500 | 1.73 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 0.549 | Reg loss: 0.016 | Tree loss: 0.549 | Accuracy: 0.792000 | 1.728 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 0.549 | Reg loss: 0.016 | Tree loss: 0.549 | Accuracy: 0.781570 | 1.726 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 0.623 | Reg loss: 0.015 | Tree loss: 0.623 | Accuracy: 0.676500 | 1.73 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 0.607 | Reg loss: 0.015 | Tree loss: 0.607 | Accuracy: 0.684500 | 1.728 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 0.579 | Reg loss: 0.015 | Tree loss: 0.579 | Accuracy: 0.744000 | 1.726 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 0.578 | Reg loss: 0.016 | Tree loss: 0.578 | Accuracy: 0.731000 | 1.724 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 0.587 | Reg loss: 0.016 | Tree loss: 0.587 | Accuracy: 0.669000 | 1.723 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 0.598 | Reg loss: 0.016 | Tree loss: 0.598 | Accuracy: 0.644000 | 1.721 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 0.567 | Reg loss: 0.016 | Tree loss: 0.567 | Accuracy: 0.728000 | 1.718 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 0.558 | Reg loss: 0.016 | Tree loss: 0.558 | Accuracy: 0.764000 | 1.717 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 0.551 | Reg loss: 0.016 | Tree loss: 0.551 | Accuracy: 0.800500 | 1.715 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 0.553 | Reg loss: 0.016 | Tree loss: 0.553 | Accuracy: 0.802500 | 1.715 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 0.550 | Reg loss: 0.016 | Tree loss: 0.550 | Accuracy: 0.802048 | 1.713 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 0.625 | Reg loss: 0.016 | Tree loss: 0.625 | Accuracy: 0.681000 | 1.728 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 0.586 | Reg loss: 0.016 | Tree loss: 0.586 | Accuracy: 0.714000 | 1.726 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 0.573 | Reg loss: 0.016 | Tree loss: 0.573 | Accuracy: 0.718000 | 1.725 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 0.584 | Reg loss: 0.016 | Tree loss: 0.584 | Accuracy: 0.698000 | 1.724 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 0.582 | Reg loss: 0.016 | Tree loss: 0.582 | Accuracy: 0.665000 | 1.723 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 0.580 | Reg loss: 0.016 | Tree loss: 0.580 | Accuracy: 0.715000 | 1.722 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 0.571 | Reg loss: 0.016 | Tree loss: 0.571 | Accuracy: 0.728500 | 1.721 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 0.545 | Reg loss: 0.016 | Tree loss: 0.545 | Accuracy: 0.779000 | 1.72 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 0.543 | Reg loss: 0.016 | Tree loss: 0.543 | Accuracy: 0.826000 | 1.718 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 0.546 | Reg loss: 0.016 | Tree loss: 0.546 | Accuracy: 0.814000 | 1.718 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 0.546 | Reg loss: 0.016 | Tree loss: 0.546 | Accuracy: 0.815700 | 1.716 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 0.618 | Reg loss: 0.016 | Tree loss: 0.618 | Accuracy: 0.691000 | 1.733 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 0.581 | Reg loss: 0.016 | Tree loss: 0.581 | Accuracy: 0.706500 | 1.732 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 0.580 | Reg loss: 0.016 | Tree loss: 0.580 | Accuracy: 0.720000 | 1.73 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 0.563 | Reg loss: 0.016 | Tree loss: 0.563 | Accuracy: 0.728500 | 1.728 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 0.565 | Reg loss: 0.016 | Tree loss: 0.565 | Accuracy: 0.706000 | 1.727 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 0.569 | Reg loss: 0.016 | Tree loss: 0.569 | Accuracy: 0.695000 | 1.725 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 0.558 | Reg loss: 0.016 | Tree loss: 0.558 | Accuracy: 0.746000 | 1.723 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 0.547 | Reg loss: 0.016 | Tree loss: 0.547 | Accuracy: 0.777500 | 1.721 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 0.536 | Reg loss: 0.016 | Tree loss: 0.536 | Accuracy: 0.809500 | 1.72 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 0.542 | Reg loss: 0.016 | Tree loss: 0.542 | Accuracy: 0.814000 | 1.718 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 0.524 | Reg loss: 0.016 | Tree loss: 0.524 | Accuracy: 0.802048 | 1.716 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 0.612 | Reg loss: 0.016 | Tree loss: 0.612 | Accuracy: 0.696000 | 1.721 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 0.589 | Reg loss: 0.016 | Tree loss: 0.589 | Accuracy: 0.708000 | 1.72 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 0.570 | Reg loss: 0.016 | Tree loss: 0.570 | Accuracy: 0.734000 | 1.718 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 0.568 | Reg loss: 0.016 | Tree loss: 0.568 | Accuracy: 0.712500 | 1.716 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 0.558 | Reg loss: 0.016 | Tree loss: 0.558 | Accuracy: 0.725000 | 1.715 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 0.555 | Reg loss: 0.016 | Tree loss: 0.555 | Accuracy: 0.730000 | 1.713 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 0.557 | Reg loss: 0.016 | Tree loss: 0.557 | Accuracy: 0.716000 | 1.711 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 0.552 | Reg loss: 0.016 | Tree loss: 0.552 | Accuracy: 0.720500 | 1.71 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 0.535 | Reg loss: 0.016 | Tree loss: 0.535 | Accuracy: 0.768000 | 1.709 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 0.534 | Reg loss: 0.016 | Tree loss: 0.534 | Accuracy: 0.820500 | 1.709 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 0.513 | Reg loss: 0.016 | Tree loss: 0.513 | Accuracy: 0.846416 | 1.707 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 0.611 | Reg loss: 0.016 | Tree loss: 0.611 | Accuracy: 0.695500 | 1.73 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 0.593 | Reg loss: 0.016 | Tree loss: 0.593 | Accuracy: 0.705500 | 1.729 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 0.576 | Reg loss: 0.016 | Tree loss: 0.576 | Accuracy: 0.720500 | 1.728 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 0.561 | Reg loss: 0.016 | Tree loss: 0.561 | Accuracy: 0.739000 | 1.727 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 0.565 | Reg loss: 0.016 | Tree loss: 0.565 | Accuracy: 0.706000 | 1.725 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 0.577 | Reg loss: 0.016 | Tree loss: 0.577 | Accuracy: 0.686500 | 1.724 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 0.559 | Reg loss: 0.016 | Tree loss: 0.559 | Accuracy: 0.726500 | 1.723 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 0.546 | Reg loss: 0.016 | Tree loss: 0.546 | Accuracy: 0.753500 | 1.722 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 0.535 | Reg loss: 0.016 | Tree loss: 0.535 | Accuracy: 0.778500 | 1.721 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 0.536 | Reg loss: 0.016 | Tree loss: 0.536 | Accuracy: 0.794500 | 1.72 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 0.530 | Reg loss: 0.016 | Tree loss: 0.530 | Accuracy: 0.825939 | 1.719 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 0.607 | Reg loss: 0.016 | Tree loss: 0.607 | Accuracy: 0.698000 | 1.73 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 0.598 | Reg loss: 0.016 | Tree loss: 0.598 | Accuracy: 0.693000 | 1.729 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 0.565 | Reg loss: 0.016 | Tree loss: 0.565 | Accuracy: 0.720000 | 1.728 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 0.561 | Reg loss: 0.016 | Tree loss: 0.561 | Accuracy: 0.709500 | 1.727 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 0.560 | Reg loss: 0.016 | Tree loss: 0.560 | Accuracy: 0.755000 | 1.725 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 0.549 | Reg loss: 0.016 | Tree loss: 0.549 | Accuracy: 0.747500 | 1.723 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 0.555 | Reg loss: 0.016 | Tree loss: 0.555 | Accuracy: 0.736000 | 1.722 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 0.535 | Reg loss: 0.016 | Tree loss: 0.535 | Accuracy: 0.773500 | 1.721 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 0.520 | Reg loss: 0.016 | Tree loss: 0.520 | Accuracy: 0.814000 | 1.719 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 0.526 | Reg loss: 0.017 | Tree loss: 0.526 | Accuracy: 0.823000 | 1.718 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 0.530 | Reg loss: 0.017 | Tree loss: 0.530 | Accuracy: 0.825939 | 1.716 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 0.608 | Reg loss: 0.016 | Tree loss: 0.608 | Accuracy: 0.701500 | 1.72 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 0.587 | Reg loss: 0.016 | Tree loss: 0.587 | Accuracy: 0.699000 | 1.718 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 0.557 | Reg loss: 0.016 | Tree loss: 0.557 | Accuracy: 0.745500 | 1.717 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 0.555 | Reg loss: 0.016 | Tree loss: 0.555 | Accuracy: 0.703500 | 1.715 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 0.547 | Reg loss: 0.016 | Tree loss: 0.547 | Accuracy: 0.773500 | 1.714 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 0.556 | Reg loss: 0.017 | Tree loss: 0.556 | Accuracy: 0.734500 | 1.713 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 0.545 | Reg loss: 0.017 | Tree loss: 0.545 | Accuracy: 0.737000 | 1.711 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 0.535 | Reg loss: 0.017 | Tree loss: 0.535 | Accuracy: 0.761000 | 1.709 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 0.521 | Reg loss: 0.017 | Tree loss: 0.521 | Accuracy: 0.795500 | 1.708 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 0.525 | Reg loss: 0.017 | Tree loss: 0.525 | Accuracy: 0.833000 | 1.706 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 0.531 | Reg loss: 0.017 | Tree loss: 0.531 | Accuracy: 0.822526 | 1.705 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 0.614 | Reg loss: 0.017 | Tree loss: 0.614 | Accuracy: 0.685000 | 1.71 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 0.585 | Reg loss: 0.017 | Tree loss: 0.585 | Accuracy: 0.691000 | 1.709 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 0.565 | Reg loss: 0.017 | Tree loss: 0.565 | Accuracy: 0.708500 | 1.708 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 0.544 | Reg loss: 0.017 | Tree loss: 0.544 | Accuracy: 0.739000 | 1.707 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 0.547 | Reg loss: 0.017 | Tree loss: 0.547 | Accuracy: 0.745000 | 1.707 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 0.564 | Reg loss: 0.017 | Tree loss: 0.564 | Accuracy: 0.725000 | 1.706 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 0.554 | Reg loss: 0.017 | Tree loss: 0.554 | Accuracy: 0.723000 | 1.705 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 0.537 | Reg loss: 0.017 | Tree loss: 0.537 | Accuracy: 0.755500 | 1.704 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 0.514 | Reg loss: 0.017 | Tree loss: 0.514 | Accuracy: 0.810500 | 1.703 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 0.513 | Reg loss: 0.017 | Tree loss: 0.513 | Accuracy: 0.834000 | 1.702 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 0.557 | Reg loss: 0.017 | Tree loss: 0.557 | Accuracy: 0.778157 | 1.7 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 0.612 | Reg loss: 0.017 | Tree loss: 0.612 | Accuracy: 0.697000 | 1.715 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 0.594 | Reg loss: 0.017 | Tree loss: 0.594 | Accuracy: 0.685000 | 1.715 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 0.561 | Reg loss: 0.017 | Tree loss: 0.561 | Accuracy: 0.711500 | 1.714 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 0.532 | Reg loss: 0.017 | Tree loss: 0.532 | Accuracy: 0.770500 | 1.714 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 0.527 | Reg loss: 0.017 | Tree loss: 0.527 | Accuracy: 0.794000 | 1.713 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 0.544 | Reg loss: 0.017 | Tree loss: 0.544 | Accuracy: 0.744000 | 1.712 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 0.548 | Reg loss: 0.017 | Tree loss: 0.548 | Accuracy: 0.723000 | 1.712 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 0.527 | Reg loss: 0.017 | Tree loss: 0.527 | Accuracy: 0.764000 | 1.71 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 0.523 | Reg loss: 0.017 | Tree loss: 0.523 | Accuracy: 0.781500 | 1.709 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 0.511 | Reg loss: 0.017 | Tree loss: 0.511 | Accuracy: 0.839000 | 1.709 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 0.509 | Reg loss: 0.017 | Tree loss: 0.509 | Accuracy: 0.819113 | 1.707 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 0.619 | Reg loss: 0.017 | Tree loss: 0.619 | Accuracy: 0.676500 | 1.713 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 0.572 | Reg loss: 0.017 | Tree loss: 0.572 | Accuracy: 0.702500 | 1.711 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 0.558 | Reg loss: 0.017 | Tree loss: 0.558 | Accuracy: 0.709000 | 1.71 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 0.538 | Reg loss: 0.017 | Tree loss: 0.538 | Accuracy: 0.776000 | 1.708 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 0.529 | Reg loss: 0.017 | Tree loss: 0.529 | Accuracy: 0.781500 | 1.707 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 0.551 | Reg loss: 0.017 | Tree loss: 0.551 | Accuracy: 0.726500 | 1.706 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 0.548 | Reg loss: 0.017 | Tree loss: 0.548 | Accuracy: 0.711500 | 1.704 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 0.528 | Reg loss: 0.017 | Tree loss: 0.528 | Accuracy: 0.752500 | 1.702 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 0.510 | Reg loss: 0.017 | Tree loss: 0.510 | Accuracy: 0.794000 | 1.701 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 0.510 | Reg loss: 0.017 | Tree loss: 0.510 | Accuracy: 0.840500 | 1.7 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 0.502 | Reg loss: 0.017 | Tree loss: 0.502 | Accuracy: 0.836177 | 1.698 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 0.599 | Reg loss: 0.017 | Tree loss: 0.599 | Accuracy: 0.689000 | 1.703 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 0.589 | Reg loss: 0.017 | Tree loss: 0.589 | Accuracy: 0.690500 | 1.702 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 0.571 | Reg loss: 0.017 | Tree loss: 0.571 | Accuracy: 0.701500 | 1.701 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 0.528 | Reg loss: 0.017 | Tree loss: 0.528 | Accuracy: 0.753000 | 1.7 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 0.523 | Reg loss: 0.017 | Tree loss: 0.523 | Accuracy: 0.760500 | 1.699 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 0.541 | Reg loss: 0.017 | Tree loss: 0.541 | Accuracy: 0.761000 | 1.699 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 0.542 | Reg loss: 0.017 | Tree loss: 0.542 | Accuracy: 0.738500 | 1.698 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 0.530 | Reg loss: 0.017 | Tree loss: 0.530 | Accuracy: 0.749500 | 1.697 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 0.515 | Reg loss: 0.017 | Tree loss: 0.515 | Accuracy: 0.791000 | 1.696 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 0.503 | Reg loss: 0.017 | Tree loss: 0.503 | Accuracy: 0.814000 | 1.694 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 0.514 | Reg loss: 0.017 | Tree loss: 0.514 | Accuracy: 0.829352 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 0.603 | Reg loss: 0.017 | Tree loss: 0.603 | Accuracy: 0.695000 | 1.708 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 0.580 | Reg loss: 0.017 | Tree loss: 0.580 | Accuracy: 0.686500 | 1.707 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 0.545 | Reg loss: 0.017 | Tree loss: 0.545 | Accuracy: 0.722000 | 1.706 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 0.529 | Reg loss: 0.017 | Tree loss: 0.529 | Accuracy: 0.778500 | 1.705 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 0.527 | Reg loss: 0.017 | Tree loss: 0.527 | Accuracy: 0.777500 | 1.705 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 0.519 | Reg loss: 0.017 | Tree loss: 0.519 | Accuracy: 0.767000 | 1.704 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 0.530 | Reg loss: 0.017 | Tree loss: 0.530 | Accuracy: 0.754500 | 1.703 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 0.528 | Reg loss: 0.017 | Tree loss: 0.528 | Accuracy: 0.759500 | 1.702 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 0.499 | Reg loss: 0.017 | Tree loss: 0.499 | Accuracy: 0.809000 | 1.701 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 0.500 | Reg loss: 0.017 | Tree loss: 0.500 | Accuracy: 0.819000 | 1.699 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 0.499 | Reg loss: 0.017 | Tree loss: 0.499 | Accuracy: 0.843003 | 1.698 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 0.599 | Reg loss: 0.017 | Tree loss: 0.599 | Accuracy: 0.685000 | 1.712 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 0.583 | Reg loss: 0.017 | Tree loss: 0.583 | Accuracy: 0.690500 | 1.711 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 0.544 | Reg loss: 0.017 | Tree loss: 0.544 | Accuracy: 0.728500 | 1.71 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 0.517 | Reg loss: 0.017 | Tree loss: 0.517 | Accuracy: 0.805500 | 1.708 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 0.515 | Reg loss: 0.017 | Tree loss: 0.515 | Accuracy: 0.797000 | 1.707 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 0.531 | Reg loss: 0.017 | Tree loss: 0.531 | Accuracy: 0.770500 | 1.706 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 0.526 | Reg loss: 0.017 | Tree loss: 0.526 | Accuracy: 0.750000 | 1.704 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 0.510 | Reg loss: 0.017 | Tree loss: 0.510 | Accuracy: 0.775000 | 1.703 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 0.512 | Reg loss: 0.017 | Tree loss: 0.512 | Accuracy: 0.784500 | 1.701 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 0.499 | Reg loss: 0.017 | Tree loss: 0.499 | Accuracy: 0.801000 | 1.7 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 0.483 | Reg loss: 0.017 | Tree loss: 0.483 | Accuracy: 0.846416 | 1.699 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 0.591 | Reg loss: 0.017 | Tree loss: 0.591 | Accuracy: 0.685500 | 1.704 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 0.590 | Reg loss: 0.017 | Tree loss: 0.590 | Accuracy: 0.701000 | 1.702 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 0.561 | Reg loss: 0.017 | Tree loss: 0.561 | Accuracy: 0.715500 | 1.701 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 0.532 | Reg loss: 0.017 | Tree loss: 0.532 | Accuracy: 0.771000 | 1.7 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 0.514 | Reg loss: 0.017 | Tree loss: 0.514 | Accuracy: 0.797000 | 1.698 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 0.511 | Reg loss: 0.017 | Tree loss: 0.511 | Accuracy: 0.786000 | 1.697 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 0.532 | Reg loss: 0.017 | Tree loss: 0.532 | Accuracy: 0.738000 | 1.696 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 0.524 | Reg loss: 0.017 | Tree loss: 0.524 | Accuracy: 0.738500 | 1.694 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 0.531 | Reg loss: 0.017 | Tree loss: 0.531 | Accuracy: 0.754500 | 1.693 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 0.492 | Reg loss: 0.017 | Tree loss: 0.492 | Accuracy: 0.803000 | 1.692 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 0.482 | Reg loss: 0.017 | Tree loss: 0.482 | Accuracy: 0.870307 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 0.577 | Reg loss: 0.017 | Tree loss: 0.577 | Accuracy: 0.710000 | 1.702 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 0.584 | Reg loss: 0.017 | Tree loss: 0.584 | Accuracy: 0.714500 | 1.702 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 0.572 | Reg loss: 0.017 | Tree loss: 0.572 | Accuracy: 0.721500 | 1.701 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 0.531 | Reg loss: 0.017 | Tree loss: 0.531 | Accuracy: 0.771000 | 1.7 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 0.511 | Reg loss: 0.017 | Tree loss: 0.511 | Accuracy: 0.797500 | 1.7 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 0.502 | Reg loss: 0.017 | Tree loss: 0.502 | Accuracy: 0.793000 | 1.699 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 0.530 | Reg loss: 0.017 | Tree loss: 0.530 | Accuracy: 0.752000 | 1.698 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 0.531 | Reg loss: 0.017 | Tree loss: 0.531 | Accuracy: 0.736500 | 1.697 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 0.522 | Reg loss: 0.017 | Tree loss: 0.522 | Accuracy: 0.751500 | 1.696 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 0.491 | Reg loss: 0.018 | Tree loss: 0.491 | Accuracy: 0.797000 | 1.695 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 0.474 | Reg loss: 0.018 | Tree loss: 0.474 | Accuracy: 0.822526 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 0.571 | Reg loss: 0.017 | Tree loss: 0.571 | Accuracy: 0.710500 | 1.706 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 0.572 | Reg loss: 0.017 | Tree loss: 0.572 | Accuracy: 0.719000 | 1.705 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 0.549 | Reg loss: 0.017 | Tree loss: 0.549 | Accuracy: 0.740500 | 1.704 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 0.528 | Reg loss: 0.017 | Tree loss: 0.528 | Accuracy: 0.786000 | 1.703 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 0.510 | Reg loss: 0.017 | Tree loss: 0.510 | Accuracy: 0.806500 | 1.701 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 0.513 | Reg loss: 0.017 | Tree loss: 0.513 | Accuracy: 0.774500 | 1.7 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 0.523 | Reg loss: 0.018 | Tree loss: 0.523 | Accuracy: 0.753500 | 1.699 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 0.530 | Reg loss: 0.018 | Tree loss: 0.530 | Accuracy: 0.744000 | 1.697 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 0.514 | Reg loss: 0.018 | Tree loss: 0.514 | Accuracy: 0.764000 | 1.696 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 0.490 | Reg loss: 0.018 | Tree loss: 0.490 | Accuracy: 0.800000 | 1.695 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 0.486 | Reg loss: 0.018 | Tree loss: 0.486 | Accuracy: 0.802048 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 0.594 | Reg loss: 0.017 | Tree loss: 0.594 | Accuracy: 0.693000 | 1.698 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 0.572 | Reg loss: 0.017 | Tree loss: 0.572 | Accuracy: 0.706500 | 1.697 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 0.548 | Reg loss: 0.018 | Tree loss: 0.548 | Accuracy: 0.731500 | 1.696 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 0.514 | Reg loss: 0.018 | Tree loss: 0.514 | Accuracy: 0.799000 | 1.695 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 0.504 | Reg loss: 0.018 | Tree loss: 0.504 | Accuracy: 0.801500 | 1.694 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 0.505 | Reg loss: 0.018 | Tree loss: 0.505 | Accuracy: 0.788500 | 1.692 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 0.511 | Reg loss: 0.018 | Tree loss: 0.511 | Accuracy: 0.766000 | 1.691 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 0.508 | Reg loss: 0.018 | Tree loss: 0.508 | Accuracy: 0.779500 | 1.69 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 0.492 | Reg loss: 0.018 | Tree loss: 0.492 | Accuracy: 0.784500 | 1.689 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 0.481 | Reg loss: 0.018 | Tree loss: 0.481 | Accuracy: 0.810500 | 1.688 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 0.505 | Reg loss: 0.018 | Tree loss: 0.505 | Accuracy: 0.839590 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 0.563 | Reg loss: 0.018 | Tree loss: 0.563 | Accuracy: 0.720500 | 1.699 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 0.568 | Reg loss: 0.018 | Tree loss: 0.568 | Accuracy: 0.734500 | 1.698 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 0.539 | Reg loss: 0.018 | Tree loss: 0.539 | Accuracy: 0.760500 | 1.697 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 0.518 | Reg loss: 0.018 | Tree loss: 0.518 | Accuracy: 0.789000 | 1.697 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 0.509 | Reg loss: 0.018 | Tree loss: 0.509 | Accuracy: 0.795000 | 1.697 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 0.494 | Reg loss: 0.018 | Tree loss: 0.494 | Accuracy: 0.793000 | 1.696 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 0.525 | Reg loss: 0.018 | Tree loss: 0.525 | Accuracy: 0.752500 | 1.695 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 0.507 | Reg loss: 0.018 | Tree loss: 0.507 | Accuracy: 0.766500 | 1.694 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 0.502 | Reg loss: 0.018 | Tree loss: 0.502 | Accuracy: 0.771000 | 1.693 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 0.476 | Reg loss: 0.018 | Tree loss: 0.476 | Accuracy: 0.808000 | 1.693 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 0.478 | Reg loss: 0.018 | Tree loss: 0.478 | Accuracy: 0.843003 | 1.691 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 0.573 | Reg loss: 0.018 | Tree loss: 0.573 | Accuracy: 0.708500 | 1.7 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 0.567 | Reg loss: 0.018 | Tree loss: 0.567 | Accuracy: 0.733500 | 1.7 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 0.534 | Reg loss: 0.018 | Tree loss: 0.534 | Accuracy: 0.757000 | 1.699 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 0.502 | Reg loss: 0.018 | Tree loss: 0.502 | Accuracy: 0.794500 | 1.698 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 0.496 | Reg loss: 0.018 | Tree loss: 0.496 | Accuracy: 0.818500 | 1.697 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 0.497 | Reg loss: 0.018 | Tree loss: 0.497 | Accuracy: 0.805500 | 1.697 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 0.510 | Reg loss: 0.018 | Tree loss: 0.510 | Accuracy: 0.775000 | 1.696 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 0.494 | Reg loss: 0.018 | Tree loss: 0.494 | Accuracy: 0.781000 | 1.695 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 0.495 | Reg loss: 0.018 | Tree loss: 0.495 | Accuracy: 0.767000 | 1.694 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 0.486 | Reg loss: 0.018 | Tree loss: 0.486 | Accuracy: 0.794500 | 1.693 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 0.478 | Reg loss: 0.018 | Tree loss: 0.478 | Accuracy: 0.798635 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 0.567 | Reg loss: 0.018 | Tree loss: 0.567 | Accuracy: 0.724500 | 1.699 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 0.561 | Reg loss: 0.018 | Tree loss: 0.561 | Accuracy: 0.752500 | 1.698 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 0.543 | Reg loss: 0.018 | Tree loss: 0.543 | Accuracy: 0.745500 | 1.696 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 0.508 | Reg loss: 0.018 | Tree loss: 0.508 | Accuracy: 0.779500 | 1.695 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 0.495 | Reg loss: 0.018 | Tree loss: 0.495 | Accuracy: 0.812000 | 1.694 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 0.486 | Reg loss: 0.018 | Tree loss: 0.486 | Accuracy: 0.807000 | 1.693 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 0.506 | Reg loss: 0.018 | Tree loss: 0.506 | Accuracy: 0.769500 | 1.692 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 0.490 | Reg loss: 0.018 | Tree loss: 0.490 | Accuracy: 0.798000 | 1.69 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 0.507 | Reg loss: 0.018 | Tree loss: 0.507 | Accuracy: 0.767000 | 1.689 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 0.479 | Reg loss: 0.018 | Tree loss: 0.479 | Accuracy: 0.797000 | 1.688 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 0.487 | Reg loss: 0.018 | Tree loss: 0.487 | Accuracy: 0.798635 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 0.555 | Reg loss: 0.018 | Tree loss: 0.555 | Accuracy: 0.729500 | 1.691 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 0.580 | Reg loss: 0.018 | Tree loss: 0.580 | Accuracy: 0.739500 | 1.69 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 0.520 | Reg loss: 0.018 | Tree loss: 0.520 | Accuracy: 0.784000 | 1.69 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 0.508 | Reg loss: 0.018 | Tree loss: 0.508 | Accuracy: 0.785500 | 1.689 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 0.492 | Reg loss: 0.018 | Tree loss: 0.492 | Accuracy: 0.812000 | 1.688 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 0.491 | Reg loss: 0.018 | Tree loss: 0.491 | Accuracy: 0.800000 | 1.688 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 0.512 | Reg loss: 0.018 | Tree loss: 0.512 | Accuracy: 0.770000 | 1.687 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 0.511 | Reg loss: 0.018 | Tree loss: 0.511 | Accuracy: 0.764000 | 1.686 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 0.484 | Reg loss: 0.018 | Tree loss: 0.484 | Accuracy: 0.787000 | 1.685 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 0.474 | Reg loss: 0.018 | Tree loss: 0.474 | Accuracy: 0.788000 | 1.684 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 0.489 | Reg loss: 0.018 | Tree loss: 0.489 | Accuracy: 0.774744 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 0.565 | Reg loss: 0.018 | Tree loss: 0.565 | Accuracy: 0.730000 | 1.695 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 0.564 | Reg loss: 0.018 | Tree loss: 0.564 | Accuracy: 0.742500 | 1.695 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 0.534 | Reg loss: 0.018 | Tree loss: 0.534 | Accuracy: 0.748500 | 1.694 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 0.508 | Reg loss: 0.018 | Tree loss: 0.508 | Accuracy: 0.782000 | 1.693 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 0.496 | Reg loss: 0.018 | Tree loss: 0.496 | Accuracy: 0.805500 | 1.693 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 0.502 | Reg loss: 0.018 | Tree loss: 0.502 | Accuracy: 0.781500 | 1.692 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 0.499 | Reg loss: 0.018 | Tree loss: 0.499 | Accuracy: 0.766500 | 1.692 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 0.503 | Reg loss: 0.018 | Tree loss: 0.503 | Accuracy: 0.767500 | 1.691 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 0.487 | Reg loss: 0.018 | Tree loss: 0.487 | Accuracy: 0.779000 | 1.69 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 0.493 | Reg loss: 0.018 | Tree loss: 0.493 | Accuracy: 0.774000 | 1.689 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 0.456 | Reg loss: 0.018 | Tree loss: 0.456 | Accuracy: 0.815700 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 0.545 | Reg loss: 0.018 | Tree loss: 0.545 | Accuracy: 0.750000 | 1.694 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 0.555 | Reg loss: 0.018 | Tree loss: 0.555 | Accuracy: 0.765500 | 1.693 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 0.543 | Reg loss: 0.018 | Tree loss: 0.543 | Accuracy: 0.756000 | 1.692 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 0.517 | Reg loss: 0.018 | Tree loss: 0.517 | Accuracy: 0.767000 | 1.691 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 0.491 | Reg loss: 0.018 | Tree loss: 0.491 | Accuracy: 0.805500 | 1.69 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 0.487 | Reg loss: 0.018 | Tree loss: 0.487 | Accuracy: 0.806500 | 1.689 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 0.497 | Reg loss: 0.018 | Tree loss: 0.497 | Accuracy: 0.791000 | 1.688 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 0.499 | Reg loss: 0.018 | Tree loss: 0.499 | Accuracy: 0.783500 | 1.687 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 0.490 | Reg loss: 0.018 | Tree loss: 0.490 | Accuracy: 0.779000 | 1.686 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 0.474 | Reg loss: 0.018 | Tree loss: 0.474 | Accuracy: 0.789500 | 1.685 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 0.473 | Reg loss: 0.018 | Tree loss: 0.473 | Accuracy: 0.798635 | 1.684 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 0.556 | Reg loss: 0.018 | Tree loss: 0.556 | Accuracy: 0.738500 | 1.687 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 0.547 | Reg loss: 0.018 | Tree loss: 0.547 | Accuracy: 0.761500 | 1.686 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 0.532 | Reg loss: 0.018 | Tree loss: 0.532 | Accuracy: 0.765000 | 1.686 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 0.512 | Reg loss: 0.018 | Tree loss: 0.512 | Accuracy: 0.780500 | 1.685 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 0.494 | Reg loss: 0.018 | Tree loss: 0.494 | Accuracy: 0.798500 | 1.685 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 0.480 | Reg loss: 0.018 | Tree loss: 0.480 | Accuracy: 0.810500 | 1.684 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 0.494 | Reg loss: 0.018 | Tree loss: 0.494 | Accuracy: 0.787500 | 1.684 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 0.479 | Reg loss: 0.018 | Tree loss: 0.479 | Accuracy: 0.792000 | 1.683 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 0.495 | Reg loss: 0.018 | Tree loss: 0.495 | Accuracy: 0.764000 | 1.682 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 0.475 | Reg loss: 0.018 | Tree loss: 0.475 | Accuracy: 0.796500 | 1.681 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 0.473 | Reg loss: 0.018 | Tree loss: 0.473 | Accuracy: 0.791809 | 1.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 0.554 | Reg loss: 0.018 | Tree loss: 0.554 | Accuracy: 0.728500 | 1.691 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 0.551 | Reg loss: 0.018 | Tree loss: 0.551 | Accuracy: 0.765000 | 1.691 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 0.520 | Reg loss: 0.018 | Tree loss: 0.520 | Accuracy: 0.767500 | 1.69 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 0.512 | Reg loss: 0.018 | Tree loss: 0.512 | Accuracy: 0.784500 | 1.689 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 0.487 | Reg loss: 0.018 | Tree loss: 0.487 | Accuracy: 0.795500 | 1.689 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 0.472 | Reg loss: 0.018 | Tree loss: 0.472 | Accuracy: 0.813000 | 1.688 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 0.497 | Reg loss: 0.018 | Tree loss: 0.497 | Accuracy: 0.789000 | 1.688 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 0.495 | Reg loss: 0.018 | Tree loss: 0.495 | Accuracy: 0.770500 | 1.687 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 0.499 | Reg loss: 0.018 | Tree loss: 0.499 | Accuracy: 0.760000 | 1.686 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 0.493 | Reg loss: 0.018 | Tree loss: 0.493 | Accuracy: 0.764000 | 1.685 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 0.477 | Reg loss: 0.018 | Tree loss: 0.477 | Accuracy: 0.784983 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 0.538 | Reg loss: 0.018 | Tree loss: 0.538 | Accuracy: 0.747000 | 1.694 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 0.556 | Reg loss: 0.018 | Tree loss: 0.556 | Accuracy: 0.754000 | 1.694 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 0.538 | Reg loss: 0.018 | Tree loss: 0.538 | Accuracy: 0.760500 | 1.693 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 0.513 | Reg loss: 0.018 | Tree loss: 0.513 | Accuracy: 0.765500 | 1.692 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 0.482 | Reg loss: 0.018 | Tree loss: 0.482 | Accuracy: 0.806500 | 1.691 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 0.479 | Reg loss: 0.018 | Tree loss: 0.479 | Accuracy: 0.800000 | 1.69 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 0.485 | Reg loss: 0.018 | Tree loss: 0.485 | Accuracy: 0.803000 | 1.689 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 0.499 | Reg loss: 0.018 | Tree loss: 0.499 | Accuracy: 0.776500 | 1.688 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 0.497 | Reg loss: 0.018 | Tree loss: 0.497 | Accuracy: 0.772500 | 1.687 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 0.491 | Reg loss: 0.018 | Tree loss: 0.491 | Accuracy: 0.765000 | 1.686 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 0.452 | Reg loss: 0.018 | Tree loss: 0.452 | Accuracy: 0.812287 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 0.536 | Reg loss: 0.018 | Tree loss: 0.536 | Accuracy: 0.753500 | 1.688 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 0.555 | Reg loss: 0.018 | Tree loss: 0.555 | Accuracy: 0.758000 | 1.687 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 0.525 | Reg loss: 0.018 | Tree loss: 0.525 | Accuracy: 0.773000 | 1.686 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 0.507 | Reg loss: 0.018 | Tree loss: 0.507 | Accuracy: 0.784500 | 1.686 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 0.501 | Reg loss: 0.018 | Tree loss: 0.501 | Accuracy: 0.781000 | 1.685 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 0.476 | Reg loss: 0.018 | Tree loss: 0.476 | Accuracy: 0.828500 | 1.684 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 0.476 | Reg loss: 0.018 | Tree loss: 0.476 | Accuracy: 0.811500 | 1.682 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 0.489 | Reg loss: 0.018 | Tree loss: 0.489 | Accuracy: 0.789000 | 1.682 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 0.472 | Reg loss: 0.018 | Tree loss: 0.472 | Accuracy: 0.792500 | 1.681 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 0.473 | Reg loss: 0.018 | Tree loss: 0.473 | Accuracy: 0.774500 | 1.68 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 0.455 | Reg loss: 0.018 | Tree loss: 0.455 | Accuracy: 0.812287 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 0.548 | Reg loss: 0.018 | Tree loss: 0.548 | Accuracy: 0.745000 | 1.688 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 0.541 | Reg loss: 0.018 | Tree loss: 0.541 | Accuracy: 0.739000 | 1.688 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 0.506 | Reg loss: 0.018 | Tree loss: 0.506 | Accuracy: 0.774000 | 1.687 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 0.510 | Reg loss: 0.018 | Tree loss: 0.510 | Accuracy: 0.771000 | 1.687 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 0.489 | Reg loss: 0.018 | Tree loss: 0.489 | Accuracy: 0.798000 | 1.686 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 0.484 | Reg loss: 0.018 | Tree loss: 0.484 | Accuracy: 0.788500 | 1.685 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 0.490 | Reg loss: 0.018 | Tree loss: 0.490 | Accuracy: 0.787500 | 1.685 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 0.478 | Reg loss: 0.018 | Tree loss: 0.478 | Accuracy: 0.786000 | 1.684 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 0.463 | Reg loss: 0.018 | Tree loss: 0.463 | Accuracy: 0.799000 | 1.683 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 0.461 | Reg loss: 0.018 | Tree loss: 0.461 | Accuracy: 0.808000 | 1.682 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 0.471 | Reg loss: 0.018 | Tree loss: 0.471 | Accuracy: 0.815700 | 1.681 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 0.541 | Reg loss: 0.018 | Tree loss: 0.541 | Accuracy: 0.749500 | 1.691 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 0.525 | Reg loss: 0.018 | Tree loss: 0.525 | Accuracy: 0.773500 | 1.691 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 0.523 | Reg loss: 0.018 | Tree loss: 0.523 | Accuracy: 0.760500 | 1.69 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 0.503 | Reg loss: 0.018 | Tree loss: 0.503 | Accuracy: 0.764000 | 1.689 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 0.483 | Reg loss: 0.018 | Tree loss: 0.483 | Accuracy: 0.800000 | 1.688 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 0.477 | Reg loss: 0.018 | Tree loss: 0.477 | Accuracy: 0.799500 | 1.687 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 0.470 | Reg loss: 0.018 | Tree loss: 0.470 | Accuracy: 0.806500 | 1.686 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 0.475 | Reg loss: 0.018 | Tree loss: 0.475 | Accuracy: 0.784000 | 1.685 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 0.477 | Reg loss: 0.018 | Tree loss: 0.477 | Accuracy: 0.779500 | 1.684 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 0.474 | Reg loss: 0.018 | Tree loss: 0.474 | Accuracy: 0.778500 | 1.683 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 0.471 | Reg loss: 0.018 | Tree loss: 0.471 | Accuracy: 0.802048 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 0.529 | Reg loss: 0.018 | Tree loss: 0.529 | Accuracy: 0.756500 | 1.685 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 0.546 | Reg loss: 0.018 | Tree loss: 0.546 | Accuracy: 0.747000 | 1.684 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 0.525 | Reg loss: 0.018 | Tree loss: 0.525 | Accuracy: 0.744500 | 1.683 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 0.493 | Reg loss: 0.018 | Tree loss: 0.493 | Accuracy: 0.786000 | 1.682 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 0.484 | Reg loss: 0.018 | Tree loss: 0.484 | Accuracy: 0.790500 | 1.681 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 0.488 | Reg loss: 0.018 | Tree loss: 0.488 | Accuracy: 0.774000 | 1.681 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 0.475 | Reg loss: 0.018 | Tree loss: 0.475 | Accuracy: 0.796500 | 1.679 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 0.480 | Reg loss: 0.018 | Tree loss: 0.480 | Accuracy: 0.786500 | 1.678 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 0.489 | Reg loss: 0.018 | Tree loss: 0.489 | Accuracy: 0.774000 | 1.677 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 0.454 | Reg loss: 0.018 | Tree loss: 0.454 | Accuracy: 0.807500 | 1.677 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 0.496 | Reg loss: 0.018 | Tree loss: 0.496 | Accuracy: 0.744027 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 0.544 | Reg loss: 0.018 | Tree loss: 0.544 | Accuracy: 0.733500 | 1.685 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 0.535 | Reg loss: 0.018 | Tree loss: 0.535 | Accuracy: 0.751500 | 1.684 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 0.523 | Reg loss: 0.018 | Tree loss: 0.523 | Accuracy: 0.749500 | 1.684 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 0.496 | Reg loss: 0.018 | Tree loss: 0.496 | Accuracy: 0.787000 | 1.683 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 0.479 | Reg loss: 0.018 | Tree loss: 0.479 | Accuracy: 0.821500 | 1.682 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 0.469 | Reg loss: 0.018 | Tree loss: 0.469 | Accuracy: 0.801500 | 1.682 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 0.481 | Reg loss: 0.018 | Tree loss: 0.481 | Accuracy: 0.794000 | 1.681 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 0.481 | Reg loss: 0.018 | Tree loss: 0.481 | Accuracy: 0.779000 | 1.681 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 0.476 | Reg loss: 0.018 | Tree loss: 0.476 | Accuracy: 0.779000 | 1.68 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 0.462 | Reg loss: 0.018 | Tree loss: 0.462 | Accuracy: 0.799000 | 1.679 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 0.453 | Reg loss: 0.018 | Tree loss: 0.453 | Accuracy: 0.805461 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 0.536 | Reg loss: 0.018 | Tree loss: 0.536 | Accuracy: 0.759500 | 1.687 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 0.535 | Reg loss: 0.018 | Tree loss: 0.535 | Accuracy: 0.741500 | 1.686 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 0.509 | Reg loss: 0.018 | Tree loss: 0.509 | Accuracy: 0.762500 | 1.686 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 0.490 | Reg loss: 0.018 | Tree loss: 0.490 | Accuracy: 0.789000 | 1.686 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 0.473 | Reg loss: 0.018 | Tree loss: 0.473 | Accuracy: 0.810000 | 1.685 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 0.475 | Reg loss: 0.018 | Tree loss: 0.475 | Accuracy: 0.794500 | 1.685 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 0.477 | Reg loss: 0.018 | Tree loss: 0.477 | Accuracy: 0.790500 | 1.685 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 0.473 | Reg loss: 0.018 | Tree loss: 0.473 | Accuracy: 0.784500 | 1.684 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 0.481 | Reg loss: 0.018 | Tree loss: 0.481 | Accuracy: 0.764500 | 1.683 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 0.472 | Reg loss: 0.018 | Tree loss: 0.472 | Accuracy: 0.789500 | 1.683 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.018 | Tree loss: 0.426 | Accuracy: 0.856655 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 0.533 | Reg loss: 0.018 | Tree loss: 0.533 | Accuracy: 0.749500 | 1.686 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 0.517 | Reg loss: 0.018 | Tree loss: 0.517 | Accuracy: 0.764500 | 1.685 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 0.515 | Reg loss: 0.018 | Tree loss: 0.515 | Accuracy: 0.750000 | 1.685 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 0.493 | Reg loss: 0.018 | Tree loss: 0.493 | Accuracy: 0.783500 | 1.684 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 0.476 | Reg loss: 0.018 | Tree loss: 0.476 | Accuracy: 0.804500 | 1.683 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 0.462 | Reg loss: 0.018 | Tree loss: 0.462 | Accuracy: 0.822500 | 1.682 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 0.474 | Reg loss: 0.018 | Tree loss: 0.474 | Accuracy: 0.805500 | 1.681 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 0.471 | Reg loss: 0.018 | Tree loss: 0.471 | Accuracy: 0.794000 | 1.681 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 0.481 | Reg loss: 0.018 | Tree loss: 0.481 | Accuracy: 0.771500 | 1.68 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.785500 | 1.68 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 0.433 | Reg loss: 0.019 | Tree loss: 0.433 | Accuracy: 0.839590 | 1.679 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 0.526 | Reg loss: 0.018 | Tree loss: 0.526 | Accuracy: 0.747000 | 1.681 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 0.514 | Reg loss: 0.018 | Tree loss: 0.514 | Accuracy: 0.758000 | 1.681 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 0.512 | Reg loss: 0.018 | Tree loss: 0.512 | Accuracy: 0.766000 | 1.681 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 0.489 | Reg loss: 0.018 | Tree loss: 0.489 | Accuracy: 0.783000 | 1.68 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 0.472 | Reg loss: 0.018 | Tree loss: 0.472 | Accuracy: 0.809500 | 1.68 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 0.462 | Reg loss: 0.018 | Tree loss: 0.462 | Accuracy: 0.820000 | 1.679 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 0.477 | Reg loss: 0.019 | Tree loss: 0.477 | Accuracy: 0.787500 | 1.679 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.779000 | 1.678 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.784500 | 1.678 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.799000 | 1.678 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.805461 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 0.533 | Reg loss: 0.018 | Tree loss: 0.533 | Accuracy: 0.748000 | 1.684 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 0.534 | Reg loss: 0.018 | Tree loss: 0.534 | Accuracy: 0.740000 | 1.683 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 0.510 | Reg loss: 0.018 | Tree loss: 0.510 | Accuracy: 0.763500 | 1.683 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 0.477 | Reg loss: 0.018 | Tree loss: 0.477 | Accuracy: 0.784500 | 1.682 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.795000 | 1.682 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.796500 | 1.682 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.816500 | 1.681 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.808000 | 1.681 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.773500 | 1.68 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.787500 | 1.679 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.798635 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 0.510 | Reg loss: 0.019 | Tree loss: 0.510 | Accuracy: 0.763000 | 1.683 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 0.532 | Reg loss: 0.019 | Tree loss: 0.532 | Accuracy: 0.741500 | 1.682 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 0.517 | Reg loss: 0.019 | Tree loss: 0.517 | Accuracy: 0.751000 | 1.681 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 0.490 | Reg loss: 0.019 | Tree loss: 0.490 | Accuracy: 0.797500 | 1.681 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.817000 | 1.68 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.812500 | 1.679 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 0.469 | Reg loss: 0.019 | Tree loss: 0.469 | Accuracy: 0.800000 | 1.678 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 0.474 | Reg loss: 0.019 | Tree loss: 0.474 | Accuracy: 0.793500 | 1.677 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.790500 | 1.676 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.784500 | 1.676 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.019 | Tree loss: 0.437 | Accuracy: 0.798635 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 0.520 | Reg loss: 0.019 | Tree loss: 0.520 | Accuracy: 0.758500 | 1.678 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 0.532 | Reg loss: 0.019 | Tree loss: 0.532 | Accuracy: 0.756500 | 1.677 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 0.514 | Reg loss: 0.019 | Tree loss: 0.514 | Accuracy: 0.764500 | 1.677 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.788500 | 1.676 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.807000 | 1.676 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.807000 | 1.675 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.785000 | 1.675 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.789500 | 1.674 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.792500 | 1.673 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.788500 | 1.673 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.784983 | 1.672 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 0.521 | Reg loss: 0.019 | Tree loss: 0.521 | Accuracy: 0.764000 | 1.681 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.755000 | 1.681 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 0.509 | Reg loss: 0.019 | Tree loss: 0.509 | Accuracy: 0.755500 | 1.68 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 0.502 | Reg loss: 0.019 | Tree loss: 0.502 | Accuracy: 0.770000 | 1.68 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.807500 | 1.679 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.825500 | 1.679 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 0.473 | Reg loss: 0.019 | Tree loss: 0.473 | Accuracy: 0.787500 | 1.678 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.792500 | 1.678 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 0.472 | Reg loss: 0.019 | Tree loss: 0.472 | Accuracy: 0.774500 | 1.677 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.786500 | 1.676 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.778157 | 1.675 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 0.510 | Reg loss: 0.019 | Tree loss: 0.510 | Accuracy: 0.770000 | 1.684 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.761500 | 1.684 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 0.497 | Reg loss: 0.019 | Tree loss: 0.497 | Accuracy: 0.757500 | 1.683 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 0.493 | Reg loss: 0.019 | Tree loss: 0.493 | Accuracy: 0.770500 | 1.682 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 0.481 | Reg loss: 0.019 | Tree loss: 0.481 | Accuracy: 0.790000 | 1.681 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 0.467 | Reg loss: 0.019 | Tree loss: 0.467 | Accuracy: 0.797500 | 1.681 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.803500 | 1.68 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 0.481 | Reg loss: 0.019 | Tree loss: 0.481 | Accuracy: 0.769500 | 1.679 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.788000 | 1.678 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.804500 | 1.677 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.815700 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 0.520 | Reg loss: 0.019 | Tree loss: 0.520 | Accuracy: 0.755000 | 1.679 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 0.519 | Reg loss: 0.019 | Tree loss: 0.519 | Accuracy: 0.754500 | 1.678 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 0.506 | Reg loss: 0.019 | Tree loss: 0.506 | Accuracy: 0.778500 | 1.677 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.782500 | 1.677 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.812000 | 1.676 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.796500 | 1.675 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.795500 | 1.674 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.786500 | 1.673 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.791000 | 1.672 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.816500 | 1.672 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.019 | Tree loss: 0.417 | Accuracy: 0.846416 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 0.510 | Reg loss: 0.019 | Tree loss: 0.510 | Accuracy: 0.761000 | 1.679 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.019 | Tree loss: 0.507 | Accuracy: 0.761500 | 1.678 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.775500 | 1.678 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 0.480 | Reg loss: 0.019 | Tree loss: 0.480 | Accuracy: 0.800500 | 1.677 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.815000 | 1.677 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.800500 | 1.677 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.793000 | 1.676 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.793000 | 1.676 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.804000 | 1.675 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.806000 | 1.674 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.795222 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 0.525 | Reg loss: 0.019 | Tree loss: 0.525 | Accuracy: 0.744000 | 1.681 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 0.527 | Reg loss: 0.019 | Tree loss: 0.527 | Accuracy: 0.746500 | 1.681 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.759500 | 1.68 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.791000 | 1.679 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.834000 | 1.678 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.806500 | 1.678 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.805000 | 1.677 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.778000 | 1.676 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.774500 | 1.675 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.795000 | 1.674 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.019 | Tree loss: 0.416 | Accuracy: 0.832765 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 0.523 | Reg loss: 0.019 | Tree loss: 0.523 | Accuracy: 0.751000 | 1.676 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 0.519 | Reg loss: 0.019 | Tree loss: 0.519 | Accuracy: 0.761500 | 1.676 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 0.497 | Reg loss: 0.019 | Tree loss: 0.497 | Accuracy: 0.762000 | 1.675 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.772500 | 1.674 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.812000 | 1.674 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.819000 | 1.673 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.808500 | 1.672 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.784000 | 1.671 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.778000 | 1.671 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.783000 | 1.67 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 0.474 | Reg loss: 0.019 | Tree loss: 0.474 | Accuracy: 0.730375 | 1.67 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 0.518 | Reg loss: 0.019 | Tree loss: 0.518 | Accuracy: 0.740500 | 1.677 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 0.531 | Reg loss: 0.019 | Tree loss: 0.531 | Accuracy: 0.740000 | 1.677 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.741000 | 1.676 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.786000 | 1.676 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.813500 | 1.676 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.822000 | 1.675 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.810500 | 1.674 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 0.472 | Reg loss: 0.019 | Tree loss: 0.472 | Accuracy: 0.775500 | 1.674 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.776000 | 1.673 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.785500 | 1.673 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.839590 | 1.672 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.758000 | 1.679 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 0.502 | Reg loss: 0.019 | Tree loss: 0.502 | Accuracy: 0.763500 | 1.678 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 0.508 | Reg loss: 0.019 | Tree loss: 0.508 | Accuracy: 0.752000 | 1.678 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.779000 | 1.678 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.824000 | 1.677 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.811500 | 1.677 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.798000 | 1.677 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 0.474 | Reg loss: 0.019 | Tree loss: 0.474 | Accuracy: 0.776000 | 1.676 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.765500 | 1.675 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.788000 | 1.675 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.812287 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.750000 | 1.678 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.759000 | 1.677 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 0.509 | Reg loss: 0.019 | Tree loss: 0.509 | Accuracy: 0.757000 | 1.676 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.797500 | 1.676 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.822500 | 1.675 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.817000 | 1.674 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 0.467 | Reg loss: 0.019 | Tree loss: 0.467 | Accuracy: 0.791500 | 1.673 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 0.479 | Reg loss: 0.019 | Tree loss: 0.479 | Accuracy: 0.780500 | 1.673 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.777500 | 1.672 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.802000 | 1.671 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.754266 | 1.67 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 0.519 | Reg loss: 0.019 | Tree loss: 0.519 | Accuracy: 0.750000 | 1.673 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 0.521 | Reg loss: 0.019 | Tree loss: 0.521 | Accuracy: 0.742000 | 1.672 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 0.497 | Reg loss: 0.019 | Tree loss: 0.497 | Accuracy: 0.764000 | 1.672 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.803500 | 1.672 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.823500 | 1.671 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.819500 | 1.671 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.794000 | 1.67 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.794500 | 1.67 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.794000 | 1.669 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.802000 | 1.669 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.788396 | 1.668 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 0.507 | Reg loss: 0.019 | Tree loss: 0.507 | Accuracy: 0.764500 | 1.676 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.733500 | 1.676 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.019 | Tree loss: 0.485 | Accuracy: 0.776000 | 1.675 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 0.472 | Reg loss: 0.019 | Tree loss: 0.472 | Accuracy: 0.792500 | 1.675 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.808500 | 1.675 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.813500 | 1.674 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.802000 | 1.674 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.787500 | 1.673 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.774500 | 1.672 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.788000 | 1.672 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.019 | Tree loss: 0.424 | Accuracy: 0.815700 | 1.671 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 0.505 | Reg loss: 0.019 | Tree loss: 0.505 | Accuracy: 0.769500 | 1.675 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 0.508 | Reg loss: 0.019 | Tree loss: 0.508 | Accuracy: 0.749000 | 1.674 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 0.497 | Reg loss: 0.019 | Tree loss: 0.497 | Accuracy: 0.760000 | 1.674 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 0.479 | Reg loss: 0.019 | Tree loss: 0.479 | Accuracy: 0.784500 | 1.673 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.816500 | 1.673 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 0.441 | Reg loss: 0.019 | Tree loss: 0.441 | Accuracy: 0.812000 | 1.672 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.782500 | 1.671 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.787000 | 1.67 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.789500 | 1.669 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.802000 | 1.669 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 0.434 | Reg loss: 0.019 | Tree loss: 0.434 | Accuracy: 0.836177 | 1.668 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 0.515 | Reg loss: 0.019 | Tree loss: 0.515 | Accuracy: 0.758500 | 1.671 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.740500 | 1.67 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 0.494 | Reg loss: 0.019 | Tree loss: 0.494 | Accuracy: 0.767000 | 1.67 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.810000 | 1.669 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.826500 | 1.669 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.806000 | 1.668 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.810500 | 1.668 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.790000 | 1.668 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.796000 | 1.667 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.792000 | 1.667 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.019 | Tree loss: 0.406 | Accuracy: 0.836177 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 0.516 | Reg loss: 0.019 | Tree loss: 0.516 | Accuracy: 0.749000 | 1.674 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 0.518 | Reg loss: 0.019 | Tree loss: 0.518 | Accuracy: 0.745000 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 0.499 | Reg loss: 0.019 | Tree loss: 0.499 | Accuracy: 0.762000 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.800500 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.824000 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.800500 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.798000 | 1.673 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.785500 | 1.672 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.783000 | 1.672 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.797500 | 1.671 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.019 | Tree loss: 0.424 | Accuracy: 0.856655 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.773000 | 1.676 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.736500 | 1.676 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.759000 | 1.675 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 0.485 | Reg loss: 0.019 | Tree loss: 0.485 | Accuracy: 0.781500 | 1.675 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.830500 | 1.674 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.800500 | 1.673 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.798000 | 1.672 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.794500 | 1.672 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.776000 | 1.671 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.786500 | 1.67 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.019 | Tree loss: 0.422 | Accuracy: 0.870307 | 1.67 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.019 | Tree loss: 0.486 | Accuracy: 0.772500 | 1.672 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 0.514 | Reg loss: 0.019 | Tree loss: 0.514 | Accuracy: 0.757000 | 1.672 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.777500 | 1.671 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 0.479 | Reg loss: 0.019 | Tree loss: 0.479 | Accuracy: 0.782500 | 1.67 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.807500 | 1.67 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.808000 | 1.669 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.811000 | 1.668 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.789500 | 1.667 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 0.454 | Reg loss: 0.019 | Tree loss: 0.454 | Accuracy: 0.785500 | 1.667 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.808000 | 1.667 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 0.413 | Reg loss: 0.019 | Tree loss: 0.413 | Accuracy: 0.843003 | 1.666 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 0.515 | Reg loss: 0.019 | Tree loss: 0.515 | Accuracy: 0.746500 | 1.672 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 0.492 | Reg loss: 0.019 | Tree loss: 0.492 | Accuracy: 0.761500 | 1.672 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 0.479 | Reg loss: 0.019 | Tree loss: 0.479 | Accuracy: 0.790500 | 1.671 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 0.467 | Reg loss: 0.019 | Tree loss: 0.467 | Accuracy: 0.805500 | 1.671 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.824500 | 1.671 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.801000 | 1.67 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.789500 | 1.67 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.775500 | 1.67 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.764000 | 1.669 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.019 | Tree loss: 0.434 | Accuracy: 0.814500 | 1.669 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.019 | Tree loss: 0.437 | Accuracy: 0.856655 | 1.668 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 0.524 | Reg loss: 0.019 | Tree loss: 0.524 | Accuracy: 0.751500 | 1.675 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.019 | Tree loss: 0.494 | Accuracy: 0.759000 | 1.674 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.780500 | 1.674 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.797500 | 1.673 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.812000 | 1.673 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.794500 | 1.672 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.780500 | 1.671 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.804500 | 1.671 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.804000 | 1.67 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.019 | Tree loss: 0.436 | Accuracy: 0.813000 | 1.669 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.019 | Tree loss: 0.426 | Accuracy: 0.836177 | 1.669 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.764000 | 1.671 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.019 | Tree loss: 0.496 | Accuracy: 0.774000 | 1.67 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 0.492 | Reg loss: 0.019 | Tree loss: 0.492 | Accuracy: 0.781000 | 1.67 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 0.483 | Reg loss: 0.019 | Tree loss: 0.483 | Accuracy: 0.784500 | 1.669 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.019 | Tree loss: 0.427 | Accuracy: 0.842000 | 1.669 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.809000 | 1.668 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.804500 | 1.667 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.774000 | 1.666 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.794500 | 1.666 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.800500 | 1.666 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 0.439 | Reg loss: 0.019 | Tree loss: 0.439 | Accuracy: 0.815700 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 0.519 | Reg loss: 0.019 | Tree loss: 0.519 | Accuracy: 0.734500 | 1.671 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.755000 | 1.67 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.019 | Tree loss: 0.472 | Accuracy: 0.790000 | 1.67 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.818000 | 1.67 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.019 | Tree loss: 0.441 | Accuracy: 0.830500 | 1.669 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.789000 | 1.669 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.806000 | 1.669 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.800000 | 1.668 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.796500 | 1.668 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.019 | Tree loss: 0.437 | Accuracy: 0.802000 | 1.667 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.019 | Tree loss: 0.428 | Accuracy: 0.812287 | 1.667 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 0.509 | Reg loss: 0.019 | Tree loss: 0.509 | Accuracy: 0.754000 | 1.673 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 0.509 | Reg loss: 0.019 | Tree loss: 0.509 | Accuracy: 0.749500 | 1.672 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 0.496 | Reg loss: 0.019 | Tree loss: 0.496 | Accuracy: 0.759500 | 1.672 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.790000 | 1.672 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.826000 | 1.671 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.809000 | 1.671 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.791500 | 1.671 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.797000 | 1.67 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.782000 | 1.67 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.800500 | 1.669 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 0.430 | Reg loss: 0.019 | Tree loss: 0.430 | Accuracy: 0.808874 | 1.668 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 0.500 | Reg loss: 0.019 | Tree loss: 0.500 | Accuracy: 0.748000 | 1.672 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 0.502 | Reg loss: 0.019 | Tree loss: 0.502 | Accuracy: 0.763500 | 1.671 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.019 | Tree loss: 0.469 | Accuracy: 0.784500 | 1.671 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 0.472 | Reg loss: 0.019 | Tree loss: 0.472 | Accuracy: 0.794000 | 1.67 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.831500 | 1.669 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.790500 | 1.669 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 0.468 | Reg loss: 0.019 | Tree loss: 0.468 | Accuracy: 0.789500 | 1.668 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.783000 | 1.667 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.796000 | 1.667 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.808500 | 1.666 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.019 | Tree loss: 0.432 | Accuracy: 0.791809 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.763000 | 1.668 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 0.508 | Reg loss: 0.019 | Tree loss: 0.508 | Accuracy: 0.748000 | 1.667 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.019 | Tree loss: 0.485 | Accuracy: 0.773000 | 1.667 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.790000 | 1.667 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 0.438 | Reg loss: 0.019 | Tree loss: 0.438 | Accuracy: 0.833500 | 1.666 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.821000 | 1.666 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.805500 | 1.666 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.771500 | 1.665 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.790500 | 1.665 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 0.441 | Reg loss: 0.019 | Tree loss: 0.441 | Accuracy: 0.801500 | 1.664 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.019 | Tree loss: 0.426 | Accuracy: 0.853242 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 0.504 | Reg loss: 0.019 | Tree loss: 0.504 | Accuracy: 0.757500 | 1.671 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.769000 | 1.671 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.778000 | 1.67 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 0.467 | Reg loss: 0.019 | Tree loss: 0.467 | Accuracy: 0.788500 | 1.67 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.827000 | 1.67 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.797000 | 1.669 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.792000 | 1.669 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 0.454 | Reg loss: 0.019 | Tree loss: 0.454 | Accuracy: 0.786500 | 1.669 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.802000 | 1.668 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.798000 | 1.667 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.019 | Tree loss: 0.412 | Accuracy: 0.863481 | 1.667 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.761500 | 1.671 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 0.512 | Reg loss: 0.019 | Tree loss: 0.512 | Accuracy: 0.744500 | 1.67 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 0.493 | Reg loss: 0.019 | Tree loss: 0.493 | Accuracy: 0.765500 | 1.669 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.804000 | 1.669 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.824000 | 1.668 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.813000 | 1.668 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.795500 | 1.667 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.781500 | 1.666 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.796000 | 1.666 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.019 | Tree loss: 0.431 | Accuracy: 0.814500 | 1.665 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.019 | Tree loss: 0.436 | Accuracy: 0.832765 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 0.508 | Reg loss: 0.019 | Tree loss: 0.508 | Accuracy: 0.753000 | 1.667 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.019 | Tree loss: 0.488 | Accuracy: 0.772500 | 1.666 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 0.490 | Reg loss: 0.019 | Tree loss: 0.490 | Accuracy: 0.779500 | 1.666 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.808500 | 1.666 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.820000 | 1.665 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 0.434 | Reg loss: 0.019 | Tree loss: 0.434 | Accuracy: 0.807500 | 1.665 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.019 | Tree loss: 0.433 | Accuracy: 0.812000 | 1.665 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.784500 | 1.664 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.796000 | 1.664 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.019 | Tree loss: 0.433 | Accuracy: 0.810500 | 1.663 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.791809 | 1.663 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.758000 | 1.669 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.019 | Tree loss: 0.489 | Accuracy: 0.767000 | 1.669 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.767000 | 1.669 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.801000 | 1.669 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.814500 | 1.668 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.797500 | 1.668 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 0.454 | Reg loss: 0.019 | Tree loss: 0.454 | Accuracy: 0.793000 | 1.668 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.790500 | 1.667 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.797000 | 1.667 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.804500 | 1.666 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 0.433 | Reg loss: 0.019 | Tree loss: 0.433 | Accuracy: 0.839590 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 0.505 | Reg loss: 0.019 | Tree loss: 0.505 | Accuracy: 0.752500 | 1.672 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.019 | Tree loss: 0.493 | Accuracy: 0.761500 | 1.672 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.780500 | 1.671 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.821500 | 1.67 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.827000 | 1.67 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.804000 | 1.669 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.805000 | 1.669 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.779500 | 1.668 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.798000 | 1.667 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.019 | Tree loss: 0.437 | Accuracy: 0.807500 | 1.667 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 0.376 | Reg loss: 0.019 | Tree loss: 0.376 | Accuracy: 0.883959 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 0.495 | Reg loss: 0.019 | Tree loss: 0.495 | Accuracy: 0.765000 | 1.668 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.019 | Tree loss: 0.507 | Accuracy: 0.760000 | 1.668 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.785500 | 1.667 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.818500 | 1.667 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 0.444 | Reg loss: 0.019 | Tree loss: 0.444 | Accuracy: 0.831000 | 1.666 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.807500 | 1.666 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.801500 | 1.665 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.794500 | 1.664 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.809500 | 1.664 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.019 | Tree loss: 0.422 | Accuracy: 0.815500 | 1.663 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.019 | Tree loss: 0.405 | Accuracy: 0.866894 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 0.507 | Reg loss: 0.019 | Tree loss: 0.507 | Accuracy: 0.752000 | 1.668 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.019 | Tree loss: 0.485 | Accuracy: 0.781500 | 1.668 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 0.504 | Reg loss: 0.019 | Tree loss: 0.504 | Accuracy: 0.760500 | 1.668 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.802000 | 1.667 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.833500 | 1.667 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.798500 | 1.667 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.800500 | 1.666 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.785000 | 1.666 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.794000 | 1.666 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.019 | Tree loss: 0.419 | Accuracy: 0.827000 | 1.665 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.019 | Tree loss: 0.422 | Accuracy: 0.849829 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 0.506 | Reg loss: 0.019 | Tree loss: 0.506 | Accuracy: 0.766000 | 1.671 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.019 | Tree loss: 0.499 | Accuracy: 0.772000 | 1.67 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.019 | Tree loss: 0.475 | Accuracy: 0.785500 | 1.67 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.808000 | 1.669 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.019 | Tree loss: 0.427 | Accuracy: 0.836500 | 1.669 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.796500 | 1.668 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 0.464 | Reg loss: 0.019 | Tree loss: 0.464 | Accuracy: 0.789500 | 1.668 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 0.459 | Reg loss: 0.019 | Tree loss: 0.459 | Accuracy: 0.787500 | 1.668 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 0.455 | Reg loss: 0.019 | Tree loss: 0.455 | Accuracy: 0.791000 | 1.667 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.019 | Tree loss: 0.429 | Accuracy: 0.803500 | 1.667 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.019 | Tree loss: 0.405 | Accuracy: 0.839590 | 1.666 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 0.513 | Reg loss: 0.019 | Tree loss: 0.513 | Accuracy: 0.754500 | 1.667 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 0.505 | Reg loss: 0.019 | Tree loss: 0.505 | Accuracy: 0.761000 | 1.667 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 0.483 | Reg loss: 0.019 | Tree loss: 0.483 | Accuracy: 0.782000 | 1.667 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 0.457 | Reg loss: 0.019 | Tree loss: 0.457 | Accuracy: 0.803000 | 1.666 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 0.434 | Reg loss: 0.019 | Tree loss: 0.434 | Accuracy: 0.839500 | 1.666 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.811000 | 1.665 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 0.465 | Reg loss: 0.019 | Tree loss: 0.465 | Accuracy: 0.789500 | 1.665 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 0.461 | Reg loss: 0.019 | Tree loss: 0.461 | Accuracy: 0.785500 | 1.664 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.790000 | 1.664 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.019 | Tree loss: 0.423 | Accuracy: 0.815500 | 1.664 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.019 | Tree loss: 0.424 | Accuracy: 0.843003 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 0.510 | Reg loss: 0.019 | Tree loss: 0.510 | Accuracy: 0.759000 | 1.669 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 0.521 | Reg loss: 0.019 | Tree loss: 0.521 | Accuracy: 0.753000 | 1.668 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.019 | Tree loss: 0.460 | Accuracy: 0.787000 | 1.668 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.019 | Tree loss: 0.446 | Accuracy: 0.818500 | 1.668 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.825000 | 1.668 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.803000 | 1.667 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.808000 | 1.667 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 0.467 | Reg loss: 0.019 | Tree loss: 0.467 | Accuracy: 0.769000 | 1.666 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.797000 | 1.666 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.019 | Tree loss: 0.425 | Accuracy: 0.806500 | 1.666 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.019 | Tree loss: 0.399 | Accuracy: 0.860068 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.019 | Tree loss: 0.493 | Accuracy: 0.766500 | 1.671 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.782000 | 1.671 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.019 | Tree loss: 0.471 | Accuracy: 0.777000 | 1.67 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.820000 | 1.67 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.823500 | 1.67 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 0.428 | Reg loss: 0.019 | Tree loss: 0.428 | Accuracy: 0.824000 | 1.669 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.798500 | 1.669 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 0.462 | Reg loss: 0.019 | Tree loss: 0.462 | Accuracy: 0.783000 | 1.668 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.781000 | 1.668 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 0.444 | Reg loss: 0.019 | Tree loss: 0.444 | Accuracy: 0.797500 | 1.667 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.808874 | 1.667 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.019 | Tree loss: 0.492 | Accuracy: 0.767000 | 1.668 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 0.505 | Reg loss: 0.019 | Tree loss: 0.505 | Accuracy: 0.757000 | 1.668 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 0.484 | Reg loss: 0.019 | Tree loss: 0.484 | Accuracy: 0.774500 | 1.667 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 0.466 | Reg loss: 0.019 | Tree loss: 0.466 | Accuracy: 0.788000 | 1.667 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.811000 | 1.666 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 0.444 | Reg loss: 0.019 | Tree loss: 0.444 | Accuracy: 0.813000 | 1.666 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 0.443 | Reg loss: 0.019 | Tree loss: 0.443 | Accuracy: 0.800500 | 1.665 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.788000 | 1.665 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.792500 | 1.664 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.019 | Tree loss: 0.428 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.019 | Tree loss: 0.414 | Accuracy: 0.815700 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.772000 | 1.665 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 0.512 | Reg loss: 0.019 | Tree loss: 0.512 | Accuracy: 0.747000 | 1.665 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 0.506 | Reg loss: 0.019 | Tree loss: 0.506 | Accuracy: 0.755000 | 1.665 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 0.473 | Reg loss: 0.019 | Tree loss: 0.473 | Accuracy: 0.768000 | 1.664 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 0.435 | Reg loss: 0.019 | Tree loss: 0.435 | Accuracy: 0.836000 | 1.664 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 0.444 | Reg loss: 0.019 | Tree loss: 0.444 | Accuracy: 0.797000 | 1.664 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 0.445 | Reg loss: 0.019 | Tree loss: 0.445 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 0.463 | Reg loss: 0.019 | Tree loss: 0.463 | Accuracy: 0.781000 | 1.663 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.019 | Tree loss: 0.427 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 0.448 | Reg loss: 0.019 | Tree loss: 0.448 | Accuracy: 0.788500 | 1.662 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.019 | Tree loss: 0.418 | Accuracy: 0.846416 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 0.504 | Reg loss: 0.019 | Tree loss: 0.504 | Accuracy: 0.765000 | 1.667 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 0.498 | Reg loss: 0.019 | Tree loss: 0.498 | Accuracy: 0.758500 | 1.667 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 0.493 | Reg loss: 0.019 | Tree loss: 0.493 | Accuracy: 0.763500 | 1.666 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 0.458 | Reg loss: 0.019 | Tree loss: 0.458 | Accuracy: 0.788500 | 1.666 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 0.439 | Reg loss: 0.019 | Tree loss: 0.439 | Accuracy: 0.822500 | 1.666 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 0.442 | Reg loss: 0.019 | Tree loss: 0.442 | Accuracy: 0.803500 | 1.665 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 0.451 | Reg loss: 0.019 | Tree loss: 0.451 | Accuracy: 0.795000 | 1.665 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.788500 | 1.665 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.019 | Tree loss: 0.437 | Accuracy: 0.786500 | 1.664 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.019 | Tree loss: 0.430 | Accuracy: 0.807500 | 1.664 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.019 | Tree loss: 0.420 | Accuracy: 0.849829 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 0.499 | Reg loss: 0.019 | Tree loss: 0.499 | Accuracy: 0.769500 | 1.666 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.019 | Tree loss: 0.490 | Accuracy: 0.773500 | 1.666 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 0.482 | Reg loss: 0.019 | Tree loss: 0.482 | Accuracy: 0.767000 | 1.665 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.019 | Tree loss: 0.453 | Accuracy: 0.806500 | 1.665 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.019 | Tree loss: 0.441 | Accuracy: 0.827000 | 1.664 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 0.450 | Reg loss: 0.019 | Tree loss: 0.450 | Accuracy: 0.800000 | 1.664 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 0.452 | Reg loss: 0.019 | Tree loss: 0.452 | Accuracy: 0.791000 | 1.663 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.019 | Tree loss: 0.447 | Accuracy: 0.798000 | 1.662 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.801000 | 1.662 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.812000 | 1.661 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.020 | Tree loss: 0.392 | Accuracy: 0.877133 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 0.499 | Reg loss: 0.019 | Tree loss: 0.499 | Accuracy: 0.770500 | 1.663 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.019 | Tree loss: 0.496 | Accuracy: 0.772000 | 1.662 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.019 | Tree loss: 0.470 | Accuracy: 0.783000 | 1.662 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 0.449 | Reg loss: 0.019 | Tree loss: 0.449 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 0.456 | Reg loss: 0.019 | Tree loss: 0.456 | Accuracy: 0.810500 | 1.661 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 0.432 | Reg loss: 0.019 | Tree loss: 0.432 | Accuracy: 0.810500 | 1.661 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.799000 | 1.661 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 0.455 | Reg loss: 0.020 | Tree loss: 0.455 | Accuracy: 0.783500 | 1.661 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.785500 | 1.66 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.806500 | 1.66 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.805461 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 0.507 | Reg loss: 0.019 | Tree loss: 0.507 | Accuracy: 0.764000 | 1.665 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.019 | Tree loss: 0.501 | Accuracy: 0.753500 | 1.665 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.019 | Tree loss: 0.485 | Accuracy: 0.777000 | 1.665 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.019 | Tree loss: 0.440 | Accuracy: 0.819000 | 1.664 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.815500 | 1.664 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.830500 | 1.664 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 0.470 | Reg loss: 0.020 | Tree loss: 0.470 | Accuracy: 0.785000 | 1.664 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.803000 | 1.663 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.815000 | 1.662 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.020 | Tree loss: 0.410 | Accuracy: 0.832765 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.019 | Tree loss: 0.488 | Accuracy: 0.772500 | 1.667 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 0.505 | Reg loss: 0.020 | Tree loss: 0.505 | Accuracy: 0.761500 | 1.667 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.784500 | 1.666 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.813000 | 1.666 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.828000 | 1.665 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.802500 | 1.665 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.806000 | 1.664 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.783500 | 1.664 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.804000 | 1.663 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.803500 | 1.663 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.832765 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.783000 | 1.664 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 0.495 | Reg loss: 0.020 | Tree loss: 0.495 | Accuracy: 0.773000 | 1.664 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.769500 | 1.663 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.792000 | 1.663 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.828500 | 1.662 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.802500 | 1.662 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.787000 | 1.661 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.791500 | 1.66 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.020 | Tree loss: 0.408 | Accuracy: 0.836177 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.771500 | 1.664 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 0.510 | Reg loss: 0.020 | Tree loss: 0.510 | Accuracy: 0.772500 | 1.664 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 0.503 | Reg loss: 0.020 | Tree loss: 0.503 | Accuracy: 0.773500 | 1.663 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 0.465 | Reg loss: 0.020 | Tree loss: 0.465 | Accuracy: 0.801500 | 1.663 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.835000 | 1.663 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.815500 | 1.662 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.796000 | 1.662 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.784500 | 1.662 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.786500 | 1.662 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.825500 | 1.661 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.812287 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.762500 | 1.666 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.020 | Tree loss: 0.501 | Accuracy: 0.761000 | 1.666 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.771000 | 1.666 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.812500 | 1.665 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.821000 | 1.665 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.808000 | 1.665 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.798000 | 1.664 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 0.465 | Reg loss: 0.020 | Tree loss: 0.465 | Accuracy: 0.778500 | 1.664 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.801500 | 1.663 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.816000 | 1.663 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.853242 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.770000 | 1.663 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.769500 | 1.663 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.769500 | 1.663 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.806500 | 1.662 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.832500 | 1.662 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.786000 | 1.661 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.801500 | 1.66 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.792000 | 1.66 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.808500 | 1.659 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 0.421 | Reg loss: 0.020 | Tree loss: 0.421 | Accuracy: 0.798635 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 0.482 | Reg loss: 0.020 | Tree loss: 0.482 | Accuracy: 0.786500 | 1.663 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 0.504 | Reg loss: 0.020 | Tree loss: 0.504 | Accuracy: 0.766000 | 1.663 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 0.501 | Reg loss: 0.020 | Tree loss: 0.501 | Accuracy: 0.774500 | 1.663 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.803000 | 1.662 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.811500 | 1.662 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.810000 | 1.662 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.789500 | 1.662 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.804500 | 1.661 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.783500 | 1.661 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.775500 | 1.66 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.020 | Tree loss: 0.412 | Accuracy: 0.832765 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.764500 | 1.665 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 0.514 | Reg loss: 0.020 | Tree loss: 0.514 | Accuracy: 0.747500 | 1.665 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 0.495 | Reg loss: 0.020 | Tree loss: 0.495 | Accuracy: 0.765500 | 1.665 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.799500 | 1.664 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.820000 | 1.664 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.808000 | 1.664 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.786000 | 1.664 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.810500 | 1.663 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.797000 | 1.663 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.808500 | 1.662 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.020 | Tree loss: 0.399 | Accuracy: 0.849829 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 0.506 | Reg loss: 0.020 | Tree loss: 0.506 | Accuracy: 0.770500 | 1.664 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.777500 | 1.664 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.786500 | 1.664 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.811500 | 1.663 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.845500 | 1.663 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.792000 | 1.662 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.787500 | 1.662 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.784500 | 1.661 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.813500 | 1.66 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.791809 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 0.498 | Reg loss: 0.020 | Tree loss: 0.498 | Accuracy: 0.776000 | 1.662 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.770000 | 1.661 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 0.500 | Reg loss: 0.020 | Tree loss: 0.500 | Accuracy: 0.770500 | 1.661 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.805500 | 1.661 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.819500 | 1.66 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.808000 | 1.66 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.784500 | 1.66 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.776000 | 1.659 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.785500 | 1.659 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.808874 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.776000 | 1.664 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 0.517 | Reg loss: 0.020 | Tree loss: 0.517 | Accuracy: 0.746000 | 1.663 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.020 | Tree loss: 0.468 | Accuracy: 0.787500 | 1.663 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.799000 | 1.663 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.811500 | 1.663 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.819500 | 1.662 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 0.460 | Reg loss: 0.020 | Tree loss: 0.460 | Accuracy: 0.789500 | 1.662 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.777500 | 1.662 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.797000 | 1.661 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.020 | Tree loss: 0.419 | Accuracy: 0.839590 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.774000 | 1.663 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 0.517 | Reg loss: 0.020 | Tree loss: 0.517 | Accuracy: 0.756000 | 1.663 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.020 | Tree loss: 0.470 | Accuracy: 0.794000 | 1.663 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 0.455 | Reg loss: 0.020 | Tree loss: 0.455 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.830500 | 1.662 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.807000 | 1.661 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.808000 | 1.661 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.797000 | 1.66 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.791500 | 1.659 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.784983 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.766500 | 1.661 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.754500 | 1.66 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 0.492 | Reg loss: 0.020 | Tree loss: 0.492 | Accuracy: 0.769000 | 1.66 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.838500 | 1.659 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.817000 | 1.659 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.812500 | 1.659 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.783500 | 1.659 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.771500 | 1.658 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.807500 | 1.658 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.020 | Tree loss: 0.409 | Accuracy: 0.795222 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.020 | Tree loss: 0.492 | Accuracy: 0.770500 | 1.663 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 0.504 | Reg loss: 0.020 | Tree loss: 0.504 | Accuracy: 0.755500 | 1.663 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.785000 | 1.662 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 0.460 | Reg loss: 0.020 | Tree loss: 0.460 | Accuracy: 0.797000 | 1.662 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.834500 | 1.662 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.813500 | 1.662 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.795000 | 1.661 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.763000 | 1.661 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.800000 | 1.66 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.781570 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 0.509 | Reg loss: 0.020 | Tree loss: 0.509 | Accuracy: 0.759500 | 1.665 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.768000 | 1.665 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.785000 | 1.664 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.805500 | 1.664 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.830500 | 1.663 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.821500 | 1.663 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.787500 | 1.662 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.786000 | 1.661 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.803500 | 1.661 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.020 | Tree loss: 0.425 | Accuracy: 0.819113 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 0.498 | Reg loss: 0.020 | Tree loss: 0.498 | Accuracy: 0.776500 | 1.662 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.774500 | 1.662 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.020 | Tree loss: 0.475 | Accuracy: 0.778000 | 1.661 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.820500 | 1.66 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.804500 | 1.66 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.801500 | 1.659 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.787000 | 1.659 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.808500 | 1.658 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.803000 | 1.658 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.020 | Tree loss: 0.399 | Accuracy: 0.870307 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 0.503 | Reg loss: 0.020 | Tree loss: 0.503 | Accuracy: 0.767000 | 1.662 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.767500 | 1.662 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.798500 | 1.662 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.815500 | 1.661 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.828500 | 1.661 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.792500 | 1.661 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.783000 | 1.661 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.806000 | 1.66 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.823500 | 1.659 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.860068 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 0.502 | Reg loss: 0.020 | Tree loss: 0.502 | Accuracy: 0.765000 | 1.664 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.771000 | 1.664 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.784500 | 1.663 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.822500 | 1.663 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.851000 | 1.662 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.817000 | 1.662 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 0.461 | Reg loss: 0.020 | Tree loss: 0.461 | Accuracy: 0.780000 | 1.661 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 0.460 | Reg loss: 0.020 | Tree loss: 0.460 | Accuracy: 0.778500 | 1.661 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.805000 | 1.66 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.020 | Tree loss: 0.409 | Accuracy: 0.836177 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.781500 | 1.661 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 0.520 | Reg loss: 0.020 | Tree loss: 0.520 | Accuracy: 0.751500 | 1.661 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.020 | Tree loss: 0.473 | Accuracy: 0.792000 | 1.66 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.826500 | 1.66 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.793500 | 1.659 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.759000 | 1.658 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.807500 | 1.658 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.797000 | 1.657 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 0.401 | Reg loss: 0.020 | Tree loss: 0.401 | Accuracy: 0.843003 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.779500 | 1.667 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 0.506 | Reg loss: 0.020 | Tree loss: 0.506 | Accuracy: 0.756000 | 1.666 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.770500 | 1.666 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.773000 | 1.666 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.829000 | 1.665 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.807500 | 1.665 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.788000 | 1.665 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.784000 | 1.664 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.786500 | 1.664 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.787500 | 1.664 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.795222 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.770000 | 1.668 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 0.512 | Reg loss: 0.020 | Tree loss: 0.512 | Accuracy: 0.751500 | 1.668 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.769500 | 1.667 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.791500 | 1.667 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.815000 | 1.666 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.838500 | 1.666 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.803500 | 1.665 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.798000 | 1.665 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.792500 | 1.664 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.789000 | 1.664 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.020 | Tree loss: 0.410 | Accuracy: 0.829352 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.770000 | 1.665 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 0.505 | Reg loss: 0.020 | Tree loss: 0.505 | Accuracy: 0.761500 | 1.665 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 0.476 | Reg loss: 0.020 | Tree loss: 0.476 | Accuracy: 0.779500 | 1.664 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.806000 | 1.664 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.826000 | 1.664 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.806500 | 1.663 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.804500 | 1.663 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.809500 | 1.662 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.791500 | 1.662 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.785000 | 1.662 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 0.398 | Reg loss: 0.020 | Tree loss: 0.398 | Accuracy: 0.836177 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.773500 | 1.665 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 0.502 | Reg loss: 0.020 | Tree loss: 0.502 | Accuracy: 0.752500 | 1.665 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.796500 | 1.665 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.811000 | 1.664 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.825000 | 1.664 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.817500 | 1.664 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 0.455 | Reg loss: 0.020 | Tree loss: 0.455 | Accuracy: 0.789000 | 1.663 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.785000 | 1.663 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.020 | Tree loss: 0.406 | Accuracy: 0.808874 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 0.501 | Reg loss: 0.020 | Tree loss: 0.501 | Accuracy: 0.770000 | 1.667 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.020 | Tree loss: 0.501 | Accuracy: 0.769000 | 1.667 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 0.492 | Reg loss: 0.020 | Tree loss: 0.492 | Accuracy: 0.774500 | 1.666 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.799000 | 1.666 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.819000 | 1.665 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.020 | Tree loss: 0.419 | Accuracy: 0.821500 | 1.665 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.792500 | 1.665 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.801000 | 1.664 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 0.461 | Reg loss: 0.020 | Tree loss: 0.461 | Accuracy: 0.770500 | 1.664 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.797500 | 1.663 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.802048 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.780500 | 1.664 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.780000 | 1.664 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 0.482 | Reg loss: 0.020 | Tree loss: 0.482 | Accuracy: 0.787000 | 1.663 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 0.463 | Reg loss: 0.020 | Tree loss: 0.463 | Accuracy: 0.798000 | 1.663 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.838000 | 1.662 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.840500 | 1.662 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.788000 | 1.662 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.789500 | 1.661 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.793500 | 1.661 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.784500 | 1.66 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 0.459 | Reg loss: 0.020 | Tree loss: 0.459 | Accuracy: 0.815700 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.780500 | 1.669 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 0.502 | Reg loss: 0.020 | Tree loss: 0.502 | Accuracy: 0.762000 | 1.669 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.794000 | 1.669 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.814500 | 1.668 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.816500 | 1.668 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.815500 | 1.668 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.812000 | 1.668 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.797500 | 1.667 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 0.455 | Reg loss: 0.020 | Tree loss: 0.455 | Accuracy: 0.787000 | 1.667 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.793500 | 1.667 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.822526 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.768500 | 1.671 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.784000 | 1.67 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.020 | Tree loss: 0.474 | Accuracy: 0.790000 | 1.67 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.807000 | 1.67 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.828500 | 1.669 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.822500 | 1.669 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.790000 | 1.668 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.777000 | 1.668 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.783000 | 1.667 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.020 | Tree loss: 0.419 | Accuracy: 0.811500 | 1.667 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 0.459 | Reg loss: 0.020 | Tree loss: 0.459 | Accuracy: 0.802048 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.775000 | 1.668 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.762000 | 1.668 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 0.487 | Reg loss: 0.020 | Tree loss: 0.487 | Accuracy: 0.796000 | 1.667 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.795000 | 1.667 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.830500 | 1.667 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.822500 | 1.666 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.802500 | 1.666 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.776000 | 1.666 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.777000 | 1.666 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.789000 | 1.665 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.819113 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 0.506 | Reg loss: 0.020 | Tree loss: 0.506 | Accuracy: 0.763500 | 1.668 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.778500 | 1.668 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.790500 | 1.668 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.798000 | 1.667 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.829000 | 1.667 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.822500 | 1.667 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.808500 | 1.667 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.788000 | 1.667 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.798500 | 1.666 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.788000 | 1.666 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.819113 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.781500 | 1.67 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.767000 | 1.669 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.020 | Tree loss: 0.461 | Accuracy: 0.799500 | 1.669 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.799500 | 1.669 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.818500 | 1.668 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.821500 | 1.668 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.802000 | 1.668 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.796500 | 1.667 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.787000 | 1.667 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.789500 | 1.667 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 0.382 | Reg loss: 0.020 | Tree loss: 0.382 | Accuracy: 0.856655 | 1.666 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.783000 | 1.667 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.762000 | 1.667 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.020 | Tree loss: 0.470 | Accuracy: 0.801500 | 1.666 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.798000 | 1.666 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.830500 | 1.666 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.020 | Tree loss: 0.421 | Accuracy: 0.819000 | 1.665 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.794500 | 1.665 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.788500 | 1.664 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.796500 | 1.664 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.801000 | 1.664 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 0.459 | Reg loss: 0.020 | Tree loss: 0.459 | Accuracy: 0.767918 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 0.500 | Reg loss: 0.020 | Tree loss: 0.500 | Accuracy: 0.770000 | 1.667 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.020 | Tree loss: 0.489 | Accuracy: 0.775500 | 1.667 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.788000 | 1.667 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 0.459 | Reg loss: 0.020 | Tree loss: 0.459 | Accuracy: 0.802000 | 1.666 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.827500 | 1.666 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.852000 | 1.666 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.812500 | 1.666 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.787500 | 1.665 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.802000 | 1.665 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.785000 | 1.664 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.020 | Tree loss: 0.412 | Accuracy: 0.805461 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.776500 | 1.668 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.769000 | 1.668 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.773500 | 1.668 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 0.455 | Reg loss: 0.020 | Tree loss: 0.455 | Accuracy: 0.792500 | 1.667 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.820500 | 1.667 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.814500 | 1.667 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.793500 | 1.667 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.804000 | 1.667 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.784000 | 1.666 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.795500 | 1.666 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.020 | Tree loss: 0.399 | Accuracy: 0.812287 | 1.665 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.020 | Tree loss: 0.468 | Accuracy: 0.793000 | 1.668 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.751500 | 1.667 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.780000 | 1.667 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.803000 | 1.666 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.841500 | 1.666 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.819000 | 1.666 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.789000 | 1.665 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.771000 | 1.665 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.787000 | 1.664 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.805500 | 1.664 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.020 | Tree loss: 0.408 | Accuracy: 0.829352 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.020 | Tree loss: 0.488 | Accuracy: 0.778000 | 1.665 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.770500 | 1.665 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.794000 | 1.664 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.020 | Tree loss: 0.417 | Accuracy: 0.835000 | 1.664 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.812500 | 1.664 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.811000 | 1.663 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.790500 | 1.663 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.781500 | 1.663 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.795000 | 1.662 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.771331 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 0.504 | Reg loss: 0.020 | Tree loss: 0.504 | Accuracy: 0.762000 | 1.667 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.781500 | 1.667 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.785000 | 1.666 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.805500 | 1.666 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.837000 | 1.666 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.818500 | 1.666 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.810500 | 1.665 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.816000 | 1.665 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.793000 | 1.665 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.808874 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.779500 | 1.666 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.775500 | 1.666 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.778000 | 1.665 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.802500 | 1.665 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.837000 | 1.665 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.020 | Tree loss: 0.415 | Accuracy: 0.850000 | 1.664 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.821500 | 1.664 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.796000 | 1.663 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 0.456 | Reg loss: 0.020 | Tree loss: 0.456 | Accuracy: 0.774500 | 1.663 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 0.394 | Reg loss: 0.020 | Tree loss: 0.394 | Accuracy: 0.846416 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.772000 | 1.664 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.786000 | 1.663 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.798500 | 1.663 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.812500 | 1.663 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.825500 | 1.663 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.817000 | 1.662 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.790000 | 1.662 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.776500 | 1.662 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.780000 | 1.662 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.020 | Tree loss: 0.413 | Accuracy: 0.822500 | 1.661 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.819113 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.780000 | 1.665 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.772000 | 1.665 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.781000 | 1.665 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.825000 | 1.665 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.837500 | 1.664 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.797000 | 1.664 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.796500 | 1.664 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.807500 | 1.663 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.796000 | 1.663 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.788396 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 0.501 | Reg loss: 0.020 | Tree loss: 0.501 | Accuracy: 0.785500 | 1.667 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.781000 | 1.667 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.778500 | 1.666 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.801000 | 1.666 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.812000 | 1.665 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.816000 | 1.665 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.797500 | 1.665 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.786500 | 1.664 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.801000 | 1.664 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.020 | Tree loss: 0.418 | Accuracy: 0.815500 | 1.663 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 0.370 | Reg loss: 0.020 | Tree loss: 0.370 | Accuracy: 0.863481 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.771000 | 1.665 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.774500 | 1.664 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 0.476 | Reg loss: 0.020 | Tree loss: 0.476 | Accuracy: 0.786500 | 1.664 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.827000 | 1.663 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.825500 | 1.663 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.808500 | 1.662 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.800000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.800500 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 0.374 | Reg loss: 0.020 | Tree loss: 0.374 | Accuracy: 0.843003 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.020 | Tree loss: 0.474 | Accuracy: 0.793000 | 1.665 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.020 | Tree loss: 0.489 | Accuracy: 0.780500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.771500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 0.461 | Reg loss: 0.020 | Tree loss: 0.461 | Accuracy: 0.810500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.818000 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.821000 | 1.663 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.807500 | 1.663 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.794000 | 1.663 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.797500 | 1.663 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.809500 | 1.662 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 0.377 | Reg loss: 0.020 | Tree loss: 0.377 | Accuracy: 0.843003 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.777000 | 1.666 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 0.507 | Reg loss: 0.020 | Tree loss: 0.507 | Accuracy: 0.765500 | 1.666 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.020 | Tree loss: 0.474 | Accuracy: 0.801000 | 1.666 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.790500 | 1.665 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.828000 | 1.665 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.836500 | 1.664 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.796500 | 1.664 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.802000 | 1.664 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.803000 | 1.663 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.803000 | 1.663 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.805461 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.778500 | 1.664 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.781500 | 1.663 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.775500 | 1.663 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.798000 | 1.663 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.838500 | 1.662 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.825000 | 1.662 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.791500 | 1.662 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 0.462 | Reg loss: 0.020 | Tree loss: 0.462 | Accuracy: 0.777000 | 1.661 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.788000 | 1.661 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.020 | Tree loss: 0.405 | Accuracy: 0.825939 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.020 | Tree loss: 0.489 | Accuracy: 0.776500 | 1.664 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.770000 | 1.664 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.020 | Tree loss: 0.467 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.805500 | 1.664 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.020 | Tree loss: 0.417 | Accuracy: 0.838500 | 1.663 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.020 | Tree loss: 0.421 | Accuracy: 0.851000 | 1.663 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.806000 | 1.663 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.795000 | 1.662 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.783500 | 1.662 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.803000 | 1.662 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.795222 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.020 | Tree loss: 0.478 | Accuracy: 0.787000 | 1.665 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.780000 | 1.665 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.831000 | 1.664 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.832500 | 1.664 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.020 | Tree loss: 0.425 | Accuracy: 0.809000 | 1.663 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 0.458 | Reg loss: 0.020 | Tree loss: 0.458 | Accuracy: 0.783000 | 1.663 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.771331 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.782000 | 1.664 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.778000 | 1.664 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.781000 | 1.664 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 0.469 | Reg loss: 0.020 | Tree loss: 0.469 | Accuracy: 0.790500 | 1.663 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.830500 | 1.663 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.838500 | 1.663 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.803500 | 1.662 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.792000 | 1.661 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.803500 | 1.661 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.020 | Tree loss: 0.404 | Accuracy: 0.825939 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 0.503 | Reg loss: 0.020 | Tree loss: 0.503 | Accuracy: 0.761000 | 1.662 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.020 | Tree loss: 0.471 | Accuracy: 0.782000 | 1.662 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.790000 | 1.662 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.818500 | 1.661 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 0.463 | Reg loss: 0.020 | Tree loss: 0.463 | Accuracy: 0.772000 | 1.661 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.804000 | 1.661 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.816000 | 1.66 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.829352 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.020 | Tree loss: 0.461 | Accuracy: 0.795000 | 1.664 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.020 | Tree loss: 0.486 | Accuracy: 0.779000 | 1.664 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.020 | Tree loss: 0.466 | Accuracy: 0.794500 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.817000 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.812000 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.820500 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.795000 | 1.663 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.800000 | 1.662 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 0.398 | Reg loss: 0.020 | Tree loss: 0.398 | Accuracy: 0.856655 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.020 | Tree loss: 0.489 | Accuracy: 0.780500 | 1.664 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.767000 | 1.663 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.020 | Tree loss: 0.474 | Accuracy: 0.778000 | 1.663 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.831500 | 1.663 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.838500 | 1.663 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.787500 | 1.662 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.791000 | 1.662 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.791500 | 1.661 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.020 | Tree loss: 0.414 | Accuracy: 0.813000 | 1.661 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.819113 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.770000 | 1.661 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.782500 | 1.661 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.791000 | 1.661 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 0.457 | Reg loss: 0.020 | Tree loss: 0.457 | Accuracy: 0.792000 | 1.661 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.827000 | 1.661 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.789500 | 1.66 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 0.360 | Reg loss: 0.020 | Tree loss: 0.360 | Accuracy: 0.883959 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.020 | Tree loss: 0.483 | Accuracy: 0.774500 | 1.663 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.772000 | 1.663 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.020 | Tree loss: 0.465 | Accuracy: 0.780500 | 1.663 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.798000 | 1.662 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.020 | Tree loss: 0.416 | Accuracy: 0.839500 | 1.662 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.800000 | 1.662 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.813500 | 1.662 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.807000 | 1.661 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.799000 | 1.661 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.806000 | 1.661 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.020 | Tree loss: 0.405 | Accuracy: 0.825939 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.782000 | 1.664 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.768500 | 1.664 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.778000 | 1.664 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.820000 | 1.663 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.818000 | 1.663 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 0.425 | Reg loss: 0.020 | Tree loss: 0.425 | Accuracy: 0.820500 | 1.663 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.783500 | 1.661 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 0.399 | Reg loss: 0.020 | Tree loss: 0.399 | Accuracy: 0.831500 | 1.661 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 0.381 | Reg loss: 0.020 | Tree loss: 0.381 | Accuracy: 0.866894 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.020 | Tree loss: 0.488 | Accuracy: 0.780000 | 1.662 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.782000 | 1.662 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.020 | Tree loss: 0.465 | Accuracy: 0.792500 | 1.661 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.801000 | 1.661 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.020 | Tree loss: 0.417 | Accuracy: 0.836000 | 1.661 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.813000 | 1.66 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.020 | Tree loss: 0.421 | Accuracy: 0.807000 | 1.659 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 0.382 | Reg loss: 0.020 | Tree loss: 0.382 | Accuracy: 0.863481 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.020 | Tree loss: 0.478 | Accuracy: 0.781500 | 1.662 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.775500 | 1.662 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.020 | Tree loss: 0.460 | Accuracy: 0.803000 | 1.662 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.805000 | 1.661 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.829500 | 1.661 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.829500 | 1.661 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.802000 | 1.661 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 0.454 | Reg loss: 0.020 | Tree loss: 0.454 | Accuracy: 0.779000 | 1.66 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.020 | Tree loss: 0.419 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.020 | Tree loss: 0.399 | Accuracy: 0.846416 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.020 | Tree loss: 0.492 | Accuracy: 0.773000 | 1.663 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.764000 | 1.663 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.816000 | 1.663 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.830000 | 1.662 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.823500 | 1.662 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.804000 | 1.661 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.020 | Tree loss: 0.432 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.790500 | 1.661 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.020 | Tree loss: 0.413 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.020 | Tree loss: 0.416 | Accuracy: 0.812287 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 0.487 | Reg loss: 0.020 | Tree loss: 0.487 | Accuracy: 0.774000 | 1.661 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.020 | Tree loss: 0.475 | Accuracy: 0.782000 | 1.661 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.020 | Tree loss: 0.474 | Accuracy: 0.786000 | 1.661 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.812000 | 1.66 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.820500 | 1.66 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.804500 | 1.66 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.802500 | 1.659 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.795500 | 1.658 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.808000 | 1.658 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 0.489 | Reg loss: 0.020 | Tree loss: 0.489 | Accuracy: 0.798635 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.780500 | 1.662 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.020 | Tree loss: 0.485 | Accuracy: 0.770500 | 1.662 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.815000 | 1.662 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.020 | Tree loss: 0.417 | Accuracy: 0.835500 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.796000 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.801000 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.815000 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.794500 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.020 | Tree loss: 0.419 | Accuracy: 0.817500 | 1.661 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.808874 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 0.494 | Reg loss: 0.020 | Tree loss: 0.494 | Accuracy: 0.767500 | 1.664 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.785500 | 1.664 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.020 | Tree loss: 0.473 | Accuracy: 0.794500 | 1.663 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.817500 | 1.663 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.808500 | 1.663 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.824500 | 1.663 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.020 | Tree loss: 0.436 | Accuracy: 0.813000 | 1.662 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.797000 | 1.662 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.020 | Tree loss: 0.425 | Accuracy: 0.805000 | 1.661 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 0.450 | Reg loss: 0.020 | Tree loss: 0.450 | Accuracy: 0.805461 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.785000 | 1.662 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.761000 | 1.662 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.020 | Tree loss: 0.473 | Accuracy: 0.791000 | 1.661 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.812000 | 1.661 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.020 | Tree loss: 0.430 | Accuracy: 0.826500 | 1.661 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.020 | Tree loss: 0.415 | Accuracy: 0.836500 | 1.66 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 0.451 | Reg loss: 0.020 | Tree loss: 0.451 | Accuracy: 0.786000 | 1.659 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.789500 | 1.659 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.795000 | 1.659 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 0.434 | Reg loss: 0.020 | Tree loss: 0.434 | Accuracy: 0.795222 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.020 | Tree loss: 0.473 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 0.499 | Reg loss: 0.020 | Tree loss: 0.499 | Accuracy: 0.778500 | 1.66 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 0.467 | Reg loss: 0.020 | Tree loss: 0.467 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.020 | Tree loss: 0.416 | Accuracy: 0.836000 | 1.659 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.842000 | 1.659 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.811000 | 1.659 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.793500 | 1.658 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.782500 | 1.658 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 0.393 | Reg loss: 0.020 | Tree loss: 0.393 | Accuracy: 0.860068 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 0.487 | Reg loss: 0.020 | Tree loss: 0.487 | Accuracy: 0.786000 | 1.661 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.788000 | 1.661 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.020 | Tree loss: 0.469 | Accuracy: 0.796000 | 1.661 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.020 | Tree loss: 0.444 | Accuracy: 0.815500 | 1.661 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.020 | Tree loss: 0.427 | Accuracy: 0.815500 | 1.661 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.831000 | 1.66 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.800500 | 1.66 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.799000 | 1.66 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.789500 | 1.66 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.801000 | 1.659 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.020 | Tree loss: 0.404 | Accuracy: 0.829352 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 0.487 | Reg loss: 0.020 | Tree loss: 0.487 | Accuracy: 0.777500 | 1.661 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.020 | Tree loss: 0.490 | Accuracy: 0.765500 | 1.661 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 0.476 | Reg loss: 0.020 | Tree loss: 0.476 | Accuracy: 0.780500 | 1.66 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.833000 | 1.659 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.811500 | 1.659 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.794000 | 1.658 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.020 | Tree loss: 0.417 | Accuracy: 0.808500 | 1.658 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 0.438 | Reg loss: 0.020 | Tree loss: 0.438 | Accuracy: 0.802048 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.773500 | 1.659 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.788000 | 1.659 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.781000 | 1.659 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.020 | Tree loss: 0.440 | Accuracy: 0.812500 | 1.658 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.020 | Tree loss: 0.420 | Accuracy: 0.827000 | 1.658 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 0.425 | Reg loss: 0.020 | Tree loss: 0.425 | Accuracy: 0.819500 | 1.658 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.020 | Tree loss: 0.422 | Accuracy: 0.817000 | 1.658 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.778500 | 1.658 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.799000 | 1.657 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.020 | Tree loss: 0.443 | Accuracy: 0.791500 | 1.657 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 0.383 | Reg loss: 0.020 | Tree loss: 0.383 | Accuracy: 0.849829 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.020 | Tree loss: 0.478 | Accuracy: 0.788500 | 1.66 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.020 | Tree loss: 0.496 | Accuracy: 0.774000 | 1.66 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.806500 | 1.66 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 0.448 | Reg loss: 0.020 | Tree loss: 0.448 | Accuracy: 0.812500 | 1.66 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.825500 | 1.66 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.020 | Tree loss: 0.424 | Accuracy: 0.822500 | 1.659 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.788500 | 1.659 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.020 | Tree loss: 0.439 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.801500 | 1.658 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.846416 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.783500 | 1.662 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.020 | Tree loss: 0.493 | Accuracy: 0.773500 | 1.661 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 0.479 | Reg loss: 0.020 | Tree loss: 0.479 | Accuracy: 0.779500 | 1.661 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.805500 | 1.661 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.020 | Tree loss: 0.409 | Accuracy: 0.840500 | 1.661 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.818500 | 1.66 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.800500 | 1.66 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.020 | Tree loss: 0.442 | Accuracy: 0.798000 | 1.659 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.020 | Tree loss: 0.441 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 0.446 | Reg loss: 0.020 | Tree loss: 0.446 | Accuracy: 0.782500 | 1.659 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 0.447 | Reg loss: 0.020 | Tree loss: 0.447 | Accuracy: 0.764505 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.020 | Tree loss: 0.483 | Accuracy: 0.789500 | 1.66 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 0.498 | Reg loss: 0.020 | Tree loss: 0.498 | Accuracy: 0.777500 | 1.659 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 0.483 | Reg loss: 0.020 | Tree loss: 0.483 | Accuracy: 0.774500 | 1.659 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 0.459 | Reg loss: 0.020 | Tree loss: 0.459 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 0.428 | Reg loss: 0.020 | Tree loss: 0.428 | Accuracy: 0.829000 | 1.659 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.020 | Tree loss: 0.407 | Accuracy: 0.860000 | 1.658 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.806000 | 1.658 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.020 | Tree loss: 0.431 | Accuracy: 0.807000 | 1.658 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.020 | Tree loss: 0.437 | Accuracy: 0.790500 | 1.658 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 0.469 | Reg loss: 0.020 | Tree loss: 0.469 | Accuracy: 0.767500 | 1.658 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 0.453 | Reg loss: 0.020 | Tree loss: 0.453 | Accuracy: 0.784983 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.764000 | 1.661 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.771500 | 1.66 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.020 | Tree loss: 0.473 | Accuracy: 0.798000 | 1.66 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.020 | Tree loss: 0.414 | Accuracy: 0.840000 | 1.66 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.020 | Tree loss: 0.426 | Accuracy: 0.842000 | 1.66 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.020 | Tree loss: 0.423 | Accuracy: 0.807000 | 1.659 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 0.449 | Reg loss: 0.020 | Tree loss: 0.449 | Accuracy: 0.782000 | 1.659 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 0.464 | Reg loss: 0.020 | Tree loss: 0.464 | Accuracy: 0.770000 | 1.659 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.804000 | 1.659 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.808874 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.787000 | 1.661 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.020 | Tree loss: 0.491 | Accuracy: 0.774500 | 1.661 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.020 | Tree loss: 0.477 | Accuracy: 0.788500 | 1.66 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.020 | Tree loss: 0.445 | Accuracy: 0.814500 | 1.66 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.020 | Tree loss: 0.410 | Accuracy: 0.837000 | 1.66 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 0.433 | Reg loss: 0.020 | Tree loss: 0.433 | Accuracy: 0.834000 | 1.659 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.020 | Tree loss: 0.429 | Accuracy: 0.805500 | 1.659 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.793000 | 1.658 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.784500 | 1.658 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.829352 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.020 | Tree loss: 0.481 | Accuracy: 0.778000 | 1.659 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.020 | Tree loss: 0.484 | Accuracy: 0.780500 | 1.659 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.020 | Tree loss: 0.468 | Accuracy: 0.803500 | 1.658 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.020 | Tree loss: 0.452 | Accuracy: 0.800000 | 1.658 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 0.435 | Reg loss: 0.020 | Tree loss: 0.435 | Accuracy: 0.822000 | 1.658 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.853500 | 1.657 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.819500 | 1.657 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.796500 | 1.657 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.780500 | 1.657 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.799000 | 1.656 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.812287 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.020 | Tree loss: 0.480 | Accuracy: 0.777500 | 1.659 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 0.497 | Reg loss: 0.020 | Tree loss: 0.497 | Accuracy: 0.773000 | 1.659 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.777500 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.812500 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.832000 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.852000 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.809000 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.796000 | 1.658 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.792000 | 1.657 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.798635 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 0.491 | Reg loss: 0.021 | Tree loss: 0.491 | Accuracy: 0.773500 | 1.66 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.795000 | 1.66 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.786000 | 1.66 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.808500 | 1.659 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.826500 | 1.659 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.831500 | 1.659 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.814000 | 1.658 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.812287 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.773000 | 1.66 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.796000 | 1.66 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.021 | Tree loss: 0.458 | Accuracy: 0.799500 | 1.659 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.836000 | 1.659 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.834000 | 1.659 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.794500 | 1.658 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.801500 | 1.658 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.794000 | 1.658 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.784000 | 1.657 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.021 | Tree loss: 0.400 | Accuracy: 0.821000 | 1.657 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.781570 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.795000 | 1.658 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.021 | Tree loss: 0.493 | Accuracy: 0.780500 | 1.658 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.813000 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.815000 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.857000 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.815500 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.786000 | 1.656 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.853242 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.021 | Tree loss: 0.492 | Accuracy: 0.778000 | 1.659 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.771000 | 1.659 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.817500 | 1.659 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.828500 | 1.659 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.843000 | 1.658 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.796500 | 1.658 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.792000 | 1.658 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.808874 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.769000 | 1.659 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.789500 | 1.658 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.841500 | 1.658 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.852000 | 1.657 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.804500 | 1.657 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.782000 | 1.657 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.809000 | 1.656 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.805000 | 1.656 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.788396 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 0.495 | Reg loss: 0.021 | Tree loss: 0.495 | Accuracy: 0.772000 | 1.657 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.806000 | 1.657 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.817000 | 1.656 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.830000 | 1.656 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.832500 | 1.656 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.802000 | 1.656 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.804000 | 1.655 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.822526 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.770500 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.771500 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.803500 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.804500 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.822500 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.847000 | 1.658 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.788000 | 1.657 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.810500 | 1.657 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.799000 | 1.657 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.795222 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.779000 | 1.66 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.781500 | 1.659 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.781500 | 1.659 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.808500 | 1.659 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.818000 | 1.659 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.856500 | 1.658 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.807500 | 1.658 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.797500 | 1.658 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.793000 | 1.657 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.790000 | 1.657 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.798635 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.776500 | 1.658 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.789000 | 1.658 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.789000 | 1.657 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.816000 | 1.657 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.825500 | 1.657 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.848500 | 1.656 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.833000 | 1.656 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.800000 | 1.656 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.781000 | 1.655 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.808874 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.788500 | 1.658 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.021 | Tree loss: 0.501 | Accuracy: 0.761500 | 1.658 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.786500 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.825000 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.842500 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.849000 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.814000 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.785000 | 1.657 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.814000 | 1.656 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.791809 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.774500 | 1.659 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.778000 | 1.659 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.799000 | 1.658 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.822000 | 1.658 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.832500 | 1.658 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.835500 | 1.658 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.793000 | 1.657 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.789000 | 1.657 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.810500 | 1.656 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.021 | Tree loss: 0.386 | Accuracy: 0.856655 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.776000 | 1.657 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.795000 | 1.657 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.785000 | 1.657 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.810500 | 1.656 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.820000 | 1.656 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.819000 | 1.656 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.784000 | 1.655 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.790500 | 1.655 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.817000 | 1.655 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 0.391 | Reg loss: 0.021 | Tree loss: 0.391 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.782500 | 1.658 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.021 | Tree loss: 0.494 | Accuracy: 0.785500 | 1.658 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.777500 | 1.658 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.812000 | 1.658 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.835000 | 1.657 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.840000 | 1.657 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.828000 | 1.657 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.787500 | 1.656 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.808500 | 1.656 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.808874 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.776000 | 1.659 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.780000 | 1.659 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.787000 | 1.659 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.813000 | 1.659 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.820000 | 1.658 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.846500 | 1.658 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.805000 | 1.658 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.797000 | 1.658 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.815000 | 1.657 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.822526 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.785500 | 1.658 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 0.504 | Reg loss: 0.021 | Tree loss: 0.504 | Accuracy: 0.774000 | 1.657 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.806500 | 1.657 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.802000 | 1.657 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.844000 | 1.657 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.861000 | 1.656 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.803500 | 1.656 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.795500 | 1.655 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.796000 | 1.655 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.798635 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.785500 | 1.656 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.802500 | 1.656 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.790000 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.804500 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.842000 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.849500 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.819000 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.792500 | 1.655 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.810500 | 1.654 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.795500 | 1.654 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.788396 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.788000 | 1.657 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.021 | Tree loss: 0.491 | Accuracy: 0.779000 | 1.657 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.812500 | 1.657 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.821500 | 1.656 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.832000 | 1.656 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.810000 | 1.656 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.784000 | 1.656 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.829352 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.783000 | 1.657 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.021 | Tree loss: 0.494 | Accuracy: 0.764500 | 1.657 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.793000 | 1.656 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.843000 | 1.656 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.856000 | 1.655 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.804500 | 1.655 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.801000 | 1.655 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.790000 | 1.654 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.798000 | 1.654 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.778157 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.768000 | 1.655 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.790500 | 1.655 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.800500 | 1.655 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.808000 | 1.655 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.838000 | 1.654 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.836500 | 1.654 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.815500 | 1.654 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.781000 | 1.654 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.812000 | 1.654 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.800500 | 1.653 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.822526 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.791500 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.780500 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.797500 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.812000 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.819000 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.835000 | 1.656 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.810000 | 1.655 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.809000 | 1.655 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.793500 | 1.655 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.819113 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.789500 | 1.658 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.780500 | 1.657 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.795000 | 1.657 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.806500 | 1.657 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.820500 | 1.657 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.840500 | 1.656 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.822000 | 1.656 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.804000 | 1.655 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.795222 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.777500 | 1.656 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.781500 | 1.655 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.816500 | 1.655 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.841500 | 1.655 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.839500 | 1.655 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.808000 | 1.654 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.797000 | 1.654 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.805500 | 1.654 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.795500 | 1.653 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.021 | Tree loss: 0.400 | Accuracy: 0.812287 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.782500 | 1.656 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.781500 | 1.656 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.812500 | 1.655 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.824500 | 1.655 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.021 | Tree loss: 0.401 | Accuracy: 0.860500 | 1.655 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.819000 | 1.655 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.798000 | 1.655 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.771500 | 1.654 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.799000 | 1.654 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.805461 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.783500 | 1.657 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.788000 | 1.657 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.776000 | 1.657 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.815000 | 1.656 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.822500 | 1.656 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.835000 | 1.656 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.805500 | 1.655 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.801000 | 1.655 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.800000 | 1.655 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.021 | Tree loss: 0.396 | Accuracy: 0.825939 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.778500 | 1.655 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.777500 | 1.655 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.819000 | 1.655 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.817500 | 1.654 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.821500 | 1.654 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.808500 | 1.654 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.790500 | 1.653 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 0.456 | Reg loss: 0.021 | Tree loss: 0.456 | Accuracy: 0.782500 | 1.653 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.799500 | 1.653 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.021 | Tree loss: 0.392 | Accuracy: 0.849829 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.787000 | 1.659 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.779000 | 1.658 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.809000 | 1.658 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.815000 | 1.658 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.851500 | 1.658 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.812500 | 1.658 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.812000 | 1.657 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.797000 | 1.657 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.021 | Tree loss: 0.397 | Accuracy: 0.836177 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.793500 | 1.659 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.777500 | 1.659 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.810000 | 1.659 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.829000 | 1.659 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.836500 | 1.658 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.786500 | 1.658 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.814500 | 1.658 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.806500 | 1.658 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.797500 | 1.657 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 0.368 | Reg loss: 0.021 | Tree loss: 0.368 | Accuracy: 0.843003 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.021 | Tree loss: 0.465 | Accuracy: 0.787000 | 1.658 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.792500 | 1.658 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.822000 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.817500 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.845500 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.817000 | 1.657 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.795500 | 1.656 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.792000 | 1.656 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.829352 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.787500 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.782000 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.789500 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.817000 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.844000 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.848000 | 1.658 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.800500 | 1.657 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.800500 | 1.657 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.801500 | 1.657 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.803000 | 1.657 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.788396 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.794000 | 1.659 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.786000 | 1.659 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.816500 | 1.659 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.832000 | 1.658 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.847500 | 1.658 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.815500 | 1.658 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.806000 | 1.658 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.801000 | 1.657 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.806500 | 1.657 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.021 | Tree loss: 0.380 | Accuracy: 0.839590 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.782000 | 1.658 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.782500 | 1.657 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.800000 | 1.657 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.818500 | 1.657 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.818500 | 1.656 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.822500 | 1.656 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.791000 | 1.656 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.815500 | 1.655 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.021 | Tree loss: 0.390 | Accuracy: 0.853242 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.021 | Tree loss: 0.481 | Accuracy: 0.780500 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.826000 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.834500 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.846000 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.801500 | 1.656 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.803500 | 1.656 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.805000 | 1.656 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.784983 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.789000 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.779000 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.788000 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.846500 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.850000 | 1.658 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.816500 | 1.657 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.816000 | 1.657 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.791500 | 1.657 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.782500 | 1.657 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.798635 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.795000 | 1.658 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.772500 | 1.658 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.778000 | 1.657 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.816000 | 1.657 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.830000 | 1.657 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.842000 | 1.657 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.809500 | 1.656 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.803000 | 1.655 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 0.454 | Reg loss: 0.021 | Tree loss: 0.454 | Accuracy: 0.781570 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.773500 | 1.656 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 0.482 | Reg loss: 0.021 | Tree loss: 0.482 | Accuracy: 0.783500 | 1.656 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.817000 | 1.656 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.838500 | 1.656 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.844500 | 1.655 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.809000 | 1.655 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.792500 | 1.655 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.815700 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.779500 | 1.658 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.804500 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.828000 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.830000 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.812500 | 1.657 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.809000 | 1.656 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.810500 | 1.656 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.825939 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.021 | Tree loss: 0.492 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.772500 | 1.657 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.805000 | 1.657 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.810500 | 1.657 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.824000 | 1.656 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.845500 | 1.656 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.791000 | 1.656 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.800000 | 1.656 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.802000 | 1.655 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.829352 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.779500 | 1.656 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.778000 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.021 | Tree loss: 0.457 | Accuracy: 0.808500 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.818500 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.826000 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.852500 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.818000 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.798000 | 1.655 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.793500 | 1.654 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.785500 | 1.654 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.805461 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 0.492 | Reg loss: 0.021 | Tree loss: 0.492 | Accuracy: 0.788000 | 1.657 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 0.482 | Reg loss: 0.021 | Tree loss: 0.482 | Accuracy: 0.795000 | 1.657 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.807500 | 1.657 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.819500 | 1.657 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.858500 | 1.656 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.805500 | 1.656 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.782000 | 1.656 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.789000 | 1.656 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 0.367 | Reg loss: 0.021 | Tree loss: 0.367 | Accuracy: 0.843003 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.794500 | 1.658 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.817000 | 1.657 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.834000 | 1.657 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.846000 | 1.656 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.811000 | 1.656 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.801500 | 1.656 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 0.456 | Reg loss: 0.021 | Tree loss: 0.456 | Accuracy: 0.783500 | 1.656 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 0.455 | Reg loss: 0.021 | Tree loss: 0.455 | Accuracy: 0.794000 | 1.655 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 0.364 | Reg loss: 0.021 | Tree loss: 0.364 | Accuracy: 0.863481 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.798000 | 1.656 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.793000 | 1.656 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.786500 | 1.656 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.822500 | 1.655 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.827500 | 1.655 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.836000 | 1.655 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.820500 | 1.655 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.814500 | 1.654 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.795000 | 1.654 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.823500 | 1.654 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.808874 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.785500 | 1.656 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.784500 | 1.656 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.021 | Tree loss: 0.454 | Accuracy: 0.805500 | 1.656 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.826000 | 1.656 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.824000 | 1.656 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.843000 | 1.655 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.820500 | 1.655 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.808000 | 1.655 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 0.401 | Reg loss: 0.021 | Tree loss: 0.401 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.776500 | 1.657 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.790000 | 1.657 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.813500 | 1.656 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.831000 | 1.656 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.843500 | 1.656 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.793500 | 1.655 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.783500 | 1.655 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 0.492 | Reg loss: 0.021 | Tree loss: 0.492 | Accuracy: 0.730375 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.794500 | 1.655 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 0.497 | Reg loss: 0.021 | Tree loss: 0.497 | Accuracy: 0.765000 | 1.655 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.780000 | 1.655 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.808500 | 1.655 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.845500 | 1.654 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.855000 | 1.654 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.823500 | 1.654 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.796000 | 1.654 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.801500 | 1.653 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.791500 | 1.653 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 0.391 | Reg loss: 0.021 | Tree loss: 0.391 | Accuracy: 0.829352 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.782000 | 1.658 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.791500 | 1.658 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.810500 | 1.658 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.822500 | 1.658 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.825000 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.858500 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.798500 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.816500 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.798500 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.794500 | 1.657 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.802048 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.021 | Tree loss: 0.494 | Accuracy: 0.777500 | 1.659 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.804000 | 1.659 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.809000 | 1.659 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.826500 | 1.658 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.844500 | 1.658 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.806000 | 1.658 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.813000 | 1.658 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.795222 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.796500 | 1.658 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.021 | Tree loss: 0.496 | Accuracy: 0.766500 | 1.657 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.822000 | 1.657 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.847500 | 1.657 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.803500 | 1.656 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.817500 | 1.656 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.803000 | 1.656 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.805461 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.774500 | 1.658 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.793000 | 1.658 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.806500 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.827000 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.842500 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.799500 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.787000 | 1.656 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.782000 | 1.656 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.021 | Tree loss: 0.399 | Accuracy: 0.825939 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.782500 | 1.659 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 0.482 | Reg loss: 0.021 | Tree loss: 0.482 | Accuracy: 0.772000 | 1.658 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.787000 | 1.658 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.829500 | 1.658 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.857000 | 1.658 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.814500 | 1.657 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.792500 | 1.657 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.782500 | 1.657 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 0.394 | Reg loss: 0.021 | Tree loss: 0.394 | Accuracy: 0.836177 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.786000 | 1.657 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.792000 | 1.657 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.819000 | 1.656 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.833500 | 1.656 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.853000 | 1.656 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.812500 | 1.656 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.802000 | 1.655 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.812500 | 1.655 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.802048 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.796000 | 1.657 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.791000 | 1.657 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.795000 | 1.657 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.814500 | 1.657 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.843000 | 1.657 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.850500 | 1.656 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.807000 | 1.656 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.801000 | 1.656 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.794500 | 1.656 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.792000 | 1.656 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.819113 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.785500 | 1.658 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.773000 | 1.658 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.793000 | 1.658 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.810000 | 1.658 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.821000 | 1.657 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.843500 | 1.657 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.821500 | 1.657 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.792500 | 1.657 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.773000 | 1.657 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.799500 | 1.656 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.825939 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.788000 | 1.658 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.765500 | 1.657 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.021 | Tree loss: 0.465 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.815500 | 1.657 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.840000 | 1.657 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.847000 | 1.657 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.810500 | 1.656 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.812000 | 1.656 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.787000 | 1.656 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.822526 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.802000 | 1.656 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 0.501 | Reg loss: 0.021 | Tree loss: 0.501 | Accuracy: 0.760500 | 1.656 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.798000 | 1.656 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.807000 | 1.656 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.823500 | 1.656 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.835500 | 1.655 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.655 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.792500 | 1.655 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.800500 | 1.655 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.818000 | 1.655 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 0.393 | Reg loss: 0.021 | Tree loss: 0.393 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.799500 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.810000 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.813500 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.821500 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.851500 | 1.657 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.791000 | 1.656 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.803000 | 1.656 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.793500 | 1.656 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.802500 | 1.656 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.812287 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.777500 | 1.657 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.776000 | 1.657 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.798500 | 1.657 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.809000 | 1.656 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.826500 | 1.656 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.853000 | 1.656 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.797500 | 1.655 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.782000 | 1.655 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.803000 | 1.655 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 0.403 | Reg loss: 0.021 | Tree loss: 0.403 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.021 | Tree loss: 0.484 | Accuracy: 0.784000 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.787500 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.789500 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.803000 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.834500 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.854000 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.823000 | 1.655 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.811000 | 1.654 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.772500 | 1.654 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.814500 | 1.654 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.798635 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.796000 | 1.657 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.782000 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.776000 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.830000 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.856000 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.799000 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 0.384 | Reg loss: 0.021 | Tree loss: 0.384 | Accuracy: 0.815700 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 0.494 | Reg loss: 0.021 | Tree loss: 0.494 | Accuracy: 0.765000 | 1.657 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.816000 | 1.657 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.818500 | 1.657 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.833500 | 1.657 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.846000 | 1.656 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.813500 | 1.656 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.798000 | 1.656 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.794500 | 1.656 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.817000 | 1.655 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.822526 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.021 | Tree loss: 0.484 | Accuracy: 0.776500 | 1.656 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.791000 | 1.656 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.803500 | 1.656 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.837000 | 1.655 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.858500 | 1.655 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.817500 | 1.655 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.810500 | 1.654 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.817000 | 1.654 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.812287 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.784500 | 1.657 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.778500 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.793500 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.827000 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.835000 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.840000 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.817000 | 1.656 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.791500 | 1.655 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.796000 | 1.655 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.808874 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.776000 | 1.657 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.776500 | 1.657 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.822000 | 1.656 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.832000 | 1.656 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.852000 | 1.656 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.820000 | 1.656 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.797500 | 1.655 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.795500 | 1.655 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.781000 | 1.655 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.829352 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.781000 | 1.655 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.782500 | 1.655 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.797500 | 1.655 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.819500 | 1.654 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.836500 | 1.654 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.821000 | 1.654 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.803500 | 1.653 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.810500 | 1.653 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.815700 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.794000 | 1.658 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.769500 | 1.658 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.021 | Tree loss: 0.455 | Accuracy: 0.798000 | 1.658 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.801000 | 1.658 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.838000 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.856000 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.811000 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 0.452 | Reg loss: 0.021 | Tree loss: 0.452 | Accuracy: 0.787000 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.798500 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.805000 | 1.657 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.021 | Tree loss: 0.396 | Accuracy: 0.812287 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.797000 | 1.659 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.772500 | 1.658 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.793000 | 1.658 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.807500 | 1.658 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.829500 | 1.658 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.850500 | 1.658 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.807500 | 1.657 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.816500 | 1.657 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.802500 | 1.657 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.813000 | 1.657 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.808874 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.776000 | 1.657 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.803000 | 1.657 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.021 | Tree loss: 0.447 | Accuracy: 0.803500 | 1.657 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.820000 | 1.657 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.835000 | 1.656 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.840000 | 1.656 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.795500 | 1.656 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.793500 | 1.656 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.793000 | 1.656 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.819113 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.780500 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.777000 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.021 | Tree loss: 0.444 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.834000 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.846000 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.814500 | 1.657 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.823500 | 1.656 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.793000 | 1.656 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.021 | Tree loss: 0.397 | Accuracy: 0.843003 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.779500 | 1.658 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.789000 | 1.658 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.792000 | 1.658 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.803500 | 1.658 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.825000 | 1.657 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.854500 | 1.657 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.809000 | 1.657 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.821500 | 1.657 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.819000 | 1.656 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.812000 | 1.656 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.798635 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.793000 | 1.657 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.021 | Tree loss: 0.484 | Accuracy: 0.778500 | 1.657 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.801500 | 1.656 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.831000 | 1.656 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.842000 | 1.656 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.803500 | 1.655 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.791809 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.777500 | 1.657 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.786000 | 1.657 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.791500 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.801500 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.828500 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.849500 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.817000 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.804500 | 1.655 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.815700 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.789500 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.790000 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.809000 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.805000 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.829500 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.852000 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.802000 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.794500 | 1.657 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.810000 | 1.656 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 0.360 | Reg loss: 0.021 | Tree loss: 0.360 | Accuracy: 0.846416 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.792000 | 1.657 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.799000 | 1.657 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.812500 | 1.656 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.829000 | 1.656 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.844000 | 1.656 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.802000 | 1.656 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.794000 | 1.655 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.817500 | 1.655 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 0.383 | Reg loss: 0.021 | Tree loss: 0.383 | Accuracy: 0.856655 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.021 | Tree loss: 0.496 | Accuracy: 0.771500 | 1.656 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.834500 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.834000 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.821500 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.807000 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.813000 | 1.655 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.815000 | 1.654 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 0.384 | Reg loss: 0.021 | Tree loss: 0.384 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.800000 | 1.657 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.789000 | 1.657 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.813500 | 1.657 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.812000 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.833500 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.855500 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.807000 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.808000 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.815000 | 1.656 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.791809 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.778000 | 1.657 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.788500 | 1.656 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.806500 | 1.656 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.821000 | 1.656 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.846000 | 1.656 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.815500 | 1.655 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.797500 | 1.655 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.797500 | 1.655 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.778157 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.021 | Tree loss: 0.466 | Accuracy: 0.791500 | 1.655 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.790500 | 1.655 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.791000 | 1.655 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.822000 | 1.655 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.824000 | 1.655 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.855000 | 1.654 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.796000 | 1.654 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.793000 | 1.654 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.021 | Tree loss: 0.442 | Accuracy: 0.791000 | 1.654 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.809000 | 1.654 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.812287 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.790500 | 1.657 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.776500 | 1.657 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.792000 | 1.657 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.814500 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.831000 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.835000 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.806000 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.816000 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.807000 | 1.655 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.812287 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.774500 | 1.657 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.021 | Tree loss: 0.476 | Accuracy: 0.783500 | 1.657 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.792000 | 1.657 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.824000 | 1.656 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.829500 | 1.656 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.851000 | 1.656 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.828000 | 1.656 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.806500 | 1.655 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.803500 | 1.655 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.021 | Tree loss: 0.400 | Accuracy: 0.812287 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.794500 | 1.656 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.766000 | 1.655 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.844500 | 1.655 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.851000 | 1.655 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.806500 | 1.654 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.799000 | 1.654 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.806000 | 1.654 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.750853 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 0.505 | Reg loss: 0.021 | Tree loss: 0.505 | Accuracy: 0.763000 | 1.656 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.775000 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.788500 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.021 | Tree loss: 0.453 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.835500 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.859500 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.808000 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.811500 | 1.654 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.800000 | 1.654 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 0.485 | Reg loss: 0.021 | Tree loss: 0.485 | Accuracy: 0.780000 | 1.656 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 0.450 | Reg loss: 0.021 | Tree loss: 0.450 | Accuracy: 0.802000 | 1.656 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.021 | Tree loss: 0.457 | Accuracy: 0.796000 | 1.656 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.818000 | 1.656 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.824000 | 1.656 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.862500 | 1.655 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.816000 | 1.655 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.816500 | 1.655 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.793500 | 1.655 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.799000 | 1.654 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.021 | Tree loss: 0.390 | Accuracy: 0.849829 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.790500 | 1.655 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.780500 | 1.655 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.021 | Tree loss: 0.457 | Accuracy: 0.785500 | 1.655 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.819500 | 1.654 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.823500 | 1.654 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.852000 | 1.654 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.803500 | 1.654 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.809500 | 1.654 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.802500 | 1.653 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.814000 | 1.653 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 0.394 | Reg loss: 0.021 | Tree loss: 0.394 | Accuracy: 0.836177 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.788500 | 1.658 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.784000 | 1.658 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.781500 | 1.658 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.823500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.821500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.847500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.808500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.807000 | 1.657 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 0.401 | Reg loss: 0.021 | Tree loss: 0.401 | Accuracy: 0.829352 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.021 | Tree loss: 0.477 | Accuracy: 0.780500 | 1.659 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.784000 | 1.658 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.795000 | 1.658 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.831500 | 1.658 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.825500 | 1.658 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.818000 | 1.658 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.819500 | 1.657 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.806000 | 1.657 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.812000 | 1.657 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.836177 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.021 | Tree loss: 0.484 | Accuracy: 0.778500 | 1.657 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 0.492 | Reg loss: 0.021 | Tree loss: 0.492 | Accuracy: 0.781000 | 1.657 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.021 | Tree loss: 0.458 | Accuracy: 0.799500 | 1.657 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.817500 | 1.657 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.021 | Tree loss: 0.411 | Accuracy: 0.843000 | 1.656 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.847500 | 1.656 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.806000 | 1.656 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.788500 | 1.656 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.815500 | 1.655 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 0.379 | Reg loss: 0.021 | Tree loss: 0.379 | Accuracy: 0.860068 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.792500 | 1.657 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.791500 | 1.657 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.786000 | 1.657 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.804500 | 1.657 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.836000 | 1.657 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.852000 | 1.656 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.801000 | 1.656 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 0.396 | Reg loss: 0.021 | Tree loss: 0.396 | Accuracy: 0.822500 | 1.656 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 0.389 | Reg loss: 0.021 | Tree loss: 0.389 | Accuracy: 0.836177 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.796500 | 1.658 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.779000 | 1.658 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.804500 | 1.657 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.822000 | 1.657 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.833500 | 1.657 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.821500 | 1.657 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.807000 | 1.657 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.801000 | 1.656 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.809500 | 1.656 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.822526 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.791000 | 1.657 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.785500 | 1.656 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.021 | Tree loss: 0.448 | Accuracy: 0.800000 | 1.656 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.825500 | 1.656 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.851500 | 1.656 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.818500 | 1.655 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.798500 | 1.655 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.807500 | 1.655 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.808874 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.787500 | 1.657 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.775500 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.021 | Tree loss: 0.443 | Accuracy: 0.797500 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.812500 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.840000 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.853000 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.822000 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.805500 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.800000 | 1.656 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.798635 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.021 | Tree loss: 0.481 | Accuracy: 0.781000 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.788500 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.819000 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.021 | Tree loss: 0.407 | Accuracy: 0.834500 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.847500 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.818000 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.790000 | 1.657 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 0.449 | Reg loss: 0.021 | Tree loss: 0.449 | Accuracy: 0.784000 | 1.656 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.814500 | 1.656 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.021 | Tree loss: 0.397 | Accuracy: 0.822526 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.021 | Tree loss: 0.483 | Accuracy: 0.773500 | 1.657 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 0.508 | Reg loss: 0.021 | Tree loss: 0.508 | Accuracy: 0.758500 | 1.657 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.021 | Tree loss: 0.461 | Accuracy: 0.806000 | 1.657 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.821000 | 1.656 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.858000 | 1.656 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.811500 | 1.656 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.786500 | 1.655 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.807000 | 1.655 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 0.389 | Reg loss: 0.021 | Tree loss: 0.389 | Accuracy: 0.839590 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.021 | Tree loss: 0.465 | Accuracy: 0.797500 | 1.656 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.021 | Tree loss: 0.473 | Accuracy: 0.788500 | 1.656 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.799000 | 1.656 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.827000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.814000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.832500 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.805000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.808000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.809000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.817000 | 1.655 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.767918 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.021 | Tree loss: 0.490 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.021 | Tree loss: 0.481 | Accuracy: 0.771500 | 1.657 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.021 | Tree loss: 0.459 | Accuracy: 0.803500 | 1.657 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.021 | Tree loss: 0.437 | Accuracy: 0.814000 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.839500 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.849500 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.827500 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.790500 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.800500 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.803000 | 1.656 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.781570 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.781500 | 1.657 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.800000 | 1.657 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.791500 | 1.656 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.823500 | 1.656 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.021 | Tree loss: 0.413 | Accuracy: 0.833000 | 1.656 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.853500 | 1.656 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.809500 | 1.655 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.794000 | 1.655 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.789500 | 1.655 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.807500 | 1.655 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.784983 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.021 | Tree loss: 0.478 | Accuracy: 0.787500 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.778500 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.803000 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.827500 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.826500 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 0.422 | Reg loss: 0.021 | Tree loss: 0.422 | Accuracy: 0.837000 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.816000 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.021 | Tree loss: 0.424 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.820000 | 1.654 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.021 | Tree loss: 0.390 | Accuracy: 0.822526 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.021 | Tree loss: 0.455 | Accuracy: 0.789000 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.791500 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.021 | Tree loss: 0.427 | Accuracy: 0.811500 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.818000 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.021 | Tree loss: 0.410 | Accuracy: 0.855000 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.823500 | 1.656 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.805500 | 1.655 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.021 | Tree loss: 0.395 | Accuracy: 0.815700 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.021 | Tree loss: 0.464 | Accuracy: 0.801000 | 1.657 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.021 | Tree loss: 0.487 | Accuracy: 0.780500 | 1.657 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.021 | Tree loss: 0.455 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.815000 | 1.657 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.831500 | 1.656 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.851500 | 1.656 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.820000 | 1.656 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.021 | Tree loss: 0.431 | Accuracy: 0.801000 | 1.656 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.792000 | 1.655 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 0.401 | Reg loss: 0.021 | Tree loss: 0.401 | Accuracy: 0.825939 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.021 | Tree loss: 0.489 | Accuracy: 0.774500 | 1.656 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.021 | Tree loss: 0.472 | Accuracy: 0.793500 | 1.656 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.021 | Tree loss: 0.454 | Accuracy: 0.795500 | 1.655 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.021 | Tree loss: 0.419 | Accuracy: 0.809500 | 1.655 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.021 | Tree loss: 0.408 | Accuracy: 0.839000 | 1.655 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.860500 | 1.655 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.811000 | 1.655 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.800000 | 1.654 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.807000 | 1.654 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.809500 | 1.654 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 0.433 | Reg loss: 0.021 | Tree loss: 0.433 | Accuracy: 0.771331 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.803000 | 1.656 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.772000 | 1.656 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.021 | Tree loss: 0.463 | Accuracy: 0.812000 | 1.656 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.802500 | 1.656 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.821000 | 1.656 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.021 | Tree loss: 0.406 | Accuracy: 0.857000 | 1.655 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.021 | Tree loss: 0.417 | Accuracy: 0.807000 | 1.655 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.814000 | 1.655 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.021 | Tree loss: 0.430 | Accuracy: 0.804500 | 1.655 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.809000 | 1.655 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.021 | Tree loss: 0.405 | Accuracy: 0.832765 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 0.451 | Reg loss: 0.021 | Tree loss: 0.451 | Accuracy: 0.806000 | 1.657 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.791000 | 1.656 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.021 | Tree loss: 0.467 | Accuracy: 0.797500 | 1.656 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.809500 | 1.656 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 0.398 | Reg loss: 0.021 | Tree loss: 0.398 | Accuracy: 0.835500 | 1.656 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.850500 | 1.656 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.021 | Tree loss: 0.426 | Accuracy: 0.810000 | 1.655 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.021 | Tree loss: 0.421 | Accuracy: 0.805500 | 1.655 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.788000 | 1.655 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 0.370 | Reg loss: 0.021 | Tree loss: 0.370 | Accuracy: 0.849829 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.783000 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.021 | Tree loss: 0.471 | Accuracy: 0.779000 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.801500 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.021 | Tree loss: 0.403 | Accuracy: 0.835500 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.021 | Tree loss: 0.398 | Accuracy: 0.861500 | 1.655 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.021 | Tree loss: 0.402 | Accuracy: 0.828000 | 1.654 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.021 | Tree loss: 0.438 | Accuracy: 0.796500 | 1.654 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.021 | Tree loss: 0.428 | Accuracy: 0.798000 | 1.654 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 0.382 | Reg loss: 0.021 | Tree loss: 0.382 | Accuracy: 0.839590 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.021 | Tree loss: 0.481 | Accuracy: 0.780000 | 1.657 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.021 | Tree loss: 0.470 | Accuracy: 0.786500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.021 | Tree loss: 0.445 | Accuracy: 0.796500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.021 | Tree loss: 0.434 | Accuracy: 0.816000 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.021 | Tree loss: 0.412 | Accuracy: 0.832500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.021 | Tree loss: 0.414 | Accuracy: 0.838500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.021 | Tree loss: 0.401 | Accuracy: 0.814500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.021 | Tree loss: 0.423 | Accuracy: 0.793500 | 1.656 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.021 | Tree loss: 0.416 | Accuracy: 0.806000 | 1.655 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.822526 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.788000 | 1.658 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.021 | Tree loss: 0.486 | Accuracy: 0.784500 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.021 | Tree loss: 0.454 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.021 | Tree loss: 0.429 | Accuracy: 0.811500 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.840500 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.021 | Tree loss: 0.400 | Accuracy: 0.861000 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.021 | Tree loss: 0.420 | Accuracy: 0.804000 | 1.657 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.021 | Tree loss: 0.432 | Accuracy: 0.798500 | 1.656 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.021 | Tree loss: 0.425 | Accuracy: 0.800500 | 1.656 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.802000 | 1.656 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.846416 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.021 | Tree loss: 0.457 | Accuracy: 0.800500 | 1.656 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.021 | Tree loss: 0.479 | Accuracy: 0.783500 | 1.656 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.021 | Tree loss: 0.446 | Accuracy: 0.808500 | 1.656 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.021 | Tree loss: 0.439 | Accuracy: 0.814500 | 1.656 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.021 | Tree loss: 0.415 | Accuracy: 0.832000 | 1.655 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.835500 | 1.655 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.021 | Tree loss: 0.436 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.021 | Tree loss: 0.418 | Accuracy: 0.809500 | 1.655 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.808000 | 1.655 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.825939 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.021 | Tree loss: 0.481 | Accuracy: 0.775000 | 1.655 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.021 | Tree loss: 0.480 | Accuracy: 0.782500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.021 | Tree loss: 0.454 | Accuracy: 0.801500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.021 | Tree loss: 0.435 | Accuracy: 0.804500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.021 | Tree loss: 0.409 | Accuracy: 0.835000 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.021 | Tree loss: 0.404 | Accuracy: 0.851500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.803000 | 1.654 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.817500 | 1.653 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.822526 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.021 | Tree loss: 0.475 | Accuracy: 0.777500 | 1.656 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.021 | Tree loss: 0.474 | Accuracy: 0.776500 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 0.440 | Reg loss: 0.021 | Tree loss: 0.440 | Accuracy: 0.810000 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.021 | Tree loss: 0.441 | Accuracy: 0.796000 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.837500 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.841500 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.817000 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.787000 | 1.655 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.797500 | 1.654 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.818500 | 1.654 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 0.384 | Reg loss: 0.022 | Tree loss: 0.384 | Accuracy: 0.832765 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.021 | Tree loss: 0.469 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 0.503 | Reg loss: 0.021 | Tree loss: 0.503 | Accuracy: 0.755500 | 1.655 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.783500 | 1.655 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.815500 | 1.655 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.838000 | 1.655 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.833500 | 1.654 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.805500 | 1.654 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.799500 | 1.654 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.805500 | 1.654 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.798635 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.021 | Tree loss: 0.462 | Accuracy: 0.800500 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.021 | Tree loss: 0.488 | Accuracy: 0.783000 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.021 | Tree loss: 0.460 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.022 | Tree loss: 0.447 | Accuracy: 0.792000 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.821000 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.860000 | 1.654 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.795000 | 1.653 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.818000 | 1.653 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.819500 | 1.653 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.797500 | 1.653 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.795222 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.021 | Tree loss: 0.468 | Accuracy: 0.791000 | 1.655 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.793500 | 1.655 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.787500 | 1.655 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.819000 | 1.655 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.831000 | 1.655 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.839000 | 1.654 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.814000 | 1.654 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.806500 | 1.654 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.805000 | 1.654 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.811500 | 1.654 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 0.387 | Reg loss: 0.022 | Tree loss: 0.387 | Accuracy: 0.836177 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 0.482 | Reg loss: 0.022 | Tree loss: 0.482 | Accuracy: 0.772000 | 1.656 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.804000 | 1.656 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.795500 | 1.655 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.812000 | 1.655 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.838000 | 1.655 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.841000 | 1.655 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.799500 | 1.654 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.805000 | 1.654 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.829352 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.798000 | 1.655 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.783500 | 1.655 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.809000 | 1.654 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.827000 | 1.654 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.851500 | 1.654 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.821500 | 1.654 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.821000 | 1.653 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.795000 | 1.653 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.797000 | 1.653 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.822526 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.802000 | 1.655 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.791500 | 1.655 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.816000 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.836500 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.854500 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.810000 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.808000 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.817500 | 1.653 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.815700 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.022 | Tree loss: 0.493 | Accuracy: 0.773500 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 0.489 | Reg loss: 0.022 | Tree loss: 0.489 | Accuracy: 0.777000 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.806500 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.828000 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.840500 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.829500 | 1.655 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.819500 | 1.654 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.786500 | 1.654 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.812000 | 1.654 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.797000 | 1.654 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.798635 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.792500 | 1.654 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.781000 | 1.654 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.799000 | 1.654 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.826500 | 1.654 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.849500 | 1.653 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.817500 | 1.653 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.809500 | 1.653 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.798500 | 1.653 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.802500 | 1.653 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.819113 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.022 | Tree loss: 0.484 | Accuracy: 0.784500 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.810500 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.826000 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.848500 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.860500 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.820500 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.797000 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.794000 | 1.654 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.789000 | 1.653 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.819113 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.022 | Tree loss: 0.484 | Accuracy: 0.780500 | 1.655 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.022 | Tree loss: 0.486 | Accuracy: 0.776500 | 1.655 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.787000 | 1.655 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.824000 | 1.655 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.848000 | 1.655 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.846500 | 1.654 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.821000 | 1.654 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.811500 | 1.654 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.804000 | 1.654 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.022 | Tree loss: 0.380 | Accuracy: 0.846416 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.776000 | 1.655 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.796500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.793500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.816500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.840500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.855500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.805000 | 1.653 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.802000 | 1.653 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.803000 | 1.653 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.788396 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.786000 | 1.654 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.794000 | 1.654 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.777000 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.811000 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.839500 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.856500 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.808500 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.796500 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.801000 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.790000 | 1.653 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.836177 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.790000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.775500 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.801500 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.811000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.826500 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.841000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.818000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.803000 | 1.654 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.811500 | 1.653 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 0.378 | Reg loss: 0.022 | Tree loss: 0.378 | Accuracy: 0.849829 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.784000 | 1.654 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.791000 | 1.654 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.801000 | 1.654 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.811000 | 1.654 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.837500 | 1.654 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.840000 | 1.653 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.828000 | 1.653 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.802000 | 1.653 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.809500 | 1.653 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.807000 | 1.653 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 0.373 | Reg loss: 0.022 | Tree loss: 0.373 | Accuracy: 0.856655 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.786000 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.795000 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.798500 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.805500 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.831000 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.847500 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.817000 | 1.653 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.806000 | 1.652 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.796000 | 1.652 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.806000 | 1.652 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.849829 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.022 | Tree loss: 0.491 | Accuracy: 0.775000 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.784500 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.803500 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.833500 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.839000 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.797000 | 1.654 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.802500 | 1.653 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.803000 | 1.653 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.815000 | 1.653 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.846416 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.770500 | 1.655 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.794000 | 1.654 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.805000 | 1.654 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.825000 | 1.654 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.841000 | 1.654 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.830000 | 1.654 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.805000 | 1.653 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.790000 | 1.653 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.803500 | 1.653 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.788396 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.801500 | 1.654 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.022 | Tree loss: 0.487 | Accuracy: 0.777500 | 1.653 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.803500 | 1.653 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.815500 | 1.653 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.825000 | 1.653 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.841000 | 1.653 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.810500 | 1.652 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.808500 | 1.652 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.798000 | 1.652 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.802000 | 1.652 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.819113 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 0.486 | Reg loss: 0.022 | Tree loss: 0.486 | Accuracy: 0.789500 | 1.654 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.783500 | 1.654 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.816500 | 1.654 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.836000 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.849000 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.817500 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.807500 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.022 | Tree loss: 0.440 | Accuracy: 0.789500 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.796500 | 1.653 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.815700 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.802000 | 1.654 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.790500 | 1.654 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.808500 | 1.654 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.804000 | 1.654 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.823000 | 1.654 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.842500 | 1.653 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.821500 | 1.653 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.796000 | 1.653 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.794000 | 1.653 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.796000 | 1.653 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.805461 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 0.482 | Reg loss: 0.022 | Tree loss: 0.482 | Accuracy: 0.777000 | 1.653 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.794000 | 1.653 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.808000 | 1.653 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.820000 | 1.652 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.826500 | 1.652 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.855500 | 1.652 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.816000 | 1.652 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.801000 | 1.652 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.799000 | 1.651 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.814500 | 1.651 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 0.367 | Reg loss: 0.022 | Tree loss: 0.367 | Accuracy: 0.832765 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.805500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.785500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.806500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.808500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.850500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.829000 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.803000 | 1.654 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.791809 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.785500 | 1.656 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.792000 | 1.656 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.789500 | 1.656 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.832000 | 1.655 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.832000 | 1.655 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.851500 | 1.655 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.817000 | 1.655 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.818500 | 1.655 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.794000 | 1.654 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.781570 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.022 | Tree loss: 0.484 | Accuracy: 0.783500 | 1.655 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.807500 | 1.655 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.808500 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.839500 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.851500 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.817500 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.792500 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.800000 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.804000 | 1.654 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.798635 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.790000 | 1.656 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.776500 | 1.656 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.801000 | 1.656 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.812500 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.844000 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.840000 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.816000 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.820500 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.801500 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.785500 | 1.655 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.839590 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.785000 | 1.657 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.793000 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 0.440 | Reg loss: 0.022 | Tree loss: 0.440 | Accuracy: 0.809000 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.822500 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.830000 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.842500 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.809000 | 1.655 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.813500 | 1.655 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.802048 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 0.486 | Reg loss: 0.022 | Tree loss: 0.486 | Accuracy: 0.779500 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.788500 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.807000 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.841500 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.855000 | 1.655 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.815000 | 1.654 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.809000 | 1.654 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.799500 | 1.654 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.804000 | 1.654 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.795222 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.776000 | 1.656 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.791500 | 1.656 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.796500 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.817500 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.832000 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.843000 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.810000 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.809500 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.818500 | 1.655 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.798635 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 0.484 | Reg loss: 0.022 | Tree loss: 0.484 | Accuracy: 0.785000 | 1.656 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.799500 | 1.656 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.790000 | 1.656 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.819000 | 1.656 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.830500 | 1.655 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.839500 | 1.655 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.800000 | 1.655 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.822500 | 1.655 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.817000 | 1.654 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.788396 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.787500 | 1.655 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 0.482 | Reg loss: 0.022 | Tree loss: 0.482 | Accuracy: 0.774500 | 1.655 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.807500 | 1.655 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.813500 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.828000 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.848000 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.822500 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.788000 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.805000 | 1.654 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.805500 | 1.653 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.795222 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.789000 | 1.655 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.785500 | 1.655 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.822000 | 1.655 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.827500 | 1.655 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.850500 | 1.654 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.815500 | 1.654 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.790000 | 1.654 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.799000 | 1.654 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.813500 | 1.654 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.802048 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.783500 | 1.656 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.781500 | 1.656 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.803500 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.815500 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.814500 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.843500 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.834000 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.808500 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 0.440 | Reg loss: 0.022 | Tree loss: 0.440 | Accuracy: 0.793000 | 1.655 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 0.370 | Reg loss: 0.022 | Tree loss: 0.370 | Accuracy: 0.825939 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.792500 | 1.656 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.779500 | 1.655 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.810500 | 1.655 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.816500 | 1.655 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.824000 | 1.655 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.856500 | 1.655 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.817000 | 1.654 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.807500 | 1.654 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.803500 | 1.654 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.811500 | 1.654 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.825939 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 0.493 | Reg loss: 0.022 | Tree loss: 0.493 | Accuracy: 0.780000 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.788000 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.813500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.824500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.836500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.854500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.816500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.813500 | 1.654 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.825500 | 1.653 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.822526 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.800000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.778000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.795000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.813000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.819000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.841000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.800000 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.816500 | 1.655 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.801000 | 1.654 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.810000 | 1.654 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 0.387 | Reg loss: 0.022 | Tree loss: 0.387 | Accuracy: 0.832765 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.791500 | 1.655 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 0.482 | Reg loss: 0.022 | Tree loss: 0.482 | Accuracy: 0.787000 | 1.655 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.811500 | 1.655 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.820500 | 1.655 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.839500 | 1.654 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.849000 | 1.654 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.828000 | 1.654 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.787500 | 1.654 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.796000 | 1.654 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.793000 | 1.653 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.815700 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.793000 | 1.654 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.790000 | 1.654 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.778000 | 1.654 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.797500 | 1.654 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.832500 | 1.654 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.855500 | 1.653 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.815500 | 1.653 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.796000 | 1.653 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.799000 | 1.653 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.811000 | 1.653 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.825939 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.799000 | 1.655 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.800000 | 1.655 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.798000 | 1.655 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.817500 | 1.655 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.836500 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.852000 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.809500 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.813500 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.802500 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.812000 | 1.654 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.808874 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.795000 | 1.656 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.789500 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.787000 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.807500 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.824000 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.865000 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.829500 | 1.655 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.803000 | 1.654 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.779500 | 1.654 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.817000 | 1.654 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 0.351 | Reg loss: 0.022 | Tree loss: 0.351 | Accuracy: 0.829352 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.792000 | 1.654 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.797000 | 1.654 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.805500 | 1.654 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.840500 | 1.654 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.848500 | 1.654 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.828000 | 1.653 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.789000 | 1.653 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 0.440 | Reg loss: 0.022 | Tree loss: 0.440 | Accuracy: 0.790500 | 1.653 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.826000 | 1.653 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 0.354 | Reg loss: 0.022 | Tree loss: 0.354 | Accuracy: 0.863481 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.791000 | 1.655 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.799500 | 1.655 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.804500 | 1.655 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.022 | Tree loss: 0.440 | Accuracy: 0.816000 | 1.655 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.821000 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.831500 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.814000 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.816500 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.814500 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.832500 | 1.654 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 0.355 | Reg loss: 0.022 | Tree loss: 0.355 | Accuracy: 0.866894 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 0.495 | Reg loss: 0.022 | Tree loss: 0.495 | Accuracy: 0.771000 | 1.655 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.802500 | 1.655 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.805000 | 1.655 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.822000 | 1.655 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.832500 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.855000 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.811000 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.804500 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.800000 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.823500 | 1.654 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.819113 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.808500 | 1.654 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.793000 | 1.654 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.800000 | 1.654 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.800500 | 1.654 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.840000 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.843500 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.827000 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.798000 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.800500 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.799500 | 1.653 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 0.372 | Reg loss: 0.022 | Tree loss: 0.372 | Accuracy: 0.853242 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.794500 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.795500 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.792500 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.807000 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.831000 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.862000 | 1.654 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.822000 | 1.653 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.808500 | 1.653 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.803000 | 1.653 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.803000 | 1.653 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.805461 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.781500 | 1.655 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 0.444 | Reg loss: 0.022 | Tree loss: 0.444 | Accuracy: 0.801000 | 1.655 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.809000 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.823500 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.864500 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.804000 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.801000 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.803000 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.812500 | 1.654 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.812287 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.792000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.788500 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.797000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.813000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.832500 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.852000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.818000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.798500 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.792500 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.818000 | 1.655 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.022 | Tree loss: 0.380 | Accuracy: 0.853242 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.800000 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.790500 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.802000 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.809500 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.819000 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.857500 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.809000 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.810500 | 1.657 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.804500 | 1.658 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.811500 | 1.658 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.812287 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.785500 | 1.659 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.817500 | 1.658 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.831000 | 1.658 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.844000 | 1.658 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.828500 | 1.658 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.802500 | 1.658 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.799500 | 1.657 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.812287 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.795000 | 1.658 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.783500 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.805500 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.840000 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.849500 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.796500 | 1.657 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.779500 | 1.656 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.811500 | 1.656 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.822526 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.792000 | 1.658 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.775000 | 1.658 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.022 | Tree loss: 0.450 | Accuracy: 0.800500 | 1.658 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.813000 | 1.658 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.833500 | 1.658 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.864500 | 1.657 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.800500 | 1.657 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.806000 | 1.657 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.801500 | 1.657 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.815500 | 1.657 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 0.355 | Reg loss: 0.022 | Tree loss: 0.355 | Accuracy: 0.856655 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.790000 | 1.658 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.796000 | 1.658 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.816500 | 1.658 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.832000 | 1.657 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.858500 | 1.657 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.816000 | 1.657 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.803000 | 1.657 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.807000 | 1.657 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.815500 | 1.656 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.836177 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 0.489 | Reg loss: 0.022 | Tree loss: 0.489 | Accuracy: 0.779500 | 1.657 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.787000 | 1.657 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.801000 | 1.657 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.817500 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.839500 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.848500 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.823000 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.800500 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.818000 | 1.656 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.804000 | 1.655 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 0.370 | Reg loss: 0.022 | Tree loss: 0.370 | Accuracy: 0.853242 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.795500 | 1.659 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.799500 | 1.659 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.022 | Tree loss: 0.451 | Accuracy: 0.798500 | 1.659 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.807500 | 1.659 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.830000 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.851000 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.812500 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.806500 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.802500 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.800500 | 1.658 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 0.333 | Reg loss: 0.022 | Tree loss: 0.333 | Accuracy: 0.873720 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.788500 | 1.66 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.804500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.791500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.810500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.824500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.856500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.797000 | 1.658 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.814500 | 1.658 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.810000 | 1.658 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.805461 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.781500 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.805000 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.814000 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.815500 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.822500 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.856500 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.802500 | 1.658 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.813000 | 1.657 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.820000 | 1.657 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.815000 | 1.657 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.791809 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.808000 | 1.659 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.022 | Tree loss: 0.477 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.795500 | 1.659 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.810000 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.839500 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.847000 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.806000 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.808000 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.798500 | 1.658 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 0.385 | Reg loss: 0.022 | Tree loss: 0.385 | Accuracy: 0.839590 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.792500 | 1.66 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.786000 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.812000 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.825000 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.848000 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.820000 | 1.659 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.812000 | 1.658 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.791500 | 1.658 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.816000 | 1.658 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.798635 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.022 | Tree loss: 0.477 | Accuracy: 0.783000 | 1.659 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.793000 | 1.659 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.806500 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.824000 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.853500 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.819000 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.819000 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.797000 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.808000 | 1.658 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.819113 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.797000 | 1.66 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.788500 | 1.66 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.790000 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.803500 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.825000 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.869500 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.822500 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.801500 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.789000 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.824000 | 1.659 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 0.372 | Reg loss: 0.022 | Tree loss: 0.372 | Accuracy: 0.839590 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.792000 | 1.66 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.816000 | 1.66 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.838000 | 1.66 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.843500 | 1.659 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.818000 | 1.659 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.811000 | 1.659 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.798000 | 1.659 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.826500 | 1.659 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.022 | Tree loss: 0.388 | Accuracy: 0.825939 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.797500 | 1.659 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.791500 | 1.659 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.793000 | 1.659 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.818000 | 1.659 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.834500 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.847000 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.824500 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.798000 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.791500 | 1.658 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 0.368 | Reg loss: 0.022 | Tree loss: 0.368 | Accuracy: 0.849829 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.803500 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.783500 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.816500 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.829500 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.846000 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.810500 | 1.659 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.809500 | 1.658 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.803000 | 1.658 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.811500 | 1.658 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.764505 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.793500 | 1.66 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.788500 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.813500 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.839000 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.855500 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.794000 | 1.659 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.810000 | 1.658 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.811500 | 1.658 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.819113 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.788500 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.795500 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.803000 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.833000 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.854500 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.800000 | 1.658 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.803000 | 1.657 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.776500 | 1.657 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.804500 | 1.657 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.829352 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.779000 | 1.659 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.790500 | 1.659 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.022 | Tree loss: 0.444 | Accuracy: 0.815000 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.823000 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.839500 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.810000 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.809000 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.800500 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.820500 | 1.658 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.022 | Tree loss: 0.386 | Accuracy: 0.836177 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.812500 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 0.487 | Reg loss: 0.022 | Tree loss: 0.487 | Accuracy: 0.774500 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.810500 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.819000 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.830000 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.854500 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.814000 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.811000 | 1.659 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.801500 | 1.658 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.022 | Tree loss: 0.388 | Accuracy: 0.843003 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 0.447 | Reg loss: 0.022 | Tree loss: 0.447 | Accuracy: 0.810500 | 1.659 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.797500 | 1.659 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.798000 | 1.659 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.827500 | 1.659 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.832000 | 1.658 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.852000 | 1.658 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.795500 | 1.658 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.801500 | 1.658 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.802500 | 1.658 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.809500 | 1.657 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 0.387 | Reg loss: 0.022 | Tree loss: 0.387 | Accuracy: 0.853242 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.782500 | 1.658 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.783000 | 1.658 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.801000 | 1.658 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.807500 | 1.658 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.837000 | 1.658 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.856000 | 1.657 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.820500 | 1.657 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.815500 | 1.657 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.807500 | 1.657 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.804000 | 1.657 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.791809 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.791500 | 1.659 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.800500 | 1.659 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.820000 | 1.659 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.821500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.852500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.815500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.814000 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.787500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.800500 | 1.658 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.815700 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.798000 | 1.659 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.781000 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.022 | Tree loss: 0.451 | Accuracy: 0.802000 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.812500 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.827500 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.849500 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.812000 | 1.658 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.789500 | 1.657 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.794500 | 1.657 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.836177 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.784000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.794000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.798500 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.827000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.827500 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.862000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.804000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.793000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.808000 | 1.657 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.811500 | 1.656 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.022 | Tree loss: 0.388 | Accuracy: 0.849829 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.791500 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.787000 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.794500 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.838000 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 0.393 | Reg loss: 0.022 | Tree loss: 0.393 | Accuracy: 0.848000 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.823500 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.800000 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.819000 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.809500 | 1.658 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.803500 | 1.657 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 0.373 | Reg loss: 0.022 | Tree loss: 0.373 | Accuracy: 0.843003 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.782500 | 1.659 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.782500 | 1.659 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.791000 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.821500 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.839000 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.866000 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.824000 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.793500 | 1.657 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.856655 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.790500 | 1.658 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.790500 | 1.658 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.786000 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.807000 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 0.389 | Reg loss: 0.022 | Tree loss: 0.389 | Accuracy: 0.843000 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.832500 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.821000 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.801000 | 1.657 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.814500 | 1.656 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.816500 | 1.656 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 0.394 | Reg loss: 0.022 | Tree loss: 0.394 | Accuracy: 0.836177 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.793500 | 1.658 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.790000 | 1.658 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.801000 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.824500 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.846500 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.841000 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.810500 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.806500 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.805000 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.804000 | 1.657 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.832765 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.803000 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.022 | Tree loss: 0.493 | Accuracy: 0.774500 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.793500 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.835000 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.828500 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.859000 | 1.658 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.812500 | 1.657 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.807000 | 1.657 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.808500 | 1.657 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.825939 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.795000 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.786500 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.795500 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.798000 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.828000 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.852500 | 1.657 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.806500 | 1.656 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.807500 | 1.656 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.816500 | 1.656 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.809500 | 1.656 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.022 | Tree loss: 0.380 | Accuracy: 0.846416 | 1.656 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.794000 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.799000 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 0.441 | Reg loss: 0.022 | Tree loss: 0.441 | Accuracy: 0.811500 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.807500 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.827000 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.863500 | 1.659 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.806500 | 1.658 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.798000 | 1.658 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.805000 | 1.658 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.808874 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 0.441 | Reg loss: 0.022 | Tree loss: 0.441 | Accuracy: 0.811000 | 1.66 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.791500 | 1.66 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.809500 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.812500 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.851000 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.818000 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.801500 | 1.659 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.808000 | 1.658 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.819113 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.809500 | 1.659 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.784500 | 1.659 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 0.441 | Reg loss: 0.022 | Tree loss: 0.441 | Accuracy: 0.801000 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.818500 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.839500 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.803000 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.794500 | 1.658 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.823000 | 1.657 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.814000 | 1.657 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 0.377 | Reg loss: 0.022 | Tree loss: 0.377 | Accuracy: 0.832765 | 1.657 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 0.490 | Reg loss: 0.022 | Tree loss: 0.490 | Accuracy: 0.772500 | 1.659 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.791000 | 1.659 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.022 | Tree loss: 0.447 | Accuracy: 0.799500 | 1.659 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.805500 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.827500 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.856000 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.813000 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.801000 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.813500 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.815000 | 1.658 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.812287 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.775000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.806500 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.830500 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.850500 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.806500 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.824000 | 1.66 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.812287 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 0.451 | Reg loss: 0.022 | Tree loss: 0.451 | Accuracy: 0.806500 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.022 | Tree loss: 0.450 | Accuracy: 0.804000 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.804000 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.821500 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.843500 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.817500 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.808000 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.817000 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.825000 | 1.662 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.822526 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.782500 | 1.663 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.813500 | 1.663 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.834500 | 1.663 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.826500 | 1.663 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.837500 | 1.662 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.809000 | 1.662 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.819000 | 1.662 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.794500 | 1.662 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.808874 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.790000 | 1.663 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.793000 | 1.663 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.798000 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.813500 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.833500 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 0.391 | Reg loss: 0.022 | Tree loss: 0.391 | Accuracy: 0.853500 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.817000 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.798500 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.813000 | 1.662 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 0.376 | Reg loss: 0.022 | Tree loss: 0.376 | Accuracy: 0.832765 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.791500 | 1.662 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.797000 | 1.662 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 0.444 | Reg loss: 0.022 | Tree loss: 0.444 | Accuracy: 0.800500 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.816000 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.827000 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.850000 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.818000 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.808000 | 1.661 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.757679 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.791000 | 1.662 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.792000 | 1.662 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.802500 | 1.662 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.802000 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.824000 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.847000 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.824500 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.787500 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.802500 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.818000 | 1.661 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.822526 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.791000 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.792500 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.802000 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.819500 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.840000 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.845500 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.814500 | 1.661 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.807500 | 1.66 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.781500 | 1.66 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.822526 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.796500 | 1.661 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 0.482 | Reg loss: 0.022 | Tree loss: 0.482 | Accuracy: 0.782500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.022 | Tree loss: 0.451 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.813500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.833000 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.852500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.796500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.813500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.815700 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.785000 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.772000 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.784000 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.804500 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.835500 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.860500 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.807500 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.803000 | 1.661 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.786000 | 1.66 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 0.378 | Reg loss: 0.022 | Tree loss: 0.378 | Accuracy: 0.849829 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.793500 | 1.662 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.781500 | 1.662 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.793500 | 1.662 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.815000 | 1.662 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.827500 | 1.662 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.846000 | 1.661 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.807500 | 1.661 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.802500 | 1.661 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.808874 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.797500 | 1.662 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 0.447 | Reg loss: 0.022 | Tree loss: 0.447 | Accuracy: 0.804000 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.833000 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.852500 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.831500 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.804000 | 1.661 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.802048 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.780500 | 1.661 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.815500 | 1.661 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.808000 | 1.661 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.830500 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.847500 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.804500 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.829352 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.791500 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.022 | Tree loss: 0.477 | Accuracy: 0.787000 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.805500 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.818500 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.839000 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.855000 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.818500 | 1.661 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 0.367 | Reg loss: 0.022 | Tree loss: 0.367 | Accuracy: 0.832765 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.794500 | 1.661 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.022 | Tree loss: 0.452 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.822500 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.852000 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.801500 | 1.66 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.792000 | 1.659 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.812000 | 1.659 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.819500 | 1.659 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.822526 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.801500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.779500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.792000 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.799500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.856500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.853500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.819500 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.822000 | 1.662 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.787000 | 1.661 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.794000 | 1.661 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.812287 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.800000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.790000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.788000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.792000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.828000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.849000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.817500 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.789500 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.800500 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.022 | Tree loss: 0.386 | Accuracy: 0.846416 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.801000 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.781000 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.811500 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.833000 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.849500 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.805500 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.792000 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.803000 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.826500 | 1.664 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.819113 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.795500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.789000 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.793000 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.801500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.822500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.846500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.815500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.817500 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.790000 | 1.665 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.798000 | 1.664 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 0.376 | Reg loss: 0.022 | Tree loss: 0.376 | Accuracy: 0.822526 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.793000 | 1.665 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.782500 | 1.665 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 0.473 | Reg loss: 0.022 | Tree loss: 0.473 | Accuracy: 0.778000 | 1.665 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.022 | Tree loss: 0.441 | Accuracy: 0.796000 | 1.665 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.835000 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.849000 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.831500 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.804500 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.794500 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.803000 | 1.664 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.812287 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.797500 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.775500 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 0.462 | Reg loss: 0.022 | Tree loss: 0.462 | Accuracy: 0.792000 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.806000 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.828000 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.851500 | 1.664 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.817500 | 1.663 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.809500 | 1.663 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.815500 | 1.663 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.795222 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.795500 | 1.665 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.789000 | 1.665 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.789000 | 1.665 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.811000 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.840500 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.852000 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.815500 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.800000 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.796000 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.801500 | 1.664 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.832765 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.785000 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.786000 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.799000 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.817000 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.847500 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 0.389 | Reg loss: 0.022 | Tree loss: 0.389 | Accuracy: 0.860500 | 1.665 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.820000 | 1.664 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.795000 | 1.664 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.783500 | 1.664 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.802048 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.784000 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.795000 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.819000 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.839000 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.834500 | 1.664 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.823500 | 1.663 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 0.434 | Reg loss: 0.022 | Tree loss: 0.434 | Accuracy: 0.809000 | 1.663 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.663 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.022 | Tree loss: 0.386 | Accuracy: 0.829352 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.782000 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.789500 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.794000 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.815000 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.843000 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.857500 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.805000 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.810500 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.806500 | 1.664 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.833500 | 1.663 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 0.387 | Reg loss: 0.022 | Tree loss: 0.387 | Accuracy: 0.819113 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 0.453 | Reg loss: 0.022 | Tree loss: 0.453 | Accuracy: 0.806000 | 1.665 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 0.481 | Reg loss: 0.022 | Tree loss: 0.481 | Accuracy: 0.785000 | 1.665 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.796500 | 1.665 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.817500 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.835000 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.022 | Tree loss: 0.394 | Accuracy: 0.857500 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.816500 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.804500 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.821500 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.811000 | 1.664 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.795222 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.787500 | 1.664 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 0.491 | Reg loss: 0.022 | Tree loss: 0.491 | Accuracy: 0.781500 | 1.664 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.817500 | 1.664 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.022 | Tree loss: 0.430 | Accuracy: 0.808500 | 1.664 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.820500 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.863500 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.822500 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.818500 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.785000 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.810000 | 1.663 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 0.354 | Reg loss: 0.022 | Tree loss: 0.354 | Accuracy: 0.863481 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.803000 | 1.664 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.787500 | 1.664 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.022 | Tree loss: 0.443 | Accuracy: 0.805000 | 1.664 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 0.450 | Reg loss: 0.022 | Tree loss: 0.450 | Accuracy: 0.797500 | 1.664 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 0.392 | Reg loss: 0.022 | Tree loss: 0.392 | Accuracy: 0.840000 | 1.664 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.857000 | 1.663 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.804500 | 1.663 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.806500 | 1.663 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.813500 | 1.663 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 0.375 | Reg loss: 0.022 | Tree loss: 0.375 | Accuracy: 0.832765 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 0.469 | Reg loss: 0.022 | Tree loss: 0.469 | Accuracy: 0.780500 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.812500 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.809000 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.852000 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.806500 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.804500 | 1.664 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.805000 | 1.663 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 0.363 | Reg loss: 0.022 | Tree loss: 0.363 | Accuracy: 0.866894 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.785500 | 1.664 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.022 | Tree loss: 0.470 | Accuracy: 0.801000 | 1.664 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.801500 | 1.664 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.831500 | 1.664 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.812500 | 1.664 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.852000 | 1.663 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.822000 | 1.663 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.791000 | 1.663 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.794000 | 1.663 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 0.385 | Reg loss: 0.022 | Tree loss: 0.385 | Accuracy: 0.819113 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.789500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.783500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.798000 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.812500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.842000 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.860500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.806500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.794500 | 1.662 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 0.384 | Reg loss: 0.022 | Tree loss: 0.384 | Accuracy: 0.812287 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.787500 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.800000 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.794000 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.814500 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.838000 | 1.664 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.826000 | 1.663 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.788000 | 1.663 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.836177 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.788000 | 1.664 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.798500 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.823000 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.822000 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.864000 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.831000 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.815500 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.810000 | 1.663 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.801500 | 1.662 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 0.385 | Reg loss: 0.022 | Tree loss: 0.385 | Accuracy: 0.805461 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.767000 | 1.663 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.022 | Tree loss: 0.449 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.797500 | 1.663 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.821500 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.845500 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.022 | Tree loss: 0.417 | Accuracy: 0.821000 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.805500 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.807000 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.830000 | 1.662 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.819113 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 0.449 | Reg loss: 0.022 | Tree loss: 0.449 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.022 | Tree loss: 0.484 | Accuracy: 0.782500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.022 | Tree loss: 0.445 | Accuracy: 0.811000 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.822500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 0.398 | Reg loss: 0.022 | Tree loss: 0.398 | Accuracy: 0.843500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.844500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.815500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.797500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.821000 | 1.663 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.802048 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.786000 | 1.664 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.794500 | 1.664 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.827000 | 1.664 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.832500 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.853000 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.801000 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.804000 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.826000 | 1.663 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.798635 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.789500 | 1.663 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.781000 | 1.663 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.022 | Tree loss: 0.437 | Accuracy: 0.809000 | 1.663 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.814000 | 1.663 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.827500 | 1.663 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.843500 | 1.662 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.813000 | 1.662 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.793500 | 1.662 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.812000 | 1.662 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.811000 | 1.662 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 0.378 | Reg loss: 0.022 | Tree loss: 0.378 | Accuracy: 0.846416 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.789500 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.804500 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.022 | Tree loss: 0.435 | Accuracy: 0.798000 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.829500 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.857000 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.022 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.663 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.819500 | 1.662 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.810000 | 1.662 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 0.377 | Reg loss: 0.022 | Tree loss: 0.377 | Accuracy: 0.853242 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.792000 | 1.664 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.793000 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.795500 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.821000 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.842500 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.854000 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.823000 | 1.663 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.824000 | 1.662 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.808874 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 0.476 | Reg loss: 0.022 | Tree loss: 0.476 | Accuracy: 0.792500 | 1.663 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.022 | Tree loss: 0.475 | Accuracy: 0.786500 | 1.663 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.022 | Tree loss: 0.449 | Accuracy: 0.814000 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.814500 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.828500 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.853500 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.832000 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.794500 | 1.662 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 0.381 | Reg loss: 0.022 | Tree loss: 0.381 | Accuracy: 0.824500 | 1.661 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 0.378 | Reg loss: 0.022 | Tree loss: 0.378 | Accuracy: 0.836177 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.022 | Tree loss: 0.466 | Accuracy: 0.794500 | 1.663 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.022 | Tree loss: 0.467 | Accuracy: 0.788500 | 1.663 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.825000 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.794500 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.827000 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.852500 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.813000 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.822000 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.810500 | 1.662 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 0.382 | Reg loss: 0.022 | Tree loss: 0.382 | Accuracy: 0.829352 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.022 | Tree loss: 0.468 | Accuracy: 0.792500 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.790500 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.792500 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.809500 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.831000 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.854000 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.815000 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.821000 | 1.663 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.811500 | 1.662 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.805000 | 1.662 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.836177 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.022 | Tree loss: 0.465 | Accuracy: 0.796500 | 1.663 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.807500 | 1.663 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.812000 | 1.663 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.835500 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.022 | Tree loss: 0.406 | Accuracy: 0.849500 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.820500 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.817000 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.800000 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.813000 | 1.662 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 0.377 | Reg loss: 0.022 | Tree loss: 0.377 | Accuracy: 0.832765 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.806500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.022 | Tree loss: 0.479 | Accuracy: 0.795500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.803500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.816500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.833500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.843500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.022 | Tree loss: 0.414 | Accuracy: 0.818000 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.803500 | 1.662 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.809500 | 1.661 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.817000 | 1.661 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.843003 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.791000 | 1.663 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.794500 | 1.663 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 0.441 | Reg loss: 0.022 | Tree loss: 0.441 | Accuracy: 0.793500 | 1.663 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.832000 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.820500 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.864500 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.819000 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.022 | Tree loss: 0.426 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.815500 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.809500 | 1.662 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 0.379 | Reg loss: 0.022 | Tree loss: 0.379 | Accuracy: 0.829352 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.792000 | 1.663 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.777500 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 0.438 | Reg loss: 0.022 | Tree loss: 0.438 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.811000 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.827500 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.853500 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.022 | Tree loss: 0.408 | Accuracy: 0.821500 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.805500 | 1.662 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.797000 | 1.661 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.802048 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.022 | Tree loss: 0.474 | Accuracy: 0.795500 | 1.662 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 0.449 | Reg loss: 0.022 | Tree loss: 0.449 | Accuracy: 0.808000 | 1.662 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.811500 | 1.662 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.811000 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.829500 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 0.390 | Reg loss: 0.022 | Tree loss: 0.390 | Accuracy: 0.864000 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 0.397 | Reg loss: 0.022 | Tree loss: 0.397 | Accuracy: 0.822000 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.807500 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.802000 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 0.429 | Reg loss: 0.022 | Tree loss: 0.429 | Accuracy: 0.800500 | 1.661 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.798635 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 0.480 | Reg loss: 0.022 | Tree loss: 0.480 | Accuracy: 0.781000 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.803500 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.022 | Tree loss: 0.446 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.022 | Tree loss: 0.427 | Accuracy: 0.815500 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 0.401 | Reg loss: 0.022 | Tree loss: 0.401 | Accuracy: 0.838500 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.022 | Tree loss: 0.405 | Accuracy: 0.849500 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 0.400 | Reg loss: 0.022 | Tree loss: 0.400 | Accuracy: 0.828000 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.808000 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.819000 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.022 | Tree loss: 0.403 | Accuracy: 0.818500 | 1.662 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.022 | Tree loss: 0.412 | Accuracy: 0.798635 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.789000 | 1.663 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.795500 | 1.663 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.022 | Tree loss: 0.460 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.820500 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.828500 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.022 | Tree loss: 0.399 | Accuracy: 0.857000 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.801500 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.812500 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.798000 | 1.662 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.022 | Tree loss: 0.413 | Accuracy: 0.808000 | 1.661 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.022 | Tree loss: 0.386 | Accuracy: 0.853242 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.787500 | 1.662 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.022 | Tree loss: 0.457 | Accuracy: 0.791500 | 1.662 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.821000 | 1.662 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.022 | Tree loss: 0.431 | Accuracy: 0.800000 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.841000 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.022 | Tree loss: 0.396 | Accuracy: 0.863000 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.022 | Tree loss: 0.404 | Accuracy: 0.815500 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.820000 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.022 | Tree loss: 0.421 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.022 | Tree loss: 0.415 | Accuracy: 0.809000 | 1.661 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 0.389 | Reg loss: 0.022 | Tree loss: 0.389 | Accuracy: 0.822526 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 0.458 | Reg loss: 0.022 | Tree loss: 0.458 | Accuracy: 0.802500 | 1.662 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 0.454 | Reg loss: 0.022 | Tree loss: 0.454 | Accuracy: 0.804500 | 1.662 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.022 | Tree loss: 0.450 | Accuracy: 0.797500 | 1.662 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.022 | Tree loss: 0.428 | Accuracy: 0.802500 | 1.662 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.847000 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.022 | Tree loss: 0.410 | Accuracy: 0.850000 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.814500 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.022 | Tree loss: 0.420 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.800000 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.022 | Tree loss: 0.422 | Accuracy: 0.797500 | 1.661 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.022 | Tree loss: 0.395 | Accuracy: 0.802048 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 0.448 | Reg loss: 0.022 | Tree loss: 0.448 | Accuracy: 0.806000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.022 | Tree loss: 0.472 | Accuracy: 0.792000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.022 | Tree loss: 0.455 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.022 | Tree loss: 0.424 | Accuracy: 0.811000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.815000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 0.383 | Reg loss: 0.022 | Tree loss: 0.383 | Accuracy: 0.873000 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 0.416 | Reg loss: 0.022 | Tree loss: 0.416 | Accuracy: 0.803500 | 1.662 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 0.436 | Reg loss: 0.022 | Tree loss: 0.436 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.791000 | 1.661 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.812000 | 1.661 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.784983 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.022 | Tree loss: 0.463 | Accuracy: 0.793000 | 1.662 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.022 | Tree loss: 0.483 | Accuracy: 0.776000 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 0.439 | Reg loss: 0.022 | Tree loss: 0.439 | Accuracy: 0.816000 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.022 | Tree loss: 0.433 | Accuracy: 0.812500 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 0.411 | Reg loss: 0.022 | Tree loss: 0.411 | Accuracy: 0.835000 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.022 | Tree loss: 0.402 | Accuracy: 0.858500 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.022 | Tree loss: 0.407 | Accuracy: 0.824500 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.022 | Tree loss: 0.409 | Accuracy: 0.811500 | 1.661 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 0.442 | Reg loss: 0.023 | Tree loss: 0.442 | Accuracy: 0.798635 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.022 | Tree loss: 0.471 | Accuracy: 0.789000 | 1.662 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.784500 | 1.662 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 0.423 | Reg loss: 0.022 | Tree loss: 0.423 | Accuracy: 0.826000 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.022 | Tree loss: 0.432 | Accuracy: 0.819500 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.022 | Tree loss: 0.418 | Accuracy: 0.819500 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.022 | Tree loss: 0.394 | Accuracy: 0.857500 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.802000 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.808500 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.825939 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 0.456 | Reg loss: 0.022 | Tree loss: 0.456 | Accuracy: 0.801500 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.791500 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.022 | Tree loss: 0.442 | Accuracy: 0.820000 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.022 | Tree loss: 0.425 | Accuracy: 0.809000 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.841500 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.845500 | 1.662 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.803000 | 1.661 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.811000 | 1.661 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.795222 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 0.496 | Reg loss: 0.022 | Tree loss: 0.496 | Accuracy: 0.772500 | 1.662 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.022 | Tree loss: 0.449 | Accuracy: 0.805000 | 1.662 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.809500 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.826000 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.853500 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.826000 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.808500 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.799000 | 1.661 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.832765 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.022 | Tree loss: 0.464 | Accuracy: 0.789000 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.022 | Tree loss: 0.478 | Accuracy: 0.787500 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.805000 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.826000 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.830500 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.844000 | 1.661 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.818000 | 1.66 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.800000 | 1.66 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.819500 | 1.66 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.023 | Tree loss: 0.392 | Accuracy: 0.836177 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.022 | Tree loss: 0.461 | Accuracy: 0.797500 | 1.662 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.023 | Tree loss: 0.469 | Accuracy: 0.791000 | 1.662 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.801500 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.816000 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.831500 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.844500 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.819000 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.817000 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.800000 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.813000 | 1.661 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 0.479 | Reg loss: 0.023 | Tree loss: 0.479 | Accuracy: 0.812287 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.022 | Tree loss: 0.459 | Accuracy: 0.797000 | 1.662 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 0.474 | Reg loss: 0.023 | Tree loss: 0.474 | Accuracy: 0.789500 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.023 | Tree loss: 0.449 | Accuracy: 0.801000 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.820500 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.824000 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.855500 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.829500 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.836177 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.802500 | 1.661 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.023 | Tree loss: 0.479 | Accuracy: 0.775500 | 1.661 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.788500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.814500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.833500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.849500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.826500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.810500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 0.436 | Reg loss: 0.023 | Tree loss: 0.436 | Accuracy: 0.812287 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.790000 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.813000 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.023 | Tree loss: 0.445 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.813000 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 0.392 | Reg loss: 0.023 | Tree loss: 0.392 | Accuracy: 0.843500 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.832500 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.814000 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.790500 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 0.428 | Reg loss: 0.023 | Tree loss: 0.428 | Accuracy: 0.784983 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.796000 | 1.662 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 0.439 | Reg loss: 0.023 | Tree loss: 0.439 | Accuracy: 0.807000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.834000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.841000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.812000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.810000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.807000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.810000 | 1.661 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.784983 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.023 | Tree loss: 0.468 | Accuracy: 0.791500 | 1.661 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.023 | Tree loss: 0.463 | Accuracy: 0.782500 | 1.661 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.808500 | 1.661 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.831500 | 1.661 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.849000 | 1.66 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.816500 | 1.66 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 0.439 | Reg loss: 0.023 | Tree loss: 0.439 | Accuracy: 0.799000 | 1.66 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.810500 | 1.66 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.849829 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.793000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.789000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.803000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.811000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 0.390 | Reg loss: 0.023 | Tree loss: 0.390 | Accuracy: 0.847000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.849500 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.811000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.796500 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.790000 | 1.661 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.798000 | 1.66 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.853242 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.797500 | 1.662 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.023 | Tree loss: 0.464 | Accuracy: 0.781000 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 0.444 | Reg loss: 0.023 | Tree loss: 0.444 | Accuracy: 0.803000 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.821000 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.845000 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.826500 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.828500 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.798000 | 1.661 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.774744 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 0.468 | Reg loss: 0.023 | Tree loss: 0.468 | Accuracy: 0.787500 | 1.661 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.023 | Tree loss: 0.476 | Accuracy: 0.774000 | 1.661 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.793000 | 1.661 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.804500 | 1.661 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.838500 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.849500 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.778157 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.023 | Tree loss: 0.463 | Accuracy: 0.791500 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.803500 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.796000 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.819500 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.834000 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.854000 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.809000 | 1.661 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.797000 | 1.66 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.805000 | 1.66 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.836177 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.794000 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.804000 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.797000 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.814500 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.828500 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.851000 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.817000 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.846416 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 0.463 | Reg loss: 0.023 | Tree loss: 0.463 | Accuracy: 0.793000 | 1.661 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.023 | Tree loss: 0.468 | Accuracy: 0.786000 | 1.661 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 0.463 | Reg loss: 0.023 | Tree loss: 0.463 | Accuracy: 0.791000 | 1.661 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.023 | Tree loss: 0.428 | Accuracy: 0.805500 | 1.661 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.832500 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.851000 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.815000 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.814500 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.825939 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.783500 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.783000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.023 | Tree loss: 0.447 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.820000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.830000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.844000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.820000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.793500 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.798000 | 1.66 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.826000 | 1.659 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.825939 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 0.474 | Reg loss: 0.023 | Tree loss: 0.474 | Accuracy: 0.801000 | 1.661 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.791500 | 1.661 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.023 | Tree loss: 0.442 | Accuracy: 0.794000 | 1.661 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.792500 | 1.661 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.834500 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.852000 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.824000 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 0.432 | Reg loss: 0.023 | Tree loss: 0.432 | Accuracy: 0.797500 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.815000 | 1.66 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.832765 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.023 | Tree loss: 0.473 | Accuracy: 0.782500 | 1.661 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.783000 | 1.661 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.815000 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.820000 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.858500 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.825500 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.820000 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.805500 | 1.659 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 0.358 | Reg loss: 0.023 | Tree loss: 0.358 | Accuracy: 0.873720 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.796500 | 1.66 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 0.453 | Reg loss: 0.023 | Tree loss: 0.453 | Accuracy: 0.807000 | 1.66 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 0.453 | Reg loss: 0.023 | Tree loss: 0.453 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 0.393 | Reg loss: 0.023 | Tree loss: 0.393 | Accuracy: 0.835000 | 1.66 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.837500 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.850500 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.828500 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.807000 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.802500 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.802500 | 1.659 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.849829 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.804000 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.821000 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.828000 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.846500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.812500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.807000 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.812500 | 1.66 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.825939 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.803500 | 1.661 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.788000 | 1.661 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.809000 | 1.661 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 0.428 | Reg loss: 0.023 | Tree loss: 0.428 | Accuracy: 0.818000 | 1.661 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.834000 | 1.661 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.850500 | 1.66 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.823500 | 1.66 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.807500 | 1.66 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.810500 | 1.66 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.827500 | 1.66 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.798635 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.023 | Tree loss: 0.461 | Accuracy: 0.795000 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 0.473 | Reg loss: 0.023 | Tree loss: 0.473 | Accuracy: 0.785000 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.812000 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.832500 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.852000 | 1.66 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.823500 | 1.659 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.805000 | 1.659 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.790000 | 1.659 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.826500 | 1.659 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.802048 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 0.450 | Reg loss: 0.023 | Tree loss: 0.450 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 0.483 | Reg loss: 0.023 | Tree loss: 0.483 | Accuracy: 0.775500 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.781000 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.023 | Tree loss: 0.436 | Accuracy: 0.808000 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.847000 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.853500 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.795500 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.827000 | 1.659 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.815700 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.804500 | 1.661 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.792500 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.784500 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.822000 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.845500 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.827000 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.815500 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.797000 | 1.66 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 0.432 | Reg loss: 0.023 | Tree loss: 0.432 | Accuracy: 0.786500 | 1.659 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.808874 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.806000 | 1.66 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.791500 | 1.66 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 0.441 | Reg loss: 0.023 | Tree loss: 0.441 | Accuracy: 0.805500 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.832500 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.845000 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.816500 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.802000 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.659 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.802048 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.023 | Tree loss: 0.462 | Accuracy: 0.800000 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.782000 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.786500 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.825500 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 0.382 | Reg loss: 0.023 | Tree loss: 0.382 | Accuracy: 0.862500 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.808000 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.811500 | 1.661 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.817500 | 1.66 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.819113 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.786000 | 1.662 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 0.462 | Reg loss: 0.023 | Tree loss: 0.462 | Accuracy: 0.802500 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.807500 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.813500 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.842500 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.852000 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.812000 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.819000 | 1.66 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 0.391 | Reg loss: 0.023 | Tree loss: 0.391 | Accuracy: 0.832500 | 1.66 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.832765 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 0.444 | Reg loss: 0.023 | Tree loss: 0.444 | Accuracy: 0.808500 | 1.661 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 0.484 | Reg loss: 0.023 | Tree loss: 0.484 | Accuracy: 0.782000 | 1.661 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.788500 | 1.661 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.830500 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.839000 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.847500 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.821000 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.790500 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.815500 | 1.66 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.839590 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 0.478 | Reg loss: 0.023 | Tree loss: 0.478 | Accuracy: 0.790000 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.023 | Tree loss: 0.470 | Accuracy: 0.788500 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.828000 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.831500 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.842000 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.853000 | 1.661 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.811500 | 1.66 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.822526 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.797500 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 0.461 | Reg loss: 0.023 | Tree loss: 0.461 | Accuracy: 0.808000 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.795500 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.803500 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.837500 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.852500 | 1.661 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.805500 | 1.66 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.815700 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 0.481 | Reg loss: 0.023 | Tree loss: 0.481 | Accuracy: 0.786000 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.795500 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.822500 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.823000 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.856500 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.826500 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.807500 | 1.659 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.023 | Tree loss: 0.380 | Accuracy: 0.822526 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.797500 | 1.661 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 0.492 | Reg loss: 0.023 | Tree loss: 0.492 | Accuracy: 0.779000 | 1.661 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 0.450 | Reg loss: 0.023 | Tree loss: 0.450 | Accuracy: 0.810500 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.814500 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.820000 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.853000 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.815500 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.806000 | 1.66 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.795222 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.793000 | 1.661 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.805000 | 1.661 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.783500 | 1.661 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 0.442 | Reg loss: 0.023 | Tree loss: 0.442 | Accuracy: 0.796000 | 1.661 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.818000 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.860000 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.823500 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.808500 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.806000 | 1.66 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.825939 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.779000 | 1.661 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.813500 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.856500 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.835500 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.824000 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.824000 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.797000 | 1.66 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.812000 | 1.659 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 0.390 | Reg loss: 0.023 | Tree loss: 0.390 | Accuracy: 0.825939 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.797500 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.797500 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.818500 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.837500 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 0.385 | Reg loss: 0.023 | Tree loss: 0.385 | Accuracy: 0.851500 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.816000 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.816000 | 1.66 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.808000 | 1.659 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.816000 | 1.659 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.836177 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.797000 | 1.661 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.798500 | 1.661 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.800000 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.827000 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.842000 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.820500 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.823500 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.810000 | 1.66 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.829352 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.798000 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.814500 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.815000 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.831500 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.848000 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.824500 | 1.659 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.820000 | 1.659 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.815700 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.793500 | 1.66 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.781000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.801500 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.826000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.823000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.851000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.802500 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.813000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.801000 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.820500 | 1.659 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.798635 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.794000 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.023 | Tree loss: 0.478 | Accuracy: 0.785500 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.796500 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.812500 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.841000 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.852000 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.833000 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.811500 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.800500 | 1.66 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.815700 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.788500 | 1.661 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.796500 | 1.661 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.806000 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.797500 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.837000 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.842000 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.812500 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.801000 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.819500 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.812287 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.802500 | 1.66 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 0.485 | Reg loss: 0.023 | Tree loss: 0.485 | Accuracy: 0.792500 | 1.66 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.821000 | 1.66 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.826000 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.847500 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.822000 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.808500 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.819000 | 1.659 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.798635 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.800500 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 0.470 | Reg loss: 0.023 | Tree loss: 0.470 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.813000 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.811000 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.839500 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.849000 | 1.66 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.820000 | 1.659 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.804500 | 1.659 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.799500 | 1.659 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 0.356 | Reg loss: 0.023 | Tree loss: 0.356 | Accuracy: 0.863481 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.023 | Tree loss: 0.470 | Accuracy: 0.798500 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 0.447 | Reg loss: 0.023 | Tree loss: 0.447 | Accuracy: 0.815500 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.807000 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.023 | Tree loss: 0.436 | Accuracy: 0.806500 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 0.376 | Reg loss: 0.023 | Tree loss: 0.376 | Accuracy: 0.850000 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.854000 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.835000 | 1.66 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 0.441 | Reg loss: 0.023 | Tree loss: 0.441 | Accuracy: 0.796000 | 1.659 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.822500 | 1.659 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.803000 | 1.659 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.788396 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.807500 | 1.66 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.806000 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 0.444 | Reg loss: 0.023 | Tree loss: 0.444 | Accuracy: 0.793500 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.811500 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.824500 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.858000 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.813000 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.810500 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.797000 | 1.659 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.805500 | 1.658 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.849829 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.808000 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 0.488 | Reg loss: 0.023 | Tree loss: 0.488 | Accuracy: 0.776000 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 0.439 | Reg loss: 0.023 | Tree loss: 0.439 | Accuracy: 0.809000 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.804500 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.821500 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 0.384 | Reg loss: 0.023 | Tree loss: 0.384 | Accuracy: 0.859500 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.818000 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 0.432 | Reg loss: 0.023 | Tree loss: 0.432 | Accuracy: 0.785000 | 1.66 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.804000 | 1.659 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.805461 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 0.464 | Reg loss: 0.023 | Tree loss: 0.464 | Accuracy: 0.794500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.023 | Tree loss: 0.469 | Accuracy: 0.777500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.023 | Tree loss: 0.449 | Accuracy: 0.798500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.811000 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.835500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.842500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.812000 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.811000 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.826500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.800500 | 1.66 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 0.386 | Reg loss: 0.023 | Tree loss: 0.386 | Accuracy: 0.846416 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 0.454 | Reg loss: 0.023 | Tree loss: 0.454 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.023 | Tree loss: 0.468 | Accuracy: 0.798000 | 1.66 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.819500 | 1.66 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 0.432 | Reg loss: 0.023 | Tree loss: 0.432 | Accuracy: 0.799000 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.830500 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.842500 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.825000 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.811000 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.806500 | 1.659 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 0.371 | Reg loss: 0.023 | Tree loss: 0.371 | Accuracy: 0.843003 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.804500 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 0.464 | Reg loss: 0.023 | Tree loss: 0.464 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.023 | Tree loss: 0.449 | Accuracy: 0.792500 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.823000 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.835000 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.848500 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.810000 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.812000 | 1.659 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.796500 | 1.658 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.817500 | 1.658 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.825939 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 0.447 | Reg loss: 0.023 | Tree loss: 0.447 | Accuracy: 0.820500 | 1.66 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.807000 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 0.470 | Reg loss: 0.023 | Tree loss: 0.470 | Accuracy: 0.782500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.815500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.830500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.863000 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.798500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.798500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.796500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.800500 | 1.659 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.808874 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.023 | Tree loss: 0.462 | Accuracy: 0.795000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 0.480 | Reg loss: 0.023 | Tree loss: 0.480 | Accuracy: 0.789500 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.799000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.812000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.824000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.849000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.824000 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.815500 | 1.659 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.796000 | 1.658 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.811000 | 1.658 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 0.374 | Reg loss: 0.023 | Tree loss: 0.374 | Accuracy: 0.843003 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.799000 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 0.475 | Reg loss: 0.023 | Tree loss: 0.475 | Accuracy: 0.791500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 0.447 | Reg loss: 0.023 | Tree loss: 0.447 | Accuracy: 0.814500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.800000 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.835500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.839500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.802500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.810000 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.820500 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.809000 | 1.659 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 0.363 | Reg loss: 0.023 | Tree loss: 0.363 | Accuracy: 0.853242 | 1.659 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.790500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.796000 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 0.448 | Reg loss: 0.023 | Tree loss: 0.448 | Accuracy: 0.809500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.825000 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.853500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.818000 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.803000 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.792500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.803500 | 1.66 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.795222 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 0.472 | Reg loss: 0.023 | Tree loss: 0.472 | Accuracy: 0.796500 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 0.479 | Reg loss: 0.023 | Tree loss: 0.479 | Accuracy: 0.788000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.802000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.823000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.824000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.857000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.825000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.815500 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.790000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.793000 | 1.66 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 0.445 | Reg loss: 0.023 | Tree loss: 0.445 | Accuracy: 0.781570 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.800000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 0.468 | Reg loss: 0.023 | Tree loss: 0.468 | Accuracy: 0.791500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.798500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.807000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.827500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 0.392 | Reg loss: 0.023 | Tree loss: 0.392 | Accuracy: 0.854500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.814000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.821000 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.799500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.787500 | 1.66 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.808874 | 1.66 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.809000 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.023 | Tree loss: 0.493 | Accuracy: 0.772000 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.801000 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 0.436 | Reg loss: 0.023 | Tree loss: 0.436 | Accuracy: 0.806500 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.825500 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.840500 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.823000 | 1.661 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.802500 | 1.662 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.802000 | 1.662 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.807000 | 1.662 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.791809 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 0.454 | Reg loss: 0.023 | Tree loss: 0.454 | Accuracy: 0.798500 | 1.662 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 0.454 | Reg loss: 0.023 | Tree loss: 0.454 | Accuracy: 0.805500 | 1.662 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.813000 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.827000 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.821000 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.851000 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.816500 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.798500 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.813500 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.811500 | 1.663 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.815700 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.789500 | 1.665 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.023 | Tree loss: 0.476 | Accuracy: 0.791000 | 1.665 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.795000 | 1.665 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.811000 | 1.665 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.823000 | 1.665 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.854000 | 1.664 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.825500 | 1.664 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.813500 | 1.664 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.818500 | 1.664 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.819500 | 1.664 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.798635 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 0.450 | Reg loss: 0.023 | Tree loss: 0.450 | Accuracy: 0.820500 | 1.665 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 0.493 | Reg loss: 0.023 | Tree loss: 0.493 | Accuracy: 0.776000 | 1.665 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 0.475 | Reg loss: 0.023 | Tree loss: 0.475 | Accuracy: 0.795000 | 1.665 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.807500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.825500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.840500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.822500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.818500 | 1.664 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 0.393 | Reg loss: 0.023 | Tree loss: 0.393 | Accuracy: 0.805461 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.023 | Tree loss: 0.462 | Accuracy: 0.799000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.787000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 0.442 | Reg loss: 0.023 | Tree loss: 0.442 | Accuracy: 0.794000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.841000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.872500 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.809000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.805000 | 1.664 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 0.363 | Reg loss: 0.023 | Tree loss: 0.363 | Accuracy: 0.843003 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.804500 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.801000 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.797000 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.813000 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.836500 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.845000 | 1.665 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.821000 | 1.664 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.816000 | 1.664 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.785500 | 1.664 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.836177 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.023 | Tree loss: 0.473 | Accuracy: 0.792000 | 1.665 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.785000 | 1.665 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 0.445 | Reg loss: 0.023 | Tree loss: 0.445 | Accuracy: 0.807500 | 1.665 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.828000 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.851000 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.826500 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 0.428 | Reg loss: 0.023 | Tree loss: 0.428 | Accuracy: 0.801500 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.827000 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.809000 | 1.664 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 0.377 | Reg loss: 0.023 | Tree loss: 0.377 | Accuracy: 0.843003 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 0.470 | Reg loss: 0.023 | Tree loss: 0.470 | Accuracy: 0.795500 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.794500 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.810000 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.833500 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.835000 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.851500 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.800000 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.824000 | 1.664 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.794500 | 1.663 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.827500 | 1.663 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 0.399 | Reg loss: 0.023 | Tree loss: 0.399 | Accuracy: 0.805461 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.799000 | 1.665 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.785000 | 1.665 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.809500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.815500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.822000 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.846500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.821500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.809500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 0.436 | Reg loss: 0.023 | Tree loss: 0.436 | Accuracy: 0.797000 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.800500 | 1.664 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.836177 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.799500 | 1.665 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.797000 | 1.665 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.800000 | 1.665 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.805500 | 1.665 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.844000 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.851000 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.805000 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.798500 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.795000 | 1.664 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 0.393 | Reg loss: 0.023 | Tree loss: 0.393 | Accuracy: 0.822526 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.808500 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 0.471 | Reg loss: 0.023 | Tree loss: 0.471 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 0.454 | Reg loss: 0.023 | Tree loss: 0.454 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.804500 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.818500 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.837000 | 1.664 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.832500 | 1.663 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.805500 | 1.663 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.808000 | 1.663 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 0.380 | Reg loss: 0.023 | Tree loss: 0.380 | Accuracy: 0.836177 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 0.455 | Reg loss: 0.023 | Tree loss: 0.455 | Accuracy: 0.802000 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.023 | Tree loss: 0.478 | Accuracy: 0.791500 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 0.444 | Reg loss: 0.023 | Tree loss: 0.444 | Accuracy: 0.814500 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.809000 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.820000 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.851000 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.821000 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.811500 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.792500 | 1.664 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.815000 | 1.663 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 0.385 | Reg loss: 0.023 | Tree loss: 0.385 | Accuracy: 0.812287 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 0.461 | Reg loss: 0.023 | Tree loss: 0.461 | Accuracy: 0.794000 | 1.665 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.799500 | 1.665 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 0.456 | Reg loss: 0.023 | Tree loss: 0.456 | Accuracy: 0.795500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.825500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.840500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.853000 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.815500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.802500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.812500 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.807000 | 1.664 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.829352 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.796500 | 1.664 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 0.476 | Reg loss: 0.023 | Tree loss: 0.476 | Accuracy: 0.779500 | 1.664 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.814500 | 1.664 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.797500 | 1.664 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.829000 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 0.390 | Reg loss: 0.023 | Tree loss: 0.390 | Accuracy: 0.856500 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.814000 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.825000 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 0.390 | Reg loss: 0.023 | Tree loss: 0.390 | Accuracy: 0.834500 | 1.663 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.822526 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.795500 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 0.447 | Reg loss: 0.023 | Tree loss: 0.447 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 0.429 | Reg loss: 0.023 | Tree loss: 0.429 | Accuracy: 0.814000 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.818500 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.823000 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.848500 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.818000 | 1.664 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.803000 | 1.663 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.810500 | 1.663 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.817500 | 1.663 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 0.373 | Reg loss: 0.023 | Tree loss: 0.373 | Accuracy: 0.839590 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.798500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 0.463 | Reg loss: 0.023 | Tree loss: 0.463 | Accuracy: 0.799500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.802500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 0.438 | Reg loss: 0.023 | Tree loss: 0.438 | Accuracy: 0.806000 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.837500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 0.394 | Reg loss: 0.023 | Tree loss: 0.394 | Accuracy: 0.847500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 0.397 | Reg loss: 0.023 | Tree loss: 0.397 | Accuracy: 0.824500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.804000 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.813500 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.810000 | 1.664 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 0.373 | Reg loss: 0.023 | Tree loss: 0.373 | Accuracy: 0.863481 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 0.462 | Reg loss: 0.023 | Tree loss: 0.462 | Accuracy: 0.790500 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 0.478 | Reg loss: 0.023 | Tree loss: 0.478 | Accuracy: 0.793500 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.809000 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.820500 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.840500 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.847500 | 1.664 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.825000 | 1.663 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.808500 | 1.663 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.810000 | 1.663 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 0.390 | Reg loss: 0.023 | Tree loss: 0.390 | Accuracy: 0.830500 | 1.663 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.836177 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.790000 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.798500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 0.437 | Reg loss: 0.023 | Tree loss: 0.437 | Accuracy: 0.801500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.817000 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 0.387 | Reg loss: 0.023 | Tree loss: 0.387 | Accuracy: 0.844000 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.823500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.820500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.806500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 0.417 | Reg loss: 0.023 | Tree loss: 0.417 | Accuracy: 0.807500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.833500 | 1.663 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.815700 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.804500 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 0.460 | Reg loss: 0.023 | Tree loss: 0.460 | Accuracy: 0.794500 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.793500 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 0.425 | Reg loss: 0.023 | Tree loss: 0.425 | Accuracy: 0.809500 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.826000 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 0.398 | Reg loss: 0.023 | Tree loss: 0.398 | Accuracy: 0.842500 | 1.664 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 0.410 | Reg loss: 0.023 | Tree loss: 0.410 | Accuracy: 0.825000 | 1.663 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 0.414 | Reg loss: 0.023 | Tree loss: 0.414 | Accuracy: 0.818500 | 1.663 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.812500 | 1.663 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 0.431 | Reg loss: 0.023 | Tree loss: 0.431 | Accuracy: 0.791809 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 0.465 | Reg loss: 0.023 | Tree loss: 0.465 | Accuracy: 0.790000 | 1.664 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.802500 | 1.664 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 0.464 | Reg loss: 0.023 | Tree loss: 0.464 | Accuracy: 0.806500 | 1.664 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.815000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 0.402 | Reg loss: 0.023 | Tree loss: 0.402 | Accuracy: 0.826000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.845000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.818000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 0.430 | Reg loss: 0.023 | Tree loss: 0.430 | Accuracy: 0.802500 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.805000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 0.416 | Reg loss: 0.023 | Tree loss: 0.416 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.802048 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 0.449 | Reg loss: 0.023 | Tree loss: 0.449 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 0.469 | Reg loss: 0.023 | Tree loss: 0.469 | Accuracy: 0.782000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 0.449 | Reg loss: 0.023 | Tree loss: 0.449 | Accuracy: 0.810500 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 0.421 | Reg loss: 0.023 | Tree loss: 0.421 | Accuracy: 0.823500 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.830000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.844000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.826000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 0.423 | Reg loss: 0.023 | Tree loss: 0.423 | Accuracy: 0.799000 | 1.663 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 0.435 | Reg loss: 0.023 | Tree loss: 0.435 | Accuracy: 0.799000 | 1.662 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.804000 | 1.662 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 0.392 | Reg loss: 0.023 | Tree loss: 0.392 | Accuracy: 0.829352 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 0.477 | Reg loss: 0.023 | Tree loss: 0.477 | Accuracy: 0.792000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.804000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.806500 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.830500 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.852000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.827000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 0.443 | Reg loss: 0.023 | Tree loss: 0.443 | Accuracy: 0.786000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.816500 | 1.663 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.819113 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 0.458 | Reg loss: 0.023 | Tree loss: 0.458 | Accuracy: 0.803000 | 1.664 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.803500 | 1.664 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 0.466 | Reg loss: 0.023 | Tree loss: 0.466 | Accuracy: 0.792000 | 1.664 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 0.434 | Reg loss: 0.023 | Tree loss: 0.434 | Accuracy: 0.812000 | 1.664 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.817000 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 0.387 | Reg loss: 0.023 | Tree loss: 0.387 | Accuracy: 0.858000 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.814500 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.805000 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 0.409 | Reg loss: 0.023 | Tree loss: 0.409 | Accuracy: 0.807000 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.819000 | 1.663 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 0.384 | Reg loss: 0.023 | Tree loss: 0.384 | Accuracy: 0.815700 | 1.663 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.799500 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 0.459 | Reg loss: 0.023 | Tree loss: 0.459 | Accuracy: 0.804000 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 0.433 | Reg loss: 0.023 | Tree loss: 0.433 | Accuracy: 0.813000 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.813500 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.828000 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.855000 | 1.663 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 0.411 | Reg loss: 0.023 | Tree loss: 0.411 | Accuracy: 0.820000 | 1.662 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 0.427 | Reg loss: 0.023 | Tree loss: 0.427 | Accuracy: 0.803000 | 1.662 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.800500 | 1.662 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 0.406 | Reg loss: 0.023 | Tree loss: 0.406 | Accuracy: 0.818000 | 1.662 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 0.388 | Reg loss: 0.023 | Tree loss: 0.388 | Accuracy: 0.825939 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.795000 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 0.451 | Reg loss: 0.023 | Tree loss: 0.451 | Accuracy: 0.798500 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 0.452 | Reg loss: 0.023 | Tree loss: 0.452 | Accuracy: 0.800000 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 0.422 | Reg loss: 0.023 | Tree loss: 0.422 | Accuracy: 0.809500 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 0.420 | Reg loss: 0.023 | Tree loss: 0.420 | Accuracy: 0.834500 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.839500 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 0.407 | Reg loss: 0.023 | Tree loss: 0.407 | Accuracy: 0.815000 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.822000 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 0.400 | Reg loss: 0.023 | Tree loss: 0.400 | Accuracy: 0.819500 | 1.663 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 0.404 | Reg loss: 0.023 | Tree loss: 0.404 | Accuracy: 0.818000 | 1.662 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 0.375 | Reg loss: 0.023 | Tree loss: 0.375 | Accuracy: 0.836177 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 0.467 | Reg loss: 0.023 | Tree loss: 0.467 | Accuracy: 0.793000 | 1.664 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 0.457 | Reg loss: 0.023 | Tree loss: 0.457 | Accuracy: 0.805000 | 1.664 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 0.440 | Reg loss: 0.023 | Tree loss: 0.440 | Accuracy: 0.813000 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 0.418 | Reg loss: 0.023 | Tree loss: 0.418 | Accuracy: 0.808500 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 0.401 | Reg loss: 0.023 | Tree loss: 0.401 | Accuracy: 0.836500 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 0.403 | Reg loss: 0.023 | Tree loss: 0.403 | Accuracy: 0.845500 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.810500 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 0.424 | Reg loss: 0.023 | Tree loss: 0.424 | Accuracy: 0.801500 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 0.413 | Reg loss: 0.023 | Tree loss: 0.413 | Accuracy: 0.811000 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 0.405 | Reg loss: 0.023 | Tree loss: 0.405 | Accuracy: 0.817000 | 1.663 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.839590 | 1.662 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 0.473 | Reg loss: 0.023 | Tree loss: 0.473 | Accuracy: 0.790000 | 1.663 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 0.446 | Reg loss: 0.023 | Tree loss: 0.446 | Accuracy: 0.802000 | 1.663 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 0.461 | Reg loss: 0.023 | Tree loss: 0.461 | Accuracy: 0.792500 | 1.663 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 0.419 | Reg loss: 0.023 | Tree loss: 0.419 | Accuracy: 0.812500 | 1.663 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 0.393 | Reg loss: 0.023 | Tree loss: 0.393 | Accuracy: 0.847500 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 0.395 | Reg loss: 0.023 | Tree loss: 0.395 | Accuracy: 0.856500 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 0.408 | Reg loss: 0.023 | Tree loss: 0.408 | Accuracy: 0.811000 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 0.426 | Reg loss: 0.023 | Tree loss: 0.426 | Accuracy: 0.808500 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 0.415 | Reg loss: 0.023 | Tree loss: 0.415 | Accuracy: 0.813500 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 0.412 | Reg loss: 0.023 | Tree loss: 0.412 | Accuracy: 0.808000 | 1.662 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 0.396 | Reg loss: 0.023 | Tree loss: 0.396 | Accuracy: 0.808874 | 1.662 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 6.4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7IAAAJ9CAYAAAAISU4hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAxOAAAMTgF/d4wjAADY+UlEQVR4nOzdd1hT5/sG8DuDKSg4QGQJLlQEUXFj3daqVZyhaF1F695aW9s6OhxV66hWrQNHiRPrrKMu3IoIOHChgAtQAUFW1u+P/sy3FLAg4yTh/lyXVyU5Se7Yh3Py5D3nfUUajUYDIiIiIiIiIj0hFjoAERERERERUWGwkSUiIiIiIiK9wkaWiIiIiIiI9AobWSIiIiIiItIrbGSJiIiIiIhIr7CRJSIiIiIiIr3CRpaIiIiIiIj0ChtZIiKiIpo9ezZat25dpOdo27YtZs2aVaqvSUREpK/YyBIRUYnatGkTJBIJ5s6dK3QUnbZnzx588cUXxfZ8SqUSIpEIp06deq/HL1++HLVq1YKJiQmqV6+OrVu3Fuhxe/fuhUgkwsCBA9/rdYmIiAqCjSwREZWozZs3Y+LEidi8eXOJvo5CoYBGoynR1yhJFStWhIWFhdAxAADz5s3D4sWLsWDBAkRFRWH79u2oVavWfz4uISEBkyZN4kgxERGVODayRERUYmJjY3H16lXMmzcPGo0GZ8+eBQC8ePECRkZGuHz5co7tx44dix49emh/lsvlqFevHszMzODu7o5du3Zp7zt16hREIhH+/PNP1K9fH2ZmZnj58iX27duH5s2bw9LSEtWqVcPo0aPx5s0b7eM0Gg2mT58OKysrVKlSBYsWLULr1q0xe/Zs7TaJiYnw9/eHlZUVKleuDH9/f7x8+fI/3+/KlSthZ2eHypUrY/r06Tka6/96zn+fWhwWFoZGjRrB1NQUrVu3xrp16yASiQr8mjVr1gQAtGvXDiKRCEOGDPnP/MDf/2++//57bN68Gb1794aLiwuaNWuGZs2a/edjAwICMHnyZNSoUaNAr0VERPS+2MgSEVGJ2bx5M3r06AFzc3MMGDAAgYGBAIDKlSujffv22L59u3ZbtVqN3bt3Y8CAAQCAEydOYNy4cZgzZw5u3ryJL7/8Ep9++ikuXryY4zXmzJmDdevWITIyEuXLl0dmZia++uorhIeHQy6X4+TJk5gzZ452+/Xr12PNmjVYv349Tp8+jcuXLyMiIiLHc/bt2xcAEBISglOnTiE5Ofk/T5WNiIjAlStXcOLECfz222/4+eefceDAgfd6TqVSid69e6NWrVq4du0aJk2alKPRLshrvv132r17N549e4Zly5YB+Pva2urVq+f7Po4fPw6RSIT79++jZs2acHV1xYQJE5Cenv7O979hwwa8fv0aY8eOfed2RERExUJDRERUQmrVqqXZt2+fRqPRaMLDwzXly5fXpKenazQajWb9+vUaBwcHjVqt1mg0Gs3Jkyc1pqammtevX2s0Go2mXbt2mhUrVuR4voCAAM3w4cO12wPQnDp16p0ZgoKCNC4uLtqfmzRpovnqq6+0PyclJWnMzMw03377rUaj0WhOnz6tsbW11SgUCu02T5480QDQxMXF5fka3377rcba2lqTkZGhva1z586aKVOmFPg5P/jgA22uAwcOaMzNzTUpKSna7WfOnKn552H7v15ToVBoAGhOnjyZI+uKFSs07du3z/ff68cff9QYGRlpPDw8NKdOndIcO3ZMU7NmTU1AQEC+j3n48KHGzs5OEx0drdFoNJrBgwdr/P39892eiIioqDgiS0REJeL8+fNITExEly5dAAAeHh5wcHDAH3/8AQDo3bs3EhIScP78eQDAjh078NFHH8HS0hIAEBkZiWnTpsHCwkL7Z9OmTYiOjs7xOl5eXjl+vnXrFnx9feHk5ARLS0sMHToUcXFx2vvv3buHxo0ba3+2srLSnob79nUTExNhZWWlfd3atWsDQK7X/qdatWrB1NRU+3PVqlWRkJDwXs9579491KxZE+XLl9fe1qRJk0K9Zn7Gjh2Lv/76K9/71Wo1FAoFli9fjg8++AAdO3bETz/9hE2bNkGlUuX5mMGDB+Obb76Bi4vLO1+biIiouEiFDkBERIZp8+bNSE5Ohrm5ufY2tVqNwMBAyGQyWFlZoXPnzti+fTuaN2+O3bt3Y+XKldpt09LS8NNPP2kb4bfMzMxy/PzP5weAjz/+GB4eHti2bRtsbGxw5swZjBgxIsc2eV1r+s/XrVmzJg4ePJjrPnt7+3wfZ2RklOs13jZ+hX1OjUbzzowFec33ZWtrCwCoU6eO9rY6depAoVAgPj4e1apVy/WYM2fO4Ny5c9rTitVqNYC/r3F+8+YNTExMipSJiIjo39jIEhFRscvKysL27duxadOmHKOfCQkJ6Ny5M549ewY7OzvIZDJMnToVPXr0wJs3b9CtWzfttp6enoiOjs4xWvpfXrx4gQcPHmDXrl1o2LAhgL9Hev+pVq1aCA0NRa9evQAAKSkpuH//fo7XjY2NRfny5WFjY/Me7z63wj5n7dq1ce/ePbx+/Vo7KhsaGlqo15RIJBCLxYVubJs3bw4AuH//PqpWrar9u7GxsbbJ/bfIyMgcP8+aNQsqlQo//vgjjI2NC/X6REREBcFTi4mIqNi9PX34k08+gbu7u/ZP+/btUbduXe2apD179kRKSgomT56Mjz/+OMfo6pdffolffvkFS5cuxd27dxEeHo6VK1fmmCDq36ytrWFtbY1169YhOjoa27dvx5o1a3JsM3LkSKxYsQLBwcG4ffs2RowYAalUqh0B7dy5Mxo0aIDevXsjJCQE0dHROHbsWK5R3cIo7HN26dIFNjY2GDlyJG7fvo09e/ZoJ8oqKJFIBEdHR5w4cQIJCQlIS0sD8Pcsxx06dMj3cfXr10enTp0wceJEhIaG4tKlS5gxYwaGDx8OiUSS53P88/+xu7s7rKysUKFCBbi7uxdoZJmIiKiw2MgSEVGxCwwMRLdu3XKd+gr83by+bcosLCzw0Ucf4caNG9rZit/6+OOPERQUhC1btqBBgwbo2LEjDhw4AGdn53xfVyKRYNu2bTh69Cjq16+PNWvWYO7cuTm2GTZsGAICAjBkyBC0adMG3t7eqFWrlvb0V7FYjD///BN16tRB7969Ub9+fYwbNw5WVlbv/e9R2OeUSqXYvXs3oqKi0LBhQyxZsgQzZswo9Cm6CxcuxLZt22BnZ6c97fftqPW7/P7776hRowY++OAD+Pr6omPHjli8eLH2/oI8BxERUUkSaTR6vHo8ERFREb158wbVqlXDb7/9hn79+gkdJ1/fffcdgoKCcPPmTaGjEBERCY7XyBIRUZmSkpKCLVu2oFOnTsjMzMR3330HY2NjfPjhh0JHy2HXrl2oXLkynJ2dcenSJSxevBjTp08XOhYREZFOYCNLRERlikgkws6dO/HVV18B+HtZm5MnT2qX/dEVSUlJmDZtGp49ewYHBwdMnjyZjSwREdH/46nFREREREREpFc42RMRERERERHpFTayREREREREpFfYyBIREREREZFeYSNLREREREREeoWNLBEREREREekVNrJERERERESkV9jIEhERERERkV5hI0tERERERER6hY0sERERERER6RU2skRERERERKRX2MgSERERERGRXmEjS0RERERERHqFjSwRERERERHpFTayREREREREpFfYyBIREREREZFeYSNLREREREREeoWNLBEREREREekVNrJERERERESkV9jIEhERERERkV5hI0tERERERER6hY0sERERERER6RU2skRERERERKRX2MgSERERERGRXmEjS0RERERERHqFjSwRERERERHpFTayREREREREpFfYyBIREREREZFeYSNLREREREREeoWNLBEREREREekVNrJERERERESkV9jIEhERERERkV5hI0tERERERER6hY0sERERERER6RU2skRERERERKRX2MgSERERERGRXmEjS0RERERERHqFjSwRERERERHpFTayREREREREpFfYyBIREREREZFeYSNLREREREREeoWNLBEREREREekVNrJERERERESkV9jIEhERERERkV5hI0tERERERER6RSp0ACIiKn3RiWkIDnuCuKR0pGYqYWkqhaO1OXy97OFaxULoeERERETvJNJoNBqhQxARUclTqTU4fjse60KiERabDLEYUKj+dwgwkoigVgNeTlYI8HFFx7q2kIhFAiYmIiIiyhsbWSKiMuB1pgLDN11BxJMUZCnV/7m9iVQMD4cK2DDYG5amRqWQkIiIiKjg2MgSERm415kK+K46h7hX6chWFXyXbywRwbGiOYJHt0J5NrNERESkQzjZExGRAVOpNRi+6Uqhm1gAyFZpEPcqHcMDr0Cl5neeREREpDs42RMRkQE7fjseEU9ScjWxySHbkHIuKMdtZrWaw6bPrBy3Zas0iHicgr+i4tG5XtUSz0tERERUEGxkiYgM2LqQ6HyviTW2qw2bPl9rfxZJ8z59OFupxrqQaDayREREpDPYyBIRGajoxDSExSbne79IIoXEwvo/n0cD4FpMMh6+eAOXyuWKLyARERHRe+I1skREBio47AnE79jLZyc8RNyKgXiyZgReHl0NVWZavtuKxUBw2OMSSElERERUeByRJSIyUHFJ6TnWif0nE3s3VLaZBKl1NShT4pF8OhCJu+bB1n8+RKLca8cqVBrEJWWUdGQiIiKiAmEjS0RkoFIzlfneZ+baWPt3Y5vqMKrshKdrApD9/D5M7Grl+ZjXGYpiz0hERET0PnhqMRGRgbI0Lfh3lUbWdhCblIMyJT7fbcqbcS1ZIiIi0g1sZImIDJSjtTmMJLlPE86LMiUB6qw3kFawyfN+I4kIjtZmxRmPiIiI6L2xkSUiMlC+XvZQ573yDpJObkBm3E0ok+ORGROBxOAfYGLvBuOqNfPcXqXWwNfLoQTTEhERERUcr5ElIjJQrlUs4OVkhasxSbnuU6Yk4sXe+VBlpEJiURFmro1g1WYQRKLc32+KADR2tubSO0RERKQz2MgSERmwnnUscO3RC6hFkhy3V+k1o8DPIdKo4Fu3fHFHIyIiInpvPLWYiMgAaTQabNiwAWN6+aCCKgXGBbxW9t+MJSKUVyZhdE8fbNmyBRpN3sv5EBEREZUmNrJERAYmPj4evXr1wqxZs7BdLsfpeTI4VjQvdDNrLBHDsaI5Qr77BFs2b8bUqVPRr18/vHjxooSSExERERUMG1kiIgMSHBwMd3d3mJqaIjIyEl27dkV5UyMEj24FT0crmEjF+K92VgTARCpGQ8cK2Du6FSxNjdCjRw/cuHEDarUa7u7uOHDgQGm8HSIiIqI8iTQ8T4yISO+lpKRgwoQJ+OOPP7Bq1Sr4+fnl2kal1uCvqHisPRONsNhkiMWAQvW/Q4CRRAS1GmjkbIUAH1d0cLOFRJyz7dVoNNiyZQvGjRuH/v37Y8mSJbC0tCzx90dERET0T2xkiYj03IkTJzBkyBDUrVsXGzZsgL29/X8+JjoxDXuvP0FcUgZeZyhQ3swIjtZm8PVyKNDsxLGxsRg6dCgePnyIwMBA+Pj4FMdbISIiIioQNrJERHoqIyMDX375JdatW4eFCxdi1KhREIneb1Kn96FWq7FixQp8+eWXGDNmDObNmwcTE5NSe30iIiIqu9jIEhHpodDQUAwaNAjly5fH5s2bUbt2bcGy3L59G4MGDUJWVha2bt0KT09PwbIQERFR2cDJnoiI9IhCocDcuXPh4+MDf39/nD17VtAmFgDq1q2LCxcuoE+fPmjZsiXmz58PlUolaCYiIiIybByRJSLSE3fu3MGgQYPw5s0bbNmyBY0aNRI6Ui6XL1/GoEGDULlyZQQGBqJmzZpCRyIiIiIDxBFZIiIdp1arsXLlSjRu3Bg+Pj4IDQ3VySYWAJo2bYqwsDA0adIEXl5eWLNmDfh9KRERERU3jsgSEemwuLg4DBs2DPfu3cOmTZvQtm1boSMV2PHjxzF06FA0aNAA69evh52dndCRiIiIyEBwRJaISAdpNBps27YNHh4ecHR0REREhF41sQDQsWNHREZGolKlSnB3d8eOHTuEjkREREQGgiOyREQ65sWLFxg1ahROnz6NdevWoWfPnkJHKrJdu3bh888/R5cuXbBy5UpYW1sLHYmIiIj0GEdkiYh0yMGDB9GgQQMolUrcuHHDIJpYAOjbty8iIyORkpKCBg0a4NixY0JHIiIiIj3GRpaISAekpaVh5MiR+OSTTzB//nzs2bMHNjY2QscqVnZ2dti/fz++/fZb9O7dG2PHjsWbN2+EjkVERER6iI0sEZHAzp49C09PT9y7dw8REREYPHgwRCKR0LFKhEgkQkBAAMLDwxEeHg4vLy9cunRJ6FhERESkZ9jIEhEJJCsrC1988QW6dOmCcePG4fjx43B2dhY6VqlwdXXFqVOn8Nlnn6F9+/b4+uuvkZ2dLXQsIiIi0hOc7ImISAAREREYNGgQpFIptmzZgnr16gkdSTD8tyAiIqLC4ogsEVEpUqlUWLBgAZo3b46ePXvi4sWLZb5x8/DwwOXLl9G5c2d4e3tjyZIlUKvVQsciIiIiHcYRWSKiUvLgwQMMHjwYiYmJ2Lx5M5o1ayZ0JJ1z7tw5fPrpp3B0dERgYGCZOdWaiIiICocjskREJUyj0WDt2rVo2LAhvLy8EBYWxiY2H61atUJ4eDjc3Nzg4eGBTZs2gd+3EhER0b9xRJaIqAQ9e/YMn332GcLDw7Fx40Z06tRJ6Eh649ChQxg+fDiaNWuGtWvXGtxyRERERPT+OCJLRFRCdu7cCXd3d1hbWyMyMpJNbCF99NFHuHHjBoyNjeHu7o4//vhD6EhERESkIzgiS0RUzJKSkjBu3DgcPnwYv/76K/r16yd0JL2m0WgQFBSEMWPGoFevXli2bBnKly8vdCwiIiISEEdkiYiK0fHjx9GgQQMkJSXhxo0bbGKLgUgkwieffILIyEg8fvwYHh4eOHXqlNCxiIiISEBsZImIikF6ejrGjx8PX19ffPPNNzhw4ADs7OyEjmVQHBwccOTIEUybNg3du3fHlClTkJmZKXQsIiIiEgBPLSYiKqLLly9j0KBBqFKlCgIDA1GjRg2hIxm8O3fu4NNPP0VaWhq2bNmCRo0aCR2JiIiIShFHZImI3pNCocC3336Ldu3aYdiwYTh9+jSb2FJSp04dnDt3DjKZDK1bt8Z3330HpVIpdCwiIiIqJRyRJSJ6D7du3cKgQYOgUCiwZcsWeHp6Ch2pzAoNDcWgQYNQvnx5bN68GbVr1xY6EhEREZUwjsgSERWCWq3Gzz//DG9vb3Tq1AlXrlxhEyuwxo0bIzQ0FC1btkSjRo3wyy+/gN/REhERGTaOyBIRFVBsbCyGDBmCR48eYfPmzWjdurXQkehfTp48iSFDhqBOnTrYuHEj7O3thY5EREREJYAjskRE/0Gj0SAwMBANGjRAzZo1ER4eziZWR7Vr1w4RERGwt7eHu7s7goKCODpLRERkgDgiS0T0DomJiRg5ciTOnz+P9evXo1u3bkJHogLau3cvRowYgXbt2mHVqlWoVKmS0JGIiIiomHBElojo/z19+hQBAQHatUn37dsHd3d3SCQS3Lhxg02snunVqxdu3LiBzMxMNGjQAIcPHwYAvHnzBsOHD0dCQoLACYmIiOh9SYUOQERUEqIT0xAc9gRxSelIzVTC0lQKR2tz+HrZw7WKRZ6PmT59OrZt2waJRAKFQoHdu3dj5cqV8Pf3h0gkKuV3QMXBxsYGe/fuxaZNmzBgwAB88sknyMrKwqZNmyAWi7Fu3bp8H/s+NURERESlg6cWE5HBUKk1OH47HutCohEWmwyxGFCo/reLM5KIoFYDXk5WCPBxRce6tpCI/25QIyIi0LhxY+1apI0aNcLevXvh6OgoyHuh4vfo0SN8/PHHiIyMBABIpVJERkbCzc1Nu01RaoiIiIhKDxtZIjIIrzMVGL7pCiKepCBLqf7P7U2kYng4VMCGwd6wNDWCj48Pzp07p50YyNbWFnfv3kX58uVLOjqVkqSkJNSsWROvXr0CAIjFYnTo0AFHjx4FUPQaIiIiotLDa2SJSO+9zlTAd9U5hD9OLlADAgBZSjXC45LRa9U5/Lo+EGfPnoVGo4GZmRlMTU0RHx+PQ4cOlXByKk379+/Hq1evYGpqCjMzM6jVahw7dgzbt28vcg29zlSUcHoiIiL6J47IEpFeU6k1kK29gPDHychWFX53ZiwRoaImFa/3zkOvnh+jTp06qFmzJmrUqAFbW1teG2tANBoN4uPjcf/+fdy/fx9RUVHYt28fhgwdhiuWLYtUQ56OVpAHtOBpxkRERKWEjSwR6bUjN59jvDws1yhacsg2pJwLynGbWa3msOkzK9dzmEjFWOHnhc71qpZoVtJNrCEiIiL9w1mLiUivrQuJzvdUUGO72rDp87X2Z5E07+sYs5VqrAuJZhNSRrGGiIiI9A8bWSLSW9GJaQiLTc73fpFEComF9X8+jwbAtZhkPHzxBi6VyxVfQNJ5rCEiIiL9xMmeiEhvBYc9gfgde7HshIeIWzEQT9aMwMujq6HKTMt3W7EYCA57XAIpSZexhoiIiPQTR2SJSG/FJaXnWOPzn0zs3VDZZhKk1tWgTIlH8ulAJO6aB1v/+XlO4KRQaRCXlFHSkUnHsIaIiIj0ExtZItJbqZnKfO8zc22s/buxTXUYVXbC0zUByH5+HyZ2tfJ8zOsMLqFS1rCGiIiI9BNPLSYivWVpWvDv4oys7SA2KQdlSny+25Q3y3siHzJcrCEiIiL9xEaWiPSWfQVTGEkKtm6nMiUB6qw3kFawyfN+I4kIjtZmxRmP9ICjtTlriIiISA+xkSUivaJQKPDnn39iyJAh+GFELyhVeS+bknRyAzLjbkKZHI/MmAgkBv8AE3s3GFetmef2KrUGvl4OJRmddJCvlz3UeZdQoWsoW6HElZ2/4NixY1Aq8z9lmYiIiIqO18gSkc5TqVQ4e/YsgoKCsGvXLpiammLAgAE4unsrFoQqcDUmKddjlCmJeLF3PlQZqZBYVISZayNYtRkEkSj393ciAI2drblsShnkWsUCXk5WxVJDbpVNYJslxuDBg6FUKtGvXz/IZDK0atUK4ndNjUxERESFJtJoNHlP10hEJCCNRoPLly9DLpdjx44dyM7O1jYGrVu31jYGR24+x3h5GLKU+QyrFYCJVIwVfl7oXK9qccUnPVLcNaRSqRASEqL94sXc3BwDBgyATCZD48aN85zxmIiIiAqHjSwR6QyNRoPIyEjI5XLI5XK8evUKvXv3hkwmQ/v27SGV5j6JRKXWQLb2AsIfJyM7n2VU3sVYIkZDxwoICmgBiZgNRllUkjWkUChw/PhxyOVyBAcHw9bWFjKZDDKZDPXr1y+ut0BERFTmsJElIsHdvXsX27dvR1BQEGJiYvDxxx9DJpPhww8/hImJyX8+/nWmAr6rziHuVXqhGhFjiRiOFc2wd3QrWJpyttmyrDRqKDMzE4cOHYJcLsf+/ftRs2ZNbVNbo0aNor4FIiKiMoWNLBEJIjY2Ftu3b4dcLseNGzfQtWtX+Pn5oXv37ihXrvDXqr7OVGB44BVEPE5BtlKNd+3YRACMpWJ4OlTA+sHebGIJQOnWUGpqKvbt2we5XI4jR46gYcOGkMlk6N+/PxwcOOkYERHRf2EjS0SlJj4+Hjt37oRcLselS5fQvn17+Pn5oVevXrCysiry86vUGvwVFY+1Z6IRFpsMsRhQ/GN0zUgigloNNHK2QoCPKzq42fJ0YspBiBp69eoVgoODIZfLcerUKbRo0QJ+fn7o27cvqlSpUtS3REREZJDYyBJRiXr7IT0oKAinT59Gy5YtIZPJ0KdPH9jY5L0eZ3GITkzD3utPcPDURaRkKODTvAkcrc3g6+XA2YmpQN7WUND+YzCrUBGNG9Qr8Rp6/vw5du3alePLHplMBl9f32L5soeIiMhQsJElomKnS6dNzp49G48fP8Zvv/1Wqq9LhmPQoEGoV68eZs6cWaqvGxsbix07diAoKEh7+r1MJkOPHj3e6/R7IiIiQ8KF7YioWGRkZGDPnj3o378/bG1tsWDBArRo0QK3b9/G5cuXMXnyZF77R1QITk5OmDp1KkJDQxEZGYnGjRtj7ty5sLGxgZ+fH/744w9kZWUJHZOIiEgQbGSJ6L0pFAocPnwYn376KWxtbTFjxgzUqVMHV65cQUREBL788kvOxkpUDGrXro2vv/4aN2/exIULF+Di4oJJkybB1tYWQ4cOxZEjR6BUKoWOSUREVGpyL8pIRPQOKpUKZ86cgVwux65du2Bubg6ZTIaTJ0+iUaNGEIk4eRJRSRGJRPDw8ICHhwe+//57XLlyBUFBQRg6dCgUCgX69esHmUyG1q1bQyzmd9VERGS4eJQjov+k0Whw8eJFTJw4EY6OjpDJZDAyMsIff/yBmJgYLFq0CI0bN2YTS1SKRCIRmjZtiqVLlyIuLg67du2CRqNB79694eTkhMmTJ+PKlSvgVBhERGSI2MgSUZ40Gg3Cw8PxxRdfwNXVFV27dkVqaioCAwPx5MkTrFy5kqM+RDpCIpHggw8+wOrVq/Hs2TP89ttvePXqFTp27IiaNWviq6++wo0bN4SOSUREVGz4CZSIcrhz5w7mzJmDevXqoWXLloiNjcXy5cvx/PlzrF+/Hp06dYJUyqsSiHSVkZERPvzwQ2zatAnx8fFYvHgxHjx4gKZNm8Ld3R3fffcd7t+/L3RMIiKiImEjS0SIiYnBwoUL4eXlBU9PT4SHh2POnDlISEjA77//jh49esDExETomERUSKampujVqxfkcjkSEhLw1Vdf4cqVK6hfvz6aNGmCxYsXIy4uTuiYREREhcZGlqiMevbsGZYvX46WLVuiZs2aOHnyJCZOnIj4+HjtMjpcq5LIcFhYWGiX7Xn+/DlGjx6NI0eOwNXVFT4+Pvjll18QHx8vdEwiIqICYSNLVIa8fPkS69atQ4cOHeDk5IQ9e/Zg8ODBePbsGQ4fPozBgwejQoUKQsckohJmbW2NYcOG4ejRo3j8+DH8/Pwgl8vh4OCAzp07Y8OGDUhKShI6JhERUb7YyBIZuNTUVGzduhXdunWDnZ0d1q9fjx49euDRo0c4deoURo4cicqVKwsdk4gEYmtri9GjRyMkJATR0dHo0qULVq9ejapVq+Ljjz9GUFAQ0tLShI5JRESUAxtZIgOUkZGBXbt2oW/fvrCxscGiRYvQunVrREVFaZfRsbe3FzomEekYR0dHTJkyBVeuXMHNmzfRtGlTfPfdd7CxscGAAQMQHByMzMxMoWMSERGxkSUyFNnZ2Th48CAGDRoEGxsbfPnll6hfvz5CQ0MRHh6OmTNnwtXVVeiYRKQnatasiVmzZuHmzZu4fPkyatWqhalTp8LW1hZDhgzBn3/+CYVCIXRMIiIqo9jIEukxlUqFEydOICAgAFWrVsWoUaNgZ2eH06dP51hGh4ioKP65bM/x48dRqVIlfPbZZ7Czs8Pnn3+O06dPQ6VSCR2TiIjKEDayRHpGrVbj/PnzGD9+POzt7eHn5wcTExPs378fjx49wsKFC9GoUSOIRCKhoxKRgRGJRPD29sbixYsRGxuL4OBgiMVi9O3bF05OTpg0aRIuXboEjUYjdFQiIjJwbGSJ9IBGo0FYWBhmzJgBFxcXdO/eHRkZGdi2bRuePHmClStXolWrVhCL+StNRKVDLBbDx8cHq1atwrNnz7Bx40akpKSgS5cuqFGjBr788ktERESwqSUiohLBT71EOuz27dv49ttv4ebmBh8fHzx58gS//PILnj9/rl1GRyqVCh2TiMo4qVSqXbYnPj4eS5cuxcOHD9GiRQvUr18fc+fOxd27d4WOSUREBoSNLJGOefjwIebPn4+GDRuiUaNGuHnzJr7//nskJCRg69at6N69O4yNjYWOSUSUJxMTE/Ts2RNBQUFISEjAt99+i2vXrqFBgwZo3LgxFi1ahNjYWKFjEhGRnmMjS6QDnj59ip9//hnNmzdHnTp1EBISgilTpiA+Pl67jI65ubnQMYmICqVcuXIYMGAA9u7di/j4eIwbNw5//fUXatSogVatWmHlypV4/vy50DGJiEgPsZElEsiLFy+wZs0atGvXDs7Ozti/fz+GDx+OZ8+eaZfRKV++vNAxiYiKhZWVlXbZnqdPn2LgwIHYuXMnHB0d0bFjR/z222949eqV0DGJiEhPsJElKkUpKSnYvHkzunbtCjs7OwQGBsLX1xexsbH466+/EBAQgEqVKgkdk4ioRFWpUgWjRo3C6dOn8fDhQ3Tr1g1r165F1apV0aNHD2zbtg2pqalCxyQiIh3GRpaohKWnp2PHjh3o3bs3bG1tsXTpUrRr1w737t3TLqNjZ2cndEwiIkE4ODhg0qRJuHz5Mm7fvo0WLVpgwYIFsLGxQb9+/bB7925kZGQIHZOIiHQMG1miEpCVlYX9+/fD398fNjY2+Oabb+Dp6Ynr168jLCwM06dPR/Xq1YWOSUSkU/65bM/Vq1fh5uaGL774Ara2tvj0009x6NAhKBQKoWMSEZEOEGm4wBtRsVAqlTh16hSCgoKwZ88eVKhQATKZDH5+fvDw8IBIJBI6YpkSFBSEAwcOICIiAunp6WjevDlatmyJMWPGCB2N9MTPP/+MK1eu4Pz586hQoQLq168PX19f9O3bV+hoZYpGo8G1a9cgl8shl8uRnp6OPn36wM/PD23atIFEIhE6IhERCYALUBIVgVqtxvnz5yGXy7Fz506IxWIMGDAAhw8fRrNmzdi8CujWrVuQy+VQq9UA/l7WSCqVspGlAjt//jx27dqFt9/3RkZGon79+gKnKntEIhEaN26Mxo0bY8GCBdp9rkwmg1gsRv/+/SGTydC8eXPuc4mIyhCOyBIV0j9HB7Zv3443b96gb9++kMlkHB3QIa9evYKDg4P22jqpVIo7d+7A1dVV4GSkL6KiouDu7g6VSgUAsLCwwJMnTzibuI54exaMXC7H7t27tWfByGQyeHp6sqklIjJwvEaWqIBu3ryJr7/+GrVr10a7du0QHx+PNWvW4Pnz59pldNjE6o6KFSti0qRJkEqlEIvFkMlkbGKpUNzc3NCrVy+IRCJIpVLMmDGDTawOkUql2mV7nj9/jhUrViAuLg6tW7dG3bp1MXv2bERFRQkdk4iISghHZIne4cGDB9i+fTvkcjnu3buHHj16QCaToWvXrjAzMxM6Hv2HV69ewdbWFiqVCvfv32cjS4UWFRWFunXrwtjYGImJiWxk9UB6ejoOHjyIoKAgHDp0CHXr1oVMJsOAAQM4yR4RkQFhI0v0L0+ePNE2r9evX0eXLl0gk8nw8ccfw9LSUuh4VEh+fn6Ii4vD2bNnhY5Cesrb2xsNGjTAhg0bhI5ChZSSkoI//vgDcrkcx44dQ5MmTSCTydC/f38ue0ZEpOfYyJJOik5MQ3DYE8QlpSM1UwlLUykcrc3h62UP1yoWxf56iYmJ2LVrF+RyOc6fP4+2bdtCJpPB19cXFStWLPbXo5JX2jVEhoc1ZFhevHiB3bt3Qy6X4+zZs/Dx8YFMJkOfPn1QqVKlEnlN1hARUclhI0s6Q6XW4PjteKwLiUZYbDLEYkCh+l95GklEUKsBLycrBPi4omNdW0jE7z+ZR3JyMvbu3YugoCCcOHECzZo1g0wmQ9++fVG1atXieEtUykq7hsjwsIbKhqdPn2Lnzp2Qy+UIDQ1Fp06dIJPJ0LNnzyKfPs4aIiIqHWxkSSe8zlRg+KYriHiSgiyl+j+3N5GK4eFQARsGe8PS1CjX/bdu3YKVlRWqVauW4/Y3b95g//79kMvlOHz4MNzd3eHn54f+/fvDycmp2N4Plb7iriEqe1hDZdPDhw+1l5NERUWhW7dukMlk6NatG8zNzXNsGxcXh/T0dNSpUyfP52INERGVHjayJLjXmQr4rjqHuFfpyFYVvByNJSI4VjRH8OhWKP+PDwDnzp1Dhw4d0LNnT2zfvh1ZWVn4888/IZfLsW/fPjg7O8PPzw8DBgxA7dq1S+ItUSkr7hqisoc1RABw+/ZtbN++HUFBQXj69Cl69uwJmUyGzp07w9jYGN27d8eJEydw+vRpeHt753gsa4iIqHSxkSVBqdQayNZeQPjj5EId+N8ylojg6WgFeUALSMQiXLx4Ee3atUNmZiaMjY3Rv39/7N+/HxUrVoRMJoOfnx/c3d25vqABKe4aorKHNUT/ptFocP36dcjlcsjlcqSmpqJ79+74/fffoVKpYG5ujpCQEDRq1AgAa4iISAhSoQNQ2Xb8djwinqTke+BPubATqaH7oc58A9Pqnqj04ThILKy192erNIh4nIK/ouJRITUGbdu2RVZW1t/3ZWcjOTkZR48ehbe3N5tXA5VfDSWHbEPKuaAct5nVag6bPrNy3PbPGupcj9dGl0XFuR9iDRkGkUgELy8veHl54ccff8SlS5fw7bffQqVSAfh7iZ/WrVvj3Llz8PLy4n6IiEgAbGRJUOtCovO9jigt4hhSzm9H5e6TIbWqilfH1yLxjwWo6j8/x3bZSjXWnL6PvWM/gFqd+7maNm1aItlJN7yrhoztasOmz9fan0XSvE/by1aqsS4kmh8gy6ji2g+xhgyTWCxGixYtoFQqc9yekZEBb29vZGZmcj9ERCQANrIkmOjENITFJud7f2roAVg2+RjmdVoCACp1m4inv36G7PhoGNu6arfTALge9xqL126BNOMVHj16hFu3buH+/ftITU0t4XdBQvqvGhJJpDlGzvKjAXAtJhkPX7yBS+VyxReQdF5x7odYQ4YtIyMDtWvXRq1atVCvXj04OzujYsWKiE3K5H6IiEgAbGRJMMFhTyAWA/9/plYOGqUC2QkPYdVuqPY2I6uqkFSwRdbTOzk+QAKAWAyonRpjbKe8Z5Ikw/SuGgKA7ISHiFsxEGJjc5i6eMGqzSBITPNeu1EsBoLDHmMya6hMKe79EGvIcF24cCHP2xcfvcP9EBGRANjIkmDiktJzrK33T6qM14BGDYm5VY7bJebloUpPzrW9QqVBXFJGCaQkXfauGjKxd0Nlm0mQWleDMiUeyacDkbhrHmz95+d5vTRrqGzifoiKivshIiJhsJElwaRmKt9xb+FnfXydoXj/MKSX3lVDZq6NtX83tqkOo8pOeLomANnP78PErlaej2ENlT3cD1FRcT9ERCQMsdABqOyyNM3/exSJWQVAJM416qFKf51rdOSt8mZcf6+seVcN/ZuRtR3EJuWgTInPdxvWUNnD/RAVFfdDRETCYCNLgnG0NoeRJO8lcURSIxjbuCAzNlJ7myL5OVQp8TCplvvaISOJCI7WZiWWlXTTu2ro35QpCVBnvYG0gk2e97OGyibuh6iouB8iIhIGG1kSjK+XPfJYLUfLslE3pF7dh/Q755EdH42Xh5bDxKF+rglWgL8Xo/f1cijBtKSL3lVDSSc3IDPuJpTJ8ciMiUBi8A8wsXeDcdWaeW6frVAieMlMrF27Fi9fvizB1KRLuB+ioiru/VDIliXYt2+fdk10IiLKG6+RJcG4VrGAl5MVrsYk5Xm/hWdnqNKT8eroaqiz3sDU2ROVuo7PtZ0IQGNnay5XUAa9q4aUKYl4sXc+VBmpkFhUhJlrI1i1GQSRKPf3dyIAnvbl0b7aB9i4cSPGjh2LTp06QSaToWfPnihfvnwpvBsSAvdDVFTFuR+qZ2sGN3FFTJgwAUlJSejduzdkMhnat28PqZQf2YiI/kmk0WgKP5sFUTE5cvM5xsvD8l1IviBMpGKs8PPiIvJlVEnU0MOHD7F9+3bI5XJERUWhW7dukMlk6NatG8zNzYsrOukI7oeoqIq7hjQaDS5fvoygoCDs2LEDSqUS/fr1g0wmQ6tWrSAW84Q6IiLuCUlQHevawsO+AowLeH3RvxlLxPB0qIAObrbFnIz0RUnUkIuLC7744gtcv34dYWFhaNCgAWbNmgVbW1sMHDgQBw4cQHZ2dnG9BRIY90NUVMVdQyKRCM2aNcPPP/+MuLg47NixAyqVCr6+vnB2dsbUqVNx9epVcCyCiMoyjsiS4F5nKuC76hziXqUjO5+1+PJiLBHDsaIZ9o5uBUtTzvJYlpVGDWk0Gly/fh1yuRxyuRypqano3bs3/Pz80LZtW0gkkqK+DRIQ90NUVKVRQwqFAsePH4dcLkdwcDBsbGwgk8kgk8ng7u5e1LdARKRX2MiSTnidqcDwwCuIeJyCbKX6nas3igAYS//+9nr9YG9+eCQApVtDGo0GFy9ehFwux44dO6DRaLSn/bVo0YKn/ekp7oeoqEqzhjIzM3H48GHI5XLs378fNWrUgEwmw4ABA1CzZt6TSRERGRI2sqQzVGoN/oqKx9oz0QiLTYZYDCj+8a22kUQEtRpo5GyFAB9XdHCzhUT8fqdxkWESooZUKhVOnz4NuVyOXbt2wdLSEgMGDIBMJoOXlxdEItaoPuF+iIpKiBpKTU3F/v37IZfL8eeff8LT01Pb1Do4cCZtIjJMbGRJJ0UnpmHv9SeIS8rA6wwFypsZwdHaDL5eDpwVlArkbQ0dPHURKRkK+DRvUuI1lJ2djePHjyMoKAh79+6FnZ2d9rS/evXqlchrUsnhfoiK6m0Nrft9D6rXrge3Gs4lXkNJSUkIDg5GUFAQTp06hRYtWkAmk6Fv376wscl7/VoiIn3ERpaIDNrs2bPx+PFj/Pbbb6X6uhkZGTh06BDkcjkOHDiA2rVra0dIXF1zr0FKRIarZcuWmDBhAgYMGFCqrxsfH49du3ZBLpfj4sWLaNeuHfz8/ODr6wsrK6tSzUJEVNx4IRcRUQkwMzNDnz59sHPnTiQkJGDatGk4d+4c3Nzc0KxZMyxduhRPnjwROiYRGTBbW1uMGTMGISEhePDgATp37oxffvkFtra26NmzJ+RyOd68eSN0TCKi98JGloiohFlaWmqX7Xn+/DkCAgJw8OBBVK9eHW3btsWvv/6KxMREoWMSkQFzcnLSLttz48YNeHt7Y968edqZj/fu3YvMzEyhYxIRFRgbWSKiUlSxYkV89tlnOH78OOLi4tCnTx9s2bIF1apVw4cffojAwECkpKQIHZOIDFitWrUwa9Ys3LhxAxcvXkSNGjUwZcoU2NraYujQoThy5AgUCoXQMYmI3omNLBGRQKpWrYpx48bh3LlzuH//Pjp06IBly5bB1tYWvr6+2L59O0/7I6ISIxKJ0KBBA3z//fe4f/8+jh8/DmtrawwbNgzVqlXDqFGjcPr0aajVaqGjEhHlwkaWiEgHODs7Y9q0abh27RrCw8Ph5eWF2bNnw9bWFp988gn27duHrKwsoWMSkYESiUTw9vbGkiVLEBcXh927d0MkEqFv375wdHTE5MmTcfnyZXCOUCLSFWxkiYh0TJ06dfDNN9/g1q1bOH/+PJydnTFhwgTY2tpi2LBhOHr0KJRKpdAxichAicVitGnTBqtWrcKzZ8+wYcMGJCUloVOnTqhRowa+/PJLREZGsqklIkGxkSUi0lEikQgeHh748ccfER0djSNHjqBChQoYMmQIqlWrpp2NlKf9EVFJkUql6NKlCzZu3Ij4+HgsXboUDx8+RPPmzeHu7o558+bh3r17QsckojKIjSwRkR4QiUTaZXvi4uKwY8cOqFQq+Pr6wtnZWTsbKUdIiKikmJqaomfPnggKCkJCQgK+/vprhIaGwt3dHY0bN8ZPP/2E2NhYoWMSURnBRpaISM9IJBLtsj3Pnj3D2rVrkZiYiPbt2+eYjZSIqKSUK1dOu2xPfHw8xo0bh2PHjqFGjRpo3bo1Vq5cifj4eKFjEpEBYyNLRKTHjIyM0LVrVwQGBiIhIQGLFi3CvXv30LRp0xyzkRIRlRQrKysMGTIER44cwZMnT+Dv74+dO3fCwcEBnTp1wvr165GUlCR0TCIyMGxkiYgMhKmpqXbZnvj4eMycOROXLl1CvXr14O3tjcWLF+Px48dCxyQiA2ZjY6Ndtufhw4fo2rUr1qxZA1tbW/To0QPbtm1DWlqa0DGJyACwkSUiMkCWlpbaZXvi4+MxatQo/Pnnn3BxcdHORpqQkCB0TCIyYA4ODtple27fvo0WLVpgwYIFsLGxQf/+/bFnzx5kZGQIHZOI9BQbWSIiA2dtbY1hw4bh2LFjePz4MQYMGICgoCDY29ujc+fO2LBhA5KTk4WOSUQG7O2yPREREbhy5Qrq1KmDGTNmwNbWFp9++ikOHz4MhUIhdEwi0iNsZImIyhBbW1vtsj3R0dHo0qULVq1aBVtbW+1spDztj4hKUv369TFv3jzcvXsXJ0+ehI2NDUaMGIGqVati5MiROHnyJFQqldAxiUjHsZElIiqjHB0dMWXKFFy9ehU3btyAt7c3vvvuO9ja2mpnI83MzBQ6JhEZKJFIpF22JyYmBn/88QeMjIwgk8ng6OiIiRMn4uLFi1xWjIjyxEaWiIhyLNtz8eJF1KhRA1OmTIGtra12NlKe9kdEJUUsFmuX7Xny5AkCAwORmpqKrl27wtXVFV988QXCw8PZ1BKRFhtZIiLSEolEOZbtOX78OCpWrIhhw4ahWrVq2tlI1Wq10FGJyEBJpVLtsj3Pnz/H8uXLERcXh1atWqFevXqYM2cO7ty5I3RMIhIYG1kiIsqTSCSCt7c3lixZgri4OOzevRsikQh9+/aFo6OjdjZSjpAQUUkxMTHRLtsTHx+POXPmIDw8HJ6enmjUqBEWLlyImJgYoWMSkQDYyBIR0X8Si8XaZXuePXuGDRs2ICkpCZ06ddLORhoZGcmmlohKTLly5bTL9sTHx2PChAk4efIkatasiZYtW2LFihV4/vy50DGJqJSwkSUiokKRSqXo0qULNm7ciPj4eCxZsgTR0dFo1qwZ3N3dMW/ePNy7d0/omERkwCpUqIDBgwfj8OHDePbsGQYPHozdu3fD0dERHTp0wLp16/Dy5UuhYxJRCWIjS0RE783U1BS9evWCXC5HQkICvv76a4SGhsLd3V07G2lsbKzQMYnIgFWuXBkjR47EqVOn8OjRI/To0QPr16+HnZ0dunfvjq1btyI1NVXomERUzNjIEhFRsbCwsNAu2xMfH4+xY8fi2LFjqFGjhnY20vj4eKFjEpEBs7e31y7bExUVhVatWmHRokWwsbFB3759sWvXLmRkZAgdk4iKgUjDC5qIyAD9+uuv2L59Ox4+fIisrCy4ubmhXbt2+Oabb4SOVuYkJCRg9+7dCAoKwoULF9C2bVvIZDL07t0b1tbWQscjKjGfffYZHjx4gGvXrsHBwQE2NjaYPn06unbtKnS0MufWrVvYvn07goKC8OzZM/Tq1QsymQydOnWCsbGx0PGI6D1wRJaIDFJSUhLOnDmDmJgYPH/+HGfOnMHTp0+FjlUm2djYYNSoUThz5gwePnyIrl27Ys2aNbC1tUWPHj3w+++/Iy0tTeiYRMXu9u3bOHXqFF6/fo1bt27h1KlTXI9ZIP9ctuf06dOws7PDqFGjULVqVQQEBODEiRNQqVRCxySiQuCILBEZpLS0NNjZ2WkbJKlUikePHsHe3l7gZPTWgwcPIJfLIZfL8eDBA3Tv3h0ymQwfffQRTE1NhY5HVGRnzpxBx44dtc2rm5sbbt26BZFIJHAyAgC1Wo2LFy9CLpdjx44dEIlE6NevH/z8/NC8eXP+fyLScRyRJSKDZGFhgS+//BJSqRRisRjDhw9nE6tjatSoga+++gqRkZG4fPky6tSpg+nTp8PGxkY7GylHr0iftWnTBk2aNAHw95dpCxYsYHOkQ8RiMVq2bInly5fj8ePH2Lp1KzIyMtCtWze4uLhgxowZCAsL47JiRDqKI7JEZLDS0tJQqVIlKBQKxMXFsZHVAxqNBqGhoZDL5di+fTsyMjLQp08f+Pn5wcfHBxKJROiIRIVy5swZfPDBB6hWrRoeP37MRlYPZGdn4+jRo5DL5di7dy8cHBwgk8kgk8ng5uYmdDwi+n9sZInIoI0aNQqPHz/G/v37hY5ChaRWq3Hu3DnI5XLs3LkTUqkU/fv3h0wmQ7NmzdgQkN7w8PDAmDFjMHLkSKGjUCGlp6fj0KFDCAoKwsGDB+Hm5gaZTIYBAwbAxcVF6HhEZRobWSIySNGJaQgOe4K4pHSkZiphaSqFo7U5fL3s4VrFQuh4VEhKpRInT56EXC7H7t27YW1trR0h8fDwKJGmljVERcUaMiyvX7/GH3/8AblcjmPHjqFx48aQyWTo168fqlWrViKvyRoiyh8bWSIyGCq1Bsdvx2NdSDTCYpMhFgMK1f92cUYSEdRqwMvJCgE+ruhY1xYSMUf19E1WVhaOHDkCuVyOP/74A05OTtqmtk6dOkV6btYQFRVrqGx4+fIldu/eDblcjpCQELRu3RoymQx9+vRB5cqVi/TcrCGigmEjS0QG4XWmAsM3XUHEkxRkKdX/ub2JVAwPhwrYMNgblqZGpZCQSsKbN29w8OBBBAUF4dChQ6hfv772tD9nZ+cc2z579gyRkZHo3Llzns/FGqKiYg2VTc+ePcPOnTshl8tx9epVdOzYETKZDL169UL58uVzbHvp0iVUqFAh32ttWUNEBcdGloj03utMBXxXnUPcq3Rkqwq+SzOWiOBY0RzBo1uhPD8A6L2UlBTs3bsXcrkcx48fR9OmTbWn/VWtWhVjxozBqlWrsGHDBgwdOjTHY1lDVFSsIQKAR48eYfv27ZDL5bh9+za6desGmUyGbt26wczMDC4uLnj16hXOnTuHBg0a5Hgsa4iocNjIEpFeU6k1kK29gPDHyYU68L9lLBHB09EK8oAWPDXLgCQmJmpP+zt37hx8fHxw6dIlpKenw8jICBs2bMDAgQMBsIao6FhDlJeoqChs374dQUFBePz4Mdq0aYOjR49CrVbD0tISFy5cQL169QCwhojeBxtZItJrR24+x3h5WK5TsJJDtiHlXFCO28xqNYdNn1m5nsNEKsYKPy90rle1RLOSMJ48eYIffvgBq1ev1q4HKZFIsG7dOgwdOjTfGnor5cJOpIbuhzrzDUyre6LSh+MgsbDOsQ1rqGx7Vw0VpH4A1pAh02g0CA8Px+eff45Lly5pb7ewsMDFixdRv379PGuoMMcxgDVEZY9U6ABEREWxLiQ63wbE2K42bPp8rf1ZJM37lKtspRrrQqJ58DdQ9vb2yMjI0P4skUigUqkwbNgwtGzZEutCXuZbQ2kRx5Byfjsqd58MqVVVvDq+Fol/LEBV//k5tmMNlW357YcKWj8Aa8iQiUQieHp6IiYmRnubVCpFWloamjVrhrS0tHxrqKDHMYA1RGUPG1ki0lvRiWkIi03O936RRJrnyMe/aQBci0nGwxdv4FK5XPEFJJ1hZGQEb29v1KtXD3Xq1IGrqysqV64MacVqCIt9kO/jUkMPwLLJxzCv0xIAUKnbRDz99TNkx0fD2NZVux1rqOx6136ooPUDsIYMnVKphKOjI+rWrYt69eqhVq1acHZ2houLyztrqKDHMYA1RGUPG1ki0lvBYU8gFgMqVd73Zyc8RNyKgRAbm8PUxQtWbQZBYpr3untiMRAc9hiTOxVt+RbSTWvWrMnz9sVH7+RbQxqlAtkJD2HV7n8TQxlZVYWkgi2ynt7J1Yiwhsqm/PZDha0fgDVkyIyMjHD58uU873vXfqgwxzGANURlCxtZItJbcUnpOdbW+ycTezdUtpkEqXU1KFPikXw6EIm75sHWfz5EotwTYShUGsQlZeTxTGTI3lVDqozXgEYNiblVjtsl5uWhSk/OtT1rqGzKr4YKWz8Aa6isyq+GCnscA1hDVLawkSUivZWaqcz3PjPXxtq/G9tUh1FlJzxdE4Ds5/dhYlcrz8e8zlAUe0bSbe+qob9P1Csc1lDZk38Nvd9cmqyhsie/Gnqf4xjAGqKyQyx0ACKi92VpWvDv4oys7SA2KQdlSny+25Q34/p7Zc27akhiVgEQiXONnqnSX+caZXuLNVT25FdD71M/AGuoLCrosawgxzGANURlBxtZItJbjtbmMJIUbL08ZUoC1FlvIK1gk+f9UrEIjtZmxRmP9MC7akgkNYKxjQsyYyO1tymSn0OVEg+TarmvPzOSsIbKovxqqLD1A7CGyqqCHsv+6zgGsIaobGEjS0R6y9fLHuq8V01B0skNyIy7CWVyPDJjIpAY/ANM7N1gXLVmntsrlEqs/3okvv/+e9y/f78EU5MueVcNAYBlo25IvboP6XfOIzs+Gi8PLYeJQ/08J+pRqTXw9XIowbSki95VQ4WpH4A1VFblV0OFPY4BrCEqW3iNLBHpLdcqFvByssLVmKRc9ylTEvFi73yoMlIhsagIM9dGsGozCCJR7u/vRAAaOVuj7/gRkMvlmDNnDjw9PeHn54f+/fvDwYEfCgzVu2oIACw8O0OVnoxXR1dDnfUGps6eqNR1fK7tRAAaO1tzyYsy6F01VND6AVhDZVl+NVSY4xgAQKOGMv4+dqy/gZEjR8LKyqrkwxMJSKTRaN5vNgIiIh1w5OZzjJeH5bmQfEGZSMVY4eelXUQ+KSkJwcHBCAoKwqlTp9CiRQvIZDL07dsXNjb5n9JF+qkkaojKFtYQFVVx1dCnNZX487eFCA8Px4gRIzBx4kR+GUsGi6cWE5Fe61jXFh72FWBcwGtl/81YIoanQwV0cLPV3mZtbY1hw4bh2LFjePz4MQYMGICgoCDY29ujS5cu2LhxI5KTk4vpHZDQSqKGqGxhDVFRFVcNfTGoO0JCQnDkyBFER0ejZs2aGDx4MG7cuFHMiYmEx0aWiPSaRCzC+iHecKxoXugPAMYSMRwrmmH9YG9IxHk/1tbWFmPGjEFISAgePHiATp064ZdffoGtrS169uwJuVyON2/eFMdbIYGUdA2R4WMNUVG9raGKJhpIC/npPK8aatGiBfbs2YPw8HCYmJjA29sb3bp1w6lTp8CTMclQ8NRiIjIIrzMVGB54BRGPU5CtVL9zBUcRAGPp399erx/sDUvTwi9VcO/ePWzfvh1BQUF49OgRevToAZlMhq5du8LExOS93wcJp7RriAwPa4iK4v79+2jcvBWaTtuI2DRRsdbQ8+fPsWLFCqxatQq1atXC9OnT4evrC4lEUuzvg6i0sJElIoOhUmvwV1Q81p6JRlhsMsRiQKH63y7OSCKCWg00crZCgI8rOrjZFnkERKPR4MaNG5DL5ZDL5Xj58iV8fX0hk8nQvn17GBnxw6k+EaKGyLCwhuh9ZGdno1WrVmjdujV+WrykxGooNTUV69evx5IlS2BsbIwpU6ZgyJAhMDPjkj2kf9jIEpFBik5Mw97rT3Dw1EWkZCjg07wJHK3N4OvlUGKzgmo0Gly9ehVBQUHYvn07srOz0a9fP8hkMrRu3RpiMa/m0CdvayguKQOvMxQob2ZU4jVEhoU1RAU1ffp0HDt2DBcvXsxxVk9J1ZBCocCOHTuwcOFCPHv2DGPHjsWYMWNQqVKl4ng7RKWCjSwRGbTZs2fj8ePH+O2330r1ddVqNc6ePYugoCDs2rULJiYm6N+/P2QyGby9vSEScQSGiIiAo0ePok+fPrhy5Qrc3NxK9bU1Gg2OHTuGhQsX4sKFCxg2bBgmT54MFxeXUs1B9D44PEBEVALEYjHatGmD1atX4+nTp1i/fj2SkpLQqVMn1KxZE1999RUiIyM56QYRURmWkJCATz/9FMuWLSv1JhYARCIROnfujOPHjyMkJAQvXryAm5sbZDIZQkNDSz0PUWGwkSUiKmFGRkbaZXvi4+OxePFiPHjwAM2aNYO7uzvmzZuHe/fuCR2TiIhKkVqtxuDBg9G2bVsMHTpU6Dho1KgRgoKCEBUVhSpVqqBNmzbo2LEjjhw5wi9dSSexkSUiKkWmpqbo1asX5HI5EhIS8PXXXyM0NBTu7u5o0qQJfvrpJ8TGxgodk4iIStiyZctw+/Zt/Prrrzp1uYmLiwtWrFiBmJgYtGnTBgMHDoSXlxe2bt0KhUIhdDwiLV4jS0QGTahrZAsrOTkZwcHBkMvlOHHiBJo1awY/Pz/07dsXtra2QscjIqJidO3aNfj4+ODYsWNo2bKl0HHeKT09HYGBgfjpp5+gUCgwefJkfPbZZ7CwsBA6GpVxHJElItIBVlZWGDp0KI4cOYInT57A398f27dvh4ODAzp16qS9xpaIiPRbWloa/Pz88OWXX+p8EwsA5ubmGDVqFO7evYslS5bg999/h6OjI7766is8f/5c6HhUhrGRJSLSMTY2Nhg1ahTOnDmDhw8fomvXrlizZg2qVq2Kjz/+GL///jvS0tKEjklERO9h/PjxqFatGr744guhoxSKRCJB3759cenSJQQHB+P69etwcXHBiBEjcOfOHaHjURnERpaISIc5ODhg8uTJuHz5Mm7duoVmzZrhxx9/hI2NDfr37489e/YgMzNT6JhERFQAcrkcf/zxB7Zs2QKJRCJ0nPciEonQtm1bHDx4EFeuXEF2djY8PT3Ru3dvXLhwQeh4VIawkSUi0hM1atTQLttz+fJl1KlTB9OnT4eNjQ0GDx6Mw4cPcyIOIiId9fDhQ3z++efYsGEDHBwchI5TLNzd3bFp0ybcv38fNWrUQJcuXeDj44N9+/ZBrVYLHY8MHBtZIiI99M9le06cOIEqVapgxIgRsLOzw8iRI3Hq1CmoVCqhYxIREQCFQoFPPvkEAwcORM+ePYWOU+wcHBywaNEixMbGonv37vj8889Rv359rF+/HllZWULHIwPFRpaISI+JRCLtsj0xMTHYu3cvpFIp+vfvD0dHR0ycOBEXL17kGoBERAKaPXs20tLSsGjRIqGjlCgrKyvMmDEDDx8+xLRp0/DTTz/BxcUFCxYsQHJystDxyMCwkSUiMhBisRitW7fGL7/8gqdPnyIwMBCpqan48MMP4erqipkzZyI8PJxNLRFRKTp58iSWLVsGuVwOMzMzoeOUChMTEwwbNgw3b97EmjVrcODAATg5OWHq1Kl4/Pix0PHIQLCRJSIyQFKpVLtsT3x8PJYtW4aYmBi0bNkS9erVw5w5c3D37l2hYxIRGbQXL15g4MCBWLx4MerXry90nFInFovRo0cPhISE4MiRI4iOjkbNmjUxePBg3LhxQ+h4pOfYyBIRGTgTExPtsj0JCQmYM2cOrl+/jgYNGqBRo0ZYuHAhYmJihI5JRGRQNBoNhg0bhubNm2PEiBFCxxFcixYtsGfPHkRERMDU1BTe3t746KOPcOrUKZ4pRO+FjSwRURlSrlw59O/fH8HBwUhISMCECRNw8uRJ1KxZE61atcKKFSu4wD0RUTH45ZdfEBYWhnXr1kEkEgkdR2fUrl0ba9aswaNHj+Dl5QVfX180a9YMO3fu5CSFVChsZImIyqgKFSpol+15+vQpBg0ahN27d8PR0REdOnTAunXr8OrVK6FjEhHpnYiICMyYMQPbtm1DxYoVhY6jk2xtbfH9998jNjYWn3zyCaZOnYo6depg9erVyMjIEDoe6QE2skREhCpVquDzzz/HqVOn8OjRI/To0QPr169H1apV0b17d2zduhWpqalCxyQi0nnp6emQyWSYNm0a2rRpI3QcnWdpaYmJEyfi/v37mDNnDn799Vc4OTlh7ty5ePnypdDxSIexkSUiohzs7e21y/ZERUWhVatWWLRoEWxsbNCvXz/s3r2b35YTEeVj0qRJqFSpEmbNmiV0FL1iZGQEf39/XL9+Hdu2bcOZM2fg5OSEcePG4eHDh0LHIx3ERpaIiPL1z2V7QkNDUa9ePcycORM2NjYYNGgQDh48iOzsbKFjEhHphF27dmHHjh3Ytm0bpFKp0HH0kkgkQufOnXH8+HGEhITgxYsXcHNzg0wmQ2hoqNDxSIewkSUiogJ5u2zPnTt3cPr0adjZ2WHUqFGws7PDiBEjcOLECU7UQURlVmxsLAICArBu3To4OTkJHccgNGrUCEFBQYiKioKNjQ3atGmDjh074siRI5zpmNjIEhFR4YhEIu2yPY8ePcK+fftgYmICPz8/ODg4YPz48bhw4QI/ZBBRmaFUKuHv74/+/fujb9++QscxOC4uLli+fDliYmLQpk0bDBw4EF5eXti6dSsUCoXQ8UggbGSJiOi9icVi7bI9T548wdatW5GRkYFu3brBxcUFM2bMQFhYGJtaIjJo3333HV6+fImlS5cKHcWgVa5cGd988w1iYmIwcuRIfPvtt6hRowZ+/vlnpKWlCR2PShkbWSIiKhZSqVS7bM/z58+xcuVKPHnyBD4+Pqhbty5mz56NqKgooWMSERWrkJAQLFq0CHK5HObm5kLHKRPMzc0xatQo3L17F0uWLMHvv/8OR0dHfPXVV1wLvQwRafg1OREZoAULFiAwMBCJiYlQKpWws7NDly5d+G25ANLT03Hw4EHI5XIcPHhQO2nHgAED4OLiInQ8IqL39urVKzRs2BDTp0/H2LFjhY5TZmk0Gpw+fRqLFi3CiRMnMGjQIEyZMgV16tQROhqVII7IEpFBMjMzw927d/HixQskJyfj7t27kEgkQscqk8zNzbXL9iQkJGDKlCk4c+YM6tSpgxYtWmDZsmV4+vSp0DGJiAokOTkZGo0GGo0GAQEBaNiwIcaMGSN0rDJNJBKhbdu2OHjwIK5cuYLs7Gx4enqid+/euHDhgtDxqIRwRJaIDFJmZiaqVauGpKQkAICJiQni4uJQpUoVgZPRWy9evMCePXsgl8sREhKC1q1bQyaToU+fPqhcubLQ8YiIclGr1bC2tkbbtm3Rtm1b/PTTTwgPD+c+Swc9fvwYy5Ytw5o1a+Dp6Ynp06ejW7duEIs5jmco2MgSkcFavnw5pkyZAo1Gg0mTJmHRokVCR6J8PH36FDt37oRcLkdoaCg6duwImUyGXr16oXz58kLHIyICADx69Ag1atSAVCpFdnY2li9fjnHjxgkdi94hOTkZa9aswbJly1ChQgVMmzYN/v7+MDExEToaFREbWSIyWJmZmbC2tkZ2djaeP3/O0Vg98fDhQ+zYsQNyuRy3b9/GRx99BD8/P3Tr1o0TqRCRoA4ePIj+/fsjPT0dACCRSLBs2TKeWqwHsrKysG3bNixatAgpKSmYMGECRo4cCSsrK6Gj0Xvi2DoRGSxTU1NMmTIFAwcOZBOrR/65bE9YWBg8PDwwa9Ys2NjYwN/fHwcOHEB2drbQMYmoDLp16xaysrIA/H2MMTExgZmZmcCpqCBMTEwwbNgw3Lx5E2vWrMGBAwfg5OSEqVOn4vHjx0LHo/fAEVkiMkjRiWkIDnuCuKR0pGYqYWkqhaO1OXy97OFaxULoeFRIGo0G4eHhkMvlkMvlSElJQZ8+fSCTydC2bVtIpdJif03WEFHZUpDf+TZt2iAkJATGxsaYPHkyZsyYwRE9PXbhwgUsWrQIhw4dwoABAzBt2jS4u7u/9/PxuFG62MgSkcFQqTU4fjse60KiERabDLEYUKj+t4szkoigVgNeTlYI8HFFx7q2kIhFAiam96HRaHDx4kXI5XLs2LEDarUa/fv3h0wmQ4sWLYo0kQdriKhsKezv/A9j/FDO3AybNm2CjY2NgMmpON25cweLFy/Gli1b0L59e0ybNg0ffPABRCIRwsLCMHXqVAQHB+c5ZwOPG8JhI0tEBuF1pgLDN11BxJMUZCnV/7m9iVQMD4cK2DDYG5amRqWQkEqCSqXCmTNnEBQUhN27d6NcuXIYMGAAZDIZGjVqBJHofx8WoqOjcfHiRXzyySd5PhdriKhs4e88/dvz58+xYsUKrFq1CrVq1cL06dOxYcMG/Pnnn+jcuTMOHTqU48tS1pCw2MgSkd57namA76pziHuVjmxVwXdpxhIRHCuaI3h0K5TnAUXvZWdn4/jx45DL5QgODoadnR1kMhlkMhnq1auH4cOHY8OGDVi4cCGmTZuW47GsIaKyhb/z9C6pqalYv349Fi5ciGfPngH4+xrb6dOnY+7cuQBYQ7qAjSwR6TWVWgPZ2gsIf5xcqAPJW8YSETwdrSAPaMFTfQxIRkYGDh06BLlcjgMHDqBmzZq4e/cusrOzYWxsjAULFmDixIkAWENEZQ1/56mghg4dii1btkClUgEAxGIxAgMD4feJP2tIB7CRJSK9duTmc4yXh+V7Sk/KhZ1IDd0PdeYbmFb3RKUPx0FiYZ1jGxOpGCv8vNC5XtXSiEylLDU1FfPmzcPixYuhVv9dJxKJBHPnzsWXX36Zbw0lh2xDyrmgHLeZ1WoOmz6zcr0Ga4hIf/B3ngpCqVRql3wTi8VQKpVQqVQwMjLC/rBYfvbQAcU/zSMRUSlaFxKd74EkLeIYUs5vR+XukyG1qopXx9ci8Y8FqOo/P8d22Uo11oVE82BioCwtLfH48WOo1WpIJBJIpVJkZ2fjq6++Qp8+fbAu5GW+NWRsVxs2fb7W/iyS5n0aGGuISH+867jB33l6SyKR4Pz588jKyoKpqSnMzMxgamqK8uXL4/Ndd/nZQwewkSUivRWdmIaw2OR8708NPQDLJh/DvE5LAEClbhPx9NfPkB0fDWNbV+12GgDXYpLx8MUbuFQuV8KpSQjOzs7o1asX6tatixo1asDV1RXOzs6ApQ3CYh/k+ziRRJrrW/S8sIaI9MN/HTf4O09viUQiNGnSJNft/OyhO9jIEpHeCg57ArEY+P9LV3LQKBXITngIq3ZDtbcZWVWFpIItsp7eyXEwAQCxGAgOe4zJneqUdGwSwI8//pjn7YuP3sm3hgAgO+Eh4lYMhNjYHKYuXrBqMwgS07zXAmQNEem+dx03AP7O03/jZw/dwUaWiPRWXFJ6jrXa/kmV8RrQqCExt8pxu8S8PFTpybm2V6g0iEvKKIGUpMveVUMm9m6obDMJUutqUKbEI/l0IBJ3zYOt//wcy/q8xRoi0n38naei4mcP3cFGloj0Vmqm8h33Fn4eu9cZivcPQ3rpXTVk5tpY+3djm+owquyEp2sCkP38PkzsauX5GNYQkW7j7zwVFT976A7xf29CRKSbLE3z/y5OYlYBEIlzfQOqSn+d65vSt8qbcT23suZdNfRvRtZ2EJuUgzIlPt9tWENEuo2/81RU/OyhO9jIEpHecrQ2h5Ek7/XXRFIjGNu4IDM2UnubIvk5VCnxMKmW+1oUI4kIjtZmJZaVdNO7aujflCkJUGe9gbSCTZ73s4aIdB9/56mo+NlDd7CRJSK95etlD3Xes98DACwbdUPq1X1Iv3Me2fHReHloOUwc6ueabAEAVGoNfL0cSjAt6aJ31VDSyQ3IjLsJZXI8MmMikBj8A0zs3WBctWae27OGiHRfcf7OK1VqdHfPu8klw8XPHrqD18gSkd5yrWIBLycrXI1JyvN+C8/OUKUn49XR1VBnvYGpsycqdR2fazsRgMbO1pz+vgxyrWKBho4VEJrHUgrKlES82DsfqoxUSCwqwsy1EazaDIJIlPs7YNYQkX5413GjsL/zeBGNDk1HYdKkSQgICIClpWXJvwESHD976A6RRqMp/FXJREQ64sjN5xgvD8t3YfKCMJGKscLPi4uSl0FHjx7F8G+Xw6Td51BqCna6YV4kUGOlXyN09bAvxnREVBKK67ixbIAn0u9exMKFCxEVFYVRo0Zh/PjxsLOzK8a0pIv42UM38NRiItJrHevawsO+AowLeM3TvxlLxPB0qIAObrbFnIx02evXrzFixAj07dsXs4b2REOniu9dQ0ZiAC9j8M1nvXHv3r3iDUpExa64jhud6tmhd+/euHDhAvbv348bN27A1dUVn332GaKiooo5NekSfvbQDWxkiUivScQirB/iDceK5oU+oBhLxHCsaIb1g70hEb//aBzpl+PHj6NBgwaIjo5GZGQkRo4IwIYi1JBTpXK4tGgIfFq3QqNGjbB+/XrwZCci3fXP44YEhRtRy+u4IRKJ4OPjg/379yM0NBRqtRoNGzZEz549ce7cuZJ4CyQwfvbQDWxkiUjvlTc1QvDoVvB0tIKJVIz/OiyI8PcpPQ0dK2Dv6FawNOXU92VBamoqRo0aBV9fX8ycORPHjh2Ds7MzgKLXUOUKFliyZAl2796Nr7/+Gn369MHLly9L/D0R0fspb2oEWcXHyH52D8YSUbEdN+rVq4cNGzbgwYMHcHNzw0cffYRWrVph7969UL9rhiDSO/zsITxeI0tEBkOl1uCvqHisPRONsNhkiMWAQvW/XZyRRAS1GmjkbIUAH1d0cLPlt6FlxMmTJzFs2DC4uLhg/fr1cHFxyXO74qihFy9eICAgAJcuXUJgYCA6depUou+NiArv/v37aNSoEX4PCoKxS+MSO26kpKRg3bp1WLp0KSwtLTF16lQMHDgQpqamJfG2SAD87CEcNrJEZJCiE9Ow9/oTxCVl4HWGAuXNjOBobQZfLwfOEFiGpKWl4YsvvkBgYCDmz5+PUaNGQSwu2MlIRakhjUaD9evXa2cz/eGHH/jBlUhHZGdno1WrVmjdujWWLl2qvb0kjxvZ2dn4/fffsWjRIrx8+RITJkzA559/Dmtr66K+HdIh/OxRutjIEhGRQTp9+jSGDh0KR0dHbNiwATVq1Cj1DHfv3oW/v7/2Q2z9+vVLPQMR5TR9+nQcO3YMFy9ehImJSam+tlqtxuHDh7Fw4UJcu3YNAQEBmDhxIpycnEo1B5Eh4DWyRERkUN68eYMJEyagW7dumDhxIk6ePClIEwsAtWvXxvnz59GjRw80bdoUK1as4ERQRAI6evQoVq9ejaCgoFJvYgFALBajW7duOH36NI4fP47Y2FjUqlULgwYNQkRERKnnIdJnbGSJiMhgnD17Fg0bNkRoaCjCwsIwfvz4Ap9KXFKMjIzw3Xff4c8//8RPP/2Ejz76CM+fPxc0E1FZlJCQgE8//RTLli2Dm5ub0HHQrFkz7Nq1Czdu3ICFhQWaNWuGDz/8ECdOnOAXXkQFwEaWiIj0Xnp6OiZPnowuXbpg9OjROH36NGrVqiV0rBx8fHwQHh6OihUrokGDBti3b5/QkYjKDLVajcGDB6Nt27YYOnSo0HFyqFWrFlavXo2YmBg0bdoUffv2hbe3N7Zv3w6lUil0PCKdxWtkiYhIr50/fx5DhgxB5cqVsXHjRtSpU0foSP9p27ZtGD16NPz8/LB48WKUK8dJQIhK0tKlS7Fs2TJcv34dVlZWQsd5p7S0NGzYsAFLliyBWCzGlClTMHToUJibmwsdjUinsJElIiK9lJGRgW+++QarVq3CnDlzMGnSJEgkEqFjFdijR48waNAgJCYmYtu2bWjcuLHQkYgM0rVr1+Dj44Njx46hZcuWQscpMKVSiZ07d2LRokWIjY3F2LFjMWbMGFSpUkXoaEQ6gacWExGR3rl48SK8vLwQEhKC0NBQTJ06Va+aWACoXr06Tp06hUGDBsHHxwcLFiyASqUSOhaRQUlLS4NMJsOXX36pV00sAEilUvj5+SE0NBRyuRznz5+Hs7MzxowZgwcPHggdj0hwbGSJiEhvZGZm4osvvkCHDh0wbNgwnD17VicmbXlfEokEX331FU6dOoXffvsNHTp0QFxcnNCxiAzG+PHjYW9vjy+++ELoKO9NJBKhY8eOOHr0KM6dO4fk5GTUq1cP/fv3x5UrV4SORyQYNrJERKQXrly5gsaNG+Ovv/7C5cuXMX36dEilUqFjFYumTZsiLCwMNWvWhIeHB7Zv3y50JCK9FxQUhD/++ANbtmzRuzM28uPl5YVt27bh7t27qFatGtq1a4f27dvj8OHDnOmYyhw2skREpNOysrLw1Vdf4YMPPoC/vz8uXLiA+vXrCx2r2FlYWOC3337Dhg0bMHr0aHz66ad4/fq10LGI9FJ0dDQ+//xzbNiwAQ4ODkLHKXbOzs74+eefERsbiw4dOmDIkCHw9PTE5s2bkZ2dLXQ8olLByZ6IiEhnhYaGYsiQITA2NsamTZvQoEEDoSOViqdPn2Lw4MF48OABtm7dqnfX9hEJSaFQwMfHB02aNMHKlSuFjlMqMjIysHnzZixevBgZGRmYOHEiAgICUL58eaGjEZUYjsgSEZHOyc7Oxtdffw0fHx8MGDAAFy9eLDNNLABUq1YNR44cwbhx49CpUyd8++23XE+SqIC+/fZbvHnzBosWLRI6SqkxMzPDyJEjcfv2bSxbtgw7d+6Ek5MTZs6ciWfPngkdj6hEcESWiIh0SlhYGIYMGQKxWIxNmzbB09NT6EiCCg8Ph7+/PywsLLBt2zbUqFFD6EhEOuvEiRP4+OOPcenSJYO8BKGgNBoNzp49i4ULF+LYsWMYOHAgpk6dqteT4xH9G0dkiYhIJygUCsyZMwetWrWCr68vLl26VOabWADw9PTElStX0LRpU3h5eWHjxo2c1IUoDy9evMCgQYOwePHiMt3EAn/PdOzj44P9+/fj2rVrUKvVaNiwIXr27Ilz584JHY+oWHBEloiIBBceHo4hQ4ZApVIhMDAQXl5eQkfSSYcPH8bQoUPRunVrrF27FhUrVhQ6EpFO0Gg0+Pjjj2FkZITdu3dDJBIJHUnnPHnyBMuWLcOaNWvg7u6OadOm4eOPP4ZYzHEt0k+sXCIiEoxCocC8efPQokUL9OjRA1evXmUT+w5du3ZFREQEsrOz4eHhgRMnTggdiUgnrFy5EtevX8dvv/3GJjYf9vb2WLhwIWJjY+Hr64sxY8agXr16WLduHTIzM4WOR1RoHJElIiJB3LhxA4MHD0ZWVhYCAwPRuHFjoSPpDY1Gg7Vr12LKlCkYNWoUvvvuO5iYmAgdi0gQ4eHhaNmyJQ4fPow2bdoIHUdvZGdn4/fff8eiRYvw8uVLjB8/HqNGjYK1tbXQ0YgKhCOyRERUqpRKJX744Qc0bdoUH374IUJDQ9nEFpJIJMLIkSNx9epVnDhxAs2bN8ft27eFjkVU6t68eQOZTIZp06axiS0kY2NjDBkyBJGRkVi/fj2OHj0KJycnTJ48GbGxsULHI/pPbGSJiKjU3Lp1Cy1atMDWrVtx6tQpfP/99xxJLAI3NzdcuHABXbp0gbe3N1atWsWJoKhMmTRpEipXroxZs2YJHUVvicVidOvWDadOncLx48cRGxuLWrVqYdCgQYiIiBA6HlG+2MgSEVGJUyqVWLBgAby9vdGhQwdcu3YNTZs2FTqWQTA2Nsb8+fNx4MAB/Pjjj+jRowfi4+OFjkVU4nbt2oWdO3di27ZtkEqlQscxCM2aNcOuXbtw48YNWFhYoFmzZvjwww9x4sQJfklGOoeNLBERlaioqCi0bt0aGzduxF9//YX58+fD1NRU6FgGp23btoiIiEC5cuXQoEEDHDx4UOhIRCUmJiYGAQEBWLduHZycnISOY3Bq1aqF1atXIyYmBk2bNkW/fv3g7e2N7du3Q6lUCh2PCAAneyIiohKiUqmwdOlSfPvttxg9ejTmzp0LMzMzoWMZPI1Ggy1btmDcuHEYOHAgFi1aBHNzc6FjERUbpVKJtm3bon79+lizZo3QccqEN2/eYMOGDVi8eDHEYjGmTJmCoUOHct9CgmIjS0RExe7u3bsYMmQIXrx4gY0bN6JVq1ZCRypzHj58iIEDByIpKQm///47GjZsKHQkomIxe/Zs7NixA1evXmUjVcqUSiV27dqlXcZn7NixGDNmDKpUqSJ0NCqDeGoxEREVm7ejsF5eXmjevDmuX7/OJlYgLi4uOH36NGQyGVq2bImffvoJarVa6FhERXLmzBksWrQIcrmcTawApFIpZDIZQkNDIZfLceHCBTg7O2PMmDF48OCB0PGojOGILBERFYt79+5h6NCheP78OTZu3AgfHx+hI9H/u3jxIvz9/VG9enUEBgbCwcFB6EhEBfbw4UO4uLjg1atX8PT0xIwZMzB27FihY9H/u379On766Sfs3LkTPXv2xLRp0+Dt7S10LCoDOCJLRERFolarsXz5cnh5eaFx48YIDw9nE6tj3o6OOzk5wcPDA7t27RI6ElGBREZGwtXVFcOHD8fQoUPh5eWFMWPGCB2L/qFhw4bYunUr7t69i2rVqqFdu3Zo3749Dh8+zJmOqURxRJaIiN5bdHQ0hg4diri4OGzcuBEffPCB0JHoP+zcuRMjR45Ez549sXz5clhaWgodiShf27Ztw2effQalUgmNRoOTJ0/yizId9+rVK6xevRrLly+Hra0tpk6dCplMBmNjY6GjkYHhiCwRERWaWq3GL7/8Ak9PTzRo0AARERFsYvVEv379EBERgdjYWDRs2BAXL17U3qdQKPDq1SsB0xHldOPGDSgUCiiVSqjVarRv3x6HDh0SOha9Q8WKFfHVV18hJiYGY8eOxXfffYcaNWpgyZIlSE1NFToeGRA2skRE9J/GjRuH2bNnA/j7erUOHTpg0aJF+OOPP7By5UpYWFgIG5AKxcHBAceOHcOoUaPQvn17zJ07FwqFAn379kXjxo25TiTpjCtXrkClUgEAjI2NUbt2bV7jrSdMTU0xYsQI3L59G8uXL8fOnTvh6OiImTNn4tmzZ0LHIwPAU4uJiMqQ6MQ0BIc9QVxSOlIzlbA0lcLR2hy+XvZwrZJ3M3rt2jU0a9YMADBx4kT8+uuvGDhwIBYuXMjTUg1AWFgY/P39kZ2djbi4OIjFYixfvhwBAQF5bv8+NUSUl4LUUrly5ZCeno7q1atj8eLF8PX1hUgkEjg5vQ+NRoNz585h4cKFOHr0KAYOHIipU6fCzc3tvZ+T+6OyjY0sEZGBU6k1OH47HutCohEWmwyxGFCo/rfrN5KIoFYDXk5WCPBxRce6tpCI//6gqNFo0Lx5c1y9ehVqtRoSiQR79+5F9+7dhXo7VAJu3LiBhg0bake+KlWqhLi4OJiZmQEoWg0R/VNha2mKrDOGDR2CCRMmQCKRCJicitOtW7ewePFibNu2DV26dMH06dO1S7WtWLEChw4dwv79+yGVSnM9lvsjeouNLBGRAXudqcDwTVcQ8SQFWcr/XkPURCqGh0MFbBjsDUtTI+zZswcDBgzQnmpqbGyMUaNG4eeffy7h5FSamjVrhitXrmhnGBWLxfj6668xe/bsItcQ0VusJfq3p0+fYvny5Vi9ejXc3d0xYcIEjBgxAmlpaRg3bhyWLl2aY3vWEP0TG1kiIgP1OlMB31XnEPcqHdmqgu/qjSUiOFY0x55RLVG1YgVkZmbCxMQE2dnZEIlEaNq0KS5cuFCCyam0/fzzzzh+/Dhu3bqFuLg4KJVKSCQSvExNR+/V59+7hoJHt0J5fngkFH1/xFoybK9fv8a6deswb948vH79GhqNBkZGRti4cSP8/f3/3oY1RP/CRpaIyACp1BrI1l5A+OPkQh3w3zKWiODpaAWz82vRrKk36tWrBxcXFzg4OOR5qhcZDpVKhUePHiHyxk1se1a5yDUkD2jB0/rKuOLaH7GWDJtarYazszMeP36svU0ikeDEiRNo1dqHNUS58NMIEZEBOn47HhFPUnId8JNDtiHlXFCO28xqNYdNn1k5bstWaRDxOAUrZsxH53pVSzwv6Q6JRIIaNWrgfmY5RFwNy/dDY8qFnUgN3Q915huYVvdEpQ/HQWJhrb3/bQ39FRXPGirjimt/xFoybFevXsXjx48hFouhVv992rBKpcJnn32GFcFn8qyht7g/KpvYyBIRGaB1IdH5Xj9kbFcbNn2+1v4skuZ9qlW2Uo11IdE86JdR76qhtIhjSDm/HZW7T4bUqipeHV+LxD8WoKr//BzbsYYI4P6ICsbb2xu3b9+GVCqFqakpzMzMtP/tv/Yi90eUC9eRJSIyMNGJaQiLTc73fpFEComFtfaP2DTvJQo0AK7FJOPhizclE5R01n/VUGroAVg2+RjmdVrC2NYVlbpNRFbcDWTHR+fYjjVE3B9RQYlEIri5uaFmzZpwcHBApUqVUK5cOTx6mc79EeWJjSwRkYEJDnsC8Tv27tkJDxG3YiCerBmBl0dXQ5WZlu+2YjEQHPY43/vJML2rhjRKBbITHsLU2UN7m5FVVUgq2CLr6Z1c27OGyjbuj6iouD+i/PDUYiIiAxOXlJ5jTb1/MrF3Q2WbSZBaV4MyJR7JpwORuGsebP3nQyTKPQGGQqVBXFJGSUcmHfOuGlJlvAY0akjMrXLcLjEvD1V6cq7tWUNlG/dHVFTcH1F+2MgSERmY1ExlvveZuTbW/t3YpjqMKjvh6ZoAZD+/DxO7Wnk+5nWGotgzkm57Vw39fYJe4bCGyi7uj6iouD+i/PDUYiIiA2NpWvDvKI2s7SA2KQdlSny+25Q347p7Zc27akhiVgEQiXONdqjSX+caFXmLNVR2cX9ERcX9EeWHjSwRkYFxtDaHkaRg6+QpUxKgznoDaQWbPO83kojgaG1WnPFID7yrhkRSIxjbuCAzNlJ7myL5OVQp8TCpVifX9qyhso37Iyoq7o8oP2xkiYgMjK+XPdR5r1KApJMbkBl3E8rkeGTGRCAx+AeY2LvBuGrNPLdXqTXw9XIowbSki95VQwBg2agbUq/uQ/qd88iOj8bLQ8th4lAfxrauubZlDZVt3B9RUXF/RPnhNbJERAbGtYoFvJyscDUmKdd9ypREvNg7H6qMVEgsKsLMtRGs2gyCSJT7e00RgMbO1nCpXK4UUpMueVcNAYCFZ2eo0pPx6uhqqLPewNTZE5W6js+1HWuIimt/BGjg5WjFWiqDuD+i/LCRJSIyMC9evMDry8FApZaA1DjHfVV6zSjw82hU2WhsnlzM6UhfBPi4IvJJGLKUeQ+FVGjRHxVa9H/ncxhLxQjwyT0qQmWHQqGAxeNLEKldoRHn/NhZmP0RVEqE/f4Tjri+QZcuXYo5Jek67o8oLzy1mIjIgAQHB6N+/fowe3Ufnk4VYVzAa9P+zVgihrOFCD+M9cO4ceOQkcHlCsqajnVt4WFfoUg15OlQAR3cbIs5GemL+/fvw8fHB1f2boCbjXmRasnb1QZT/LpgwIABkMlkePbsWTGnJV3G/RHlhY0sEZEBePnyJfz9/TF8+HAsWbIEe4P3YEtASzhWLPyHR2OJGI4VzXBgejeEXbuGq1evokmTJggPDy+h9KSLJGIR1g/xLlINrR/sDYn4/T54kv7SaDTYuHEjvLy80KxZM1y9chnbx7QtUi1tGOKNMaNHISoqCiKRCG5ubvjll1+gUqlK6F2QLuH+iPLCRpaISM/t27cP7u7uSE1Nxc2bN+Hv7w+RSITypkYIHt0Kno5WMJGK8V+HbxEAE6kYDR0rYO/oVrA0NUKNGjUQEhKC/v37o2XLlliyZAnU75p1gwxKcdQQlS2vXr1Cv379MHPmTOzcuRPLli2DmZlZsdVS1apVERQUhJ07d2LJkiVo0aIFwsLCSvx9kfC4P6J/E2k0msKvJExERIJLSkrChAkTsG/fPixfvhyDBg2CSJT70K5Sa/BXVDzWnolGWGwyxGJAofrfrt9IIoJaDTRytkKAjys6uNnm+a31hQsX4O/vjxo1amDTpk2wt7cv0fdHuqO4aogM24kTJ/Dpp5+iUaNGWL9+PapUqZJrm+KspYyMDPzwww9YvHgxPv/8c8yZMweWlpYl9v5IN3B/RG+xkSUi0kMHDx7EiBEj0LBhQ6xdu7bATWV0Yhr2Xn+CuKQMvM5QoLyZERytzeDr5VCgmRxfv36N8ePHY//+/Vi3bh169+5d1LdCeqaoNUSGJysrC7NmzcLq1auxePFijBgxIs8v1f6tuGrp9u3bGDVqFB48eIDly5ejV69eBXp90n/cH5VtbGSJiPRIcnIyJk2ahODgYCxduhRDhgwR5APbjh07MHLkSPTu3RvLli2DhYVFqWcgIuHdvn0bn3zyCcRiMbZt2wY3NzdBcmg0GgQGBmLq1Klo2bIlVqxYAWdnZ0GyEFHp4DWyRER64s8//4S7uzuePXuGyMhIDB06VLBRh/79+yMiIgLR0dHw8vLC5cuXBclBRMLQaDT45Zdf4O3tjQ8//BAXLlwQrIkFAJFIhCFDhiAqKgpVqlSBu7s7fvrpJygUCsEyEVHJ4ogsEZGOS0lJwZQpU7Bz504sXrwYw4cP15nT5lQqFRYvXozZs2fjyy+/xMyZMyGRSISORUQlKD4+HsOGDUNkZCQ2b96Mtm3bCh0plzNnzuDzzz+HkZERfv31V7Ro0ULoSERUzDgiS0Skw44dO4YGDRogJiYGkZGR+Oyzz3SmiQUAiUSC6dOn4+zZs9i6dSs++OADPHr0SOhYRFRCDh48iAYNGsDS0hLh4eE62cQCQJs2bXD9+nXIZDJ07NgRn3/+OZKSkoSORUTFiI0sEZEOSk1NxciRI9GnTx989dVXOHr0KJycnISOla9GjRrh2rVr8PDwgKenJ7Zu3Qqe8ENkONLT0zFmzBh88sknWLx4MYKCgmBtbS10rHcyNjbGzJkzERkZiZiYGLi5ueH333/nvonIQPDUYiIiHfPXX39h+PDhqFGjBtavX4/q1asLHalQ9u/fj+HDh6NDhw5YvXo1rKyshI5EREVw/fp1fPLJJ7C2tsbWrVvh4uIidKRC02g02LlzJyZMmAB3d3esWrUKtWrVEjoWERUBR2SJiHREWloaRo8ejV69emH69Ok4duyY3jWxANCjRw9EREQgOTkZnp6eOHPmjNCRiOg9qNVq/PTTT2jVqhVkMhlOnz6tl00s8PdkUP3790dUVBTc3NzQsGFDzJ07F1lZWUJHI6L3xBFZIiIdcOrUKQwbNgxOTk7YsGEDXF1dhY5UZG9nNZ0xYwYmTJiA2bNnw9jYWOhYRFQAjx8/xuDBgxETE4OtW7eiefPmQkcqVleuXMHIkSORnp6O1atXo127dkJHIqJC4ogsEZGA3rx5g3HjxqF79+6YPHkyTpw4YRBNLPD3CMjYsWNx+fJlHDx4EC1btsSdO3eEjkVE/2HXrl3w8PCAs7MzwsLCDK6JBQBvb29cvnwZn3/+OXr27InBgwcjMTFR6FhEVAhsZImIBHLmzBl4eHjg+vXruH79OsaOHQux2PB2y/Xr18fly5fxwQcfoEmTJli3bh0nWyHSQampqRg6dChGjBiBtWvXYsOGDbC0tBQ6VomRSqWYOHEibt26hbS0NNSpUwe//fYb1Gq10NGIqAB4ajERUSlLT0/Hl19+id9++w3fffcdxo8fb5ANbF6OHTuGwYMHo2nTpvjtt99QuXJloSMREYCLFy/C398f1atXR2BgIBwcHISOVOoOHDiAMWPGwNHREWvWrEH9+vWFjkRE71A2PjkREemIc+fOoWHDhrhy5QquXbuGiRMnlpkmFgA6deqEyMhISCQSNGjQAEeOHBE6ElGZplQqMXfuXLRv3x6jRo3CsWPHymQTCwDdu3fHrVu30LJlS3h7e2PmzJlIT08XOhYR5YMjskREpSAjIwOzZs3Cr7/+innz5mHChAmQSCRCxxKMRqPBxo0bMWHCBAwfPhzz58+Hqamp0LGIypTo6GgMGjQISUlJ2LZtG7y8vISOpDMiIiLw+eef49mzZ/jll1/w0UcfCR2JiP6l7AwDEBEJ5MKFC/Dy8sL58+dx7do1TJ48uUw3scDfE0ENGzYM165dw4ULF+Dt7Y3IyEihYxGVCRqNBps3b4aXlxe8vLxw9epVNrH/4uHhgbNnz2LmzJnw9/dHv3798PTpU6FjEdE/sJElIiohmZmZmD59Ojp27Ihhw4bh7NmzqFOnjtCxdEqtWrVw9uxZ9OrVC82bN8eyZcs40QpRCUpKSoKfnx+mTZuG33//HStXroS5ubnQsXSSWCzGiBEjEBUVBVNTU9StWxcrVqyASqUSOhoRgacWExGViMuXL2PIkCGwsLDApk2bUK9ePaEj6byzZ89i4MCBqFOnDjZt2gQ7OzuhIxEZlFOnTuHTTz9FgwYNsGHDBtja2godSa8cP34co0ePRvny5bFmzRo0btxY6EhEZRpHZImIilFWVhZmzpyJdu3a4dNPP8X58+fZxBZQ69atER4ejipVqqBBgwbYu3ev0JGIDEJ2dja++OILdO/eHV988QUOHDjAJvY9dOzYEREREejevTt8fHwwYcIEvH79WuhYRGUWR2SJiIrJ1atXMWTIEJiammLTpk1wd3cXOpLeCgoKwqhRo9C/f38sXboU5cqVEzoSkV6KioqCv78/1Go1tm3bxi/WismdO3cwevRoREVFYdmyZejTpw9EIpHQsYjKFI7IEhEVUVZWFmbNmoU2bdpAJpPhwoULbGKLyM/PD+Hh4bhz5w4aNWqEq1evCh2JSK9oNBr8+uuvaNKkCdq3b4+LFy+yiS1GderUwfHjxzF//nyMHj0a3bt3x6NHj4SORVSmsJElIiqCa9euoUmTJjh48CAuXryIWbNmwcjISOhYBsHZ2RknTpzAkCFD0KZNG/z444+cZIWoABITE9GzZ09899132LdvHxYtWgQTExOhYxkckUiEQYMGISoqCvb29nB3d8eCBQugUCiEjkZUJrCRJSJ6D9nZ2fj222/RunVr9OvXD5cvX4aHh4fQsQyORCLBzJkzcebMGWzcuBHt27dHTEyM0LGIdNaff/6JBg0awMTEBBEREWjfvr3QkQxexYoVsXbtWhw5cgRbt26Fl5cXzp07J3QsIoPHRpaIqJDCw8PRtGlT7N27F+fPn8c333zDUdgS1qRJE4SFhaFOnTrw9PREUFCQ0JGIdEpGRgbGjx+P/v37Y/78+dixYwcqVqwodKwypVWrVrh27Ro+/fRTdOnSBQEBAXj16pXQsYgMFhtZIqICUigUmDt3Llq0aIGPP/4YV65cQcOGDYWOVWaUK1cOa9euxaZNmzBu3DgMHDgQKSkpQsciElx4eDi8vb1x+fJlhIWFYciQIZx4SCBGRkaYPn06bty4gWfPnqFOnTrYvHkzOLcqUfFjI0tEVACRkZFo1qwZdu7ciZCQEMydOxfGxsZCxyqTevXqhYiICCQmJsLT0xNnz54VOhKRINRqNZYuXYqWLVuiT58+CAkJQY0aNYSORQCqV6+O/fv349dff8XMmTPRoUMH3LlzR+hYRAaFjSwR0TsolUp8//33aNasGbp27YqrV6+icePGQscq86pVq4bDhw9j0qRJ6NKlC77++mtOsEJlytOnT9GlSxcsX74cR48exZw5c3iJg44RiUTo06cPbt++DQ8PD3h5eeHbb79FZmam0NGIDALXkSUiyseNGzcwZMgQZGRkYNOmTfD29hY6EuUhMjISn3zyCczNzbF161bUqlVL6EhEJSo4OBifffYZunfvjhUrVqB8+fJCR6ICCA0NxciRI5GSkoLVq1ejY8eOQkci0msckSUi+helUokff/wRTZs2RceOHREaGsomVoc1aNAAV65cQYsWLdCoUSOsX7+e16ORQUpLS0NAQACGDRuGVatWITAwkE2sHmncuDEuXbqE8ePHo3fv3vD390d8fLzQsYj0FhtZIirT/t3w3L59Gy1btsTmzZtx8uRJzJ8/H6ampgKlo4IyNTXFzz//jF27dmHWrFno27cvXr58qb3/zp07UCqVAiYkKpzExEQkJCRof758+TK8vLxw7949hIeHY8CAAQKmo/clkUgwbtw43L59GwqFAm5ubvj111+hVquFjkakd9jIElGZ9eDBAzg5OSEiIgIqlQqLFi1CkyZN8MEHH+DatWto1qyZ0BGpkLp06aL9/+nh4YHjx4/j2rVrqF+/PpYuXSp0PKIC0Wg0+PDDD9GyZUukpaXh+++/R9u2bfHZZ5/hr7/+gpOTk9ARqYjs7e2xY8cObNu2DQsWLECrVq0QERGhvX/Hjh2IjIwUMCGR7uM1skRkEKIT0xAc9gRxSelIzVTC0lQKR2tz+HrZw7WKRZ6P6datGw4fPgwXFxdUrlwZSUlJ2LRpE1q2bFnK6am4aTQarFu3DpMmTYJEIkFqairKlSuHx48fw8rKKs/HvE8NERVEYWtr//796NevHwDA2toaFSpUwLZt2zjRnIFKT0/HvHnzsGzZMowZMwYymQzNmjVD9erVcfv27XdO4sX9FpVlbGSJSG+p1Bocvx2PdSHRCItNhlgMKFT/26UZSURQqwEvJysE+LiiY11bSMR/r614+vRpdOrUSTvTbfPmzfHXX3/B3NxckPdCJaN///7YtWsXNBoNjI2NMXHiRCxYsEB7f1FqiOhd3re2VCoVateujejoaACAWCzGnj170LNnT6HeCpWSGzduYOTIkbh69SqUSiWMjY0xe/ZszJgxI8d23G8R/Y2NLBHppdeZCgzfdAURT1KQpfzva4tMpGJ4OFTAhsHeMDcS5/igCPz9YfHatWvw9PQsydhUikJCQtCmTZsct0kkEsTGxqJatWpFqiFLUy5zQvkrSm1t3fgbxo4dm+OayYoVK+LZs2dcu7oM2LhxI0aMGKG9pt/Y2Bh3796Fs7MzgKLVFvdbZGjYyBKR3nmdqYDvqnOIe5WObFXBd2HGEhEcK5qjafIZzJ/3LSQSCdRqNcRiMWxtbbFq1SqOehiQZ8+e4ddff0VERARu3LiBR48eQalUolu3bvh9V3CRaih4dCuU54dCykNR909Xvu+P1FeJsLCwQK1ateDh4YGmTZti5MiRkEgkJZicdEHlypXx+vVriEQiZGdnAwDq1q2LW7duFbm2uN8iQ8NGloj0ikqtgWztBYQ/Ti7UgfwtY4kIrlYSNE29gI4d2sPFxQXVqlXjB8QyQK1W48GDB6hgZY0xu+8WqYY8Ha0gD2jB0/Uoh+LYP9WubIIN/p6wqVK5BBKSrjt79izu3LmD+Ph4PH36FNeuXUNCQgKi7tyF37qL3G8R/YNU6ABERIVx/HY8Ip6k5DqQJ4dsQ8q5oBy3mdVqDps+s3Lclq3S4GGKGpP9PkebelVLPC/pDrFYjFq1auHIzed51tBbKRd2IjV0P9SZb2Ba3ROVPhwHiYW19v5slQYRj1PwV1Q8OrOG6B/y2z8B/11XwN+1de9lNq4nKtG5SmmlJl3SunVrtG7dOtft+e23CnPs436LDA0bWSLSK+tCovO9LsjYrjZs+nyt/VkkzfsUqmylGutConkwL6PeVUNpEceQcn47KnefDKlVVbw6vhaJfyxAVf/5ObZjDVFe8qutgtYVwNqivPHYR5Qb15ElIr0RnZiGsNjkfO8XSaSQWFhr/4hN8156QAPgWkwyHr54UzJBSWf9Vw2lhh6AZZOPYV6nJYxtXVGp20Rkxd1Adnx0ju1YQ/Rv76qtgtYVwNqi3HjsI8obG1ki0hvBYU8gfsdeKzvhIeJWDMSTNSPw8uhqqDLT8t1WLAaCwx6XQErSZe+qIY1SgeyEhzB19tDeZmRVFZIKtsh6eifX9qwh+qf8aquwdQWwtignHvuI8sZTi4lIb8QlpedYK++fTOzdUNlmEqTW1aBMiUfy6UAk7poHW//5EIlyT2yhUGkQl5RR0pFJx7yrhlQZrwGNGhJzqxy3S8zLQ5WenGt71hD9U361Vdi6AlhblBOPfUR5YyNLRHojNVOZ731mro21fze2qQ6jyk54uiYA2c/vw8SuVp6PeZ2hKPaMpNveVUN/n3hXOKwheiv/2nq/xSFYW/QWj31EeeOpxUSkNyxNC/7dm5G1HcQm5aBMic93m/JmXE+vrHlXDUnMKgAica5RMlX661yjaW+xhuit/GrrfeoKYG3R//DYR5Q3NrJEpDccrc1hJCnY+nfKlASos95AWsEmz/vFUKO8mN9KlzXvqiGR1AjGNi7IjI3U3qZIfg5VSjxMqtXJtb2RRARHa7MSy0r6Jb/aKmxdAawt+ptKpcKpU6fw6EZosR37WFtkSNjIEpHe8PWyhzrv1QeQdHIDMuNuQpkcj8yYCCQG/wATezcYV62Z5/ZqDbBgVB+0bNkSy5Ytw9OnT0swOemKd9UQAFg26obUq/uQfuc8suOj8fLQcpg41IexrWuubVVqDXy9HEowLemTd9VWYeoKYG2VZSqVCqdPn8aYMWNgb2+PAQMGoHJaNNTqvE9RL+yxj7VFhoTXyBKR3nCtYgEvJytcjUnKdZ8yJREv9s6HKiMVEouKMHNtBKs2gyAS5f6+TgSgiUslrLoVij179mD79u2YOnUqWrZsiQEDBqBPnz6wtbUthXdEpe1dNQQAFp6doUpPxqujq6HOegNTZ09U6jo+13YiAI2dreFSuVwJJyZ98a7aKmhdAaytskitVuPcuXPYsWMHdu3aBZVKhT59+iAoKAht2rSBRCJB31/PF8uxj7VFhkSk0Wj+r737joryztsGfs0MHaXYQBEF7CUi9oYFu6KCOBBjTRQLVsxmn80mbkyM2SRrxAYqGkuMBVDBFisq9t4LKIIKKEWl12Fm3j98JRpKAAfuKdfnnOecfWAYronf+d1cc7fKXYWAiEgAR+4lYu7OG6XeGL48DPXEWDXW6b2bwicmJmLXrl0ICgrCxYsX0adPH3h5eWH06NGoXbu2KqKTmqiqGSLibFF5KRQKnD9/vqi8ymQyeHh4wNPTE71794ae3vv7mjhbRMXx0GIi0igDWlmhnY05DMp5vtBfGUjEcGxojv4t39/jam1tjdmzZ+PMmTOIjY3F8OHD8euvv6J+/foYOnQoNm/ejLS0NBW8AhJaVc0QEWeLyvJ2z+v8+fNha2sLNzc35OXl4bfffsOLFy+wdu1auLi4FCuxAGeLqCTcI0tEGicjTwb3gHOIe52DglLurVcSA4kYtrWMEebTEzWNynfVxidPniA4OBhBQUG4e/cuBg8eDC8vL4wcORI1a9as7EsggVXnDJFu4WzRuxQKBS5duoTg4GCEhIQgNzcXo0ePhqenJ/r27Qt9/fL/W3O2iN7HIktEGikjT4YpW67gdnw6CgoVZd6pUQTAQO/Np9G/Tupc6Q15dHQ0goKCEBQUhEePHmHYsGHw8vLC8OHDYWrKc440jRAzRLqBs6XblEolLl26hJCQEISEhCArKwvu7u7w9PSEi4tLhcrrX3G2iP7EIktEGkuuUCI8MgmBp2Nw41kaRFCi8J0VTV8igkIBdGhsAW9nB/RvaQWJuHKHZf3VgwcPikrts2fPMGLECHh5eWHo0KEwMjJSye+gqvfXGRKLAdk7ezqqcoZIu3G2dItSqcSVK1eK9rxmZGS8V14NDAxU9rs4W0RvsMgSkVaIScnCN5sP4ubDZ+jSqy/MjPVha2kMd6eGVXqFRqVSiTt37hSV2uTkZIwaNQpeXl4YNGiQSv94oaoVk5KFsJsJiEvNRUaurNpmiLQfZ0s7KZVKXL16FSEhIQgODkZaWhrc3Nzg6emJAQMGVMv6/3a2jp6/jucpaejfuztni3QGiywRaY2ffvoJN2/exI4dOwT5/UqlEtevX0dQUBCCg4ORnp4Od3d3eHl5ffDhZEREJLy363xwcDCCg4Px+vVruLm5QSqVYuDAgTA0NBQkV2BgIPbs2YPDhw8L8vuJhMCrFhOR1nj58iXq1q0r2O8XiUTo2LEjfv75Z8TGxuLQoUMwNzfHlClTUL9+fUyfPh0nTpyAXC4XLCMREVXM2/L6r3/9C02bNkW/fv3w/PlzrFy5EsnJydiyZQtcXV0FK7EAYGpqiuzsbMF+P5EQWGSJSGukpKSgTp06QscA8KbUduvWDX5+fnj27BlCQ0Ohr6+PTz75BDY2NkW3+lEoKn9PQCIiqhpKpRI3b97Ev//9bzRr1gx9+vRBfHw8li1bhuTkZGzduhUjRowQtLy+y8TEBDk5OULHIKpWLLJEpDVSUlIE3SNbGrFYDGdnZ6xevRoJCQnYvn07ZDIZ3N3d0ahRI/j6+uLixYvgmR5ERMJRKpW4desWvvrqK7Ro0QK9evXCkydPsHTpUiQnJ+P333/HqFGj1PKCfiyypItYZIlIawh9aHF5SCQSuLi4YN26dXjx4gV+/fVXpKenY+jQobC3t8c///lPXLt2jaWWiKgavL1g38KFC9GyZUv07NkTMTEx+PHHH5GSkoLt27fDzc0NxsbGQkctk6mpKYss6RwWWSLSGup0aHF56OvrY/Dgwdi4cSOSkpKwevVqvHjxAv369UOzZs3w1Vdf4fbt2yy1REQqpFQqcffuXfznP/9B69at0a1bNzx8+BA//PADkpOTsWPHDowePVrty+u7TExMeI4s6RxetZiItEbNmjVx8eJFtGnTRugoHyQvLw+HDh1CUFAQ9u/fj0aNGsHLywteXl5o1aqV0PGIiDTSvXv3im6V8+TJEwwfPhyenp4YNmwYTE01+1Y1kZGR6NChA/fKkk5hkSUirZCXlwdjY2MkJibCyspK6Dgqk52djYMHDyIoKAh//PEHmjVrVlRqmzZtKnQ8IiK19uDBg6Jb5cTExBSV1+HDh2t8eX1XXFwcGjVqBLlcDrGYB1ySbmCRJSKtEB8fD1tbW8hkMujp6Qkdp0pkZmZi3759CAoKwpEjR9C2bVt4eXnB09MTdnZ2QscjIlILkZGRRXteo6OjMXToUHh6esLV1RU1atQQOl6VePXqFerUqYOsrCytKuhEZWGRJSKtcOPGDQwYMACvXr0SOkq1SEtLQ1hYGIKCghAeHo4OHTrAy8sLUqkUDRs2FDoeEVG1evjwYdGe14cPH75XXmvWrCl0vCqXm5sLExMTJCcnq/1FD4lUhUWWiLTCsWPHMHv2bERFRQkdpdq9evUKe/bsQVBQECIiItCtWzd4eXlhzJgxsLa2FjoeEVGVePToUdGe1wcPHmDIkCHw9PTEiBEjYGZmJnS8aqVUKiGRSBAbG4vGjRsLHYeoWvAgeiLSCup6D9nqULt2bXh7e+P48eOIj4/H2LFjERISAltb26Jb/bx8+VLomEREHyw6Ohr//e9/4eTkhLZt2+LixYv4xz/+geTkZOzduxfjxo3TuRILACKRiFcuJp3DIktEWkHTbr1TVaysrODj44OIiAg8efIEo0aNwpYtW1C/fv2iW/2kpqYKHZOIqNweP36MH3/8ER07dkTr1q1x7tw5+Pr6IikpCfv27cP48eNhbm4udEzBmZiY8KrFpFNYZIlIK7x8+ZJF9i9sbGwwb948nD9/Ho8fP8bAgQOxZs0aWFlZwdXVFVu3bkVGRobQMYmIiomNjcXPP/+MTp06oVWrVjhz5gzmzp2LpKQkHDhwABMnToSFhYXQMdWKqakp98iSTmGRJSKtoMuHFpdHo0aN8I9//ANXrlzBgwcP0LNnT/zyyy+oV68e3N3dsXPnTmRlZQkdk4h02JMnT/C///0PXbp0QYsWLXDy5En4+PggMTERBw8exKRJk2BpaSl0TLXFPbKka1hkiUgrsMiWX5MmTfDll1/i5s2buHXrFpycnLB48WLUq1cPnp6e2L17N3Jzc4WOSUQ64NmzZ/jll1/QtWtXNGvWDOHh4ZgxYwYSExNx6NAhfPbZZ6hVq5bQMTUCiyzpGhZZItIKL1++ZJGthBYtWuA///kP7t27h8uXL6NFixb48ssvUa9ePYwbNw779u1Dfn6+0DGJSIvExcVh2bJl6N69O5o0aYIjR47A29sbL168wOHDh1leK8nU1JRFlnQKiywRaQVe7OnDtW3bFosXL0ZUVBROnz4NW1tbzJs3D1ZWVpg8eTIOHToEmUwmdEwi0kDx8fFYvnw5evToAQcHh6K9rS9evMDRo0cxdepUruEfiFctJl3DIktEWoGHFquOSCSCk5MTfvzxR8TExODo0aOoXbs2pk2bBmtr66Jb/RQWFgodlYjUWEJCAlasWIFevXrB3t4eBw4cwKRJk/D8+XMcO3YM3t7eLK8qxEOLSdewyBKRxpPL5Xj9+jWLbBUQiUTo0qULfvnlFzx9+hT79u2DsbExJkyYgAYNGhTd6kculwsdlYjUwPPnz7Fq1So4OzvDzs4Oe/fuxfjx45GQkIDjx49j+vTpXKurCA8tJl3DIktEGi81NRUKhYKf7FcxsViMnj17YuXKlYiPj0dwcDCUSiXGjBlTdBjy+fPnoVAohI5KRNUoMTER/v7+6NOnDxo3bow9e/bgk08+QXx8PE6cOIEZM2agXr16QsfUejy0mHQNiywRabyUlBQYGxvD1NRU6Cg6QyKRoG/fvlizZg1evHiBLVu2IDs7G66urrCzsyu61Y9SqRQ6KhFVgaSkJAQEBKBv375o2LAhgoOD4enpibi4OJw8eRIzZ86ElZWV0DF1Cg8tJl3DIktEGksmk+HJkyd49uwZ98YKSE9PDwMHDsSGDRuQmJiItWvXIiUlBQMGDEDTpk2LbvXDUkuk2ZKTk7F27Vq4uLigYcOG2LlzJzw8PPDs2TNERERg1qxZsLa2FjqmzjIxMUFGRgZSUlK4Z5Z0gkjJvyyISEOtWLEC8+fPB/DmXM5atWqhQ4cOOHr0qLDBCACQl5eHI0eOICgoCPv27YONjQ28vLzg5eWFNm3aCB2PiMohJSUFe/bsQXBwME6fPo2uXbvC09MTHh4esLGxEToeAfj666/x888/v3dV+S5duuDSpUsCpiKqetwjS0Qaa/jw4RCL3yxjSqUSaWlpqFmzpsCp6C0jIyOMGjUK27dvR3JyMpYsWYIHDx6gc+fORbf6efjwodAxiegvXr58ifXr12PgwIFo0KABfvvtN4wcORKxsbE4e/Ys5s6dyxKrRnr16vXetQmMjIzg6ekpYCKi6sE9skSk0Xr27Inz588DeLPxjomJQf369QVORWXJysrC/v37ERQUhEOHDqF169ZFe2rt7e2Fjkekk169eoXQ0FAEBwfj5MmT6NSpEzw9PYsu5kbqS6lUwtnZGefPn4dSqYSxsTESExNhZmYmdDSiKsU9skSk0Xx9fSGRSCCRSPDDDz+wxGqAGjVqYOzYsQgLC0NycjJ8fX1x5swZtGjRouhWP3FxcULHJNJ6r1+/xsaNGzFkyBBYW1tjw4YNGDJkCB4/fowLFy7A19eXJVYDiEQirF69GiKRCCKRCNOnT2eJJZ3APbJEpNFkMhlMTU1hZmaGxMRE6OnpCR2JKun169cIDQ1FUFAQTp48iS5dusDLywtSqZQfUBCpSGpqKsLCwhAcHIzw8HA4OTlBKpVizJgxsLOzEzoefQBnZ2ecPXsWz5494wcQpBNYZIlI4/33v/+Fo6Mjhg0bJnQUUpGUlBTs3r0bQUFBOHv2LHr27AkvLy94eHjwfpREFZSWloa9e/ciODgYx44dg6OjY9FhwzycX3s8evQIGzZswE8//SR0FKJqwSJLRBopJiULoTcSEJeag8y8QtQ00oOtpQncnWzgULeG0PFIhV68eIFdu3YhKCgIly5dQt++feHl5YXRo0ejVq1aKv99nC2qKtU5W+np6UXl9ejRo2jXrh2kUimkUikcHBxU+rtIeFy3SBexyBKRxpArlDj+IAnrz8TgxrM0iMWATP7nEqYvEUGhAJwaWcDb2QEDWllBIhYJmJhULS4uDiEhIQgKCsKNGzcwYMAAeHl5wc3NDebm5pV+Xs4WVZXqnK2MjAzs27cPwcHBOHLkCNq0aQNPT09IpVI0adJEVS+J1ATXLdJ1LLJEpBEy8mSYsvkKbiekI79Q8bePN9QTo11Dc2yc1Bk1jfSrISFVt9jYWAQHByMoKAj37t3DkCFD4OXlhREjRrx3G6aIiAgcPnwYS5YsKbpd07s4W1RVVDlbmZmZWLhwIRYuXIjatWv/+TsyMrB//34EBwfj8OHDaN26dVF5bdq0qcpfE6kHrltELLJEpAEy8mRwDziHuNc5KJCXf8kykIhgW8sEoT49YcYNt1Z7+PBhUamNjo7G8OHD4eXlheHDh2PkyJEIDw/HZ599hvXr179XZjlbVFVUOVuZmZlwcXHB1atXsXr1akycOBEHDhxAcHAwDh06hBYtWhSV1+bNm1fVSyI1wXWL6A0WWSJSa3KFEh8HXsCt+LQKbbDfMpCI4GhrgZ3e3XlIlY64f/8+goKCEBQUhLi4OOTm5kKpVMLQ0BATJ07EunXrIBKJOFtUZVQ5W7k52ejXrx/u3LmD/Px8WFhYIC8vD82aNSsqry1atKiCV0HqiOsW0Z9YZIlIrR25l4i5O28UO3Qq7cw2pJ/b8d7XjJt1Qz2Pr4s9h6GeGKvGOmFQa+sqzUrqRalU4rvvvsP333+PwsJCAIBYLMbQoUOxf/9+HL2fVOJsvZV+IQSZ1/ZDkZcNIztH1B4yB5Ialu89hrNFJVHVuvVf12b457ghiI2NhULx5rlEIhFOnTqF3r17V90LILVV2mwB5VuzAK5bpD14w0UiUmvrz8SUWjQM6jdHPY+FRf+/SK/kQ6UKChVYfyaGG20dIxKJcPLkSSiVSpiamiI/Px+FhYU4cuQI4uPjsf5MXKmzlXX7GNLPB6GO6wLoWVjj9fFApOz9CdbjfnzvcZwtKomq1q2f91/D48ePIRKJYGRkBLFYjLy8PNy6dYtFVkeVNlvlXbMArlukPVhkiUhtxaRk4caztFK/L5Lolfhp818pAVx/mobYl9mwr2OquoCk9iZPnoyhQ4eiSZMmRf9nZmb2/2frTqk/l3ntAGp2GgmTFj0AALWHz8fztVNRkBQDA6s/b13C2aK/UuW6laKoiYcvUiHOfoXo6GhER0cjKioKbdq0UV1g0hhlzVZ51yyA6xZpDxZZIlJboTcSIBYDcnnJ3y9IjkXcqvEQG5jAyN4JFr0nQGJU8v3yxGIg9EY8FgzkuWS6ZPLkySV+vazZUhbKUJAcC4t+nxZ9Td/CGhJzK+Q/jyr2RyFni96l6nVr/50kLBjYAk2aNMHgwYOrMDmpu9Jmq6JrFsB1i7QDiywRqa241Jz37on3LkOblqhTzxd6lg1QmJ6EtIgtSNm1GFbjfoRIVPwCFjK5EnGpuVUdmTREWbMlz80AlApITCze+7rExAzynLRij+ds0bu4blFVKW22KrpmAZwt0g4sskSktjLzCkv9nrFDx6L/bVDPDvp1GuH5Om8UJEbDsH6zEn8mI1em8oykmcqarTcH3lUMZ4ve4rpFVaX02arcdVs5W6Tpit8ZnohITdQ0Kv9nbfqW9SE2NEVhelKpjzEz5n3z6I2yZktibA6IxMX2ZMhzMort8XiLs0Vvcd2iqlLabFVmzQI4W6T5WGSJSG3ZWppAX1K++9wVpidDkZ8NPfN6JX5fXyKCraWxKuORBitrtkR6+jCoZ4+8dy4GJUtLhDw9CYYNip9Pxtmid3HdoqpS2mxVdM0COFukHVhkiUhtDWxmDrmi5EOmUk9uRF7cPRSmJSHv6W2khP4AQ5uWMLBuWuLj5Qol3J0aVmVc0iDuTjZQlHx3FABAzQ7DkXl1H3KizqMgKQav/lgJw4ZtSrxoCmeL3lXWbHHdog9R1mxVZM0COFukHXiOLBGpnczMTKxevRpLly6F1bifkGNav9hjCtNT8DLsR8hzMyGpUQvGDh1g0XsCRKLin8+JAHRsbMnbDFARh7o14NTIAlefppb4/RqOgyDPScPro2ugyM+GUWNH1B46t9jjOFv0V2XNVkXWLSgVKHjxCNvW3sDcuXNhZmZWDelJnZU1W+VdswCuW6Q9REqlsnJniBMRqVh2djb8/f3x888/o3nz5vj222+haNAWc3feLPEG8OVlqCfGqrFOvPk7vefIvUTM3XmDs0Uqp6rZmtpajH0B3+Pu3bvw9fXFvHnzYG5ursKkpGm4bhH9iYcWE5HgcnJy8Msvv8De3h67d+/G77//jnPnzmHgwIEY0Moa7WzMYVDOc86KURSiTf0a6N/SSrWhSeMNaGX1QbNlIBHDsaE5Z4uK6d3EEobZSZCIKrev4O1sLfAahNOnT2P37t0IDw9H48aNsWjRIqSlpak2MGkMrltEf2KRJSLB5ObmYvny5XBwcMCOHTuwadMmXLx4EUOGDCm6p6JELMKvkzvDtpZJhTfcBhIR9PMzkLD9a2Skp1XBKyBN9mGzJYZtLWP8OqkzJOJKfshCWqmgoABjP/aC+Ow62Foaf/BsiUQiuLi4ICIiAmFhYYiIiICdnR2++eYbpKaWfGg8aS+uW0R/YpElomqXl5eHVatWoUmTJtiyZQsCAwNx5coVDB8+vKjAvsvMSB+hPj3haGsBQz0x/m7zK8KbQ6fa21rgwrfusK5tARcXF6SkpFTJ6yHNVfnZMkeYT0/UNOLtK+hP+fn5GDNmDOLj43Hi8EHsm9NbpbPVt29fnDx5Evv27cPZs2dhZ2eHhQsX4vXr11Xyekg9cd0ieoPnyBJRtcnPz8fGjRuxZMkS1KpVC99++y1GjRoFsbh8n6nJFUqERyYh8HQMbjxLg1gMyOR/LmH6EhEUCqBDYwt4Ozugf0srSMQiyGQyTJgwAbdv38axY8dgY2NTVS+RNFRlZ4vorbclNjExEUePHoWlpSWAqp2ts2fP4ttvv8WlS5cwZ84cLFiwALVr166S10fqh+sW6ToWWSKqcgUFBdi8eTOWLFmCGjVqYNGiRfDw8Ch3gS1JTEoWwm4mIC41Fxm5MpgZ68PW0hjuTg1LvBKjXC6Ht7c3IiIiEB4eDjs7uw94RaTNKjpbRHl5efDw8EBKSgqOHj0KCwuLEh9XVbN1/vx5fPvttzh//jxmz56Nzz//HHXq1Kn085Hm4bpFuohFloiqjEwmw2+//Ybvv/8eRkZG+OabbyCVSiGRSATJo1AoMH/+fISGhiI8PBzNmzcXJAcRaY+8vDyMHj0ar1+/xpEjRwS9qvCFCxfw3Xff4cyZM5g1axb+8Y9/oG7duoLlISKqSjxHlohUrrCwEFu2bEHLli3x008/4fvv39w+4uOPPxasxAKAWCzGihUrMH78ePTu3Rt37twRLAsRab68vDy4ubkhNTVV8BILAN27d8ehQ4cQHh6Ou3fvwt7eHl988QWSkpIEzUVEVBVYZIlIZeRyOX7//Xe0bt0aixcvxjfffIP79+9j3LhxghbYd4lEIvz3v//FnDlz0LdvX1y9elXoSESkgXJzczFq1ChkZGSoRYl9V9euXXHw4EGcPHkSkZGRcHBwwOeff47ExEShoxERqQyLLBF9MLlcjh07dqBNmzZYuHAhvvzySzx48AATJ06Enp6e0PFK9NVXX2HhwoXo378/zp49K3QcItIgOTk5GDlyJLKzs3HkyBGYmZkJHalEnTt3xv79+3H69GlER0fDwcEBvr6+ePHihdDRiIg+GIssEVWaQqFAcHAwPvroI/zrX//C559/jqioKHz66afQ11f/y/vPnz8fS5cuxdChQ3Hs2DGh4xCRBnhbYvPy8nDo0CHUrFlT6Eh/q2PHjti7dy/Onj2L2NhYNGnSBPPmzcPz58+FjkZEVGksskRUYQqFArt374ajoyM+//xzzJ07F48ePYK3tzcMDAyEjlch3t7eWLt2Ldzd3bF//36h4xCRGsvOzoarqysKCgo0psS+q0OHDggLC8P58+cRFxeHJk2aYM6cOUhISBA6GhFRhbHIElG5KZVKhIWFoUOHDpg7dy5mzJiB6OhozJgxQ+MK7LvGjRuHrVu34uOPP0ZQUJDQcYhIDb0tsQqFAn/88Qdq1KghdKRKa9++Pfbs2YNLly7hxYsXaNq0KWbNmoW4uDihoxERlRuLLBH9LaVSiQMHDqBTp06YOXMmPv30U0RHR2PWrFkwNDQUOp5KuLu7Y/fu3ZgyZQo2btwodBwiUiPZ2dkYPnw4AODgwYMaXWLf1a5dO+zatQuXL19GSkoKmjdvjpkzZ+LZs2dCRyMi+lssskRUKqVSiUOHDqFr166YMmUKxo0bh8ePH2PevHkwNjYWOp7KDRkyBAcPHoSvry9Wr14tdBwiUgNZWVkYNmwYJBIJDh48CFNTU6EjqdxHH32E4OBgXL16FampqWjevDmmT5+Op0+fCh2NiKhULLJEVIxSqcTRo0fRvXt3TJw4EVKpFDExMViwYAFMTEyEjlel+vTpg6NHj+I///kPfvzxR6HjEJGAMjMzMXToUOjr62P//v1av/61adMGO3fuxPXr15GZmYkWLVrA29sbsbGxQkcjIiqGRZaIiiiVSoSHh8PZ2Rljx46Fm5sbYmNj8cUXX2jlXojSdO3aFSdPnsSyZcvw9ddfQ6lUCh2JiKrZ2xJrbGysEyX2Xa1bt8b27dtx8+ZN5ObmolWrVpgyZQpiYmKEjkZEVIRFlogAABEREejbty/GjBmDoUOHIjY2Fv/617+05lywinJ0dMTp06exadMmLFiwgGWWSIdkZGRgyJAhMDU1xd69e7XyVIryaNmyJX7//Xfcvn0bMpkMrVu3LrpGAhGR0FhkiXTc2bNn4eLiglGjRsHFxQVPnjzBV199BTMzM6GjCa5ly5Y4c+YMwsLCMH36dMjlcqEjEVEVS09Px+DBg2FmZqbTJfZdzZs3x2+//YY7d+5AqVSibdu2mDRpEh49eiR0NCLSYSyyRDrqwoULGDRoEIYNG4ZevXohNjYW33zzDczNzYWOplYcHBxw5swZnD59GpMmTUJhYaHQkYioirwtsZaWlggNDYWRkZHQkdRKs2bNsHnzZty7dw8SiQQfffQRJkyYgKioKKGjEZEOYpEl0jGXL1/G0KFDMWjQIHTu3BmxsbH47rvvYGlpKXQ0tdWwYUNERETgzp078PT0RH5+vtCRiEjF0tLSMGjQINSpU4cl9m80adIEGzduxP3792FoaAhHR0eMGzcOkZGRQkcjIh3CIkukI65duwZXV1e4uLjA0dERsbGxWLJkCWrXri10NI1gZWWFkydPIj4+Hm5ubsjJyRE6EhGpSGpqKgYOHIh69eph9+7dWnN/7Krm4OCADRs2IDIyEqampmjfvj3Gjh2L+/fvCx2NiHQAiyyRlrtx4wZGjRqF3r17o2XLloiJicGPP/6IOnXqCB1N49SqVQvHjx9HTk4Ohg0bhszMTKEjEdEHelti69evj127drHEVoKdnR0CAwMRFRUFc3NzdOjQAR9//DHu3bsndDQi0mIsskRa6vbt2xg9ejR69uwJBwcHPH78GEuXLkW9evWEjqbRzMzMcOjQIRgaGmLAgAFITU0VOhIRVdLr168xYMAANGzYkCVWBRo3boy1a9fi4cOHqFWrFjp16gRPT0/cuXNH6GhEpIVYZIm0zL179+Dp6YmuXbuiYcOGiI6Ohp+fH6ytrYWOpjVMTEywb98+1K9fH/369UNycrLQkYiogl69eoX+/fujUaNGCA4OhoGBgdCRtEajRo0QEBCAhw8fol69eujSpQvGjBmD27dvCx2NiLQIiyyRlnjw4AHGjh2LTp06oW7dunj06BFWrlyJBg0aCB1NKxkaGiIkJAStW7dGnz59kJCQIHQkIiqnly9fon///nBwcGCJrUK2trZYvXo1oqOj0aBBA3Tt2hWjR4/GzZs3hY5GRFqARZZIwz18+BDjx4+Hk5MTzMzMEBUVBX9/fzRs2FDoaFpPX18fW7duRa9eveDs7IzY2FihIxHR33hbYps2bYqdO3dCX19f6Ehaz8bGBitXrsTjx4/RqFEj9OjRA25ubrh+/brQ0YhIg7HIEmmo6OhoTJo0Ce3atYORkREiIyOxbt06NGrUSOhoOkUikSAwMBAjR46Es7Mz76dIpMZSUlLg4uKCFi1aYMeOHSyx1axBgwZYvnw5Hj9+DAcHB/Tq1QsjR47EtWvXhI5GRBqIRZZIw8TExOCzzz5D27ZtIRaLce/ePWzYsAF2dnZCR9NZIpEIfn5+mDRpEnr37s3zwIjUUHJyMlxcXNC6dWts376dJVZA9evXx7JlyxATE4PmzZujd+/ecHV1xZUrV4SORkQahEWWSEM8ffoU3t7eaN26NQoLC3Hnzh1s2rQJTZo0EToa4U2ZXbJkCebPn4++ffvi8uXLQkciov8vKSkJ/fr1Q9u2bfH7779DT09P6EgEwNraGkuXLkVsbCxat26Nfv36YdiwYbh06ZLQ0YhIA7DIEqm5uLg4zJw5Ey1atEBOTg5u3bqF3377Dc2aNRM6GpXgyy+/xKJFizBw4ECcPn1a6DhEOi8xMRH9+vVD+/btsXXrVpZYNVSvXj38/PPPiI2NRbt27dC/f38MGTIEFy5cEDoaEakxFlkiNZWQkIDZs2ejefPmSE1NxfXr17Ft2za0aNFC6Gj0N+bOnYtly5Zh+PDhOHLkiNBxiHRKVlYWFAoFAODFixfo168fOnbsiN9++40lVs3VrVsXP/74I548eYIOHTpg0KBBGDRoEM6dOyd0NCJSQyyyRGrmxYsXmDdvHpo1a4bExERcvnwZO3fuROvWrYWORhUwZcoUBAYGYvTo0QgLC0NycjLat2+PgwcPCh2NSKt169YNrq6uePr0Kfr164fOnTtj8+bNkEgkQkejcqpTpw5++OEHPHnyBF26dMHQoUMxYMAAnDlzRuhoRKRGREqlUil0CCJ6cw7XTz/9hLVr12Lw4MFYtGgRHB0dhY5FH2jv3r0YO3YsLCwskJiYCBcXFxw/flzoWERaKTIyEm3btoWenh4kEgk8PDywadMmllgN9/r1a/j5+WHlypXo2LEjvvnmG/Tp00foWEQkMO6RJRJYSkoK/vnPf8LBwQHR0dE4e/YsQkNDWWK1hIuLC6ytrfHixQsolUqcOnUKKSkpQsci0kpBQUHQ09NDfn4+8vPz8fr166LDjElz1apVC4sXL8aTJ0/g7OyMUaNGoW/fvjh16pTQ0YhIQNwjS1QBMSlZCL2RgLjUHGTmFaKmkR5sLU3g7mQDh7o1yvzZffv24fXr15g8eTIA4NWrV1i6dClWrVqFPn36YNGiRejcuXM1vAqqTh4eHggNDcXbpVZfXx/Lli3D7Nmz33vch8wWkbYr7/ujUaNGiIuLAwCIxWIoFAps3boV48ePFyo6VYG0tDSsWLECy5cvR7t27fDNN9+gX79+EIlEAAB/f3+4ubnBxsamzOfhukuk2Vhkif6GXKHE8QdJWH8mBjeepUEsBmTyP982+hIRFArAqZEFvJ0dMKCVFSRi0XvPERkZifbt20MkEuHu3bvYtGkTVqxYgZ49e2LRokXo1q1bdb8sqibnzp3DmjVrsH//fuTm5kImkxXtoVXFbBFpq4q+P0xSo9Hb2RlisRimpqbw8PDAJ598gn79+vEiT1oqPT0dK1euhJ+fH9q0aYNvvvkGNWvWRLdu3eDk5IRLly4Vu18w110i7cEiS1SGjDwZpmy+gtsJ6cgv/PvD0wz1xGjX0BwbJ3VGTaM3G8/c3Fw4Ojri8ePHEIvFEIvFcHZ2xrfffouePXtW9UsgNSGXy3Hx4kUEBgbixIkTuPco5oNni0hbVWbttTcTI/Pgz/j26y/Rt29fllcdkpGRgVWrVmHZsmUoLCxEZmYmDA0NMWPGDPj5+f35OBVs04lIfbDIEpUiI08G94BziHudgwJ5+d8mBhIRbGuZINSnJ8yM9DFhwgTs3LkThYWFAN4cWvr8+XPUqVOnqqKTmlPVbBFpI74/qLJOnTqF/v37F50XLZFIsGfPHowcOZJzRaSFWGSJSiBXKPFx4AXcik97b4OXdmYb0s/teO+xxs26oZ7H1+99zUAigqOtBXrlX8P8eXMhEolgaGgIiUSC3NxcLFq0CAsXLqyW10LqpbTZeiv9Qggyr+2HIi8bRnaOqD1kDiQ1LIu+/3a2dnp35+FupHVKen+Ud90F+P7Qda6urjh06BAkEgkUCgXkcjnEYjGiHj7Cl8cSue4SaRked0NUguMPknA7Ib3EDZ5B/eao5/FnCRXpFf+EtkCuxO34dAzu0RVffPEFmjZtColEApFIBJFIhF69elVpflJfZc1W1u1jSD8fhDquC6BnYY3XxwORsvcnWI/7segxb2crPDIJg1pbV2d0oipX2vujPOsuwPeHrpsyZQq6du1adNEnmUyGR48e4dZLBdddIi3EIktUgvVnYko9f0Yk0Xvvk9rSFBQqcORZIUJ+/lnV8UiDlTVbmdcOoGankTBp0QMAUHv4fDxfOxUFSTEwsHIoelxBoQLrz8TwDyrSOqW9P8q77gJ8f+gyd3d3uLu7F/v6mLXnue4SaSHeR5boL2JSsnDjWVqp3y9IjkXcqvFIWDcNr46ugTwvq8THKQFcf5qG2JfZVROUNE5Zs6UslKEgORZGjdsVfU3fwhoScyvkP496/7HgbJH2Kev9Ud51F+D7g97HdZdIe7HIEv1F6I0EiEt5ZxjatESd4b6w8voeli5TkP/sDlJ2LUZpp5qLxUDojfgqTEuapKzZkudmAEoFJCYW731dYmIGeU5ascdztkjblPb+qOi6C/D9QX/iukukvXhoMdFfxKXmvHdPuXcZO3Qs+t8G9eygX6cRnq/zRkFiNAzrNyv2eJlcibjU3CrLSpqlrNl683l/+XG2SNuU9v6o6LoL8P1Bf+K6S6S9uEeW6C8y8wrL/Vh9y/oQG5qiMD2p1Mdk5MpUEYu0QFmzJTE2B0TiYnsB5DkZxfYWvMXZIm1S3rW3POsuwPcHvcF1l0h7scgS/UVNo/IfqFCYngxFfjb0zOuV+hgzY953jt4oa7ZEevowqGePvGd3ir4mS0uEPD0Jhg1alPgznC3SJuVde8uz7gJ8f9AbXHeJtBcPLSb6C1tLE+hLRCUeipR6ciOMm3aFXs06KExPQurJjTC0aQkD66YlPpe+RARbS+OqjkwaoqzZAoCaHYbjdfh6GFo1eXMbiPANMGzY5r0rZ77F2SJtU9r7o6LrLsD3B/2J6y6R9mKRJfr/lEolwsPDcWzDVhTYe0AklhR7TGF6Cl6G/Qh5biYkNWrB2KEDLHpPgEhU8sENcoUS7k4Nqzo6aQh3JxsEnHpc6vdrOA6CPCcNr4+ugSI/G0aNHVF76NwSH8vZIm1T2vujousuAMgK5TB7HQW5/M09vEl3cd0l0l4iZVmX/SPSARkZGdiyZQv8/f3x6tUrTJs2DbfrDcCdxJwPel4RgE52lgiZ3kM1QUkrjFl7Hlefpn7Qc3C2SFup6v1RT5yF57/9A2KxGNOmTcPUqVNhZWWlmpCkcbjuEmknniNLOuvevXvw8fFBgwYNsG3bNnz11VeIi4vDkiVLMHtAKxjqfdjbw0BPDG/n4ocmkW7zdnbgbBGVQlXvj8Xj+iA2NhYrVqxAREQEGjVqhLFjx+LMmTNl3raHtBPXXSLtxCJLOqWwsBC7d+9Gv3790KlTJ+Tk5ODkyZO4ePEiJkyYACMjIwDAgFZWaGdjDgOJqFK/x0AihmNDc/RvyT0A9D7OFlHpVPn+0NPTg5ubG44ePYq7d++iQYMGGDlyJNq1a4c1a9YgMzNTxelJXXHdJdJOPLSYdEJSUhLWr1+PdevWQSwWw8fHB1OmTEGdOnVK/ZmMPBncA84h7nUOCkq9B11x+hIRGtUyQZhPT9Q04tUNqbjKzpaBRAzbWsacLdJqGXkyuPmfw9OXmZBX4PP28rw/cnJyEBQUBH9/f0RFRWHixImYOXMm2rZtq6r4pKa47hJpH+6RJa2lVCpx4cIFjB8/Ho0bN8bp06fh7++PmJgY/N///V+ZJRYAzIz0EerTE462FjDUE+PvPscVARAr5dBLj0fozB7c4FGpKjNbhnpitLc15x9TpPXMjPQxRHwHipexKn9/mJiY4NNPP8XVq1dx4sQJ5OTkoHPnzujduzd27tyJgoIClb4WUh9cd4m0D/fIktbJzc3Fzp07sXr1akRHR2Py5Mnw8fFBixYl3xPu78gVSoRHJiHwdAxuPEuDWIz3LuOvLxFBoQA6NLbAWCcrzHJzxs8//YQJEyao6iWRlqrIbHk7O6B/SytIxJU7NI5IU7x48QItW7bErt27oWzQtsrfH69evcLmzZuxdu1aZGRkwNvbG9OmTUOjRo1U+bJITXDdJdIeLLKkNWJjY7FmzRr8+uuvqF+/PmbPno3x48ejRo0aKvsdMSlZCLuZgLjUXGTkymBmrA9bS2O4OzWEfR1TAMCePXvg7e2NBw8eoF69eir73aTdyjNbRLpg3LhxkMvl2LlzZ9HXquP9oVAocPz4cQQEBOCPP/7A0KFD4ePjg4EDB0Is5gFs2ojrLpFmY5EljaZQKHDs2DGsXr0aR48exYgRIzB79mz06dMHIpFwn6B6eHjAwMAAO3bsECwDEZGmOXXqFEaMGIHIyEjY2NgIluPZs2dYv3491q9fjxo1amDmzJmYPHkyateuLVgmIiJ6H4ssaaS0tDRs3rwZAQEBSE9Px7Rp0zB9+nQ0bKgeNyp/8eIFWrduja1bt8LV1VXoOEREak8mk6F9+/b47LPP8PnnnwsdBwBQUFCA0NBQBAQE4PLly/j444/h4+ODzp07Cx2NiEjn8VgZ0ih37tzBjBkzYGNjg+DgYCxatAjPnj3D4sWL1abEAkD9+vWxdOlSzJw5ExkZGULHISJSe8uXL4dIJMLcuXOFjlLEwMAAXl5eiIiIwJUrV2BiYgIXFxd06tQJGzduRE5OjtARiYh0FvfIktqTyWQIDQ2Fv78/Ll++jE8++QSzZs1Chw4dhI5WJqVSiQEDBqBFixYICAgQOg4RkdqKj49Hq1atcPDgQfTu3VvoOGXKzMzE77//joCAAMTHx+PTTz/FjBkz0Lx5c6GjERHpFBZZUlsvXrwouveroaEhfHx88Omnn2rUOUqPHz9Gu3btcPjwYTg7Owsdh4hILUmlUhgZGWHr1q1CRyk3pVKJc+fOISAgALt370afPn3g4+MDV1dX6OnpCR2PiEjrsciSWlEqlTh//jxWr16NPXv2wMXFBbNmzcLQoUMhkUiEjlcpS5cuxfr163Hr1i0YGRkJHYeISK0cPXoUnp6eiIyMhLW1tdBxKiUpKQm//vor1q5dC6VSiWnTpmHq1KmoX7++0NGIiLQWz5EltZCTk4MNGzbAyckJw4cPh7W1Ne7evYtDhw7B1dVVY0ssAMyfPx81a9bE4sWLhY5CRKRW8vPzMXv2bCxevFhjSywAWFlZ4d///jdiY2Ph7++Pc+fOwc7Oruj8Wu4zICJSPe6RJUE9fvwYAQEB2LhxI2xtbTF79myMGzcOpqbadf+2W7duoXv37rhw4QIcHR2FjkNEpBaWLFmCXbt24cqVK1p3OG50dDTWrVuHjRs3wtraGjNnzsSECRNgbm4udDQiIq3AIkvVTqFQ4PDhw/D398fx48fh5uaG2bNno1evXoLe+7Wqff311zh8+DAuXryodX+wERFV1JMnT9CmTRscP34c3bt3FzpOlcnNzUVwcDACAgJw7949jB8/HjNnzuSHmkREH4hFlqpNamoqNm3ahICAAGRnZ2P69OmYNm0aGjRoIHS0apGXl4f27dtj6tSp+Mc//iF0HCIiQY0aNQp16tTBr7/+KnSUanPt2jWsWbMG27dvR4cOHeDj4wMPDw8YGhoKHY2ISOOwyFKVu3nzJvz9/bFt2zZ07NgRs2bNwujRo2FgYCB0tGp39uxZDB48GLdv30aTJk2EjkNEJIgDBw5g4sSJiIqKQt26dYWOU+1SU1OxZcsWBAQEIC0tDVOmTMH06dNhZ2cndDQiIo3Biz1RlSgoKMDOnTvRq1cv9OjRAyKRCBcuXMCZM2fw8ccf62SJBYBevXph8uTJmDZtGi/+QUQ6KTc3F3PnzsUPP/ygkyUWACwtLTF//nxERkZi+/btiIqKQvPmzTFixAgcOnQICoVC6IhERGqPe2RJpZ4/f45169YhMDAQpqamRfd+tbS0FDqa2sjIyECbNm2waNEiTJkyReg4RETV6ptvvsEff/yBixcvavQV6VUtPj4e69evR2BgIExMTDBjxgx8+umnqFOnjtDRiIjUEossfTClUokzZ87A398fYWFhGDhwIGbNmoXBgwdDLOZO/5IcPHgQ48aNw/3793XmHGEioujoaLRr1w4RERHo3Lmz0HHUkkwmQ1hYGAICAnDhwgV4enrCx8cHXbt21eoLIhIRVRSLLFVadnY2tm3bhtWrVyM+Ph6fffYZZs6cyXM/y+mTTz5BXl4e9uzZI3QUIqIqp1QqMWzYMNjZ2WHNmjVCx9EI9+/fx5o1a7BlyxY0bdoUPj4+GDt2rNbdoo6IqDJYZKnCHj16hICAAGzatAn29vaYPXs2xo4dCxMTE6GjaZSUlBS0atUK69atg4eHh9BxiIiq1J49ezB9+nRERUWhVq1aQsfRKFlZWdi2bRsCAgLw9OlTTJ48GTNmzEDLli2FjkZEJBge90nlIpfLceDAAQwZMgRt27ZFUlISDh48iOvXr2PKlCkssZVQt25drFixArNnz0ZqaqrQcYiIqkx2djbmz5+Pn376iSW2EmrUqIHp06fj5s2b+OOPP/Dy5Uu0b98e/fv3x+7duyGTyYSOSERU7bhHlsr06tUrbNy4EQEBASgoKMCMGTPg7e0Na2troaNpBaVSieHDh6N+/fo6dS9FItItX375JSIiInD27FleO0FFUlJSsHHjRqxZswYymQzTpk2Dt7c3r7tARDqDRZZKdP36dfj7+2P79u3o2rUrZs2aBTc3N+jr6wsdTes8e/YMbdu2RWhoKPr37y90HCIilYqMjISTkxMuXLiA9u3bCx1H68jlchw+fBgBAQE4fvw4Ro4cCR8fH/Tt25cXhyIircaPRalIfn4+tm3bhh49eqB3794wMDDA5cuXcerUKUilUpbYKtKoUSP88MMPmDZtGrKzs4WOQ0SkMkqlErNmzcK0adNYYquIRCLB8OHDcfDgQTx48AAODg6QSqVo3bo1Vq5cibS0NKEjEhFVCe6RJcTHxxfd+9Xc3ByzZs3CpEmTYGFhIXQ0naFQKODs7Ixu3brhl19+EToOEZFK7Ny5E/Pnz0dUVBTMzc2FjqMz8vLyEBISgjVr1uDWrVsYN24cfHx8+GECEWkV7pHVUUqlEidPnsSYMWPg4OCAW7duYevWrYiMjMS8efNYYquZWCzGhg0bsGbNGly5ckXoOEREHywzMxMLFizA0qVLWWKrmZGRESZMmIDz58/j7NmzAICePXuiR48e+P3335GXlydwQiKiD8c9sjomKysLW7duxerVq5GYmIipU6dixowZsLe3FzoaAfj+++8RHByMq1evwsDAQOg4RESV9vnnn+PatWs4efIkz9VUA2lpafjtt98QEBCAly9fYsqUKZg+fTocHByEjkZEVCkssjoiKioK/v7+RTdVnzNnDry8vGBsbCx0NHpHQUEBOnXqBE9PT3z99ddCxyEiqpDs7GyYmprizp076NKlC65evYo2bdoIHYve8faIrDVr1mDfvn0YMGAAfHx8MGTIEEgkEqHjERGVG4usFnt771d/f3+cPn0aUqkUs2bNQteuXfnpuBq7cuUK+vTpg/DwcKxatQoGBgbYvHmz0LGIiMp0+/ZtdOzYEXPnzsXFixfRvXt3LF26VOhYVIaEhARs2LABgYGB0NfXx4wZMzBlyhTUrVtX6GhERH+LRVYLvXz5Ehs2bMDatWshl8sxY8YMTJ06FVZWVkJHo3JQKpUYMWIEDh8+DACwsbHB06dPBU5FRFS2/fv3QyqVQqFQQC6XY/PmzRg/fjw/ONUAMpkM+/btQ0BAAM6dO4cxY8bAx8cH3bt3578fEaktXuxJi1y9ehWTJ0+Gra0tjh49imXLliE2NhZfffUVS6yGkMvlcHV1xbFjxyCXyyGXy5GcnCx0LCKiv5WSkgKJRAKZTAaFQoGJEydi4cKFQseictDX14eHhwfCw8Nx8+ZN1KlTB8OGDYOTkxMCAwORlZUldEQiomJYZDVcXl4etm7diq5du6Jfv34wNTXF1atXceLECYwePRp6enpCR6QKKOkAiby8PGRkZAiQhoio/F6+fIn8/HwAb66a6+joiPHjxwuciiqqZcuWWL58ORISEjB79mysWbMGNjY2mDNnDu7fvy90PCKiIiyyGurZs2f497//DVtbWyxZsgQTJkxAQkIC/P39eWENDaanp4eDBw8iLCwMDRs2LLrwRlxcnMDJiIjKdvfuXcjlchgaGmLp0qW4fv06WrZsKXQsqiRTU1NMnToV169fx5EjR5Ceno4OHTqgX79+CAkJgUwmEzoiEek4niNbRWJSshB6IwFxqTnIzCtETSM92FqawN3JBg51a1TqOZVKJU6cOIHVq1fjjz/+wLBhwzB79my4uLjwHBYtlJ+fjyVLluD7779HSEgIPDw8AFTNbBER/Z2/W3vc3Nzw8OFDHDt2DDY2NkLHpSrw8uVLbNq0CWvWrEFeXh68vb3h7e2Nhg0bftDzcrtGRJXBIqtCcoUSxx8kYf2ZGNx4lgaxGJDJ//zPqy8RQaEAnBpZwNvZAQNaWUEifr+AKpVKzJ07F23btsX06dMBABkZGfjtt9/g7++Ply9fwtvbGzNmzECjRo2q9fWRMPLz8yHR00d4ZPIHzRYRUUWpYrtG2kehUODIkSMICAjAkSNH4OrqCh8fH7i4uEAsfnOw3/Lly2FiYoJp06aV+BycLSL6UCyyKpKRJ8OUzVdwOyEd+YWKv328oZ4Y7RqaY+OkzqhppF/09cWLF2PRokWoVasWTpw4gXXr1mHLli1o3bo1Zs2aBU9PTxgZGVXlSyE1o6rZIiKqCK49VB5PnjxBYGAgNmzYAEtLS8ycORNjxoxB8+bNUVBQgJCQELi7u7/3M5wtIlIFFlkVyMiTwT3gHOJe56BAXv7/nAYSEWxrmSDUpyfMjPSxY8cOTJo0CTKZDGKxGGKxGJ988glmzZqFLl26VOErIHWlqtkiIqoIrj1UUfn5+di9ezcCAgJw+fLlotswGRoa4vTp00V/x3C2iEhVWGQ/kFyhxMeBF3ArPq3Ygpx+IQSZ1/ZDkZcNIztH1B4yB5Ialu89xkAigqOtBSY0eA13t1FQKP78ZLJjx464evVqtbwOUj8lzVbamW1IP7fjvccZN+uGeh5fF/v5t7O107s7D8cionIra7sG/P22jWuPblMqlbC3t3/v/ufGxsa4evUqWrRsVeJslXfbxtkionfx3iwf6PiDJNxOSC+2sc+6fQzp54NQx3UB9Cys8fp4IFL2/gTrcT++97gCuRK349Mxc6M/FAoFRCIRDAwMAADXrl3Ds2fPeC6sjipttgzqN0c9jz/vzSjSK/mT6bezFR6ZhEGtras0KxFpj9LWHqB82zauPbrt1q1bePr0KUxMTAAABQUFyM3NxejRo7Fi96lSZ6s82zbOFhG9i0X2A60/E1Pi+R2Z1w6gZqeRMGnRAwBQe/h8PF87FQVJMTCwcnjvsQWFCvT4dCECD29HWloaUlNTkZqairy8PNSvX79aXgepn9JmSyTRK7ZnvzQFhQqsPxPDDT4RlVtpaw9Q/m0b1x7dZW9vj8DAQJiZmaFWrVqoXbs2LC0t0aBBA4zbdK3U2Srvto2zRURvsch+gJiULNx4llbs68pCGQqSY2HR79Oir+lbWENiboX851HFiqwSwPVnaUiTG8De3h729vZVnJzUXWmzBQAFybGIWzUeYgMTGNk7waL3BEiMSr49gRLA9adpiH2ZDfs6plUXmIi0QllrT0W2bVx7dJe5uTm8vb2Lfb2s2QLKv23jbBHRW2KhA2iy0BsJEJfwX1CemwEoFZCYWLz3dYmJGeQ5aSU+l1gMhN6IV31I0kilzZahTUvUGe4LK6/vYekyBfnP7iBl12KUdao7Z4uIyqu0tQeo+LaNaw+9q6zZqui2jbNFRAD3yH6QuNSc9+559qeKXz9LJlciLjX3w0ORVihttowdOhb9b4N6dtCv0wjP13mjIDEahvWblfhcnC0iKq/St2tARbdtXHvoXWXNVkW3bZwtIgK4R/aDZOYVlvh1ibE5IBIX+4RanpNR7JPsd2XkylSYjjRZabP1V/qW9SE2NEVhelKZj+NsEVF5lLX2VGbbxrWH3irvdg0o37aNs0VELLIfoKZRyTu0RXr6MKhnj7xnd4q+JktLhDw9CYYNWpT6fGbGvC8avVHabP1VYXoyFPnZ0DOvV+bjOFtEVB5lrT2V2bZx7aG3yrtdA8q3beNsEREPLf4AtpYm0JeISjxUpmaH4Xgdvh6GVk3e3KIgfAMMG7YpdqGnt/QlIthaGld1ZNIQpc1W6smNMG7aFXo166AwPQmpJzfC0KYlDKyblvpcnC0iKq+ytmtAxbZtXHsIANLS0rBv3z6cj3gCcT0nKErYh1LRbRtni4gAFtkP4u5kg4BTj0v8Xg3HQZDnpOH10TVQ5GfDqLEjag+dW+pzyRVKuDs1rKqopCFSU1MRGBiIwB1hUA77utj3C9NT8DLsR8hzMyGpUQvGDh1g0XsCRKLSD67gbBFReZW1XQMqtm3j2qO7UlNTsXfvXoSEhODYsWNo27YtBo4eh5AccYmnWld028bZIiIAECnLutwp/a0xa8/j6tPUD3oOEYBOdpYImd5DNaFI4zx69AgrVqzA5s2b0bFjRyxYsABbX9TlbBFRteN2jSrj1atXReU1PDwc7dq1g1QqxZgxY9CkSRMAnC0iUi2eI/uBvJ0dYKj3Yf8ZDfTE8HYu+ZBj0l5KpRKnTp3CqFGj0LZtW6SlpSEiIgIREREYNWqUamZLIuJsEVGFcLtG5fXy5Uts2LABgwcPhrW1NdauXQsXFxdERUXh6tWr+L//+7+iEgtwtohItVhkP9CAVlZoZ2MOA4moUj9vIBHDsaE5+re0UnEyUlcFBQXYunUrOnbsiNGjR6NNmzaIiYnB77//jo4d/7wFwYfOlkhRiPzER6gnS1ZVdCLSAdyuUVlSUlIQGBiIgQMHon79+tiwYQMGDRqE6OhoXL58GV988QXs7e1L/FnOFhGpEg8tVoGMPBncA84h7nUOCkq9/15xBhIxbGsZI8ynJ2oa8ep72u7Vq1dYt24dVq9ejRo1asDX1xcTJ06EqalpqT/zIbPV0NIY3TPPYOUvP2Pjxo2QSqWqeBlEpAO4XaN3JSUlITQ0FCEhITh9+jQ6d+4MqVQKDw8PNGrUqELPxdkiIlVhkVWRjDwZpmy5gtvx6SgoVJR523gR3hwa49jQHL9O6swFWctFRUVh+fLl2LJlC7p37w5fX18MGzYMYnH5Doj40Nnat28fJkyYAB8fH3z//feQSCQqeV1EpN24XdNtiYmJ2LNnD0JCQnD27Fl07dq1qLw2bPhhF1qqyGwpFQoY6kvQ3taCs0VE72GRVSG5QonwyCQEno7BjWdpEIvx3i0M9CUiKBRAh8YW8HZ2QP+WVpCIK3d4Dak3pVKJEydOwM/PD8ePH8fHH3+M+fPno3379pV6vg+drcjISLi5ucHOzg7bt29HrVq1PvQlEpEO4HZNtzx//ryovJ4/fx7du3eHVCrF6NGjYWNjo9LfVd7ZMs1LQpOCJ9i1chFni4jewyJbRWJSshB2MwFxqbnIyJXBzFgftpbGcHdqCPs6pR9KSpotPz8fO3bsgJ+fHxISEjBz5kz4+Pigfv36KvsdlZ2t9PR0TJw4EXfv3kVYWBg++ugjlWUiIu3H7Zp2SkhIwO7duxESEoILFy6gV69eReVVlduuspQ1W7nJT9GhQwdER0d/8J5gItIuLLJEKpCSkoK1a9fC398flpaW8PX1xfjx42FiYiJ0tPcoFAosXrwYS5cu5XmzREQ6Ki4urqi8Xr58Gc7OzpBKpXB3d4e1tbXQ8YqRSqVo0KABVqxYIXQUIlIjLLJEH+D+/ftYvnw5tm7dCmdnZ/j6+mLw4MHlPv9VKDxvlohItzx9+rSovF69ehV9+vSBVCqFm5sbrKzU+yrAN27cQM+ePREbG6v2WYmo+rDIElWQUqnEsWPHsGzZMkREROCTTz7B/PnzNe5QXZ43S0Sk3Z48eYJdu3YhJCQE169fR79+/YrKa926dYWOVyGurq5o27YtfvzxR6GjEJGaYJElKqe8vDxs27YNfn5+SE5Oho+PD2bOnKnRnw6np6djwoQJuHfvHs+bJSLSAjExMUXl9ebNm+jfvz+kUilGjRqFOnXqCB2v0i5cuIBBgwbh6dOn/OCViACwyBL9raSkJKxZswYBAQGoV68efH19MW7cOBgZGQkdTSUUCgW+++47/PLLL9i0aRPGjBkjdCQiIqqAx48fIyQkBCEhIbhz5w4GDBhQVF61qfQNGDAAvXr1wqJFi4SOQkRqgEWWqBR37tyBn58fduzYgb59+8LX1xcDBw6ESKSdl//fu3cvJk6ciFmzZmHx4sU8b5aISI09evSoqLzev38fAwcOhFQqxciRI2FpaSl0vCpx6tQpuLu74+nTpzAzMxM6DhEJjEWW6B0KhQJHjhzBsmXLcPbsWUyYMAHz589H69athY5WLd6eN2tvb4/t27dr7R9DRESaKCoqqqi8RkZGYvDgwZBKpRgxYgQsLCyEjlfllEolnJ2d4erqin/9619CxyEigbHIEgHIycnB1q1bsXz5cqSmpmLWrFmYMWOGxl0MQxXenjd7//59hIWFoW3btkJHIiLSWQ8ePCgqr48ePcKQIUMglUrh6uoKc3NzoeNVu0OHDmHSpEl48uSJ2t3ijoiqF4ss6bQXL17A398fa9euhY2NDRYsWICPP/4YhoaGQkcTFM+bJSISzr1794rKa0xMDIYOHQqpVIrhw4fr/CG1SqUSnTp1wsSJEzFv3jyh4xCRgFhkSSfdvHkTfn5+CAoKwoABA7BgwQL069dPa89/rSyeN0tEVPWUSiXu3r2LkJAQ7Nq1C0+ePMGwYcMglUoxbNgw1KxZU+iIaiU0NBSzZ89GTEyMzn/wTKTLWGRJZygUChw8eBB+fn64ePEiJk2ahHnz5qFly5ZCR1NrDx48gJubGxwcHHjeLBGRiiiVSty+fbuovMbFxWH48OEYM2YMhg0bhho1aggdUW0pFAq0a9cOc+bMwfTp04WOQ0QCYZElrZednY0tW7ZgxYoVyMzMxOzZszF9+nTUrl1b6GgaIz09HePHj8eDBw943iwRUSUplUrcvHmzqLwmJCTA1dUVUqkUQ4cOhampqdARNcaOHTvw73//Gw8fPoS+vr7QcYhIACyypLUSEhKwevVqrFu3DnZ2dliwYAE8PT1hYGAgdDSNpFAo8O2332LZsmU8b5aIqJyUSiWuX79eVF6TkpKKyuuQIUN4waJKksvlaNmyJb7++mtMmjRJ6DhEJAAWWdI6165dg5+fH0JCQjBkyBAsWLAAvXv35vmvKsLzZomIyqZUKnH16lXs2rULu3btQkpKCkaMGAGpVIrBgwfD2NhY6IhaYePGjfjpp59w//59bouIdBCLLGkFuVyO/fv3w8/PD9euXcPkyZMxb948NGvWTOhoWuntebNNmjTBtm3beN4sEek8pVKJy5cvF5XX169fY+TIkZBKpRg0aBCMjIyEjqh1CgoK0KxZM/zvf/+Dp6en0HGIqJqxyJJGy8rKwqZNm7BixQrk5eVh7ty58Pb2ZrGqBjxvloh0nUKhwKVLl4rKa3p6OkaNGgWpVIqBAwfyirrVICAgAGvXrsXNmzchFouFjkNE1YhFljRSXFwcVq1ahcDAQDRr1gwLFizAmDFjeMGHavb2vFk/Pz9s2rQJHh4eQkciIqpSCoUCFy5cKCqvWVlZcHNzg1QqxYABA3gdhmqWl5cHe3t7rFu3DiNHjhQ6DhFVIxZZ0iiXL1+Gn58f9uzZA1dXV/j6+qJnz548/1VgYWFhmDhxImbPns3zZolI6ygUCpw7dw67du3C7t27kZubW1ReXVxcWF4FtmzZMuzcuROXLl3i3wNEOoRFltSeXC5HWFgYli1bhtu3b2PKlCmYO3cuHBwchI5G7+B5s0SkTeRyOc6ePVtUXgsKCuDu7g6pVIp+/frxCCA1kp2djcaNG2P79u0YNGiQ0HGIqJqwyJLaysjIwMaNG7FixQooFArMnTsXU6dOhbm5udDRqBRvz5uNjIxEWFgY2rRpI3QkIqJyk8vlOH36NHbt2oU9e/agsLAQo0ePhlQqRd++faGnpyd0RCrFkiVLcPToUURERAgdhYiqCYssqZ0nT55g5cqV2LBhA9q0aQNfX1+MHj2af0BoCIVCgUWLFmH58uU8b5aI1F5hYSEiIiKKyiuAovLau3dvbns0RHp6Oho3box9+/ahd+/eQschomrAIktq48KFC1i2bBn27duHUaNGwdfXF927dxc6FlXS2/Nm58yZg++++47nzRKR2igsLMTJkyexa9cuhIaGQiwWw8PDA1KpFM7OzlyvNNTChQtx+fJlHDlyROgoRFQNWGRJUIWFhdi9ezf8/Pzw4MEDTJ06FXPmzIGdnZ3Q0UgF7t+/Dzc3NzRt2hTbt2+HhYWF0JGISEfJZDKcOHGiqLwaGBgUldeePXuyvGqBly9fws7ODidOnECXLl2EjkNEVYxFlgSRlpaGDRs2YNWqVRCLxZg3bx4+++wzmJmZCR2NVCwtLQ3jx49HVFQUz5slompVUFCA8PBw7Nq1C2FhYTAyMsKYMWMglUrRo0cP3ndUC33xxRd4+PAhFi5ciICAACxevBg2NjZCxyKiKsAiS9Xq8ePHWLlyJTZu3AhHR0f4+vrCzc2Nn4RruXfPm928eTNGjx4tdCQi0lIFBQU4fvw4QkJCEBYWhho1ahSV127durG8armwsDCMHj0aEokECoUCx48fR79+/YSORURVgFcwoCqnVCpx9uxZ+Pn54eDBgxg9ejTCw8N52I8OEYvF+O677+Dk5IRJkybh+vXr+Pbbb/kBBhGpRH5+Po4dO4aQkBDs3bsX5ubmGDNmDA4dOoQuXbqwvOqISZMmYdu2bVAqlSgsLISxsTEv1kWkxfjupiojk8kQEhICPz8/PHr0CNOmTUN0dDRsbW2FjkYCcXd3R4sWLeDm5oYbN25g27ZtsLCwgFKpxJMnT2Bvby90RCLSEHl5eTh69ChCQkKwb98+WFpaQiqV4ujRo+jcuTNEIpHQEamaSaVShISEoKCgAHK5HAqFgh+YEmkxHlpMKpeamorAwECsWrUKRkZGmD9/PiZPnowaNWoIHY3URFpaGsaNG4eHDx8iLCwM+/btw9dff43IyEg0a9ZM6HhEpKZyc3Nx5MgRhISEYP/+/ahTpw6kUimkUik6duzI8kp49OgRhg0bhtjYWCiVSly4cIFHgBFpKe6RJZV59OgRVqxYgU2bNqFTp07w9/eHq6srPw2lYiwsLLB//35888036Ny5MwoKCiASifDTTz9hw4YNQscjIjWSk5ODw4cPIyQkBAcOHICVlRWkUilOnToFJycnlld6T7NmzXDr1i14enri4MGDyM7OFjoSEVUR7pHVMTEpWQi9kYC41Bxk5hWippEebC1N4O5kA4e6Ze8xzc3NhVwuf2/PqlKpREREBJYtW4YjR45AKpXC19cXHTt2rOqXQlrg8ePHaNOmDfLz8wEAenp6iI+Ph5WVVbHHfsjsEtGHq873YHZ2Ng4dOoSQkBAcPHgQDRo0KNrz6ujoyPJKf0upVGLz5s0YN24c4tMLuP0g0kIssjpArlDi+IMkrD8TgxvP0iAWAzL5n//s+hIRFArAqZEFvJ0dMKCVFSTi9/9IyM3NRdeuXWFtbY2jR4+ioKAAQUFBWLZsGZ4+fYoZM2Zg1qxZvMQ9VUiPHj1w6dIlKBQKAG8uCjV//nz88ssvAFQzu0RUedX5HszOzsbBgwcREhKCP/74A7a2tkXl9aOPPmJ5pQrh9oNI+7HIarmMPBmmbL6C2wnpyC9U/O3jDfXEaNfQHBsndUZNI30Abz7V9PT0xP79+1FYWIg5c+YgKCgINWrUgK+vLyZOnAhTU9OqfimkhS5evIgDBw7g5MmTuHHjBnJzcyEWi1FQUIBsmeKDZ5eIKk8V2493LV26FDExMQgICCj6WlZWFg4cOICQkBAcOnQIdnZ2ReW1TZs2LK9UKaqeXSJSTyyyWiwjTwb3gHOIe52DAnn5/5kNJCLY1jJBqE9PmBnp44cffsC3336LgoICAED9+vURGBiIYcOG8ZYGpDIKhQJRUVE4ceIEJkyZppLZJaLKUdX2460lS5Zg0aJFAIDY2FicPn0aISEhOHz4MJo0aQKpVIoxY8agTZs2qn4ppGNUPbtEpL5YZLWUXKHEx4EXcCs+7b2FPCfqPDKvH0B+YjSU+Tlo9M+9EImLX4zJQCKCo60FeuVfw/x5c9/7nr6+PhITE1GrVq0qfx2ke0qb3bfSL4Qg89p+KPKyYWTniNpD5kBSw7Lo+29nd6d3dx4mRlQJqn4Pfv/99/juu+8gk8kgkUggEonQqlWrovLaqlWr6nx5pMXKmt3y/P3D7QeRZuHuNC11/EESbiekF1vIFbJ8GDV2hHm3MWX+fIFcidvx6ThyNwH16tVDo0aNYGVlBTMzM+jp6eHOnTtVGZ90WGmzCwBZt48h/XwQag2cAesJ/4MiPwcpe3967zFvZzc8Mqm6IhNpFVW+BydPnoyFCxdCJpMBAORyObp164bbt29j4cKFLLGkUmXNbnn+/uH2g0izsMhqqfVnYko8L6RG234w7+EFwwYt//Y5CgoVMO04EklJSXj69CkSExORnp6OnJwc9OnTpypiE5U6uwCQee0AanYaCZMWPWBg5YDaw+cjP+4uCpJi3ntcQaEC68/ElPgcRFQ2Vb4Hb926hRo1akAkEkEikcDAwACXLl1CXl5edbwU0jFlzW55//7h9oNIc7DIaqGYlCzceJb2wc+jBHD9aRpiX/IebFQ9yppdZaEMBcmxMGrcruhr+hbWkJhbIf951PuPBWeXqDJU/R7cc+wsMjMzkZeXh3v37mH37t1Yv349DA0Nq/BVkC7i3z5EuodFVguF3kiAqq7BJBYDoTfiVfNkRH+jrNmV52YASgUkJhbvfV1iYgZ5Tlqxx3N2iSquqt6DBgYGaNGiBVxdXTFp0iRejZhUjn/7EOkeFlktFJea89690j6ETK5EXGquSp6L6O+UPbsVm2nOLlHF8T1Imop/+xDpHhZZLZSZV6jS58vIlan0+YhKU9bsSozNAZG42J4feU5GsT1Eb3F2iSqG70HSVPzbh0j3sMhqoZpGeip9PjNj3k+NqkdZsyvS04dBPXvkPfvzitmytETI05Ng2KBFiT/D2SWqGL4HSVPxbx8i3aPadz2pBVtLE+hLRCUeYiPPzYQ8IwWytBcAgILkWIhEYuhZ1ofYwLjY4/UlIthaFv86UVUoa3YBoGaH4Xgdvh6GVk2gZ2GN1+EbYNiwDQysHIo9lrNLVHF8D5Km+rvZrcjfP5xdIs0gUiqVqjmhgNRGTEoWBvqdhryEf9qs28fx6o/lxb5uNfaH965E+ZZYBIQv6Av7OqZVEZXoPWXN7lvpF4KReXU/FPnZMGrsiNpD50JSw7LY4zi7RBXH9yBpqr+b3Yr8/cPZJdIMLLJaasza87j6NPWDnkMEoJOdJUKm91BNKKJy4OwSCYvvQdJUnF0i3cJzZLWUt7MDDPU+7J/XQE8Mb+fih4sRVSXOLpGw+B4kTcXZJdItLLJaakArK7SzMYeBpHL36jOQiOHY0Bz9W1qpOBlR2Ti7RMLie5A0FWeXSLewyGopiViEXyd3hm0tkwov6AYSMWxrGePXSZ0hEfOm9VS9OLtEwuJ7kDQVZ5dIt/AcWS2XkSfDlC1XcDs+HQWFijJvZy/Cm0NqHBua49dJnVHTiJeeJ+FwdomExfcgaSrOLpFuYJHVAXKFEuGRSQg8HYMbz9IgFuO9y9PrS0RQKIAOjS3g7eyA/i2t+GkkqQXOLpGw+B4kTcXZJdJ+LLI6JiYlC2E3ExCXmouMXBnMjPVha2kMd6eGvMw8qTXOLpGw+B4kTcXZJdJOLLJERERERESkUXixJyIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaRQWWSIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaRQWWSIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaRQWWSIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaRQWWSIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaRQWWSIiIiIiItIoLLJERERERESkUVhkiYiIiIiISKOwyBIREREREZFGYZElIiIiIiIijcIiS0RERERERBqFRZaIiIiIiIg0CossERERERERaZT/B1q0lrjs1tjWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "1036\n",
      "============== Pattern 3 ==============\n",
      "2696\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "3445\n",
      "============== Pattern 6 ==============\n",
      "5097\n",
      "============== Pattern 7 ==============\n",
      "3847\n",
      "============== Pattern 8 ==============\n",
      "1\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "4171\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "Average comprehensibility: 59.4\n",
      "std comprehensibility: 26.773867856549973\n",
      "var comprehensibility: 716.8399999999999\n",
      "minimum comprehensibility: 16\n",
      "maximum comprehensibility: 96\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
