{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/models/epoch_26.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/models/epoch_26.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/data/test_data.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b39098d87ed4bcfb54ab73effcdfe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3d0c4854a94d219ba4406df8f984f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 40))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAADzCAYAAAChbyKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9d7Qk53XeC//eyqFznz45zZmMGaQBBokEAYIiwQSKlGhRgZJs61q2ZVsOS7aC5e9KDuvKQbZs+iraCqRIiaISKSYxgCRA5DgAJp4JJ+fTubtyvd8fdWYIUyI5IwIgsO48a9Xq6rcr7K6u2r33fvd+tpBSchVXcRVX8TeF8p0W4Cqu4ipe27iqRK7iKq7i28JVJXIVV3EV3xauKpGruIqr+LZwVYlcxVVcxbeFq0rkKq7iKr4taN9pAa7iKq7i8nHvG125XU8ua9unngv+Ukr51pdZpKtK5Cqu4rWErXrCY385flnb6iPnBl5mcYCrSuQqruI1Bkki0++0EP8HriqRq7iK1xAkkPLqyjK/qkSu4ipeQ5BIInl5MZFXCleVyFVcxWsMrzZL5DU5xSuEeKsQ4rQQ4qwQ4me+g3LMCSGeF0I8K4R4cmesIoT4vBBidue1/KLtf3ZH5tNCiHtfNH7TznHOCiH+hxBCvETy/bYQYkMI8cKLxl4y+YQQphDiozvjjwkhpl8GeX9BCLG8c42fFUK8/VUk74QQ4ktCiJNCiONCiH+6M/6yXWMJJMjLWl4xSClfUwugAueAGcAAjgHXfIdkmQMGvm7sPwE/s7P+M8B/3Fm/ZkdWE9i18x3Unc8eB24HBPAZ4G0vkXxvAI4AL7wc8gE/Afz6zvr3Ax99GeT9BeCn/pptXw3yjgBHdtbzwJkduV62a3z9dbrcWB69rAV48pV4Dl6LlsgtwFkp5XkpZQj8IfDd32GZXozvBn5vZ/33gHe/aPwPpZSBlPICcBa4RQgxAhSklI/I7E754Iv2+bYgpXwAqL+M8r34WH8MvOnbsaK+gbzfCK8GeVellE/vrHeAk8AYL+M1lkAi5WUtrxRei0pkDFh80fulnbHvBCTwOSHEU0KIH98ZG5JSrkJ2kwGDO+PfSO6xnfWvH3+58FLKd2kfKWUMtIDqyyDzPxZCPLfj7lx0DV5V8u64RjcCj/HyXmPSy1xeKbwWlchf98/xnYo0vU5KeQR4G/CPhBBv+CbbfiO5Xy3f528i3ysh+68Bu4EbgFXgl7/FuV9xeYUQOeBPgH8mpWx/s02/wfkvW2Z5mfGQVzIm8lpUIkvAxIvejwMr3wlBpJQrO68bwJ+RuVrrO+YpO68bO5t/I7mXdta/fvzlwksp36V9hBAaUOTy3ZHLgpRyXUqZSClT4LfIrvGrRl4hhE6mQD4spfzTneGX7RpLCdFlLq8UXotK5AlgrxBilxDCIAuQfeKVFkII4Qoh8hfXgbcAL+zI8qM7m/0o8PGd9U8A378Tbd8F7AUe3zF3O0KI23b88x950T4vB15K+V58rPcC9+/49C8ZLj6MO3gP2TV+Vci7c/z/DZyUUv7XF330Ml5jQXKZyyuGVyJ6+1IvwNvJIuHngH/9HZJhhizSfgw4flEOMh/7i8DszmvlRfv86x2ZT/OiGRjgZrKH4xzwPwHxEsn4B2QuQET2j/ZjL6V8gAV8jCxA+Dgw8zLI+yHgeeA5sgdq5FUk7+vJ3I7ngGd3lre/nNf40LW6PLUwclkLr9DszEVBr+IqruI1gMPXGfKPPlW7rG0PTa48JaW8+WUW6WrG6lVcxWsJWbLZK+iqXAauKpGruIrXGFJ5VYlcxVVcxd8QVy2Rq7iKq/i2IBFEUv1Oi/F/4FUzxSuusKjuRRmirwlclfflxf9X5L1oibyapnhfFUpECKEC/y9Z5uc1wA8IIa75Fru9pm4arsr7cuP/I/IKEqlc1vJK4VWhRHj1F9VdxVW8KpAxmymXtbxSeLXERP66wqRbv36jHRMw0+CqelNBVGRaclA7Af6oBYC1GpDkTRCgBClhQUFqoHmQamC0EkhT/FEVRZGkkZKRC0gQikSGCuhZ7oyhxYShhlAlQkhsPaLXNxGxAAlSBd2OkA0dKSA1QAlBDSRhQSB2qqCMdorhlnFqEzLVQSpgNlNiR0ENJFFeUC526cUGmpJiKAlIiKWCBPxYRwJSCkwtxvMNFC2lYPikCNo9m4Lr0Y8M4kTBMSK8ronUJIqWYmoxipD0PBNNT4iTHZ86EqDs5AlJgdBThJCksYJaLmPOjMtJd5uFdhXDioliFSkFQuxcJ0VmlR2JQO9ClAP0FKQAIUEKFDXNrnMqsmudChAgEpBmim1E5LWAjW4eEoHInhKkSnb97AQZZ/sJPfsuvq+DsvN7xQogUctlrNEJqbgJmpIQBHr2xKkStaOQWGTn1VPGnCZLvRLKzndXhCSOVRRPIN2dH81XUJyEVIJML7oGIqtsSwSmExJ4BqYdkkgFuXMpYl/L7p+d7dBTSAWGGRMGGqYZIRGYQ3ny+4dlkip4Z1e3pJSXl/zBSxdYFUL8NvBOYENKefjrPvsp4D8DNSnl1jc7zqtFiVxWYZSU8jeB3wRwahPyutf/JPbHH8d79y20JzSEhMrJAL+iUXp8hZV3TqB5ktgRDDzvsXarzeCTAe0pg/o9PmmgQqwgnBhjziQqSFI7QdgJipYyXGmz/uwQTHoMVdocLK/zyPI0wdkCSEgNyaEjc3g/P4LqRWxfm8fdiEl1Qb+mcnEmLr8UY633QQhW7irQ2Rex9/dCEltj83oTvSfZ/cNnONeoMlFoUTA8+rHBlpej5Vk0VwsgJIWhLnGiIJ8tEuzzuHZyhRTB2c/PsO/N53juwjh0NK67bo7zH99N7II3FjM6vUVOD5ldHmSg2mH7TBVpSEQkUHxBMhqgL5hE4yG6HRH5GjJWUMyEX739g/zER36c2tF11usFAJK6ib2sEruSxJboHYXRBwLO/y0Vd6hHr2VlykJCqdbFNUMGnQ5ntgbpNezs144Eel1j3+1zvG3wBX7lE+8EIC4miFBBpKAEgutfN8tTs9MQKEzt3uCm6gIPru1GSoGtRyydGUStBqTrFkZLYeoN80zl6tz/5Rtw9jUZzHeZf2IcCcTlGMVT+c/v+H1+8cQ7KTke86tVDCtC1xO8cwX0qR5hoJG2dZyhHnk7oGhmyjpOFeYWawhFcnjXMs+fH2N0pMFUvsEjJ/bgVvv0Nh2Er2KM9AhXXczRHtpjeXJvWqcXGNRyPW4oL/FCc5QwVemFBk+9/f+Zv9wHRUrxUroqv0uWHfvBFw8KISaANwMLl3OQV4sSueKiutSE3AOzdN99C/afP87yr9yGFDD6qS282giN28cYeM5j6U02QSVBCS1SFVQ/ZvAr29TfVCZX7dPddBGKJJgIsXIhwYoLdgJSEMQacTVGA9q+SSuy6G062C2BEoM3Itno5YgPWCSGTXdCEuV0SucimgckSpBpkcK8ZOuGAqkOsQPGpkZipdQPmpTPxKzcqTJhN9jo5wGoGH0KWoClxgy7CieluKRRx0stzkzbWFbMuNPES3Sen4goGB6lSpemcAlTFb8miaoxVtnH0SNsLYK2jl/QkOUIFIm6bBIVE0TDINVBNRIsMyLsGIi+SpoImolLVE6JUyWzJgIVpRgS9i0SO0WaKUoo8AZ19KagZ1kQqAgnRsYKXqADcKo7RLCUQ/cEqSExmgr+Xp8oUdmK8sS1CKWlYWyrqJ4gGEhRYji+NgKRQMSCxY0yQkg2F8ugpWhODKkgbhqosUD1YX67kllktZD2ap5Oy6Y0D90JUNsa2lSXdmLR3M7R65vkCh6WHrO5UcDdUpC7UkwrQp6xMcYSWj2bja1MecpYQSgS0dAJJ1VERyM/FXBqe5DKcAtVkXi9PHIgJKjbCCGJIhV/T8ywEbK2UGEk3+HRzWkcPUIRkiC68kcwfYksESnlA9+A3e2/Af+Ky6zherUokUtFdcAyWVHdD36zHcytiFP/ZR+FWZXlX7mNPf/sUYRpcvrfHyF1UgqnVTbeJWFZUn5BwR8QBIf7XCg7xFWd4WqDtaUKpaEOI4U25x6aIlENRm9cp+ObjBTabPZc0FLKxR6HqmtM29sM39TmL5QbIIXcYI87h8/xydFB3CWJtSXoj0i8YQ13QRAWM6t+8S0a9rpAJODv8TGdiCVyKCH4d3mIczn+7PgNyFihVe1yemOQ0NdIOzp6S6VwDmJHcMcPP83nv3QjuU1Bd7fCp3uHQZHYSzpP1yYQjxSxNdgacFE9wfTvhmzclGf+DiV7MNwYU0sI502CoZjEkhh1lWgiwDhl4Tkm+UqbylQfRUgurAywneSwl1XCSZXUVxGaxDlmo8SQWCqxrSJS2DwC8UCElQvxpYk+byI1KAy1sLSYI0NLzJUqrLXyCAFhqDLxYYvwJ1WCVEPf1ImGQqIBSaKlqEKSSsE/vOZBfuPEnRh6zPVDy9xavMBsZYhebOJqAR/3bmB6cpO5hRqernH72CIl3WPl+SHuen1Wq3dqaJD28UESJyXdcOikNmOjdVw95NxaDekEvPf6p3lkZBfThTrLvSKrhwUjhTYDVhdTSVBESpyqHK8P446HvGnwFMb1CZpIuGloiS88c4jR6S2Kexo050tce908ZzcHuHvqLHM/MILzOx6vu3aWAbPLbmuTZzsTlPQ+q3bxUkXh5UAiCOVlP7YDYoe2cwe/uWPNf0MIId4FLEspj10uX9OrQolIKWMhxD8G/pIsQvHbUsrj33yv7AvuuN4I00QGwc4BueQMiXTHt5ZkG77YSZJZ9l+SZuYzOx8rQiKlQBFAKlCEREGiXgxyKBkFRLaNvHQOJeHScS6uXzr3zrpMBYqSku7IlsosPiCERIos9iLEjoP9IldcCohS9dJ3uvjlhcj8byEkUtk5t5DZdVGVnbGLx724XXYRLm4vlK/texGpFCiqRCVFKqAqElSZfXcl21cKdo6VxQqEsnMeNb20jSIkipBoSoImsviIEBJFUZDq1z6/9KuK/9OLVZGXxlSRyQOgiDRb3/m+F6+RrmQxkYu+5MVjS2Xnd4sEKimakv6Vc0m+dj8oiryUGaorGbt6Kr42lkolWxeZLP8HC5AABbnjeghEFF+SOZUi2x5I/gbBz4uB1cvE1pXUzgghHLICwbdciUyvCiUCIKX8NPDpy90+qGoc+A8XCA5PMPqpLU7/+yMA7P6Xj9B7762IJKHy2yrrRwXtPZLa0yktaTP1B4vEQyVm/1ERq+zTWSjQKdrIwQQlF7H+/BBpLaRZd9k3uc62X2a7mePZZJSgqvHQ8b0YGxoigWC7wOP5KYrnUlJVEOWgcB7Kp/rMv91B62U33N7fb9OdyRNbgnjWxB/Q2f+RbbZvrmIcd9i8Ed59zTFOtocpGD4TdoNUClb9IimCkweHAHhybYK9R+c59cIEheEOd46dJ0g17t+6lnvHz/PV23fR2Xa5tljnOTnI2R/S0Ys9Do+uktMDHtzah6XFbI5FIEHpqETFFBmqBAMp1nCPetfB23BQPQUx4rPb2MAfSph0enhFHa9vENzYQ847JE7mzqgdlfH7ExbeqpHYEUKRJNM+SahkClpIntseZW21jOipSE2itVUW74s4avUYMZpEpQRjxSBVQfNEFmvpCn4ndxte0yJs6DwNBInGI2dm0MwE2wkQPZXzF4ZQuipGU+FMs8ZYrkWSS7j/hQPYJR/jKwXYlSJiBVEJGdUbbLRzWEaEpscIIfnTEzdgP2/zzM0uSSKQ8y6rWsJSs4SUO7GIRCFsWGz3Fb5yY8zpZyeZOLRGJ7Jwh3t4oU77dAVZjjh2fhxtw+D+aD/JT5mM9zdYXKqyZ3qdhV6FTmSSpApt37ziZyV5+dLed5Nxv160QsaBp4UQt0gp177RTq+WKd4rhlSh9YZd+BWNxq0jpE5Kaqf03nsr7h8/hhJLYlelcjohtiWxKRh9sE/zllG6Uw4yVPG7BtJJMOYstLaKTAWplWb/pKmg7jlYKypJpOCHOv1Yh0SQmDKL9gPdwCS34BPmMzOmMB+ycZOLEglSHVIdOnvyxGY2WzPyiE/qpLQOVzA62T9SbgFyWkA7sHDVkJyaWVQlw6MdZidKpaB7ukw7sJB2QrdrUdA8Snqf1E7ZClxG8h3y1R4Afi1F2DHVUpeNfp4tP4dVDNjquGhWjGrHpEMB0kmydTMlCjUCT0c4CYmTydaT2TXa7rukqaBY6KNqKUkuReYS1FyMVMFohkhD4lghMhUMlDtUBjrUmy5bXZecEaIYCdJMQZXElRgzH7DWK5BKBfSUsJKQjAT4IzFROcYfSsiZIUJPScoRQajRjizsfEASK/T7JtLesRLMlNiWdH0zsxDMFLvk41ohaiiz2Z5iiNw22YwLxLFC0fYzywjQLlioISRnc8g5l8RJ6cwXkRLydkDB8ck5PugpshJxaj6jOdnuOZhqTK9p0+naJNUIYaQQqMTFBHXWQWqS9WYeu+hzfmUATUnoRzoLaxW6/StTIhJBgnJZyxU/U1I+L6UclFJOSymnyWKVR76ZAoHXsBJRfZCqoPR4Fn8tnFYpnFazuMN9t2D9xeO0JzWkgPycgrMZs3mjgxJDUFLQ3CibmdEkwXiI6glo6Ug3QdVSdDcklWT/iFZMLd9j2O6QG+oCmbWcuClj+RZaJ2DguIe7LBGpxGhLcgsSaxusLZCKILfoU5jtUN9vgp6idxOsekhzt4rekxxvj9Do2WwGOWZ7g8x2BznZHGK+Xqa9kqe7lkNM9tho5DFXdFJf5UR7hNOdIcx1jW3f5dxajc56jnZoYW0omOct1lbK9AKDdmAR+hqKIkkaJklfQ3Y1lKZGGisYdZXY0zCsGM2MUXIRSZwlLWlbOqmEwNfp9k2CNQdzQ83G6wZGSyGxMqM2jFVkpLCxVaBRz2GYMYYWk9d9LDtEGGn2kCmSeD4HQCRVlJaGiAQy2rkld1zP0VwLoWYu1EChx0xui3KuT7XSZbjShljBKAQImQW7K26fvB4gOho5O6Bke7R3geoJUk9DCQWqSLGsKDt3qJFKgdzbozuRkk76xIMR0khRaz5l16Nk7Sy2j2IkaGbM1NgWshpStH0cLYQwizsJNUX2NayqB0C8x6NwWqOc7+NaISO1FqN2i6rdZ3SwSaXQv+J7P3OlvvXyrSCE+APgEWC/EGJJCPFjVywMryJ35kqRuJLcvMfKOycYeM7LgqhA5bdVYldl/Z/cwdAHHmb+F+8gmvGQio3qgUglg1/ZoPnGPAMjLbZWihAL4r19Boo9GscHUKYDgo5JodqiPpBgKZJ636bp2gSnikhTokSg9hVWugWiO8v4AxDUEsKihd6VbN8SIzwVhCR/v2T9FodUByUG0VMJC4Lta3QmPnSWM/9qhvdXTwNQNXscdleIpEonsegPGJwcGCZOFU4tDPPG/Wd41JhiyA64e+A0/cTk1L4hri2vULO7nGsMUDH7nJyIKQx3mCq0GbB6uFrA6mYRxwwxJxvEiUp7I4c+3iPoGUSTAQOVLmGs0q7nUNsaxnQXVaSoM10GnD6qIul6JsP7NlgqljGsCMeIabsu/SEdSEkSBT0XUi32CCKNMNbQ1ZTZ7Rrecg5UkFqK1tAYuH6DgukDYE116G+4CE9F6ypEpQTVV3hyfhKhSPAV1raLPA2sztaQRppNxbsRYcMCMyF2BJudTDGZoz02F8psOTHTXwi58D4FhMScaaOQZlaMFAxV2uhqwtLpUQZOwMZdKkJPUeoGohhQ7zqs9EsAyGQn8LJtsm2H6Asm5mjMhWaV0ektokSlv5IjP96ms5JHSEG6bRC+rkNeSVlfLFMZbfHI2i4qdp9eYFyxa5Klvb80//1Syh/4Fp9PX85xXrNKhESw+GaX3JJk6U02LGcBzvWjgsrphNxKwvwv3sHU//0wc//udqxtidVMaOzTWD86iGm22VoqoRcDlDMuyqrG5qiBMuJTdn22Qw1FSMrPqvD2kPFii2lnm2P72hCpyFQh2bQ4UFln/XGLrRtyuIsKA8+2uPCeArWHNLxaljzVnAFnI8XopJjNCPONPXoXBhh8JuDCP9zD/v+xyOm7hjnfqFIe8lgIKjQjh05scmJziH7PAiHRlk2eKE7gL+YJagbHq2NZsG7Z5vnaaJY0lSgoIkVEgva2i2OGdEKToukzOVynFxr4oY6qppSH23T7JlNj2yyul+n5BlIK3FqfsKhSK2RWVxxppAgsLaZS6WdxgkghNb429axGktJIm7Fii9VOnoIREGox9Z6DpcXcNXKWY8UxOmEWCxCTku1mDlNNKKp9vJ6JkosYHGjT9U0INJK8yvccOMaDa7tp6TbXjy5zY2GRY/lxmqGNIiSLzRLOQJtWz8Y3DG4YXmavu8HvLt3O0WvPUTb6fPFt16M6AQPlDr0vD9I5aHNgdJ1pt85DK7uwjIi4kLB9vYKxoqPEAutInc5siX1HLzCd286uQ6ry2dmDDAw32dgswERAo2/z1omTfOxzr0Ob6VKabNLtm9SmGmzXcyirFumZHI0DKTcenOPY/Dj/4MYHeKyxi7OdGmO15hXd9lcL8F5CaB4E1ZTYEQSVhOIsFGchKkqW7xJ0JlWiGY+5f3c70//mEUQCS++JiW0IRyM0NXvQklUb5VAbrQfVJ1VSX6MfGKSRwpHKIt1JMLSEEbuFqcRMVRrkXZ+c66PEcMBdZ/mePO5GQvmMz9J3FTFaAnctYugpn6EnPWIbUk3gVVTO/pBGxe6TGFDfbxLv63Ph/RM8vjFF1e3TCG2ea4xxvlPlybkp+n2TtG6Q9HRuvusUAPv/5zqjA00eW53kibVJ9ny4w5DdYf7xcYInK5hKwuSnU9wzBs1Hhthd3GJPfpMwUblz+BzeUp7ufJFW2yHetnH0LI6RswPSVNBr2MSBxqDToaZ2YNXkhvISJcujFVh4PQNtSydZceis5RFaSv2Aesm0H8l3mF0aZG6xxl3jZ7lraJZJc5trSyvU7B6DbpecEWI/6nJH7Tx51cOYMzFP2XR9kyDQkPMuypLFoNEmlYJg1eHu8mnemDtBxehza2WOO6tnCSONEbeNEBLNiPmuygkO20uUHzNQhGTKquPsbmHZIVGikJhwgzXPDaUlXC2gtVBkbbXM244+hzreZ+iWNZKDXSpun+HDG+T0gGlri2lriwmrTrpl0urZvOvwcxh2RJSo7LY2EDKbWWo2XZhzOVRdQ122uPa2s7hL4FohYapxaHKVI/YccapycHyNYfebkcP/VUjJq6525jVriaQqVI8JSuc8lNDCH8h86NrTKbEpcDYjpGJjbUuaP3w7pQ89Qm/sDgafDNjqmnRuSJFGit7W8NdcbBM606Bva3i2gdLSaUYOZlPQ7NrMWxVMJebM6iBppCATBXdTYbY/yPBjPnrDxx9yGHnUIyjrRDkVtZ4iFcHYAx76yQWolgnKgyyUy0w80ENreSzmBrC2JIcqa2wGOfJ6wLDVISWbPg5ijRWKqGrKc+uj1PJdVu8dw0rXOTCQkYifv2kfM8Y50ikPf9tEVxL6gxr+YEpSjFF3plE7vsl6UMiCkYlA+pm7VfccZKwgpSAKNJRONvu0PepiiYRUh3ZsEcQaQaRh2hGRsJCmzAKNgcrE5zos3eiiqwnd0MB0IuJIZaFXwbMMGpHDydYwGzvuhh/oxNMp7dhGRRLbYPWh17KRfRXTF6gBHGtP0OraaD2FZ7qTKELyXH2UgpkprKBncK4+gNexkL7K871xxswmfk0wWx/IznWiRDLhE5tfK2k40x3EUqMssGwmPLC4G864LE1pyL7GoiyDkOhqgiImSaVCLBXUvoJftzhWGiNacVHGu5zzB4ndFCIV2jqxLTm+PYzWEzy3NIZeE6iBwUJUQlcTnvam6URmZqUZ0RXe+eIlSzZ7qfDaVSJOijco8AdsUhWCw32Qgpa0GX2wz+aNDqoHVjNh6T0xvbE7GPulh1n62TtQIjLn0kiRu3sUHsrRmUnRx3qEHRPHDvFKCk+uTSASSGKV5VaRguGTtIxL9RDeUMoLWyPoQwbbbzJJDag9rbF5ZCeXQDcAcFYV5B37USKY+OgcZ24eZPluB6vuYDQlUsBuZ5Pj9WHGnSYTVp167HKkssjzzVFq5Q5xopJ8qkry7j7+3R3a2wXeMXacRCo8c88E5ztV3rTnNIsjZeqhQ/0tPq7rc7C2zqnGIDkjpGAFPD4/RW2siZQiC5ZGOkGkodkxnb6JoqVooz1CT6fec3jWH8ce7/DE2iSamjKY69IJTdbHNXJOcCnbs7k/Rxh4RIlKo+Nw3egKYaJx7Pw4phtyzfAay40icawgUwXdiBk5tMZDK7uYsrfQp7uEE4Ky69O1LRjIsjnrgYOmJSS7uzy0NMNiuYwfa6xsZ9PephvSadsoekoSKty/tI+DA+twfRtDS1jv57HXBe0RBbcU0HJcvtS9hpObQ+ytbjI82kAVkt5fDCNccB6ziB3oXRtjn7So3xphKEmWfZqqyEkP14pYfmIUmc/yXs72akgrzVLxR7qoasrmahH1YJ/S/TbNN3pEnkGt0mbjdI2nKlMEsUZrrkRv6MoCqxJeUSvjcvDqkuYKIAKFic80GHwyYOwrPcSSjViymPqDRXqjJrnlhPxyTGOvhnnOYvDJgKWfvYPx/+dhqscjZCrQrRjmXFrXh0gFgpaFsapn/2qeyt7qJv2RbKrTNUNcLQRVonQ01JaG0VSouV1EIqk9k2KvCaSAPR9pYTQFhbMKhXMKE793lqEnA0rnY+Z+ZJq4bTD1W6ext1KspqQ/ItiOXIbdDu3IYivKkUrBhV6VguHjhzpJKpBvbVCx+sTnc9QqHVaCEgteBeW8zYHiOg8tzXDyzBi2GuE+adNey/Pk3BR7S5vsL2ywullk38gGm+tFtlaKNM9W6K/m6PYs4paBacQoSkq05KKtmEhgn7GOt5JjX3UTRUjOb1Rp9mxYtukuFdhcLSL6GoUFn7iv0ejbqGrKs4vjPDc3xvT4FtcMr9GLTOJYIeoZJKGKv+oyP1/jyNASFbVHsOGgP5ejdaaCOO0ijuexn7Gpew79toU8nWMg12NPfpN6M4dth9TKHfwtO0sabBhodY2JUpO87hPO51hfL9H2zayoL1CprxVJSjG3u7PYRsSWl2OzkWe749J5XR8k9O7s0r/eg5aOcksTgIZvs913qfcc5JqFf67AzG0LqH0FxwwB0PMBlh0SzhborOVRnRj9tEPzHo90y2Sg3GFtrcTYwXVsNUJTUkb2b5Bzgiu+91+uKd6/KV6zlojUQPR82kdKDH5lm7ia1WjEQyUAvAGFwa9ssH50kHA0YqtrokQQvO0o5meeQP3hG8jnPOqDOkKRKDWfYr5Po19BNRKkkXChWSWpxOhKStc36cUGwlMxWgoiAn8wZb2bIx1SCfPgjSUgVKRSoD8Z4/sKUkDujl20p1QuVvDqdZX2XXtozaiMfLXH2m02Zb2PF+tYWkRODYikyqjdwkt08lZAIgXbbTcLmk716PomNaNDPzFIdvn0YpORYpsVCY3AobM7QSuEVEtdtvwcfc0g7enM1SuYuezGjXouuDHJtokAwiizLholA9lV6Wy7LMdlZC5mrlWh3nKJIxVdT0hyCVgJqpEiuxrtKQutDh3NQUYKdjmb4lzeLrJtOfi+jpx30SUkpsSsKyg3dpnrVpi0K6jlgL4w0DoqIhH4QzFaR8Vr5EEKwlrC/OIAvdAg3TLpaQZdZ6eYr6NDmiWpnVgeZq2YRxnrk27Z1OMCw5sp3W5Wze1MtZkLa2yulNDciIFSF0NNWJodRA0AKdD0BGVbhfHM7Wpv5rLU6FhBqQXIbZNuaJLqkgGnx4mNYQZLXRIp6DkpVtUjWHPwh2PwNJRqgGuEEKioSsqz62MM5rqkUidOr+xhl4irHKsvFRQ94fT/XQJ86m8qM1xtADD7j4rIMEVzA5pvzGOabSw1pXNDChK6hwTqD9/A7h98loVfuANRSZBWCls6dddAKJLpXxOc/SGdo3tP8ZmF69j9KyHNwyVmS0XEHR5RbJK4Waqz//AAwe0eQkhcO6RfNukmgnKpR88zEUKydJ+JYoSoWkISqciGwcp9MYoRcP5GQSlfR0FyXXmZEaOFowRsxXlSKVjzC5QsL0tEswNSqZBzAmwjwlFCcqqP7QQcdFeZ61YouR53D5zmxMAQmpbSD3WurbZwtQAOncfRQp74y8M4qxLeuY3/8ADVu1dZ2SoReDq137dp3ZWlpNeGW9xqrWAumAzPdLD1iIlcg+Nbw0QtFdnK6maicsLmPQkyFbzlmhN8dXGGWr6HqcY0fJvRXJtrCqs8UZvi7Llh9G0NeV0HgDfUznJv/nn+9Im76U6mKNM9klSgLTokuZSc62N/tsTWjZLS3iZVp0d/XMdbyqPWdfIXFJRIkpiC7oSkVu7wPRPP8lufeAuMBdy9f5YH7T0kfY3p6Q3mV6vc7Zzl9ya3ODKwyAO/dZQkgOK7GzTNPJoURA0TpZQSLOepPKsw/P65bOYrVTj1/AQoknrXIbVSlj8+zQ//2Of59cfvYuhLGvoBgXUmj25AZ0+WUDdRbbL1ZxNwTczc2SH+6Rs+xyf++Xex9E6VXR+Pr/jefyWtjMvBa1aJpJGCkwvobjvkqn3WliogwSr7+Kkg7ulZHshSKUtiMlIwUnQrJp/zWPiFO5j8hYc5879vRngqY19O6ddUtu8OOP8eE7SY850q+bMac++u4I9HCDNksNKhoSdYekJvwyW6vkvSMkEKenUTqacYpYDmfAmp7tR8FCOSnka6c7kHZho0Wi5J22BgvMnWUomPpTcSRBqHh1fZ9HJs95wszfpcgcSUKDFoEz2euzCN1CQNVfKHwU1YRkR3Ncfvq0fp9iyEInk8v4uobxB3VaST8HAyjWNGTBXrLHbLpKqkPyzICQiLEkcPQUjUNYvuCMiBAKFIipbPV7wpUlOy2C5SsAL8RKfVcbJ8D3WnxkdPkYmgMtgmTlVcK6TlWVhGRM83qGsOIwMtZvJbeJM6rZrFYL7L+TPDfLW4mwG9g0ggzSVUCj2kFKz3dVQ75taRBb7w+hzCTrihtsJuZ5MT1ghzboUwUdnMlRB2gvRVFDvmQHkDRwnR+oI9k2vsdjZ5rjBCWzhoSsrA/SZ/cdNhJvMNipqH3ClXcM2QfiEg7BkYWyrudXW8p6vU3+DzluIyipBEUuWkMU5huIPnGailECXWKKp9Zj4s6Q9BMuORbjv0b+4jQhUh4OzcEOVYMjazxdrxQaaNLRbeoiFiWLjXgC9c/n1/dYr3pYQq6a7lIFbobrqUBjsUBrsEq05GbqOnbK0U0YsBIhIYWxq6HcG8Q32zQFBJOPO/b2bfjz2JVCSL9wqaB2DfB7L07emPwQ+OPkZ/ROLvCsgPdhkabNF4qkat2CVnBez+aMyuWj0jyAkFIsySkZTjOaSR4ixq2Msa2jkLta0iIgUtF2HrEcqihQgFW0sljLLPzcOLvH/fE7hayMHSOu+cOs5wscPb7nmS224+zW23n2Kmts333/kIWlvlvUefZHdlixG3jb2i8WN7HkYokmjdZtrZpvCcweCebYSe8tOH/pJ/vf/TrPUK/MD4E6S7PcJiyvZWHjnpUTH77BvZ4OjrTtGZgQP/tsHu/zfl1uoce4010kmPn9jzAG8feYHFTom37D2FOtlDH+9hT3XYs2sdpaUxUWgRSYW7Rs6SSsF2M8f37XmG908+xqw3iJfoVOw+Y8UWjb6NUfX5obHHKKl92nsS9v52yNpSha0TA+z/NZ+ZD0i+ujCDWfY58G8bHMitssda45Fzuxh228yUtjHLPrYb4A70KZV63Fo4T0nto3lw6oUJnm5O0Dhf4eDYGt3QYPP1EW92T3Igt8aSV6Y7KemNSywtRn0uh13wCYcjbCPCuKGBUCRz/SqnOkPMdmpc8x9WMP+sxEStwcCnLbqTknqcY+7vpKy/TlL7pEVvLGWg1OXgzy7jWAGjY3Ua1ydsPD2EMtbntD9CUomhFsC4d0W3fVYk+NJkrL5UeM1aIqYeI9zMFBSKZKTQJkkVOkUbc84iGA8hFihnXLRDbfw1l8JDOVrXh1nVqpUiPJUzv3oL+37icc7+19tIhgPOvTePNdxh40iBz9UPM3BMsvW2lDDUKNk+qi92qlJTVm+zMLo51JaKHA5IexrGlkZ0oA99jf5EnFXJpuKSVZJ7yMH+ni2igQi1oUMkkQ2Xg4dWebS5i13uNiNGk35icvvABc72aihIYqnQ/I1JFn6yj7W/xZPbk7xp6DS6SHjumjG+Ut/H9x94itnxQfppliV5qLjNzbVFvtI8QEnvc6iyyp+v3cDBsTU2yy4T+SatwMZWI3w1mwlJTMnJf1FD9RT2RC5PeDPsHtriS40DDJltjtYW0EXCeLVJXg9wtJBNP4cSZn56SfdY9YvcN/0CXqLzqaVDDLpdrisus7ST+ZlIhclik0LN4w9XjvLG2hmKUy3O/kOHG6fnWO0VmP2nBWSisLuyQT/SOflzVQqtKRbNCmODTeZbFbxQZ6TcZq1ZoJrvIYGHW7vZ527g39xjtNyhbHiYDYUg1thb2qTxxCAfbtzKalBk0q5j72+SJAqNPxlDN2H0F2JQuvi/nJD//QLh+/pUjD6KSAlSjS/+092IUQ/9N0bozmSsapFUYSuLK23f55F3Auotl/VfKDP1q2D+1Crmpop74zbO75Sp/xsX3Y4Y+pjF0juSK773r7aMeIkQdg1krGAu6QQTIecemspK7QcTEltiLhnEe/soqxraw4UsD2QmRfQ0lJoPWzpjX05ZvFdw9r/exp5/8SjhvTcz9z0x8ZYDEzH9WM/M7C0TdTwklYLcHZusNfJZDUpZUrN9gqaCsmWj9SAsQFQ3KZ1SibNEU/qjKea2mkX/J7K8DLWpYdYF4bUe+gsOf7p0A/1QRxMpJ9vDNH2bRt+mt5LPLBxAe0+f1RN7cM4ZLFxj8snoMIaaYD7vsD3o8qFztyIThXuuOUV6JseTsweISglD03XyZoCpZkr31EO7EAl0D5v4Z4pwM8wuDiFDBXdVobc3JrETZts1vq/yOP/jkXfC7Yus9gu4esjxhRG0BYtUz4Kk0kowIlholSibfS60KxxbH0VT0qxSVqSMmQ02wzynm4P0Qp3IVDn+yAw33XmacaOO/EIFZTplvlwmTlRk20BqKUcqi3zs+SMQCwbNDofdZXqxyapXABdOnh/FKgQsr1QQiuRobYERvYn7VZfB962wy9nioWs6nFur4Q3ojH055Kbvn+OTwfUAyK+WwQDjvg02NgucuiaPuabheJLOUQVbj6kZWfwmkipSA9sOaf9gSL9tYZ21GDGazPx5wNotNt6gQvJABevOBvG2xcqPeohHJlASQcXxOHtvkTfkT/Gx8AhL96Zfo6e8TGRT868uB+LVJc0VwswFRAWJlQuRalbZq+QikuGAsJJQKfbwRmOMliR2JPpYD+kkFPN9EjeLgah9BVkNCe+9GeMvn8xqIxSJkILduS06kwrSSSi5HqYWkzcDKoU+1UIPqUiG3Tb+YEJiZjNGQS1FqlnylBKDiLNCvSgvCYuSZNyn5nZJnJSwJKmWunjDCf1Qp+xkpm0vymoqvH4WY5GGJLVT9g5tojkxuWVJtdKl65t0AwNnTVI0PETDQGlp2GqE0cwUj1FXGc83mXQbGErMwcIaIgU12OFKVSVVq4fhhBjFgNgGgozTdNxtoosYqcHuwhYjThsFiW7GJMYOl4iQ6IWQqJLlkOQ1n7FcizDU8EOdA+UN9hfWKal9puxtRnMthnJdSqaH3hNMOXV0ESMS0LoKqiJRlRS1qyB8FUcJ0c2sDmm/s8Y+Yw1XC9iT32RPfhPFTKjke+hOiFPwOWCvMqY3yK0maEpKWetRLfQoFnrYWkRrxqCqdtntbJJTffSeRPXhUGUNO5e5rUEtyWZPRn3yVsCA3qGi9RjQOyiBII5VdlXqKHpKqkuqapfOhEnsZrE61YfRQhu1r7CrVifVIcrt8PaWAmpqB0WT6MUAKxde8X1/NWP1JYKwE+JFlzSXEKy4jN64jgTWnx/KEn/cJCumG/HZvk2ib2uEHRNjXaPRz/61tu8O2PeBkHPvzTP3PTG85xb2/YPHOfPrt7DnIwEjd7UIyhKhp9Q7LnnHJ/n4AOp3b+H5Jns+3GH+UBlpSPyRGH84c12sVY3eoQB9yUAqYC9pBNWU1MpKxDd7OfRmxlFaP1ZDm+7zhrFz5NSA870BDpVWGTQ6PGlPsb+wzlaQQxGSpzfGeN81T/Hh9u3cNzRHJ7YIEo3HD5f5gco5VvYVWV8rARC7MHPbAivtAt9VPUle9fmV2TdxfXEZ59oGzY08oungzGS5KTO1bQqGz5OL+zjwGx3EyhbFz3o0Uwdzf4td9iZ9w+QTC4d5x+7jfMHYj6qkGFpC3gyYmx9HSsFcr8q406Sc79PxTFw1JK/6/Mn6EVqhfYn0Z6OVo3T7BhWtx0pUpnk0YOQvdVaHyugNleHHU/Rewu/m7sAqBOz+44CTd4zSih0++egRRvdsoikpthOyUS9g2SGakrIUVqjHOdaPKqx/dR+L15doPzDE1FvmWOvkadwRkSBYC4os9Us0rk8ghYcWdlH4VI7NuyJELFjv5FG1lJXVMh9TbiJOFRIp2P1Hbdp788x9f8zMByTn3yv5Yusatt/ukzRMBr5ksPm6iLBRYuZPe5wdrbHn5kVmT46x/OUJwpmA31h/I2mkkHR0YufKZmcyUqKr7sxLgjQVpLUwu5x2Qsc3UYTMxtQUTUtRpgPKrk8/MPBsI8tEVSWqkTD9a4Lz7zE587d1rOFO5sIokjO/nimSsx+6kZO9EeSkx8xvKrSnc0ROnvDeFukTNaJcyumfjLAfLmJe2yGJVRwnIIpVPMuiVmvTsrMiMb9voBjZjI6mJXSeGiCeCjDd7F9oqNjBVGKCVGN/bh1dJHQTk1GnxWK/zLbv0o0MpooNXmiNkhvq8tTWBHcNnQXg2amsUK4f6lj5gFGzSbDLZ3Z5ECkFT3WmsdUQ1wg51x8g/VKF0ZWUjXdHGJ8rsPDWlM5GDvSU0gWF03+3CFqBuQs9/unRLyMeLPGV8j5WO3kGnD6fPHsY7ViOJIUohq2RFCY9Zo+PceTGc3zuzEEMM0LTEp7cnGDYLTHuNNnycqw8N4y1LQj2hWwGGtG4ys3WecqPG2zeACgZN+zGTRqxK7CLXcZ+RWPuXRbryzNU3T7agMfqyUGkAmNflvglBd2zaOxX+IxykHdNvoAaCPyRiLFci/bNFqeenyA/2SZX7jOjtXhsfYqD1TXKx7Jp6tr76py9V2W43KHZdfCfqcD+HrWvGPCDoCkppAqn/56L2oVd+S6zPz6IviooaX0UNcVcUOmNQfVxjaBcZPaHEyYH1+lFBlpHISinmG7IPned039xiLXvDRj4tMVlszQDF/vOvJrwmlUiupYFpBQta1EwUmgjpaBZd5GhiqonBB2T7VAjjRSUlo5XUpCeijQSzv6QDlrM9Mdg40gBJmJEorDnIwFnP3Qje374Ge48tcRDJ65n/sf7OE6TnBXAXwyz632zxFIl/BdVgl/aYm5pACGg3cpnrlCksLlRQF/NLBFhSBJHJdEzOr+b7jnDUy/M4PdtUGApUrlrKMv7WA2LlLU+g0abBa/CmN28FNj7/IUD/ND+Jzh2ZpL7bn6BdmwDECzk0A8mjBdbzG1XsrqbDZMjt59hrVfg7tIpSmqPNb/ArcULLLy9wtJWiXKhj/nuNrfW5uiOmqhC8oX1G5n+iwiRwrs/8CCLSY7CW9Z47/BTMAyf3LyO/+vQQ3y6chhNSTHVmIrZ46tPHuSttx/DVkKOXjfPU61JGoHDrdU5xo06S2GFg+U19r9hAy/R2fRypAjKWo+1uETnTo/cVx3i/T26TZvcIiAF3gT0f77NwAct/tbbH2NUb/D/23oXN96czQA1rnFIPIuC41HRIt41dIyC4hHlUtAklhoTzhZ48z3Psh24PP3cbhavz/FDux5nNSzx4JEYEkF3s0ruEYf1vSZ6W+HgG89xbHaC5r19biytE6cqkVTI/YbN8ncVOb8ywO4PSlZeLxg02oRdg2QwxdpUqL8hYKjWovi/Bpg3B7h27xIbOy1G4kUXDsLKfTHavM3mW3340OXf9xJedVO8QsorC+y8WuDuG5E3/er7idMs8p5KUARU7D51zyGVULACFCE5UlmkGTkZvWB1kwvNKkeHFjjfqfKDo4/xufph+rHO7twWI0aLk70R7iye5sMHxil+tUoqBXGqsje/QSRVPv7EEUSg8Pfv+SJf2DhAKgVVq8eE3eBCr4qlxpQMjyGjTYrgXK+GIlL8RGehXWZzO8/RmXkUJJt+Dl1JWG0XkF+s0LohpFDp0a67TE9sMrc0AF4Wu9l94xIXHpvgb7/zfv78l+/B+sE1Ein43vFn+V8ffStH3nYCW4144AvX8cP3fYmPnjvC9UMrzP+X/aheyvy74eAv10l/3edAcZ3PXTjAULHDxv1jxK7EaAp+4Ee+yJS5xXpU5EO/9lZ6r+8xXGlj/5scWzfkaL7BR1m2cPY3sY0IVUlZf2GQ6245xwtf3QMC7HWBvLuBa4bov1HFXvM58/d1tE2DuJDRUCJh79gGjd+dZOPuiLuvOc3NhTk+ungzthbxrpFjdBOL5aDE6dYQ3zf6JP/pj99DWEu48/pTPL44haqmVHN9eqGOlILGagFnTifKS37svi8w71e5/8Je9gxuMfvVaaJiyi1HZnn83DTFxy2ah2N+4vVfxBIxH128icl8g3jnX/6puUkmh+pUrR4vrI5gGjFJqvCzhz7LWX+I090h9ufWGTfq/IfPfzdHbjzHlFOnoPl0E5O/XDjAP97/Ff587QY2ezneOfEC/dSgE1t88fw+fvTgY6yFBTqxxQdv/Z2nLpcLdfhQRf7IR950Wc/If77hjy/7uN8OXrNKxNw1Lod/7ifRGllbB7SsKE74akZpaEvCgYTysyrdSTCbGdt6fyTN5uh9hfxZjf6IZOCYRCTQmVQIyhI56WGccDj81tO0Xr/Nmd+5iVKlx0ihzcmzY5SHMqsneaBC7W1LbH5mnLAoMbczdq3WNQn5WfUS8bFXy/hCRQpGS1L8nhWWnh7FXRa09yeUn1M49LePc741wP7SBqYaZzU0vsv8doV0p3mScjKHcUOD3mwJa3ebA7V1DCXhqfsPsPv18wSJxlbX5bbROb7w0PWoI32KOZ+q06Ni9kkRNAObC5tVIl9jdKjJRiPPDRNLnK0PECUq4fEiUTFFJILbbznF99Ue51888X3cMLGEo4XEqcq6l2durZqxlukxcaIgHi8y9OYl9hQ2WfcKKCIlTDUavk3N7vG66lme74yx5efwYh1TjZnfrrB3cJN3Dj7HL91/H8SCyYNrWa+d5axNwz1HTvDk2gTtlTzvOHqMPc46D2zvpR8bRKnKVte91NgrTQVvnj7NAXuVX/7Eu5g5ush15WX+5KFbqO5qMJTr4P27Uf7WBz7LU51pxq0Gv/vw63eaaQn0poKzIsitJsgf36T7qWGUN2/z1omTWUV1qvGJT9+G3NNDzrsImTUr+wff+xn+4JfeRmsvRDM+aVcnP9yhf66I3lbwxyOsRZ3qHWvUHxzmn73/z/nl574L9bkcwQGPuff/6ytSIu//yJsv6xn55Rv+6BVRIq9Zd0aoKXohROYjNKBc7KGIjOjGzynoVoylSHh7SEVLaHZtkljNEiyVlN2/EjL37grJQMjW21LSLRPpRAg9ZeY3FeZ/vE8qBWd+5yb2/Z2nCN5xlEaxhPO9HXrPV4jyKeJGD+Xj44R3dlAUie34dDwTLVLJ7+7SD/VsBqTjkO7EQ3qhRueZUcRkn3hvjJUKrPf0qRld9FLKuN1AFwmayFPSs9kaP9Zp+yb2bV1UIcldu06rb7Mvt5EVxd20ybWlFR5an8ExQ2pGF3dXC69v0vMNxvKtSwpEU1L0Z3K4TUnrXgvrCZdZp0ZzK4fwVAZPStbvTpGK5KnlCX5q5C8xX3BYrRTo+ia7K1ssbpXQz1sZdUAIYSVFuTHLQK1e1+O5+THK5S62HlPvuKhCMu8NcK41wOqZGtaGSnhNH11PmHa3Gdaa5GdV2tdELG2VvkZy7yQ8tjJF+UM52m9L+cKFfTyZm6DVs4nP5VASKJwDJYJkh8rhc/IA5u6Y1JDMLg5RND2skR7b58uoe1Lqfy/iqH2B35i9k3hIoXRcQ/Ul2vdsstXMER2MWO6aqM8OER+KGfzjCg+/fwYhJEmqEI6F0DIpHWjQbtvoJ22WgzIb94QMPGDQsC2KiwLlWIn09gBrT59Rt8/GmXGWLwzATMhGVGDst3QufF/Ent+EuSu47zM+kauB1ZcElhZj2yF5K6DtmxyqrqEgeTYZxQ91avke9b7NeLHFiN1i3qqw3CrimiFd36R5uIQ/HpEv9wlDDXU8pOR61Dsu7ekcjtMkTlVKlR7BO45ifuoJjNuvZ9M3EBqgQrHQZ/DRmOBdbfJGwJDVYaFbxot1DldWL7FvnVUHKFkelhqz3CnSil1q5Q5ly2O+UWZ3cYu99jrnxCBF1WNIb2EqEXnFv5TktB24LHeL7CrUqRg9Hl2f5oCd8ctuDuTIqz7XVVcIEg1LiTgwsEEjcMjrPoaaYCgxhpoQxBq9mQi/ozJk+6zOJLipgpkPCBSDzqSJ5vqXGMva0qQ/E+ImKpqaYKkx1WKPtYms4JEoa6c5VOqyUreI06zB00i+Q04PWK8X8CKdNT9Ps2ejBApKlE2FHhxfJkg1fKnT3h+TH+wShtktKSo+lhXRXS5gDSgYRY+gYbHeMSERlBYFek/SGxUMPh2xeYNO7KSEDZvFfhlrqoO3kmOjn0dKgbSyNhG7anU6qUXPM2iFFq19CaqnYO4E5lUlxbQj0sQELaU4G7DYyqMoWal/ruTR3XbwQx2hSoJqyqpfZHi4SWt0iNRKaO+HyU+nqO/o4oU615ZX+ExpDLWrklRSzvQGSWwVpET1rpRPhKsFeC8VDCXhUG0NVwtpRRbT9jaqSAmqGv1YZ9ju0HSzFHBTiTGVmILh42ohvdjIiunMENcMKdk+6U6f27zjEzl5clbA3vwGYarSKJYwbr8e8cgxij+3j8awwHQiam4Pb6jI9eVTmErMgN6loHnUI5d9zhobWgFFSHQloaL3dmTweHCowIHSBhWjR04POJRbxVECJs06RbWHqwQUVQ9LhFT1jL3dVGIcLbMyCprPeL6Jq4SoIqWq9xjU2zQiB1sJGdGbbFj5rKcvMGY30UVyybKZtQZJA4URt82KU2Gi1KQfGWwoOVLdxLQiFCWllutRVTx0N2I83ySVChNOAz/RaBcskkQhCjVsJ+Sa8jrbAy67c1usVIpUzB4FLWCo0mY012KPu0mYahzvG3iaSbXapWx47HE2GNZaYKbkrQCsICvM9U3Kjkc3b+NXLBwrxBrKusb5oU53QkPrC+KcpLlbx69KxFBAwfXZn1vnpDaEl4/ZXdiiYPqcV6tM5huYakxN7TEx0GR3botnxQxCwr6BDeq+i6OHtAKLlWETqxAQVG3GK6soSFIEncDEcwwq+Z0WEanNgdwaW75LU+US4VN9v07N8hnJt9ltbSIVSNwExUw4lFtlub8XYUJQta7ovs+qeF+a2Zm/rhevEOI/A/cBIXAO+DtSyuY3Pc5rNSZSPViTuX/+r3AHe/Q2He676VkA/uKZGyAR5Ia6BKeK6PvaTFUanFkdzAiFVInwVEQ1oFbp0Hiqhupnmah5M6D5R2P497Yx7i9y1999nI8/fSNOpU/gGxQLPQbuO0PvszMZy9f9A/hHeyAkxZyPpiZsNvIknsbM1AYrjSKKkuKt5TBqmfne3XKzwsCNArodoZ5yyR3d4h/sfoCFsMqKX+K63BJFtcdSWGVIb9FKHBIEv/rQPfz0nZ/mE+vXsye/yYy9SZDq/PnSdbxj7Dh+qvNsc5wRu82jK1N8967n6SYmu61NHCXgt+fv4EBpxwXqVKn3HPYPbLDtu0zn6igi5QunD+A8b2NtSm79iacZNZuc6I4wZjfJqz6fWz3I+yae5KHGHmw1wlRjmqHNI8f3sH/PColU2FvYJEg0GqFNLFWqZo8XtkboeiaVfA8BbDRzfPe+56mHLrYacbYzwOlTY+jlIKuiDRRUL4sjDdy8ztYTQ4zfvoypxpx/cIpoxgcJe0Y3mV0aZGp0GykFJdNDUxLmWxXaz1ZJZjzKX7DZ9XfPsNgpIaXgneMv8GRjirrvoCopfqzReGKQ4Udjlu/WSDVJYXeT5noeBJhFP7NmUoF2wkUNsizkXX8ecv5vC3aNZ1wrF9arqOdtJm9f4uzCIPqqkc1yveMR/uiZm7HmDaJ8Sn5vk75vIM+5xGMB8z/yc5cduxi8pirf+/tvu6xn5Ndu+vA3Pa4Q4g1AF/jgi5TIW4D7dxrK/UcAKeVPf7PzvGYtkW7PorquEbQL2C2RtbZUJMaGRmJKvG4ho++L1IzzI8p6mygdDaOlEMUmDT2hdnQdRcgslV1I1O/eIn2ixq73zfLxJ45QHm3Re76C0KAxLLA/O4P71vPkbjpE5z+s0TsxSOombPUMyrUOaaQguhpNz8LeIazxgKBjEqoSYSZU/r3F9g9oJHpCPJhgxCpfae7jhc2RHZ4JwXbgYmkRa72DLG+ViH2Ng/uW+e/H72G41OZTD9zEPa97niBVsbSYxxvTXKhXSBKFdj6LV/zBl1+HGPSRiUCoErlmsWIPYG6qRDmJGgiOb1dQEuiuj9HaraBf26F4zxpepPHYrx7hH/30n/CRj7+JR0ZSchcU2ocifvXL96F3ITWyBN/KqRjuk/Cvypz7Oy7dh8ZYf51Emgn6lk5iSsSQj3rBJvfxjHG/d0uOP+7fRKXW5p6xWWaPTWCN95An8uimRJnpkqYZt0fwx0OY72yy8blxeuMJTAXknrGJ8tD51DjlgkAuDeIeX+PM+3cjb+iQpiKLXwQq23cFFH9+iubdFrXbVxnRm1z4s3toXx+gmQmanpA/ss3KXpuZwdXMJfxpF+XftmgfrxKGDlKRiESw6+4Fzq0PcHRqgbl9FfZZHhc2q9SKXQwjxhuNOHtmhOIJDf3eLWw94k9O3MjYaJ0VtcR108usdAvEocqh113gheenrui+l2QzhS8F/rpevFLKz73o7aPAe7/VcV6zSgSytPJEZq9ZG8NsBkakGcOYEoFMFdI040QlzRpDiyhLRbf0hDhV0JSM5k5VUjzfJMqlxFJFBDu8o/kUVDCdiCDWyN10CPnUcaJkL6mT5amQQpJmvSWVUJCmytd811hAmjGjC0CKTE6ZCkSUkcw0QwcvMNggh6EmdEITL9YvNdImzXzhNBVEiYqQsB042SxFotIJTaJIJQo16opDFKlZWnoqSGMFkYDZVAhUMuJoJ8Xa1IhdyC3IjB1MA8cKiVOFKFFxQslSWCF2suurxKC2slsmdrLGXFKFyFVApog4RW8q2QMXCkh22n4mkLQNrI5Aagr9IQu9n7X/DCKdZa9EaqdImfUTEokgSQVJrKIokqAikKnCxa6XMlRJDEgsiVdViG3oDWlYmyUAgr6OnQsIYzP7Dc00a6TeyvZfj4roPYnS0rGmPAwt3jnf19yExNXR1YDUlCieIDVACTKe1TRWUMjoG3P5gLBjoJQkthniYYGVEOU0RKKQaII0EeSMICM8UhI6fQvLCdFEgnSvnE/kFcxY/bvAR7/VRq9ZJSL0lLCYIlXwRiS5wYyHItguZA+cm6L2s7YOnVULd1PBG0rRuwJ/MCMU6m24DH/UZPU2i6QsWVdc9ny4w+mfjAj/RZW//7tf5MO/+2bEjR7FQp+a22P1U5N0/sMaUbKX8jtmKX9xnPNPToCAYLWELkANBZ3ZEu5y1gvXVSF2FBJTkhqS8Be3MR5yoWWRmJLecp6ZyTPszW1Qj1yKukdR83imOcFkrsGg00FTMsrBt+49wae/fBN3vf4FIqngaBHzX5nih773fj4VH2J9vcR3TZ3m8398CwN3rdHxTf72nscoqn0+NHobrx88x6fmD9HczhFd18O0Ina9eZ12aDFh+Dzz8D7GvhyT8xJu/OXHuN2d5Q+vvYm/s/dx/DfpfHFtP4crqzy4NIOlJph6jH1nhHP/OPp/r3OTtkrlLX1eaIzQ8ixuH51jyGizEhRZP1ggfquCkir0ey6lVOF7dz3LoN7mEX03xU/m2HqLh2wa1D7looaS9vtCtNfVcT9Y5HU//zC7rQ3+09P3MvWWjONj5ZoCJCqJmrD9vZL3Tz5OUevzgT9+JwzHXHN4gQuf24XzL+eIAovVY8PcuucTNP6hw1aY48vPHgRAzUdUv2Bx9sZxtI5g4OfXWV+qYI71uG50hThViKVK8C9r5F5v8aQ+yfjv6By7p8wP3PsQf/DYbYhAIb+g0NkXYd2xRelX8pz/WwXKoy1mn50AFZ56YYYfuO1RPnbyCM8+O4M2cGX0iFc4O3PFDb0vPV9C/GsgBj78Lbd9rcZEBg4OyH3//ceo2H02ejnuHD6HIiSPb03RDUzG8i1WugUOVNY54K4z2x/kha0Ram6X9W4O/+EBouu77KrV2ejmKNo+w26b+XaZ1sNDDL1hGU3JGjBvf3yc4Uc7eEM2qz8UkKzapE7Kvr0r8KYl8g8OUDV7jJotTneH8BONm0sLtBIblZTnWmPUrC6GEnO6OcTyMyMcvO0CNbPLc1ujvG7kPEfcOY71Jpmw6kzodRajCsNai+f6E0RSZSvMcaoxyDXldQ7kVvmThRv5mb2fJZEKp/0Rgp1O8VGqogiJl+gs+yUqRp8NP6u98ROduudQ7zoEvs708DbzGxUMI0ZVU/pdE1k3qe6uEycKfd/gPx75U37u2LvRtQRFSA7V1ljuFS81iYpjhdDXOTy1wvMnJjl0cJGTi8McnZmnqHt8/tRBnFzAWLHF+fUB5IqFWVfw9vu89ZoTNEObuyun+W8vvInJSoPVTh6AvBXg6BHnj41h1hXKr19j7cQgIoUklzL4sIq9FbP0JpWhRyWbNylEhQQlUJi5bhkFyeypMQam6/iRRqfuMjG2zXRhm7dXnufnHnsPBybWOLs+QBxoWG6IokgcMyROFDonKiQjAVMfUVj8keTS7MzkYJ35jQqalqDrCb2WzTXTKyhCcvyZaWQ5xLAjhj5ok/+pRRaaJb5v5hl++8G7MnKscsiByTVW/2Sa5pGQ0c9qPPrRn7rsmMjAwQH5jt/77st6Rj54629/y+PuuDOfvBgT2Rn7UeAfAG+SUn5LJunXrCXSbdkEfzSEN+sTH7D45Gh2gxXPpZQWfIKOSnRnmfXHLZ6+53DW1mHIoJWUSYdUgts9kpbJmc4IakslaCosDA4hDYl5bYe5pQFmJjazRLI7OwTvanN9+RSrZw+SulnX+fNPTnD9gz6dO7fYesdRjpVVql+cY/Xdu1ju7sncAAl2PSVc8VG7AelMAfG+Ps3/Nkl8YpvwbQM8/0Ke8Jc0nt4cZ185x5wxQC82eSDYy2K7TL3hgoC0q+OHOg9+6VrklMdfbN9AiuDhzx/m4BvOs9Aq0e443LbrAk9+4SBhUWKM9BgptykaPnONMq4Z4vcyGsi51Sqpp6ENBnRW8+hlH1nzqTdd0lClUOlRUvp4Gw5j+1fZ7jmcrg/SaDuwapHkEoSdQMMg+i8l1J8Jafo2hhVzemuQOFUoFvuUHI9xt0mjaNPWEuIJgZYKPvPstezfvYIhYvxNm9k1B7UaEAcqnSCPiASvv/0Ejy9OsbJc4eCNCxwpL/LQ5gzRtIpQE9LVKs3vD4haNqKrcv1N59id2+JPHr6F0T2bDDodTnxpLwzFrDXybDw6wtu//3nGhxrk9MwKcAs+3bqDsaazVcj6ETHmIz0NfmqDI3ZWmxQmGsfOTqC7YRYk33ApDXeYdBt89uEb0Ef6xJFK0DbZ/rs9Fk5MAPDhp+/BvTErm2j3LY6UF3nyewRho0T0o+FlOAxfw8vNsSqEeCvw08Bdl6NA4DVMBaCGWYKR6kUkhsBdkrhLklQVbF3nEJcs/AHYuiFH5VSM3vDZPpxF+8N81mgIKRCeghwOUGIyzg+yFhFCQNXqERYliiLJGwGmElPM+aBJ2KEGrJpfyyNxNmI6t05ib6XY9ZjcSsY4D6BtdRFxyta1GoqaovqSaKSAPyhpzRgs9MqoSko/NljzC2z6OeZbZfqBjmwbpD2N0ekt4lShcA5MK2KuW2GhWyY/D4Ya01guIpYsUikons1aVcSLLq4e4mghUaQx5HQRdQPZNEi7esa4BohQQVUlQklJg4z7xNASLBFhNFSqVg9NTekHOkk/Y7rX6xo0DISE7u4iyk7QwtBjWg2Xbt1hpNBmzM04XsuWh64nGEYMQmLP6zv0AilGXcXcUokDFQIVraWitxVyWkiaKIieyoTbYJe5ianGDLttJnINZKhkvVsyUn4m3QYDehdrPbPIcnpAUItBkaSJitQkNa1NxepnlIddA98zmBjfJiwnuOMdGAxQVElhsIupxhR1n7wWUDA8CBWSSOXQ4BrCTtDUlJLWzzoUaglpX0NtapQcD62rUploYm9IXDNESsF4qcWQ3kZTUkZL7YzA+QqR7vSe+VbLt8I36MX7P4E88HkhxLNCiF//Vsd5zVoiqQbdSVD9PN0JibUlUBKIcoCE5oxFUEtwFxXM7Qh/yCE1oD2l4o0luHZIr24iYkHa09B6oAbgD4PjBLRbeSbsBqe3BbbjM2R1GNC7aGpCudYhSRWC1RKjZotjZRXlLTejf+5J/PfdRndcYeKPlkkreUScsvKmKlq/ghJn3emnqg02ZiYpXoCwkqAfV7DUiLyhUDY8UinQRIqupviAtBIUI8GPNGwjwtxOsRwPXck4M+K1BEuNEE5MkggGrQ5nFQhLksRNqVldKnqP9XKeUafFsVIEqcAqBPiWwa5yneN9k2o+axh+MQK8u7yFo0SE1YQJu5EVvFkOK4DnqUgrRd0pZfeqFmMDTcbcFm07q0yNE4Uxp0XF6DFpZq0obS0iReDFOnN5l4reo6p1iR2J1oPKQIe+bxL2XaQimLK2qZU7rNYtDrqrHDBXmMlPk9d8dJGQH+hltIyJShDoHLBXqWkdRAzjuSb73A2+6u4hV/TIWwG9R210kbC/sI6lRDwmZgAwtRh0iecZpB2dyT3rrDYKDNodDrqrQFb49lBwDdZgSDcyM5Ip32CfvYbRUEmGFdRcRBoojOebrKdDxIkKlqAf6kyXG2z0cuw11/iKupeNXo7R3BV2wOOlSzb7Br14//eVHuc1q0SkBu6iwN2IiHI6/ZGsF2/hPBTmQ0QqCYsWA8+2WPquIiOPetSe1pBCglDpl3cIf7SMfzUsZIRCIhVEsQqK5EKvihJDxzNZ6JYpaB6bjXw2XSwFuoDT3SGqX5yjc+sk/vtuI//RR+n/kzuo3zlObGa9eN3VBKMVofZCBp9SWLq2SHkjwX1+hcKBKXLLHlNOnafrEygipWb06CYm00WFRaVEFGWzFI2FMkMzW2zcrKD1bA5Vspv7gZt3MZaqDNba1A0XL9Fp7xJEpZjcUJftwM3aGYQGs+0aqpm5Y0kiEALW+zmSUKXtm6SpgtAkMlJY6pTopzpoKRd6VbZ9N+ujK8h4ZfU061PTMag93mDhbTlMNWaz5+7EFlRONIao2n08V+f5xmjWuwVIU4VkKGS+X6GTy2JMkarQ900CT0c6KSIUnOkN0erbSCPl4cZuglTnXHuAWCroSoLvGVygQq9rkfY1HmvPsNvZxBtNWO1nyX7GvEl3ZyY1LkI9yXG8NcKQ1cHIhRhGzNnzw5irGrGrokeClXqRsG6xWi2i7lhYUaqSWilRpHJ2YwDZ01DyASe9UaJCippmuSTSTTi9NUjipnTmi+gDEiXSsnhUy+Xp/jTdyKTZdi5Zb5d/4790U7wvFV6z7oxIoHw2JNUFpXNRNv0noHyqz/Y1Ju1JE70rufCeAiKFoKyzeURQOtnGWUtJE4FRCrBWNJJdHv5IQmqmmOsqXstCRAqWGtO6JiGKVLxYpx65JJ4GTQNl00ANBX6isfruXSS6oDuusP5P7mDoAw/TmlFIDYgtyC16eIMm9WuLrNyp0t9wyZ3rsv7WSZQI5t5useyXKBo+890K5/sDdGKLxU6JIaeDYWQuwO4DK9ScHqovqOb6LPdLzHWqKAEMmF3WV0qI8w6bfo7Smewh7C3nqZldKkaf9cUyRcMjaRmk2wbK6RyiobOxXYCeltXVGDHaioG5otPxTdqphdrUcLQQL9JZXKwSRSpaW0HZ0om3s2vVPFyi37CZ364QRDpr81XqsxWKpk9OC3h2e5zVRoF23aXbtvHm84i6zqDVZSUqo3gKxTMK8kQe9wWLwmmN6rMKz26O0mvYuGd12mHW2Gt2cYi2b5FIhXjLotezkDvNq3qxwbxXRW8pzJ0f5LmNEZQwaxnaXSrg7/VxRMC25/Ds5iiRrxEEGuWhNiIWFPfV0fZ0iDZtxnZtsdHJcWxjlGfXxzi2PoreVFFP5bh5fBGtq1J0PE62hzHGe+ScAPOUjeip2fTvnEppV4PYkQwVOyzPDbBvZIMXOqMEicauwe3Mrb4CXCQleincmZcKr1lLBAkLb9EonRY0D0jchcydmX+7gxKB3oXtW2JqD2m4ayFRLtPeC28v0Z+MKZd6WVuHsQj6WsaJakPvUECt1mZzo0DJ8MjPZsV0hyur7HPWWJgq0/Qs0lShM1vi5tICy9096P2UiT9apn7nOIs/fwcT//5horfcjIgly3fnKJ1NsLdjtL7O3qNLLN0zTelszOJ9KWOfUfFv06n7DnuLm0RSoRcbJKnC80tjxC0DjBR1p/dJ8VyKd6uOrUXoakJhLmWpX0IYKWE1YdRuc76ioHck4WBMK7JQhGTPnjXyepAFQ12JNt4lbFtcN7HCC2IUTc0CC3LaIxWSvdVNqmqPJJdS0AKG3Q7FXT5zWxXifIq0ElQnRqYCv2wzM73BiNOmHjisqgmJFAxYmQK7u3KaE/1RFnplUqkQTqjMPj5FIgV7zDWkAu3dKcPXbND1TdrrOUgFP7X7ET5m3sSCWuWtQ8c5ap+H60AXCbqSsD3lMFPe5lxuAC/QuatyhmGtxYPpdbzlxhfY42zwW97rcc2YkuPR+8QwvA5eP3SenBrwweatqKqk5vaY3eMSdxzilsGegyvMb5Z548ws1+WWAAhSnQ9svonSWJNmaCMmsyZif2/6q/ynL7wX//o2ypEWcsPlyOgSD8/spygFpdOC9h6T1113hnOtKv984nP8zvqdXGhXGMu1rvjWf7XVzrxmLREEqN5OUlkgCIsQlEDrZY29Y0cgPBWvJlDDFKsekeoSowWKr9DzTKQqceZ0CBViK0tO05cMWl0bfdVgyGiDyBjDmqHNRlhgpVHcIcsVuMsKrcQmdkAqgrSSJzYF9qYk2omRGA8fx9qSqL4ksbJcES/Wsx68qkD0VToTKpYWMZ5rAqCLlKLuAzBQ7qCXApySh2VEjBbamI2EoVzn0r9Yft5j0m0gE4HaVbDVkNK5iNiVEGU8prvsLYJYY9DsZCZ3TyPwdWSyQwKtxxQtn8DTYcEmXXJwtBBLZPU3NaND2cy69BVc/xLNAVKgGzGq/7V/1JweZDwvgU5J9xjQu7QSmzDVdhaVbmgiFRg1s4dICQX5CwqNrkOnbWMv6LjzGrPeEL3QwJwzAYikxoVelX5q4KdZH+FOaBHtJLbpO/Jam4Kz7RpbUZ6obWBocZZEV4CS2kcXCf006wrodw3CVIWGcek7eJGOriesewUasctGVGArymEv6GyuFTOOmQ2bXt/ET3WinCQKNXrrLkpfoR/r5M6rJFLQHxH4oc7Z5gBhrNJMXDa9HP3AoBNeae3MTtLhZSyvFL4tJSKEmBNCPL8TxX1yZ6wihPi8EGJ257X8ou1/VghxVghxWghx74vGb9o5zlkhxP8QQlzxFRASRPqiAZkNZuMvMhmz1jCXHkCpZGNCZvtIZecz5WuZgUJIlIuL8rWTSAVUss56IpWIOGVn0gcRSxTLIvX9nePLTA7xNTkuBjAvyp0i/oqPLKW4tFzc5q+zgC/dNF9//Iuff/2xxde9XnwrsuDvN2Mhv3Q5d84ppfgrx/l6qDvCXLyOfx0uXnsh5KXf6aJSkMrF/dNL+6s7UzKXfsuve3A0kWbbvPgu/2tOLUQm14u/88VzKDs/jkImt1QAZec7KNnXVkSW8XtRbnaOJy9Syuz8vuqL7p2L3/NK3Rl49SmRl8KdeaOUcutF738G+KKU8peEED+z8/6nhRDXAN8PHAJGgS8IIfZJKRPg14AfJ8vV/zTwVuAz3+ykUoHJv/SICjqFecniW7SMLev323T25JGKIH+/pDkDy6+3GXvAw1lVGPnYWXJ37GLpPhO1GJFuqYhU0B9NSdwUe0nD7xsIQ3KuV8OrZXwgZ9UBdCXBW8vhAcQCV4XnWmPY9ZTYVlh5UxV3NaF8ymP57hzWzI3ZNPBvPULyxiOEBQ1zWzA3X+Oazy7QvXGMkQcEzT0wYTeY71cIUpURq41KynTRREESJRkLesezGMu1eO4NGoO+zXXVFaJU5atv3sX1Sky+3KcT5WhGDs3dOmkhRLPjSzSKy1slZq0aQk2RqiCpm6CnnNmsEXRMmo6NUEBJQO0pnKoP0RvOruu5/gCL3TKb7Ry2GWaJUxqksSDpmVRf6DG7MkC7YhEnCnGsksQqJ1vDbFh5upHJ+a0qgacjFEg6OlQjVoIiU2aOxE1ILA1vKY/ezaxJzYOvrs+wXc9hxoLPrB1msVTh6flJZnMejhnS33aY9bLpasVT+MLYQYatNr2JjE+k4duUn9ZoHM1n/YZ2xXRSi69u7EZXM5Y1TU+YW61SOKPSuUWAkbJ4oYZV9ThXH2C9n7U0jROVxJFo6wbdMROjrmCOB3y5vh85mU1f01CJygnPLY2hq9Br2TCYUNRjVlYq1IZafLp+Hd0ws6zWd5LrLhevxl68L4c7893A7+2s/x7w7heN/6GUMpBSXgDOArcIIUaAgpTyEZmlz37wRft8Q4gY2jM21nqfzriOva5gryl0Z/IkusBZ8WnOaDgbKblliX5yASkgODxBe0pFMRKSnkbsZG0TzG0FvaESVFMUIyF1UhSRpclrekLJ8qjoPYxaH2klSCchdqBmdXFWfArPbVE6G+GuBPRHbEpnE9y1hNxKTPLGI6hfepr8k0soERj5kHi0gnumTnuXirsi2QpztILsYe/EFpthnm3f5UK7QrPpUm+5CCFZ7RWwN7Ipw63ApRVZWFvQjBz6vSzICaB5Em1LJ+7pdGITLzWyEn8hsySqRCDNFBEpWaFgpBDGKghJVEwIagm6mmCQonjZMaUUWdvOnoXqi6wuJVCzjoNJ1koiZwY72Z1k7SSUjMtk2q0zVOzg5AIsO0QvBqiN7D/MUQLUnpq13ShEhJWEOCcJyrC3tIlmxETFlFG3xYy9yXitwUihzYjbBj0ll/OzVp66ZNDsMmY2UX0YqGVd8i7OzGhWhLGtoouEitVnyM4KJmUqcHIB/TGJUIBUoBZCpISBXI/dxS12F7fZVdomsSTxYFa9HNQyK2nCaZA2zKyuqZwgQkEx7xHnJJqRYC9nrlap2kVVUg7kVnH1EE1NccwrzBOREEvlspZXCt/umSTwOSHEU0KIH98ZG5JSrgLsvA7ujI8Biy/ad2lnbGxn/evH/wqEED8uhHhSCPFk4vVITEAIUj2brVESiK3M3Fe9iFQHo5NmVne1jBJBlFdJdVB3iJ5T+2JV104dnZVm/yh6xokqUtC0jIzHVGJ0PUGomembmBJDiVG7AWgqSixReyFhTkHrp6SGINUFYUFDGxslXl5BCUFVU6KSiej7pBpYrSwI6cX6pe968SYIIo00UkljhShS8SONVONSDACyQrhebGQPrypJpSBydtweLUtg6+587id6liinp6ClSE2i7PyxpVKgqilYCdJMSKUgJGvuHaYacaqg7JQCSAGoZIl3ElJNQVG+5lYoSub6JalCmGpoOzktF4l/VC1FKtCLTVKpIDVJ4kjUnZ7JiS1JbElOy7JDEyfNet6IBEcPMS4eT8vaVggtvdRlECAxwDVCDCUm1eTO/QNKJOinmRWgKQmqkc1ISSlINUmaCEiy65AmCrYWYSoJthphqRGplaJacfb76CmammQuU5ztI6wEqUs0NSG2JVIK1BAUJVMYUgp0kWBrmUI31CvrgPdqjIl8u+7M66SUK0KIQbIMt1PfZNu/7lvJbzL+Vwez4qHfhIxjtT8kWLmrQOyAv8dHpoJ41mTkEZ+No4UsC7UZsfgOSVAeZOKjc8z9yHRGEBNlD6FaDsg95NCbkKTjfpYtqSUgYKFdxmhJeqHGcqdIwfDobrkIM2tkmBqS080h0pkCW9dqSBUGn1KoXwtaPyuBR4C5LWjtmkYJpxn81YdZvOcwG0cM3OGJrKReE+x2tjjTGCSnhYyaTRqRQ7HkcSwdQ9QkaarA5ypY9/Vo3ehB32RfbgOAJ66fph44vG7XeZZ6JYJUo3PEx84FHK5tsN7PE6cKtVyPC9sVBkZapBJURdIPDDQ1QS/5JIlCEquYbkjoZ+THc9EA1liX+XYZVWSMZZYesw7knABLj9naytOZthEiy/CMU4XDw6uEicYLS6O4ro89HNH07EuVyJYRUTqwyZlGjbViEW20T5ooDJS6dDyTOKcSRyr10EFTU8yqx4nGEJFU8GKdhU4WarPdkFbXQjNjokThhcYIaVkgpjO6zHZoY20peEMJ5XyfpuJyzJtkrZenYHiUiz00NaF7/xCmDsZ5i9QA7zbQn3PZKORwtBBFSMJERclFuE7AwpNjiHyKFxhshhnLf+AbuEUPWRDZtPlgQP5Bm8bREKXlMjlUZ+HUIGdrQzQDm43Z7NpeKV5t7sy3pUSklCs7rxtCiD8DbgHWhRAjUsrVHVdlY2fzJWDiRbuPAys74+N/zfg3h8i6inkTMcamhulkbFz+gM6592mgR4ieivnGHvvsPgvlMmduHiRux+h1FdkwGJhpYOsR9vdsUfccam6XzV6OzlMD3HTPGZ6Zm2Dye1boPDNKK3Z5cKjAwEiLyr+3kEIQ/uI2y8+MIN7XR1E9pqoNlq4tEm+47D26dMmymJuvYeRDVDVl8Z7DTLz3Bc785lHErT266zmq71pjJShmfWHaAyR5QSey0JSEEadNLxwgiFQO/+ApljolrptY4plzkzQjh15icP3MEmWzz3ObIwSRThBrlCtdGvUcp+QQupbQDwy6p8okIwH+Yp7USjE2M0XqbRYxjYxop7C7ianHeEaM9/lBmrtc4rN51gYszGWD5VqMEijobYVAtQmAwrpg4x0eu/6nwZn3TJCfU3hifx60FOGrtHsaX14/gDOnM/0HSyS1Iu2ZPFvv6TNWbXHeqxF2DJyyR+9Lg0gDuK6LokgeOzlD5XGd9G0tep8f4tHBQRJDUjol8KsCZ03iJqCGErOZsHFklM8dLpF3feaPjZJqwEGfypct2iM2h+89w7hRx/vcIF/dW8Ud7aCngtw962w3cxwcX86srvcmeB9xWHtwjOe0AVI9m0Hae+c8p8+Ncs/dz/Pc1ihTxTpfPLWfG64/z3yrTH25hHBiSg+baPdtUX7vJqrnUHV6nHlhnNuOnuYLF/YRBRo33zTLE8/tvrJn7lUYE/kbKxEhhAsoUsrOzvpbgH8LfAL4UeCXdl4/vrPLJ4CPCCH+K1lgdS/wuJQyEUJ0hBC3AY8BPwJ84FudX/EEuz6R1QclVsoSOVJg/0e2aR2uoHclYUHQuzDAsgETD/RYvtth72+dpn3XHlbui2m0XFqLVtZcu6mx5ZSyznRTAU+9MMOt153lqa/uR0z2qZU7HChtcP+JA2z/gIZIwHjI5eA952n+t0lUX7IxM0l5I2HiXJele6Yxmpklcs1nF4hHK0Qli40jBmd+8yj7fvwJgnccxR5QWZ8b59B7V8nrATk9wFYjbDVirlsFstRqRZGc2a4xnO9w4st7qN20hZfoJFJw5gu7+a7vfoKeZxJs2Rw9ssCzH7oWDib4wJG95ynpHp/t2hweX+X5YAISQZzP4kHs8wi3LQqjHXp9k/ZaCSUU5O6uc4M1T2JKbj94jgtjFertLDaTBrnMXLcSopLCnl9Nmf2/dMoDDcIJDdXXSUOVXQdWKZt9wlTj3ECVk/troEhkN4WGxZ5d5ziSm+eTvZvgXJH+aIrWFygnchh9qL55hQWtin68yPQ7FrihssSfnb4OuSegaoYsnq+h5CNky0Dtalx/xxlG7Raf+eLNyDGfoWqb7v1DbB+NQU956vgM/3j0fnJvXWNAi5lbraIZCT3PRH/O5XlllDhUSX9RJ9ft4hzdwtbjrN9LojL77AQKsOoVaJyqwAG4Y+95Hr2wK+vC11KJU+i9sYfyeI3mgR7pkoO4RkIpYqVX5J5ds5xsDrHcLVKZaLJwhc/e189Cfafx7cREhoCvCiGOAY8Dn5JSfpZMebxZCDELvHnnPVLK48AfASeAzwL/aGdmBuAfAv+LLNh6jm8xMwNZ/GLrOofE1qhfY6KEAtUXbN9cRYkkei+mM64w8FyA3gGt5WHVJe279tCa2QmsdjJyX7WhY24raJ2staXphiCzaT13WWCYMWXLo2L00O0IUQmQ1SwgVjO75E5so3kxxQsRxUeX2LqpQOlsjN1IsbdTujeOobY87JNruCuSXO1rRXvekKD2bEQsVTqRia1mxL1hqhFLhfV+jsA3CHydZj1Hw7dRPUEQaUQ7cROtD/XQzWI5dhbIvEgkpOopfqIRpBrFQsbXKswEYccwECCdhEqhB3ZW6p9KQZpLiIsJpZ3ewKmdxTIcPaJS6KGqaaZA3Bg1FyPNFLUbkCv3Gcx1yVkBhZyHW/TQREaQvDe3wWipTbnaoVTq4Q73Mg6WVMVRAlCzXsXqSJ9wNCSopPg1yd7iJk7BJyqnjLtNJs1tdtXqjBbaDNhdtGJIqdhDKYXElZhhq8OEVUcqMDHY+P+z999RkmVXnTb8nOvjho/0PiurKsubLtNd1d63TMsiCRkQCAkjIdw7GhhmYNAwAwg3Ay8MgxikkUEGSQh52632vrq6bJfNqvQuMry7/nx/3OxCg5Gq+RheehZnrVhVecNkZMS55+6z928/Pzbning5iZL0SeY7JOZiMPRAss54qoxQJaoaEQbxd++VLSiaWD0dmhWbnmSLbfmVGMFQWEHqMct1uZEmSkgcX2PcLiFXYihU2OeBJsmmOvjpCL9pYNQFbdegr6fGWjPJpsQqCc2n2kpg6f84KNH/FYpVKeUlYM/fc7wE/L3uOlLK3wB+4+85fgTY+Xef8T1+vxWRnQ4o7jHJnw9wbukQSTBO2wAsH0ww8vGLXH73JoLJNnOp7isCr4FHW1y6RtA9XGVtPge+xNvVoSvXpHy8J/4FChSdFPUtIVYkmKnkSeku6tkkQW+I8AWhKTmxNoj38m6cXolXCMlsHUPxYe5VEaIdCwUGHhbMvaaXSIuVtM2VFIluldIvXs/g7zzO1O8f4i2pWdbcJLoSsjmxQiQVBs0q7dDgfKqXSAqeOzPONd0LPHKtScpyOZiZwZEaTx4cp8+sc2Ag5Hyih06o05z06Ruq0J9skNbjDuRGM8GqGjHQW8XxNSqlNKlCm2I55r3aZkw1a9YNtKpKuWWzHGRJ9TUpuzZhpNByDcYKFS56GlbCw9IDyqSo7MrRrPrMr18l+zIN2r7OYj1D0zc4U+yjsZBB6lGMqazojO1cpuwmmfG6SQ3XaSynoWqh1VWCTAiuygMXJ1G1ECkkj1zeyIVCD/Pne+PKkhmiGSHlpWy8MHoKDy9M0JPqw5yoMzPVy1wqz6YvNrnwXp1208S6ps5ykOXk4iC25THcXUVVIhYfHabrVMjC7RClQ9wlGyXvMVvOc3GpFylBrle0ZNGkpYWYKyr5TR3uW9xC//bV+DNdS5AfqVKczSNMiVbS0PZXsPSA5bkCye42n5o+QC7RQUpBrZ14MdMeKf8vy4n8fzp8hYVbNdKXJYs3qYipFCKE4jWQmoXUQsT5X5xgy/87x+UfGsFak0gBlUmF5UMJcukya/M5jLyDrCTRT9ms9pto4236sg3mfRVdiY2lrNe12ZhdY0dqiamD3RiBSiTjvpQbBi5x8lSa2oSBflohtdBh+hUWQ1+PlagiguomSC5KrFpIpAm6Xr3MyvQwPcd8pn7/EBv/zZM8c+MGLpR6yPY7nGv3U/UT1L0E51Z7cds6QpEoLZWH5zbiXsjgjmk8ldmAroRwOcnRrhGkFDQcEzUnEW2Vlfk84aBC2bEpWG0m+tZo+waVdgJdDenuqdPsmEwOrnBxuYeGYyKEJDdQx+tRGcrWUEWE0zHQChGW5jOej7kkgavhqbEUX0ZxcnhooMJgqsZqO40mImw9rkDkrQ6vHjrJ8Z5hql4ixkiOwtRyDxv61+jRGrQaFmraZ6S3TNM1aXZMgozKj+18ggeKkyzpGW4fPc+B1GWeLGyiGRioQnK20ku2z6HYStK0LO4aOce4VeK/zt3JnftO02M0+NSPHcJKdOjLNqh8bRBnu851o9MMWHXum58koQe4Ew4LfRpaXUV1ofe6ZebP9XJg0xSTyTit50uVTz1/gMHxGoulLMHWNk6g8caxo/z5F+/G2F6jb2KNSsNmcnKRqZVuQmHiH80j9lW4adc5nrg8wfu2f5uHqluYKebZ1LfGmRc18WND9H9J4yVLNjMnhuUv/PV1XGz1MJKo8Nen9yKE5LXbj5PSXE7XB7it6xzn2v08vTrGjsIyG+0iJT9JXm+jIPns5Ws40D/HtuQSn5/fS9vTuXloClMJsBSfL03vYlfvIj1Gk82JFWzFxZcaD1UnqXo2E6k19iWnebKxidlWHkv1GbPLLDg5nEDH0uKtyUiiwpqXil3l7TUW3SymEhBIlb2pWZ6pb2D2uhaz778ee1HS9+AqUdIisnUWb7SRasxO+cBbP8a//9jbSR0qsraWZut/rhBlbdwPNFl8ahC1E+c5jIogfesKywt5tJJOkI95GoSCobESHV9jJFPjxMlxBjcVaThmHGWs2bzz0COs+SlUIs41+nh+doDe7jodTyebcJg/24focunKN/EDFSEkCcPn+t7LnGv0cWp6kJ3ji1y8byIWBN4c7/i9SGUiXVrP93icrA5y/tQwRn8bb9VmbMsyPYkmp1f60dSIHT3LRAiuycyx6qWpBwl2p+YZM4o8VN/KruQ8acXhi6W9bLTXmO50UXFtbui6yBPlCZxQZ8iuseKkubf3BJ9fuoZzlwaYGF/FDTS25Vd4bG4DewcWSGoeb+5+kuOdMbq1Omc7gzRDky69ha26PN8cBOIu3pzeoezZ/GjfoxxpT6CKiG8sb6fumNw6eJG5Th4n1Lmn5zRH6uO8rutZ/p+nf5D37HmIc+0+rknN8nx7kO324hU17k9sffSqyWapyQG5849+9KrOkade9oF/dcD73kNytDJCsZWMDYoCBSkkZ+r91F2LSisOEy9VuuhKtim6KU6X++lPNugEOrvzC7i+xobEGk9WN9D2dPJ2h5Tq4kYatuIh7y9w6RUeei5iSvQyapZZC1KcKg7QcQ02p1Y53hrlaHEYVYlIGwpHyyNkDYeyYzOcqhIhmGkXqLkJOoHO+UovScMjrbs0fJM1N8mFUg/O+wuMvv9xpv/LYVpv6yO5KKnsiMifkoSWwKjDhxdvJH8uou+uKtFfd3Phx/uRmuRQcgHjK1kWfjGkYLm07+9lY7bE8lIee0uV3IfSRJrC/L0hte/003XnIj1WE7Xgoqsh9cU0IhIk51Sca3VGzTKN0OL0uWGQAlWJkA/nWRiNIO/BqombcjC0EE0NWVwscNJwubTSDQ2dk+dGEJscNCNk+okRlEBg71/jscoGujMtErpPuZ1A6+2Q+2KS1eti6NOGZIliJoWhhEwk13AjDUVElP0km+1VPjl7gLFMhTG7zHcqWzGVkKZvcqHVy5qTZL6a41KlQLtlcffkGXJam0uNLh6tbuL8bD8ECmvNJI2ZLEuFLKoe0ms1SKkuH1q+mYTqU/MnqHnx3On4Ojf0XaLopLBUHy/S2J5aIqe3+cu16xiwarEJeyW2/KwHCcbsMmteiieqG9mYLPLp1esIOypLXpZ+s85T9QmKTorNiVVOtEZioNGLmvX/8rYz/7LiohcxFEWSMzuMZGrkzA65rib5riYZw2FLbpWN3SW6zBb7++bJGh26zBb7euZJ6S55q82AUWNn/xIX2r1sSJbY2rVKX6LBpVY3KdVlyctS2+uxJbfKcKLCgFEjq7ZYdHL0ppokTI+yn2TEKjOZX2UgWWckWWUyu4oTamzOFkmoPmnNxVJ9+uw645kS+3rm0URESneZSJfoNlsc6J/DXpRM/5fDjP/KE/QcC5ECsmdVKtd71LYHVHf73Np1nsW7YpFV9TaHzBT0PhNPqkuvTdJaSbJ6qYu+p9qsOUlyXU2aTYvZ10XMvyIifdrA3dekYLVIai76GZsuq4WQAr2nQ3uHw7BR5kxrgDPNfjJ9TaxljZ5Ei/aBNuZwk8IjJvpw7EtcSLTJmg79AxVGklVSSQd7TqVnqIo5ZaGfSJLcXUbbU6V2voAQkNB9+hINGs0EmhaxcqdP13MKfVaDhU6O6Yt9nL8wyKVWN2fq/TxQ3EJBb/F8c4CxTIVuo8UDi5vpMZpsSKyRNRxyeofeRIOedJPJriKhq6Ig+c7SJJcu9GMqIQP9FRLzGvv651F7HLLZNtrpFHmtTVp12JlexI009mXn2J2L/W3uGDiHLkLOL/VyfG6Y5xf6mXEKPLi8meuzF1lwcgwbJfyFJAeG55hMLnOyOkhBbzGUqHKh2cuezDzd/XV22As8vLqJPek5zi70U9CabLOX2Gitfv/J/t1DxnmRq7n9c42XbCQiZdyduuRkKBhtzq32IoRkJFG5Ek3sTC4y68YIwRey9mtqipTqYisuxU6K7twKA0aVM/V+Wr7BjtwSugjJa20yhRamGqCLkD69RlJx2Z2aJ5ICQw3J6h1G9DLTRjdepBFJQY/RomYm8KWCLmJ154BVpxHE3ZqDZpUwLa5UYTYnVjjX7qfvwVVab+uj/brrsP/6KaybriHSFVrDRtwMqEkmzFW0kkbFtUFIeh+vIMKQZmCihKBXYjVuaVeCfKTSapvImgGJEFRJczxkMNek5iVoGJ24AhLq2AOxHNt34umQ1FySmkszZ3K6N00n0EknHbIJh8WNaRQglApuoKEqce6jx2gwnK1xciLDplSdc6InFt+lmkgpWN0UsaV7lW6jRUbrsNST4dJsL0bSo77RoOZbGEqIUXBQ1QhTDUjhsilZRBURzdBlW3KJQb2CG2nsSs6TU9vMu3lGrDJzToGeRJP92VlqYwmqfoLJ/CqaErE3PcuKk2ZxKKDhm2TSbXZ0L/PYpE3JT5JSXW5OnwOIFwWpsildJKU6WCJg51AsW4qkQkL1GUzVGDeKzCcK+FIju7HCSieNn9LIGA5V3+a67CXW3BTbrAU63rUATKRL5NQ2B8ZnUInIqi0s5R9ho/nPWHm5mvESXkTiK3A7MMhoLp6jxTBjKagHFnXfwpcqVd+OvUIQlIMkECfI1oI0pZZNrrtNOzSpOglCKeg1GjRDk16jTr2cpN5roYk0puKTVTtk1RYlN0nDM8lqHeb8Aq3AxAl0NBHRDOMybSswMNfb+VUi/EglkAoV36bhWyTWw+NIKlT9BFHSIrkoafUpWDddg/LIczTedgh7SRBaceJy2c+SXBAoByVyxaIzqhMkFHpFlcSyoDkqCTMBUolZI35bByExFnUQ4HWFrJQz5DJtGpZJlIiouxauE4vilKLBWpCmW49VlI81J0CBumdSLaZwc1rMEalZVO0EYSQQQhKEKn5BperE24DFZoZIl0RJmKvErnMZ26Hpm2jr3sKRFOALvJKFakLNTbAxsxbbZRo+SdUjqXqkVYc5p0CX3mLOKQCQ09sUgzStyKTqJUiqKcqeTcW1udTppuGadFkteo1W7PcCrLZSIKDirvv7dNKoWkQjsOLvILSxFJ9amKQeWDR8iwXydOvNWJ6uhHiRSl5rE5gqxSBDQWtRC+NqYKlls5zNkNUdKl6CJS9HwWhRDlM4HYMVP0tadygHKQwloBSmiKRCOXxxC4Lk/y6dyP+3I1Qoe0nWOqn4JGzoRA2dJSdLJzJoeCaN0KIRmMysdHGhHpduZ9oFmqFJOzQIQ4WnSuO0o7hisVLKcqQyRjWwOVIbZ3ykyJqTJJIKacXBEh7zXheW5mMoIc9VR+jXapRcm7l6lovVblbdNOerPczW85wsDXCyNMCldjfTzQLzrRwhCpoSMt3sYraVpxbGVZjI1qnsiGIuia5Qe9shsp94Eqc75skGtqSgNWmOSi4u9aAPtbAvlMicKHJ6aYDajoDsBcie0mn3S84u9CPaKskZDWVLE21rHSRsG1qm2TFZbmXQaioNxySd6qDpIVGPRy1I8HRlnKPVEUYyFTL9DRodi/GxIiP5KkpHQUkEtF2dVsek4xokDJ+FTo6M6aAkA0YzFYy6wF4U7OpfYt/gPGnTZciuktRcMlonPjFXNcY2rRL1uhRbSY4Wh6nXExTLGY6Vhnh2bThmfiB4qjTOruQ8e61Zqr5Nn1ZjRC/hhBq26hEhUEXE/vQMOwrLnC/1sOxk0JSIWmCTNR0QkmrHYvfAIpOZVQwzYLGVZbGTIae2WXBz6wbqOoqQZLUOCpKs7mCqAZYa8ERpA6cqA+TUNqdag6z4GfrTDVw/1uKcq/ay3IoXmKeK49iKS9jScCKdxU4WRUT0mQ3cKF64LfFiI5Gr65u5mryJEOLDQohVIcSp7zr2D6I8/qHx0l1EIvClQq1jxbzNmopeVYkQTDcLrNZTtCOD54t9ICRuoHGyOnjFk+VSuxtvKsOO3BIXWz20FtOELY0tmRWqXoIBq8b0fDczpQIXG9083x7ksttLn15juZWh5Rv0WQ1OtEeYq+dxPJ1Gx7yCNCxVU1dYEQqSpmdQatkcLw9dYWSWOjbt0ODcai+LN9rkTynUrnWZu9PAKSjM/KfrGfu1x+k54dN3JOTblZ1MfL6NYQZoz6aZ/sF+LryzD9vyyJzRKO8NaR7q4A179HfVMEsq7dEQ9Uia6GSWwQ1rnLo8hNsycAKN1NYKrbaJpkY4VYvEWYtzjT52ZxfYlV1kppanP92g3TSZvtjH9FqBvl0riCWLdt2K0YIdnZWVLFUvQdVJMNRT5dxaL63tLp1DTZ6+OM5T0+Pc2D0FEF/hnRw9iSaRAbNn+xgdKNNoJtiUW2PP2DzXjk+zLb/C/u55Dicvsuqk2Juf52RrmGm/m1tzZ5n1ujnRGaXmJVh0cqy00xRbSZ5tjJHWHVRFcnvhLLd0n4+T26U8+YE65bkcl6pdzLdzXDc4Q821qLg2xzujJFSfI/VxjpWG6IQ6X5/fTi1MsCO1yL70LPsys2xKr3Fn/1m+XdvBiFVhqtnNrtwi/oUMJ0qD7O2ap+6YnGv3s7uwyLOtDRgrGs9Uxrgpf5EnqxMcTF1i1i1wtDHKs43xFz/1I3FVt6sYHyHGbnz3eAHlsRm4f/3n7zlestsZ1YMzpX6qSxnOSEFmChBwZlsfAO2VJGe6+2m3LKKywSLZmBpez5C2XHJWh9CUrLkpFNZtH1FYc1OUnCQFow0dlcgWOIGOG8UfVS20WVjLEUWCXrtBTm9TriSRdSOWgPsqtXaCoGZQEqnYhjNUqFaTRL6K6JG0vG5ajoHrxEIyt62TUCG0YhYoCoRWbCn5grJV3bSBhOrhFQycKqT9uOyr+FBvJNALYBY1PF8BM2KlnCF/WSKFilRjqM/Sag7pKnE/i23hVi0IBUVHw5rX0eLdF2U/3vaV19JsmSxysRQboftrCZY9ldS8oB0a8evqEt0RzGbzdFoGiioJWjp6SSNI6Mh0QEjMI1lqZ6k5MarRC1TsRUF9Z4SuhgQtnYVWlnLLxjY98lYcrSynsyy1MhhKSISg36wRSYVVL42pBCxVM6R1l+VqBqdhspLJYKgB9ZbFvFeg6MXbBm/VxjXjvNRopsJMrUDZSOL6MVs2lApLTpaE6pPUPS5Wu8laDo3AohWYKCK2Vo23yQoTiTVWvTSWGjDTLhCaEl2JWOxk6bRNyp6NF6lsTLmkZyB1u8u8l6fXbDDjdRNJQUFvXWG9XO2Ik6b/ZLT3v+PFS4zsuHX9/x8FHiT2ofkHx0s2EpHKOmFLSCQxDtG31+lf66StIFLi+60olmqHsa4hlIK6a6EEMYHqhUSVVGNiVdM3UERswwlQd0xKbpKSnyREEDgakaOiKRFrXrzXllrMIXmhBZ71FnPdCFAViVAlQouIIoW2a1x5XCQFQpGE5jrhTJFxS/r6FibSBOqmDYQXL+NGGno9QEkEWGVJaEkiEwwzQMjYm1aaUZwHMX2kCpEZWzFo7fhvVNoqiicIglhNqzgKBAqqC1YpRjcqSBQkdOKISXEFWlNBaSsIBTRHxu3t/vp9rTg3EnXihVa01ZgApklEQ0O2NIJIpePrtBwj7tKNFMIE4K+fEKHAUMIre35NidBEhBPFwGhFSCqOzaqXwYl0Vt00a14Kz9Wpugk8R0N0VMqOjRdqsYfves4pkgK1pcQJBQWCSKXlGERSEEpBEClXtk1uqBEhaDomHV+n7Nl0wvgi4oZaLO4D2lGMUgykEs8zCU6gXelSLjlJNCXO//jJOHcXREr8c6RR9RO4kU7JtV/03P8/jAL4h1Ae/+B4yUYiqXybIFTJ9MVJwOt/+Ch+pHJkeYT2+Rz6aIuzs/1oCybX3nKWEyuDhF/tQr6sQqmeJJlw0UZalF2b6gdH0V7XZnNfkaOrQ4xlK3z78lY2XjPP7COjJA41WWhmsTWPLz97O9smF4ik4NjcMF25JlFTZ3B8LZY9z+YZ2bqIKmRsqgQ0OhaJhIfvq/CtAjvfepbzpR5aboLnzoyjtFQ+8NaP8eHFG/nhrvNMmKss+1kKWpNv372ThOrhRgmmr+0w/ORFJpSAE12D/NbGb6OLgE+vXke5y+bnR+9j2utmyuml7CXp+5k6u+05PrNygKzhMJoo85XZHdwzfJZ2ZGArHs3QjD1md2ksdTIs1LKoIqLqJLj7wEkGzRrddzY5X+9la3aFIbNCe6fJvJOnYLSwFY/nqiMcvzjC4R0XGUpUmRvL86ru4/hS5beOv4xM0mFzKi59j6YrtAODHqvJjh9+gg9fOMwtPRcYsGsEkcr+wixLbhY31HBCjWfqG9jes4ITxpWguU4eBclUJY7mbtl4gdPlfq4Zn6MdxAvDeKpExuhQ9W0SqsdXHtvPzkOX2ZJe4XPPHsBQA3b2L7E3M8+MUyCIVB751cPM366w8bMdmuMJNv3kZYr/fQPFnwgYKcxhKgFupPFUcZz+ZJ0Lv7KD6bdFpLIdXj52hmP9wzQ7JkYu5PbN53lsbgNmT8DZn9zKnj86xaPnN9GzpcnRj+9G/9FnWXNSHP/STsKbXjyo+UWUb//RXrwvZrx0FaujI/KuT7yWqbUuhnM1Lj0bUwY2H5yh7lqsVtLctGGKZ5bj413JNmGkrDufRURS4cTxcd580xPMdgo89vwmNDvgB7c/y6naIAfyM3zsK7eR2F4lm3AYTVfoM+tsTqzwh6dvJ4oE92w8QyswOboyTBApJAwfVYnosVtcLhcYzNSJEGSNDkutDI6vYekBAsiYDhUnwTXdCzw8t5HouSz5c1GsAylpJBfiasvE59t4BQO9HjD8uxdZPNSg9rVNKB/tZm2PIDIlY3sWqfzVEJWDPlbaxSkm2LJ1gZmHx3BGPAa+reElBc4r63jnM/g9Pj39NRptC7ejo6gSMW9ROA1D77rIgdwsvlT5yLPXMza8xsxcN4nLBk5fSGqkTvh0nvbougpWkag1jQ17F5he6cK0fFqrSdAiNDuA+QSRJrn9hpPMNvM0/fhEt7SAmTP9GFWF9L4StabFbRMXuNToJq07DCbqaErIwdRl/nz2Rg53X+ZYdZgd2SVGzTJHG6MoSI4VBxlIN5ir5mg2LYZ6qhSsNjXP4hUDp1h2Y2e+zz11ED3nErgauUKTIFIYztZo+wa27rE3N48baZT9ZMxkCTS8UGVHYZm07sQK40jBVGI4VT2w6NJbXGj30gl1nn5uM8ObV9nbNc+jixOMZqtsTq/iRhpfv+8Ae264wMHcDKebA+xKL3C500MoY+TDX17/P69aWZrYNCjHf/cnr+ocOfv6979oL14hxDng1u9CeTwopdzyvV7jJRuJKC6cPD9CYlbn/HiCVDG2yDx7agSZCDEXdZ40xnDm0mz54xWW7hnCubXB0rF+xFjsESI1yWe/eQPWlhr2lEFqQecT9cOk+pocPz/Kj9/7HT78zdtJ7VqhYLTIaA5fWtlDf66OH6p87cH9/OTLvs0jD+yKrS1LEasHFGqOIDsV4VaSCAknbtZIrAoiDWrXdNg9Ms/zD25C7QgeudbEvZAhf6hI311VhpSIimujHJQ0lnqY+38inCooCZhQAmpf20T2FRcxH2pSL3WhKhHWOyX7P3OCh+7fjZgyeOVrn+X5X96JvAkS0wbX/tunyWodHi1u5I5XHOHPH7mVtakCWm8H6agc2DHFMX0Ie1+d00sDPPf8BjAi7txxhh/vfZA3n3svP/Sm+7nQ7mWq1k3xmgZiyQZVIK0QY6zJ9DPDbDt8mS6zRTggeGxqI6Gn8iP3PERBa2ErLhsSWS60e/EjlaZvkpxV+aEf+TYbzFV+9bNv5bFj12BcX2LezXH6/CakgKFXV/BDlb984Hp+695Psddc5E/WbmFXagFT8XlmeYSM7qCpIamUw/smvoklfP7Db76L595Z59rcZT45fZD+8RKqkBSP9PGnt/wFj7a20AgtPvrgzchEyI4DS3xjehuDuTpzpRxb+1Zp+Ca+VNhuxzoRX6r8t6+/EnW4zWsnT/DRM9ehqhHv3/1ljnS2sNZI8pWl3ajLBsN3TPOlrx7i4B1nMGqC2XqedmCgCMlNfed5pLSZIFLIGZ0XNe8l4v90ifcfQnn8g+OlG4mMD8vN/++P0WpYGFaAU4oTVJn+Bs2mReSo9A1WWaukGeyu4kcKq6UMPYUGTccka3dYnOniDQePcKQ0yuxyga5Ck0N90zy7NsItfRe57w9uoP2a2ObQNj2G01WG7SpffXg/QsItN5zidLmfUjWFafnk7A6VVoKuVJuOr9OXagBQdRK0PR0/VOm0TUJPoae3HhscWS7FaorA1Sg8YlK9LS5FyhULfaiF9mwaxQerLFF/cBXxkR663zONe8sylz+9G0WR7B5c5PRXt2DfsEZC95m73MP+HZd47sgmUhtq9P6eiZczmH+zT+9XTVZe5tHbU6dUTRG4Kmo5ZtT2HnExf3WJffk5mqHJt//qWvpvn+fSdC+jX1CobNHwDzXQnk7TGgvjjlxVkjproNxYoXM2h9/nkT5p0trXIWF75D6ZIrAExg+vsFDMEbXj65ae9mLbjadszLuLNDsmN4xc5lhxCFML2FFYIpIKPUaDv764h1dMnObzz+4n31/nYP8sj85NIAQ4HQNVC/GaBlpJJ0xE2ENNxgtlhuwq042YybL4zVGam3y0lI9tu7QuZkluqjGQqccmZZ5FUvdo+QZN12StmEZP+PTnG6hKhKGEBFLhQNcsrcBkulVgNFmhE+qcWhugtJamr6/K5lyRs+U+qnWbV0ye5nh5iOmpPrZtmWc8VeZSo4ucGVulOqFOqWPz5D2/c9WRiLVpSI79ztVFIud/4Ne+5+uue/HeCnQDK8CvAV8gRnaMArPAG6WU5e/1e166i8iGYfmez9/AVLOHYbvK107vBCF5xfbTZLQOz9cHuLX7HKebQzy1NMrW7lV2ZxZYdHP0GA1sxePTl/azsbDG3uw8X5nfSdMxOTg4y4BZox4kOFEepNduMJlaZWtikaTiMecXON0couTapHSX13cf5culvUw3C+hKyFiqzEI7h7+exIVYqbjmxhWPydQqVT9O1vlS4WBmhqdqGyi/Pc+FH++PpeyPV+iMprEvlJj+wf4Y52hJfusNn+CXP/c2jK11PE9lw5tPoI2PsvxHFrXzBazV2HUPQNtfoVFKoq/oRGNOLApr6mzbvECxlWIgXefU8TFGt67Q9mP/lmbD4h17nmCq3YOCpBUaHJkZpTvXxFBDUobLueOjqP1tCpk2XqCiKpKU6bIjt8yqm+LozCjXjk/z3Le2IQLY+fJzKEJScpJsyaxiKj4pzeXZyihnnhsju7FCZSHL+KYVTDVgplRAUSI2dpcIIoUbu6aohQkut7q4t/s4o3qZz1f2c2PmPDkl7mGZTK5wqdNNxbO5p+s0j9Y2M90oMJEuUXKT3NF9lvvXtnJ8aoShwTIJ3WdHbon7ZyfZ0btMUvX4we6neKK1mQGjyrl2PwAZzaGgtTjVGrySoO2EOs3A5D1DD/BAYxtZtcPDa5tYaaS5ffg85xpxdfB1fc/x5dU9/OjgY/ybJ9/IT13zMGebA+xNzzHtdLHFXiaSAltxeceWJ69+Edk4JEd/+6eu6hy58Mb/+K8NeN9vdNYz6p1QByX28HAjDV/GepF2GJfmtHW/j1AquKFGOzRiSbPhE62Dcw01xNAC3PBvPpJQxhWDKz4n60IkN1JpBwa25hOuq2GBK7/HD1X07wLw+n/LO7UVGoTrIakj44x/lLWRmsSsgwhDgkRcGfJTEsWPvYd1ERCZMehYURS08VGC6Vl0dRORue6xE8TAauCKj4qiRDFKoB0jDISIqy8vVJ8cX4sZrus+Ly+8XyfQYf3xkRT4kYrWFH8HgCtlDMFpBwZhR8WLVFQnrt5AXE2oORbVRIKkpqApUWxepUo0NUL4cTVGShFXWdTY6c8NNZqhGds1RArtyKQlDRqBFRtGKSq+VPClShCpeKFGOzJp+CaGEtIITKpugnZkxGBlRyGh+6giohWYdDoGhhJgqjEYyI00DBHnPKqBTb9aw5cqFc++UvEw1BA30LCEjxtpWLpPwzOxTQ9TCfBDlYTmE/I3xHWhSmzFoxUa2IpL1bcxRBAbsP0jfGf+pSlWX7KLiOIIvv7sbhLzGidHfBLzOkTwnbVdRIkIc0Xj7GQf0UKCTZ9ocGn/JM/dPoJyKUG4wSFhuzSXUtQW+zixfQjzpI29LHl6Z55jY03c2RQ/ffe3+OBf38Ol/UWK3Sm69BYPLG/G0uLJMvPQGNt/YInHv72T9AwEyyEPH9iA4kJmOiI9E+93H71rA9ZaTGV/Zs84eybmOX/fRrQ2PHlwHC4nGfrAIoeSC0SHBM3ApFdUOb00QMqqUG8kMMy4CjO2ZxHrnZLkJ9pM/VEXuhrnSK49MsNXvf0YNYW7Xv00p39+F51747DkDVufI6+3+Fb3du7oPcufPHoHa14BbbzFzFw3N2y7yLHlITYPrvI/T94AixaRKXnFoWP86vVf5g3feC8/c9N9XOz04l8bG3+vTBdAlYhEiJN2+M6T17D1lkvcuSumY3jXNAkDld2ZBbq1BkqXZNXPcL7VSyswyZgO5prKm29/lrHNa/zyX70Ne1GQuqNCx9GZvn8cAP2NUzy0tIm1Z/p48w8+zbhWQRMR814Xa0GGYytDtLsMLlcLRJHC2GCRdw/O88u//hOYP3aZlw+c5uMXrqWQbDOyscjcoyN84u1/wEOtrQzvqMQ5ESvE36vynQuT9BQaFMtpdowscbw0yFCqxi2F80Tri9Wf/dXL8DY4fCp9Hd86tQPNCvjP+7/Ir3zpzfxVLYVXNdGqGn231Zn++gb+/OWgXbD5oH0jBbvDx1uHeP/GL/Fb06+gE+h0J5rEVNCrH//SNg8v2e2MvXlAvv3Td7LYjinsR5dGEEJy0/Al1twkJSfJrvwiJyuD9CUa5IwOlxpdbM2u0ApMtiWX+ItLB3nnpsd5qDxJyUmSNTrcWIiVlboI+R+ffiXjt0+zK7dIWnXo1eus+hmerozT8Ezu6DuHL1WOV4cx1ABL9QkilW6zyXw7x2iyQiRFfGXzbVqBQdm1GbDrpDUnLsOadY6WR1h8apDxr7S49NokSgiJZUFtR0DmjIZbiB36xm6eYfWzo+z/0RM8/oU9uN0RkSm599BRzh3wqX99I1nTYf5bY9zzxif5xvQ2etItmn85AAKadzfhbIq9d5xjS2qFr8zuYFvXKk9Oj8fR1rTNT73qm2TVNo3I4k9P34S8kGLPLec5+e0tuBMOpu3jtnW2ji6vR2kRF0o9HBqc4flKH0vLeQzbQ9MiTN2neqGAEggG9izjhSrj2TJJ1WPFSbPaSqF/tIviPsEtt51gW3KJp2vjaCLiQHYaX6pxq72XY9go86WVPQwk6kwmV1jzU7HOAwVdhNQDi+lGFy3fYLWa4i3bniWlOjxW2sTGVJHvzE/Sapv05BvUHuhHPVwB4AcnjmIpPkdqYwxYNaaaPbQCg/FUmecrfbx2+DjH6yMkVD/OiWSmcSOdxysTHMzNYCo+H75wGAHcNnKBAaNGOUjiRhobrSKPVTdyfHGIN00eJa+1eLyyES/UeH3fszxQ3cbW5DL/fufXr3rbYW4cksO/+Z6rOkcuvflX/lm2My/dRaR3RG77yNspraXJFZr4j3QhFVAOVxhIN5ha7uHA+AwlJ8nM08NEYx3u2HSOx+YnGMjGydKZ1QJCkbx567N8/Nh1iIpBz+QabU9nOFujYLZZamdwA43dXYskFI+E6vPFy7vwfZV8us3dg2f58sxOKgtZhB3Q21NnZTE2137B5zadb9NuWShKxA0bLnGiOBCDgfWQAwNzXK53sfrQIMr+Gq2V5JVu3OwFKO8NMYsaoSX5ox/4MO9++IfRl3Wyu0t0HulGSGj3R/RuK5J5+RTRTdcw854IcdHGXhEM3F/E/2MHXQk5e2KUuw4f56GvX4OXD7EXVNyCRIy2CYoW5Hwmh1dYaaSxDJ8wUrht8AJf+Oph9tx6nnNrvbTP5eLP2QfVFUhV4g4E9AxWaTkGTsegkGthfDQPQrDzfccxlYDpVhcpzWW+mQOgWE/R978sdv6nEyw7aY49tYkwFaG0FaQpkWbcefzynad5ZH6CTsfgx3Y+wRZriY8vHWZPbp6s2uGPj97KzrFFzi72EYYK79z9OMtehq89tJ/+HasUEm1G7QrfvLCNaMUit6nMz23+Dt8q7+R0sZ9qOYme8PnjA5/iL4qHuSV3npPtYZ5cGefe4VOcqA9dYc8GkcLFcjeep/GhAx/lDxfvQhMREYJTKwMMZWtcmOuDhsZd153g/otb+L2Dn+OXP/F2bnvlUY4Wh/nR8SdYC9KcbfZzODfF6dYQHzzwF1e/iEwMyeHf/OmrOkcuveU//GtO5HsOAeVKEhoaVZEkocXHGqU4gRk2NaYq3QShgtYWOCWTuYE8raLNooRcsoNQJP5KggvDvchQQUhYWc5hpV2mSwWGRmKwrm16uKFGQvE4Vh0mDBV8T2NlJYffr1Jv2Gh1lTAUlI0kxoqO1xWiNuM8Q8NPIXyFUJWxBsHXcdcSuImYidpwTIK0jIFCYYpIhzATEJo6atbH8xWkGTHtdWOlXcSUQUL3aRlxDsSoKWRNh3C9+5f37MFeEXgZcIYydJtlLNVHdUSMJzBjpy69AU6PjG17DYmV8HECnepqGqFHDPZXCCIFqUFac0lZLl5H4Az4KBUt7tTVAC2iVElhJTxkoGBqAX5CwaqEqCI2nCp3bCJL4AQauhriORpOQUVTwthQS4C5qiKC2HQrsBVECLMb8kSRQC5b+DtUHKkzV88ynirFZDAJnSD2yJFhnJsJpIriCoq1FEGk0PBMFDUCT+D6Or6MlaUdd90sTAoueP2cLfcxbFVZ7GRjh0E/RTswcMPYuCuMFBrFFMJRmPZ7OLvWy3h+/TNad6ZTSjqREfdnsWhxzhkg0iXNwKBUTTHvFSj7SbxIZcHNx497keNf2nX/JbuIiEzANeNzcUItUlnrTqIIya5sXI2qr0cSiogw751DV0LKns2te85ScW1u7T7H0+kNjE+WaEcGt28/e4XxMWjGRLKPf/k2br3rGD1GE0vxGdCr1IME9bRFWbG5cyyuPBzacJloXNBrNeiEOsWhFIOJOgk1JsJX/VjaHEkRy6cDjYP7ZjGUgE6oo+YkzzyRj4lkT7Up7UoglbgbN/TiXhiEZMrpxSkmeOVrn+Wrz+4hQZxEvevVT3PfZ6/Fe08L3rOHDW85Tt8TGc6W+6hcpzL7xCRKAPaq4OuPXcPhm88wbpf4TOdGrj18jiMPb0Ud6+CULX5457eZ6umlHiT4+tnt3PfNQ+x7w1kefHInUdYnW4Zg0mdyy3xszo1kpprnQP8cj8xMkH/CYPW2FHInSKEye2oHuCrmisbCoE//cJmuRJslmSN6S4mvf+Mgvc9G3PsfjqAS8fnj+xBqxKt3nqDmJ5hv5bhr/BzqeMTXF7aTNjeiKnHPkxdp3LYlbrDrGWsy08jz+ek9NE8VeMsrH+b+pS0snenl8HVnKbVsoorg5pEp/suDr0bYAakTFj/xjm9gKj6nW0NM5ouEKPRbdUqpJJEU7Mgu8ZmjB+IkdSR4+7WP82xllHmvwM1Dl7g5e5b3f/RtHH7VCXakFnk60WRLaoVFJ8drbnuaVS9NYd8qt+fPstpJk1YdPvnY9fzmnZ9hzi8wYNT42Iud/P/CFpGX7HYm0T8ih3/uFzBLAqdHonbWbSMlOD0R1qpCZyRA+ILRr0W0ezXKdzskjyRobAzRujv4bYPMCQPvhgbR+RRGVRAkwd3goKya/Og9D/C/7r+V5IYaW7tX6bfqPDC3OW6q81W0Yyle9oYn+epXryN7Me7nqW8Q5M5HOAWF3JQPAqobdbSOxLcFjX0O+UKT6BvdBDY0J31EW6VvssjGbIk1J4kfxRn+swv99HfVWClnMEyfQ0MzLLSzeL/ZT+HXZjhfivEGfb9vsfsPjvOVr12HvSLY9bZTrByuc+kDhzHqgte88VFSqstfnD/IGzc/x8cfvAm1IwjtCKRgdNsy88U8I71l5ot5tHM2kSHZc8t53jt4Pz/2+Z/iR+9+kOcbAxxdGEbXQzoXs7GVaCJCy3nop20yN6ySNR0Sms/xM2MobYU33PYkBa3Fkpel7NnMNgoIIVlrJpFP5fiRH/omWbXD73/uNXSflCzeHnv/5p8XiAA2vuscxxeHMB5L8yPv+gY7rTn+y9S97CwskdYcPntyH8N9FZZKWaJI8G/3fQtdhHzwt19H5zU1rumf59ELm0hlOjiOTlBM8Ol7/5i/LF/HkpPlqae3ECUibt17hgePb2V8YpXVegpTD+hJtvAilW25FUIp8CKNp7+4C7cguff2Z/jSk/ux+lr84OajfOTpG0jkO4Rn0qDAthsvMfeJCUbedonnH58gt3uNat1mcmCVe3tP8On5g1cYvN++7Q9fxHZmWA7++tVtZ6Z/+N//63bmew29GeH1+6iOgd8VMP4RD6kqXHybjkgEyLJFpr9BvZRkbbeB0xuRTDrUt+homTjxFzRVEnevsiNb4siFrbh5ycShWS4s9LLv8Hn+cmof6kCbTtuk4toYSshrNpzkUw/egFQk3bcss+Dk8LKS1qCCl5P4uYDVvEBvSBob1dinNuOhrekIKUmkXCrlFGwLIYK+oQor83mWF/IsL+XJdTVptU38to5oq5Qu9MfduGqCvp+p8/g3diNvgpkjm4gysfS8c6/B4vS2K1uYs+U+Kh/YwcS/e4Lyjx3m08cPohohgavyhcu7yZ8WNMZBBILMJYUZu4fcMZ2l3iHsPRUGbiliqCHPnNrI84Wz5E8LPj5yLf6yDTmP7JdTpAJJpAsCSyW0bGo7fZypLqp9bbwVG3tJJbAlX5vejqEF7O1ZZKGVY241jwwFqh4x9pTDV2/fxfU9lzAaguVXOYhIoGZDmkMRiiLZn53hqdMbSd25xsOlzTRyFnu75plv56iqCZQVk1rGQi5aaI7gK8O7GU1WqGyDw/3zmErIzrFFLjwwgd8bQCLkaGecBxY205Vsx546SZ/N9iqlrUnGkyUu6d08PzPAcLYGAXTCuCkxlIKe2xbR1ZCNVpHrrzkXs12d/BX/XnN3hfbJPMVOkuiVFbZnlqg8NkbyUIuJXIkhq0qIgqkG7MgucbQ88n3n+v82/gm7eP+pxku2ize0FOxch85QgJV3WN2fZHVfAj3r0tdbwxn1GMjU6R+sEJoQZgO29aygpn26ck00LUTaIavFGK/o50JEBIv1DFIKllsZ9vQtkk056EZAep1S1gxNRK+DKHg0HJOC0cYYaOH0RPi5kFRfExTwegMiIyIyI7REQFAI8Ps8tvSsYiU9SPuo3S79yQbdgzW0kk6m0KLeSBCULXAV7HkVtyekPiGoTcJuew5nxEMJILWhhr6ioy8ZiBB60i0G7i/S97SLH6gYdUH5xw5T+PATbBxZZcvgCrgq1w7MEiQEqiswywpeGvSUR2NDhDvkoSoR02sFZip58oM12pGJWxDsGFgiOVqHhk55BzRGFZy8wE8J6ptD7K42WreDEJAaqZNckGQuwZ6+Ra7rn6UVGnRZLbKZNtlcDCdWOwG7CwtUA5tIA7FioqwZBCULb9mmU7S52O5FWCFrKxl2ZhfZZK0w3epiMFFn3C4RJSLSlkuYDfF6A/bm5klrDnpTcL7SS823yOgO7riLCAVW1qWgxSzWtq+jdBSitsYmaxlNhIxbJXqsJsmMw0RqjUAqdEKdRmDSDgwWyxkuL3czZhQpu/YVTY2a9tHVkOpSBhEKBpJ1qmspJhPLFPdoGErIubVe8nobN9IJpEJGc8gazouf/PIqb/9M46W7ndk0KO/6X6+n2Eli6z4zq3EddOfgEqvtNC3XYEfPMpfrBTZm1654lGzOFVlzUgzZNR6fH+eXdnyTh6pbOV3uZzhd5c6uMzzbGOfW3Fn+9JffQPK981hqgKXFdhDDRoU/OHo7UaDwMwcf4PHKBCUnSVL36LGalNwkPWaTmm8xZq/nZ4LElUlY6tg0HZNdvUs4oUZad5mqdTM3083olwWzr4sFa8aijrKliXokjVRBa8HGH7jA8p9s5Np/e4TTP7eTy+8WKErEG7Y+x7f++Abyb409eY8+Mcmb7nyMTx8/yMaRVZQ75tCGh7j4+110fy5B+I4S2worHFsZQtdC1i4V0BoKuXNw288/wTX2DPUowX/97GtQd9Qx9YDsn2Qo7tUR11UJjueQ2xvoeizEa13Ise/weZ45uRHMCGvWQOysk7Edws/2xCXrH1piuZoGQNMiEoZPJEH/ywLlV3bY1F/k7t7neXBtC7bmcUP+Ir5UsYTPl1b28KaBI/znZ15Jb3edVw2d4tsrW1GEJIwUVCWi7liU1tJIV0HPurxm8iTdepMnyhMM21W+8eA+oj6Xgd4qS6s5RNkgs6HK7cPnY8vSdhd9Zp2il6LuJZip5rFNj8lckYprY2seXqTy6p5j1MIkJ5rD7E7N40iNT04dJJKCff3zbE0uc7o5wHS9ix8ZfYKHqpM8Ob2BV02eZMQqc7o5RCMw2ZZapuQnaQQWH7vuf139dmbDsBz4Tz9zVefIzI/8u3/dznyvIYTE1jxSeswTNS0fISQp3cUx44x3UnPJmjENXBGSlOGR0VzamkFSc7FNn4zqkNPbpE2XrO6QVh0SqkdObaF2IgpmmwiBocTAZltxYzZICFm1HWP0DAdb8yjoLUIpSOsOipB06a0ralZdWbe3XOeJ5PQOrqphKgEFq82cIok0BUIBauzhqygRYSIGColIkDUcZpOCrNbByxkI4SEUSV6PLSl1JcRS/ThSUV1UI8RUA+TwEMH8AjKKS8KWFpBUPRQlwtICRCBiGLQBadUhrcYiORHGuZ9csoPiR4hgXZ2qSkwjQFdDVEXS1GOEYAx2EbEFqRIrawMVhCdxQ5UoVFDUKOatAJYeEAhBGCikdJec2l5XGIckFRdvvYpiqT624iJDJZbai9jWUxESbV1tamgBQo2QkUroq2S1DrqI+12Smht7DkeCrOmwJEH4Ai9QyWgOlhJ7H6c0F1+qqEIyp2RJGy6aiGJmiFQIIpWc2saRBhmtQ1rtoEdxy0Ah1SapethKPMd0NSStdshoLqblk9EcVCQZrUPJjXmuGc0hkOrfndzfb/wLu+6/ZCMRc3xY9v+Hn0UkQqjryGSAECCDOGT11snlo/1lvDBmiWYsl6Vilqils3/HJQCWWxl2FJZYaOcwlIC5ep6k4dFrN3jm9ATX7Zyi6iYw1JDxZIln10ZYfr4Xo6rQd+MiGTN2hPN9jcF8jZZnsDKXZ9OmZdxAQxGShbUcpuWjKBE9qRazzw4hhx2ymRaNZoKJvjXOXRgETZI+bdAcD5GGBAmDG9ZYWs0B8MN7nuKvpvbSl2kwvdxF1NZQ2ioTuxaYK+XwlpKojsBeFvg31Ok0THDV2OUvEmx4y3EufGQ/ibRDLtnB+p08cz8e0PPXCdZ2C7zegNfsf44jxVHank7rRAFlS5PAVwk9FXXZYPOHVjj7sz2QDkBIhCKRZRN7pEH0XJbRbzS4+KYUubOgBFC+p4MMBV33W7T7BF5e4ucDup7RKF3vYyQ9giWb3i1F2q6B+dc5Qgu8l9VwOgZBRyOZ78RbhWIKM+MSzCXRRlrk07EFqKYHeI6OUjQICz7WtMnwzXNcOjlE7oygc1cDbybFlj9apP/TZUpukjOPTjD0kE//r02R1DwCqXBkaYRXjZ+iGZp87cH9vO72p3hidQO1++JeGiWEgVfNcG5qkPfd8A0+OXuQHxg+xl/88T3sf8cJkprLl07tZsPQGsVmEl0NuWHgMl8/v4N37XqMP338Vn7mhvv54Bfu4d6XP0W/WeNsc4CPXPeRFxeJ/NpVRiLv+NdI5HsOzQjpGa4C4GQ0TC1ECImlBaw1kiSSHrbp0fIMbuqfYsXN8PTMGNtGlpkuF7A1j7lmnrePPckXlvcCsCFZYk92gal2N9dlL9P8/RzVP03EhKog/qi25lZZTHTjqnBj7xQPLm8maXr05csM2jUu1HsY3n6ZtO7Sa8ZdvBesHhQhcUKdy6UC4YDLzuElAFbViLZvMDRWovadfpxrmwzmmqyUM2wbWubU5SGkq6C0Vb4yuwPvfIY7XnGEL3/oNvI/MUskBXf0nuVDj93NXS87hi4ivv7YNfzQ5uf4wuXdXDswy/O/txMh4cJH9rP5R59FfGeITek1HvlZk0G7w/KOJFKTpM/p2Ac93j72JKt+hs9+53aCbRF9hRapX02yfNjk0m8kEQuCZK6Nvc5PWV3qYyhb47Kd4cJbk6RmBMrri6RNl9yvFdBWqpz7tRxi2SJMhwgrpHJrwK6RJYp/Os7aqxw2Zkvsy8zypbftRldDXt53mkZosehmOV/r5fWDx/iTo6/E7VW59tB5jkyPsVZJ09dVo+GYGEZAo62Ret6k0yu5pecC44fLPDo0QU+6xUo7zfO/3IfqepyaHiS9Krj8Fsmd6UUsxefD5w8zmq/wVClW70a9HvfPTzKYqVO6roGhB4SRwusHnmM6P8cXlvZyqGcaRURU9oYstLOMJCu8dc8zNEOTb5S38eM7HucTMwfJZ1usemlec+A5nqxugM0tEqrP8foITvgiT0EJ8ur4qf9s4yUbiST6R2Tf770HUTKQeR9jxkQqEm/IR7MCwopJfrSC4+l05tPIREjPUJXiShYz5SKeTxOpkmhjh21Dy5x9bAMiAntXheiBAvlXLGJpPpeLXejPpWhN+Agr5I4t53jkm7sJEpCerOD6Gk7LQJQNopyPaoaENQORCK982UKNkB0NVEn3QI3yxQJRMkSYIQO9VeodC0MLSJseBatFzUtQdyyaHRO3ZUBNR/EEb7rzMT519FqEo0LKp6uriRCS4lye63Zd5Nh9W4lMybU3n+HxZ7aSPy0IEoLEPatYWkCxkWQkX0XevoB3zwFmXq2y7XeXOPebBcyTNkEyRi4a401MPaBWSfKRmz/Eb9/+aor/3WStmCGZ7RA9k2P4O03UugN+wMIr+mke7EDRZGj7CnOXe9CqKmFCkp8oU7A77M4tcK7Rx+mzI2h1lWjQIfuYxcF3HOOGzAU+/uOvYuqNxrpRd4Rsq2BF3L79LEc/tpvadQ6bh1fZkl1htlXgcqWAqQcon+xi9TpJ4YSC0yUYvmeGw12X+auP3Ur6rmV2FpaYa+W5/OA43qYOCPjY9R/ifefeyEi6ysVPTOKnBK952yM8sbaBiXSJS40upk8NMrpjienZHg5vjVshIgQnlgaJIsFbtjzLo2sbmVro4a27n+ETTx9CaalEdsjoVwRzPxCi6BGv2XqcLz54LQw4pJMOm7uKvKL7JL9+3+vYt2eKZ09PMPuTv3j1kcj4sOz/1Z+9qnNk9l2/9M8SibxkqzPr3OQ47Fckbl+A1xMrGCUgzZAgVFHVeP9NuA5zCeIT216SKIEgWrYotpOIEFRHUF1Nk1qMmF/LsTW7gu9oGFWJ2lChpnOp0YWfkoTpkGophevEZttSi/fcSIHwRSxOchXwFKS3vu9VJJGEyIri3IeMO2h1NWQkU2NztshossJ4qsyGXImeTJPunjrGQAuGOrQjg57+GoTQ21NnIF1nMFVHeIItqRW8fEiQkIzbJdROXMb1crCtsMLO/BK5ZIdN6TW8ew5gfPMI0g5ob+0jm27TGQrx+gKSCwrOUpLqatxOMKI2aezpZyJXore3Rt7u0N7gU9mSpLqrQH1PD+1BiYwEUTagP1kn0d0myAfIjM9AusF4qkxeb5PRHUQiILQiND2k0yfYZK8ypFdQ2x6JpRhSLSOBPa+hreqsOGlUX6ItmuStNhnNoerG7JgwEkS6QHEVQkvgZSW9iQYDRpXkYkS5kYytMV0LtzfETrnIUDCitemzm/SaTXIXPHJTIc3QZK2ZpOTaVNoJIjOi6ZqoVQ1TDdCUMGa+riRx1xIUvTRzpRzqokm33oBIYK0qmMs6yfNlpK/AsknVt4l6POSqRa1qs9jMYisu1pLKfCOHXv7H5ETE1d3+mcZLdjsjAHXZJDIk6oIZQ4oVUBoqUV8EnkJ9NUW+v47ocpGOSiRBaan4rSTcWyIloLSWZiRdpbnTRAiJqNqsvtYnn2nzrctbGeyrUrvHoi/hMJCsM1XuRnUFVlHD391ipKfC9FIXUpNYGZcwFGgVFW24iautIwjKZgxQViSqIjGKKkFaQkpSKaXp7qlz4uR47I17xsYtRESJCK2mktpaiansAmzFo9G20Ho7lKopVi92obYVtPEWX5ndgb2gojfgM50bCe0IEQi0tuDYyhCKEpH7b2ke+VmTxqtV5Jv3M/mOZ7n4B4dI3d+NmQVnQDLwqhka830QKFQv53mX/VYWblUoLQ/QXkmy8bMByqtVKjsAKZBKTJSTocCaMZh6chJnd8TY1yP0ZsTsL+RYqGV59Nk9BCmJFkKQikjfl6Q+Ifni/B4+WL6J8L0qohWy8ZMhflJj7p4QxVE4fWEYrg0gEBxbGGI+k2PlWB/axia26bG6VyLzHnXdQK8Lji6N8Mjzk6ivclDPp/jWsWvxdrRRWwq9f5RA+Q+r/GHxZo6fGWP69EbkeyskDJ+an6DVNhkermKpAUdOFdi7b4EnPJ3Hv70TEQlECNr2JmIqyUSiiJRbuPnWk/z3r7ycTQfmMbcGnH98nPP/MY1lODjSihPbHZW7rj/Odx7Yy827L/JLD72JXXdfYkt6hRPXOUy92Ln/L2zz8JKNREQIQSZEcQR+NkRrCfSmIEpEyEBBcRTMnEPzBStJT8H1daQhkXkf5/Fuqqe6UIyQmpvAOZ+lfTGLnXFIP5zA1AL6sg1WK2nEIzmWLvbw7IVxtnSvYpYEkQGmFZeWo46G2lJwmgaBo+NnItp1i6CjETgaUo8QrgKuStuN2/OlKolclWS2Q7NjMripyHBPhe13n2fbNTOMbChe8YUhFChNlWZo4nZ0/LpJ4KqMbl1haN8SfstgW9cqbkHSmIg4cNNZkLGQTISgayFJw2fuxwPydodtv7vE+F8qXPyDQ2z6+Sep7/bQm3HvysVnRzEsn2SugzQjfn3iC2z97csYWgiaZOE9PlZRYfNHy0x+cIktfzhL9iIQCSJdory6hAgECzdpTL3eJIoUbNPj+lecoHf3ClIFa1WldCDAKgo254r8yr6vMf5pgQgEl16nM3tv/B2HmYD9Wy/T95AKZkTadhlNVxjev0jgq9RaCUa+GaItmgw+LEkuQMpyef3eo1gnbKwdVXa+/Bx9hTpWUXDprYLplS7emH+GZG+LwddPwwN5Go/0klQ9unNNyl6SxVYWJYAzlT5ay0k23DDL+E0zjN4yi6pKwnGHFT9DT7bJgxc3c8/tR7k428uZY2MEmYj+vzZwV22EFbNZlI7K/Re3oE00Odvo4z2Hv8Op42NcaPRy4fTQi5v4V6sR+Vcv3u8/IjP+lMJBF1Ex8EfceFvhqahWQNQd4bYMxoZK2H1rlDs2rq+h5l3CkknXrUvYukfBbMc9Mwegy2pR9y1mXxZxXc803/jcIfa+8hwX7B6SkcJIrkrJiVv1U7OSDXetcGp5AK3XRfTChnyZlXaKVS3D7pHFK+/1fLGHhOmhCNDUkE4xC5MdCpkWxXKGycEVFmpZ6otpZqTAHmjiOjrpVIdcpk3R0Yg0GTeWqZIDO6Y48uQk7V4dx9e4YdvFuJ1/tI0Ajjy8ldH9S8zYPegpj7VLBUQg6H8ClnckCX6zTTZdI3V/N+c/fIDJHzvCys9cT6TBTTedohPGNgyNT6f5pfE3sPBbeaxnTbrnJEEijdMLZ346G1+ChMSeFqS7WmS+lkF7PAv9guZo3Ibgn8lQ9rIcLfVjr4RkM9Duh8H7FJZf0+HB57bxcHoT2Z9uMJFss3j/CIEtGbxukY6vc2x2hPC2gG0bFln40jhHerrwCyFqS6GTDpl5lUBEEYs3CewlgX9fL/eFvYy/7jKnp4Y489gWgmsbGB6MfxbG/9MUvzn3SvIfT1FKZdj8E+ex1IBLzS68QKVgtEjmXMrVQcYzZVYLKZa+NEZogeLBzW9+jkfnJrAUn5zV4a5dZ/nCB2/l1refxI9Uji0Pwbs6yMvd7BlbwI1U1P42r9tygr96/hp2ZRf5n1+5m2tvOsemZJHaNouZFzXz/3m3KlczXrKLiOIIlI6CtmYS6WCctZAKuN0RoatglFX8UZe5lTwyEshAiZWjNQMBLK7lQEgmB1ZxVI0Lc33M2B4TPSUaqymagyZBUnKx3E11LYWZdmn7BpuzRZorQyCh7lmoakRjKY3wFE63TUJPhZbGKTGIpq8TsxomnfUmPD3nYBrglSyW2zp6wufick/sHRMJ9J4O6rqOouMaOFULa15HdcHdpSHmLY7pQyRWFNyNMZHs2HLcrh4ULaQhUcc6zBfz5I7pNDaoaF6sA1nbTUxPO2lTGTIxs5A+bbDyM9fT90ePU3z3Yc5WehlI1tGUCNWLuLHvIl/9+o14OUiuBszfrjLxuQ7VLTahCZEu6HusxrlNSbISFm7VGHw4pDWggRnP9zAh8W+vs7iSJHNOoHpQ3KdgXEjgT3bYNzbL0ScmCTYrdPoipBVS61iEkcKhDZd5+qFtNIdNGuMRieEGY7k6F2d70cwQ87hNezgkNauQuxAwf4eCOdLk7EI/I8Mlql0WaT2g8DjM35GmujLEy0ef54tbJ2hPulQXB7FMnx09y9QNi6qfoOrZSAVWOmmEAP/G+rpvkULJTZJPdvAjFVvzuNDqxb29zrPLw1eqVQuXu+OeKTdBSndRzieZGupmuKdCyUux9fBl5hs5uo0WpdaL9535l6YTeckuIpEBxnALt2WgGiEd2wQBVn8L39PwTI3uQpOWY5BKuEgpaLRN7IEGnh/bP6rLFrmxDmXXRnoKMiHIGA7oEaqQGFURs1I7Kq5isKqk2JJbobZRIdJgxHC40OxFzzuoqqQr3aLumGiFCE2NyFqxpLlqJ/CCGE0YhgrtwYjMYANFSGwzls83VlMk51TaGQ3f0VCKBm6Ph30pdqazSpKlTobCabD31VGOGEzvinMlmwdXufTUKAy5cTt/2WJ8YpWl3iHCXEDhEZ3IgNLBgPQ5HTcfm1w5AxK5qhJpUHz3YXr+xxPM37mDzbkiQRQwM6lxqj6I2yXJnZdUJnWiHofSbptOr0CqQARr+zJkuqqUt+QJcj71MR2nN0KmArqe0PGTAnOXRzth0RpSUQLwuwICX0GbtVjsyhJZEYOZOhe6LEwzbn7T1TDmeNiSXrvBnNmDEGCqsSYoDBTcLgkKON2SNUtDKnHPTTrVwdQC0pZLSvdY3d9Npz/Ca5s8Ux7DLUjMpEcYqLieRnOdtlb17NjedChioZIlDBVsy0NTI0JVYGseGdPhUrsbS/WZbRSwDJ/Kapp2IqSvq0ZdT0Kg0J+s0/RN3N6QS5UuupJtLja6yRgOtY7FpWYXrab1j5j8/7Tn0v+/4/uWeIUQHwbuBVa/y5uiAPwlMA5MA2+SUlbW7/tl4J1ACPyslPKb68f3E3t/JoCvAT8npZRCCBP4GLAfKAE/KKWc/n5v3N48KCd+/124ro5l+qSt2P293LRxOzqGFWDoAZ6vEUUC39VQtFgtmbJdUh/K0hxQaUxAaErsJYUgAUEyIn1ZoTUi+dFXfIePful28mckjVGFSAdvcwfdDLAtj8bZ2D5R7XEQStwRG0UKurEegXT0OFm7HvarakQYqKRTHVrtmB2aSHgIIXnTxHM4kc6wEUvl14I0tSBxBfzbCXRWGyk2FtY4vTTAxt41ru+6hC5C/ufJG3jP7of51uo2nEDnh0ee5APPvQzbdlGViLtGzpFWHdb8FLbi8Zn7rye5oDDwqhkuPjvKTTed4myll3rbYvgHTnPpA4eRmuT6G57nt4e+xo2ffx8vv+E5/PV+j9O1Ac6cG46/CAlWdwfP0RjoqfGywed5ujKOgsTSfNqBwXiyxJ7kHM81x3iuNMRaLcVod4XVRop3bH6CW+1zvP2PfoHWcMTA9lW8UKX8fDdhOmRi0zJzTw8RmZJbbjjFcKLCw6ubWK2n8D2NcCWBCCHSJWq3y+2bznF37jT/8SM/hHZthTdOPMdfnD1IwvTZ1bvI449v59tv+D1+f/UOttjLfPo/vRy9EdJ+b5XiYg60CHXNILGphnM+i+LBq1/55BWK2te/cRCvz0e1A+SKxcbPdnjXR77Ar372rWz8WJFz7+4me05Q3RFX4PT+NqPdFfj1bi7/BChzFr/3Ax/lt/7j2ylvF2Sn4Nn/9W+uvsQ7OiIHfunnr+ahzLz3fVfjO/MLwLvib5KTwDuklC+qoedqEqsf4SpNf4UQ24E3AzvWn/MnQogXalj/A/gJYPP67YXXfCdQkVJuAv4b8NtX88alhMZKCt/RqK/F6kBdDems2shQIQwU6ispVDXCa8Y6Dk0LCRdtKmtpFm5RqF7vsvnPlpF6RGuzh9sTsPnjNWqTEcP3eYyZa/jZiJVbQ7xdbdS9NeyTCbozLQwtZOjBgK6N5XiRahn4LZ3IVwgvpAh8FXXRRCxbKHMWYc3Aa+toeoCpB4TLCcKWTnMlhSoka36Kgtbi2cY4ZzsDAFfMtYftKlsyqwxk6hzIzeJVLPbl55hq93CyMQSLFlm1zUojzcxMD1NOL9o5m4FMnbZjcI09wzX2NEeKo4xbaxjjTRoTIefn+9DGmnRCnYFknf2Dc1z6wGEm/t0TTH6oxAa7xKXAxhxuck1qhi32Co+uTMS4yP4G6f4G+aEaG3vWiBo6hUSbs81+tqRXWGhkOb44xDW5Obbbizxe28SykyZtuAwWaizX06hKRE5tc8IdorHFZ+T+kIWZLiqnutnwFYeJz4ZcmumFDW0m/7zEcKJCr15n+mIf/dkGG3pL6IMtGHCwh5vksy022asUgzSRAZ0zOR4pbiKYS7IhX+JCtQcGHBpSI6e3eaq2geVDMH97XGZPXtSxUh5hwSdh+CS2VPF6Qk7XBjhRHeJkZZCNf7HG0NdV+gt1Bh+WTL/a5kR7FLG5ydn3djH4sKS6PSI51GDyYw1syyOh+Uy9wcA6mSAcdniyuYnlmyK8QZ/izf7VTPf/bQh5dbfv+zpCDAE/CxxYDxBU4vP3RY3vu4hIKR8G/rbvxGuIzX5Z//e133X801JKV0p5GbgIXLvupJWRUj4h49DnY3/rOS+81ueAO4QQ3zdzJERML5eBgmirf0Nk7yjIjkoYKKh1Dc9TEVqECMHr6KiuQDRjk2uhSIKeDGpHiXUdZoRYXIs9ZCNY8bOIMNZ8SOJeFqso6fgabU9H64QEoUL0gg5EgPQVFD8mpEuVuKzrCEQkIBR4jk7Hi8VjhAKtqsb9IEQ0QotGYF75GzURooq/iV2rTgJfqmBENEMz9sslTjI3IgvL8BF6RD1IEBkSQw0xjYB6lKARxt43q34GUw+QVlzF0rQo9rKNYr9cqUnUbZsJz1yg4tuUwhSaFlIOkzRCC8fTY29bIdHVEEML8UMVxVFo+TFDthWul8uFpOilWQvSrDhpllsZGp6JE2g4HYOE4VMM0qwEWYQV4mZVRKCgeODmdSJNxMI6IQkKSRadHEteDrWlUO1Y1FwLRZFEfkwW8wOVVS/Dkp8jsOPPvdhKYpYUnFC/UhlbDtIUvRRrTpLIjgiTYawpWZW4HR18hY6n43nxbr/csSm3E5RbNkHeRvUkDcfEXuoQ6ZIlJ4uiSKQqCXWBtCJcVyO0DRqt9e3KOlJSBgrT7S7Q5RXN0ose/7TVGQ1ICCE0wAYWv8/j/864KsXq32O1V5VS5r7r/oqUMi+E+GPgSSnlX6wf/xDwdeItzweklHeuH78J+CUp5b1CiFPAy6SU8+v3TQHXSSnX/p738RPE0QxqV27/8B/8IkKRRG0NYYWxHeT6YhIGCmbCpyfTpNduUHKSlFs2khih2NNfI2s5XNc1TdlPxnL1ZJWs3uFbl7fy7m2P8OE/eSU73vY8zy7EzIfBfI1t2RWe+pN9qJ7kpn/zFF+6sAvL9DG0kI35NeYbORqOyeauIrYWk83Olvuu7O+lFHS+3Ytya5mc3aHcshnK1lCE5PS5YTJ9TUZyVVaaaUYyFWZqecpraeio3H3gJN86tYM7d5zh8S/uYdcrz+IEOsN2lfsuT5JJOhhqyFIxy/4NszxzamPczn+sgAjjEqy1JmjsdckVmlQv55FmxOB3FFQvojKpcc3rTrHBLlHxbc4d8Bl/OsG3ju4if0yl//64byZ/UqG2RRLpcaNg3xOC8G0lMr+XZvEGiw2fXOT59/Ui7JD0cyaKD9U9Pok5HXtJ4mUFfhqi7U28isXk5kUW6xkODMzx4KktqImQ6yem8CKNpm9y+swIrzxwnMc+up/GmKR/1wpLZ3pjFkhbQRoSvaaQvhRba1S2S/btu8haJ8XsYhfDA2XU/9pF8RqD6157Ak0JOf8fdzJ7t8o1By9iax5N38SLtPX2f5VHP7WPyded5/TyAE7ZAiPenvz4tY9wtDbC1vQKzdBkwKjxwYdv58Z9ZwgilbpvUerYrEx18+rDz1LzEzw2vYHXTJ7kRGWIg10z/PXUbrb0rLIru8g3F7bxzMs/8KK2M4P/9uev5qFM/+z7ZoDvPo/+jhevEOLngN8AOsC3pJRvu6oX/67xT51Y/fuWVvk9jn+v5/zdg/EH8GcA4ztT8k+u/wS6CKiGSUphCpWIjcYqLWkQSuXKVbxHbWCJkGPOMJPGCgtBnuusRR7qjLHZWOaZzgRvKjwdv1Zk83MHH2QuTNG6scWbep7mfQPfpC5NupQOX6zv5ad/6a+Y9wocTl7ghn0XyCltLOFjKz7tSKceWXSpLSwRG8C0+jUMIjwUpv1uqhuS7LXiwt5yEBto/8zjbwUp8I7lOd0b07G8fpX+dIMtk0UABs0aY8Nr/Hjvg1y6vYsjM6MgBb96/Zf59rf2cdsrThJECvd98xDvveF+ni+cpR2ZfMg/jO+rqKok2BbxkWs+wYja5F32W/n1iS/wS+Nv4Ma+i5yqD/LbQ1/jUhBHIO7Te5i+tsNHL/0Z7+1/C90/soZ6OcM97z7KzelzKOsZvlO3jXC9fYEv/ddr+NSxg2z+7DyyHjvcNftNWq6BqNqM3LbEy/tPMahX+HZlJwczl/ndL7+G6P09fOLj/wN/3acllIL39t9POUxxf307/+VlX6AldbR3hNT8BGnNYc/LHidEYa81w3KQJae2ueT18lB1Cw8e28Zvj36Bj1Wv43ONJD829hjF30/zyamDvLH7Gd7//ncw/B8uMjM1xh+NfQFVCH63eCObEys0QwtFRBz6ic+wEmR59+ADfGD6Fagi9sW5M32KrNZm1c9wKDXFLYk5/lS9jdd0HWOzscoFr5d95iJHNw4ypFX4TnM7h/dMcVPiIl83dqKLkE7D4k+v/QJfaG7mz7b/Bdd8//PqfxsvQmy29n0c8PLEO4ENQBX4rBDih14IAq7+/fzjIpG/1/R3PamKlPK31h/3TeD9xJHIA1LKrevH37L+/J984TFSyifWQ6ploEd+nzdm943Inl//WYw1FT8fkVhQkQo4fTFsSFvTUSeaBL4GS3EZODHcoLOYQqYCzFmTyJREox029q0x/cQIUgNzSw3xSI7M3csoQrJYymKesmlPeOhJn2vHZjj2xe0ENmi7akSRoLNqY1RUvK4wTsxVNcLUd6XQJSgdJRZaDTUJLqYJTUmUiEj1NXE6Bl25ZmwGnmjRCXTqnkmjY9FumoiSgeIKXn7nEb58fA+irSITIX2DVYSQLE93cXDXFMcfnERqsO+mcxx5YpL8aYFbEGi3lEgYPqvlDH2FOtmfCmjs6WfhVoWtv32Z539rmMwxE7dL4mck5nAMbWosp/no3X/Gb0zsZfWLW6kupzHzDvozaUY/M4d0XETCYuWOIUrXBugljezuEqWpAlpbEFoSe7xOJuGws7DEmUo/C8/3obUEXr9P/lmdnW8/zS2583zi5+5l5hUaUpNII0J4CtKIuHX3WY5/ZCeV6zwGBypsza2y1MkwXYp7Z5Ifz7J0vaDnWfAygtRrl2ONz6cOk7i1yGS+yOV6gfr9/TS3eAgj4uM3/jnvPvE2xvMVpr84gZ+Cu179DM8UR9mYLTHbyLN8tJ/+fcvMzXexZ9McEIOYL611EQQKb9r6HA+vbGKplOXlk6f58nN7Ea6C1CSD31FYvDtAGBF3bT3DA/fvRUy0sEyfLd2rXJ+f4g8fvYv92y/z7PMbmP2Jq+9xMUdH5NC/+YWreSiXf/57J2yFEG8k3gW8c/3ntwOHpJRX50mxPv6xkcg/ZPr7JeCTQoj/CgwSJ1CfllKGQoiGEOIQsVPP24E/+luv9QTwBuA7328BgRjUPL5pBX+DShApeKOxpeOo3aLUThL1Qbcds0D2bpunHlg8szzKrn0XmK4V6J9oMFfP8p5ND/NAZSscnmNjZo0NiSIP5Sd5Q/+zfOrt97D39+dZKmRIhirD6SpDiSpPDEQQwTs2P80npw4wtGWJLqvFSKLC5VZXXAbUXHqMuIt3qt0NgBdpzNTzLHdbHN42hSIkZddGK0TMVPLIh/OsHGiTTjpUiynGx4pMryRjk6hIcL7eS+KywQ+96X6++e9vQf5MvEX6mZvu439+9mXsuescac3lwSd38o67H+TjI9eyY2CJym+Oo/gR4Q9D6leTFP+7w0TuIqXlARY/GAvJvBzkzksO/NxzXJOaoRwm+dQ37ua9/W9B/aKk9zVnUd95mOY94GUkF38nj67HkVZwCvZvv8zp+ycpTRUonBI4L6+TtTuk3p9Cbeg88CuThCsJpCbxuiOEEZF9/SLP//kOnrp7nI2/vMhP95zl8/N7Seoed/adwY805t08vKLM+zY9zh9/5lV8p6+bG/ac52y7H1fRCd7WQHU1ygOCYDWBf98AfzXcx1vf9ghLTpZnlkeY7CpS1fvBV9i9eY53PPUOEk8lObXX5tVvexpTCThSGmVzrogXaXQnmsz1+XihyshwifPFHkw9ZsH84s5vcdHp42Krhzv6z6EORHzomRvZNLF8ZQ4s7c6ycnkD77vmW3ytuIvsrhIH+2ZRkNQDk49NXcede58nkoJNm5aZfTFnnuSfssQ7CxwSQtjE25k7gCMv9kW+7yLy3aa/Qoh5YtPfDwCfEUK8c/2NvBFASnlaCPEZ4HkgAH5aSvmCqeO7+ZsS79fXbwAfAj4uhLhInMC9quxw4KssVzMEvkoUCSJHBVXSyepEkcB1dFQlRgOca/ThBhqaGjFdK1CuJUnoPhnLpRzGBlJL7Qx136JtmCw10tAPa3tT9GkeTcdEU0MiqZBWHVKXFZQAnDv02Ge2ZVNzLCIpKDlJ5v0c/ckGrTBO5M0188h13xJVSMwFg8tDBWw99naxNJ9swmFhNMI0ArIJBzcXI/WMgoO/liDSYWt2hfN9w1xo91LZojFsuPiRysVOL+6Ew7m1XlKWS5T1eb4xgL9sc8HoQe7VEQGoy5Llwyb1YuxO315J0tYk3XOS5GpAZTL2B66FNu3QpP/+Fbp/ZI1HTmxBfedhuj70BKX91zJwWrKStPH1WKPRe06yel2awvMR5e0KPU+VObcnSztlkbreRPVShGEHEYC9qBLY4GUV5tQC0TURG7qqzNeyXEr1sFzOkEh4zGS6r2xvKqUUS6M5RAAoEifUkL6CMEOaSylIhKhlncyMQmiCNCIutnpijcm6Pqf7ZEBR1cjs7XBo/DLnv7Kdekdl2cmQ1lzGU2UiBGN2mU6oc3phExNbSlyo9BAECkKohKFCKzIpeil2pxeIEKTUGMbUZcXt/UtOlul6gdBTWPJzjCdLnJoZpDDcYraTZzK5yoVqD26ksjW5wnSzcDXT/X8/J/+JxGZSyqeEEJ8DjhKfr8+xni54MeP7LiJSyrf8A3fd8Q88/jeIEzV/+/gRYOffc9xhfRF6scPt6KhaROSqCC3u5u20DbKZNkGg0OyYFAptiu0krq/Rm2pyabWLwFcZSVVwQp3Pz+3lYM8sST1Orn1pdifddpuvFHdTvdkhiFQ2Ftaw1IARu8K3lrZR3+Gj1jTuX97Cjp5lzpV7aTkGFcsmjBRWFnNkNziUnTgCKdZTWEYMJRpIN1joCSjXk5Bp0XINxvMO50/2Qd7D/kaaxY1pgnTE1OU0fbtWWPZUyMGQWSE1Umeq1o1/qMG546NoTYF/rYpp+7TP5fA6gmwZjiaHIefRWkiTvK6KlILNv9Th0m8kSeqx9/DGzwYsvMcnSKSZv10l6nHIaA6fnbsGx9Op/WwW9XIGM+/QvAdK+69l8j1Pc/6DB1FTAYK4SrZyq07eNagfFAw/4HP2p9P0PK6gd3Sab63gRwrZb2dojkFzPEDNedhHbZoZlfRwndmjQ/TtXuHhhQny9yWI9AQPvWwjnbZJFAi6exp8aXonck+DvmSHo0c30bOpRLfd4rLZhW25NG2LWtLC7mmhXspQcpJMPT9I+qLK9B0+lXslmz7eRn9lRNFNsXq9ZPjbAm1PhCIiEmrAo4sTvGYszit5PSGbU6uUnCTNI93IMC7gfaFrLxen+vmVm77Mh2eu540jR0mdNUjvcsnpbf7q5DVsHl5lZLDMl6Z38fLR59H0kG69wSeeOcTB62conurltjsvYKsuO3JLPPhiJ/4/oWJVSvlrxIHBP3q8ZBvwFEegGQFh2UQxQpKnTJKnYhl6xzUIigm60y3mqzlW1zI4nk7Di13nEkmX02v9HJ0d4ZruBXQRcnp2gLMrvdw6eJHptQLX5S+jLFisdNKcWhzgcr3AhUYPPzhyBPuyjlkW7CwssdDKUqnbtCsJFqsZyi0bI+0xvVag4+u0PJ2E6dFqm9QaNhUngeIqCCFjcVe2xnQlj+hyESWD5t1NlI1NpBkhBxxWTvaROpIg+7BFOzQJn85TrKfQnk6j9reRm2KVrNvW45zQgE/rUBtdD+m5z0RtKgTHc/inM5z92R78hSTRMznmZru5/GoD5bm4F2bicx3yj5mcrg1wc/8Ud46cI39S4Y3bjqI/k0aeSzHwsML5Dx5k8iefIfOIRerJBPbTNmN/DUGkkFwUzLxK0P2kRnNUsHItNKeztOfS5H9gAXVjE7WlwpJFc7tH8oJBp21y3Y1nWDrTy0C6QeXODt7LamwolNk+vMRP7nuEtfkc+/vn8X2VtOny6huPEEaCmmsRXErRaCaQU0myz2t0GhY9O1eZL+e467oTjL3+Etf0LjDwgMrMvQlOlgbYmVlEcQSVH2ry3NIQx9eG6DfqHBqYIUJQDxIojsKFZi8t32Ds5hk23XmJ8Tum2Zuf5/bdZwhRuHvgLJc6PeTvWuLJxTGOlkfYPrbE+UsDrDWS7O1bwFY9tFNJzrQGuHvPKRqhxb23P8Pz9QHcSOdkZfDFT/5/bcD7pxkihGjBJrGm4LUtlCD2fZEzNm4qwiyqzGfzSF9BW9PxhcXKsAYLCcJUiF9TQYX71UmGu6posxaBAfcZW9COp/haYSf2lirTy13olyyWR3TqGQtLDdCbENjwyPxEzChZskhUFTqdmIeh1RWCdERpLeZeCF/EWhEBK4BeV4jcFF5CctHTCFyN7p46bsphIFMnlApVO0Hb1Wl3K7RDA9WDeSdPezRALNm4YyG9mZiavjJdYNu2eS4ujKFUNCa3zHP2mXFSgcSsKAT7G5hGgF+1SebaDPylTmVLksoOGPtiiTM/naW6JZaynzk3zHx/FlVIGlskN6fPcfwzY1z8nTwrSRs1FVD8qcP0/OkTaCPDRNkU9W1ZmnMZrCTYvS1qmzOIaF2309fBSnj0JRpU2gnCINbyhAkFoyqZGFrmuuxlLj++hQv6EISCpmlwciUFWkTXzhbZ0xoPpzaRSjl0WS3OVPtptCw8M6DnOcmKnqDndIyCyBWa7Cws89hjezieHKI/2eBYcRCZEwTpiLVihpu3nOOzA/voSTdZfmSIUlIyVejhWHGQjfkSi80sRlVhppFnaSXH5uFV3EAjkArfnN2K72t0m02eWN1AqWmzf3COublJmsUkKJKupzRKBxM84m4inIj7lh6bn8DQAmpdCXamF/nSzG4MNWB6rufFzfurFJL9c46XbCSCgDARxTSuRERoEVtD2BHSDglSEsPy0ZMekS4JkxG27RIlIrDCeLFe15WkdZdIJ5amKxEiAk2JieSGGVzpmgxDhYTqExkQ6aCrcbonTIUEtkRaESQDIl0irRBphkgjIkqEBHZEmIrfg1QlQUIikwFWwsOwvdjmQQuRUuAGGmEU7+eFEovWIg0KRiuOqVWQeoQXxEI1VImhhLGQLiAW3imxL4zqgK7Hal6EjJvE6g6aE9ObRMcFJf7svpsZrChR3NpPhHRcdD1E6uviNl2gjQwTzM2jlKr4toK0Q6QWA5ojnVhwFwh0I8DU4s9JUyOkuv57lJhZ+oKWRgnWv9a/dYKEMuaVIMHUA7T1sr2mRehqiAjlejcxhBYYWoiuxPYfYaSgiIi06RFaAikkmhmgiwBtPSmsBPGtYLRIGj55o03KcIlUSUr3rnz3Yh0MLYAoEmTVuDdHCNBFhNCjWKwQCRQf0OKzPad3CBIyhm5HSgzIRsb8FfXFq1WBf4US/VMNqQBmRJiIfWqDhIpU4sYy1QwJEyq2EcTiMjsCPcLSAxpWiGrECwUinoy25hGa8ZduaCF+EDd5tRUdQw/wPcCP/XdNNYi/IzWe1G0nRiEGtopqByhKRGBpqPbfLD5RIOL/ahJLD3ABaYWoiRBLD1DX1Z2aGnNiVSWK/1UjAkUi9ZgcZiterJ61YoKbqsRnnEiEKCKKGSUasdFSIiKwYmWuImIYklgnsONHKKFEKgI8H0S84LB+smpKhK5GVxQ8IvE3qssXlLhRNoUW9BMsLSPVDahWiBTx/fHlUly5RL1wAr7QnfyCCDc0BZqIcxKIuMOYUKw/H4QeEUkF1ZMIVcak+Rc+00jEdhG+RAQCJYDoBYAcMl7oZfy3W5ofzxcFdCOm9l9ZFCIgAlMJSGgx9d1SAxCgKhFyXVX6gmGUoYV4voauBPFcWP8diiYJ3e+aoJFA00ISqk+YiN+7ZP27EBH4CrpYJ9y96Mn/4p/yf3K8ZBcRBKgVDb2poHjrVysEakNFtlXMmkI9GZt7CztAuirF1QzCVZFNDT8fgh4xkK1TdFJIK0TPeKRNl7WBiILZ4uyRMcwNDbxCTCVL2B5VL0HhbICfVEjc5LPi6FAxrlxB/YYRU+ej+CospSBsxWAhAsHaWprMisDPKYRtjTIpZAT9PTUWFwswUMHWfYJQJWM7rDRMdEegtQTPVUdQaxrGWBP1yQypsbg72Uk7XCj14A4EoEXMVPNoOY/QsmluCFEu5GjqEilgdamP5isE7cF1af/LR7Gn43b+tX0Z2NVhMFOPu5efEJy6bYSVO4YITsVVmJVbdfLnferbsvh2HqluoPDhJ1i79lr6T4Qs9KYZfTBi8UZBmIiITmapWpLzu6C4kCM3I1B8SZBQaQ1Knp4Zo2C0KO5RsXtqdJZSoECqu4UQEi9ScboEPfkGq2d6ODpsMpCv4xVtXD1C7VdRfAhM6DrtcHlTN09sUXA3OOS0kJlaAS9QGf7SElM/2o/o5f/H3n9HWXKd57n4U7nq5Ng5T0/uiQgTkEEQiSCYk0hKokhFSzatK1GmrBxsyVfSlUxZsgJJiRRJiWIQI0QQkcRgMBhgMLlnprunczx9cqy47x81BGUvWRrYWv4Rv+u9Vq/VU9N9urpP1a69v+99n5dnGtsR5xIs7ZJxd9iohsep8iDljsXVRo6abRJZlVhvhOeyUkuE8GYhMZopsSHHuNIMCfCD6QqnC/2omoeWDh8ipT1xlIpKbrDJhWovsi2RjrRxA5mqa/JSbZBItkXdM5B0/x+/vv+pS/97zMX7qp1EhAK9zwr0ik27S6NwEAhg4AkfveLgmyqtbg3FFZR26Aw+WqeyPUZioUNt2KRwt4/wJa6e6Ud2JHQXXE9mbn4Ahto888JO9t88w5WvbUU+0KA71WBXep3HzuyC1wsQAZEnBpi47yru76RobEnSzprkny9TmUjRSVsonfCJmj3fRPLDXJn6iMXG69qM/1GA0rAp70kRqBJH/81pzuk2g9EKeb2Om1FYbqfIWC0WkmkkSXBmepDx/cvMnRxAu7XM7tQaARJPPHeA2+47yxm9j2I5xo09ixz78j6qEy6RbIvd3WsktQ7Hl0foT1aZ7suHjBVfopgLgUKXx6MkshWcusmFi4PIHZnsu4scjUzxkZtfww27Ztk4FCdt65R+RKaxmEBEXBTTZ/Pmm9n2Y89z9dP76c/UqHzAIC4HOJ5KcmubtNkmptnYrkrztjATyHdUEHDz0CI3xmb5SvcBtCsJRI+D1FJpzyRQbInG0SrOviYbl/Js3b/IvvQyfz+/k+xwGV31WDVSqIZHoUtn80adg/unGY0W+fzVQ9TjJr3JGhtfGWTyww5apIU9leDg3jkevWWFlNHmwnIvmu4xeXmA+JTK+dEQ4BTcaMNGgu7eCikzzOFxA4ULL40QRHyatk5nKokxXuNI/xyPndmFlrDpFKOQ8tDjDuunu1mJBsiaYLMePtAqjQj3jU2ynohzdqXv5TTA67/wv/dqIq9a2rsxPCD6P/hv8SMBWkXB7XaRZAE1NYQ3Xxup3hpJq0OhHsWxNbyWilrS8HIumXyNwUQVgIVqiq5YAyEkpi70c/+RMzz2zQMM3rzM1SvhU8fItRnJleBDaSQvQPuDEucuDqEkHWRJ0J+rsFGL0SpbjI1svHwOsys5NMNDlsNtS98f6sx8v0ws3aJRsejvLYcO2JVuYtEOA8kqlY4V8i06VpiV01Y5snuak/PDIZ3++Cj7b71Cy9NRpYBCO0qleQ1g7MukYm02ZrKouQ5uxQAB1qqKFwmJ7kHSw5zXCTRB/qUASUBpu0L6jjUyVoumqyP9Zo6bf+8FPvfILSiOROZiwPpNEtEVCTca+lSEBLmzPutvsRn7vtMs/PJReo/ZLDyg4VsB5poKMtg5n9isQt8TVTp9EZpdKsU7bVTdZ6J/hZemh+nuqVA52RVS4ndv4n6n5vNCkvxdK9Q/30t9GLx+m8ikiZMQWOsSgQFGWZC+0mFzwqIy4ZEfKrNZjENVQ+tqM/jHKpt7TUbeMsOB1CLf/PXbWL9ZJrWriKF6qHJAuWWxO79Gx1ep/uoQfLjA7MVelJaM0ASSK3Ho9kleWu3njqEZrtaz9EeqPHFqF7t3LVLpWGxUYgSBjHE+wsBrFtAUnyurXewdWObM4gB3bpni6dktmKbLTT2LPDW1lbn3/PvrVqya/YNi6Cd++rrukalf+On/kzvzTw1JFdBjE7UcmqaJGQu5HL7lEjEdHC8UB/Unq0RUB00J3abllkVdjXDvrot4gYIrZFJam7TRIq52mGtmOXhgBkt2QILxRIHs3iZeILMltsnp8gAz74uiVWRuUFfZvTO8eAD6o1UMxWM+kOmN1F4+11rGJGaEG2ZD8bjypkHSuTJdsQZLQqIvVuWFmWGoazino5wbSwAgRz368xVkRSDHXPqtCmfNPrJGE7fb4dT8EH5b4Z49k5ydHUBSQ2du+rhO8u1lKt2tkMxlBOBJDP19nanvi9K/a52eaI2Z57YhP1xEfTbJ8p0qXsrlXX0XudTowQtk5m4x+czpm8hdk7KXdskMPOky/3qJSFczrLNIguWuOP2ZGgu/fJShX32W2f9wBGNTAkmmNewi2TL9j0OrSzDzrgReziX5koQVs2kVoly4Os6B26bxhELd7kISErlIk5ptUq2m6L59nYTRYXGHIIj5SA0Ne2+LWLRDsEtCBDKtQKJ4MILcCcidVNh3cIXHVneSPS3jvt5l9o0Rtny2ydYf3OCR5V00RxW2/G2dPX8yRURxmKz30PFURiJFXKHw+TeN8ZbMIvJuwcZXBgnUsO6SuJbJPGoVWGqluCN1ielHdrH7yCpm2uUrzgQT+VVOR/qJ6x26zAaLVop7spOcWRxgd2yZ517cx9vf+wQxpYO1w+WPX+nF/z323H/VTiLCl/AdmaZjgq3QEQaSEiDJgmongnBltJjDaj1Ob7xOw9Ep1yMoSoBwZZ5ZHCNqOtzRO81qJ8lsLUN/rMpApMKjV3Zy0955rHWJ9XaCs/P9SLJgJZPkYH6JxrF+hCzI3NvixNWRlwFINcuk0IyiKAElO0JMCycOz5epdYyXVavxORlnUH158tloxZkYWeHc5UEit5cYj9VYaSQYSpS5vNmF19SQWgqLw2maG1H8Xon4OYOJt0ziXKsm6hGHeMTGUD027ooxqLo46xFigzVYiIOA6berxOYlFq08m7konb0B0nQGeiT6vuVTG9Z4fvsI2+PrYbzDp3W2/u0SX37mRjLnJfInSlz6V3FyzylUtyZe7mgNPRVQ+YBB7zGb2f9whNGfP86VP74ZOeaSPhYWZVfe3EFdMElOgT+v0+oVBHa4tdl92zTnl/s4OLRIa9xBNT1imk1EdcgNNzn34igTt6wxPyfTGIHB3WssneuhqhuoDTnEHpRluucCnJjE5iGP5VaS4aFNVqJJ8qZNz6ebLNwfJ9XKcDC/xOmFHJffb+GXB7BUFy+QSZltVu0ktq8Sn1E4vXOA2fUswX4bWQsIfIm8XmdrbpP5do6BSIUL7QEW3hSQrnXT8VWy0Ran1/tpzSTp6p9l047SbJqcrI0y0l1kutWNeccmT25sC4Vmi+Ov+Nr/XtvOvGonEdmWwi5MW0WKeGjzRtiqHOmQz9bY2EyQTTZJ6DYX53sxIi57+1Y4vTiAlW6Tjzeptk0eXdjB60fOc2a9j81qjHS8hW64vFgdQtxZRpYC0ukGvfE6GaOJ7aus3yKQHInz5V5uGpvn8mYX1XKUeSBiOJSWU6wqPl4Qtic8Twmr+LJgomeVk9vjKB0NXfXoTtRRpYDpx8aQxjvUT+S5LOUJNMHlWp7mLhutqCJkeH3uDCfUMY7NbIGDbV56dCdKB5wDDXTdR//LNK4lIybgTGWYyKqC+UIK7/U1ZFkQ/VoC+c0F1HM52pLF8CMBy7eFwrBmr0qnK2AYwVMrW0MWyM+kETUHtRV6YS7vS5J/Vg4hzIFAa0ggSazcKhGXAxYe0DA2Ja788c1s+/HnkSd2UP5PdSRJECxm8EfbFLs0JD0ktEdORMi9folLG924VYPnp0dIvqTjxnVe8IYhkOjurkDe5ltzW+h7aJl9sTLPLw6zZf8SWbPJc6e3EeuvUV+L46RlBvasYtejXF4KiXCBo5Dta3HhnV1kzwoarsHO+Bqbb2qhzUZZKqWwDIcfGX+GL6/v48bEHAt2luO7XN6Yu4rtqSytp/Gd8L18bHU7q2tpfuzWp/ij5bsYiRbDB5djkjZavHRpBFTB4MQ6T82P88HdT/Di81tJ7Wjx9NpW3tJ7ihfkQbqtOtsja2x2x5j8/80t9C82XrU6ESFB4MphWJUnI9RQS+E7Mq4fHrddFSdQEL6M5yo4vornKHiugqF4mLpLu63T9jXUa2zOettAVX3KdoSo4eAEKpYWPhkTqk3ZsUL9hyKotk2SWhsvkBG+hOfLeL6C3JbxhYTjqDiOiuuoeJ6C58k4vgpqQOAouL5Cy9XwhIyQw2hQ2b0mpNNDurhqunhWqHNxhYIa8RCejBVxQk2IC76nYGguSBJmOWy1yi051K4okIh0SEbayB7EDRvfEqAHaA0P3xL4hsCLCUTMw1RdGm2DRttAioQGP98UJCNtiHlobYGTDPDNUOviRcJzczwV3wpbtXLMRZ7YQXD+EkmjQ9LoQCCFMnndRzPDFmqgQVRzCAIJ9ADRUVBsgdoE0Q67bI6noOo+blsjqoXGRs9VsVSXqOogDD/UoRg+gSlIGB3Ua4FlgaOAK1N3DIJI8N9IJ3Q91JLo10hzHaFRs03sQMMOVCQtQEZgqN53tw+BRL1jINoKjlAotSMvw7B02cdUPCRHRtZ9IppDp6VjB1oIx5Y9fEfGJwwltxSXQIS6o1d+8V/nx/+m8aotrEa39YrR3/lhFEnQtjUS0Q6yJPADmVIlim54KEqAIgfcMTDNQjPDmasDjAxsslxMkoq3aXZ03j7+El9b2o0iB+xIbxBVHF4oDHJP32W+9StH8H9sk1I9iucqdGdqYQH0+CgA97zmJf5+chfJZIveRI3+SJWL5W6SRoec2SClhVX9yWoPmuzjBzIza3mCss7ojlVUKWClliBhdYhpDnPHB4nuLdEVa7BYTrGnZ5Xnp0cQvoxUV9F6WgTzUd5z39N889dvo++D0wDsTSzz8cfu5J5bz6BIgkfO7+bt+1/k63O72Ne9wuU/24lQoHJ3m7H/AsV/16I3XmehkiIIZNzJBEKCxFXoe+8sB1KLFJw4x/7qIPEH1qi2Tfp+W2X1aAzp9jKNuSRKd/tllmxwLknm8BqlZ3vo9LukX1LRHy6Ek8drllC2bWH6V2OIxUh4I8sCP+Wxc2yFtc8OIz1YJGV1uDG7wJMrW9EUn8P5OVyhMNfMcmm1i7duP81XPnUr7W7B6MElZs73IwmQuzp4dQ1UgVrUiM9CZSLgwZtPM9fMMFPIYeou7vEMzXGHkeECa5UE+rE4zZva3LZlGhnBU9Nb6clVadk6kiSozKcg6WJGHBxHRdN8fF/i3TtfYKGd4YW1QfZ0rWIpLo9P7qC3p4wmBwzFS9Qci7NnR3jv7c/w2SsHScVapM02EdVhuZGk1jIZzpSp2SaVlsXkm371+gurfYNi5Eeur7B6+Vf/T2H1nxwygqjhoFx7EpiqF4qHJEEnqqKrHpoSYKoeaa1F29Qxog5po0XRjNAXq1FSI/TqFbqiDVQpoMuoE1c69ERTDOglrLUOltUMJypXoy9WJXpNmCb50K3XiMRsUpE2GaNFRm+StVrEVJuM3iKnNQDYMOPosocTqESjHWpNlbTRQpUDGq5O2myHiEEvpMuLawQ0VQpQtACfUKGaiHYoqREyahPPlF7WLuTUOrInYcihkApbIaM20VWPhNZB9kFyQpWkul4hE/HotWosV5PEI21KTjKMdYhKjESL9GoVNClcFTVtnYTVQalrKE4MN5AREpiW83KIesUUpM02JZkwpAtexiPK27bgX5nBcw+i+hBYguBaGmBEdVBsaHQMtmY26dUrIdJR9eg3KrQCnZITJRnr0KuHXTQpIFyBWOGKKxHt0ABkJcBtKUiBjFKX6TMqVFwL35dIWh3WdSCQyFsNFgtprI7AtxWGrBKa5HMiMhze6JqLjKBsJEgk2sRMm/VS4trvBAN6iYZvkIm26DZqZLUmoq2Qt5rENJtuo05MdTgb9+jSaiSjbUzVYyRWwr8mlitWYozESmxqUQzVe+Xbme+x5/6rdjujKT4tW6crUsd2VbYmC2xPrdPxVHoSdYaTZZq2zqH8HF1aDUtx2NWzRtM16HQ0diVWedvAKc43+9mbXOa1+YvsjSxyqjLIQKTCkpPhyo9q3JKdZm9mheFkifFogfObvUjdHfxYwIqdpD9ZZSxeJKM3GTKKjESLLDVS7IqsYMguETl0BydUmx6zxsGeJRAhW6TfrNDoGNyWncYJFCI3bNK+mmCjEQuFZu0479n9PLdum+bIvikeHLzA3becIyLb6O9dp9iJMlfNIEuC3n1rzDWznCiMYKyrrDpJ9udXKDkRrPesIr2nQPZxk8u/nGJvaplhq0jnxQw7UhuYRdC31TDuLbAvusjJ2ijPlUap7HOpViJMZFaZ/gWT2uE2+jcTjO5eYSK/xtZ0CP1J7dkMdSA5n/4noXpLh7XFDJev9DP9qzGmPnGQrd9/isyFMLNYS3eInzNI623q9zfwZ2IcSCwy3eqmeLyH5WMDnGv082xxjOeO7aQ/XuVEdZROLrx7Lj++BVSBkQy9Rqbl0JepYvY2qW4VdL0YkFRbPHd8B1t+w2GzEUXaWyPzosqb86cYyFVod0vs/N06o0aBIX2TkUyJ9UacO/JT3N11GWTBO8ZOYaoeA3+lkf+URc8nTa50evjy1B4e7D3PZK2Hg5E5dn6kwnt7j3NX+hJPr46zLbJGJNHhbGOAg/kl9mWWeU3yIo+e3c39+QsM/YnCtsgad2cusT+99Ique4l/OVDzv9R41a5EHF+hthrnSiBjL8WYS2VQpYC11TSy7mNaDu3lGGeS/QQpiclqD8vlJJ4nI+ajnMwPMxbfpO1rLLVSFJw4w1aRqmOx2Y6xM72GWtA5V+9nppqj0gxhxo22gTJrYdYl1ncmuLqeo5y0SF8TJJ0r97FRjXGx1YdzLXX86maWpqujygGVtkVkTmMml6XtadSXE5zJDzAWL3KsPIo21GR7boOGa9AfqbBiJ1ltJWm7Gh1fpekajFpJlgspHth+kYplseEmcHyFmGoTmBLLfS4lJ8JyM0XWbLJWiRP4Mnq3hLRmcnmwm4TWwYsJLlW6iKz7rKxHaVkmL3UNU3VNKh0La1Fj8K6QSOavW0geNIahXU5Sblmo17aLheUUtqsSm1VodQnUBRN/tI0EiMUIqg/V9xwm+VfP0c4dpd2IElsJKHRi2HUDa7zOuXo/npDDTGUJinaUhmMgVMGl9S62dRfInhd00jLtLoE1o+MmNWaEhLsWoZkykTd1Upcl3KjEgp1FyFDel6K5EbJYdUPibGuQjNnEuRywcTjDlXaoAZotZui0dM6n+7AUF2tW5/SOARY2MvQYMnZSRmsGlJwoQsDFRh+ldoRznQFKBzOcbw9QcOK0HY3JZi/tuTjlrggbrTgpo01MGQYhcbHVR2mHwZn6IHG1w9nSK4zRhO+5lcirdhJxPQWlJdMsW2htibVqPBRzNRUCT6LtyaBA3TGYqnexUY/heTJuU0cTMD3TQ3sojDkAuFzpou6FWTArZ3vYfvsGXsJnsxNj9Uoe2Za50NLpyVeJfQmEKuPdLyNWTGqqT9vRsFSXUjOC3TBYaKZxAhVZCgnim0r05e3HyGeWmNyep6S7CC0IpfSxFrlEE0tzyenNl41mddek2jFpdnSG4mXWW3GmWl0ELRVDdomqMleaXYwkSyw1UuFKbKDEQj3D4kaaciJsI8tKgJMW+HGfC5cGkSwP1YeNs90kE5C4LNHsV3ip2E9ctxFAZFXwQM95PvLo/QhVEFlRaIx4BHMxfO+7hr3UvETzNo2xJ6rMvCtBcoqXuzCyFG5hNg4L2rmj9Pz+s3B4L/XhCNPFHPmeKvtyKzw5vY1cuo7alPANwXItgesrBCkX43ic4n1NPEPCSYGbDIjPy3R8CW0hhmILJM8guuFRGdOoHnI4vjGK0ASbByTkqEvumEljEBbaGe7NXeRj5jjlCcGTa1vR5IBEpPOyP6buGox+epnq3RbSvEVx4lrOsKvQ9HQCX0GXPXTF52Kjj/XbfKaaXSzU0zTrJk/NbiUxI1PeF8FUXdaacWSpB4Lw/azd1uH4wgg3D84zP//KXLzfi4rVV+12BniZNh7oAuma+Uuo3/0LCzV0TgbXnGQikJGUsBuhFVWqbRPHV2h7Ifej5oTRjWZRou1ryDGXtqdhbiiYmxJsGuFyMhC0u028QMYoyS9ffAGhyUpSQ+NYcG0PLMnfNXAFgYSfT4ZGOiGBFp6jpThYmku3VSehtrEUl5hiEyC93AFoeTqBkHCvaUNiqk1KaxOI0B0K4TYvHwl9J8K/VltRA3Tdw0174eRRU6Cq4cUCtLpEq0cKnbcebFZj2L5KICScpESfVkZtSghN4EVASYUmQKUtobRDT4/shufX6Yvg5Vx8XfpuF0YOayBqrk27R8DhvfDcWVpdYcxDVHcYMMv4dQ3HU0JXdkTgeOHzTVYDoith98dJSXjR0GQpe6C2wCoERAo+sVUPyQc3BpF0m4ath4bEnBumAAiBnQno+Cp5tU6zV4K8TakWpVCL4Qch46Xl6TRcgyAZpdyx0CsSdibASX73+5EEVdcKHzjNJFa2Tc01KdRiCFfGqetYxYBASNi+iiQJVpsJEBIFO0Ym1cAuWWx2YmjF/4nn+PdYd+bVO4kICa0qgyuhV2QcR8Fx1PAGkUIxmloO38C2p9GxtbCbUFcxSjLKeIOueIONRoyo6hA3HHTZZ6Mao7nNodCOhfZzxcPZ1aK1q0Nma4mNSoz1mxPYSZlCM0p7ewcRSNi2StvTCAIZqajjBAoNx6Bmm+ENYqs4roqu+tTGooiGiuOpyOVQcHWu0kepZfH8/DAniiNM13KcLocpc46noCgBebOBqXo0XAMt7vBieYgThRFqrsl6J06hFmOtkOT8fB+bjSiKFtBomli6i6H6ZE+qSIog6OugdbfJnJVpD7ikr/jYGYGb9RjKldlsRNksx3Hj8M3yBE6PixTxcJIB1qkIftqj0+Nhd3t0ej1qY+A5Ks0uleRLoZBMXTEQs1H8lAemj/l8jNxpQX048nL2r+uoNGyDR1d3EMk3abYNul4IyJ6WaLcM6qUoyqLJxk1QrVt4JgSqIH5FozEoUR8N2DwgUdivsnpUpbhbw4sJnOkElu4SnVNJP6vjlw2q4zJdJ8FUPM62B0P16XMWvekaA5mwoNuqWGSNJhmjRWVXgpFkieaIR/cJ6DoJPc8KNlpxWDHpNyts1qIMxcpwJkFKbzGaKyJXVeK5Jpv7JNZrcZJ6h46jsTVVwFhXGYkWqZzLMThaYCRWwthefcWXvhRc38f/rvGq3c4YRfBGO2iLJp2tHQY/ZSIUWHy9ixG38eZj5PZtUKzEaD0XxRsJ6N29xnzZQj4Qdk2uXulBz3ZI5NtcOD7GRlMidWSDgq0SILG1f4P5YgZN89k5sExab5Ppa/K51g0QQCqQuX/XRR45vQdrXmMuHsXvdpAETD0/jPjOFJ11CVYsfBlSOwpsvqkFZZPmRpThibWw7VvUUbvaqJrP1YWucHLcUFnQIbIiEViw+73HefzEBNEFheBgi8mXhhGKwNhUiB8q0P1xk05GIXhXkda38wyf6KC0PTb/PZiax8pRlz2Dqyz99SjtbonamCB+WWPtDW30KQvPldmox9AUn0SmxuaugJsSs7z44l6Sb15hUcnQSChEr+joFYHsh3b+Zl/IJineaWPFbAJbI3IiQqDB+KEVIqpDelebQifGdDGHEDB7dB+j7zpD9liawUiZr3/qKO6wz/JrBRgONFVQBe963bf45EuHkYTEPW86yb7oIs9Wx5ElQUJt88Vnbia7b4O12SxOSuKho6ewFJevfO4o4w9eJW80KDsWp8+NsdotWLuyhQeOnGPwwTncQOHq2X6CmM/P3fJ1vmrupdeocqneTfmNTbbIPtu3L9N/IMwF8oTMC6uDxLdW2BddYKY7z3Qtx50Pn+KR03tI5BtI3TbuqTTb7p5lcqmHW7ZNs/Kxeym9r8rgrYtYistPP/xlvrh6gLjaYTRT4uIrvfi/x7Yzr1qdSGJ7t7jzz9+GJ2RcX8EJwhS8rNlkrRm25BJGh4ZjcDR/lZpncWxllIPdS8w1Mtyen+aZzS28u/8Ef71yEymjzXCkREZt4gqFtNrkE7/1EN3vn2UkWsQOVMYjG1xq9HK60Iftarxz/EUu1nsp2RF6IzUyWpP5VoYus4EvJPqM8CmzYicBaHoGV8p5orrDeGITO1Ao2VFsX2WxmCL1pSjr97joERenaDI8vsHCpW6EFYArEe+r45xN8b43Pcan//y1KK8poioB7xx+kc//2r0c+NmXUGWfR/7+Jj7whkf52soe9maWef73b0BIEpWHmmT+LsLef3uG8cgGX1rax9ZUgade2omScJEXTH7i4UdIKS0KXpw/euY1qDWFI7dd4OKf76Z0ICA2UKPdMtjZv0ZEdVClgOfnhzk4tMiphUHcapjgNza+RlRzWPrrURQb6vc3sOsG+Z4qUd2hYRtsTRco3lJm+vcO89a7nmPUKPBEaQcAd1/LzFm2U9iBxoHYPL958kHMiMOdw9NMVrrxA5ntqQ2KdgRVDpgpZyktp5DbMj9939eY6+R4fGkbfYkal5bDAurEwAozX91CY9RDTTn87IFH0SWP37v0GnbkNlhrJl5mn5SaEe4YmObExjCm6uH6Cu8bfpZ1N8lXlie4vWeGLeYG//HYg2wfW2VfepmM2gTgycI23tz7En9+9RYimssd3VMAvFge4upmlg/ufoLHijsZtMr8/sHPXreew+oZFGM/cH06kYv/6Z/XiUiSlAL+nJB/LIAfEkIcv64fcG28alciUdXh9uwUhuyy6caxrxUxe/UKQVoO4yaBpNIirrRREAxbm2SUJkNWhvvi58hpdTTJ5678FQb0EprkseKmudG8ypqXYuNOl/d3naVHrdARGj1qlfl2jrv7p1hup+jSanRlauiSh0xAVm1Qj1msuGnGjbWXz3XYiBGRbQIhs5ZMcrWd52BsnohsM+/kyKt1fm3yTWwcEuS+pVPboqMYsLCaYWjH+ssEtTvyU3xJ3cuosYFxb4HCQhrJlRjeuknhoMRaJ07H18IW55vbHM1fpeJFKL2uje/JsBph8/UdbklM0a+V+ZPSbXxg5Bm+FR/n4PACK9kkd0Yuc9YOOwbbtq4Q/EqeO15/hRP3jjCarbBwqp9Dt05yKDkLgCwFZPQmN8ZmcXyFC1fH2X3bNJc2ugkCieiDRRodg2AmhjVeZ19uhQGzzKOrOxiMlDnxe4cZ/+nnuH36EnXfYraSBSCSs2kFBlONLt7Te5xWYHBwdIGOr3G52sUt+auYssuQvsm6lySptBmOdHMh1svU88PsMFZ4fHMH5aUkDx69SH+kwqMvTfC2nhf4TX0L+eEy1dM5bjgyh4LgzoFpYorNTak5AiGz6cbo6qmRUZp8vbqL7wS7xpU2LzZGeOPAWXwhc9i6irGi8bZbXmSLvsHTjR3cEbuE1uWjSx6vG7hAgMQD8bP82cYd3Je/yO+/9ACHb7xKkJHYaqzx+6/w2v8XLqz+AfD3Qoi3SpKkE0ZpvsLzeZWuRMy+QdH9mz+JWtDw8i5aIawtuKkQNiRXVczhOu2mgT5n4FmgjTSwNyIoaZvIyQiSD7Vxn+RwFfFYBsmHyk026ed16re1uXVshqdf3EV8SqG23QMjYPvIKlNnBkPMohZgJmw6BQu9pOBFBEEkCIOqZMK8XcCP+ijNMOdW7Wvh1HWkpgqKIDZQo1k3GeopEddtus06VdekalsUmlHqDQuvqYEvcdu+Szw3P4I0E0XZXqc7WUeVA+afH+COu8/y5Lf3gAQP3fkCj3z9JvS6RKDC8GvniGk2i/UUW5JF1n9uFKXlcPknTUb+WqL6r+rULmcIzIDYrEJ9u4tk+kQTHT514GN88Ed+EvPDKyxVkyRMm9XJLnqeFSHSUILCPgW72wM94MD4POeX+8IViR4wNrRB3mpwILHIuXo/z82O4tc1Ivkm8vEkD777WW6PX+I/j+9g4ZeOYo93kGSBOmviRQW3H73Ai3+7h+ZAwKGbL7Mrvsrz5RHWGmEgeOer3VR3+cSuKtgZwaG7L3A0OcNf/eJDeO8vckfvNE+vjlO4nMMartNeiPPFN/4+vzD/RoYiZSb/3QSdrMau/+scJ1aHQiVvKU2nYhLJtGhtRnjdwbMESPhC4tFTe5Asjzu2T/H88hDuVIKfev3X+ciXHsSoSLR6A3b83hKTv9EFFZ0bb5jihRe3IntgjNSJmg4/v+0RPvT59xLfXaR6JcPsT//MK1qJbHnv9a1ELvzOP70SkSQpAZwBxq4n6+l/NF61hVWhglIKJw65quJ2O7jdDnpRASdMImttREPVpC1hFiV8X8JaVvAqOo2hgOoel/HPdKhWIzRGAtrd0PsNjfowxJ6JcGNiDjyJ2i6XeG+dvr4Sly/1ow80MTJtup7WGMqUkXwJ2Q07FZItk5iWkXyILUhElySi8ypaXUKxJQJfJpJuY63JKA2Z+locSYa81WBnYo22r6HLPlsSm+iqz6GROUZGNti2dQUvULhrbAqtLnHL4CyG4iEjiKxI7Iyu4scC1KaEQkDunKC1p42dCbi36yIP5s7RsnUOJhaYeZvO4r1JpKbKwgOh8zSytcK23Us0BwKGvwzd39C5sXcRV8jMP6hyT/4St/dfpdYxGN+7xOodsPwaWLpXoExU0coK3T0VPKFwcGgRyfLAlbgxu8Dh1FUWOxk8IZNL10n3VfE8heawz6hRoO5bLPzSUYZ+7Vmkko46bzL+Jwts+4sKp9YGqO9w2fYXVbbGNhjSN5lc7qEnVmcwXqGyx0PNdKjvcvCGO+yJh/R+NyKzPhcS+tcXMqTGS2E+UczDlMI4iJITYfWoweYeiaIdoX0pRblj4boKVrpNItJBjnps2DHW2nFW20mGvgrpY0bYvTuRxIsHlLwo3oBNY8yj+zmYe88Q6UyD7R+rUuxEUbvaBFkXZzoR8mrsbrxeO1Qp979CKBGvSGyWkyTphX/w8SP/3UuNAQXg45IkvSRJ0p9LkhR9pefzqt3OIIVPQjfloxcV3Fwosw6UazhEV0ZqK3TlajRutGlWLdLRDtVsBLWuII80ySSaTP1QhgMjc8yn0yiyYLU7DbLA297kbxZvZGjnGkubqTAh3rTR0jbiYhylA5v3tmnX4yhZm05MIZOr0+oYtCtxenZtUB6KhN2hpTgi4aLoAblUg+aTXbT6ApTeFlRMBrtKXFjvoZCIMTfdjZ7pEDEdajWLvlj15SDyGzILnCwNox8tcrrQT6Nt4HRUYq8p83x1BLklI3kSXzhzEO4OkIKQMP/U5nZkKcD4Yoovv3svwghojQVs+bTP1TdprDw+SLs7YCpr0rtrg/loFskTrJ3fDoTs0y8s7WetlCD9mMXMPSHuURKAL9FejSF6HConu6jbXbTGHZIv6Si24MmBreiKT/F4D74pUJthm7brhYDl1wqeKO1gtpLFHu8w/fuHGf/gc8j7dnL53wyhOCAmQbYEV34gyZVzNxOJ2lgvRDjXHERP2GgVBc8OtUKyrfNR/SiuoyI/0CZ1LMLU1a2ww6E8myZ/Umb8A7N8vbGbL5y6gdiUhnKkgqn4qHKAl3e5sWuRSsri7Jd3suNN86ytppn8u+1c66BTf2ebyGmVvfFlnt89wo1b5vnEE7czcXCOpquzcXWAzu42rfUEpR+TuTu2yGolwY1DC5w/u5Nb7rnKHz3zGvpGNjmQXeb42vArv/avf83wT2bxEt7/B4GfuhZk9QfAvwN+8ZWczqt2JYIEaktCcmSUtoSsBkiKQG1LiLYKnozakGl0DGxbRbQUGi0TtSmhtqQQDygklKrCajOB5yt4voxWDmspjYqFpbpU2+bLieMS4JavZfhqICphwp1nK2ArtDoGdltDa4Z+kE5Lp9PSQ8t8R8HvKNTbBr4enrvb1lBrCg3bQFUCdNkP2bFKgKm7SHLYkowYDhHdZdVOEtc6tG0dQw15npIiaHc0VClAGCJ8CinhlkrVfQIj9KjENBvfDHUkkhGAInCjIQUuJNX76IaH4yuoFRWtJKNYPr6QEHpAVHOwLCfMNLYchBEQmME1qn2A1FLxrRCQrJoebhw8S0K75oURcui89g1BoAvciAzGd/mikhxOMPK+nQRnJtGaEnol1KIIVaDXZGRZoMoBej28i4QAXw8fu74uCAyBpvkhcd6X0JohR1apqCHMOwJNN3TWSrrPNUExQkhstOIoJY2VVpL1dpxAg6pjQhCK6gIthIMHNQ1rU7Bip8KcINdAKALHD13ZaiP02CAktIrCciuJLAsanoHWFGzaMbhGot+wQwraK7rsr3MVcp11kyVgSQhx4tq/P0c4qbyi8eqdRAAnGVLb7VwQErclEdrcr/0F3ZQfTiDzUYxCqBmRfIlOt4dYjLC+mmL8rxtsbCaoLyeoT2bofj5A31DJPKfzcO8ZKssJvIpOcM0GLtsy8lgDf1eD/AmZuGmDraBWFZzFKKKlIrtQW49hXjYxLlsorfCCkushEsDf2wjh0jUNL+HTaBvszq9xU3aeI3umODQwzw25JXqyoeEvbbbpijawfZU+q0ZwJcbuzCpbckW29W6gno9xY3IOYYTRFQ9PnCV9UUJWwszgW9LT3J2+hHN/lQe6LyBaCpFpncX7JCRXou/QCuneGkOZMqWLOUa+3qH7RZ+jYzP8ZM/jSI7MPd2T3DEw83Kw1MtDCqHKclsivXuT0TvmODC0SHtHh9oul8P5OV7Xc54j951j55FZYvuLWGM1ivd3wJW5O3OJnxp/AnXWZPyPF5h6T4qFXznK0K88y+DXNvF2tBCqYOzji7xxxxk+sPUYxcMuY6PrjHdvhrWplEsQ9/EHOrxjyyk+uP9xksdMyrug93UL+JGAXH+V1j0N5tayPBw/w0O7z7H9/im8l1K0T2dIGB0UB1YaCRY2MngTDVYaSRASA/fN0/3gIpnXLTP8FYHaEax14sTmZS6dH+Rtt51gZi3P4kye1IyDPmkRzbbY8tcVbF9FkQPOTQ/Qzktc3Ozm+285xvpSmgvrPdRq1iu/8P+FxGZCiDVgUZKk7dcOvQZeecf51budAdSmjG+E25rvRAloDQkvGvJElI6MH1fCqBY75ItYTVDrCn4sQLE8Ai3kjQg1wDcltKaPFw1zOxp+SOWSIj6m6ZKOtGm3U6Hq1FNQHEFEc5Fc6Vp4loTkSOhVEeaP+OHTV21DoEsIJcwQlmWB3iL8ObaCl1AIkLADlZprEsMmqjj4InTmypJAJsygVeXQvRoIGS+Qsf3wLXRFmD8j+VB1Q5+LfC1uwhUKjlDptHXqvglmgJMQyB0ZP+HRdjX8QEZTfPy4T6DKKO0AJ1Ap+TGEHuAGKq6QabcMyADqtfgH7Vq8hS3hekpISlcdCCTwJVyh0Ap01ttxGo4RUuSl0EGLKmgFBq3AwIsKgkwcxQG1CcqubfgXrxAEB0LWiGnQ9A1agQ62jB+EtgZUgawIfDVABBItX8cXMm5MQvIEpXYEtaYQCHBsFUmCamBQcS3KdgQnFSBUQd0xUBsSfvAdZbEcRncYPnXHwA9CTk1Ul1Ds8A5VOwIR9dmw46iajx/1CHQVNy6w5ADJCVd2gZDQIi6goyoBa3YCNeaGq0/Te8XX/b9wd+angE9d68xcBd73Sl/gVTuJpKMt5N01JrrXuLDWy4/v+jYKgo/HDpMznJBbOj/Em3ecoWt/jTO1QUp2hFI6QrscJxbtcKh3gWd+eowtmQ0OZhaJyA5/ETuKlWzQHoRlO8XdBy9yYmWYxnKCRtxCC0C9tmSuvcNh/Uw/tx65SEx1GDaLXGl2c3q0j5/Zcpypdjea5PPM+hj7UgViqkPJiXBicozsa1fYmizw5PQ2fmjiOMq1/JULopfxaIG40iHb1+BIdJq1eJJOoHGyNspNsVn6Hy5TdqOMWps0fAPtbTNEZIcHJi6wMJpmqZliywcuc0NynumhLkzJDcOt2yordpK7d11ifSzOhakBbtgxy+mFQQ6PzhIIibHxNa5+XxdSRyHqGjxe28Wdey+xZKeBMEPnlswM2YnmNWu7HKpzj1aZL6WpVlPkhpt0d1dwvJAHUnKiXHlhGKEKgpSLrAZoaxrvet23WLZTTDW6uP3oBU6NDSAmwbNg6hciBMEBxt/zEkuf382ln09w+dJOAmeC7R9rMfuGXgINchNFypNZUtvKqErAF6b2Efgyd7ztDLM/t53Veg5v2KM71qC0keCnDj3BWXuQ0+v9NOaTHD18iYZrsFxP0hry2JPZhAycmBxjeLjM5gvdNF7sCbcIAWy+t4ZyIkE3UN4TsGfLEk+d3slP3foYj27sZPbmIQ7dNslzx3Yy/xs1+rQaPzryLT67dhP1Tw2SfG2dx57ZhzEUKqZn13Kv/OL/F5xEhBCngf8l5sirtsVrjA6Inl/8qXBD5kqYmU5YxKyY4dNRCcOacqkGgZCoNixU1adVM0FIdD2psXGrh5HukE00Wd1IvUxkH/gDhdYv1IhpDmv1OIm/TNDOyXQyEsm71rA/142dkVBvKWGfzBDsrRP4Mvl0nWrLolm2GB7cpOmENZNiKYaqe2iaj6oEqF9PUT7qEEl0cF2FoVyZvallSm6UjNYkptoUnRgBEhudGKvNBPWOwa78OuutOK6vUKxHeXj8HIGQeHZjlLt6pvjS7B6CQOK1I5f5xtWddMomkumzY3ANU3G5stlFPt6g8sV+FFdQvNmj+2mF9bs8tA0NLxLWJRhtgSRwyyZfvP8j/NBvfxAeLFEuxsjl62wupUheUMN6gyPoZCWcfU20s1ESt6+zMZmHfEhyFwKSsQ798SqX1rswjseJrgRs3ARBzuXB3ee5PXmJ3/iTd1Pf4SJfa4ULNaSvRZJtBt5ygSsfvwF8CTNp47oK2pUIvi4Y/6si1d1pEperFA6lKd1qc8f2KZ4+uQthBAwOb1JuWQQnU3R2tunNV/mjHZ/mjU//BKP9mywfGwBJMHLrAkuVFEPpctih+UIX7usqyI+mse+uhUX7QEZVfRrrMW6amOHSZheNSoSj22aYLHZRvRLGTZiFsB7TuKPJePcmhWaMzfUEUluhb7zA/uwy3/rMDUTu2cD+ehdnP3L98KBI96DY+o7ra/G+ktf9Xxmv3pqIJJAbKtgykiOjax6a6qOUNYQrEzgKfkWn2rTYnMkgrkbxfRl9SQdHZvOAQLJ8xn6+wcpKBtFRcNciDPxnlbmHLYJPdPH2vheorcRZeiCgfk8TXUI5DgAAeIVJREFU7WiJzZPdOA9VEIerRD+RJH3rGnbdwCuYrM7kaRYjRK/oLKxkabyQo34qizFtEixHaG1EsR0V54Eq2pJOazWGUzRZrSaoeRZbIxus2wnW7AQJtc3lahfDkRLD8TJ7u1bxApkjuVlWL3bx4NgFZptZZho5Nk92M6CXaLd1OvNxFAL0Y3FyfVWEK/P23hd4Z8/zaIrPm/tOUz3UoTYGeBLrd4eEsZ6Daxw8OE1gCMZ/rcXIR2Red+MZmkKjfMjh/ePP8u4Dz2N7Cnfun6R+qE3tcJvKXW3id6zjr1nk71ohH2ly9y3nXkYavnX7ab5/9AQxzWZbd4HYfWu47y0h97dRdJ8DsXlagUFzIGDHHzbCNLpNhV3/cZWdv12h3TC48vEb2Pa+F3n7DS/wk7ufwq/p9N26xPiReSZ/Ns7aGxyufMiieW+DDxw4xsH4AtF5BW1TDbkzcwm67lomEgsBQwqCN02cJqm3CQyBr4MvZIJTSdbqcQqlBO37axiaR+WAy2C6QleiQT7RoPd3dAYfCc2QxpdSUFPpNau0OgZ+NGDwmw6dnMC9t8r4L9Sp2ubLiMXIkkK1bRJVbeo7Xcr1CLVD7Vd23f/LFlb/RcardiWS3tElcr/+U/SmayxupLllywyKJDi1NoDtqOQSTdaKSW4cmefO9GVeagxxbGmMXKzJ/GKOTFeN/fkVdsRWebE6TJcRgnMnm318a3mMHxw/wZ988T5ee/8pHpvdhl02SXQ3yMWabDw6AAIe/L5n+dyTh9l+YIHBaJmd0VWeLW+h5pjc333h5XN9ZG2CvmgVGcHFcjfNb3bT+7oFBqIVvj27hfu3XmSrtcGnF25kOFF+WVexJ7rEueYAy50U5U7k5RjKH+w/xs899XZ+9fYv0goMMmqDv1i+hVuzM7hC4ZHlXbx96BTfKm5lIrnCZ87dGNZ9PAlzUWfo9gXSZovTy/3EIzbO4znqIwHCCLh7/0UGrDIrnRQvfHIfd7zveU4Whig914PkgdhXx3UVohEbQwsT/kpXMmzdv0jhb4ao7BDE5mQyDy0T1RzmvxqiJDs5Qfa8uGbnl/BMuOdNJ/nqxT0cHF1Al0Ptxl+duxlZFrxxxxmavsE3Lu0ksBXefsMLnD4A7j03MPsuia6nNNwYlG9yMRd07JyPVpYZ/9gKs+/p5wfe/k0+ev4ozEWQtzSQzsex8z4fuOMpVuwUFz88wfyDGu+461l8ZL48vYd4pMNIskRUdTj1N3voeWiBKzO9RK9q+GbIvv2Rd36dR9YmiOsddNlnX2KJj37lHn7woSeYanVR6MTotWo89/l9vOM9T/D3K7vYm11Blz2eWNzKod4Fnnp6LyM3LDESK/Ht+TGm3vbL178S6RoU295+fSuRM//l/+AR/8kRVWy2dG2yM7GGJAkOJWdRCLD9sDg5FtvkFHAgschBaw5ZEiym04zHCzQdnWykyZZIgWF9k0Ujw0R0mW36GlUvQjbaok8r4+R9xiPrvBAbZL1uIEsCQ/FoDoStyS3mBlIAB9OLjBoFdhgr2IHGphvjJusqrlCRpYDFVIYxqxCKoITMc11d7M8sMWQUmcrkuTE2S0ppMZwok9Ob9GllAEa0TYhCj1Flw0qw2E4zaJXZb6yQ7qkxpJVoCp0RtUyvVWO7uUpHaMSNLUyYi9RTJuPmOl25Go6n0Gwb2F0K25PrJNQOS4kUQ/EyL+SzWAP1EP9nlenSavhCpj4cWt53pDZ4ojsHsqA72iZu2GTNkHkSCIlTAwb70st8cXiIIObTGIF9sTIJ1eZS98jLjtJOWsZJERZRVcG+6CKPRbbT8TX2J5cY0jeJRG1UOWDE3KQV6ATOBGbSZsgocvKee9EeexHedSN+aNEhnmnS3tRQsx2ciEZrWx7Zhe3mKrISEEiQjrdYGzaQDZ8Ja5GZVp7KVh1hBGwxN3CFgqZ5dEUb5IwmKbVFuzv0YaGGkCQhC0BiUCuRNNrIkiBnNOjWqkgCdlnLNHwDX4TCv2ci+xg1CsR0m8PxaZacLJriE1Vt1JZEXOuE+AfjlYOav9diNF+1K5HMzryI/9sPkR8sU1hM84abTgHwpbP7sOI26ViL1ak8hw9eIaO3OFvqo+OplCoxgk0Da6DOgd5ljs+M0t9VYWuyQFS1+epzB1FzbRASh0dmabgGk+s9dMomZrqDmIzhDNsIJwzw9usaI1vWMRSPsXiRmVqOqcVu3rL3FLPNLLIkODU/xEC+TERzaHsaC+d6UfpajOZLTF3s5/7DZ7BkB1UOeHJlK/tyK6S0FhU3wp2pSyw5GTqBxuVGN0dSV5np5Gn7GobsUfdMVClgPLLB8dIYi7UkiiwwVY/92SXmmlkOp2dRpIA/e/Q13Hj4Ch1Po2JbLJ/qZeCGFeZmuhnfuhpOkK7O3HQ3SlOmZ2KDg7lFput5MkaLjq9y6tQ4D9/6ApOVnpffCydQKDUj1KsWNDSGtq6zVonjuSqjPZtEVYfLj28h0ARuMlzxxK9o3PS2sxiKx+VqF1HNYXK5B+uFCHpdUDzsgi2z/WMtpn9axa/poIV3z7b3v8D8rx3Biwq0oSb+dAxGQuWnEBK+K7N/dJHOj2dYvzVDbQyyewpsTuZ4+93P0m+U+f3Tr0G+arH79mmcQGW+nKYzlWT3oasAnHtxlBtunuKlY9tIXSLk1AaC4oMd4t+y2Pbey5w8sY0DN03z0gvjvOfub3OiOMLiN4cZvW+WyydG8HttBrrLvKbnMmeq/ZR/c5jsL8xx9ttbie4u05+scmUtz8w7fukVrUR2vOX6ViIv/df/sxL5J4cQEpLph2AfNaDpGchSgGr4dFo6jukg9JAatj0WJqitFLuxIjZNVae9FGcumuHAyCLz1Qyr7QTj8QJ94wVWJ7s4cOM0zy8OM5Ir4c3ESC1KNAZVgrEOsZcsfB2G712m+PFh3BGFrNkkrnbwhEw2Fxr7BiIVAKZibSJayCtZqKdJXZIQ4zaaEgq1Gp7OzemrPFHewW09M+yJLlHw4uyJLDJr59lw4mzYcWbKOQzFY09smY9ePsIv7/kqnUBjycmy6cbYl1piJFZk046R0DostVIMRcp8c30HsiRQB5u8MDdMNBqmuKlbGiwV0ihNmemFLiQJrHiHsa1rVNomq5Nd7Lv/WR6b286lVg/ClcmPF/n2yhj1pomqBgSBhFOIkB0uE5k0sfe2WDrXw5b9S1iqy9nTowjLhxEHa0YnPi8jezKNQZAlwWSlm1vyV3l0eQc7+9c41xwEYGx4Az+QmX1DL9oVieFbl2j8ZT++AfO/doThXzqOvHcH0+9O03fMZ8MO1dqRVUH5iIOu+Jz7/gy+FSB0weaFPEGXTdGNMmJuol2IYO9oU3UsbE+lUY4gddusNeOockD6osSF4R5i8xLNAa5JCWQODC1y6rYhpko5IqM1qo6FyDostdMsVVLYE22K7Qijf9ci9tsrXFzroZE3WKylKd2v0qylcXsdKqsJZDlAvvIKVeb/m4FD1zNetYVVRQrQLBdLc1EjHlHVJqbYWBEbWQ0wVA/J8pElgSm7oXYBiJk2IuIheyGnVZUD2o728uuqchAqE4WEogS4gYLsg9YUqC0JBLhxcJOhuM3a9NAUP4zqlHw02SdltdFkH0P2iMhOqDhVHUw1XLp2shIxw0GXPSTDR5EEcbmDIftktSYppUVEdkgp4dPVkD0sxaXZ0cMsFNlFkiAlt8iqDUw5bOEmlTY5rUFC6xBXO5iKR0JtvyzES8dbSJLA0DxUxScVa6HpHn7cRzV8kAWuo6JKAZoSEFgBPjKG5iHJAtnwyUWamJqHYYS5PYbuIbQAXfVwEiKMttTDrUDebIQFPgmMmI2bFHRyEk4ipIUl1HZIkpNdFDnAVNxQyh63MRSPiBYqZH093Ea6sVA56kUF8t4dBGcv4VsBniXhmQI3KvANCUX30WUPPxpAwgMtVNdG4jZeoNDwTby4IBrvhLk6QgInpN41bZ2Wo9HJSrSbBrIvsNMBbirAToeiRk33qNVDS0PD0TEjDnagYnc0VM37bgKB4uHaKhHFodE2CMyAasNCMz0kW6besMJu2Csd32Nks392JSJJ0seAh4ANIcTEtWO/AvwwoXkH4OeFEF+/9n8fBt4P+MC/FkJ849rxG4C/ACzg68C/EUIISZIM4BPADUAReIcQYu6fO6960yJWsFjasCCQ+FJ7P0ggNRWE5bNcyyFHXRYrKf6icBi7GUZGrM1lQYL4rEwhlqLWNOlN15i82sdloxsr4tD/lKC8K0I21mKzESUxA80+CS8mGO8rUP/aAO2szMquBLXXKASrWeacPPFck05bx9s0KQ5HsN3wz9sqRljR0shqgBV1iKwJFq/mWU0mUXWfS+UufLGfhmvgiS6W7DQVx6Lj76DqWKxWEji2xh1bpjhd6OPk2iCdts7fbB7CFTKn1/u5e3CKPzx1Jwi4a/sV/n7yIPK6wXNWwNC2dVTFo1BK0J2t4n+yCzSJjf2CwW/4zL9ewjgTwc6G8vF5JY0sC+SWzH5znugnk3jvrtNYjTFrZPGuxsi/JJB8geIKlB6FVSNFfF0i2CWhNmSeO70NYfioXR0S0ZDKPiNCJqpVCGj1SXzxmZt5zeFzDOmbdL7azck9WbSKgq8LJssDoApyE0XyH4LJ3l64ySWeaaK5KtPvTuN/4BBb//UJij98hG0fmadydJDV17scHZnj2Wd3Qdalr6dMrWMgn05T1SOcM3r5qe7H+UjX3XSZNqvH+wg0uOHWKaaKebZmC2y2Y7TWBbE7atTyXVgjYa5yEEhMFrrpVA3u3XuBUxuDlGpR9vUvc26jF6+qI5oy9cU4lTtgdn6YAyOLfObijWG7RBVs7S4wEi3xzZmDjO7ZZN56ZSuR79Dev5fG9Wxn/gL4Q8Ib/R+O/0cI8Tv/8IAkSbuAdwK7gT7gMUmStgkhfOCPgR8BniOcRO4HHiGccMpCiHFJkt4J/Dbwjn/upBTDC631CQevojMyVECWBFdnu0FI6OkOTtkkkquxJbPJTClHvWYhRTyoa8hu2OJVlIC1SgIzYZOJN9koJeikZPy2GXJKCVPmuk65VLZoTHV3kU5IeBbgK3Q/J6i808ZMuwwmqsySoZmRGUsXqTvha0y1dWKxDrrqU22YRH2Q4y6JeIvSapJkd4ctkU2mml1EFYdBs0RUiRFRHFY6KeKaTcW2uFDqoTdeJ6F1eNEeZFt0PVSEZnU0yWdieCVENCIx0F2mmjCJm/bLXR1V86h3DBqHQmezSDus3G4gBQGtAR9kkDsSnqeE6YK6YM1LsnpUQrFVsHwipk19SGZds0CWkDwJ2QXV8AiMkGMb6IJYfw1D9dlcTtIAFgMZdy0SJtzZAUpHJbsvBAqte0mqu3y0pI1nWyAJpKiHrAjKk1n03QGy5qBPW7Q3NSQf+o75eJZE8YePkP2z42x+/xE6GQl1SeZKNo821ERMxlhVkgRtFSMvkIyAuGGz4KWRKyqVlIXd44EncXapH7epcVUOcH2FqCfYLMcZet5hLh8HOVQhK0M1pKbKC+uDVCpRkGC2kiVpdai7CSRfor7NZ8d/KbNws8bF9R4m+ld46cow5rLG1XSWYjuCUZa4upElOf8/MSO82iYRIcS3JEkauc7XewPw10IIG5iVJGkauFmSpDkg8R1ikiRJnwDeSDiJvAH4lWvf/zngDyVJkv45voFvq8gtmaBtongScwv5MMKxoRAYAe6mBYZPtWlRa5m062YYylzWQwOUISE6ClIMsvEmyysZCr6EaTlobZNEpE25ZYWFOgMK+zU6WcFwXxGx1EWzW8VXfAo3yLhVi1Zg4foKzYaJKOvMxHK4fmjmCxoaNVtBUgNUw0NxBKKqU5VAMnwKzShzkSybnWjYzelkKDkRAiTWW3HWKgmcjsqBkUWminlUxcdp6Fxt5/AChdlKhqzR5NJKN0JI5IcbrBaTIYk+6ZPrrYbbjY6Grntkzsr4pkRN0+k+GbBym0RsQQ71DfGAiOkiDPCWDFJKi/yLUOqVUEoajYgJsxHyF0T49/bAM6DQpZMoC1qBhF4OEQd1w78mSw/IxZo0UyaSZ6B0BEIWrM1mGdxbIam0iV1VqJsaWlvC1yGwFXw1ILWtTOKTsIaFnfNRs2E9Z8OO4pmCbR+ZZ/P7j5D6xHHsB26itkOiJ1Zncy6DGHTIpJo0DYP4czE2cwp1O/ydhC6wdBe7rCJUwfD2EsvlJD3xUNuBHScea7N+Ywa1qwmSQAQyuurRSLjszG4wSRdtWycfbbBaSyB0gS8EkUWFlXty+H6doUyZ88t9aDGHzgBsS1fojtR4LtfFYK7CymDsOm+t7w7pe6wZ8r9SE/lJSZLOSpL0MUmS0teO9QOL/+Brlq4d67/2+X9//L/5HiGEB1SB7D/2AyVJ+pHvsBGCehOjLKNXQzq7XFeR6ip6RUatKWhlGbmm0mnpuK6C6Cj4TRWlIWMUFRqDAtnyMDQ3JLTLAl0PLeHl7TKW6lJeTYTGuxFodwXQF8YKRC+skT3fQJEFbsJHaihILQXb1ghaKkpTpm1rYUSFG2bzSm0F0VJxWzpGxUdpyPgtFRoajZZJ2Y6wVEmx2EizYccodGJUbItax6BTNxA1PSSRN0w8X0EtapSdCEU7ShDIzNWz+L6M78rM19MEgYTSCWs4xc04a4UkckGnvhankw25qFpNopWXiazKpKY81IaEkrOJGA666hO/CledLpyEhLdhEZuXw9XdmoSQJTxLopOSSF7tILUU0lc6tAsREnMBSlNGaqmoRQ13w2K9Gkfe1IluhF4RvRquYmbKWaba3diZsOYi2xKKIyFbXujKVgIKh9LohodWlnHb361fCQUqRwfpZCTsB27CeOQkWllhsZKCqIesBdTqEZyORuZiE3NDxfNlpuwerGWFUjmK1+VAb4eGo+O5Skh9B4yyhyQJ7GyA29bwOhpeWyWquyhGmFGsyILeVC38eYCe7iCiHp186BhWVR/bV3FrOtlkE8XysVSX5WYKf7CDpbp0el+hd0b8/w+o+Y+BXydcWP068LvADwH/WJVI/BPH+Wf+7789KMSfAn8KkN6RF8aNJbrjdeaLGY70L6LJPlcqeRqdMOawUI+xv2eZezIXOdcc4ImlbQxur3BxuYd8us6O9AaHEld5trqFm/IL7LBWWXIyPCLv5OHuM8x+Y5TX3niZR8UOnLJFItohZbS58p4tALxn6Hk+OnUX+26YYShaZoe1yonaGE1P547MlTDSEnisfyddRgiHPl/uZeNgH/uOXqHHrPOt5TFeO3iZHqPK1XKGrNnkhuQCV9s5bojP82J9mPVEglInQiAk+vMVfmbsG3xw8b3cl71AKzAY7ivwn2bu5/17nyVA4gtz+/jZg4/y1YG97E8t8akThxGBAhmX2EWD/gfm6bLqnFodRLvRxn2si6XXyAg54LXjlxmPbLDhJHhCPczTle3E3riG+1gvvgGRfJPaQZNUpoGuhr/f7HiOg/unmbm0DbkT4MQkBvaskjA6LP31KFIgU7VjpC5LVMZCkZgXEzx09BRfeeEAF2K9HLr7Anviy3xUP4qh+bxjyylavs4Xpvbh3Grzge3Pc+xDO2hty7P0AxKR1bCIuvp6F3VJprZDQrv9CKMfPs7Sh4/yr9/zdT42dYTm1STprSUW78lh531+cPQU3yztovukzVyPwbvvOAbAl2b3MJgvkzWbDEQrPHdzD33RDcrpOMaijm+GWIEfOHqcb0R30/FVbuxaZNQq8GfP38s7HnqSmVbYes/oLZ76u4P8+LZjfGFlP/ceOI+luJxgmLzZ4NyLo2zdv0ivVeNq/pVDiV5125l/bAgh1r/zuSRJfwZ89do/l4DBf/ClA8DKteMD/8jxf/g9S5IkqUAS+Ade8//BicsB3fE6w7ESLVcnpbVRZZ/+WJUgKhHXbAC2RjeIyx36jQo7c+vEtQ5ryThvHjxNRHaIKx22RTfo1Sr0a2VKXoyHh86TkNu4ccEOaxVji8diK8322Drnan2I/XXslkZSbTG2d5ktsU1yWoO8WmdLpMB8O0uP+t0ogB6zRr9RASBISzw6kaLPqjJolsjHuhkxizxZ3EarafJiaYTqsEXdNvAChbjWQVc8MmaLkViRuUYWU3KJ9Dd4prqVumvw431LbFRirOUSeEKhcT6DttVnKFoOhVRJG99VwppCl+BIdpZevcK3L27jvv2TPOZ3YQw2kGXBvakLFLw4DcWkvEvw1OmdvPXmk3x+oBuhByhXE3RPbDCRWUOTfWQEx7fLjEaLvDjhkTupsHnIw65HKbcs6hMBSl2m+0SY9Vs95BBJtwmmE1iKi9yWmXp+mIdf/+VQjOeo+F4IyvaFHJrptk8RVzrMvqcf2QXfDSgfcVB0n6Mjc1zJ5umJ1VmspFj68FEG/uOzbH3/GkEgk5iRSeyxae6tY1yIs91c5aMXjmAcMMidEhx53RQKghOJEeJah+2xdeJKh2/v2cKh7BxJvc2V89vwnNCpHVdCmfquxBp2oHJb5AqPffkWJt62xKhR4FvVbdySmOL4zSNEZJvxxCYprc3t8Uu8UBhif3yByWMTvPPBk3QCjfyWOr99nffdd8b3WmH1usRm12oiX/0H3ZleIcTqtc//LXBICPFOSZJ2A58GbiYsrD4ObBVC+JIknSS0HZ8gLKx+RAjxdUmS/hWwRwjxY9cKq28WQrz9nzsns39QdP/GT6IWNby8g1LUQEj4MR+MAKmuYvQ16TR00id0OnkJ9tVw5mMhku5qFLUlobahc2OT6DNRYqs+6zfJoa09FvDD9z3On3/tHgJdYA7X0dXQQFdrmqiqj38hwfAti0xN9mOuK0getPt8tKqMFIBZCBdZzUGB0gFfB2mkSdRyaFxKI2Qwxmq0aiY7R1YZiZWQEVRcC0/IXCnmUWRBrWkiy4KbBuZZbKRpfKaP7u8PSVq67FP+xCAP/PS3+NRjtyHbEu963bd45P+5nfLOEI3wujcdJ6m2ebqwlTvyU3z2L+8muhJQeH0H82yEkftnubTcQzzWxjmRIdDBiwj2HZnit4f+jjf+4Yd4w7u/zXQzT7ETZamUQjoTRwog0MEe7UBFJzdeZF9uheVWkstL3QA8sOMifUaFpNpiwc5yfGOUhq1j6S7lb/fwr979FXYYK/zyhz6AG5EpPdAm8CWSx0zcmMTht53h8ecniM4rvOcHvsl2c5VPrB5BV8I27rPP7kIbamJvRCDq8a9vepytxhr/eXwH1a+P89DAef726gFcTyEda1E41c0j7/6/+dPSrZiyy+O/fit2XObgj5/m8entdGdqrBWSxOIdBNDpaLx75wvXGKsyX/yb22gP+Bw9cJlj57dirKn84js+y6997u1IPgQq9D7rY/9EifajXYy+aYaz50eQXAljoIGm+vzens/ygUffT7S7SathMP/en79uUVg0Nyh2v+7fXs+XcvIT/9f3hthMkqTPAHcS8hqXgF8G7pQkaT/hwmoO+FEAIcQFSZI+Swg28YB/da0zA/DjfLfF+8i1D4CPAp+8VoQtEXZ3/tlhxBxwZSLbKtRW49xx63kAnji/AyvRIdZtU1hIc9OeGeRtgqlSDl31Wc8aBJsW9NuMD61x6fwgfek6Xe9YQZUD1p/ZRqfXBVUw38kydtMiU4vdtFditOMe2pKO0+/geAb0eExd6qdvvIC8VTAQq7DaSjB3tYt7D5xnupZHlQKmFrtJ52tEdQdZEsyf6UP0dxjsKjM/08U9B0OfTUpt8cTqNralN+jSmwQZibszoWLVDlQqboQHe8/z0vtrxLUOXqBQ9wyMH5olpnTo2b1BoRrj8dXttN9Q5UjPElfKXeS0Bprkc/VcPyNHSsRfu0apHkW5EkM5XObCTD+DA0UM1WPjZon2ZAq1KbHZjvGJyiGsOwusdpIEQmLmYh+vPXSWM9H+MF1QQEr1qcdNNotxHlvdyfDQJgCBozDXzFBxLZ47viOkm2kiJIHNqYw/eDWMddjcgff+IhtzGVLHImhNQXkXSJ5g9ue2I74vwM7IfPT8UWQlYOxXnVBIFg0g6yImY4hBB1kN+NjUEYJAJvZ1m+SD03zmw3fT6QrIbC2xNtnFA689xcnOIF+b3U17Lk72fUV8X+ZssQ+WLboHlkkPtpk8NcyBm6a58Ng2Pn/yTiQvrDO0dnv0f1NCPhiglVS6Dq3xC8+8idff9wLPrY8gPpej8aMVaufz+Ft82p7GW448z8nNYSI/IaF+tM0Hvvl+5LhLb7LGTK3rei73747/zea66xnX05151z9y+KP/xNf/JvCb/8jxFwizLf774x3gbf/cefz3Q5YESsKhK94I5dbXjlmpDlHTIWl22Ix4pPUWw2a4O1pvxUln65S8BHdun2JLpIC5zyWttxmNbJJWmyzuS9Efq2IqHk/MbuX14+dJGm02WnG2JDZ5Wh8HW0GyZXZNLLDRjNEVqYcu1egGsiQodkcYj2yQ0tooBJQ7FmOpIrrsUXMsAhW6szWG4iUWY2nyep3ztT6u1rOsz2dQ5YCYHnpI6oFJwYkRiDBqc81OcnNqlkfWJrBUl4pt8UDvBY4Vx8lYLbxAZnWyi1sOX8SQfYYSZY6XxvCETGpS4pn+MW4fnsbKuzx6+ma25zaYPLadSjZsB79t7CW+HR+n0IyysJLlc/UoB3qXObk2iOcpxKcVVvcm6YnWkaVQfDVfzYQ3xHIf2dMyK9EkgaOAKzNTyOH7Els/XqG8L8XmAYkg52GUFfJGg8eXtlFeSvL2I88zFcszdXUrviExdsMCpXaE1XqOweFVtDGfxZP9BBKs3wq+FUDMo6+nzKqSJJNqUqtHaF5NkpiReehHn+AzH76bgf/4LFc+fgPNjk5iRmbiviU+u34jzqUEI487vO7eFwH4/NJ+6G8zEKlgyB4X1SFGo0VmDpbo+k0NocjIrs+udy5zXBrnFrNGdGeZHxg6zif/8+s5eMs8PXqNv7j3MO8ausAny4fYNbSKqbi4QuFtAy/y+z/yED+QfpL6Fwe4+bdeIiI75M0G86/04v8em0Retd6ZyNY+sf01P0N9DGLzYL0hLNM0v9aD4ghqozD4mMPCAxqRLVU6F1NY6yFc2CoEFB6ySSaalK9mMMoyYledbKJJ5ekeuLGKM5Vg++E5Lp4bwuxtIoTE1q4Cq385SvGO0DsTmdVQbi7jnElj5z2kqIc+byA7Et7uJm5NBxnSp1Qaw2EEpLkp09rZIXrexEkJRr7U4MoPWXz4jq/yTGUcQ/bZH18AoOqFbd75dpZASDx5YoK33HaCp1fHSZttHuo5RyvQ+eTUzdw3PIkdaFyo9NAfrfLs9BgTwysktA4prU1UtfnKzAS5eJOYblO1TdZLCbozNUr1KPFIh7TZZr6YxluMYhRlsnes8kPDx/izuVvpjdYIhMRcJcOBrmVOF/qIG6EKd7WWgKfSdI40MA2XmGmTtVrUHYNKyyJpddhsRGluRJGjYTauXzbYv+cqTqCyP7XEYyvbWV/IgBqgVFT8SIBaU/BSPrHuBq25BFp/k3S8xebZLrxkGA0Szzapb8QwUh2cjkY63SBh2pSaERqzSYKUy7b3vcjyF3bj2Cqa7vHDO47x0StHUeWAVCSscRQf6afvySoz70zgmwIR8UGAFnfw3LBVL3wJuaqhtCXcnEfmBRXvgQqGFqqWqy2L9kKcru0F1mazyB0ZoyjTe9cSq5UEneUYJF2iyTYJq8PKcgY95jD99uv3zsSyg2Ligevbzpz41P+e7cyrdhIxBgZF/09/EMmTwr25IcLYShG2/pS2hJf1UCIepuXQbuoEngy2ErZXkz5q1GVn/xq2pzKzlieZaNIda3Dp3CCvPXyWpx7dT2J/keLVNMIMiOZa7Opao/QLw3gRhcjPLjM52weODLIg3tWgUbUQHYVovoWuhu278mb85fOWNZ/0UybFm3zkqIsQEoblMpwtcWWhh96eMlmrxUYzRtLoMF9M42xEUJoyOw/Pcu7iED0jRRpPdrPldTN4QqZmm6GIrGkiKwHGNWeoezqNPWIjF8Jwa0mA0pKQPQm7y0dpypgFCdmB3mfrbNwQQzxQZjRdpONrtH+rjwd/90k+8Zf3hRSxcx4LDwl6n1TopCR8U0LIMPDlVSY/nGXLJwNm36gz/ukm0++MEkQConNqWAvaW8NeiJF7SUISguq4jJ33kVMOr9l6iceO7SM1XqI8m0YYIRM1ENAdazD/zRG67lpm7dv9dIYdunsroRfGDEhcUejkBfE5yFxssnhPjGBvHVkWRE0ntArIgv43X2Dhl4+SOrzO/X2TfPHP76R2UwfdDKX7uViTWsekP16l6elU/moA5W0F1ldSKJaPLIeO5RuHF5ivpdmVXqdgxzAVl3OrfcQsG88PkZWNjSj6psrQ4SUUKeDypX6271im1I4wntqk0ImxXo8xkV/jxNUR5t7z71/RJLLnvg9e1z3y3GeuP8/mf2W8ag14yNcI3HEPpRY+uZAFkicjJR28dpjenkvXcX0Zz5CJpmxKa0mECiMjG6hyQKEVZWuqQDunYakua/U48aEaRTuKmwzojtVRxgNUOWAoXma2lqFyp4leBdc2GewvslaOE/hKCG0GGvUEqUgb7xrJW5IFqhn6XdLxFrVeC7RQAt9qGHQn61y+2gueTPmZHlb6PZBgXRKke2vYhomnB2yPr3Mp1Y0iCRrjLmdmBqEjM7ilQLNlEKyb4EgEZQlnfwO3y0OqqwTdNgQSO36lwMUPdyNHPKIxm66PWFz9PomRv4Wl18Rp9wTc0rXCVCVPy9bxD+h8euYmGtvD+lNBURn/ZIv5h1S8eICQBMgw84M9aJEWm3ujbPlsk4X742TPCoSksHlLyFvNfDOObkg0BsHOCLpOBqx2hw+wR1+aIDpcD4VbJ2W8iEwjY+DYKqWNBPLONpuNaDjpGD6bkzmCLptI3KaqR5CMgM2cQmMohp33MS7ESR9ZY22yi8SMTPOWJgu/fJShX32W5DNZztX6cBIw8AWVyL9eJ6aFW8fVSoKR3iJ112RpeJDbc8s852iIb6URSqhcLnZHWVvK8MaBs1wo9XBP32WufGs7o+9YwVRcvn1xG5m+KtV4hM1GlBt7Frls9bAvvcxnL9/M6wYucPLbO7j1zvMMWSVaQzpzr+S6/x404L16VyLDA2Lod34UbzWC1tfE2YhAAFLGQRTDLYUxVkM+kcQ3INDCaAQ/5RHLtmjWTXJPGBRudTFWNPqfcqiO6RSPusTSLZorcW7eP0Xhl0ZZ/GGX0XyJvmiVyVI3+rVYy9UzPRy57QLPPzqBUAXWmoSThM7WDskTJm4CENAc9dCLCrIrIWTBxF1TvHhhDGtRhQM1tGcS5B5aYrMR5WDPEnXXoGxHqLRNSoup8BeWCUHMWoC2pBOMtenO1LA0l8VnBskeWqPjqtiuxu2DM/z9c/sQlo+ZDOX8SaNDr1WjYMe4sNiL8CW2DBSYW89y69gMp9f7w4loxYLeUBV6x9g0b8ud5CeefTd7h5dJ6G00KeBcsZfNQgLV8NB0D0kCeyrB7sNX2RrfYL6VoeGGwI+2p5G3Grw5f4qzrUEW2hk6voqpeDx7ZQt7R5d4W88L/NIjbyOIeezZuhTiCNaySBL8xL6n+cLSftZLCd43cZwJa5HnGuMU3SheoHCu2EvcsMOWuC/zjtFTbDdX+dDfvpd7X3uKiegSH5m8k0Skw2C8QvXWIm+Z3GCuk6PfKPO7TzyIUAWJnjqNuom0bhBZlRl7eIbLT25h9I453thzGghh2L/z9AMMjhVYLqRAgLxq8ptv/DQf+77Xc/WtceK7i5Tn02zducyVuR6QBXJZQ3Yljtx+gWdO7uRjr/szfnbyrRTn0sQHapx/w69f/0okMyj2vvaD13WPHP/sP78SkSRJAV4AloUQD13XC//3r/FqnURGJuLi//67bShSQM03qQcWCgF9WpmClwjBxwTUA4v9Zli6erKxiyPRKeacPHdGpvlKY4LXRif5VPkQN0TnyCoNfCTG1CqLfowffOaH+NCN3+Ama5Z6YJJXmnyxdoBercK6m+RQdJqCF4aH59UamuRT8mNEpHBF8h0Xbj0w0SSfVmBwpj3EgF6iXy3TERprXghh/quFQ6xc7kLJd0jEW7Rtnb29K1ytZBlKlPECBV3xmC7l+K97/oofPvP9dMUbKFLAb45+kR/4rx/kQ+/7LK5Q+Y2nHuZv7v8vnGqPkFEb/Lvn3gIC9o4sc3aun0/e9ucMqi3+oHA7b0uf5D8svo6JxAonS8P86fhnqAuVNS/O50s3cuY/7+N3f+2PeN+J93F4ZJaKE2EiscLt8ctokheCqBvbORiZ40RzC48s7+Jgfom8Xgfgs1cO4roKA7kKGbPJvbmL5NU6Z9uDbDPX+K0/fwe+Dn/9w7+HKfl8vbEbO9B4OH6GamBw1h7ksHUVBcEfF+6k7WscTMyjST4N3+Tu6CQLXpqU0mLK7uGbpV2cWhzgkSN/xMnOIJ9dv5Hbs1NUvQjnan3cm7vI53d2kT6WYb0V59PbP4UiSfzk/MPclp5mstWLKvkcjU/zzfJu3pt/ls+VbsKQXXxkfij7DIteio+v3spbu17koLnEO06/nx8Yf46brJBF0qe0ONEZZJexyh+s3cOe+BKHrRk2/DiPVXfz3PoIX9rzcT5SPMrDyZe4ZXT2lU0i93zwuu6R4397XZPITxOCmhP/s5PIq3Y7U3Bi/NzJtxCPtakUY/T3lVDlgI1aDM+TMU2XVstgR986q6kUVxpdTBa6+bK+h8JKir8c2mQoXqbhm6zaSb5q72NLpMCaneTE+jDvHn2e5PMmL24f4U+mbqPZ1hnMVYhrNrNfvDtsQ/54hM+fP8BAd5mM2WJ7Yp0L1V6K7Qi3dl99WbH6zMYWMmY4oaw147Qf7SJ2/xq90RrnVvo4NDTHzvQ6q5kk8Vib3bk1Ntpxuow6Tlxhvpqh2dGZ6Ak5q880t9OcTrL7rimansHTzR0oR8o8WpogEBJSxONvSod4cnkr27IFpJKO5EpMLo0R35D4ma630R1pcGZymG907ST9yRhf2jGGnRH8buI1pLQWBSfGlV+aYODfT/PjZ9+NdSLKla/uYuOo4GJnhL/tPYiq+aFJ8VyCR29ZwfvTbpqjCqcXcmy+qYWu++jH4lgdwXp3BOdywMfMcZq9oXBr8ME5GqMe+eEyvzD/RrbGNvjCqRuQdJ/l3SkqrsXp9X5+q3o/b5o4zcUPT1DZqvOt28bRLkTw4oKPdN2NXAlDuKxlhe6TNsYBgz/dfitfm92NcynB1M48yhMpnARse+sG6WMZyreUWP/FHfxe9jY0yWehluZvGjewNVVAlyX+w5+8i/E3TvHDJ74f9VIE3xLILvS/ucyThe0MRst8ozxBIZ7AeS7DxlCCT7WOstRKkdA6nHhmJ2977THWO3GWmhNsZuM8trKd23pmKF7O8m9TDzNolfngpXcAv/WKrv1/qRavJEkDwOsIu6nXRzr6R8arliciITBMh1SkjWa5RLWwU2DqLv3ZKtloC8tyGImWcIWCqbhszRYwVQ816nIwt8iWaIHZVo4hq8SQVcKQXZZaKXZm11h1UlQmPAbMMvu7l5noW+VgZpFSJ0Jtn015t2DTibFjcI2eaC382bJLt1nH8RRiio2PjI/8Mm8kobfZlt6gvtXHVD2SWhiX2WvWOLY4iqL5dE5mOTa9havrOZ5Y2EZabxPVHXLxJvsTSwwkq9R9k+h4lccXtvHkzNYwSwa4UOjh1PIAsbMmq50k2WiLxXqKxGgFY6xG/9Mutf02g/EKA5EKyQsqQ+kyTkymtc1GHW6wPbLGXCvLfCPDwr0Kp2aGGUmXqe+3Wb81oP9xMEfrDOTL9KRq9KZqBLsapIw26zfL9D9VZ/WuAGajuJMJmje1Kd/sMvq5Ik5UorhP0NndRm2DGyioKYfq6RxDkTKrnSSxKQ3rsslSK/QRNeaTjPZvMtvMMv+gRmUiQL5qYe9oY+6o0N9Xwhqpkx0p09nVZu71GtZGyJBpz8UZfDwkxtVu6pC74NFvlFlvxVn8xaMM/vqzdOk1uvQa46lNfCGxI7rGmLVJfZvHofQcA/ky6csB2XOCzAXBYifD9FqeA7EFCp0Yg3qR/qebDBlFtkXWmC+n6bcqWNsqzLWy7EysEdVsJqwl1pfT7LBWyZ2WOJBYZIe1yuHuuVd24QvC6L/r+fjns3h/H/gQ8L/ktHnVbmeMkQEx/gfvp1k3iSXatJrhjaRqYeyD66h0Z2o0bZ3qQhIR8enpK1Mox8mlGjhfziMUicaQwNpeQTyTDlcX+3zSZxTKBz1+4sgT/NG3X0Pqgkp1WxhxObxjjaVCGtNyaKzH0FJh/cBt6CCBHnNwOyqSLAia1wLHYy6BK6PoAelkk46r0moYSIpgIFehbutszxToMuuk1RZFN0rdM1lpJqnaJrar4guJI31zXK50szDZw7Y9i6SMNrrs8czzu/jAXU/yZy/eCsBP3fQk/+Wb9xJYYZzmG29/noTa4XKjm4n4Cp/747tJTTkUf7IFT6bZ+tYrnFvpI/Bluv7OZO0wBJGAG3Zf5SPDf8d9v/8h7n7386x1EqhSwEur/QRnksjfEWDtsFF0n1SixZ19U5wuD7BUSqFrHgd7lhiySowaBa60e3hybSulWjQEbF/o4efu/zI3mHN86AM/zupRA+VgBQDvpRROKuDI4Uu88M1dBIbgrfcdY4u5wdcKe6g6FoGQWD3eh93joZZVvC6Hdx94niOxKf7jv/sBnPeVeOfwi3xldQ+r5QQjuRJXLg5w7OHf5fc2b6NLr/HYRBx1dJgtf7vC00vjdMUbzBfS6LqP78vYGxHee+szoQRfSHz2sVvwMy5Hdszw/Pww8WMRfuHf/BUf/vy7SU9CcQ9s/VSV6Q/piGWLm49e4vRqP/ILCbyDdRQl4E8PfJIf+68/iXtjHTEVY/rnr78VG0sPiv13/ZvrukeOffFn/4evK0nSQ8CDQoifkCTpTuBn/j+3nUEWmJqHa7qYmoeIhHUISRJIhHm2muJj6i6VqI9yjSCmaT664uPb4F5zYfu+DDp4gRQWZwPCjBMp7JIoHYHSlpEEdDwVVfNfbt96tko00UFc68TouocQoCiCjicjSWHYlVACND0kimmBhKqHLBNFDrA0j6jqEFNsTNklptgEQiaq2ThBSF/zAhkvUIhoTlgwVTyiioOheAjTx5RdNMsFIWHIbjiBRF0CScVSwlVSVHUwZRc3JuEmVCzdpR4JCVym4WI7KlrdR/JVCCCiOiiShBsL6Wpx1UaWAuKWTTEaJg8S8HKB1VA9IoqDpbpYhoOhecgINMlHvvaw0+QAWRZoik8Q89ElDwVBJ6vh6wJTCZGXrhIGiTdcI7ThAz5ymOYXqNheOLEGGuCFeb3KtWxfBYEdl8P39dowdI+YZodfJ0khhU7yUUeH8Wbn8YSJAFxfuZZ+5+LYKnJbenkCgWvXRiDR8nSEL6HXBYoUEKigNQMkIWN3RfC9MPvICRSS0TbtZoKOp4QxqshoNUGzo2G9QrLZvyCU6BbgYUmSHgRMICFJ0l8JId7zSl/oVbsS6dqVFdbP/SyxVJtGxeKt+0JQ8xcu7kedNRFbm4i5KF7C54GbzvKtxS1ojyep39IiKBkkB6tEDAdT9Sh/vh/99QV2Z9Y4tjDKSK7EbCFLPtlgZSZP12iRRsdgW26Di0+NEz9YxA8kavUIuuHSrpovy8anr/aQ7q6RjzZxAgVZEsytZonE7JALezxJ7O51SrUovqegTVnYYx3+7Ja/5KNrtzMRX+FgZI6KHyGltDjTHsIXMq1A59u/eIRbfu05XKHwQnGID49+HYC/Ld5M09P5we5nmHJ6uNDsp+1rbI1sMG6u8eXNA1iKiydkTq4M8caxszR8g6prEVUcrjaypPQ2Dc9grRFHU3xK9Sg7u9fYEtvEDlQuVHoZiZWIqjY9eo2ZVp6M3sSQPU6VB5m8PMDw2AZdkToN1+Dh7jN0hMZHXroLK+IwkikxW8yQiHTwAxld8XnP0An+6Mrt3DkwTdvXKdqRsK7VipMwOtQdg4ZtkI2EZrylUgrtWkxFoxwBR+aG3Vc5u9TPcFeJhqPT6Bj0JOqMxTc5W+xDlgT1R3vIP7hE3mowWehme26DhVqa8dQmGb2JJxRmbuqw9OGjxBcDGgMy+tEi7rezqLeWSEXaL0/k34nFDD7dReFQQNdYkYzVouEYNGyddKTNzuQ6Xzuzh7GRDbQPJ9nzJ+f5yvQEuUST0rEeEkc2SJlt5p8ehok6V956/ZER8dSA2H/n9a1EnvnSh67rdf8/uxJpuAb5CwbtHo3opszx3lEEYJ2zUBxorETIXYTiPpnHr26DK1GkKFgvRVBsqBhxWgkb5WwMzYCNQoJ62yDxtRjT9ynEjkcYeucihUo3m5UYsiQodaL8v+29d5hdZ3m3e7+r7L5nt+mzp2pGo1G1ZHV3uQEGGzCdgMMBHCAQCCGhHHIukosEko+Pj0DgC6YkFENCNYa4dyzLVrO6NJqi6W3P7n2v8p4/1ljwcQhIMUnsk31f11ySltZ69ztr1nrmLc/z+7U+bTI34MUyFWIPeSjdUsO1oDNdbQJd4p7XKE/HGOn3Q9oFiqThrEqpw4OtSdw6JDMB9GN+8EtiJyxmWzSOlrvxqganCm0AeBSDA4VevKrh1K0gmNmjULU17psYojeWYl9xgKqt8cjIal45dJRvJ3ZxJtXC6kiCvccHSK7xc1A46dsBrcqPxjfRFUmzb7nXySAtuZ2pnamSc3locFdIzIXxj+oEliS12zQGvIv848QuBsIJbARPzvWxs22SI4l2/C7D0V2peAmOaKRbvVRMjbCnzN2Lm8hVPbQ2Zol4yiwWglRKLieQCkki4+Vnno2saVwioFZ5bKaf8pkwZpOBmtIdT96CoNRlUm7RsQ+HCO5K0OwvMLy/B9FSRQQsRpJNGEWd2XQI01DpbEoT1Cs8PDoIs17oKNP/aJbkHg/zmQaqZZ0rBkb5l8KljGVjHK+2OaOcj0aIf+oppj6xG1uVlHM+ZKcFWR/lqo6UAttScB33AVDebtFzl8nE74Xwdhikij5KOQ+lI1ESG/1gCxYejsN1YKQ6qebcLJ4LYDXaBKRgOh3GaLCxf0kf5UJ50dXOvFDxaAbmjjw6IHttehpS2FLw7FY/tdEAoqXK0lUqrjmdlnCemW4N3zMeilcUQAo0KagVXXg352j/hMmZtUHUkE3iKoPWSJ7FATemVPDNCYwhR0jYp9cYuVqjr3kegNHNcWLuGssNNoG2AuWyC9OvElqdxMz7oMlZL8lvFwgFbEvgGvcwFJ/luNJOLeVhdg9oOZVGLUfW6GNLaJq4K0nW8tPnTXAw14NXNahaGqu+Xya1y097OEfRcNHmyuASJk3RPGOFJl7RfJS4J4OFQk/fEj3+JD2eJEfycQyp8oqeEzyT7KEvmCTkKhOPZ0jV/ERdRTKGl0zNB5qNtTVPsqyzK7BMwfKwKpSkZmt0+1Lc0n0cG8GqSJKIq4RXNRgvNHKiN8rOpgV6fEnmqyG2NkxQtXW+NbYdn25wS9cxTkTaAcc9LtZRpM2d5cmlVWwLT9AWzJPeYLK1eZq5Uoi5QgOWLdgQXSZX87Cw3aQnlKLRXUTbYbNQDFKsuhiIJRhXbFqDeSxbIeYpMhhw7EZb4rPEfRnufcNW1gXH6WlL8rNHt3K61MZAOMEa/wIPLa3BsFQq0yGmPrGbrk88hdi6Hu0zSQrfiVN8T4E9HY42TNXW+JF9Cc2xHG1/E2LiJjeyZnJtyzBfG7sSVEls+yJBV5VJQ0M0loh/SqHhpiITOY3BHRNkvtDFlVcOc9/0EKH9sHTzf708opTyMeCxf+/1L9ogYlgqUoJR03B7DGaLISxbwbIEUoBVVRG6jWIKFtJBZEnD9IFlqmi6hZF241pWKduAUsC9oJG3gghTkCn40HPOfDowbzFbcOP2GmSrzmiiajm3TcsLTEtBGIJS0Y2d19ENQbnqwsy6ftFZlw22AEtgu6Bma5g1R/HMDlqoVThTbidb85I0/BhSJWd6qNo6I5km/HoNG0Ghx8tMMcx0MkzQV2W41IpbMUmkggTdVY6X4syVQ7R6cizlAozrjeRND7ma97wCuSIk4/kY6ZIXj2oyVwzhD1fJ1Hwkyz7UZRcVW4ChYEoVRdhM5SM0eguULR3TVsiZXuYKIXIuDx7VJFf1IExBxdIwpErV0piqxqjaGkLI8+siXtUgb7ipWc45Z/ItKEJiS4WpVATDUMmEvU6yXXZFwDgK6YqXXN6Hv7lGWCsxRwhNcda8lssBDEslW/UggLg/Q1CtsJAIEeks41ZMLI+kaLrIGx588wqasHApzlrEZCKCbSt44gq2KhFb1yMPnsCw4mhli3JNRxeW4y4gLGTaRTHgQml2oRigLmr4lBreGQ3TJ8nGvKQLPqSEYtZDdkAnXwjhSSgUDRd60SaiO8WCwZLtqNtdJC+0kciLdk3E3RuXXe/8ILWohWdBQ6536iWMsSCWz3YEdPIqwYEMUX+J6UTEKaTK6riTKrWwTagvjddl4FItsmUPzYECi/kglWejDF4zxvHpdtoasyweaUGxoNZqEG4s0PIJFcuvU/p4jqUjLdgdFRRVEm9KM5cKYSS8rBqao2zojvPeuSbUhhqqaiMl9L9vntN/0YunqUx13kd8cIlLYjOczrbiVk36gwnyhgdFSMqWzmimkULFTX9smVTFR9RT4uh4nFdvfJay7WKmFKbFk+d4so1STacpUCRd8pJJ+/E3VNAUG0WxyQ5HsZtriJQL222jp1UUE1wZpwam1GHj68nhdRmUazri52E+dPv3+OSPX4vRYuCa1ak1WSgVBVfGeekQ4JsXZLZW6fs2nHuVRnBMJbfWQOg25DWk2ylJ8J5z0fudWeyQn8zaBtKvLNIRzbIlOs0PDm3FGymjHGhwbCHWF7BtR0Iy9oxO+SU5tCdDlFscRfrIKUElJvAtShRTolYl7rTJ4nY3xoYiXm+NwnjIMQf32jQ+pVHoFgxeM8YbWvfz119+I/nVJv7mIqriLHBncz6G4guONu61MxTv6yO5rxVbl0gVhAldu2cYHW/lsnUjnEy00h1Oc3yqnXVd80xnwmTmGxAei+heF/LmJH6XQbGmE/WVGTvZzpbNY5xcaKNa1tnYPcux4z1M/uG/vYvyqwRDcbnl8j+6oHfkiXs+XK+d+U14XAa+DWk0U8XVYdHW4FSZzmsW+ckQWtBAhKrkR8L419dASLynPSjbMxCH6myQ8uEY1iVpgt9uIL9NIdvuQ9VsGCxydKSTnu4E6bs7MNeZoNl4GqpkFoMof5lFV6sszkQRbVVkWSPQXGA+3UAt5aGjP8FkIoKuO7sFnlgZKcG2FPRjfsrf8REoFCikfSiRGjPDzVx71TBlQ2d34zgBtcIsEUJamXtn1hLyOIEg8cVerN9fJm+40b0GDVqFVjXL0WQ7ybKPl8dPsGwEsKVgWLQQD2XpCyxzINFF0FVlz54RHp5ZTagtSaHq5pIts5xOt9DTkGKxHGQ2HaJyNoQYzGAYKutfdZZFM0TrlgVqlkrfYJKBwBIjhWYm8xECeg1VsVksBGCpAT66xK3RaY4MxXll4zgKkm8c3UlDQ5nX9x3myJo42T1e0hWDntAIqxSLk4lWmltz+KIlGnwV1rxqkmzNw1whhKrYdHenOdnYil83ibx8ipiniCkVTna3Ui66CVyVYzntiCqXhaTdv8SO2AR3T6xn87ZRev1JfnBwK+prE1zZOMsT92zmwZcU6X/lCDsiE/zTmZ3Uqhruw35kp0XhO84IpHKfC/9Lxlm4M8Srho6gCIlpK/x0ZD3d3QlmPjlA5mZB8VSEt9/0CN/88bUICe07F8iVPSR3CqJA9c5WGm+bZu7eLtbcNEH6E928+X/t5WsHLyf9mW6Uqy/ed+aFZqP5og0iAFVDo8FXIVv00uhxNExnMmG88TwRf5lUwcfqbecI6I7bXGrHinlUVSd6RCF1ZYVawUPt9SW8uknQU2VuPkLT4y4yN5aIeYpkr0/S/IMooZEq1ZiXqVskuZMxbLfE3VGk7Wtu+NASbtWk2ZtnPhZiKR/gmr4RFssNKMJmLNVIY6CIVzNYagiw8PMOfNuWGeyZZyoVYWv/GD61ymUt41hSwSNMGvUCCpJr2kfImx5SNR+J2016AikMqWBYKlGtiCFVOgJZtoUnOJDpoWS6WBeap2arYMLxdDurwwk0YbNvqZf2hhwnxzpQMxr7ajrFBT9L0QBCOFvdag3yC07V8UnRxrvbH+VLM9fQGU8yknaUzYqGi/nFMEiBNAUo0NKW4dypNpR1knOLMaqmhlsz8fhqBDxVHl4aZGopipj04soIDvREGByc5ar4KFG1SGnZR8XvYmE+4qQ+SYFwWywfbME3J0hsNkjIMGgSLaETmBR4LEmuqZmu/TUWt0apxmzSkSAhV5lKRefkQ6sZ25JCD9ZYnAvzdE2n96oJ3tL0FO985q2kqz6qSz6UskC7PAVZH8X3FCjXdKx9rSzcGWLVm5/lh9/cgqJKbEugqJLZw234bs9AIoDpt9mf7qF11xxzh9pIPtNKtdmk42HB0q1e/LemGQwtMtoeZ/reHoq3VQhlOmm7X2PqFpP4PYJzF/vgv8BmDy/aIKIIm1pNIxSpsLTcgFtx3OekhKC3SsBVZa4UpieQpMezjCK6HCnBipdcIkDr701wQ2iWiVKMqKtEkytPo57n+8ql8CbYHF7k8Yl+Xj1wlKd+r4/pbJB4dB53Mkyt5jx4G9vnOPzWLrZ4C4T0CkP+eVQhSRZ9bAzMkPY48/rFUpAOfwa34mSuHtMa8eomnf4Mo/PNrPYvcarQTqIS4Ox8M+s75lCEJKRXWBeYo2i6HT+a6DSGVFnrm2O6EOFEsZ10zcdV0bPsy6xCEc56zfcOb+WlG09QtnTKlk666sOUCtmHWknuyLNrzRhu1eSpB9czeNkU83d3Y1yew+epcd1NRzmZbSNV9rE4HeHTEy9jU/80ZxNNmKZC4WAj3VdOMhBfOv+zmMs1EPaUSZYUln7aiX1JlZnFiFOgptssphqIf1un1a2QXC8ot9q0Pa7QsTnLM0vd3JNdy01bjrFUDXD6rkGkCvEbJ50t3kOtVPfkWB3JMPtgF1KAb0FSjEM1IvH25JhoCqI1F7HKOu5pF2dPrObNtz3GDw9cTfNf6Yy8V0X1WsgnIrzynY/zg9Q2tDM+qj/x8JaP/RxLKuxN9FGu6ucXUb9/6nJeNXSEH35zCwNvPYw60AeJJG33WTxSHOKVvce43zXEm7sP8O1Pv4zbP/Zjzja18oOTm7ll6Dh317ayJT6LR3XcC2+//mG+ftd1vOOSvdz9t9fQ//4z3Ohf4omefvjhxT37L7Q1kRdt2jsAUmCviMUrK2M8KX/98NCWilPnIQWs2EoqQq7kctjnz3mufN+0VdwuE0VIZ3FQsVFwrpeKxHY5w1tFee7aX3zuc30wpHo+UcmWyko/pSNhcP7cX/TRoxrnr3UpFjZOUpQibLQVW063Yv7i/3+pba/qWGmatqNtYkmBgsSWAp9WO28j6tKd6y0pELbzWZbHSc7TVEczw7BVDEsBl426cm/cuomuWwiL81XM4pcWa8GRPrQ1J3BISyBtga5b6LqF7RJUQ4qzvqA4WqSKcIzHhQAbp+9qzfGysaTi2DdI53NqtorlwalhsRxvXOlauXmKoxkoFInlkZhepz1hglRX7rtiIx1tIaeYzuusczz389EUGynF+UVUqTr9U1SJOtCHNTKOiISp2hpYznmWrWAhWKk6cO6BLZxnQZWYtoquWOf/T7EEqrAx3QJFOPdWudiIcKEWmi8kG80XKhKBbTkPnjSdbE5bSCxLwbAUDFtFWgLTVqnaOqZ0jpmWAqYTLAzpJA5VbWdXwWLFl9VWMFYe4qqtYdmKkyeA82IIS6BUnd0LKQU1S8NYkcEzbBXTVqjauuPjKySmpZ4PVjVLRakJDEulutLH566r2RoSJ5g9l6hmSBVTqivfh+a85FLFPB9AfvFv03b6jC2o2SsZnVJQW7lesXB0UXECj7CcgKfUwLQULFVgsdKurYAlkFL8om1LQZE42bNSQVl5Up8LPMJwCutsayWg2gLLEgjhLErqRRvFUBG2sxBqrkzLwAlq5spbLiznMwzLEby2V74vp55RIGwbxXT6Z9sCbIG0FaTlCFQ9F4SEDYphIS0NW3Oc+pyfs4JiOJ/zXPA3bUdZvmpr6MJCmM8dE5BIns9staUzwjKkimU791ExOH/fWDmGDaZUqFrait+vc8yQzs/Blr94/i4GJ2P1hTUUedHuznj622XLx94PiiP609ycxZaCxHQEdBvFZWEbKqrHxF52o5YUZFcZueCBpioy7UK6JGv/ao7h98eRGihVwarv5Rh+p5/BLxd5yz/fx19+/3XUOmoEwmX87hq5fc10Xj2FKRW0j4Uw/zrL+GQz1BSUqoLtsdEzKkbUxDulIxWwfM5vSNtjowQMBtqXGDnS6YgWu230YI0b+s/Q7s4wWYniVQ0iWol9yV76g8tkDC+6YjGWbeTy5jG+98hubt3zNNOlCDaCIw+t4X2v/SlfPns5+USAt25/ih/881U0XTPHXKqBj19yD2G1xJemr+HVbc/yuZN7qCz60WJlVFVyZfcoyaofn1bjmUfWserby5gRH1f/w9NcFzzB7x96G3+2/gGKtpu75i/hksgM90+tQQAuzaItmOPksz3s2n6GBr1CkyvPQ/OD5Ctubu09StyV4myllVTNT9F0UbE0lkpBsmUPH1jzCEG1zIcfeT1dP4OZNxjYOZ3un0psl2D5LSVU1abtMy5edsdjdOopPnLw1WzumkYRktOJFmwpcGkmfpfBbV37CKpl/uIf30yp22TXhhGeOTDItm1nSVb8jJ5t4ycv/Tz3F9YxXYlyz6NbETY0b1ok83grlXVlZNrFqrVzzvavpXLVqpHzATx5WZrEu3ahvjyJ6xsRkutVbr/1Pr7wzB7UjEb4jCC1XtKyOkH4o26m/lzQFCwyc7wVYQnMRoP373iIv9t3HZgKSsBg4s0XrmzW0BCXW7e994LekUcf+Wh9d+Y3IQA1q2HHDERaxx93huvJkoKMWs4uS9JNY2uGrG5RSXnwewwqlheZdDuZpK15Ens6Ee1lvN4apqmSGwiiFmD2uhCjlRZkfxGybgpJH2WfC28VxhYbsU2FwOUeSksqur+G5VLxNNcwDBV1IUC4I0NCDYEi0RZd2A0mqsfE76syPNaOAijRGjLhpr0nS6rmI6yXOJlqoz2QxXSrVEydVncWY2Uq1OrP4VZM1HiJoummYLqpmhq13gpVW6dW0xAVhUPpLqrRlfoUSyVr+alIF8Nj7UxEprFtgRQSMebH6Knw5HQfEX+ZBneFWotBbm0UtSY5nO0kpJUwTYXRSguJWoDRsVa6NqYxDA3bFtQMjSUlgO2zeHbeMTUcaFxmfiGCLKtMtUYpWG7uHtmwskOlOpP6OQ/BAUeX5VChB+E1KbS78R3R8C5LaiFntKI+00BhlUmlEe5dWE/IXSb4hJfDV3Shu0wqWTeiqFFoMMi6Le73rwOgHLfoeFCwT/SjlwWTuQgLM1E6+xJMm2EeTQwyutCEFTVWRg/OiKA5lqMYcDE63kp3d4LZw208UhwCy3ETjLxLpekf9uF+YyuzkRjGqjLfn95MX/cSk54YTd+qktzhY2E2gr1RJ+xfJOopMh8v0fEVnfwH8tx5bjvYgkCLs0N30c/+C+wX/4t2JBJfF5Jrv/g2XKpFzVK5tvkMtlR4fHmAM5NtdHcskyz6KGS93Lz+GEdTHcweaKdv5xSFmptUwXc+Tbry5TZyb8rTG00xkY7QEiwwPtfIjr4JDjy5hsCaNJWaTjRYZOFMM9u2nUVBcnCqC0W10XWLdc0LFAw3o0uNbI1Pk1mpMlWEpFBz41ZNTKkwdbCDK68+zny5gYV8kGLZjbQFf7/tO/zL8g52h0bpcSVImA2E1RIPZtfR7Mo7tTN/uovNnzqMpticzLbx3vgjeITBd5M7SFb9/HHHA0wYTczUoszXQqzyJOh2JfhpajMNWpk+b4K75i/h8sYxErUgfd4Ei0aDU5Rnq4yXGjk42UVrNEe+4uYVPSdQhY0lFUaLTWwMztKiZ7FQGKs0E1LL6IrJ2WIrT891c0V8nF5vgslyIy+PHKEmVf785C1E/SVe1naCU4V2XIpJ1vDS4cmwyT/F/z53Fa+MH+NMsZWypbMxOMtcNcxC5Zd0aYWkZLrwqAbayhrUSKqRXN7Hnv5hDi52MhRbwpZOwtvaBkfOQBE2bZ4c3zu4jT0bTrPav8gdRy9nc/c0MXeRzYEpnkivpmS6mP9qH4ntNn0/NCg3u+j9wBlmPjlA+vYCr+w95hhrSZV7ptfRFsxRvWqBs3dsA03yrm2P87WTuzDKOptXTRFxlXlyso9VzcuUP93O9f/j53zt4Wu4dNsIy5/o4dK/PcwDU2sIfzXI9GssJm+78BFDQ7BDbrv0Dy/oHXnk8Qsf4TwfXrQjkYzhZeRoJ3bYcDx4Nzn+qMNHuhDApBJDn3JDZ5X7xoYw5vzIoM3YgS5s3ZlaqOEaxX9qp9CnUMp5OFVupe8LkpHbm1n1Tcng5xY5KNeQy3kRqqRc0+m9q8bE6ihVQyP+jzqJd5coLPnZn+9xRKKLGs9MDCG6iphLXlDAlXJEidFtRNDm2HI76TNRbK/EvahirilxsNRHmyfLk9kBZrxRolqRR7Nr6PSkWao5L9TEm22u04t84/QOru07y6P5Iaq2xgMn1vGubY/zd3PXc2a5mSs7xrln/yXs3jzMvdV1vLT5JEG1zFcmLmdn0wRPLq9iOhlGykGaQgXCnjI+reYs7C56EN930zpfpvBFNzsDY3xp4mqubR3GRvD1yd3c0HaGfUu9uDUTt+os1FZGQoyHYsyUwsR9Gb40ew2pso8NzfO0uHM8mhgkVfbhUp2f05HZDsZamriydQxLKuyf7UJ5JsT+dT2IpIvApIJWkaQ32ATiOdw/CTPwjjM0ugv86xOX4uvN4fVVObzUSSbj5zTNqIpka/M0VVtj74kB9JSGfyhN9KBGYnWAk6lWkPCa5kPcn17Pz5Y2OibolkDusJ1amJvcKAaUEq1kbhaQCHC/a8hZRLUF3m+FmY3ESN3RyerbDzD2mZ2cLLShqhKzpDH24wEqOwtYpsryN7pJ3SA5knPMH48/MIjxEpuGfDP5jI/ybhVpXnzSxwttd+ZFG0QUIenbOEvJ0Al2V9GEo/fRuW6BZNFHyFvB3W6SLnkxLBUl7thE+tw1Gn1FZn/Sg2JqJDc6S9meUQ+2Lhl/jUSfF8xdLoi7Uig10E97qcZscraXzO/XWO0pEwhWObongpX1Em7No6m2oywerBLylSlU3BQbV8yh41XcgKZalKsuukMpWAMVQyPSX6ZiaqjCxqfUuCZ8GkNqZC0fYb3MkWwcz8ooJhAqM1JqRlVtypZO3J3GoxtoHhO3YqAJm55ImitDZ3ikZYDlih/DVqlIDd3WuTV+BEXY/Hj2EtQ5N1defZzHRge4fsMZRorNTOWjrPp+mYmbfdi6jz2uLFd5p/nzZAi1zSagVnht52HGy00kCz6EcHaXOiMZ3P05OnxZrgqf4WQ5To8/SV9gmaLpJqYX+aPOhzleiXOq0M5cMcRQ9yKjuUZWeZbY6R3nn0b2YHZbbF01SaHLzRm9E+m32LBqhhPnOihstnlNwwwtepa5bSGyNS+FmotUzg8CylUXbeEcvd4EV/jO8tOFnTTvWOC2rn18Xl6DRzW4rn2YfzlxOVs8MySCDXTGkvzF99+KKy8Rb0gw8XshZM1EXdToDqcpnopg+m3e3H0Aa2Ux+o71L8FYVYaaythndrLqQ0/zjvEj7H/w3fTfV2LkbS48pwLINUWWL7XRGitowmb11zJM/6WKciDERzrv4d3fex+WB9wZ12950n8NL7DZw4t2OhMbapL9n3s7vQ0pziSbubRlBkXYjOcbcasmPq3GuUyM6zvOsMqzxFilmdFiEwCnllp5y8B+QmqJlBnAkCptrgwxtcDD2bWEtRLNrhyff/hG/uja+5mtRpivhFgTWODRpdXMJMPU8i7eeOl+jmU76PKnCWslVnsXOF1u53SulVe3HKZiO2nvj6UG6fSlUbFJ1II8fGaQ3QPj9PiSPDQ3yK1dz3LP/Hrm0w0Ys35Cq9IAtAbzbAjPMVmKYtoKqwLLTJaivLrpEP/j7I3EfEXyNTfv632Uvz79EtY2LZKreZh4qIfXv+4xZioRAA4udFI1NDwPBUlfYvHmnfto1PN88Wcv5cY9h9n7j5dS3ZPD4zL4yJr7OFbqYr4S4tFD65Cq5BVbn+WnRzeBhMAZF5Hr5+lpSKKvbFEeSXRwSdMsDx1aR9e9MPUqG6H84rmSZZWhL2RIbYmyeIWFN1aGow1cffNh7j26Hveczh/eeg8p0883H7kSqUpee8UzLFWDPHZkiMs2nqXNk+Un9+1ESMfiU8ZqeHw11rfOcy4To8lfYDoTpno6RO/dRd76jX/l40++iu4fCUrvyZAveXA/EeRj77uTT51+KbWno3Q8XuQP/unHqMLmjpkrKZs617YM41NqfPHZq3j7pqfYn+5h/qt9WB5H7f0NH7qf709vdtTeC228o+UJ/qrvEt49MkrRdvPd+e28quVZvjhyFa/oPkGiFsSlmFwfOsGfHH4tf7rhQb77npfxxi85Mg7nqk18atOPL3w6E+iQOy55zwW9Iw/t/Xjdd+Y34e6Jy7VfvI3UQohoa5bUbBhs8LcWKWa8UFNo71lm6WiL8+D5baTHRg9WaQ4XmJ2N0nenZOJtNiy76burSr7TTfJlFRTVplZwcenqCabvGGBpT43W1gwhdwVLKv9HTcyGoSlOHOkBIXGlVYwGG1e8iDgWxAhIZ4rTVcZOu8EUoEgu2TTOsek4ctFN69ollg+1ENiYpFh2szU+zWI5SLLoo2poGCMNWG6nHdFawU65ncK9phqhSBGfu8bi8Rb8A5mVrWjY2THJI8edxUA1aOD3Vwh6qgxFFpkthTg90gG2oH9gntGpZq4eOsuhhTj5ZT+uRR0x4IzatnZMcUvsCB96/PX09y0Q8xQJalWenuumkPQ5BY6aRNNNqhkP61bPsC40z3CuhWzNg0uxKBgumrxF3tK2jxPlOCPFZnKGh7CrxM9PDDK4ao7Xth3ib378Ksx4lfXdc9QslbGFJjTd4p1De7nz3FZKFTdvGdzPWu8sz5a6mSlHqNoax5faCHkr5CuOuvytvUdY753hw997CzfeeJAtgUm+MnE5NVOjN5wk94E2XvKNJ1mqNdDlTvLZn9yMrUH7xgVSRR/FJT/eGY3+68c5+1gfrbvmeEvn04Czjfupp15GX/cS08thVFUiTwf49Bu/xf8e6GfmY7vx7V4mczJGfMsc00tRVM3CSHhxL6tsu/EETx4Y4jsv/xJvO/T7VJd8aLEyY6+/cPOqhkCH3LHp3Rf0jjz01J/X10R+E6puoQjwx0qoiqS9x/F/Ldd0qh4Td9jAsFS0voKTFGU4OR1ut4klBS2PapRaQGYdJbSF7V5MP1hpN+4pFavZptuX4sQAND7hItveQkYFY00Jl8vE664hqgonn+1Bbys5Bt+tCqot8LprVDbloOZUseq6hd3kKJlVKy4ms468otFhUTE0XGuzXN0+Ss70stq/gBHQWAg1ULU1jvna0RWbiqlRKLtpHMiynPcT8pfZ3XwOt2Lyw2yAazpHOJ1pxZQK6wJz7Iv0nFd329Q8h1+toSkWnf40w8UuPEsK7jUmStbJZ/G5DEpei1XfzHDmvTGkKjHbVAZcS4iqU2LvVQ3CeonmYIFCwo+sqVhV0CMmekOVTMWLJ2JQsTQi7hIe1aRqaQT0KhnLR6IWZCofIZEL0NuYpKGpwKbILKtcS7gzAiOqUTRcGJaKmXFh+U0eWBoie9bx3R0pNVOw3BxOdzKTCVOt6JhZF3mjAemSuCIVxkpN9LoTCAueXuyh1ZUjW/I6KneqwZHXBNnmHefO0m6aXTkip538lcKgi1LO4ySJ+STTmbCziXSojbNNrYCTU6JmNCY9MayaglnS6L+vRPH1bmY+tpv4Xz/F2S9vo2FeMDHSAppEBCTdgwt4vupn/9pu3EmVBStE8F8DiLjAf8h/8Q//C+z3/os2iFiWQno4ihWwKBeDhPqdKUBuOIoVM7AMhdJcgHBXhkzGDzkdb1uBynCIos9GXyOw+sq0/MxD8hVlys0KtqHQ+KiLYgd4EgoNWgWjr0La68H2WEivhWvcS7ndoIyH4JRCYWMF01CpZd2oAQNpCyr7Yyhbso4cgADSKkbEQngs/KEyqdkwWlbFaqmRXvbS0rfMdDlCty/Fw0traHBVCOkVhjPNbGuaYq7smGm7whbN7jw/m9/I1o4phvMtGJZKLeOmzZXl7ulNKEmd/d4C1ukg7o1p0nMh1vQexqfU+F8Hr+VNmw5g+ywsj8LZp3qQDTZHFjpQFZuWWJbhdzfT8ajE0hVyaz2M1JqRmqTT6wgp//D4ZtZ2zztZoisZr4piU0n6WQJ+WltPzF/i2TM9iJrC5ZeepsWd58vjV1Cu6RTzHqShMDzdhWipEtWKPF5YQ6nNpu1RhaXxOFoBesZq2C6Nc9u7EC7o/YFFYk0ASwqmH+ymur6MppvIooKwBJaU1PIuypbOE9nV2BrIHzTyTzfsxFr0IrpzjrHUuiQAM6Uwe2d7KW4AIRW6fGVKR6LEti+SjXnJzDfQvnOB5DNOKrtcSWqLnhE0fauK9/MJxn48wMjbXHx3fju+3cuc/fI2Vv/BAcb/ZpdTT/S4iue2BADDH/QQ/45G8e3LfH3ucpIbnZT9ZFvgNzhb/3peaFu8L9q0d1FzhvSioiIba2Qmw2SmwlgRA6HZTmVtPEeh5EabcztDe9XGDFu420r4FgT+Az4SW8Dvq+I77CV0wE3iMhPvkqS0tkLBcmMXdALTTpEZlqBr1wxUFRCS/GoDl9fArqioORU76cauqVRaLMd3tqyglBUngNQEsuyMhoTPxAxayJpCuDNDOu+jYuks1wKsC80T1KqkV7aIH5vp5+h0nKNTcYJalSfn+lAXXJxJtQDg1Qy0jEbK9ENew3ZJBgOLoEDpeAT3osbJQhvDpVZ6O5YpWG66fibo+84SdneF9scg7CuTmw8yNxMlNCyYvQbmr7NIln1scc/R/ojCfCXERC7KQHyJkyNxYk/rNO3VaNqrUT0XBM3GthXWN82zmA84v4XDNXI1L0XLzZt6DrC74xy62wQJ/r4s+rBjxH5V4AxrPjtDtk+htLFM/rIyi9tczF2usu3a03gSgunrXLR5c1wSnKH3xnM0RXM0NhTxTytYARvfvIKa1Yi6SlwZOkvbUxb2q5O8ce1BmgcTFJb8RJtzpCcjtKslGvQKL+k+zcCdWTofMhgKLeLamCHoqmLbAuGxyJU9VJtNXjF0nJevPc7L1p0gtV5y5r0+Iq4ylZ0FPDM6r2p5lszJGA2ndcb/Zhd9H96H8JksXyJo92eZmGpCm3Oz+EZHpOo9HY+i5x0XALV4ka+gBCx5YV//Sbxo10SCq1tl9M8+iDtcoZrysmFoCgXJ0fE4VFU8sTKVhJem7jTrYgucTLaSmA+h+kzspJvAOZXS1hKN4QKprB+Pt0Z7Q47pdBjlaUcHtVh1Vs61+8M0TJmkBnXyG6qEDrsxAuDZvUzgy2GS/1eRsK9MPJhheLmZqqGxpX2GkumsnRyb6SAULKOpFkvJBkJPeiheUyQUKJOYirB69Rw3tx1lX2YVHd4MqzxLzNfCRLUiw6VWUjUfyYqf6XSY1U0J4r4MD4yu4cOX3I+FwqG8I634XMGdW7FIVAIkyn7a/DmWSo5uaqLgp1bTqJVcSEPBE65QXfIhFUcXBECYAn9HnmpVw1z28j9u+C4feuJ1qF4Lq6bQ2Z5iOe+nnPSC5oxG1IyG2lVEOxpA3Z6mNBaiff0iPr3G2ZNxCJr4GiqUJ4I0jCl4kzbLmwSrd01g2go3NJ/m7w9dTSRaILXYAFLgj5VQFJvSSJiGcTBuyMK+MKYPTK+k9y7Hx2fuKj9ddy8zd10jpg9sFdzbU7h1k+SJJqzWKrKg4UqrWL1lVrUs87b4Xv78J2/AuzpDueTGMhVkzSkX8EQqSAmBBwMkdxp03Ksyd4VTC4MNLQPLLMxGcAVrWKaKotgE/BVC3oozhVFA+EwG3nqY3L2rWFpuYNeqc+w93Y+W1CFeJhgoU9kfo7qmTPQRD4e/duFq7yF/u9y59g8u6B154OAn6msiv4maoaEUVGplP0JIRhONSCnQllyYIYtK0ouQgmQqwN7j69GKAnWohD7so9Jqku83ETWV2NsLLH4igpn0MFYK0/ejIiNvsQh9tZH3fvKn/O09N2PvqqLeVKDJU6GyvwP9xmWEpRD+XBDvn08zdaqTaqGBRbsFy28TmFB5qm+QwLiKVEFXIRfwYXolNFfRXrGMsr+JVNCHcEvGFhs56O9hlT/BSKGZ5WqAqKvIwwuDbIzOUbNVIp4S7iaTgeASd//rTl72sgP8dGkTplSYuLeXP3jrv/J3R6+BOQ+3XLOfw3duxL4pzaHZHj6++2cE1TJ/M3wj71z3FH93ZA8kdSrSgwgabOqeJVP10urPsf/gajo+KbB8Kus+d4gOLY1w2Xxo8wPMG2HuntjAjvgkP6/2g5BomkVjZ5HFIy3Er50i6KrQ3HGOxyb7qZQivOXKJ2nWcxwrxEk3+0hv8jlTs1yQ0zOtfPjS+3EJEzIumj+rknqXgp5R6fx7G1EzmfxkjkKnSv+fltn9o8P0uhP8+TO3EPibOTyqybnJbqa261hWHk2zePfqvfiUKp//8quxVlms7Zrn9OFuunY6NqVnJ1pZu2qe116/l4lSjP1PrUGokt6Ncyw8HEc0lihmPfhvThIFlm71siU+i2k79UnWB5xM1Njb5lj+RjfLl9q8Yu1B7jyxHTRJy+Mqy5e4yd27ioaXjmH96wCnlltAODogLo/BLd3H+VZ+B5pmUXhp4aKnM/Ut3t8RjUONsuH/+SC6y8QwVG4YOIMlBY+MDqKO+DD7y9hJF2pFYcPOUY7NdBB+xEtmTxmrrOEOVhECfJ4qwS+FmLutSm9TitH5Jjqb00xONTK0ao5zj/Xg2ZLCsFT6YkmGf95Ly7YFLCmYm2gk2JonPxck2pnBtFTykyHCvWmkdIr5BJDPetFcjhVC8Odeml4zzbmlGEbBhZbUsAI2n7vxW/zz0g42Ncww5Jl17DiVKoeKvQBUbY0zf7CG1f8wTKrmZ7YY4v09DwPw1dkrCOhV3tC8n+FKG0u1IGOFJtY2zLPau8Aj6TU0rNg97F/q5rKWcTKGD7dqYthOIaAtFQqGm9MLLfg8NfJFD1f0jTHgW2KyEmW+HKLHnySgVvGpNc4WmwnrZbyqwclsG6eOd7F2wxRd/jTLVT/XxU5TtXW+dOpKQv4yW5pmOJVuxaM5erEhV4XLYqN8f3ILN8VPcjLfRrLipyeQYrYUompp1CwVn26gK9Z5+cOAq0rJcLGUDWBUNTb3THNqsZXOSIaq5Sxk9zcsk6gEKJs6QVeFU/etpuvaSTr9GR46tYY9Q8MsVoIMNSwwWw5Ts1UKH2xj+rognQ/lyQ740W9bpHpnK7Vb06xrWkBXLKqWxtGFdsL+Mq7/FWX6Bg2lvcx1/cM8PL4a21LoaMzQ7s/yzHgPsWiByE0jdD3j56Fja+nvWyD1vTgDbx1mOh+m/JMW8leUGX/jhW/Fhvztcuead17QO/LA4b/8je0KITqBbwKtOCoud0gp/+6CGv/ldl6sQaShIS4j//OPCIzoFPpNhj6TQBgmpz/U7hg0D2vULstjnw3gn4Fyk8AYLGMvuxGxKn1ty4xOtNDekSKg15jY14mtQ9/WaYqGi6i3xHy+geX5EH29i2yIzLHKk2CmFuGHpzZjW4JwuMire45y5w/24F2SWB5BpdFZ3Q8PC0ptjo9NtdnCO6s6gj/rajQ250ifjuHKCbRL0xiHIxiDZayySmNrjnJNp1J2YRU1XIsawUkw/IJNbzjBvsfX4coKSt0mit9AqBJtxIdYm4eTQWxdEt2yRPJIM217LRKbNMz1Rdweg0rZRSRUJHUmht1UQ5ZVlLKK2lpCOeun2mzRPzCPV3PEm87MN/OhTQ/xmbtuIbQhSTIVQNMttBN+UJxpheWVKFWB1CXKSg1SsehBLrqRmqRlYBmPZrIpOstEIcZCMYgQkkpNR/9RBN+b57mqZYRvPXYFWnMZTbdQFImqOLIEfzr0AF8Y3YNbM9kYm2NncJRzVWeXxqfW+O6prazvmOPEbDtGzsUNm08Q1st8/6kd3LprP4ZUOZ5uZ/xsK3gtlLTO+268j3sX1uPXq5xZaiHkL3N5yziHU53EPEVmCyEqhkajr8RgaBGv6twPWwoOLHcT9RTZGp7iSC6OJmyaPXnu3ncp3YMLAExMNXHZ0Cinllu4tGWGqR1FYnsjNHvy2FKwyT/NqZIjWl2w3Hx127cuPIj42uXOwQsMIkd+axBpA9qklIeFEEHgEPBKKeWpC/qAFV600xnRZtDTt0SlS6PVVcP3j2UA4qUlFjNBvDeUCCo26TU22iU11KoLo+yieWAZv6vG8o87iZiSuVoj7oSKYgkst2TkdAdaXmFJwptvepx/3n81S2fj3BvuQCpgtBh0tKcIuKqMHOnk69NX4d+cweWuUarpKIZGZyhPrt+NqDl2ACHdhE5nF0PJ+on5ioi1klLVhUc3EVvSvLP/GeZrIdb5ZgFYNByj7wPpbgJ7qpRMF0+e7WfLZSNM5SJ0+4pc0+QkRn3Zdzmv6j1GoidIwXSxJ3KGL9lX4d9ZJKJYrAokaNAqeBSDpVqQn2T9iCUP1+8+ysOjg7xq8BhjHY2Mp2Pwl42ceI0LFLh1136u8I7yP/uKbGuZIhov0qjnOb2qjb0zfXhWNDgivjLLeT9rWxa4LnaaA7lewmtKuBWTI5k4PYEUVwTPElC7UUQr88UGNjTPkXpblksjU7w0eIw7zSswqxpbu6YomG6Oj8bRfQbfW9jG8mIDSNjaNMVMLcbRbAfTuQiFsmNs9ezZbvRAjdauFF7V4MrgGX5g7OTAcjevjR/igXNrGFwzy6bILD96YBc7vWMsx4Ks987wyXveSLnYwE+v8FHNuZnIaXgSCu3XOZKGo+1xbr/eGfFZUmHmeCvz8RJHD60CYPXXMrz5rn3cv7wdz1f9DH/Qg57Q2Us/CHhocS279o6SvCzNibuGUB+M8PI/PsLff+3VKJbEVgXwrYt79n9Hv/illPPA/Mrf80KI00AH8N8jiEhg4lwzweYCC1NRLtswgiJspmdieEOOQdLidITNQxPUbI0pI0xTNMfCQhiqKqw16ehbpnC4Bf/mJFGfE4RmH+ukGnFEaUq2i9juBWbPNaIWVCy/hWfSxZwaBilABWEIAh7HmKonkiZV9jE53sxlG88ymmlEVWzm5qKEYwU8uklXS4qzJ+IQNmhpyrIwHeWKDcMMl1podef4xvQu+oJJgnqFuXKIG5tOMVOLYNoKTYMFVvsXKZkuegIpzhTaKFouor4yEa3I/bNDJDMBlspBMjkffeEkw8vNXN04jIrks0/ewC1bnyXor5CNKDzy6CXofQV+eGoz8aY0MX+Jc7f78B1XUQw4tqaDe13r8bgNFCRT5Qh3HtjJDZtO4NJMJ7kNMFaEnI5Oxzk6HaenJcnjCwNYNYVL+yexpOBDT7zOuWcr0ofp/S10Xj4NwFeWrsLdk6c22sCJY0PoRYm3SQAu8nd2Il4i8M2oPNI4gK5aNPx9A6mXaNgeGzSJZ1anEoeEpfIM3RxMdOGOF/C9R/C521+O5ZakvFW+N7ydK648yZLlmGvfObsT95Y8FVOltaHI4rkAgzsmKBouxk62s+amCabv7eHrd12HYq24IwYkHV/RafzECMcfGGT6L1X+5PBr2X7jCfav7Sb+HY3FNxbRJv0IG7q3zdDsyXPiriHaXnma6gM9vPfAmzA2mDR1pUlNRv4dD//vfvYghOgBNgPPXOy1vzWI/FvzJiFEFPgXoAeYAF4npUyvXPNR4O2ABfyRlPL+leOXAv8EeIF7gPdLKaUQwr3yGZcCSeD1UsqJ39Qvt2rSv2oBWwragnka3QVsKejvWWR8rpFooES0PcvRyTjruubRVYul4SY6hxZRFZuJ0RYWTjaj9BXx/WOE0RtDuMJVan1V3P4a5rTjh5v6eSv01bCiNorbwigpbOyZRVMsDp3oA59JruQhHs6yVAyQyvpZt2aasWzMEUeyFJpasqiKjWUrTJ1pZue2YeaKIZYLfvyNJfad6+NPLnmQZ3J93NJ+lPBKOn6PZ5m9mVU0u/NUbY3D39qI8Xsnzts+3NRyAp9S5VvFnTyVXsXv9+xjphYlqDrCzh2eDOt75zhZ6KBBK/O+yx7m6UwvA7EEc+4QV24c5Uy+hQ2hOZK1AKP5RpRpD+X1ZaSpsC02iS4sBhuXyJluVvuX2LZ7krzlIRtz3PMUIckaHjIFH1evGmFdYJbRUgu3th3GQuHbk9uxpeCjl93DqVI7ecNDohqgZ2sSr2pwKN3FjU2nOO5pw7cuyWXXjbNcDXBquQVNtQldn8cq+8h2eNjRNoVfqzLz8TDFXIRswctAS4LxSIzVkQxezaDJU+CS4BR3nL0c7Wtlbos8ytcPX0Z/eJmb4if5xuNXEN5d5orWMdb0zvPZU9dimiqpva1YjTaZL3ShF222/N9jpD/RTfG2Cu+4ZC+qsDGkytcOXk7+A3n4RA/GS2yUAyH+9K0/4q8evAV3UqX49mUCQC6u4PIYpL4Xx37nAuqDEaoP9OC+YYL3nUzz2cdvpPG2FKmPxC7ujZUS7Asu2msUQhz8pX/fIaW841dPEkIEcEQaPyClzF1chy5gTeTfmjcBvw+kpJSfFkJ8BIhIKT8shFgLfBfYDrQDDwGrpZSWEGI/8H7gaZwg8nkp5b1CiPcAG6WU7xJCvAF4lZTy9b+pXx3rwrL/8+9gS3SapxM9vKHzEIqweTCxFk2xaPdm2bfQy+t6DrHFO8Hhcg+Hss5W6JHFDm7rf5oe1zLDlTZSpp8rg2doUvN8efEaVvsXAfj6qV38ycaHWDIaOFtsZl1gnn8evxRds8iXPLyq/yhHMnG2RKZp0XMMuBc4XOrhRL6dt7c+QcZyshHvSW1kTWAeXViMllp46Nxq9vSO0O9d4rsTW/nD/sc4XOhmvNDImdlWtvZM4lJMWtx5tgXGmaw1Ytgac9Uw3d5lrvCd5XPz19PhyZAxfLyl6Sk+N3M9WyOTzFXD3Ld/E392zc+wcBTW9md7qFkqJ5/oh4EiH910Hz6lyocffx3v2fUIX/nZDazZdQ6PavB7Lft4utDPRCnG0YV2ynkPf7TtYb45tgO3bpI40czL9xw4LyStCptnc520enLcM7IW96EAnquWURWbmqk6qfs1ja4vq6TWuMldUSEaLpA53sgHb76bz5+6hup0gP/58m8zUm3hS09eCza89bK9LFQbeOjJTbz0imfxa1V+9OAutJLAckmMthq6x+SG/jMcWo4zEE4wWwwzcaydtr2Sv/jbr/KOB99Oz49tlt9VorUhz8SBOF957Zf5szO3khyO0XhE8NlPfBELhT878xpsKbiufZiIXuTrp3fz5sEDHM50MnfHKkfS0II3/em93HluO1e3jzCSb+Yjnffw8be/k09+7SssWCG+Pnc57+l4lI+deiW3dB/ndKGV66On6NSTvPfAm3jfxsf42boIbx2eJmEGSZt+PrnxJxe+JuJtk7v6/q8LOZX7T/31b21XCKEDPwPul1J+9oIa/tU2LnZhVQjxE+DvV76ullLOrwSax6SUgyujEKSUn1o5/37gEzijlUellGtWjr9x5fo/eO4cKeU+IYQGLABN8jd0LrymWV76v3/P0RFF0OV3MlYnClFKhk7MW8KwVIKuCqatkjccAR9NsZ1dib9pZeoGDStqonsNx0xKk9iGQvtPNeZeYfKOLU/yjdM76PiKjuVV0UoWM7c753p8NWpVncBTPlpePYmm2HhUg4LhpmppdPizJMoBxIqeiF93jK4zVS/TixG6WlN4NUfHI+7PsKNhHENqRLUCKjZJK0DV1pmqRrGlIGN4Wa4E6AmkmCmFnarU2GlcwuS7s9t5c8fTPJBaT81WeXXTYf5h8qrzGiZXNo2eXw/xqgbfeXI3nnmV/hvGOXG0m+2XjjCTD5Mtewh8r4GFK2zQJZsHJ/iH3rvYcc8HuO6SU1Rtlbgnw6lcG8cmO5CWY3Lli5VobiiQLPp4Xd+zPLq0mhZv3rEALTfQE0ix2rfA0Xwn+6Z6qKa8dPYm8GoGt7YfZqd3nNf88x9jtlVpa3FqgBZnImgBwzFBf6qB/JBBf98CQb3CuXTM8XepKuh5gTvtLGhbnRUG2pd4Q/sB/uLRV6IEDV6z7lmeXOwjX3FzaesMJ5bbuHvj1/nj6ZvZ3DDNt791PXpO0vy6KabTYdy6SS7vw7YE0lRou1+j7/1nUISNLRX2nuoHW9DQXCCf8RE64Oa97/0RnzlxPcF/DZDcKNHzjmRibahMSyzLzqYJHvnaTjIbnES7T177Q7452Mno53ay6l/KPHwRNS4hb5vc3fu2C3pX7zv9qd+2sCqAb+AMBj5wQY3+Gi5qTeRX5k0tKwszrASS5pXTOnBGGs8xs3LMWPn7rx5/7prplbZMIUQWiAHLv/L5twO3AwRa/edFlRUhCeslLBwtzsVECLVZUqy6GM03MRR3iquyE2HaBpewpc7My1WECYpu0fJ9DzM32iihKlZeZ+HWKtqkl4VaA+qxAOdeZ4CUCDdoY37WXXYOTVgcOdJHZkuNWjpMezjHUjFAJuejtznJuVyUUtV1Xsy4ZqlkhYelkUa2XjrCbCHEUi6AlILJRITLN41wrNjJkG+ekFrElgq6sMgaXqJ6kQatytG719P5xjSmrVBBd9S4BJRNnUczQ+wKjzFbjTBtRPFoButC8zRoFZKG3/GocWc5mutky6YxZvrCDAYXUTZJ+v0JGl1FxgsxxtaGwG2DKdgQmuOuwgCXrj2HLQVr/Iv41CqhaBmXauJRDXRhkzfdHJtr57KucwTUCuvC8wz6FrClwnDmUpZ1Pxv9BkGtwvbOSZabnGAY1Co8lBzCjgqC65IYlsrm2CxL1QD5mBtNtWkOFshfV6Ga99ETSOFVa9RsDUWxyRe89G5YZnwpRmejM51p8+ao2Dr+liJtoRw+pcbcbJRda8bo8qZ4fGItX+jYTac3TaOWx9iap1jRKT/ejdFgE9oPwZJN5EOTpD/TzdQtJjf6l85PZ/aaqwm0FAh/NUh5t3pepLm65EPEBd6eHLpqkct70TSL8k9a4B0TKJakqStN420pElcFGf3cTvo/8DTjn94FT13MW8jvck3kMuAtwHEhxJGVYx+TUt5zMY1ccBD51XmTE8R+/am/5pj8Dcd/0zX/5wFnPncHQGiwRWZKXnTNompozHtDAOQqbjSX6QgLS0FHU8YR3HEZFFtKlGuOFWTvT0ymbnSBKpm5yQJFOsIyPpPGezwkXlIhb3qorinTfweoZYNqzMPkm6qcON6N9JtojVWa7/Jg3FbDtBXaAzkUIcnV3HQEsuR1D0JIFvNOxqhLtfB0FDhwbBXRzgxhf5lsyUt/ixMrw1oJn1LFoxikLIFHOBohOdNLsurDuiJL3vAQdpWZzEfwKVVUIWn0FljjX+BksYOypdPmygJwONVJyFUh4i5hSpX5SoiKpXHoZB96SuXYjgojJzvIDnlIFn0UCx5iY5DoAHTJ/bND3LH223zq1E309y8wUYiyLjzP8XQ7E9OOrAKWQLgsQuESj40M4F1j8Nh0P8stAbyqQabkxa2ZjJRbOJbqYHKyCT2pMT0YpjeaYiCwxIB7gezZKEpHiX0L3di2QqXkwuUxObfQSHCfF3NHmZ9P9uF1GxTLLpSzflxFwaTXT2hSMtcZoNJmMt5UomlVnlLBzViumSZPAVegxjPjPZS6XATjOW4OPcsHzrweo0VFjgTwFgX25jx2WWfpZolV0pg+3oNytSB+j+CJnv4Vu0+BEjAopH0UX2MhTRt3xsW5ahNarIz/kJ9kWwC1qBA9A4WXFshfUXbKJ1RBajJC6iMxtpkLrPqXMuOf3kXfR/Yx/m+9Sb8OR8n7Yq74t5uS8kl+/bt3UVzQdObXzZuEEMP8F05nhBB5YPjf+X3/V9DIr4ysXuDU+/sfyy/3t1tK2XQhF4U8rXJ3120X9AH3jfztCyPtfWXe9DXg9K8svNwN3AZ8euXPn/zS8e8IIT6Ls7A6AOxfWVjNCyF24kyH3gp84Vfa2ge8BnjkNwWQFYb/M27Q7wohxMF6f//j+G/V3xdYguiFTGd+7bwJJ3h8TwjxdmAKeC2AlPKkEOJ7OAkrJvCHUsrnHHzezS+2eO9d+QInSH1LCDEKpIA3PL9vq06d/58iAeuFZcb7W4PIb5k3XftvXPNXwF/9muMHgfW/5niFlSBUp06d34QE+SILIi9g/j9JMy9w6v39j+W/T39fhNOZFyS/LvPuhUy9v/+x/Lfp7+9wd+Z3xYs2iNSp89+W+kikTp06z4t6EKlTp86/GynBsn77ef+J1INInTovNuojkTp16jwv6kGkTp06/35kfXemTp06zwMJsp5sVqdOnedFfSRSp06d50V9TaROnTr/bupbvHXq1Hm+yAsXav5PoR5E6tR5USHr05k6deo8D16ABXjKf3UH6tSpc5FI+8K+LgAhxEuEEMNCiNEV65eLpj4SqVPnRYQE5O9oJCKEUIEvAtfjuC8cEELcfbFevPWRSJ06Lyak/F2ORLYDo1LKcSllDfhn4JaL7VJ9JFKnzosM+bvb4j3v97TCDLDjYhupB5E6dV5E5Enf/5D8QeMFnu75LV68F+T39NuoB5E6dV5ESClf8jtsbgbo/KV/x4G5i22kviZSp85/Xw4AA0KIXiGEC8eq5e6LbaQ+EqlT578pK77X7wXuB1Tg61LKkxfbzgXZaNapU6fOv0V9OlOnTp3nRT2I1KlT53lRDyJ16tR5XtSDSJ06dZ4X9SBSp06d50U9iNSpU+d5UQ8iderUeV7Ug0idOnWeF/8vMajeijlkptwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9543192233775193\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.6554219477846388\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.988950276243094\n",
      "layer 5: 0.8344267955801105\n",
      "layer 6: 0.6953556629834255\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 3.439 | Reg loss: 0.016 | Tree loss: 3.439 | Accuracy: 0.058500 | 0.424 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 3.433 | Reg loss: 0.016 | Tree loss: 3.433 | Accuracy: 0.049000 | 0.355 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 3.426 | Reg loss: 0.016 | Tree loss: 3.426 | Accuracy: 0.059500 | 0.333 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 3.422 | Reg loss: 0.016 | Tree loss: 3.422 | Accuracy: 0.045500 | 0.32 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 3.418 | Reg loss: 0.016 | Tree loss: 3.418 | Accuracy: 0.063500 | 0.303 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 3.416 | Reg loss: 0.016 | Tree loss: 3.416 | Accuracy: 0.080500 | 0.298 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 3.407 | Reg loss: 0.016 | Tree loss: 3.407 | Accuracy: 0.107000 | 0.29 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 3.401 | Reg loss: 0.016 | Tree loss: 3.401 | Accuracy: 0.117500 | 0.281 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 3.394 | Reg loss: 0.016 | Tree loss: 3.394 | Accuracy: 0.112000 | 0.276 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 3.390 | Reg loss: 0.016 | Tree loss: 3.390 | Accuracy: 0.126500 | 0.27 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 3.378 | Reg loss: 0.016 | Tree loss: 3.378 | Accuracy: 0.139932 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 3.422 | Reg loss: 0.015 | Tree loss: 3.422 | Accuracy: 0.089500 | 0.279 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 3.415 | Reg loss: 0.015 | Tree loss: 3.415 | Accuracy: 0.112000 | 0.276 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 3.412 | Reg loss: 0.015 | Tree loss: 3.412 | Accuracy: 0.122500 | 0.273 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 3.404 | Reg loss: 0.015 | Tree loss: 3.404 | Accuracy: 0.147500 | 0.272 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 3.396 | Reg loss: 0.015 | Tree loss: 3.396 | Accuracy: 0.155000 | 0.27 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 3.389 | Reg loss: 0.015 | Tree loss: 3.389 | Accuracy: 0.141500 | 0.272 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 3.379 | Reg loss: 0.015 | Tree loss: 3.379 | Accuracy: 0.133500 | 0.275 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 3.373 | Reg loss: 0.016 | Tree loss: 3.373 | Accuracy: 0.132000 | 0.285 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 3.364 | Reg loss: 0.016 | Tree loss: 3.364 | Accuracy: 0.141500 | 0.286 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 3.361 | Reg loss: 0.016 | Tree loss: 3.361 | Accuracy: 0.128500 | 0.286 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 3.354 | Reg loss: 0.016 | Tree loss: 3.354 | Accuracy: 0.133106 | 0.285 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 3.407 | Reg loss: 0.015 | Tree loss: 3.407 | Accuracy: 0.137000 | 0.292 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 3.401 | Reg loss: 0.015 | Tree loss: 3.401 | Accuracy: 0.152000 | 0.291 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 3.391 | Reg loss: 0.015 | Tree loss: 3.391 | Accuracy: 0.196500 | 0.29 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 3.381 | Reg loss: 0.015 | Tree loss: 3.381 | Accuracy: 0.176500 | 0.289 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 3.374 | Reg loss: 0.015 | Tree loss: 3.374 | Accuracy: 0.151000 | 0.288 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 3.362 | Reg loss: 0.015 | Tree loss: 3.362 | Accuracy: 0.145500 | 0.287 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 3.355 | Reg loss: 0.015 | Tree loss: 3.355 | Accuracy: 0.126500 | 0.286 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 3.340 | Reg loss: 0.016 | Tree loss: 3.340 | Accuracy: 0.157000 | 0.286 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 3.335 | Reg loss: 0.016 | Tree loss: 3.335 | Accuracy: 0.146500 | 0.286 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 3.323 | Reg loss: 0.016 | Tree loss: 3.323 | Accuracy: 0.143500 | 0.285 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 3.309 | Reg loss: 0.016 | Tree loss: 3.309 | Accuracy: 0.156997 | 0.285 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 3.392 | Reg loss: 0.015 | Tree loss: 3.392 | Accuracy: 0.132000 | 0.29 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 3.385 | Reg loss: 0.015 | Tree loss: 3.385 | Accuracy: 0.227500 | 0.29 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 3.371 | Reg loss: 0.015 | Tree loss: 3.371 | Accuracy: 0.187500 | 0.289 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 3.358 | Reg loss: 0.015 | Tree loss: 3.358 | Accuracy: 0.175000 | 0.289 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 3.347 | Reg loss: 0.015 | Tree loss: 3.347 | Accuracy: 0.167500 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 3.337 | Reg loss: 0.015 | Tree loss: 3.337 | Accuracy: 0.163500 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 3.326 | Reg loss: 0.016 | Tree loss: 3.326 | Accuracy: 0.143000 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 3.305 | Reg loss: 0.016 | Tree loss: 3.305 | Accuracy: 0.152000 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 3.304 | Reg loss: 0.016 | Tree loss: 3.304 | Accuracy: 0.130500 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 3.293 | Reg loss: 0.017 | Tree loss: 3.293 | Accuracy: 0.130000 | 0.288 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 3.274 | Reg loss: 0.017 | Tree loss: 3.274 | Accuracy: 0.143345 | 0.288 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 3.379 | Reg loss: 0.015 | Tree loss: 3.379 | Accuracy: 0.142000 | 0.292 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 3.363 | Reg loss: 0.015 | Tree loss: 3.363 | Accuracy: 0.223500 | 0.292 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 3.351 | Reg loss: 0.015 | Tree loss: 3.351 | Accuracy: 0.200500 | 0.292 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 3.338 | Reg loss: 0.015 | Tree loss: 3.338 | Accuracy: 0.189000 | 0.291 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 3.316 | Reg loss: 0.016 | Tree loss: 3.316 | Accuracy: 0.185000 | 0.29 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 3.299 | Reg loss: 0.016 | Tree loss: 3.299 | Accuracy: 0.181000 | 0.29 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 3.286 | Reg loss: 0.016 | Tree loss: 3.286 | Accuracy: 0.167000 | 0.289 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 3.285 | Reg loss: 0.016 | Tree loss: 3.285 | Accuracy: 0.141000 | 0.29 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 3.271 | Reg loss: 0.017 | Tree loss: 3.271 | Accuracy: 0.135000 | 0.291 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 3.246 | Reg loss: 0.017 | Tree loss: 3.246 | Accuracy: 0.162500 | 0.291 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 3.233 | Reg loss: 0.017 | Tree loss: 3.233 | Accuracy: 0.170648 | 0.29 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 3.360 | Reg loss: 0.016 | Tree loss: 3.360 | Accuracy: 0.214500 | 0.292 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 3.347 | Reg loss: 0.016 | Tree loss: 3.347 | Accuracy: 0.250000 | 0.291 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 3.328 | Reg loss: 0.016 | Tree loss: 3.328 | Accuracy: 0.219000 | 0.29 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 3.304 | Reg loss: 0.016 | Tree loss: 3.304 | Accuracy: 0.211000 | 0.289 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 3.287 | Reg loss: 0.016 | Tree loss: 3.287 | Accuracy: 0.203000 | 0.288 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 3.270 | Reg loss: 0.016 | Tree loss: 3.270 | Accuracy: 0.186000 | 0.287 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 3.253 | Reg loss: 0.017 | Tree loss: 3.253 | Accuracy: 0.179500 | 0.286 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 3.234 | Reg loss: 0.017 | Tree loss: 3.234 | Accuracy: 0.169000 | 0.285 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 3.226 | Reg loss: 0.017 | Tree loss: 3.226 | Accuracy: 0.146500 | 0.284 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 3.211 | Reg loss: 0.018 | Tree loss: 3.211 | Accuracy: 0.157000 | 0.283 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 3.205 | Reg loss: 0.018 | Tree loss: 3.205 | Accuracy: 0.129693 | 0.282 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 3.344 | Reg loss: 0.016 | Tree loss: 3.344 | Accuracy: 0.267500 | 0.284 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 3.323 | Reg loss: 0.016 | Tree loss: 3.323 | Accuracy: 0.243500 | 0.283 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 3.300 | Reg loss: 0.016 | Tree loss: 3.300 | Accuracy: 0.224500 | 0.282 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 3.276 | Reg loss: 0.017 | Tree loss: 3.276 | Accuracy: 0.210500 | 0.282 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 3.249 | Reg loss: 0.017 | Tree loss: 3.249 | Accuracy: 0.200000 | 0.281 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 3.233 | Reg loss: 0.017 | Tree loss: 3.233 | Accuracy: 0.187500 | 0.28 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 3.212 | Reg loss: 0.017 | Tree loss: 3.212 | Accuracy: 0.181500 | 0.279 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 3.194 | Reg loss: 0.018 | Tree loss: 3.194 | Accuracy: 0.171000 | 0.279 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 3.176 | Reg loss: 0.018 | Tree loss: 3.176 | Accuracy: 0.188000 | 0.278 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 3.168 | Reg loss: 0.018 | Tree loss: 3.168 | Accuracy: 0.178000 | 0.277 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 3.163 | Reg loss: 0.019 | Tree loss: 3.163 | Accuracy: 0.184300 | 0.277 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 3.321 | Reg loss: 0.017 | Tree loss: 3.321 | Accuracy: 0.245000 | 0.278 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 3.298 | Reg loss: 0.017 | Tree loss: 3.298 | Accuracy: 0.249000 | 0.278 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 3.269 | Reg loss: 0.017 | Tree loss: 3.269 | Accuracy: 0.245000 | 0.278 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 3.242 | Reg loss: 0.017 | Tree loss: 3.242 | Accuracy: 0.214000 | 0.278 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 3.214 | Reg loss: 0.017 | Tree loss: 3.214 | Accuracy: 0.202500 | 0.277 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 3.182 | Reg loss: 0.018 | Tree loss: 3.182 | Accuracy: 0.213000 | 0.277 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 3.169 | Reg loss: 0.018 | Tree loss: 3.169 | Accuracy: 0.197500 | 0.277 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 3.146 | Reg loss: 0.018 | Tree loss: 3.146 | Accuracy: 0.202500 | 0.277 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 3.137 | Reg loss: 0.019 | Tree loss: 3.137 | Accuracy: 0.204000 | 0.277 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 3.121 | Reg loss: 0.019 | Tree loss: 3.121 | Accuracy: 0.188500 | 0.276 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 3.116 | Reg loss: 0.019 | Tree loss: 3.116 | Accuracy: 0.191126 | 0.276 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 3.294 | Reg loss: 0.018 | Tree loss: 3.294 | Accuracy: 0.259000 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 3.270 | Reg loss: 0.018 | Tree loss: 3.270 | Accuracy: 0.253000 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 3.238 | Reg loss: 0.018 | Tree loss: 3.238 | Accuracy: 0.236500 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 3.203 | Reg loss: 0.018 | Tree loss: 3.203 | Accuracy: 0.230500 | 0.279 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 3.166 | Reg loss: 0.018 | Tree loss: 3.166 | Accuracy: 0.253500 | 0.279 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 3.144 | Reg loss: 0.018 | Tree loss: 3.144 | Accuracy: 0.228000 | 0.279 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 3.124 | Reg loss: 0.019 | Tree loss: 3.124 | Accuracy: 0.222000 | 0.279 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 3.103 | Reg loss: 0.019 | Tree loss: 3.103 | Accuracy: 0.222500 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 3.085 | Reg loss: 0.019 | Tree loss: 3.085 | Accuracy: 0.213000 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 3.075 | Reg loss: 0.020 | Tree loss: 3.075 | Accuracy: 0.222000 | 0.278 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 3.063 | Reg loss: 0.020 | Tree loss: 3.063 | Accuracy: 0.197952 | 0.278 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 3.267 | Reg loss: 0.018 | Tree loss: 3.267 | Accuracy: 0.268500 | 0.279 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 3.234 | Reg loss: 0.018 | Tree loss: 3.234 | Accuracy: 0.263500 | 0.279 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 3.193 | Reg loss: 0.019 | Tree loss: 3.193 | Accuracy: 0.253500 | 0.278 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 3.160 | Reg loss: 0.019 | Tree loss: 3.160 | Accuracy: 0.243500 | 0.278 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 3.125 | Reg loss: 0.019 | Tree loss: 3.125 | Accuracy: 0.239000 | 0.278 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 3.102 | Reg loss: 0.019 | Tree loss: 3.102 | Accuracy: 0.224000 | 0.278 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 3.074 | Reg loss: 0.019 | Tree loss: 3.074 | Accuracy: 0.222500 | 0.277 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 3.059 | Reg loss: 0.020 | Tree loss: 3.059 | Accuracy: 0.220500 | 0.277 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 3.051 | Reg loss: 0.020 | Tree loss: 3.051 | Accuracy: 0.212000 | 0.277 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 3.021 | Reg loss: 0.020 | Tree loss: 3.021 | Accuracy: 0.223000 | 0.277 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 2.989 | Reg loss: 0.021 | Tree loss: 2.989 | Accuracy: 0.208191 | 0.276 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 3.230 | Reg loss: 0.019 | Tree loss: 3.230 | Accuracy: 0.265500 | 0.278 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 3.198 | Reg loss: 0.019 | Tree loss: 3.198 | Accuracy: 0.286500 | 0.277 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 3.159 | Reg loss: 0.019 | Tree loss: 3.159 | Accuracy: 0.269500 | 0.277 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 3.123 | Reg loss: 0.019 | Tree loss: 3.123 | Accuracy: 0.261000 | 0.277 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 3.085 | Reg loss: 0.020 | Tree loss: 3.085 | Accuracy: 0.244500 | 0.277 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 3.051 | Reg loss: 0.020 | Tree loss: 3.051 | Accuracy: 0.250000 | 0.276 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 3.019 | Reg loss: 0.020 | Tree loss: 3.019 | Accuracy: 0.246500 | 0.276 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 3.001 | Reg loss: 0.020 | Tree loss: 3.001 | Accuracy: 0.237000 | 0.276 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 2.988 | Reg loss: 0.021 | Tree loss: 2.988 | Accuracy: 0.234000 | 0.275 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 2.966 | Reg loss: 0.021 | Tree loss: 2.966 | Accuracy: 0.237000 | 0.275 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 2.961 | Reg loss: 0.021 | Tree loss: 2.961 | Accuracy: 0.218430 | 0.275 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 3.194 | Reg loss: 0.020 | Tree loss: 3.194 | Accuracy: 0.270500 | 0.276 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 3.159 | Reg loss: 0.020 | Tree loss: 3.159 | Accuracy: 0.280500 | 0.275 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 3.116 | Reg loss: 0.020 | Tree loss: 3.116 | Accuracy: 0.259000 | 0.275 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 3.066 | Reg loss: 0.020 | Tree loss: 3.066 | Accuracy: 0.253500 | 0.275 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 3.044 | Reg loss: 0.020 | Tree loss: 3.044 | Accuracy: 0.224500 | 0.274 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 2.994 | Reg loss: 0.021 | Tree loss: 2.994 | Accuracy: 0.217000 | 0.274 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 2.960 | Reg loss: 0.021 | Tree loss: 2.960 | Accuracy: 0.232000 | 0.274 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 2.961 | Reg loss: 0.021 | Tree loss: 2.961 | Accuracy: 0.218000 | 0.273 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 2.930 | Reg loss: 0.021 | Tree loss: 2.930 | Accuracy: 0.237500 | 0.273 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 2.920 | Reg loss: 0.022 | Tree loss: 2.920 | Accuracy: 0.226500 | 0.273 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 2.898 | Reg loss: 0.022 | Tree loss: 2.898 | Accuracy: 0.235495 | 0.273 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 3.158 | Reg loss: 0.021 | Tree loss: 3.158 | Accuracy: 0.266500 | 0.274 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 3.115 | Reg loss: 0.021 | Tree loss: 3.115 | Accuracy: 0.271500 | 0.274 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 3.070 | Reg loss: 0.021 | Tree loss: 3.070 | Accuracy: 0.256500 | 0.274 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 3.034 | Reg loss: 0.021 | Tree loss: 3.034 | Accuracy: 0.238000 | 0.274 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 2.980 | Reg loss: 0.021 | Tree loss: 2.980 | Accuracy: 0.226000 | 0.274 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 2.932 | Reg loss: 0.021 | Tree loss: 2.932 | Accuracy: 0.243500 | 0.275 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 2.917 | Reg loss: 0.021 | Tree loss: 2.917 | Accuracy: 0.217500 | 0.275 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 2.897 | Reg loss: 0.022 | Tree loss: 2.897 | Accuracy: 0.231000 | 0.275 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 2.875 | Reg loss: 0.022 | Tree loss: 2.875 | Accuracy: 0.239500 | 0.275 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 2.868 | Reg loss: 0.022 | Tree loss: 2.868 | Accuracy: 0.231500 | 0.275 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 2.853 | Reg loss: 0.022 | Tree loss: 2.853 | Accuracy: 0.204778 | 0.275 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 3.126 | Reg loss: 0.021 | Tree loss: 3.126 | Accuracy: 0.249500 | 0.276 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 3.072 | Reg loss: 0.021 | Tree loss: 3.072 | Accuracy: 0.254500 | 0.275 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 3.024 | Reg loss: 0.021 | Tree loss: 3.024 | Accuracy: 0.253000 | 0.275 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 2.983 | Reg loss: 0.022 | Tree loss: 2.983 | Accuracy: 0.253000 | 0.275 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 2.921 | Reg loss: 0.022 | Tree loss: 2.921 | Accuracy: 0.262500 | 0.275 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 2.880 | Reg loss: 0.022 | Tree loss: 2.880 | Accuracy: 0.240000 | 0.274 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 2.868 | Reg loss: 0.022 | Tree loss: 2.868 | Accuracy: 0.220500 | 0.274 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 2.828 | Reg loss: 0.022 | Tree loss: 2.828 | Accuracy: 0.247000 | 0.274 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 2.829 | Reg loss: 0.023 | Tree loss: 2.829 | Accuracy: 0.240500 | 0.274 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 2.804 | Reg loss: 0.023 | Tree loss: 2.804 | Accuracy: 0.237000 | 0.273 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 2.788 | Reg loss: 0.023 | Tree loss: 2.788 | Accuracy: 0.262799 | 0.273 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 3.071 | Reg loss: 0.022 | Tree loss: 3.071 | Accuracy: 0.265000 | 0.274 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 3.025 | Reg loss: 0.022 | Tree loss: 3.025 | Accuracy: 0.254000 | 0.274 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 2.974 | Reg loss: 0.022 | Tree loss: 2.974 | Accuracy: 0.256000 | 0.273 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 2.929 | Reg loss: 0.022 | Tree loss: 2.929 | Accuracy: 0.247000 | 0.273 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 2.878 | Reg loss: 0.022 | Tree loss: 2.878 | Accuracy: 0.251000 | 0.273 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 2.846 | Reg loss: 0.022 | Tree loss: 2.846 | Accuracy: 0.251500 | 0.272 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 2.803 | Reg loss: 0.023 | Tree loss: 2.803 | Accuracy: 0.267000 | 0.272 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 2.777 | Reg loss: 0.023 | Tree loss: 2.777 | Accuracy: 0.269500 | 0.272 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 2.769 | Reg loss: 0.023 | Tree loss: 2.769 | Accuracy: 0.273500 | 0.272 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 2.749 | Reg loss: 0.023 | Tree loss: 2.749 | Accuracy: 0.288000 | 0.271 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 2.711 | Reg loss: 0.023 | Tree loss: 2.711 | Accuracy: 0.293515 | 0.271 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 3.035 | Reg loss: 0.023 | Tree loss: 3.035 | Accuracy: 0.240000 | 0.272 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 2.995 | Reg loss: 0.023 | Tree loss: 2.995 | Accuracy: 0.247500 | 0.272 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 2.933 | Reg loss: 0.023 | Tree loss: 2.933 | Accuracy: 0.251500 | 0.271 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 2.869 | Reg loss: 0.023 | Tree loss: 2.869 | Accuracy: 0.286000 | 0.271 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 2.807 | Reg loss: 0.023 | Tree loss: 2.807 | Accuracy: 0.295500 | 0.271 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 2.774 | Reg loss: 0.023 | Tree loss: 2.774 | Accuracy: 0.294500 | 0.271 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 2.749 | Reg loss: 0.023 | Tree loss: 2.749 | Accuracy: 0.289000 | 0.271 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 2.721 | Reg loss: 0.023 | Tree loss: 2.721 | Accuracy: 0.310500 | 0.27 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 2.716 | Reg loss: 0.024 | Tree loss: 2.716 | Accuracy: 0.286500 | 0.27 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 2.695 | Reg loss: 0.024 | Tree loss: 2.695 | Accuracy: 0.303000 | 0.27 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 2.709 | Reg loss: 0.024 | Tree loss: 2.709 | Accuracy: 0.249147 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 2.979 | Reg loss: 0.023 | Tree loss: 2.979 | Accuracy: 0.263000 | 0.271 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 2.939 | Reg loss: 0.023 | Tree loss: 2.939 | Accuracy: 0.266000 | 0.271 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 2.872 | Reg loss: 0.023 | Tree loss: 2.872 | Accuracy: 0.257000 | 0.27 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 2.834 | Reg loss: 0.023 | Tree loss: 2.834 | Accuracy: 0.273500 | 0.27 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 2.758 | Reg loss: 0.023 | Tree loss: 2.758 | Accuracy: 0.286500 | 0.27 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 2.719 | Reg loss: 0.023 | Tree loss: 2.719 | Accuracy: 0.299500 | 0.27 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 2.700 | Reg loss: 0.024 | Tree loss: 2.700 | Accuracy: 0.283000 | 0.269 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 2.698 | Reg loss: 0.024 | Tree loss: 2.698 | Accuracy: 0.268500 | 0.269 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 2.675 | Reg loss: 0.024 | Tree loss: 2.675 | Accuracy: 0.263500 | 0.269 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 2.653 | Reg loss: 0.024 | Tree loss: 2.653 | Accuracy: 0.276000 | 0.269 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 2.630 | Reg loss: 0.024 | Tree loss: 2.630 | Accuracy: 0.310580 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 2.939 | Reg loss: 0.023 | Tree loss: 2.939 | Accuracy: 0.264500 | 0.269 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 2.878 | Reg loss: 0.023 | Tree loss: 2.878 | Accuracy: 0.265000 | 0.269 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 2.826 | Reg loss: 0.024 | Tree loss: 2.826 | Accuracy: 0.275500 | 0.269 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 2.762 | Reg loss: 0.024 | Tree loss: 2.762 | Accuracy: 0.275000 | 0.269 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 2.724 | Reg loss: 0.024 | Tree loss: 2.724 | Accuracy: 0.303500 | 0.269 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 2.690 | Reg loss: 0.024 | Tree loss: 2.690 | Accuracy: 0.273500 | 0.268 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 2.668 | Reg loss: 0.024 | Tree loss: 2.668 | Accuracy: 0.293000 | 0.268 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 2.635 | Reg loss: 0.024 | Tree loss: 2.635 | Accuracy: 0.297500 | 0.268 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 2.635 | Reg loss: 0.024 | Tree loss: 2.635 | Accuracy: 0.275000 | 0.268 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 2.608 | Reg loss: 0.024 | Tree loss: 2.608 | Accuracy: 0.268000 | 0.268 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 2.577 | Reg loss: 0.025 | Tree loss: 2.577 | Accuracy: 0.307167 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 2.898 | Reg loss: 0.024 | Tree loss: 2.898 | Accuracy: 0.268500 | 0.268 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 2.833 | Reg loss: 0.024 | Tree loss: 2.833 | Accuracy: 0.274000 | 0.268 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 2.781 | Reg loss: 0.024 | Tree loss: 2.781 | Accuracy: 0.273000 | 0.269 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 2.745 | Reg loss: 0.024 | Tree loss: 2.745 | Accuracy: 0.242500 | 0.269 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 2.673 | Reg loss: 0.024 | Tree loss: 2.673 | Accuracy: 0.307000 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 2.648 | Reg loss: 0.024 | Tree loss: 2.648 | Accuracy: 0.270000 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 2.619 | Reg loss: 0.024 | Tree loss: 2.619 | Accuracy: 0.279500 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 2.590 | Reg loss: 0.024 | Tree loss: 2.590 | Accuracy: 0.295500 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 2.588 | Reg loss: 0.025 | Tree loss: 2.588 | Accuracy: 0.290000 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 2.558 | Reg loss: 0.025 | Tree loss: 2.558 | Accuracy: 0.298000 | 0.27 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 2.618 | Reg loss: 0.025 | Tree loss: 2.618 | Accuracy: 0.211604 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 2.851 | Reg loss: 0.024 | Tree loss: 2.851 | Accuracy: 0.272000 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 2.797 | Reg loss: 0.024 | Tree loss: 2.797 | Accuracy: 0.270000 | 0.27 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 2.733 | Reg loss: 0.024 | Tree loss: 2.733 | Accuracy: 0.278500 | 0.27 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 2.677 | Reg loss: 0.024 | Tree loss: 2.677 | Accuracy: 0.284500 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 2.644 | Reg loss: 0.024 | Tree loss: 2.644 | Accuracy: 0.286500 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 2.620 | Reg loss: 0.024 | Tree loss: 2.620 | Accuracy: 0.277000 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 2.563 | Reg loss: 0.025 | Tree loss: 2.563 | Accuracy: 0.291000 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 2.573 | Reg loss: 0.025 | Tree loss: 2.573 | Accuracy: 0.295000 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 2.553 | Reg loss: 0.025 | Tree loss: 2.553 | Accuracy: 0.296000 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 2.528 | Reg loss: 0.025 | Tree loss: 2.528 | Accuracy: 0.279500 | 0.271 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 2.502 | Reg loss: 0.025 | Tree loss: 2.502 | Accuracy: 0.273038 | 0.271 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 2.810 | Reg loss: 0.024 | Tree loss: 2.810 | Accuracy: 0.285500 | 0.272 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 2.764 | Reg loss: 0.025 | Tree loss: 2.764 | Accuracy: 0.277500 | 0.272 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 2.699 | Reg loss: 0.025 | Tree loss: 2.699 | Accuracy: 0.273000 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 2.643 | Reg loss: 0.025 | Tree loss: 2.643 | Accuracy: 0.286500 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 2.616 | Reg loss: 0.025 | Tree loss: 2.616 | Accuracy: 0.285500 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 2.539 | Reg loss: 0.025 | Tree loss: 2.539 | Accuracy: 0.308000 | 0.271 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 2.531 | Reg loss: 0.025 | Tree loss: 2.531 | Accuracy: 0.292000 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 2.522 | Reg loss: 0.025 | Tree loss: 2.522 | Accuracy: 0.281000 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 2.516 | Reg loss: 0.025 | Tree loss: 2.516 | Accuracy: 0.286000 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 2.518 | Reg loss: 0.025 | Tree loss: 2.518 | Accuracy: 0.276000 | 0.271 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 2.466 | Reg loss: 0.025 | Tree loss: 2.466 | Accuracy: 0.273038 | 0.271 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 2.767 | Reg loss: 0.025 | Tree loss: 2.767 | Accuracy: 0.273500 | 0.272 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 2.714 | Reg loss: 0.025 | Tree loss: 2.714 | Accuracy: 0.271000 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 2.659 | Reg loss: 0.025 | Tree loss: 2.659 | Accuracy: 0.306500 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 2.591 | Reg loss: 0.025 | Tree loss: 2.591 | Accuracy: 0.308500 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 2.570 | Reg loss: 0.025 | Tree loss: 2.570 | Accuracy: 0.305000 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 2.529 | Reg loss: 0.025 | Tree loss: 2.529 | Accuracy: 0.283500 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 2.514 | Reg loss: 0.025 | Tree loss: 2.514 | Accuracy: 0.289000 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 2.497 | Reg loss: 0.025 | Tree loss: 2.497 | Accuracy: 0.288500 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 2.479 | Reg loss: 0.025 | Tree loss: 2.479 | Accuracy: 0.278000 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 2.479 | Reg loss: 0.025 | Tree loss: 2.479 | Accuracy: 0.276500 | 0.271 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 2.442 | Reg loss: 0.025 | Tree loss: 2.442 | Accuracy: 0.259386 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 2.729 | Reg loss: 0.025 | Tree loss: 2.729 | Accuracy: 0.276500 | 0.271 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 2.685 | Reg loss: 0.025 | Tree loss: 2.685 | Accuracy: 0.264500 | 0.271 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 2.632 | Reg loss: 0.025 | Tree loss: 2.632 | Accuracy: 0.291000 | 0.271 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 2.569 | Reg loss: 0.025 | Tree loss: 2.569 | Accuracy: 0.295000 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 2.541 | Reg loss: 0.025 | Tree loss: 2.541 | Accuracy: 0.298500 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 2.484 | Reg loss: 0.025 | Tree loss: 2.484 | Accuracy: 0.316500 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 2.482 | Reg loss: 0.025 | Tree loss: 2.482 | Accuracy: 0.301500 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 2.459 | Reg loss: 0.025 | Tree loss: 2.459 | Accuracy: 0.286000 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 2.452 | Reg loss: 0.025 | Tree loss: 2.452 | Accuracy: 0.279000 | 0.27 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 2.436 | Reg loss: 0.026 | Tree loss: 2.436 | Accuracy: 0.282500 | 0.269 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 2.438 | Reg loss: 0.026 | Tree loss: 2.438 | Accuracy: 0.242321 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 2.702 | Reg loss: 0.025 | Tree loss: 2.702 | Accuracy: 0.272000 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 2.638 | Reg loss: 0.025 | Tree loss: 2.638 | Accuracy: 0.280500 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 2.585 | Reg loss: 0.025 | Tree loss: 2.585 | Accuracy: 0.312000 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 2.528 | Reg loss: 0.025 | Tree loss: 2.528 | Accuracy: 0.301500 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 2.502 | Reg loss: 0.025 | Tree loss: 2.502 | Accuracy: 0.301000 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 2.472 | Reg loss: 0.025 | Tree loss: 2.472 | Accuracy: 0.306500 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 2.452 | Reg loss: 0.025 | Tree loss: 2.452 | Accuracy: 0.297500 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 2.438 | Reg loss: 0.026 | Tree loss: 2.438 | Accuracy: 0.271500 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 2.407 | Reg loss: 0.026 | Tree loss: 2.407 | Accuracy: 0.289000 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 2.398 | Reg loss: 0.026 | Tree loss: 2.398 | Accuracy: 0.282000 | 0.27 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 2.384 | Reg loss: 0.026 | Tree loss: 2.384 | Accuracy: 0.279863 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 2.643 | Reg loss: 0.025 | Tree loss: 2.643 | Accuracy: 0.300500 | 0.271 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 2.607 | Reg loss: 0.025 | Tree loss: 2.607 | Accuracy: 0.286000 | 0.271 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 2.559 | Reg loss: 0.025 | Tree loss: 2.559 | Accuracy: 0.297000 | 0.271 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 2.501 | Reg loss: 0.025 | Tree loss: 2.501 | Accuracy: 0.305000 | 0.271 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 2.455 | Reg loss: 0.026 | Tree loss: 2.455 | Accuracy: 0.317500 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 2.455 | Reg loss: 0.026 | Tree loss: 2.455 | Accuracy: 0.302000 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 2.416 | Reg loss: 0.026 | Tree loss: 2.416 | Accuracy: 0.284000 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 2.409 | Reg loss: 0.026 | Tree loss: 2.409 | Accuracy: 0.282000 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 2.394 | Reg loss: 0.026 | Tree loss: 2.394 | Accuracy: 0.271500 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 2.382 | Reg loss: 0.026 | Tree loss: 2.382 | Accuracy: 0.294000 | 0.27 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 2.383 | Reg loss: 0.026 | Tree loss: 2.383 | Accuracy: 0.228669 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 2.627 | Reg loss: 0.026 | Tree loss: 2.627 | Accuracy: 0.288000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 2.563 | Reg loss: 0.026 | Tree loss: 2.563 | Accuracy: 0.295000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 2.525 | Reg loss: 0.026 | Tree loss: 2.525 | Accuracy: 0.274500 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 2.482 | Reg loss: 0.026 | Tree loss: 2.482 | Accuracy: 0.294000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 2.450 | Reg loss: 0.026 | Tree loss: 2.450 | Accuracy: 0.302000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 2.429 | Reg loss: 0.026 | Tree loss: 2.429 | Accuracy: 0.302000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 2.384 | Reg loss: 0.026 | Tree loss: 2.384 | Accuracy: 0.310500 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 2.365 | Reg loss: 0.026 | Tree loss: 2.365 | Accuracy: 0.304000 | 0.27 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 2.357 | Reg loss: 0.026 | Tree loss: 2.357 | Accuracy: 0.273500 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 2.364 | Reg loss: 0.026 | Tree loss: 2.364 | Accuracy: 0.270000 | 0.27 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 2.357 | Reg loss: 0.026 | Tree loss: 2.357 | Accuracy: 0.313993 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 2.601 | Reg loss: 0.026 | Tree loss: 2.601 | Accuracy: 0.288000 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 2.539 | Reg loss: 0.026 | Tree loss: 2.539 | Accuracy: 0.279000 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 2.479 | Reg loss: 0.026 | Tree loss: 2.479 | Accuracy: 0.302500 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 2.444 | Reg loss: 0.026 | Tree loss: 2.444 | Accuracy: 0.300500 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 2.417 | Reg loss: 0.026 | Tree loss: 2.417 | Accuracy: 0.292000 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 2.380 | Reg loss: 0.026 | Tree loss: 2.380 | Accuracy: 0.320500 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 2.339 | Reg loss: 0.026 | Tree loss: 2.339 | Accuracy: 0.316500 | 0.27 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 2.363 | Reg loss: 0.026 | Tree loss: 2.363 | Accuracy: 0.290000 | 0.269 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 2.348 | Reg loss: 0.026 | Tree loss: 2.348 | Accuracy: 0.283500 | 0.269 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 2.321 | Reg loss: 0.026 | Tree loss: 2.321 | Accuracy: 0.287000 | 0.269 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 2.343 | Reg loss: 0.026 | Tree loss: 2.343 | Accuracy: 0.262799 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 2.551 | Reg loss: 0.026 | Tree loss: 2.551 | Accuracy: 0.299000 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 2.511 | Reg loss: 0.026 | Tree loss: 2.511 | Accuracy: 0.291500 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 2.470 | Reg loss: 0.026 | Tree loss: 2.470 | Accuracy: 0.321500 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 2.434 | Reg loss: 0.026 | Tree loss: 2.434 | Accuracy: 0.298000 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 2.385 | Reg loss: 0.026 | Tree loss: 2.385 | Accuracy: 0.304500 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 2.363 | Reg loss: 0.026 | Tree loss: 2.363 | Accuracy: 0.319000 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 2.328 | Reg loss: 0.026 | Tree loss: 2.328 | Accuracy: 0.320000 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 2.318 | Reg loss: 0.026 | Tree loss: 2.318 | Accuracy: 0.312000 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 2.326 | Reg loss: 0.026 | Tree loss: 2.326 | Accuracy: 0.271500 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 2.305 | Reg loss: 0.026 | Tree loss: 2.305 | Accuracy: 0.266500 | 0.27 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 2.239 | Reg loss: 0.026 | Tree loss: 2.239 | Accuracy: 0.293515 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 2.532 | Reg loss: 0.026 | Tree loss: 2.532 | Accuracy: 0.282500 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 2.482 | Reg loss: 0.026 | Tree loss: 2.482 | Accuracy: 0.298500 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 2.425 | Reg loss: 0.026 | Tree loss: 2.425 | Accuracy: 0.315000 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 2.396 | Reg loss: 0.026 | Tree loss: 2.396 | Accuracy: 0.307500 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 2.366 | Reg loss: 0.026 | Tree loss: 2.366 | Accuracy: 0.290000 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 2.336 | Reg loss: 0.026 | Tree loss: 2.336 | Accuracy: 0.313000 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 2.321 | Reg loss: 0.026 | Tree loss: 2.321 | Accuracy: 0.318000 | 0.27 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 2.290 | Reg loss: 0.026 | Tree loss: 2.290 | Accuracy: 0.297500 | 0.269 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 2.288 | Reg loss: 0.026 | Tree loss: 2.288 | Accuracy: 0.285500 | 0.269 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 2.313 | Reg loss: 0.026 | Tree loss: 2.313 | Accuracy: 0.272500 | 0.269 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 2.294 | Reg loss: 0.027 | Tree loss: 2.294 | Accuracy: 0.276451 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 2.519 | Reg loss: 0.026 | Tree loss: 2.519 | Accuracy: 0.284500 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 2.447 | Reg loss: 0.026 | Tree loss: 2.447 | Accuracy: 0.308000 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 2.420 | Reg loss: 0.026 | Tree loss: 2.420 | Accuracy: 0.312500 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 2.382 | Reg loss: 0.026 | Tree loss: 2.382 | Accuracy: 0.320000 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 2.334 | Reg loss: 0.026 | Tree loss: 2.334 | Accuracy: 0.310500 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 2.310 | Reg loss: 0.026 | Tree loss: 2.310 | Accuracy: 0.323500 | 0.269 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 2.298 | Reg loss: 0.026 | Tree loss: 2.298 | Accuracy: 0.317500 | 0.269 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 2.267 | Reg loss: 0.027 | Tree loss: 2.267 | Accuracy: 0.296500 | 0.269 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 2.263 | Reg loss: 0.027 | Tree loss: 2.263 | Accuracy: 0.280000 | 0.269 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 2.272 | Reg loss: 0.027 | Tree loss: 2.272 | Accuracy: 0.278500 | 0.27 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 2.309 | Reg loss: 0.027 | Tree loss: 2.309 | Accuracy: 0.245734 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 2.457 | Reg loss: 0.026 | Tree loss: 2.457 | Accuracy: 0.333000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 2.441 | Reg loss: 0.026 | Tree loss: 2.441 | Accuracy: 0.307500 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 2.408 | Reg loss: 0.026 | Tree loss: 2.408 | Accuracy: 0.294000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 2.347 | Reg loss: 0.026 | Tree loss: 2.347 | Accuracy: 0.326500 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 2.314 | Reg loss: 0.027 | Tree loss: 2.314 | Accuracy: 0.322000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 2.289 | Reg loss: 0.027 | Tree loss: 2.289 | Accuracy: 0.337000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 2.268 | Reg loss: 0.027 | Tree loss: 2.268 | Accuracy: 0.317000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 2.249 | Reg loss: 0.027 | Tree loss: 2.249 | Accuracy: 0.296500 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 2.263 | Reg loss: 0.027 | Tree loss: 2.263 | Accuracy: 0.270000 | 0.27 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 2.239 | Reg loss: 0.027 | Tree loss: 2.239 | Accuracy: 0.285000 | 0.27 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 2.261 | Reg loss: 0.027 | Tree loss: 2.261 | Accuracy: 0.249147 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 2.443 | Reg loss: 0.027 | Tree loss: 2.443 | Accuracy: 0.313500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 2.423 | Reg loss: 0.027 | Tree loss: 2.423 | Accuracy: 0.309000 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 2.361 | Reg loss: 0.027 | Tree loss: 2.361 | Accuracy: 0.326000 | 0.27 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 2.333 | Reg loss: 0.027 | Tree loss: 2.333 | Accuracy: 0.318500 | 0.27 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 2.294 | Reg loss: 0.027 | Tree loss: 2.294 | Accuracy: 0.324500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 2.258 | Reg loss: 0.027 | Tree loss: 2.258 | Accuracy: 0.327500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 2.255 | Reg loss: 0.027 | Tree loss: 2.255 | Accuracy: 0.312500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 2.244 | Reg loss: 0.027 | Tree loss: 2.244 | Accuracy: 0.284500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 2.247 | Reg loss: 0.027 | Tree loss: 2.247 | Accuracy: 0.277500 | 0.271 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 2.231 | Reg loss: 0.027 | Tree loss: 2.231 | Accuracy: 0.274500 | 0.27 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 2.225 | Reg loss: 0.027 | Tree loss: 2.225 | Accuracy: 0.255973 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 2.422 | Reg loss: 0.027 | Tree loss: 2.422 | Accuracy: 0.313500 | 0.271 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 2.398 | Reg loss: 0.027 | Tree loss: 2.398 | Accuracy: 0.317000 | 0.271 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 2.365 | Reg loss: 0.027 | Tree loss: 2.365 | Accuracy: 0.314000 | 0.271 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 2.314 | Reg loss: 0.027 | Tree loss: 2.314 | Accuracy: 0.313500 | 0.271 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 2.275 | Reg loss: 0.027 | Tree loss: 2.275 | Accuracy: 0.325500 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 2.259 | Reg loss: 0.027 | Tree loss: 2.259 | Accuracy: 0.313500 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 2.235 | Reg loss: 0.027 | Tree loss: 2.235 | Accuracy: 0.304000 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 2.210 | Reg loss: 0.027 | Tree loss: 2.210 | Accuracy: 0.308000 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 2.188 | Reg loss: 0.027 | Tree loss: 2.188 | Accuracy: 0.296000 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 2.217 | Reg loss: 0.027 | Tree loss: 2.217 | Accuracy: 0.282000 | 0.27 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 2.170 | Reg loss: 0.027 | Tree loss: 2.170 | Accuracy: 0.313993 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 2.418 | Reg loss: 0.027 | Tree loss: 2.418 | Accuracy: 0.315500 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 2.353 | Reg loss: 0.027 | Tree loss: 2.353 | Accuracy: 0.332500 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 2.352 | Reg loss: 0.027 | Tree loss: 2.352 | Accuracy: 0.308500 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 2.266 | Reg loss: 0.027 | Tree loss: 2.266 | Accuracy: 0.332000 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 2.257 | Reg loss: 0.027 | Tree loss: 2.257 | Accuracy: 0.311500 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 2.228 | Reg loss: 0.027 | Tree loss: 2.228 | Accuracy: 0.323000 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 2.201 | Reg loss: 0.027 | Tree loss: 2.201 | Accuracy: 0.306000 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 2.216 | Reg loss: 0.027 | Tree loss: 2.216 | Accuracy: 0.280500 | 0.27 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 2.204 | Reg loss: 0.027 | Tree loss: 2.204 | Accuracy: 0.278000 | 0.269 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 2.199 | Reg loss: 0.027 | Tree loss: 2.199 | Accuracy: 0.278000 | 0.269 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 2.172 | Reg loss: 0.027 | Tree loss: 2.172 | Accuracy: 0.313993 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 2.410 | Reg loss: 0.027 | Tree loss: 2.410 | Accuracy: 0.305500 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 2.349 | Reg loss: 0.027 | Tree loss: 2.349 | Accuracy: 0.331000 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 2.315 | Reg loss: 0.027 | Tree loss: 2.315 | Accuracy: 0.323500 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 2.272 | Reg loss: 0.027 | Tree loss: 2.272 | Accuracy: 0.314000 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 2.239 | Reg loss: 0.027 | Tree loss: 2.239 | Accuracy: 0.310500 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 2.214 | Reg loss: 0.027 | Tree loss: 2.214 | Accuracy: 0.322500 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 2.194 | Reg loss: 0.027 | Tree loss: 2.194 | Accuracy: 0.312000 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 2.195 | Reg loss: 0.027 | Tree loss: 2.195 | Accuracy: 0.310000 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 2.183 | Reg loss: 0.027 | Tree loss: 2.183 | Accuracy: 0.289500 | 0.27 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 2.134 | Reg loss: 0.027 | Tree loss: 2.134 | Accuracy: 0.309000 | 0.269 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 2.172 | Reg loss: 0.027 | Tree loss: 2.172 | Accuracy: 0.276451 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 2.368 | Reg loss: 0.027 | Tree loss: 2.368 | Accuracy: 0.334000 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 2.326 | Reg loss: 0.027 | Tree loss: 2.326 | Accuracy: 0.329000 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 2.289 | Reg loss: 0.027 | Tree loss: 2.289 | Accuracy: 0.314500 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 2.263 | Reg loss: 0.027 | Tree loss: 2.263 | Accuracy: 0.329500 | 0.269 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 2.211 | Reg loss: 0.027 | Tree loss: 2.211 | Accuracy: 0.322500 | 0.269 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 2.218 | Reg loss: 0.027 | Tree loss: 2.218 | Accuracy: 0.294500 | 0.269 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 2.189 | Reg loss: 0.027 | Tree loss: 2.189 | Accuracy: 0.307500 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 2.170 | Reg loss: 0.027 | Tree loss: 2.170 | Accuracy: 0.313000 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 2.168 | Reg loss: 0.027 | Tree loss: 2.168 | Accuracy: 0.293500 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 2.146 | Reg loss: 0.027 | Tree loss: 2.146 | Accuracy: 0.276000 | 0.27 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 2.142 | Reg loss: 0.027 | Tree loss: 2.142 | Accuracy: 0.293515 | 0.27 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 2.342 | Reg loss: 0.027 | Tree loss: 2.342 | Accuracy: 0.338500 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 2.311 | Reg loss: 0.027 | Tree loss: 2.311 | Accuracy: 0.332000 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 2.295 | Reg loss: 0.027 | Tree loss: 2.295 | Accuracy: 0.317000 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 2.232 | Reg loss: 0.027 | Tree loss: 2.232 | Accuracy: 0.336000 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 2.211 | Reg loss: 0.027 | Tree loss: 2.211 | Accuracy: 0.325000 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 2.190 | Reg loss: 0.027 | Tree loss: 2.190 | Accuracy: 0.311000 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 2.171 | Reg loss: 0.027 | Tree loss: 2.171 | Accuracy: 0.315500 | 0.27 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 2.154 | Reg loss: 0.027 | Tree loss: 2.154 | Accuracy: 0.302000 | 0.269 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 2.147 | Reg loss: 0.027 | Tree loss: 2.147 | Accuracy: 0.299000 | 0.269 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 2.141 | Reg loss: 0.028 | Tree loss: 2.141 | Accuracy: 0.305000 | 0.269 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 2.081 | Reg loss: 0.028 | Tree loss: 2.081 | Accuracy: 0.300341 | 0.269 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 2.344 | Reg loss: 0.027 | Tree loss: 2.344 | Accuracy: 0.328500 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 2.296 | Reg loss: 0.027 | Tree loss: 2.296 | Accuracy: 0.328500 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 2.262 | Reg loss: 0.027 | Tree loss: 2.262 | Accuracy: 0.324000 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 2.233 | Reg loss: 0.027 | Tree loss: 2.233 | Accuracy: 0.321000 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 2.183 | Reg loss: 0.027 | Tree loss: 2.183 | Accuracy: 0.314000 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 2.152 | Reg loss: 0.028 | Tree loss: 2.152 | Accuracy: 0.317500 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 2.157 | Reg loss: 0.028 | Tree loss: 2.157 | Accuracy: 0.326500 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 2.147 | Reg loss: 0.028 | Tree loss: 2.147 | Accuracy: 0.310000 | 0.269 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 2.132 | Reg loss: 0.028 | Tree loss: 2.132 | Accuracy: 0.316000 | 0.268 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 2.126 | Reg loss: 0.028 | Tree loss: 2.126 | Accuracy: 0.307500 | 0.268 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 2.133 | Reg loss: 0.028 | Tree loss: 2.133 | Accuracy: 0.290102 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 2.316 | Reg loss: 0.028 | Tree loss: 2.316 | Accuracy: 0.332000 | 0.269 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 2.292 | Reg loss: 0.028 | Tree loss: 2.292 | Accuracy: 0.338500 | 0.269 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 2.248 | Reg loss: 0.028 | Tree loss: 2.248 | Accuracy: 0.328000 | 0.269 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 2.213 | Reg loss: 0.028 | Tree loss: 2.213 | Accuracy: 0.324000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 2.187 | Reg loss: 0.028 | Tree loss: 2.187 | Accuracy: 0.329000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 2.146 | Reg loss: 0.028 | Tree loss: 2.146 | Accuracy: 0.339000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 2.141 | Reg loss: 0.028 | Tree loss: 2.141 | Accuracy: 0.315000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 2.103 | Reg loss: 0.028 | Tree loss: 2.103 | Accuracy: 0.333500 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 2.123 | Reg loss: 0.028 | Tree loss: 2.123 | Accuracy: 0.304000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 2.102 | Reg loss: 0.028 | Tree loss: 2.102 | Accuracy: 0.323000 | 0.268 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 2.144 | Reg loss: 0.028 | Tree loss: 2.144 | Accuracy: 0.269625 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 2.302 | Reg loss: 0.028 | Tree loss: 2.302 | Accuracy: 0.322500 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 2.257 | Reg loss: 0.028 | Tree loss: 2.257 | Accuracy: 0.343000 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 2.239 | Reg loss: 0.028 | Tree loss: 2.239 | Accuracy: 0.316000 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 2.189 | Reg loss: 0.028 | Tree loss: 2.189 | Accuracy: 0.328500 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 2.163 | Reg loss: 0.028 | Tree loss: 2.163 | Accuracy: 0.333500 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 2.134 | Reg loss: 0.028 | Tree loss: 2.134 | Accuracy: 0.331500 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 2.132 | Reg loss: 0.028 | Tree loss: 2.132 | Accuracy: 0.312500 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 2.106 | Reg loss: 0.028 | Tree loss: 2.106 | Accuracy: 0.346000 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 2.093 | Reg loss: 0.028 | Tree loss: 2.093 | Accuracy: 0.331000 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 2.095 | Reg loss: 0.028 | Tree loss: 2.095 | Accuracy: 0.302000 | 0.268 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 2.131 | Reg loss: 0.028 | Tree loss: 2.131 | Accuracy: 0.327645 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 2.279 | Reg loss: 0.028 | Tree loss: 2.279 | Accuracy: 0.323000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 2.264 | Reg loss: 0.028 | Tree loss: 2.264 | Accuracy: 0.327000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 2.223 | Reg loss: 0.028 | Tree loss: 2.223 | Accuracy: 0.330500 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.347500 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 2.160 | Reg loss: 0.028 | Tree loss: 2.160 | Accuracy: 0.327000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 2.139 | Reg loss: 0.028 | Tree loss: 2.139 | Accuracy: 0.320500 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 2.106 | Reg loss: 0.028 | Tree loss: 2.106 | Accuracy: 0.340000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 2.104 | Reg loss: 0.028 | Tree loss: 2.104 | Accuracy: 0.333500 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 2.071 | Reg loss: 0.028 | Tree loss: 2.071 | Accuracy: 0.328000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 2.058 | Reg loss: 0.028 | Tree loss: 2.058 | Accuracy: 0.322000 | 0.268 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 2.117 | Reg loss: 0.028 | Tree loss: 2.117 | Accuracy: 0.266212 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.347000 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 2.226 | Reg loss: 0.028 | Tree loss: 2.226 | Accuracy: 0.336000 | 0.268 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 2.195 | Reg loss: 0.028 | Tree loss: 2.195 | Accuracy: 0.329000 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 2.180 | Reg loss: 0.028 | Tree loss: 2.180 | Accuracy: 0.324000 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 2.158 | Reg loss: 0.028 | Tree loss: 2.158 | Accuracy: 0.317000 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 2.117 | Reg loss: 0.028 | Tree loss: 2.117 | Accuracy: 0.331500 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 2.102 | Reg loss: 0.028 | Tree loss: 2.102 | Accuracy: 0.323500 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 2.081 | Reg loss: 0.028 | Tree loss: 2.081 | Accuracy: 0.334500 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 2.080 | Reg loss: 0.028 | Tree loss: 2.080 | Accuracy: 0.337500 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 2.064 | Reg loss: 0.028 | Tree loss: 2.064 | Accuracy: 0.320500 | 0.268 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 2.017 | Reg loss: 0.028 | Tree loss: 2.017 | Accuracy: 0.334471 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 2.276 | Reg loss: 0.028 | Tree loss: 2.276 | Accuracy: 0.322500 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 2.217 | Reg loss: 0.028 | Tree loss: 2.217 | Accuracy: 0.338000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 2.199 | Reg loss: 0.028 | Tree loss: 2.199 | Accuracy: 0.348500 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 2.170 | Reg loss: 0.028 | Tree loss: 2.170 | Accuracy: 0.320000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 2.117 | Reg loss: 0.028 | Tree loss: 2.117 | Accuracy: 0.336000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 2.085 | Reg loss: 0.028 | Tree loss: 2.085 | Accuracy: 0.336500 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 2.093 | Reg loss: 0.028 | Tree loss: 2.093 | Accuracy: 0.327500 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 2.060 | Reg loss: 0.028 | Tree loss: 2.060 | Accuracy: 0.344000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 2.059 | Reg loss: 0.028 | Tree loss: 2.059 | Accuracy: 0.322000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 2.038 | Reg loss: 0.028 | Tree loss: 2.038 | Accuracy: 0.321000 | 0.268 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 2.090 | Reg loss: 0.028 | Tree loss: 2.090 | Accuracy: 0.320819 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 2.252 | Reg loss: 0.028 | Tree loss: 2.252 | Accuracy: 0.335500 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 2.209 | Reg loss: 0.028 | Tree loss: 2.209 | Accuracy: 0.324500 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 2.181 | Reg loss: 0.028 | Tree loss: 2.181 | Accuracy: 0.362000 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 2.151 | Reg loss: 0.028 | Tree loss: 2.151 | Accuracy: 0.337500 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 2.100 | Reg loss: 0.028 | Tree loss: 2.100 | Accuracy: 0.346000 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 2.070 | Reg loss: 0.028 | Tree loss: 2.070 | Accuracy: 0.339500 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 2.086 | Reg loss: 0.028 | Tree loss: 2.086 | Accuracy: 0.329000 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 2.039 | Reg loss: 0.028 | Tree loss: 2.039 | Accuracy: 0.350000 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 2.066 | Reg loss: 0.028 | Tree loss: 2.066 | Accuracy: 0.318000 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 2.055 | Reg loss: 0.028 | Tree loss: 2.055 | Accuracy: 0.314500 | 0.268 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 1.997 | Reg loss: 0.028 | Tree loss: 1.997 | Accuracy: 0.372014 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 2.233 | Reg loss: 0.028 | Tree loss: 2.233 | Accuracy: 0.342500 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 2.196 | Reg loss: 0.028 | Tree loss: 2.196 | Accuracy: 0.343000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 2.156 | Reg loss: 0.028 | Tree loss: 2.156 | Accuracy: 0.356500 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 2.132 | Reg loss: 0.028 | Tree loss: 2.132 | Accuracy: 0.338500 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 2.109 | Reg loss: 0.028 | Tree loss: 2.109 | Accuracy: 0.333000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 2.083 | Reg loss: 0.028 | Tree loss: 2.083 | Accuracy: 0.335000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 2.056 | Reg loss: 0.028 | Tree loss: 2.056 | Accuracy: 0.332000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 2.061 | Reg loss: 0.028 | Tree loss: 2.061 | Accuracy: 0.318000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 2.039 | Reg loss: 0.029 | Tree loss: 2.039 | Accuracy: 0.337000 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 2.037 | Reg loss: 0.029 | Tree loss: 2.037 | Accuracy: 0.313500 | 0.268 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 1.989 | Reg loss: 0.029 | Tree loss: 1.989 | Accuracy: 0.344710 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 2.242 | Reg loss: 0.028 | Tree loss: 2.242 | Accuracy: 0.326000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 2.197 | Reg loss: 0.028 | Tree loss: 2.197 | Accuracy: 0.349000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 2.127 | Reg loss: 0.028 | Tree loss: 2.127 | Accuracy: 0.357500 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 2.109 | Reg loss: 0.028 | Tree loss: 2.109 | Accuracy: 0.346000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 2.090 | Reg loss: 0.029 | Tree loss: 2.090 | Accuracy: 0.327500 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 2.062 | Reg loss: 0.029 | Tree loss: 2.062 | Accuracy: 0.338500 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 2.069 | Reg loss: 0.029 | Tree loss: 2.069 | Accuracy: 0.319000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 2.013 | Reg loss: 0.029 | Tree loss: 2.013 | Accuracy: 0.370000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 2.043 | Reg loss: 0.029 | Tree loss: 2.043 | Accuracy: 0.332000 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 2.027 | Reg loss: 0.029 | Tree loss: 2.027 | Accuracy: 0.326500 | 0.268 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 2.027 | Reg loss: 0.029 | Tree loss: 2.027 | Accuracy: 0.351536 | 0.268 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 2.222 | Reg loss: 0.029 | Tree loss: 2.222 | Accuracy: 0.352500 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 2.178 | Reg loss: 0.029 | Tree loss: 2.178 | Accuracy: 0.331000 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 2.146 | Reg loss: 0.029 | Tree loss: 2.146 | Accuracy: 0.359000 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 2.110 | Reg loss: 0.029 | Tree loss: 2.110 | Accuracy: 0.347000 | 0.268 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 2.093 | Reg loss: 0.029 | Tree loss: 2.093 | Accuracy: 0.336000 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 2.074 | Reg loss: 0.029 | Tree loss: 2.074 | Accuracy: 0.309000 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 2.043 | Reg loss: 0.029 | Tree loss: 2.043 | Accuracy: 0.328000 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 2.021 | Reg loss: 0.029 | Tree loss: 2.021 | Accuracy: 0.337000 | 0.267 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 2.004 | Reg loss: 0.029 | Tree loss: 2.004 | Accuracy: 0.334000 | 0.267 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 2.006 | Reg loss: 0.029 | Tree loss: 2.006 | Accuracy: 0.350500 | 0.268 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 2.036 | Reg loss: 0.029 | Tree loss: 2.036 | Accuracy: 0.317406 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 2.207 | Reg loss: 0.029 | Tree loss: 2.207 | Accuracy: 0.339000 | 0.268 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.347500 | 0.268 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 2.142 | Reg loss: 0.029 | Tree loss: 2.142 | Accuracy: 0.346500 | 0.268 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 2.098 | Reg loss: 0.029 | Tree loss: 2.098 | Accuracy: 0.359000 | 0.268 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 2.066 | Reg loss: 0.029 | Tree loss: 2.066 | Accuracy: 0.351000 | 0.268 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 2.039 | Reg loss: 0.029 | Tree loss: 2.039 | Accuracy: 0.331000 | 0.267 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 2.039 | Reg loss: 0.029 | Tree loss: 2.039 | Accuracy: 0.316500 | 0.267 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 2.016 | Reg loss: 0.029 | Tree loss: 2.016 | Accuracy: 0.346500 | 0.267 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 2.020 | Reg loss: 0.029 | Tree loss: 2.020 | Accuracy: 0.324500 | 0.267 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 2.001 | Reg loss: 0.029 | Tree loss: 2.001 | Accuracy: 0.341000 | 0.267 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 1.958 | Reg loss: 0.029 | Tree loss: 1.958 | Accuracy: 0.372014 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 2.200 | Reg loss: 0.029 | Tree loss: 2.200 | Accuracy: 0.338000 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.343000 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 2.113 | Reg loss: 0.029 | Tree loss: 2.113 | Accuracy: 0.356500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 2.094 | Reg loss: 0.029 | Tree loss: 2.094 | Accuracy: 0.351500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 2.065 | Reg loss: 0.029 | Tree loss: 2.065 | Accuracy: 0.330000 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 2.042 | Reg loss: 0.029 | Tree loss: 2.042 | Accuracy: 0.336000 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 2.028 | Reg loss: 0.029 | Tree loss: 2.028 | Accuracy: 0.336500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 2.015 | Reg loss: 0.029 | Tree loss: 2.015 | Accuracy: 0.311500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 1.996 | Reg loss: 0.029 | Tree loss: 1.996 | Accuracy: 0.335500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 1.983 | Reg loss: 0.029 | Tree loss: 1.983 | Accuracy: 0.314500 | 0.267 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 1.973 | Reg loss: 0.029 | Tree loss: 1.973 | Accuracy: 0.324232 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 2.166 | Reg loss: 0.029 | Tree loss: 2.166 | Accuracy: 0.360000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 2.161 | Reg loss: 0.029 | Tree loss: 2.161 | Accuracy: 0.335000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 2.115 | Reg loss: 0.029 | Tree loss: 2.115 | Accuracy: 0.368000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 2.079 | Reg loss: 0.029 | Tree loss: 2.079 | Accuracy: 0.358500 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 2.054 | Reg loss: 0.029 | Tree loss: 2.054 | Accuracy: 0.345000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 2.025 | Reg loss: 0.029 | Tree loss: 2.025 | Accuracy: 0.339500 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 2.015 | Reg loss: 0.029 | Tree loss: 2.015 | Accuracy: 0.339000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 2.007 | Reg loss: 0.029 | Tree loss: 2.007 | Accuracy: 0.332000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 1.989 | Reg loss: 0.029 | Tree loss: 1.989 | Accuracy: 0.345500 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 1.997 | Reg loss: 0.029 | Tree loss: 1.997 | Accuracy: 0.322000 | 0.267 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 1.951 | Reg loss: 0.029 | Tree loss: 1.951 | Accuracy: 0.327645 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.350500 | 0.267 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 2.149 | Reg loss: 0.029 | Tree loss: 2.149 | Accuracy: 0.347500 | 0.267 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 2.096 | Reg loss: 0.029 | Tree loss: 2.096 | Accuracy: 0.361000 | 0.267 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 2.089 | Reg loss: 0.029 | Tree loss: 2.089 | Accuracy: 0.351000 | 0.267 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 2.049 | Reg loss: 0.029 | Tree loss: 2.049 | Accuracy: 0.334500 | 0.267 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 2.019 | Reg loss: 0.029 | Tree loss: 2.019 | Accuracy: 0.328500 | 0.266 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 1.996 | Reg loss: 0.029 | Tree loss: 1.996 | Accuracy: 0.331500 | 0.266 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 1.986 | Reg loss: 0.029 | Tree loss: 1.986 | Accuracy: 0.351000 | 0.266 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 1.994 | Reg loss: 0.029 | Tree loss: 1.994 | Accuracy: 0.340000 | 0.266 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 1.968 | Reg loss: 0.029 | Tree loss: 1.968 | Accuracy: 0.327500 | 0.266 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 1.959 | Reg loss: 0.029 | Tree loss: 1.959 | Accuracy: 0.341297 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 2.178 | Reg loss: 0.029 | Tree loss: 2.178 | Accuracy: 0.339000 | 0.266 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 2.141 | Reg loss: 0.029 | Tree loss: 2.141 | Accuracy: 0.343000 | 0.266 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 2.082 | Reg loss: 0.029 | Tree loss: 2.082 | Accuracy: 0.350500 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 2.067 | Reg loss: 0.029 | Tree loss: 2.067 | Accuracy: 0.349500 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 2.025 | Reg loss: 0.029 | Tree loss: 2.025 | Accuracy: 0.337500 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 2.009 | Reg loss: 0.029 | Tree loss: 2.009 | Accuracy: 0.341000 | 0.267 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 1.997 | Reg loss: 0.029 | Tree loss: 1.997 | Accuracy: 0.344000 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 1.986 | Reg loss: 0.029 | Tree loss: 1.986 | Accuracy: 0.342500 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 1.971 | Reg loss: 0.029 | Tree loss: 1.971 | Accuracy: 0.328000 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 1.969 | Reg loss: 0.029 | Tree loss: 1.969 | Accuracy: 0.326000 | 0.267 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 1.999 | Reg loss: 0.029 | Tree loss: 1.999 | Accuracy: 0.341297 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.353000 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 2.125 | Reg loss: 0.029 | Tree loss: 2.125 | Accuracy: 0.338500 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 2.096 | Reg loss: 0.029 | Tree loss: 2.096 | Accuracy: 0.338500 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 2.055 | Reg loss: 0.029 | Tree loss: 2.055 | Accuracy: 0.349000 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 2.036 | Reg loss: 0.029 | Tree loss: 2.036 | Accuracy: 0.332000 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 2.014 | Reg loss: 0.029 | Tree loss: 2.014 | Accuracy: 0.328000 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 1.988 | Reg loss: 0.029 | Tree loss: 1.988 | Accuracy: 0.341000 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 1.952 | Reg loss: 0.029 | Tree loss: 1.952 | Accuracy: 0.356500 | 0.267 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 1.960 | Reg loss: 0.029 | Tree loss: 1.960 | Accuracy: 0.355500 | 0.266 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 1.947 | Reg loss: 0.029 | Tree loss: 1.947 | Accuracy: 0.345000 | 0.266 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 1.963 | Reg loss: 0.029 | Tree loss: 1.963 | Accuracy: 0.334471 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 2.148 | Reg loss: 0.029 | Tree loss: 2.148 | Accuracy: 0.343000 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 2.103 | Reg loss: 0.029 | Tree loss: 2.103 | Accuracy: 0.361000 | 0.266 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 2.086 | Reg loss: 0.029 | Tree loss: 2.086 | Accuracy: 0.346000 | 0.266 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 2.053 | Reg loss: 0.029 | Tree loss: 2.053 | Accuracy: 0.354500 | 0.266 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 2.001 | Reg loss: 0.029 | Tree loss: 2.001 | Accuracy: 0.360500 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 2.003 | Reg loss: 0.029 | Tree loss: 2.003 | Accuracy: 0.347500 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 1.973 | Reg loss: 0.029 | Tree loss: 1.973 | Accuracy: 0.335500 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 1.990 | Reg loss: 0.029 | Tree loss: 1.990 | Accuracy: 0.334000 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 1.932 | Reg loss: 0.030 | Tree loss: 1.932 | Accuracy: 0.375000 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 1.975 | Reg loss: 0.030 | Tree loss: 1.975 | Accuracy: 0.329000 | 0.267 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 1.965 | Reg loss: 0.030 | Tree loss: 1.965 | Accuracy: 0.344710 | 0.267 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 2.163 | Reg loss: 0.029 | Tree loss: 2.163 | Accuracy: 0.344000 | 0.267 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 2.127 | Reg loss: 0.029 | Tree loss: 2.127 | Accuracy: 0.345000 | 0.267 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 2.050 | Reg loss: 0.029 | Tree loss: 2.050 | Accuracy: 0.361500 | 0.267 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 2.043 | Reg loss: 0.029 | Tree loss: 2.043 | Accuracy: 0.367500 | 0.267 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 1.996 | Reg loss: 0.029 | Tree loss: 1.996 | Accuracy: 0.361500 | 0.267 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 2.002 | Reg loss: 0.030 | Tree loss: 2.002 | Accuracy: 0.329500 | 0.266 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 1.983 | Reg loss: 0.030 | Tree loss: 1.983 | Accuracy: 0.324000 | 0.266 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 1.955 | Reg loss: 0.030 | Tree loss: 1.955 | Accuracy: 0.329500 | 0.266 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 1.947 | Reg loss: 0.030 | Tree loss: 1.947 | Accuracy: 0.361000 | 0.266 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 1.930 | Reg loss: 0.030 | Tree loss: 1.930 | Accuracy: 0.347500 | 0.266 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 1.951 | Reg loss: 0.030 | Tree loss: 1.951 | Accuracy: 0.327645 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 2.133 | Reg loss: 0.030 | Tree loss: 2.133 | Accuracy: 0.355000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 2.102 | Reg loss: 0.030 | Tree loss: 2.102 | Accuracy: 0.349500 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 2.065 | Reg loss: 0.030 | Tree loss: 2.065 | Accuracy: 0.362000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 2.037 | Reg loss: 0.030 | Tree loss: 2.037 | Accuracy: 0.351500 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 2.000 | Reg loss: 0.030 | Tree loss: 2.000 | Accuracy: 0.345000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 1.964 | Reg loss: 0.030 | Tree loss: 1.964 | Accuracy: 0.368500 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 1.985 | Reg loss: 0.030 | Tree loss: 1.985 | Accuracy: 0.326000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 1.942 | Reg loss: 0.030 | Tree loss: 1.942 | Accuracy: 0.356000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 1.973 | Reg loss: 0.030 | Tree loss: 1.973 | Accuracy: 0.339500 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 1.930 | Reg loss: 0.030 | Tree loss: 1.930 | Accuracy: 0.328000 | 0.266 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 1.878 | Reg loss: 0.030 | Tree loss: 1.878 | Accuracy: 0.351536 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 2.110 | Reg loss: 0.030 | Tree loss: 2.110 | Accuracy: 0.357500 | 0.267 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.333500 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 2.059 | Reg loss: 0.030 | Tree loss: 2.059 | Accuracy: 0.350000 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 2.004 | Reg loss: 0.030 | Tree loss: 2.004 | Accuracy: 0.369500 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 1.989 | Reg loss: 0.030 | Tree loss: 1.989 | Accuracy: 0.348500 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 1.953 | Reg loss: 0.030 | Tree loss: 1.953 | Accuracy: 0.365000 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 1.967 | Reg loss: 0.030 | Tree loss: 1.967 | Accuracy: 0.347000 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 1.942 | Reg loss: 0.030 | Tree loss: 1.942 | Accuracy: 0.366500 | 0.266 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 1.971 | Reg loss: 0.030 | Tree loss: 1.971 | Accuracy: 0.324000 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 1.939 | Reg loss: 0.030 | Tree loss: 1.939 | Accuracy: 0.336500 | 0.266 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 1.936 | Reg loss: 0.030 | Tree loss: 1.936 | Accuracy: 0.327645 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 2.121 | Reg loss: 0.030 | Tree loss: 2.121 | Accuracy: 0.347000 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 2.065 | Reg loss: 0.030 | Tree loss: 2.065 | Accuracy: 0.360000 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 2.067 | Reg loss: 0.030 | Tree loss: 2.067 | Accuracy: 0.339500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 2.046 | Reg loss: 0.030 | Tree loss: 2.046 | Accuracy: 0.372500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 1.996 | Reg loss: 0.030 | Tree loss: 1.996 | Accuracy: 0.361500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 1.949 | Reg loss: 0.030 | Tree loss: 1.949 | Accuracy: 0.354500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 1.962 | Reg loss: 0.030 | Tree loss: 1.962 | Accuracy: 0.325500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 1.952 | Reg loss: 0.030 | Tree loss: 1.952 | Accuracy: 0.326500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 1.930 | Reg loss: 0.030 | Tree loss: 1.930 | Accuracy: 0.356500 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 1.898 | Reg loss: 0.030 | Tree loss: 1.898 | Accuracy: 0.347000 | 0.266 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.930 | Reg loss: 0.030 | Tree loss: 1.930 | Accuracy: 0.327645 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 2.110 | Reg loss: 0.030 | Tree loss: 2.110 | Accuracy: 0.345500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 2.063 | Reg loss: 0.030 | Tree loss: 2.063 | Accuracy: 0.350500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 2.035 | Reg loss: 0.030 | Tree loss: 2.035 | Accuracy: 0.362000 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 2.012 | Reg loss: 0.030 | Tree loss: 2.012 | Accuracy: 0.346500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 1.976 | Reg loss: 0.030 | Tree loss: 1.976 | Accuracy: 0.343500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 1.968 | Reg loss: 0.030 | Tree loss: 1.968 | Accuracy: 0.350500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 1.923 | Reg loss: 0.030 | Tree loss: 1.923 | Accuracy: 0.361500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 1.961 | Reg loss: 0.030 | Tree loss: 1.961 | Accuracy: 0.344500 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 1.951 | Reg loss: 0.030 | Tree loss: 1.951 | Accuracy: 0.342000 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 1.915 | Reg loss: 0.030 | Tree loss: 1.915 | Accuracy: 0.343000 | 0.266 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 1.911 | Reg loss: 0.030 | Tree loss: 1.911 | Accuracy: 0.310580 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 2.104 | Reg loss: 0.030 | Tree loss: 2.104 | Accuracy: 0.336500 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 2.062 | Reg loss: 0.030 | Tree loss: 2.062 | Accuracy: 0.357500 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 2.044 | Reg loss: 0.030 | Tree loss: 2.044 | Accuracy: 0.343500 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 2.026 | Reg loss: 0.030 | Tree loss: 2.026 | Accuracy: 0.352000 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 1.978 | Reg loss: 0.030 | Tree loss: 1.978 | Accuracy: 0.357500 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.030 | Tree loss: 1.955 | Accuracy: 0.346000 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 1.946 | Reg loss: 0.030 | Tree loss: 1.946 | Accuracy: 0.343500 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 1.913 | Reg loss: 0.030 | Tree loss: 1.913 | Accuracy: 0.366000 | 0.266 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 1.923 | Reg loss: 0.030 | Tree loss: 1.923 | Accuracy: 0.349500 | 0.265 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 1.901 | Reg loss: 0.030 | Tree loss: 1.901 | Accuracy: 0.348500 | 0.265 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 1.927 | Reg loss: 0.030 | Tree loss: 1.927 | Accuracy: 0.317406 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 2.101 | Reg loss: 0.030 | Tree loss: 2.101 | Accuracy: 0.338500 | 0.266 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 2.068 | Reg loss: 0.030 | Tree loss: 2.068 | Accuracy: 0.344000 | 0.266 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 2.028 | Reg loss: 0.030 | Tree loss: 2.028 | Accuracy: 0.379000 | 0.266 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 1.989 | Reg loss: 0.030 | Tree loss: 1.989 | Accuracy: 0.362500 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 1.989 | Reg loss: 0.030 | Tree loss: 1.989 | Accuracy: 0.330000 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 1.956 | Reg loss: 0.030 | Tree loss: 1.956 | Accuracy: 0.347000 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 1.938 | Reg loss: 0.030 | Tree loss: 1.938 | Accuracy: 0.353000 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 1.921 | Reg loss: 0.030 | Tree loss: 1.921 | Accuracy: 0.356500 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 1.903 | Reg loss: 0.030 | Tree loss: 1.903 | Accuracy: 0.352500 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 1.919 | Reg loss: 0.030 | Tree loss: 1.919 | Accuracy: 0.342500 | 0.265 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.866 | Reg loss: 0.030 | Tree loss: 1.866 | Accuracy: 0.344710 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 2.098 | Reg loss: 0.030 | Tree loss: 2.098 | Accuracy: 0.338500 | 0.265 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 2.079 | Reg loss: 0.030 | Tree loss: 2.079 | Accuracy: 0.326500 | 0.265 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 2.010 | Reg loss: 0.030 | Tree loss: 2.010 | Accuracy: 0.378500 | 0.265 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 1.993 | Reg loss: 0.030 | Tree loss: 1.993 | Accuracy: 0.361000 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 1.967 | Reg loss: 0.030 | Tree loss: 1.967 | Accuracy: 0.353500 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 1.945 | Reg loss: 0.030 | Tree loss: 1.945 | Accuracy: 0.352000 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 1.937 | Reg loss: 0.030 | Tree loss: 1.937 | Accuracy: 0.357000 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 1.899 | Reg loss: 0.030 | Tree loss: 1.899 | Accuracy: 0.376000 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 1.906 | Reg loss: 0.030 | Tree loss: 1.906 | Accuracy: 0.344000 | 0.266 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 1.898 | Reg loss: 0.030 | Tree loss: 1.898 | Accuracy: 0.325000 | 0.266 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 1.950 | Reg loss: 0.030 | Tree loss: 1.950 | Accuracy: 0.334471 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 2.089 | Reg loss: 0.030 | Tree loss: 2.089 | Accuracy: 0.346500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 2.053 | Reg loss: 0.030 | Tree loss: 2.053 | Accuracy: 0.352500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 2.021 | Reg loss: 0.030 | Tree loss: 2.021 | Accuracy: 0.358000 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 1.979 | Reg loss: 0.030 | Tree loss: 1.979 | Accuracy: 0.375000 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 1.962 | Reg loss: 0.030 | Tree loss: 1.962 | Accuracy: 0.372500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 1.944 | Reg loss: 0.030 | Tree loss: 1.944 | Accuracy: 0.354000 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 1.922 | Reg loss: 0.030 | Tree loss: 1.922 | Accuracy: 0.355500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 1.906 | Reg loss: 0.030 | Tree loss: 1.906 | Accuracy: 0.355500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 1.901 | Reg loss: 0.030 | Tree loss: 1.901 | Accuracy: 0.341500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 1.898 | Reg loss: 0.030 | Tree loss: 1.898 | Accuracy: 0.347500 | 0.266 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.910 | Reg loss: 0.030 | Tree loss: 1.910 | Accuracy: 0.344710 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 2.071 | Reg loss: 0.030 | Tree loss: 2.071 | Accuracy: 0.350500 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 2.063 | Reg loss: 0.030 | Tree loss: 2.063 | Accuracy: 0.328000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 2.010 | Reg loss: 0.030 | Tree loss: 2.010 | Accuracy: 0.357000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 1.981 | Reg loss: 0.030 | Tree loss: 1.981 | Accuracy: 0.345000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 1.954 | Reg loss: 0.030 | Tree loss: 1.954 | Accuracy: 0.369000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 1.928 | Reg loss: 0.030 | Tree loss: 1.928 | Accuracy: 0.360500 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 1.905 | Reg loss: 0.030 | Tree loss: 1.905 | Accuracy: 0.381500 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 1.894 | Reg loss: 0.030 | Tree loss: 1.894 | Accuracy: 0.339000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 1.908 | Reg loss: 0.030 | Tree loss: 1.908 | Accuracy: 0.358000 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 1.907 | Reg loss: 0.030 | Tree loss: 1.907 | Accuracy: 0.349500 | 0.266 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 1.879 | Reg loss: 0.030 | Tree loss: 1.879 | Accuracy: 0.293515 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 2.072 | Reg loss: 0.030 | Tree loss: 2.072 | Accuracy: 0.345000 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 2.061 | Reg loss: 0.030 | Tree loss: 2.061 | Accuracy: 0.350000 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 2.006 | Reg loss: 0.030 | Tree loss: 2.006 | Accuracy: 0.367500 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 1.982 | Reg loss: 0.030 | Tree loss: 1.982 | Accuracy: 0.375500 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 1.933 | Reg loss: 0.030 | Tree loss: 1.933 | Accuracy: 0.360000 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 1.913 | Reg loss: 0.030 | Tree loss: 1.913 | Accuracy: 0.356000 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 1.894 | Reg loss: 0.030 | Tree loss: 1.894 | Accuracy: 0.363500 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 1.898 | Reg loss: 0.030 | Tree loss: 1.898 | Accuracy: 0.371500 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 1.927 | Reg loss: 0.030 | Tree loss: 1.927 | Accuracy: 0.339500 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 1.884 | Reg loss: 0.030 | Tree loss: 1.884 | Accuracy: 0.342000 | 0.266 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 1.884 | Reg loss: 0.030 | Tree loss: 1.884 | Accuracy: 0.320819 | 0.266 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 2.085 | Reg loss: 0.030 | Tree loss: 2.085 | Accuracy: 0.339500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 2.037 | Reg loss: 0.030 | Tree loss: 2.037 | Accuracy: 0.345500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 1.999 | Reg loss: 0.030 | Tree loss: 1.999 | Accuracy: 0.357500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 1.962 | Reg loss: 0.030 | Tree loss: 1.962 | Accuracy: 0.365500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 1.924 | Reg loss: 0.030 | Tree loss: 1.924 | Accuracy: 0.376000 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 1.907 | Reg loss: 0.030 | Tree loss: 1.907 | Accuracy: 0.364000 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 1.914 | Reg loss: 0.030 | Tree loss: 1.914 | Accuracy: 0.350000 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 1.888 | Reg loss: 0.030 | Tree loss: 1.888 | Accuracy: 0.371500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 1.890 | Reg loss: 0.030 | Tree loss: 1.890 | Accuracy: 0.367500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.897 | Reg loss: 0.031 | Tree loss: 1.897 | Accuracy: 0.345500 | 0.266 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 1.915 | Reg loss: 0.031 | Tree loss: 1.915 | Accuracy: 0.337884 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 2.053 | Reg loss: 0.030 | Tree loss: 2.053 | Accuracy: 0.362500 | 0.266 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 2.030 | Reg loss: 0.030 | Tree loss: 2.030 | Accuracy: 0.351500 | 0.266 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 1.992 | Reg loss: 0.030 | Tree loss: 1.992 | Accuracy: 0.364000 | 0.266 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 1.992 | Reg loss: 0.030 | Tree loss: 1.992 | Accuracy: 0.363000 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 1.953 | Reg loss: 0.030 | Tree loss: 1.953 | Accuracy: 0.353500 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 1.908 | Reg loss: 0.030 | Tree loss: 1.908 | Accuracy: 0.372500 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 1.895 | Reg loss: 0.031 | Tree loss: 1.895 | Accuracy: 0.371500 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 1.905 | Reg loss: 0.031 | Tree loss: 1.905 | Accuracy: 0.374500 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 1.895 | Reg loss: 0.031 | Tree loss: 1.895 | Accuracy: 0.348000 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.853 | Reg loss: 0.031 | Tree loss: 1.853 | Accuracy: 0.356500 | 0.265 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 1.916 | Reg loss: 0.031 | Tree loss: 1.916 | Accuracy: 0.303754 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 2.055 | Reg loss: 0.030 | Tree loss: 2.055 | Accuracy: 0.352500 | 0.266 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 2.035 | Reg loss: 0.031 | Tree loss: 2.035 | Accuracy: 0.347000 | 0.266 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 1.991 | Reg loss: 0.031 | Tree loss: 1.991 | Accuracy: 0.348500 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 1.951 | Reg loss: 0.031 | Tree loss: 1.951 | Accuracy: 0.363500 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 1.925 | Reg loss: 0.031 | Tree loss: 1.925 | Accuracy: 0.366000 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 1.917 | Reg loss: 0.031 | Tree loss: 1.917 | Accuracy: 0.357000 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 1.880 | Reg loss: 0.031 | Tree loss: 1.880 | Accuracy: 0.377500 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 1.892 | Reg loss: 0.031 | Tree loss: 1.892 | Accuracy: 0.374000 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 1.894 | Reg loss: 0.031 | Tree loss: 1.894 | Accuracy: 0.341500 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 1.885 | Reg loss: 0.031 | Tree loss: 1.885 | Accuracy: 0.345000 | 0.265 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.875 | Reg loss: 0.031 | Tree loss: 1.875 | Accuracy: 0.406143 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 2.040 | Reg loss: 0.031 | Tree loss: 2.040 | Accuracy: 0.375000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 2.010 | Reg loss: 0.031 | Tree loss: 2.010 | Accuracy: 0.359500 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 1.975 | Reg loss: 0.031 | Tree loss: 1.975 | Accuracy: 0.378000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 1.971 | Reg loss: 0.031 | Tree loss: 1.971 | Accuracy: 0.352000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 1.948 | Reg loss: 0.031 | Tree loss: 1.948 | Accuracy: 0.341000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 1.920 | Reg loss: 0.031 | Tree loss: 1.920 | Accuracy: 0.348500 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 1.903 | Reg loss: 0.031 | Tree loss: 1.903 | Accuracy: 0.363000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 1.847 | Reg loss: 0.031 | Tree loss: 1.847 | Accuracy: 0.386000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 1.883 | Reg loss: 0.031 | Tree loss: 1.883 | Accuracy: 0.351000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 1.894 | Reg loss: 0.031 | Tree loss: 1.894 | Accuracy: 0.336000 | 0.265 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 1.794 | Reg loss: 0.031 | Tree loss: 1.794 | Accuracy: 0.382253 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 2.048 | Reg loss: 0.031 | Tree loss: 2.048 | Accuracy: 0.353000 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 2.025 | Reg loss: 0.031 | Tree loss: 2.025 | Accuracy: 0.353500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 1.965 | Reg loss: 0.031 | Tree loss: 1.965 | Accuracy: 0.373500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 1.960 | Reg loss: 0.031 | Tree loss: 1.960 | Accuracy: 0.355500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 1.937 | Reg loss: 0.031 | Tree loss: 1.937 | Accuracy: 0.367500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 1.918 | Reg loss: 0.031 | Tree loss: 1.918 | Accuracy: 0.346500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 1.887 | Reg loss: 0.031 | Tree loss: 1.887 | Accuracy: 0.366000 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.859 | Reg loss: 0.031 | Tree loss: 1.859 | Accuracy: 0.384500 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.878 | Reg loss: 0.031 | Tree loss: 1.878 | Accuracy: 0.382000 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.868 | Reg loss: 0.031 | Tree loss: 1.868 | Accuracy: 0.352000 | 0.265 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 1.783 | Reg loss: 0.031 | Tree loss: 1.783 | Accuracy: 0.399317 | 0.265 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 2.059 | Reg loss: 0.031 | Tree loss: 2.059 | Accuracy: 0.340500 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 2.005 | Reg loss: 0.031 | Tree loss: 2.005 | Accuracy: 0.377000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 1.974 | Reg loss: 0.031 | Tree loss: 1.974 | Accuracy: 0.367000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 1.936 | Reg loss: 0.031 | Tree loss: 1.936 | Accuracy: 0.371500 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 1.908 | Reg loss: 0.031 | Tree loss: 1.908 | Accuracy: 0.354000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 1.896 | Reg loss: 0.031 | Tree loss: 1.896 | Accuracy: 0.381000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 1.877 | Reg loss: 0.031 | Tree loss: 1.877 | Accuracy: 0.361500 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 1.877 | Reg loss: 0.031 | Tree loss: 1.877 | Accuracy: 0.355000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.876 | Reg loss: 0.031 | Tree loss: 1.876 | Accuracy: 0.355500 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 1.872 | Reg loss: 0.031 | Tree loss: 1.872 | Accuracy: 0.338000 | 0.265 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.864 | Reg loss: 0.031 | Tree loss: 1.864 | Accuracy: 0.327645 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 2.010 | Reg loss: 0.031 | Tree loss: 2.010 | Accuracy: 0.369000 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 2.007 | Reg loss: 0.031 | Tree loss: 2.007 | Accuracy: 0.356500 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 1.975 | Reg loss: 0.031 | Tree loss: 1.975 | Accuracy: 0.375500 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 1.931 | Reg loss: 0.031 | Tree loss: 1.931 | Accuracy: 0.381000 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 1.925 | Reg loss: 0.031 | Tree loss: 1.925 | Accuracy: 0.380500 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.904 | Reg loss: 0.031 | Tree loss: 1.904 | Accuracy: 0.364000 | 0.265 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 1.870 | Reg loss: 0.031 | Tree loss: 1.870 | Accuracy: 0.379000 | 0.264 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.901 | Reg loss: 0.031 | Tree loss: 1.901 | Accuracy: 0.357000 | 0.264 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 1.874 | Reg loss: 0.031 | Tree loss: 1.874 | Accuracy: 0.355500 | 0.264 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.836 | Reg loss: 0.031 | Tree loss: 1.836 | Accuracy: 0.358000 | 0.264 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 1.924 | Reg loss: 0.031 | Tree loss: 1.924 | Accuracy: 0.279863 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 1.999 | Reg loss: 0.031 | Tree loss: 1.999 | Accuracy: 0.375000 | 0.265 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 1.989 | Reg loss: 0.031 | Tree loss: 1.989 | Accuracy: 0.362000 | 0.264 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 1.961 | Reg loss: 0.031 | Tree loss: 1.961 | Accuracy: 0.356500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 1.912 | Reg loss: 0.031 | Tree loss: 1.912 | Accuracy: 0.379500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 1.935 | Reg loss: 0.031 | Tree loss: 1.935 | Accuracy: 0.358500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 1.890 | Reg loss: 0.031 | Tree loss: 1.890 | Accuracy: 0.367000 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 1.894 | Reg loss: 0.031 | Tree loss: 1.894 | Accuracy: 0.363500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 1.878 | Reg loss: 0.031 | Tree loss: 1.878 | Accuracy: 0.375500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.856 | Reg loss: 0.031 | Tree loss: 1.856 | Accuracy: 0.369000 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 1.865 | Reg loss: 0.031 | Tree loss: 1.865 | Accuracy: 0.359500 | 0.264 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.914 | Reg loss: 0.031 | Tree loss: 1.914 | Accuracy: 0.279863 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 2.011 | Reg loss: 0.031 | Tree loss: 2.011 | Accuracy: 0.359500 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 1.990 | Reg loss: 0.031 | Tree loss: 1.990 | Accuracy: 0.359500 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 1.971 | Reg loss: 0.031 | Tree loss: 1.971 | Accuracy: 0.358500 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 1.926 | Reg loss: 0.031 | Tree loss: 1.926 | Accuracy: 0.373000 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.890 | Reg loss: 0.031 | Tree loss: 1.890 | Accuracy: 0.379500 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 1.898 | Reg loss: 0.031 | Tree loss: 1.898 | Accuracy: 0.382000 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 1.872 | Reg loss: 0.031 | Tree loss: 1.872 | Accuracy: 0.375000 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 1.873 | Reg loss: 0.031 | Tree loss: 1.873 | Accuracy: 0.370000 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 1.859 | Reg loss: 0.031 | Tree loss: 1.859 | Accuracy: 0.372000 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 1.869 | Reg loss: 0.031 | Tree loss: 1.869 | Accuracy: 0.371500 | 0.264 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 1.798 | Reg loss: 0.031 | Tree loss: 1.798 | Accuracy: 0.395904 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 2.020 | Reg loss: 0.031 | Tree loss: 2.020 | Accuracy: 0.365000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 1.990 | Reg loss: 0.031 | Tree loss: 1.990 | Accuracy: 0.368000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 1.957 | Reg loss: 0.031 | Tree loss: 1.957 | Accuracy: 0.376000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 1.937 | Reg loss: 0.031 | Tree loss: 1.937 | Accuracy: 0.370500 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 1.910 | Reg loss: 0.031 | Tree loss: 1.910 | Accuracy: 0.366000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 1.887 | Reg loss: 0.031 | Tree loss: 1.887 | Accuracy: 0.365000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 1.843 | Reg loss: 0.031 | Tree loss: 1.843 | Accuracy: 0.384500 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 1.842 | Reg loss: 0.031 | Tree loss: 1.842 | Accuracy: 0.373500 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.843 | Reg loss: 0.031 | Tree loss: 1.843 | Accuracy: 0.371500 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.864 | Reg loss: 0.031 | Tree loss: 1.864 | Accuracy: 0.364000 | 0.264 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.852 | Reg loss: 0.031 | Tree loss: 1.852 | Accuracy: 0.358362 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 2.020 | Reg loss: 0.031 | Tree loss: 2.020 | Accuracy: 0.350500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 1.994 | Reg loss: 0.031 | Tree loss: 1.994 | Accuracy: 0.368000 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 1.946 | Reg loss: 0.031 | Tree loss: 1.946 | Accuracy: 0.367500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 1.913 | Reg loss: 0.031 | Tree loss: 1.913 | Accuracy: 0.375500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 1.899 | Reg loss: 0.031 | Tree loss: 1.899 | Accuracy: 0.367500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 1.905 | Reg loss: 0.031 | Tree loss: 1.905 | Accuracy: 0.375500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.843 | Reg loss: 0.031 | Tree loss: 1.843 | Accuracy: 0.393000 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.860 | Reg loss: 0.031 | Tree loss: 1.860 | Accuracy: 0.380500 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 1.854 | Reg loss: 0.031 | Tree loss: 1.854 | Accuracy: 0.372000 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 1.840 | Reg loss: 0.031 | Tree loss: 1.840 | Accuracy: 0.372000 | 0.264 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 1.810 | Reg loss: 0.031 | Tree loss: 1.810 | Accuracy: 0.378840 | 0.264 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 1.999 | Reg loss: 0.031 | Tree loss: 1.999 | Accuracy: 0.353000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 1.966 | Reg loss: 0.031 | Tree loss: 1.966 | Accuracy: 0.364500 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 1.934 | Reg loss: 0.031 | Tree loss: 1.934 | Accuracy: 0.364000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 1.909 | Reg loss: 0.031 | Tree loss: 1.909 | Accuracy: 0.370000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 1.923 | Reg loss: 0.031 | Tree loss: 1.923 | Accuracy: 0.369500 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 1.887 | Reg loss: 0.031 | Tree loss: 1.887 | Accuracy: 0.372500 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 1.885 | Reg loss: 0.031 | Tree loss: 1.885 | Accuracy: 0.366000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.844 | Reg loss: 0.031 | Tree loss: 1.844 | Accuracy: 0.365500 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 1.849 | Reg loss: 0.031 | Tree loss: 1.849 | Accuracy: 0.374000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 1.825 | Reg loss: 0.031 | Tree loss: 1.825 | Accuracy: 0.361000 | 0.264 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 1.802 | Reg loss: 0.031 | Tree loss: 1.802 | Accuracy: 0.430034 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 1.984 | Reg loss: 0.031 | Tree loss: 1.984 | Accuracy: 0.360000 | 0.264 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 1.988 | Reg loss: 0.031 | Tree loss: 1.988 | Accuracy: 0.353500 | 0.264 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 1.954 | Reg loss: 0.031 | Tree loss: 1.954 | Accuracy: 0.360000 | 0.264 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 1.935 | Reg loss: 0.031 | Tree loss: 1.935 | Accuracy: 0.368500 | 0.264 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 1.885 | Reg loss: 0.031 | Tree loss: 1.885 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.874 | Reg loss: 0.031 | Tree loss: 1.874 | Accuracy: 0.367000 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.845 | Reg loss: 0.031 | Tree loss: 1.845 | Accuracy: 0.376000 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 1.847 | Reg loss: 0.031 | Tree loss: 1.847 | Accuracy: 0.382500 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 1.866 | Reg loss: 0.031 | Tree loss: 1.866 | Accuracy: 0.367500 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 1.819 | Reg loss: 0.031 | Tree loss: 1.819 | Accuracy: 0.384000 | 0.263 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.833 | Reg loss: 0.031 | Tree loss: 1.833 | Accuracy: 0.385666 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 1.988 | Reg loss: 0.031 | Tree loss: 1.988 | Accuracy: 0.356000 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 1.961 | Reg loss: 0.031 | Tree loss: 1.961 | Accuracy: 0.369500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 1.918 | Reg loss: 0.031 | Tree loss: 1.918 | Accuracy: 0.388500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 1.931 | Reg loss: 0.031 | Tree loss: 1.931 | Accuracy: 0.360000 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 1.896 | Reg loss: 0.031 | Tree loss: 1.896 | Accuracy: 0.378500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 1.875 | Reg loss: 0.031 | Tree loss: 1.875 | Accuracy: 0.364500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 1.875 | Reg loss: 0.031 | Tree loss: 1.875 | Accuracy: 0.364500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 1.845 | Reg loss: 0.031 | Tree loss: 1.845 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 1.839 | Reg loss: 0.031 | Tree loss: 1.839 | Accuracy: 0.378000 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 1.807 | Reg loss: 0.031 | Tree loss: 1.807 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.859 | Reg loss: 0.031 | Tree loss: 1.859 | Accuracy: 0.344710 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 1.982 | Reg loss: 0.031 | Tree loss: 1.982 | Accuracy: 0.383000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 1.953 | Reg loss: 0.031 | Tree loss: 1.953 | Accuracy: 0.363500 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 1.944 | Reg loss: 0.031 | Tree loss: 1.944 | Accuracy: 0.362500 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 1.901 | Reg loss: 0.031 | Tree loss: 1.901 | Accuracy: 0.372000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 1.896 | Reg loss: 0.031 | Tree loss: 1.896 | Accuracy: 0.372000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 1.843 | Reg loss: 0.031 | Tree loss: 1.843 | Accuracy: 0.394000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 1.864 | Reg loss: 0.031 | Tree loss: 1.864 | Accuracy: 0.373500 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 1.848 | Reg loss: 0.031 | Tree loss: 1.848 | Accuracy: 0.376000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 1.844 | Reg loss: 0.031 | Tree loss: 1.844 | Accuracy: 0.371500 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 1.850 | Reg loss: 0.031 | Tree loss: 1.850 | Accuracy: 0.344000 | 0.263 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 1.823 | Reg loss: 0.031 | Tree loss: 1.823 | Accuracy: 0.344710 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 1.987 | Reg loss: 0.031 | Tree loss: 1.987 | Accuracy: 0.372500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 1.964 | Reg loss: 0.031 | Tree loss: 1.964 | Accuracy: 0.360000 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 1.919 | Reg loss: 0.031 | Tree loss: 1.919 | Accuracy: 0.378500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 1.925 | Reg loss: 0.031 | Tree loss: 1.925 | Accuracy: 0.361500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 1.889 | Reg loss: 0.031 | Tree loss: 1.889 | Accuracy: 0.363000 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 1.869 | Reg loss: 0.031 | Tree loss: 1.869 | Accuracy: 0.394000 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 1.865 | Reg loss: 0.031 | Tree loss: 1.865 | Accuracy: 0.375500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 1.831 | Reg loss: 0.031 | Tree loss: 1.831 | Accuracy: 0.384500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 1.809 | Reg loss: 0.031 | Tree loss: 1.809 | Accuracy: 0.380500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 1.811 | Reg loss: 0.031 | Tree loss: 1.811 | Accuracy: 0.386500 | 0.263 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 1.841 | Reg loss: 0.031 | Tree loss: 1.841 | Accuracy: 0.337884 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 1.940 | Reg loss: 0.031 | Tree loss: 1.940 | Accuracy: 0.378000 | 0.264 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 1.959 | Reg loss: 0.031 | Tree loss: 1.959 | Accuracy: 0.345000 | 0.264 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 1.927 | Reg loss: 0.031 | Tree loss: 1.927 | Accuracy: 0.378500 | 0.264 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 1.912 | Reg loss: 0.031 | Tree loss: 1.912 | Accuracy: 0.371000 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 1.885 | Reg loss: 0.031 | Tree loss: 1.885 | Accuracy: 0.376000 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 1.842 | Reg loss: 0.031 | Tree loss: 1.842 | Accuracy: 0.372500 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 1.859 | Reg loss: 0.031 | Tree loss: 1.859 | Accuracy: 0.379500 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 1.839 | Reg loss: 0.031 | Tree loss: 1.839 | Accuracy: 0.389500 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 1.815 | Reg loss: 0.031 | Tree loss: 1.815 | Accuracy: 0.389500 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 1.826 | Reg loss: 0.031 | Tree loss: 1.826 | Accuracy: 0.373500 | 0.263 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 1.840 | Reg loss: 0.031 | Tree loss: 1.840 | Accuracy: 0.351536 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 1.974 | Reg loss: 0.031 | Tree loss: 1.974 | Accuracy: 0.368000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 1.938 | Reg loss: 0.031 | Tree loss: 1.938 | Accuracy: 0.367000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 1.912 | Reg loss: 0.031 | Tree loss: 1.912 | Accuracy: 0.382500 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 1.905 | Reg loss: 0.031 | Tree loss: 1.905 | Accuracy: 0.379000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 1.866 | Reg loss: 0.031 | Tree loss: 1.866 | Accuracy: 0.393000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 1.862 | Reg loss: 0.031 | Tree loss: 1.862 | Accuracy: 0.377000 | 0.263 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 1.862 | Reg loss: 0.031 | Tree loss: 1.862 | Accuracy: 0.379500 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 1.826 | Reg loss: 0.031 | Tree loss: 1.826 | Accuracy: 0.397000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 1.838 | Reg loss: 0.031 | Tree loss: 1.838 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 1.817 | Reg loss: 0.031 | Tree loss: 1.817 | Accuracy: 0.364000 | 0.263 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.822 | Reg loss: 0.031 | Tree loss: 1.822 | Accuracy: 0.406143 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 1.980 | Reg loss: 0.031 | Tree loss: 1.980 | Accuracy: 0.357500 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.031 | Tree loss: 1.924 | Accuracy: 0.380500 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 1.917 | Reg loss: 0.031 | Tree loss: 1.917 | Accuracy: 0.368000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 1.904 | Reg loss: 0.031 | Tree loss: 1.904 | Accuracy: 0.357000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 1.865 | Reg loss: 0.031 | Tree loss: 1.865 | Accuracy: 0.403000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 1.866 | Reg loss: 0.031 | Tree loss: 1.866 | Accuracy: 0.376000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 1.844 | Reg loss: 0.031 | Tree loss: 1.844 | Accuracy: 0.381000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 1.822 | Reg loss: 0.031 | Tree loss: 1.822 | Accuracy: 0.399000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 1.823 | Reg loss: 0.031 | Tree loss: 1.823 | Accuracy: 0.380500 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 1.813 | Reg loss: 0.032 | Tree loss: 1.813 | Accuracy: 0.372000 | 0.263 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 1.861 | Reg loss: 0.032 | Tree loss: 1.861 | Accuracy: 0.382253 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 1.992 | Reg loss: 0.031 | Tree loss: 1.992 | Accuracy: 0.346000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 1.935 | Reg loss: 0.031 | Tree loss: 1.935 | Accuracy: 0.367500 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 1.920 | Reg loss: 0.031 | Tree loss: 1.920 | Accuracy: 0.373000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 1.886 | Reg loss: 0.031 | Tree loss: 1.886 | Accuracy: 0.377500 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 1.854 | Reg loss: 0.031 | Tree loss: 1.854 | Accuracy: 0.385000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 1.851 | Reg loss: 0.031 | Tree loss: 1.851 | Accuracy: 0.399000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 1.848 | Reg loss: 0.031 | Tree loss: 1.848 | Accuracy: 0.400000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 1.825 | Reg loss: 0.032 | Tree loss: 1.825 | Accuracy: 0.379500 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 1.808 | Reg loss: 0.032 | Tree loss: 1.808 | Accuracy: 0.401000 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.376500 | 0.263 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 1.809 | Reg loss: 0.032 | Tree loss: 1.809 | Accuracy: 0.327645 | 0.263 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 1.981 | Reg loss: 0.031 | Tree loss: 1.981 | Accuracy: 0.350000 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 1.938 | Reg loss: 0.031 | Tree loss: 1.938 | Accuracy: 0.362500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 1.905 | Reg loss: 0.031 | Tree loss: 1.905 | Accuracy: 0.368000 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.867 | Reg loss: 0.031 | Tree loss: 1.867 | Accuracy: 0.390500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 1.868 | Reg loss: 0.032 | Tree loss: 1.868 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.385500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 1.835 | Reg loss: 0.032 | Tree loss: 1.835 | Accuracy: 0.396500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 1.834 | Reg loss: 0.032 | Tree loss: 1.834 | Accuracy: 0.405000 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 1.833 | Reg loss: 0.032 | Tree loss: 1.833 | Accuracy: 0.383500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 1.793 | Reg loss: 0.032 | Tree loss: 1.793 | Accuracy: 0.379500 | 0.263 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 1.832 | Reg loss: 0.032 | Tree loss: 1.832 | Accuracy: 0.344710 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 1.945 | Reg loss: 0.032 | Tree loss: 1.945 | Accuracy: 0.363500 | 0.263 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.032 | Tree loss: 1.924 | Accuracy: 0.369000 | 0.263 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 1.920 | Reg loss: 0.032 | Tree loss: 1.920 | Accuracy: 0.366000 | 0.263 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 1.882 | Reg loss: 0.032 | Tree loss: 1.882 | Accuracy: 0.374500 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 1.850 | Reg loss: 0.032 | Tree loss: 1.850 | Accuracy: 0.399500 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 1.872 | Reg loss: 0.032 | Tree loss: 1.872 | Accuracy: 0.366000 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 1.835 | Reg loss: 0.032 | Tree loss: 1.835 | Accuracy: 0.369500 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 1.808 | Reg loss: 0.032 | Tree loss: 1.808 | Accuracy: 0.400500 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 1.826 | Reg loss: 0.032 | Tree loss: 1.826 | Accuracy: 0.383500 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.391000 | 0.262 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 1.777 | Reg loss: 0.032 | Tree loss: 1.777 | Accuracy: 0.365188 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 1.958 | Reg loss: 0.032 | Tree loss: 1.958 | Accuracy: 0.367500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 1.945 | Reg loss: 0.032 | Tree loss: 1.945 | Accuracy: 0.363500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 1.944 | Reg loss: 0.032 | Tree loss: 1.944 | Accuracy: 0.334500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 1.903 | Reg loss: 0.032 | Tree loss: 1.903 | Accuracy: 0.363000 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 1.851 | Reg loss: 0.032 | Tree loss: 1.851 | Accuracy: 0.388000 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 1.825 | Reg loss: 0.032 | Tree loss: 1.825 | Accuracy: 0.397500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.408500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 1.789 | Reg loss: 0.032 | Tree loss: 1.789 | Accuracy: 0.418500 | 0.262 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.392000 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 1.784 | Reg loss: 0.032 | Tree loss: 1.784 | Accuracy: 0.389500 | 0.262 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 1.830 | Reg loss: 0.032 | Tree loss: 1.830 | Accuracy: 0.412969 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 1.941 | Reg loss: 0.032 | Tree loss: 1.941 | Accuracy: 0.371000 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 1.913 | Reg loss: 0.032 | Tree loss: 1.913 | Accuracy: 0.378000 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 1.906 | Reg loss: 0.032 | Tree loss: 1.906 | Accuracy: 0.380000 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 1.881 | Reg loss: 0.032 | Tree loss: 1.881 | Accuracy: 0.366000 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 1.854 | Reg loss: 0.032 | Tree loss: 1.854 | Accuracy: 0.394500 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.393500 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.395000 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.395500 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.385500 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 1.841 | Reg loss: 0.032 | Tree loss: 1.841 | Accuracy: 0.369500 | 0.262 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 1.789 | Reg loss: 0.032 | Tree loss: 1.789 | Accuracy: 0.392491 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 1.947 | Reg loss: 0.032 | Tree loss: 1.947 | Accuracy: 0.364000 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 1.904 | Reg loss: 0.032 | Tree loss: 1.904 | Accuracy: 0.375000 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 1.900 | Reg loss: 0.032 | Tree loss: 1.900 | Accuracy: 0.384500 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 1.869 | Reg loss: 0.032 | Tree loss: 1.869 | Accuracy: 0.374500 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 1.862 | Reg loss: 0.032 | Tree loss: 1.862 | Accuracy: 0.383000 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 1.836 | Reg loss: 0.032 | Tree loss: 1.836 | Accuracy: 0.395000 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.387500 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.379500 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.386000 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 1.803 | Reg loss: 0.032 | Tree loss: 1.803 | Accuracy: 0.385500 | 0.262 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 1.828 | Reg loss: 0.032 | Tree loss: 1.828 | Accuracy: 0.365188 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 1.933 | Reg loss: 0.032 | Tree loss: 1.933 | Accuracy: 0.379500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 1.913 | Reg loss: 0.032 | Tree loss: 1.913 | Accuracy: 0.357000 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.032 | Tree loss: 1.886 | Accuracy: 0.375500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 1.888 | Reg loss: 0.032 | Tree loss: 1.888 | Accuracy: 0.374000 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 1.886 | Reg loss: 0.032 | Tree loss: 1.886 | Accuracy: 0.387500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 1.840 | Reg loss: 0.032 | Tree loss: 1.840 | Accuracy: 0.380000 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 1.810 | Reg loss: 0.032 | Tree loss: 1.810 | Accuracy: 0.396500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 1.797 | Reg loss: 0.032 | Tree loss: 1.797 | Accuracy: 0.399500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 1.791 | Reg loss: 0.032 | Tree loss: 1.791 | Accuracy: 0.393500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 1.784 | Reg loss: 0.032 | Tree loss: 1.784 | Accuracy: 0.405500 | 0.262 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 1.871 | Reg loss: 0.032 | Tree loss: 1.871 | Accuracy: 0.344710 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 1.937 | Reg loss: 0.032 | Tree loss: 1.937 | Accuracy: 0.357000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.032 | Tree loss: 1.898 | Accuracy: 0.385000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.899 | Reg loss: 0.032 | Tree loss: 1.899 | Accuracy: 0.381000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 1.887 | Reg loss: 0.032 | Tree loss: 1.887 | Accuracy: 0.387000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 1.849 | Reg loss: 0.032 | Tree loss: 1.849 | Accuracy: 0.386000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 1.842 | Reg loss: 0.032 | Tree loss: 1.842 | Accuracy: 0.391000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 1.803 | Reg loss: 0.032 | Tree loss: 1.803 | Accuracy: 0.408500 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 1.807 | Reg loss: 0.032 | Tree loss: 1.807 | Accuracy: 0.375000 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 1.795 | Reg loss: 0.032 | Tree loss: 1.795 | Accuracy: 0.383500 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 1.804 | Reg loss: 0.032 | Tree loss: 1.804 | Accuracy: 0.371500 | 0.262 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 1.868 | Reg loss: 0.032 | Tree loss: 1.868 | Accuracy: 0.365188 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.032 | Tree loss: 1.909 | Accuracy: 0.374000 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 1.905 | Reg loss: 0.032 | Tree loss: 1.905 | Accuracy: 0.376000 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 1.889 | Reg loss: 0.032 | Tree loss: 1.889 | Accuracy: 0.376500 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 1.885 | Reg loss: 0.032 | Tree loss: 1.885 | Accuracy: 0.362000 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 1.861 | Reg loss: 0.032 | Tree loss: 1.861 | Accuracy: 0.376500 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.417500 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 1.809 | Reg loss: 0.032 | Tree loss: 1.809 | Accuracy: 0.394500 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 1.795 | Reg loss: 0.032 | Tree loss: 1.795 | Accuracy: 0.422000 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.371000 | 0.262 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 1.785 | Reg loss: 0.032 | Tree loss: 1.785 | Accuracy: 0.397000 | 0.262 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.351536 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 1.928 | Reg loss: 0.032 | Tree loss: 1.928 | Accuracy: 0.383000 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 1.904 | Reg loss: 0.032 | Tree loss: 1.904 | Accuracy: 0.372000 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 1.876 | Reg loss: 0.032 | Tree loss: 1.876 | Accuracy: 0.374500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.391500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 1.826 | Reg loss: 0.032 | Tree loss: 1.826 | Accuracy: 0.390500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.371500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 1.840 | Reg loss: 0.032 | Tree loss: 1.840 | Accuracy: 0.377500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 1.805 | Reg loss: 0.032 | Tree loss: 1.805 | Accuracy: 0.393500 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 1.772 | Reg loss: 0.032 | Tree loss: 1.772 | Accuracy: 0.406000 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 1.816 | Reg loss: 0.032 | Tree loss: 1.816 | Accuracy: 0.383000 | 0.262 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 1.743 | Reg loss: 0.032 | Tree loss: 1.743 | Accuracy: 0.433447 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 1.934 | Reg loss: 0.032 | Tree loss: 1.934 | Accuracy: 0.366000 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 1.897 | Reg loss: 0.032 | Tree loss: 1.897 | Accuracy: 0.380500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 1.890 | Reg loss: 0.032 | Tree loss: 1.890 | Accuracy: 0.372500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.384500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.385500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 1.841 | Reg loss: 0.032 | Tree loss: 1.841 | Accuracy: 0.398500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 1.792 | Reg loss: 0.032 | Tree loss: 1.792 | Accuracy: 0.418000 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.385000 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 1.798 | Reg loss: 0.032 | Tree loss: 1.798 | Accuracy: 0.391000 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 1.779 | Reg loss: 0.032 | Tree loss: 1.779 | Accuracy: 0.393500 | 0.262 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 1.770 | Reg loss: 0.032 | Tree loss: 1.770 | Accuracy: 0.344710 | 0.262 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 1.940 | Reg loss: 0.032 | Tree loss: 1.940 | Accuracy: 0.367500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.032 | Tree loss: 1.898 | Accuracy: 0.371500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 1.901 | Reg loss: 0.032 | Tree loss: 1.901 | Accuracy: 0.360500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 1.830 | Reg loss: 0.032 | Tree loss: 1.830 | Accuracy: 0.390500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 1.826 | Reg loss: 0.032 | Tree loss: 1.826 | Accuracy: 0.400000 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 1.814 | Reg loss: 0.032 | Tree loss: 1.814 | Accuracy: 0.423000 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 1.803 | Reg loss: 0.032 | Tree loss: 1.803 | Accuracy: 0.411000 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 1.812 | Reg loss: 0.032 | Tree loss: 1.812 | Accuracy: 0.390500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.396500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 1.803 | Reg loss: 0.032 | Tree loss: 1.803 | Accuracy: 0.381500 | 0.262 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 1.804 | Reg loss: 0.032 | Tree loss: 1.804 | Accuracy: 0.412969 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 1.933 | Reg loss: 0.032 | Tree loss: 1.933 | Accuracy: 0.372500 | 0.262 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.032 | Tree loss: 1.924 | Accuracy: 0.370500 | 0.262 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 1.873 | Reg loss: 0.032 | Tree loss: 1.873 | Accuracy: 0.373000 | 0.262 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.032 | Tree loss: 1.843 | Accuracy: 0.384000 | 0.262 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.032 | Tree loss: 1.816 | Accuracy: 0.400000 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.400500 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.373500 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.371000 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 1.786 | Reg loss: 0.032 | Tree loss: 1.786 | Accuracy: 0.394500 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 1.764 | Reg loss: 0.032 | Tree loss: 1.764 | Accuracy: 0.422000 | 0.261 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.032 | Tree loss: 1.703 | Accuracy: 0.392491 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.032 | Tree loss: 1.909 | Accuracy: 0.381000 | 0.262 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 1.899 | Reg loss: 0.032 | Tree loss: 1.899 | Accuracy: 0.366000 | 0.262 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 1.859 | Reg loss: 0.032 | Tree loss: 1.859 | Accuracy: 0.385000 | 0.262 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 1.857 | Reg loss: 0.032 | Tree loss: 1.857 | Accuracy: 0.389500 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 1.838 | Reg loss: 0.032 | Tree loss: 1.838 | Accuracy: 0.393500 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 1.822 | Reg loss: 0.032 | Tree loss: 1.822 | Accuracy: 0.417000 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 1.808 | Reg loss: 0.032 | Tree loss: 1.808 | Accuracy: 0.408000 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 1.801 | Reg loss: 0.032 | Tree loss: 1.801 | Accuracy: 0.396000 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.384500 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 1.780 | Reg loss: 0.032 | Tree loss: 1.780 | Accuracy: 0.407500 | 0.261 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 1.732 | Reg loss: 0.032 | Tree loss: 1.732 | Accuracy: 0.433447 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 1.895 | Reg loss: 0.032 | Tree loss: 1.895 | Accuracy: 0.376500 | 0.262 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 1.902 | Reg loss: 0.032 | Tree loss: 1.902 | Accuracy: 0.374500 | 0.262 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 1.878 | Reg loss: 0.032 | Tree loss: 1.878 | Accuracy: 0.381500 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 1.857 | Reg loss: 0.032 | Tree loss: 1.857 | Accuracy: 0.373500 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 1.835 | Reg loss: 0.032 | Tree loss: 1.835 | Accuracy: 0.386500 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 1.813 | Reg loss: 0.032 | Tree loss: 1.813 | Accuracy: 0.398000 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 1.789 | Reg loss: 0.032 | Tree loss: 1.789 | Accuracy: 0.394500 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 1.810 | Reg loss: 0.032 | Tree loss: 1.810 | Accuracy: 0.404000 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 1.788 | Reg loss: 0.032 | Tree loss: 1.788 | Accuracy: 0.398000 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 1.775 | Reg loss: 0.032 | Tree loss: 1.775 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 1.805 | Reg loss: 0.032 | Tree loss: 1.805 | Accuracy: 0.344710 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 1.908 | Reg loss: 0.032 | Tree loss: 1.908 | Accuracy: 0.367500 | 0.262 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 1.900 | Reg loss: 0.032 | Tree loss: 1.900 | Accuracy: 0.360500 | 0.262 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 1.871 | Reg loss: 0.032 | Tree loss: 1.871 | Accuracy: 0.375000 | 0.262 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 1.838 | Reg loss: 0.032 | Tree loss: 1.838 | Accuracy: 0.399000 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.390000 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 1.802 | Reg loss: 0.032 | Tree loss: 1.802 | Accuracy: 0.391000 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 1.826 | Reg loss: 0.032 | Tree loss: 1.826 | Accuracy: 0.392000 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 1.774 | Reg loss: 0.032 | Tree loss: 1.774 | Accuracy: 0.418500 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 1.790 | Reg loss: 0.032 | Tree loss: 1.790 | Accuracy: 0.406500 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 1.788 | Reg loss: 0.032 | Tree loss: 1.788 | Accuracy: 0.386500 | 0.261 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.032 | Tree loss: 1.730 | Accuracy: 0.392491 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 1.915 | Reg loss: 0.032 | Tree loss: 1.915 | Accuracy: 0.372000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 1.887 | Reg loss: 0.032 | Tree loss: 1.887 | Accuracy: 0.393500 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 1.869 | Reg loss: 0.032 | Tree loss: 1.869 | Accuracy: 0.374000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 1.844 | Reg loss: 0.032 | Tree loss: 1.844 | Accuracy: 0.388500 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 1.826 | Reg loss: 0.032 | Tree loss: 1.826 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 1.802 | Reg loss: 0.032 | Tree loss: 1.802 | Accuracy: 0.395000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 1.809 | Reg loss: 0.032 | Tree loss: 1.809 | Accuracy: 0.403000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 1.779 | Reg loss: 0.032 | Tree loss: 1.779 | Accuracy: 0.385500 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 1.781 | Reg loss: 0.032 | Tree loss: 1.781 | Accuracy: 0.415500 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 1.761 | Reg loss: 0.032 | Tree loss: 1.761 | Accuracy: 0.393000 | 0.261 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 1.817 | Reg loss: 0.032 | Tree loss: 1.817 | Accuracy: 0.399317 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 1.887 | Reg loss: 0.032 | Tree loss: 1.887 | Accuracy: 0.394500 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 1.900 | Reg loss: 0.032 | Tree loss: 1.900 | Accuracy: 0.359000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.374500 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 1.830 | Reg loss: 0.032 | Tree loss: 1.830 | Accuracy: 0.402000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 1.836 | Reg loss: 0.032 | Tree loss: 1.836 | Accuracy: 0.385500 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 1.798 | Reg loss: 0.032 | Tree loss: 1.798 | Accuracy: 0.401000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 1.820 | Reg loss: 0.032 | Tree loss: 1.820 | Accuracy: 0.398000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.410000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 1.776 | Reg loss: 0.032 | Tree loss: 1.776 | Accuracy: 0.408000 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 1.783 | Reg loss: 0.032 | Tree loss: 1.783 | Accuracy: 0.385500 | 0.261 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.032 | Tree loss: 1.734 | Accuracy: 0.412969 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 1.910 | Reg loss: 0.032 | Tree loss: 1.910 | Accuracy: 0.375500 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 1.912 | Reg loss: 0.032 | Tree loss: 1.912 | Accuracy: 0.357000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 1.848 | Reg loss: 0.032 | Tree loss: 1.848 | Accuracy: 0.367000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 1.854 | Reg loss: 0.032 | Tree loss: 1.854 | Accuracy: 0.371500 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.400000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 1.792 | Reg loss: 0.032 | Tree loss: 1.792 | Accuracy: 0.410000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 1.790 | Reg loss: 0.032 | Tree loss: 1.790 | Accuracy: 0.409000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 1.787 | Reg loss: 0.032 | Tree loss: 1.787 | Accuracy: 0.402500 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 1.776 | Reg loss: 0.032 | Tree loss: 1.776 | Accuracy: 0.400000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 1.765 | Reg loss: 0.032 | Tree loss: 1.765 | Accuracy: 0.421000 | 0.261 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.032 | Tree loss: 1.733 | Accuracy: 0.406143 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.032 | Tree loss: 1.884 | Accuracy: 0.371500 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.881 | Reg loss: 0.032 | Tree loss: 1.881 | Accuracy: 0.371000 | 0.261 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 1.877 | Reg loss: 0.032 | Tree loss: 1.877 | Accuracy: 0.379000 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 1.832 | Reg loss: 0.032 | Tree loss: 1.832 | Accuracy: 0.375000 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.406000 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 1.805 | Reg loss: 0.032 | Tree loss: 1.805 | Accuracy: 0.430000 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 1.769 | Reg loss: 0.032 | Tree loss: 1.769 | Accuracy: 0.408500 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 1.784 | Reg loss: 0.032 | Tree loss: 1.784 | Accuracy: 0.399000 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 1.806 | Reg loss: 0.032 | Tree loss: 1.806 | Accuracy: 0.397500 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.379500 | 0.261 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 1.787 | Reg loss: 0.032 | Tree loss: 1.787 | Accuracy: 0.392491 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 1.904 | Reg loss: 0.032 | Tree loss: 1.904 | Accuracy: 0.372500 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 1.887 | Reg loss: 0.032 | Tree loss: 1.887 | Accuracy: 0.376500 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.032 | Tree loss: 1.853 | Accuracy: 0.368000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.406500 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 1.819 | Reg loss: 0.032 | Tree loss: 1.819 | Accuracy: 0.400000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 1.806 | Reg loss: 0.032 | Tree loss: 1.806 | Accuracy: 0.399000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 1.791 | Reg loss: 0.032 | Tree loss: 1.791 | Accuracy: 0.414000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 1.775 | Reg loss: 0.032 | Tree loss: 1.775 | Accuracy: 0.410000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 1.767 | Reg loss: 0.032 | Tree loss: 1.767 | Accuracy: 0.396000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 1.794 | Reg loss: 0.032 | Tree loss: 1.794 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 1.764 | Reg loss: 0.032 | Tree loss: 1.764 | Accuracy: 0.378840 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 1.899 | Reg loss: 0.032 | Tree loss: 1.899 | Accuracy: 0.370500 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.877 | Reg loss: 0.032 | Tree loss: 1.877 | Accuracy: 0.390500 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 1.838 | Reg loss: 0.032 | Tree loss: 1.838 | Accuracy: 0.403000 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 1.831 | Reg loss: 0.032 | Tree loss: 1.831 | Accuracy: 0.402000 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 1.822 | Reg loss: 0.032 | Tree loss: 1.822 | Accuracy: 0.387500 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 1.818 | Reg loss: 0.032 | Tree loss: 1.818 | Accuracy: 0.396000 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 1.778 | Reg loss: 0.032 | Tree loss: 1.778 | Accuracy: 0.395500 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 1.777 | Reg loss: 0.032 | Tree loss: 1.777 | Accuracy: 0.405000 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 1.790 | Reg loss: 0.032 | Tree loss: 1.790 | Accuracy: 0.390000 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.032 | Tree loss: 1.741 | Accuracy: 0.415500 | 0.261 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 1.780 | Reg loss: 0.032 | Tree loss: 1.780 | Accuracy: 0.436860 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 1.890 | Reg loss: 0.032 | Tree loss: 1.890 | Accuracy: 0.378000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 1.881 | Reg loss: 0.032 | Tree loss: 1.881 | Accuracy: 0.368000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 1.864 | Reg loss: 0.032 | Tree loss: 1.864 | Accuracy: 0.379000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 1.808 | Reg loss: 0.032 | Tree loss: 1.808 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 1.790 | Reg loss: 0.032 | Tree loss: 1.790 | Accuracy: 0.405000 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 1.781 | Reg loss: 0.032 | Tree loss: 1.781 | Accuracy: 0.414500 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 1.779 | Reg loss: 0.032 | Tree loss: 1.779 | Accuracy: 0.409500 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 1.779 | Reg loss: 0.032 | Tree loss: 1.779 | Accuracy: 0.403500 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 1.762 | Reg loss: 0.032 | Tree loss: 1.762 | Accuracy: 0.412500 | 0.261 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 1.841 | Reg loss: 0.032 | Tree loss: 1.841 | Accuracy: 0.344710 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 1.922 | Reg loss: 0.032 | Tree loss: 1.922 | Accuracy: 0.357000 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 1.875 | Reg loss: 0.032 | Tree loss: 1.875 | Accuracy: 0.381500 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 1.874 | Reg loss: 0.032 | Tree loss: 1.874 | Accuracy: 0.367000 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.032 | Tree loss: 1.808 | Accuracy: 0.391500 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 1.788 | Reg loss: 0.032 | Tree loss: 1.788 | Accuracy: 0.397000 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 1.799 | Reg loss: 0.032 | Tree loss: 1.799 | Accuracy: 0.403500 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 1.791 | Reg loss: 0.032 | Tree loss: 1.791 | Accuracy: 0.396500 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 1.773 | Reg loss: 0.032 | Tree loss: 1.773 | Accuracy: 0.409000 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 1.748 | Reg loss: 0.032 | Tree loss: 1.748 | Accuracy: 0.422000 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 1.788 | Reg loss: 0.032 | Tree loss: 1.788 | Accuracy: 0.392500 | 0.261 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 1.701 | Reg loss: 0.032 | Tree loss: 1.701 | Accuracy: 0.450512 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.032 | Tree loss: 1.879 | Accuracy: 0.380500 | 0.262 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 1.891 | Reg loss: 0.032 | Tree loss: 1.891 | Accuracy: 0.368000 | 0.262 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.392000 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 1.844 | Reg loss: 0.032 | Tree loss: 1.844 | Accuracy: 0.374500 | 0.261 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 1.809 | Reg loss: 0.032 | Tree loss: 1.809 | Accuracy: 0.399500 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 1.798 | Reg loss: 0.032 | Tree loss: 1.798 | Accuracy: 0.408500 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.032 | Tree loss: 1.767 | Accuracy: 0.427000 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 1.775 | Reg loss: 0.032 | Tree loss: 1.775 | Accuracy: 0.422000 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 1.783 | Reg loss: 0.032 | Tree loss: 1.783 | Accuracy: 0.395000 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.032 | Tree loss: 1.749 | Accuracy: 0.419000 | 0.261 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 1.801 | Reg loss: 0.032 | Tree loss: 1.801 | Accuracy: 0.395904 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 1.897 | Reg loss: 0.032 | Tree loss: 1.897 | Accuracy: 0.356000 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 1.873 | Reg loss: 0.032 | Tree loss: 1.873 | Accuracy: 0.370500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 1.845 | Reg loss: 0.032 | Tree loss: 1.845 | Accuracy: 0.375000 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 1.821 | Reg loss: 0.032 | Tree loss: 1.821 | Accuracy: 0.383000 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 1.827 | Reg loss: 0.032 | Tree loss: 1.827 | Accuracy: 0.388500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 1.778 | Reg loss: 0.032 | Tree loss: 1.778 | Accuracy: 0.427500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 1.772 | Reg loss: 0.032 | Tree loss: 1.772 | Accuracy: 0.430000 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 1.769 | Reg loss: 0.032 | Tree loss: 1.769 | Accuracy: 0.413500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 1.744 | Reg loss: 0.032 | Tree loss: 1.744 | Accuracy: 0.435500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 1.770 | Reg loss: 0.032 | Tree loss: 1.770 | Accuracy: 0.397500 | 0.261 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 1.810 | Reg loss: 0.032 | Tree loss: 1.810 | Accuracy: 0.399317 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 1.907 | Reg loss: 0.032 | Tree loss: 1.907 | Accuracy: 0.368000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 1.883 | Reg loss: 0.032 | Tree loss: 1.883 | Accuracy: 0.363000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 1.852 | Reg loss: 0.032 | Tree loss: 1.852 | Accuracy: 0.372000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 1.810 | Reg loss: 0.032 | Tree loss: 1.810 | Accuracy: 0.414000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 1.794 | Reg loss: 0.032 | Tree loss: 1.794 | Accuracy: 0.394500 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 1.786 | Reg loss: 0.032 | Tree loss: 1.786 | Accuracy: 0.412000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.032 | Tree loss: 1.760 | Accuracy: 0.423500 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 1.754 | Reg loss: 0.032 | Tree loss: 1.754 | Accuracy: 0.429000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 1.777 | Reg loss: 0.032 | Tree loss: 1.777 | Accuracy: 0.401000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.415000 | 0.261 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.378840 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 1.906 | Reg loss: 0.032 | Tree loss: 1.906 | Accuracy: 0.373000 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 1.864 | Reg loss: 0.032 | Tree loss: 1.864 | Accuracy: 0.376500 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 1.842 | Reg loss: 0.032 | Tree loss: 1.842 | Accuracy: 0.363000 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.390000 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 1.813 | Reg loss: 0.032 | Tree loss: 1.813 | Accuracy: 0.381500 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.032 | Tree loss: 1.765 | Accuracy: 0.431500 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.432500 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.426000 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.405500 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.406000 | 0.261 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.399317 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.032 | Tree loss: 1.871 | Accuracy: 0.374500 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 1.866 | Reg loss: 0.032 | Tree loss: 1.866 | Accuracy: 0.375000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 1.857 | Reg loss: 0.032 | Tree loss: 1.857 | Accuracy: 0.372500 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 1.801 | Reg loss: 0.032 | Tree loss: 1.801 | Accuracy: 0.407000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.033 | Tree loss: 1.801 | Accuracy: 0.409000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.396000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.414000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.420500 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.421000 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.381500 | 0.261 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.389078 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 1.880 | Reg loss: 0.033 | Tree loss: 1.880 | Accuracy: 0.369500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.033 | Tree loss: 1.874 | Accuracy: 0.376500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 1.852 | Reg loss: 0.033 | Tree loss: 1.852 | Accuracy: 0.373000 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.033 | Tree loss: 1.816 | Accuracy: 0.394500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 1.807 | Reg loss: 0.033 | Tree loss: 1.807 | Accuracy: 0.405500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.420500 | 0.261 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.423500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.397500 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.412000 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.412000 | 0.261 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.395904 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.033 | Tree loss: 1.878 | Accuracy: 0.371000 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 1.882 | Reg loss: 0.033 | Tree loss: 1.882 | Accuracy: 0.362000 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 1.833 | Reg loss: 0.033 | Tree loss: 1.833 | Accuracy: 0.385000 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 1.823 | Reg loss: 0.033 | Tree loss: 1.823 | Accuracy: 0.380500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 1.807 | Reg loss: 0.033 | Tree loss: 1.807 | Accuracy: 0.396500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.407500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.431500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.422500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.398000 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.417500 | 0.261 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.378840 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.033 | Tree loss: 1.874 | Accuracy: 0.372000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 1.873 | Reg loss: 0.033 | Tree loss: 1.873 | Accuracy: 0.372000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.033 | Tree loss: 1.805 | Accuracy: 0.400000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.033 | Tree loss: 1.809 | Accuracy: 0.387000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.407000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.431500 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.425000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.419500 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.404500 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.392000 | 0.261 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 1.827 | Reg loss: 0.033 | Tree loss: 1.827 | Accuracy: 0.399317 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.033 | Tree loss: 1.859 | Accuracy: 0.393000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 1.857 | Reg loss: 0.033 | Tree loss: 1.857 | Accuracy: 0.361500 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 1.856 | Reg loss: 0.033 | Tree loss: 1.856 | Accuracy: 0.370000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 1.815 | Reg loss: 0.033 | Tree loss: 1.815 | Accuracy: 0.381500 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 1.812 | Reg loss: 0.033 | Tree loss: 1.812 | Accuracy: 0.395000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.421000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.438500 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.412000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.408500 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.365188 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 1.883 | Reg loss: 0.033 | Tree loss: 1.883 | Accuracy: 0.358000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 1.861 | Reg loss: 0.033 | Tree loss: 1.861 | Accuracy: 0.377000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 1.821 | Reg loss: 0.033 | Tree loss: 1.821 | Accuracy: 0.385000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.033 | Tree loss: 1.806 | Accuracy: 0.388500 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.413000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.421000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.408000 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.397500 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.393500 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.415500 | 0.261 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.460751 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.033 | Tree loss: 1.855 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.033 | Tree loss: 1.833 | Accuracy: 0.388500 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 1.843 | Reg loss: 0.033 | Tree loss: 1.843 | Accuracy: 0.375000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.377000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.033 | Tree loss: 1.801 | Accuracy: 0.407000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.398000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.450000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.414500 | 0.261 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.415500 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.417000 | 0.261 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.436860 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 1.868 | Reg loss: 0.033 | Tree loss: 1.868 | Accuracy: 0.374000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.033 | Tree loss: 1.854 | Accuracy: 0.378000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.033 | Tree loss: 1.850 | Accuracy: 0.384000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.033 | Tree loss: 1.816 | Accuracy: 0.379000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.409000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.408500 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.405000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.411000 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.417500 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.410500 | 0.261 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.426621 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.033 | Tree loss: 1.870 | Accuracy: 0.373000 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 1.865 | Reg loss: 0.033 | Tree loss: 1.865 | Accuracy: 0.374000 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.409000 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.394000 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.401500 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.413500 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.422500 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.439000 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.398500 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.397500 | 0.261 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.402730 | 0.261 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.033 | Tree loss: 1.851 | Accuracy: 0.399000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 1.866 | Reg loss: 0.033 | Tree loss: 1.866 | Accuracy: 0.372000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.033 | Tree loss: 1.812 | Accuracy: 0.381000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.389000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.404500 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.417000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.436000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.424000 | 0.261 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.405000 | 0.26 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.388500 | 0.26 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.385666 | 0.26 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.033 | Tree loss: 1.872 | Accuracy: 0.384500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.033 | Tree loss: 1.852 | Accuracy: 0.393500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 1.828 | Reg loss: 0.033 | Tree loss: 1.828 | Accuracy: 0.382000 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.373500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.406500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.412500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.434500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.424000 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.408000 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.412500 | 0.26 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.382253 | 0.26 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.033 | Tree loss: 1.854 | Accuracy: 0.391000 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 1.851 | Reg loss: 0.033 | Tree loss: 1.851 | Accuracy: 0.373000 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.394000 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.389500 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.391500 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.420500 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.431000 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.412000 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.407500 | 0.26 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.426500 | 0.26 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.392491 | 0.26 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.033 | Tree loss: 1.859 | Accuracy: 0.374500 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.393000 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.033 | Tree loss: 1.800 | Accuracy: 0.399500 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.365000 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.394500 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.425000 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.449500 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.435000 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.425500 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.411000 | 0.26 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.378840 | 0.26 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 1.881 | Reg loss: 0.033 | Tree loss: 1.881 | Accuracy: 0.375000 | 0.26 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.033 | Tree loss: 1.838 | Accuracy: 0.396500 | 0.26 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.399500 | 0.26 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.033 | Tree loss: 1.803 | Accuracy: 0.395000 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 1.784 | Reg loss: 0.033 | Tree loss: 1.784 | Accuracy: 0.411500 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.426000 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.418500 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.440000 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.394500 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.410500 | 0.259 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.354949 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.033 | Tree loss: 1.833 | Accuracy: 0.395500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.033 | Tree loss: 1.824 | Accuracy: 0.392000 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 1.833 | Reg loss: 0.033 | Tree loss: 1.833 | Accuracy: 0.382500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.033 | Tree loss: 1.797 | Accuracy: 0.395000 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.417500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.420000 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.429000 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.426500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.423500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.407500 | 0.259 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.399317 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.033 | Tree loss: 1.858 | Accuracy: 0.379500 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.393000 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 1.844 | Reg loss: 0.033 | Tree loss: 1.844 | Accuracy: 0.382500 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 1.813 | Reg loss: 0.033 | Tree loss: 1.813 | Accuracy: 0.393000 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.413000 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.445500 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.433000 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.433000 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.418500 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.416500 | 0.259 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.385666 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.033 | Tree loss: 1.860 | Accuracy: 0.391000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.033 | Tree loss: 1.840 | Accuracy: 0.388000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 1.840 | Reg loss: 0.033 | Tree loss: 1.840 | Accuracy: 0.365000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.419000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.421000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.420000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.414500 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.426500 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.436000 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.417500 | 0.259 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.409556 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.033 | Tree loss: 1.857 | Accuracy: 0.390000 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.033 | Tree loss: 1.844 | Accuracy: 0.380000 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.033 | Tree loss: 1.803 | Accuracy: 0.404500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.397500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.418500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.421500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.438500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.424500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.409500 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.416000 | 0.259 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.375427 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.033 | Tree loss: 1.841 | Accuracy: 0.388000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.033 | Tree loss: 1.826 | Accuracy: 0.403500 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.033 | Tree loss: 1.812 | Accuracy: 0.406000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.403500 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.407000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.423000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.438000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.442000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.435000 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.411500 | 0.259 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 1.809 | Reg loss: 0.033 | Tree loss: 1.809 | Accuracy: 0.392491 | 0.259 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.033 | Tree loss: 1.844 | Accuracy: 0.393000 | 0.259 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.033 | Tree loss: 1.837 | Accuracy: 0.385000 | 0.259 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 1.838 | Reg loss: 0.033 | Tree loss: 1.838 | Accuracy: 0.393000 | 0.259 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.401000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.431000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.426000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.443000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.412000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.414500 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.416000 | 0.258 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.389078 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.033 | Tree loss: 1.826 | Accuracy: 0.404500 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.033 | Tree loss: 1.835 | Accuracy: 0.395500 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.424000 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.396000 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.403000 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.424500 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.453500 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.424000 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.428000 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.399500 | 0.258 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.460751 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.033 | Tree loss: 1.844 | Accuracy: 0.396500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.033 | Tree loss: 1.840 | Accuracy: 0.391500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.033 | Tree loss: 1.809 | Accuracy: 0.382000 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.409500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.415000 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.415500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.436500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.434000 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.431000 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.420500 | 0.258 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.433447 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.033 | Tree loss: 1.842 | Accuracy: 0.383000 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.395500 | 0.258 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.033 | Tree loss: 1.790 | Accuracy: 0.412500 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.427500 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.413500 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.427000 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.449500 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.430000 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.407000 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.407500 | 0.258 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.447099 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.033 | Tree loss: 1.846 | Accuracy: 0.394500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.033 | Tree loss: 1.828 | Accuracy: 0.400000 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.033 | Tree loss: 1.802 | Accuracy: 0.404500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.415000 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.400000 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.438000 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.425500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.441500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.439500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.407500 | 0.258 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.440273 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 1.821 | Reg loss: 0.033 | Tree loss: 1.821 | Accuracy: 0.411500 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.033 | Tree loss: 1.836 | Accuracy: 0.405000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.405000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.033 | Tree loss: 1.787 | Accuracy: 0.410500 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.418500 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.429000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.446000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.439500 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.434000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.420000 | 0.258 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.433447 | 0.258 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.033 | Tree loss: 1.824 | Accuracy: 0.396500 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.033 | Tree loss: 1.820 | Accuracy: 0.416000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.396500 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.033 | Tree loss: 1.793 | Accuracy: 0.402500 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.419000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.427000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.444000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.442000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.416000 | 0.258 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.427000 | 0.257 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.402730 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.033 | Tree loss: 1.852 | Accuracy: 0.390500 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.033 | Tree loss: 1.822 | Accuracy: 0.411500 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.417000 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.390000 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.419500 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.444500 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.453000 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.445000 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.443000 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.433500 | 0.257 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.423208 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.033 | Tree loss: 1.831 | Accuracy: 0.405500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.033 | Tree loss: 1.815 | Accuracy: 0.399000 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.417000 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.436000 | 0.257 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.420500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.424500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.443000 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.427500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.415500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.424500 | 0.257 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.409556 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.033 | Tree loss: 1.862 | Accuracy: 0.371500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.407500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.419000 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.033 | Tree loss: 1.795 | Accuracy: 0.401000 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.410000 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.435500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.462500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.445500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.445500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.442500 | 0.257 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.416382 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.033 | Tree loss: 1.830 | Accuracy: 0.412500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.033 | Tree loss: 1.815 | Accuracy: 0.389000 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.402500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.412500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.415000 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.434500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.452500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.463000 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.449500 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.447000 | 0.257 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.443686 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 1.837 | Reg loss: 0.033 | Tree loss: 1.837 | Accuracy: 0.397500 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.033 | Tree loss: 1.827 | Accuracy: 0.380000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.429000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.406500 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.422000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.456000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.446000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.461500 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.448000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.435000 | 0.257 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.453925 | 0.257 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.033 | Tree loss: 1.840 | Accuracy: 0.390000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.033 | Tree loss: 1.805 | Accuracy: 0.412000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.419000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.411500 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.411000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.431000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.458000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.451000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.424000 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.441500 | 0.257 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.460751 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.033 | Tree loss: 1.830 | Accuracy: 0.400000 | 0.257 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.426000 | 0.257 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.404500 | 0.257 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.405000 | 0.257 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.404500 | 0.256 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.441500 | 0.256 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.449000 | 0.256 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.453500 | 0.256 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.441000 | 0.256 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.437500 | 0.256 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.460751 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.033 | Tree loss: 1.832 | Accuracy: 0.391000 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.416500 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.423500 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.404500 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.425500 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.447500 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.471000 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.447000 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.444000 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.415000 | 0.256 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.457338 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.399500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.415000 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.412500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.421500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.406500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.443500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.457500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.441500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.428500 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.452000 | 0.256 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.450512 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.033 | Tree loss: 1.832 | Accuracy: 0.396000 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.412000 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.033 | Tree loss: 1.799 | Accuracy: 0.406500 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.428500 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.428500 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.442000 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.443000 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.440500 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.449000 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.425500 | 0.256 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.440273 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 1.827 | Reg loss: 0.033 | Tree loss: 1.827 | Accuracy: 0.403500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.033 | Tree loss: 1.801 | Accuracy: 0.418000 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.414000 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.424000 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.423500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.438500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.446000 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.444500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.429500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.445500 | 0.256 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.440273 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 1.831 | Reg loss: 0.033 | Tree loss: 1.831 | Accuracy: 0.406500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.033 | Tree loss: 1.828 | Accuracy: 0.397000 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.416000 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.419500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.426500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.438500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.455500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.459500 | 0.256 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.429500 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.441000 | 0.256 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.419795 | 0.256 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.033 | Tree loss: 1.836 | Accuracy: 0.407500 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 1.812 | Reg loss: 0.033 | Tree loss: 1.812 | Accuracy: 0.403500 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.401000 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.420500 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.440500 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.450000 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.462500 | 0.256 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.441000 | 0.255 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.438000 | 0.255 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.425500 | 0.255 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.447099 | 0.255 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.033 | Tree loss: 1.823 | Accuracy: 0.405500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.033 | Tree loss: 1.816 | Accuracy: 0.408500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.434500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.436500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.415500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.460000 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.455000 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.441000 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.417500 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.436000 | 0.255 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.419795 | 0.255 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.033 | Tree loss: 1.833 | Accuracy: 0.395000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.033 | Tree loss: 1.806 | Accuracy: 0.408000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.417500 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.434000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.410500 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.434500 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.439000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.454500 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.430000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.458000 | 0.255 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 1.802 | Reg loss: 0.033 | Tree loss: 1.802 | Accuracy: 0.399317 | 0.255 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.033 | Tree loss: 1.822 | Accuracy: 0.401500 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.033 | Tree loss: 1.821 | Accuracy: 0.410000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.400000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.427000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.413000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.459000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.453000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.452000 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.423500 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.424500 | 0.255 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.460751 | 0.255 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 1.839 | Reg loss: 0.033 | Tree loss: 1.839 | Accuracy: 0.399500 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.033 | Tree loss: 1.801 | Accuracy: 0.424500 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.410500 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.423500 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.438000 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.439500 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.450000 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.444000 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.445000 | 0.255 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.440000 | 0.255 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.440273 | 0.255 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.409500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.033 | Tree loss: 1.813 | Accuracy: 0.413500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.033 | Tree loss: 1.793 | Accuracy: 0.405000 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.420500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.444500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.445500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.427000 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.470500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.447000 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.417500 | 0.255 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.457338 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.033 | Tree loss: 1.824 | Accuracy: 0.402000 | 0.255 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.033 | Tree loss: 1.802 | Accuracy: 0.403500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.425500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.433000 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.421500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.457000 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.461500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.445500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.433000 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.430500 | 0.254 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.402730 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 1.823 | Reg loss: 0.033 | Tree loss: 1.823 | Accuracy: 0.407000 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.033 | Tree loss: 1.800 | Accuracy: 0.405000 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.404000 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.418000 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.421500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.430500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.465500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.444500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.432500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.448500 | 0.254 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.443686 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.033 | Tree loss: 1.800 | Accuracy: 0.412000 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.033 | Tree loss: 1.803 | Accuracy: 0.414000 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.416500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.405500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.444500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.465500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.459000 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.429500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.439500 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.449000 | 0.254 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.477816 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.033 | Tree loss: 1.832 | Accuracy: 0.391500 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.033 | Tree loss: 1.810 | Accuracy: 0.396500 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.437000 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.420000 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.395000 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.455500 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.460500 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.460000 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.470500 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.450000 | 0.254 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.464164 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.416500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.408000 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.413000 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.416500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.415500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.447500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.453500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.428500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.450500 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.449000 | 0.254 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.440273 | 0.254 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.033 | Tree loss: 1.822 | Accuracy: 0.394000 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.408000 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.432000 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.420500 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.427500 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.456000 | 0.254 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.422500 | 0.253 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.414500 | 0.253 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.450000 | 0.253 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.480000 | 0.253 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.488055 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 1.793 | Reg loss: 0.033 | Tree loss: 1.793 | Accuracy: 0.409500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.033 | Tree loss: 1.797 | Accuracy: 0.416000 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.420000 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.399500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.434500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.466000 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.462500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.447500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.440500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.432500 | 0.253 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.450512 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.033 | Tree loss: 1.832 | Accuracy: 0.399000 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.434500 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.399500 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.420000 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.430500 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.445500 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.455000 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.432000 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.448000 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.436500 | 0.253 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.436860 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.398000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.427500 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.033 | Tree loss: 1.810 | Accuracy: 0.381000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.442000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.426000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.450000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.465000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.447000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.459000 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.442500 | 0.253 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.457338 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 1.805 | Reg loss: 0.033 | Tree loss: 1.805 | Accuracy: 0.421500 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.412000 | 0.253 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.033 | Tree loss: 1.802 | Accuracy: 0.403500 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.414500 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.417000 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.456500 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.452000 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.453500 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.438000 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.447000 | 0.253 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.399317 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.033 | Tree loss: 1.806 | Accuracy: 0.393500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.033 | Tree loss: 1.819 | Accuracy: 0.398500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.416500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.409500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.427000 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.444000 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.463000 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.442500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.467000 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.467500 | 0.253 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.423208 | 0.253 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.400000 | 0.253 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.413000 | 0.253 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.429500 | 0.253 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.421000 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.438000 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.443500 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.443500 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.460500 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.443500 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.434000 | 0.252 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.464164 | 0.252 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.033 | Tree loss: 1.830 | Accuracy: 0.381500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.033 | Tree loss: 1.806 | Accuracy: 0.408000 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.413500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.409000 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.455000 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.462500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.470000 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.454500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.453500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.443500 | 0.252 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.464164 | 0.252 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.400500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.426500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.424500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.411500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.423500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.441500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.464500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.444000 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.452500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.444500 | 0.252 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 1.615 | Reg loss: 0.033 | Tree loss: 1.615 | Accuracy: 0.447099 | 0.252 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.407000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.033 | Tree loss: 1.801 | Accuracy: 0.410500 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.421500 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.414000 | 0.252 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.443000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.463000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.462000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.454000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.439000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.433000 | 0.252 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.474403 | 0.252 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 1.797 | Reg loss: 0.033 | Tree loss: 1.797 | Accuracy: 0.416000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.033 | Tree loss: 1.797 | Accuracy: 0.401000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.415000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.433000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.418500 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.442000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.444500 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.474000 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.440500 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.448500 | 0.252 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.447099 | 0.252 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.033 | Tree loss: 1.835 | Accuracy: 0.413000 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.419000 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.424000 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.420500 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.434000 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.466500 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.466500 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.460500 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.448500 | 0.252 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.446000 | 0.251 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.450512 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.413500 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 1.811 | Reg loss: 0.033 | Tree loss: 1.811 | Accuracy: 0.402000 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.445500 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.424500 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.427500 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.434000 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.455000 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.452500 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.416000 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.034 | Tree loss: 1.717 | Accuracy: 0.456000 | 0.251 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.443686 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.033 | Tree loss: 1.820 | Accuracy: 0.402000 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 1.790 | Reg loss: 0.033 | Tree loss: 1.790 | Accuracy: 0.410000 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.411000 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.433500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.433500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.452500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.451500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.460500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.435500 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.461000 | 0.251 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.477816 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.033 | Tree loss: 1.800 | Accuracy: 0.415500 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.409500 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.425000 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.432000 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.431500 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.428000 | 0.251 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.466500 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.450000 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.455000 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.034 | Tree loss: 1.724 | Accuracy: 0.445500 | 0.251 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.433447 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 1.795 | Reg loss: 0.033 | Tree loss: 1.795 | Accuracy: 0.415500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.419000 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.422500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.430000 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.430500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.444500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.455000 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.460000 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.034 | Tree loss: 1.733 | Accuracy: 0.437500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.453500 | 0.251 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.395904 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.408000 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.406500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.444500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.434500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.425500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.445000 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.460500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.446000 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.448500 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.461000 | 0.251 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.034 | Tree loss: 1.661 | Accuracy: 0.488055 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 1.817 | Reg loss: 0.033 | Tree loss: 1.817 | Accuracy: 0.411000 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 1.790 | Reg loss: 0.033 | Tree loss: 1.790 | Accuracy: 0.411500 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.425000 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.416500 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.439000 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.454500 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.458000 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.433500 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.450500 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.459000 | 0.251 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.488055 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.423000 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.033 | Tree loss: 1.809 | Accuracy: 0.392500 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.421000 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.434000 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.424000 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.456500 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.460500 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.462500 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.448500 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.441000 | 0.251 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.481229 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 1.813 | Reg loss: 0.033 | Tree loss: 1.813 | Accuracy: 0.410000 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.414000 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.432500 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.423500 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.431500 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.034 | Tree loss: 1.712 | Accuracy: 0.441500 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.466500 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.443500 | 0.251 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.460000 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.443000 | 0.251 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.467577 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.390000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.424000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.412500 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.416000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.443000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.456000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.461000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.456500 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.446000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.449000 | 0.251 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.034 | Tree loss: 1.746 | Accuracy: 0.436860 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 1.821 | Reg loss: 0.033 | Tree loss: 1.821 | Accuracy: 0.401000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.433000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.428000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.427500 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.445500 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.438000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.462500 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.458000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.447500 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.431000 | 0.251 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.430034 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.393500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.414500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.427500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.425000 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.440000 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.450500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.479500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.034 | Tree loss: 1.717 | Accuracy: 0.464000 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.451500 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.469000 | 0.251 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.474403 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.405500 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.033 | Tree loss: 1.808 | Accuracy: 0.399000 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.420000 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.421000 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.452500 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.444500 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.453000 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.034 | Tree loss: 1.704 | Accuracy: 0.440500 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.451000 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.466500 | 0.251 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.034 | Tree loss: 1.728 | Accuracy: 0.453925 | 0.251 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.415500 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.404000 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.422500 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.435500 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.432000 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.429000 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.473500 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.461500 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.458000 | 0.251 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.447500 | 0.25 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 1.641 | Reg loss: 0.034 | Tree loss: 1.641 | Accuracy: 0.488055 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 1.810 | Reg loss: 0.033 | Tree loss: 1.810 | Accuracy: 0.408000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.419500 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.407500 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.412000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.440000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.468000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.438500 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.452000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.456000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.481000 | 0.25 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.034 | Tree loss: 1.657 | Accuracy: 0.470990 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.033 | Tree loss: 1.799 | Accuracy: 0.416000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.434000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.420000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.403500 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 1.724 | Reg loss: 0.034 | Tree loss: 1.724 | Accuracy: 0.429000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.449000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.456500 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.451500 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.464000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.469000 | 0.25 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.453925 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.033 | Tree loss: 1.820 | Accuracy: 0.393000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.414500 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.425000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.034 | Tree loss: 1.728 | Accuracy: 0.425000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.034 | Tree loss: 1.744 | Accuracy: 0.414000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.464500 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.034 | Tree loss: 1.724 | Accuracy: 0.437500 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.457000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.483000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.449000 | 0.25 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.467577 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 1.798 | Reg loss: 0.033 | Tree loss: 1.798 | Accuracy: 0.421000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.034 | Tree loss: 1.766 | Accuracy: 0.414500 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.034 | Tree loss: 1.760 | Accuracy: 0.427000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.034 | Tree loss: 1.734 | Accuracy: 0.425000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.439500 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.034 | Tree loss: 1.753 | Accuracy: 0.435000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.454000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.474000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.465500 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.453000 | 0.25 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 1.722 | Reg loss: 0.034 | Tree loss: 1.722 | Accuracy: 0.433447 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.034 | Tree loss: 1.789 | Accuracy: 0.406000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.034 | Tree loss: 1.802 | Accuracy: 0.401000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.034 | Tree loss: 1.758 | Accuracy: 0.432000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.034 | Tree loss: 1.734 | Accuracy: 0.430500 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 1.719 | Reg loss: 0.034 | Tree loss: 1.719 | Accuracy: 0.434000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.454000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.034 | Tree loss: 1.720 | Accuracy: 0.453000 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.447500 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.468500 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.468500 | 0.25 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.416382 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 1.787 | Reg loss: 0.034 | Tree loss: 1.787 | Accuracy: 0.406500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.034 | Tree loss: 1.787 | Accuracy: 0.415000 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 1.765 | Reg loss: 0.034 | Tree loss: 1.765 | Accuracy: 0.426500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.034 | Tree loss: 1.745 | Accuracy: 0.420500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.439500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.448500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.459500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.456500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.034 | Tree loss: 1.663 | Accuracy: 0.468500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.466500 | 0.25 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.406143 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.421000 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.033 | Tree loss: 1.787 | Accuracy: 0.422500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.034 | Tree loss: 1.751 | Accuracy: 0.430500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.034 | Tree loss: 1.728 | Accuracy: 0.424500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.440000 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.462500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.460500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.463500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.451500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.444500 | 0.25 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.034 | Tree loss: 1.725 | Accuracy: 0.426621 | 0.25 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 1.803 | Reg loss: 0.034 | Tree loss: 1.803 | Accuracy: 0.420500 | 0.25 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 1.765 | Reg loss: 0.034 | Tree loss: 1.765 | Accuracy: 0.421000 | 0.25 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 1.745 | Reg loss: 0.034 | Tree loss: 1.745 | Accuracy: 0.422000 | 0.25 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.434000 | 0.25 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.440000 | 0.25 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.462000 | 0.249 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.452000 | 0.249 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.445500 | 0.249 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.440000 | 0.249 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.459500 | 0.249 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.494881 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.427500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.033 | Tree loss: 1.803 | Accuracy: 0.408000 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 1.745 | Reg loss: 0.034 | Tree loss: 1.745 | Accuracy: 0.417000 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 1.743 | Reg loss: 0.034 | Tree loss: 1.743 | Accuracy: 0.434500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.034 | Tree loss: 1.726 | Accuracy: 0.416000 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.448000 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.461500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.468500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.455500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.468500 | 0.249 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 1.727 | Reg loss: 0.034 | Tree loss: 1.727 | Accuracy: 0.457338 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.417500 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.414000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 1.757 | Reg loss: 0.034 | Tree loss: 1.757 | Accuracy: 0.426000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.034 | Tree loss: 1.764 | Accuracy: 0.414000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.449000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.463500 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.455500 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.453000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.462500 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.458000 | 0.249 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.457338 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 1.800 | Reg loss: 0.034 | Tree loss: 1.800 | Accuracy: 0.419500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 1.787 | Reg loss: 0.034 | Tree loss: 1.787 | Accuracy: 0.412500 | 0.249 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.034 | Tree loss: 1.761 | Accuracy: 0.403000 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.034 | Tree loss: 1.745 | Accuracy: 0.425500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.034 | Tree loss: 1.721 | Accuracy: 0.439000 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.465500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.464500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.458500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.443500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.478500 | 0.249 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.034 | Tree loss: 1.651 | Accuracy: 0.453925 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.033 | Tree loss: 1.799 | Accuracy: 0.417000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.402000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.034 | Tree loss: 1.749 | Accuracy: 0.418500 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.435000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.421000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.462500 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.452500 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.457000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.464000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.455000 | 0.249 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.034 | Tree loss: 1.717 | Accuracy: 0.443686 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 1.820 | Reg loss: 0.034 | Tree loss: 1.820 | Accuracy: 0.398500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.034 | Tree loss: 1.781 | Accuracy: 0.421000 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.034 | Tree loss: 1.752 | Accuracy: 0.428500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 1.726 | Reg loss: 0.034 | Tree loss: 1.726 | Accuracy: 0.444500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.428500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.458500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.458000 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.444500 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.461000 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.453000 | 0.249 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.034 | Tree loss: 1.673 | Accuracy: 0.453925 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 1.797 | Reg loss: 0.034 | Tree loss: 1.797 | Accuracy: 0.420000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.034 | Tree loss: 1.764 | Accuracy: 0.414000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.034 | Tree loss: 1.755 | Accuracy: 0.429000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.034 | Tree loss: 1.729 | Accuracy: 0.429000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.449000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.459500 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.456000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.444500 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.463500 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.457000 | 0.249 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.426621 | 0.249 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 1.793 | Reg loss: 0.034 | Tree loss: 1.793 | Accuracy: 0.409500 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.034 | Tree loss: 1.756 | Accuracy: 0.432000 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 1.746 | Reg loss: 0.034 | Tree loss: 1.746 | Accuracy: 0.423500 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.424000 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.448500 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.440500 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.457000 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.451000 | 0.249 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.034 | Tree loss: 1.731 | Accuracy: 0.438000 | 0.248 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.453000 | 0.248 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.467577 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.034 | Tree loss: 1.825 | Accuracy: 0.382500 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 1.771 | Reg loss: 0.034 | Tree loss: 1.771 | Accuracy: 0.405500 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.034 | Tree loss: 1.726 | Accuracy: 0.447500 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 1.720 | Reg loss: 0.034 | Tree loss: 1.720 | Accuracy: 0.429000 | 0.248 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.462000 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.467000 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.444500 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.454000 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.466000 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.459000 | 0.248 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.467577 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 1.782 | Reg loss: 0.034 | Tree loss: 1.782 | Accuracy: 0.406500 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 1.797 | Reg loss: 0.034 | Tree loss: 1.797 | Accuracy: 0.403000 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.034 | Tree loss: 1.759 | Accuracy: 0.419000 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 1.736 | Reg loss: 0.034 | Tree loss: 1.736 | Accuracy: 0.431000 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.448500 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.447500 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.442000 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.465500 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.463000 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.460500 | 0.248 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 1.599 | Reg loss: 0.034 | Tree loss: 1.599 | Accuracy: 0.481229 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.034 | Tree loss: 1.789 | Accuracy: 0.410000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 1.776 | Reg loss: 0.034 | Tree loss: 1.776 | Accuracy: 0.397000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.034 | Tree loss: 1.732 | Accuracy: 0.437000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 1.725 | Reg loss: 0.034 | Tree loss: 1.725 | Accuracy: 0.434500 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.034 | Tree loss: 1.736 | Accuracy: 0.433000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.456500 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.427000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.461000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.455000 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.459500 | 0.248 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.034 | Tree loss: 1.655 | Accuracy: 0.474403 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 1.788 | Reg loss: 0.034 | Tree loss: 1.788 | Accuracy: 0.414000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.034 | Tree loss: 1.792 | Accuracy: 0.406000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.034 | Tree loss: 1.749 | Accuracy: 0.432500 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 1.731 | Reg loss: 0.034 | Tree loss: 1.731 | Accuracy: 0.423000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.445000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.449500 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.034 | Tree loss: 1.704 | Accuracy: 0.439000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.465000 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.461500 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.461500 | 0.248 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.034 | Tree loss: 1.629 | Accuracy: 0.481229 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.034 | Tree loss: 1.789 | Accuracy: 0.423000 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.437500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.034 | Tree loss: 1.756 | Accuracy: 0.410000 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.423500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 1.727 | Reg loss: 0.034 | Tree loss: 1.727 | Accuracy: 0.435500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.454500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.455000 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.438000 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.462500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.443500 | 0.248 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.430034 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.034 | Tree loss: 1.789 | Accuracy: 0.418000 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.034 | Tree loss: 1.753 | Accuracy: 0.424000 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.034 | Tree loss: 1.766 | Accuracy: 0.429500 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.034 | Tree loss: 1.755 | Accuracy: 0.405000 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.034 | Tree loss: 1.741 | Accuracy: 0.421000 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.442000 | 0.248 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.445500 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.477500 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.463500 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.473500 | 0.248 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.464164 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 1.824 | Reg loss: 0.034 | Tree loss: 1.824 | Accuracy: 0.402500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.034 | Tree loss: 1.738 | Accuracy: 0.439500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.034 | Tree loss: 1.793 | Accuracy: 0.406500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.425500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.432000 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.451500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.464000 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.491000 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.466500 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.435000 | 0.248 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 1.636 | Reg loss: 0.034 | Tree loss: 1.636 | Accuracy: 0.474403 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 1.810 | Reg loss: 0.034 | Tree loss: 1.810 | Accuracy: 0.405500 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.034 | Tree loss: 1.800 | Accuracy: 0.404000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.034 | Tree loss: 1.721 | Accuracy: 0.443000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.034 | Tree loss: 1.741 | Accuracy: 0.421500 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.447000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.456000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.480500 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.454000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.468000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.456000 | 0.248 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.470990 | 0.248 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 1.776 | Reg loss: 0.034 | Tree loss: 1.776 | Accuracy: 0.411500 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 1.784 | Reg loss: 0.034 | Tree loss: 1.784 | Accuracy: 0.409000 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.034 | Tree loss: 1.762 | Accuracy: 0.439000 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.034 | Tree loss: 1.739 | Accuracy: 0.406000 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.423500 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.455500 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.470500 | 0.248 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.451500 | 0.247 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.468000 | 0.247 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 1.647 | Reg loss: 0.034 | Tree loss: 1.647 | Accuracy: 0.480500 | 0.247 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.453925 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 1.784 | Reg loss: 0.034 | Tree loss: 1.784 | Accuracy: 0.417000 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 1.762 | Reg loss: 0.034 | Tree loss: 1.762 | Accuracy: 0.410000 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.429000 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.429000 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.444500 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.443500 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.450000 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.456500 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.466500 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.459500 | 0.247 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 1.731 | Reg loss: 0.034 | Tree loss: 1.731 | Accuracy: 0.430034 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.034 | Tree loss: 1.770 | Accuracy: 0.440000 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 1.779 | Reg loss: 0.034 | Tree loss: 1.779 | Accuracy: 0.416500 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.444000 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 1.717 | Reg loss: 0.034 | Tree loss: 1.717 | Accuracy: 0.436000 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.034 | Tree loss: 1.739 | Accuracy: 0.418500 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.449500 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.435500 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.450000 | 0.247 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.457500 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.460000 | 0.247 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.034 | Tree loss: 1.629 | Accuracy: 0.501706 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.034 | Tree loss: 1.789 | Accuracy: 0.429000 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.034 | Tree loss: 1.774 | Accuracy: 0.419500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.034 | Tree loss: 1.755 | Accuracy: 0.413500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 1.741 | Reg loss: 0.034 | Tree loss: 1.741 | Accuracy: 0.417500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.457500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.424500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.439500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.466500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.468500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.473500 | 0.247 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 1.619 | Reg loss: 0.034 | Tree loss: 1.619 | Accuracy: 0.484642 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.034 | Tree loss: 1.775 | Accuracy: 0.439500 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 1.784 | Reg loss: 0.034 | Tree loss: 1.784 | Accuracy: 0.405000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.034 | Tree loss: 1.751 | Accuracy: 0.416500 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.034 | Tree loss: 1.734 | Accuracy: 0.431000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.464000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.426500 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.464500 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.469000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.453000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.452000 | 0.247 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.034 | Tree loss: 1.724 | Accuracy: 0.450512 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.034 | Tree loss: 1.811 | Accuracy: 0.401000 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.034 | Tree loss: 1.766 | Accuracy: 0.429500 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.439000 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.034 | Tree loss: 1.742 | Accuracy: 0.417000 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.439000 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.442000 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 1.644 | Reg loss: 0.034 | Tree loss: 1.644 | Accuracy: 0.496500 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.450500 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.437500 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.458500 | 0.247 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.430034 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 1.795 | Reg loss: 0.034 | Tree loss: 1.795 | Accuracy: 0.419000 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.034 | Tree loss: 1.764 | Accuracy: 0.422500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.034 | Tree loss: 1.736 | Accuracy: 0.426000 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.442500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.450500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.455500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.449000 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.034 | Tree loss: 1.718 | Accuracy: 0.443000 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.468500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.034 | Tree loss: 1.670 | Accuracy: 0.454500 | 0.247 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.034 | Tree loss: 1.748 | Accuracy: 0.443686 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.034 | Tree loss: 1.786 | Accuracy: 0.424000 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.034 | Tree loss: 1.781 | Accuracy: 0.410000 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.445000 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 1.718 | Reg loss: 0.034 | Tree loss: 1.718 | Accuracy: 0.422500 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.424500 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.442000 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.464500 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.447500 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.461500 | 0.247 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.450000 | 0.247 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.457338 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 1.811 | Reg loss: 0.034 | Tree loss: 1.811 | Accuracy: 0.396500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 1.777 | Reg loss: 0.034 | Tree loss: 1.777 | Accuracy: 0.415000 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.034 | Tree loss: 1.760 | Accuracy: 0.430500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.034 | Tree loss: 1.712 | Accuracy: 0.433500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.034 | Tree loss: 1.720 | Accuracy: 0.443500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.438500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.463500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.459500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.437000 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.463500 | 0.247 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.034 | Tree loss: 1.763 | Accuracy: 0.464164 | 0.247 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.034 | Tree loss: 1.773 | Accuracy: 0.422500 | 0.247 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.034 | Tree loss: 1.761 | Accuracy: 0.419500 | 0.247 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.034 | Tree loss: 1.770 | Accuracy: 0.419500 | 0.247 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.034 | Tree loss: 1.735 | Accuracy: 0.404000 | 0.247 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.429000 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.453000 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.459000 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.456000 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.455500 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.437500 | 0.246 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 1.607 | Reg loss: 0.034 | Tree loss: 1.607 | Accuracy: 0.508532 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.416000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.410000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 1.765 | Reg loss: 0.034 | Tree loss: 1.765 | Accuracy: 0.408500 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 1.720 | Reg loss: 0.034 | Tree loss: 1.720 | Accuracy: 0.424500 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.034 | Tree loss: 1.714 | Accuracy: 0.426500 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.450500 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.453000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 1.660 | Reg loss: 0.034 | Tree loss: 1.660 | Accuracy: 0.474000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.448000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.461000 | 0.246 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.034 | Tree loss: 1.646 | Accuracy: 0.522184 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.420000 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.426500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 1.747 | Reg loss: 0.034 | Tree loss: 1.747 | Accuracy: 0.412500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.034 | Tree loss: 1.768 | Accuracy: 0.418500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.034 | Tree loss: 1.730 | Accuracy: 0.431500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.448500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.440500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.450500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.034 | Tree loss: 1.659 | Accuracy: 0.471500 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.456000 | 0.246 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 1.588 | Reg loss: 0.034 | Tree loss: 1.588 | Accuracy: 0.529010 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 1.788 | Reg loss: 0.033 | Tree loss: 1.788 | Accuracy: 0.405500 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.417000 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 1.760 | Reg loss: 0.034 | Tree loss: 1.760 | Accuracy: 0.412000 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.433000 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.439500 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.457000 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.453500 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.455500 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.448500 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.459000 | 0.246 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 1.778 | Reg loss: 0.034 | Tree loss: 1.778 | Accuracy: 0.474403 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 1.799 | Reg loss: 0.033 | Tree loss: 1.799 | Accuracy: 0.413000 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.420500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.034 | Tree loss: 1.758 | Accuracy: 0.441500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.421000 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.440500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.432500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.456500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.441500 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.458000 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.469000 | 0.246 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.034 | Tree loss: 1.656 | Accuracy: 0.470990 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 1.796 | Reg loss: 0.033 | Tree loss: 1.796 | Accuracy: 0.419000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.414000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.414500 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.430500 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.450000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.473000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.453000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.450000 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.455500 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.452500 | 0.246 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.488055 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.423500 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.404000 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.418000 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 1.725 | Reg loss: 0.034 | Tree loss: 1.725 | Accuracy: 0.417500 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.441000 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.446500 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.428000 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.466500 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.459500 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.464000 | 0.246 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 1.617 | Reg loss: 0.034 | Tree loss: 1.617 | Accuracy: 0.508532 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.419500 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.410000 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.434000 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.433000 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.034 | Tree loss: 1.702 | Accuracy: 0.452500 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.450500 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.463500 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.446000 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.465500 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.456000 | 0.246 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.453925 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.423000 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.414000 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.412500 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.427500 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.436500 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.430500 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.451000 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.463000 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.443000 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.465500 | 0.246 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.443686 | 0.246 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.425500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.401000 | 0.246 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.423500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.034 | Tree loss: 1.712 | Accuracy: 0.427500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.458500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.437000 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.456000 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.460000 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.466500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.466500 | 0.246 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.477816 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.430500 | 0.246 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.422000 | 0.246 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.425000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 1.726 | Reg loss: 0.034 | Tree loss: 1.726 | Accuracy: 0.408000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.439000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.441000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.456000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.463000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.034 | Tree loss: 1.705 | Accuracy: 0.449000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.475000 | 0.245 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.034 | Tree loss: 1.650 | Accuracy: 0.481229 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.434000 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.416500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.437000 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.425500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.034 | Tree loss: 1.721 | Accuracy: 0.437500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.461500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.434500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.463000 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.482000 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.464500 | 0.245 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.034 | Tree loss: 1.654 | Accuracy: 0.491468 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.421500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.409500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.452500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.444000 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.445500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.441000 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.034 | Tree loss: 1.670 | Accuracy: 0.456000 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.446500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.454500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.452500 | 0.245 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 1.590 | Reg loss: 0.034 | Tree loss: 1.590 | Accuracy: 0.464164 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.420000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.422000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.034 | Tree loss: 1.725 | Accuracy: 0.426000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.034 | Tree loss: 1.729 | Accuracy: 0.421500 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.034 | Tree loss: 1.718 | Accuracy: 0.420500 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.468000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.460000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.463000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 1.660 | Reg loss: 0.034 | Tree loss: 1.660 | Accuracy: 0.465000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.463000 | 0.245 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.443686 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 1.783 | Reg loss: 0.034 | Tree loss: 1.783 | Accuracy: 0.409000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.418000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.034 | Tree loss: 1.766 | Accuracy: 0.416500 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.034 | Tree loss: 1.739 | Accuracy: 0.411000 | 0.245 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.438500 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.469000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.455000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.476000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.467000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.034 | Tree loss: 1.654 | Accuracy: 0.483000 | 0.245 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.450512 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.427500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.413500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.428500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.034 | Tree loss: 1.711 | Accuracy: 0.447500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.034 | Tree loss: 1.725 | Accuracy: 0.431500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.443500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.427500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.458500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.468500 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.460000 | 0.245 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.477816 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.423500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.402500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.416500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.421500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.443000 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.460500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.468500 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.458000 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.034 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.471000 | 0.245 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 1.638 | Reg loss: 0.034 | Tree loss: 1.638 | Accuracy: 0.457338 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 1.787 | Reg loss: 0.033 | Tree loss: 1.787 | Accuracy: 0.412500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.411000 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.432000 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.454000 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 1.716 | Reg loss: 0.034 | Tree loss: 1.716 | Accuracy: 0.432000 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.433500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.467500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.470500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.034 | Tree loss: 1.661 | Accuracy: 0.466500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.034 | Tree loss: 1.690 | Accuracy: 0.457500 | 0.245 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.457338 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.432500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.414000 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.411500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.415000 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.442000 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.454500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.466500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.034 | Tree loss: 1.697 | Accuracy: 0.461500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.452000 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.034 | Tree loss: 1.664 | Accuracy: 0.454500 | 0.245 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.034 | Tree loss: 1.689 | Accuracy: 0.447099 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.413500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.434500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.422500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.422500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.435000 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.446000 | 0.245 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.456500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.034 | Tree loss: 1.656 | Accuracy: 0.466000 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.034 | Tree loss: 1.651 | Accuracy: 0.476000 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.453500 | 0.245 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.474403 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.408000 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.410000 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.411000 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.417500 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.447500 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.453000 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.034 | Tree loss: 1.657 | Accuracy: 0.473000 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.446500 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.465500 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.467500 | 0.245 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.436860 | 0.245 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.451000 | 0.245 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.392000 | 0.245 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.413500 | 0.245 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.432000 | 0.245 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 1.719 | Reg loss: 0.034 | Tree loss: 1.719 | Accuracy: 0.437500 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.452000 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.451000 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.468500 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.474500 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.471500 | 0.244 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 1.704 | Reg loss: 0.034 | Tree loss: 1.704 | Accuracy: 0.430034 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.424500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.403500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.423000 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.422000 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.440500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.439000 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.034 | Tree loss: 1.701 | Accuracy: 0.443500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.456500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.456000 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.467500 | 0.244 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.034 | Tree loss: 1.649 | Accuracy: 0.477816 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.424500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.400500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.421000 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.433000 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.449500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.446500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.473000 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.463000 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.462500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.468500 | 0.244 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.423208 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.420500 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.412000 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.401000 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.427000 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.435500 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.427500 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.457000 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.460000 | 0.244 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.456500 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.034 | Tree loss: 1.661 | Accuracy: 0.484000 | 0.244 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.436860 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.416000 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.418000 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.422500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.426500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.415000 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.447000 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.458500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.034 | Tree loss: 1.654 | Accuracy: 0.456500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.470500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.034 | Tree loss: 1.655 | Accuracy: 0.472500 | 0.244 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.505119 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.412000 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.425500 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.415500 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.434000 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.433000 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.436500 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.034 | Tree loss: 1.682 | Accuracy: 0.444500 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.034 | Tree loss: 1.658 | Accuracy: 0.475000 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.475000 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.445500 | 0.244 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.034 | Tree loss: 1.649 | Accuracy: 0.460751 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.424000 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.423500 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.409500 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.431000 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.430500 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.450000 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.462500 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.459500 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.458000 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.034 | Tree loss: 1.673 | Accuracy: 0.461000 | 0.244 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 1.576 | Reg loss: 0.034 | Tree loss: 1.576 | Accuracy: 0.515358 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.421500 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.429000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.437500 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.417000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.435000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.432000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.457000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.464500 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.455000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.034 | Tree loss: 1.680 | Accuracy: 0.448000 | 0.244 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.412969 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.413000 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.405500 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.424000 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.428000 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.427000 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.448500 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.443500 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.469500 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.034 | Tree loss: 1.670 | Accuracy: 0.466000 | 0.244 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.447500 | 0.244 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 1.569 | Reg loss: 0.034 | Tree loss: 1.569 | Accuracy: 0.491468 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.423000 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.394500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.425500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.444000 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.452000 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.457500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.431500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.034 | Tree loss: 1.655 | Accuracy: 0.474500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.478500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.457500 | 0.244 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 1.581 | Reg loss: 0.034 | Tree loss: 1.581 | Accuracy: 0.453925 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.397500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.413500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.410500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.446500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.446000 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 1.655 | Reg loss: 0.034 | Tree loss: 1.655 | Accuracy: 0.469500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.449500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.034 | Tree loss: 1.699 | Accuracy: 0.434000 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.461500 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.468000 | 0.244 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 1.652 | Reg loss: 0.034 | Tree loss: 1.652 | Accuracy: 0.498294 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.417500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.429500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.445000 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.412500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.421000 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.455500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.034 | Tree loss: 1.659 | Accuracy: 0.472500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.446500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.034 | Tree loss: 1.648 | Accuracy: 0.447000 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.034 | Tree loss: 1.664 | Accuracy: 0.453500 | 0.244 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.430034 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.425000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.423500 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.428500 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.436000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.443000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.445000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.034 | Tree loss: 1.692 | Accuracy: 0.456000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.453500 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.464000 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.462500 | 0.244 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 1.647 | Reg loss: 0.034 | Tree loss: 1.647 | Accuracy: 0.470990 | 0.244 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.433000 | 0.244 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.416500 | 0.244 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.429000 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.432500 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.432000 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.454500 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.034 | Tree loss: 1.670 | Accuracy: 0.460000 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.034 | Tree loss: 1.670 | Accuracy: 0.464000 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.454000 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.456500 | 0.243 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.443686 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.417000 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.436500 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.431500 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.422500 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.441000 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.453500 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.456500 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.034 | Tree loss: 1.696 | Accuracy: 0.445000 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.457000 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.034 | Tree loss: 1.646 | Accuracy: 0.466000 | 0.243 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.034 | Tree loss: 1.734 | Accuracy: 0.447099 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.411500 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.411500 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.426000 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.444000 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.438000 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.453500 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.439000 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.459000 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.034 | Tree loss: 1.687 | Accuracy: 0.476500 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.469500 | 0.243 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 1.628 | Reg loss: 0.034 | Tree loss: 1.628 | Accuracy: 0.488055 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 1.784 | Reg loss: 0.033 | Tree loss: 1.784 | Accuracy: 0.402000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.426000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.433000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.431500 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.440000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.451000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.456000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.034 | Tree loss: 1.679 | Accuracy: 0.454000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.034 | Tree loss: 1.661 | Accuracy: 0.473000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.465000 | 0.243 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.034 | Tree loss: 1.654 | Accuracy: 0.460751 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.419500 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.424000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.442500 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.440000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.443000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.440000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.456000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.448500 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.480500 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.443000 | 0.243 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 1.637 | Reg loss: 0.034 | Tree loss: 1.637 | Accuracy: 0.477816 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.412500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.431000 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.428500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.415500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.445500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.439500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.461500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.464000 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.484000 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.457500 | 0.243 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.034 | Tree loss: 1.657 | Accuracy: 0.481229 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.414500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.418000 | 0.243 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.426500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.443000 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.428000 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.456500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.467500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.443500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.475500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.481500 | 0.243 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.426621 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.427500 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.413000 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.432000 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.428000 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.448500 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.449500 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.457500 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.468000 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.461500 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.478000 | 0.243 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 1.591 | Reg loss: 0.034 | Tree loss: 1.591 | Accuracy: 0.474403 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.427500 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.418000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.440000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.423000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.456500 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.454000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.445500 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.454000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.460500 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.448000 | 0.243 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.470990 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.423500 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.418000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.427500 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.422000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.451000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.445000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.468000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.463000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.468000 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.446500 | 0.243 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 1.645 | Reg loss: 0.034 | Tree loss: 1.645 | Accuracy: 0.457338 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.417500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.426500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.412500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.435000 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.434500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.456000 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.450000 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.459500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.468500 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.034 | Tree loss: 1.709 | Accuracy: 0.438000 | 0.243 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.034 | Tree loss: 1.710 | Accuracy: 0.430034 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.426500 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.423000 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.429000 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.424500 | 0.243 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.442000 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.451000 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.449500 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.474500 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.456500 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.451500 | 0.243 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 1.722 | Reg loss: 0.034 | Tree loss: 1.722 | Accuracy: 0.423208 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.417000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.410000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.419000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.424000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.469000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.462000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.457500 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.489500 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.458500 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.433000 | 0.243 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.034 | Tree loss: 1.737 | Accuracy: 0.447099 | 0.243 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.409000 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.433500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.433000 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.435500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.434000 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.426500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.472500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.464500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.448500 | 0.243 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.444000 | 0.242 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.034 | Tree loss: 1.666 | Accuracy: 0.467577 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.410000 | 0.243 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.419000 | 0.243 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.442000 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.438500 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.435500 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.462000 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.465500 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.449500 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.464500 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.034 | Tree loss: 1.676 | Accuracy: 0.460000 | 0.242 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.034 | Tree loss: 1.700 | Accuracy: 0.436860 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.425000 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.416500 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.419000 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.423500 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.420000 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.461500 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.452000 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.433000 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.464500 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.467500 | 0.242 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 1.622 | Reg loss: 0.034 | Tree loss: 1.622 | Accuracy: 0.467577 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.410000 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.434000 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.416500 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.421500 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.445500 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.460000 | 0.242 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.450000 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.457500 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.453500 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.465000 | 0.242 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.034 | Tree loss: 1.768 | Accuracy: 0.385666 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.405500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.424500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.404500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.419500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.436500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.466000 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.460500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.483000 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.465000 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.456500 | 0.242 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.034 | Tree loss: 1.757 | Accuracy: 0.402730 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.402000 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.428500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.431000 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.429500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.451500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.446000 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.453000 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.452500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.461500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.472500 | 0.242 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.433447 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.412000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.416500 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.426000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.444000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.452500 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.456000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.450000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.450500 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.472000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.450000 | 0.242 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.426621 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.423000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.436000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.435500 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.414000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.444000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.454500 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.451000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.450000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.453000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.457000 | 0.242 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.481229 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.416500 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.405500 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.420000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.419000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.454500 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.462000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.456000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.474500 | 0.242 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.463000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.464000 | 0.242 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.412969 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.409500 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.416000 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.437500 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.429500 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.460000 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.455000 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.464000 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.448500 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.449500 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.448000 | 0.242 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 1.600 | Reg loss: 0.034 | Tree loss: 1.600 | Accuracy: 0.515358 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.407000 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.401000 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.446500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.428500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.457500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.446500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.463000 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.469500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.473500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.455500 | 0.242 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.477816 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.421500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.410000 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.414000 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.441500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.465500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.427000 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.457500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.451500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.462000 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.441500 | 0.242 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 1.580 | Reg loss: 0.033 | Tree loss: 1.580 | Accuracy: 0.508532 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.416000 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.434500 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.417500 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.414500 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.427500 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.452000 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.460000 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.463000 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.456500 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.242 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.474403 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 1.797 | Reg loss: 0.033 | Tree loss: 1.797 | Accuracy: 0.405500 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.420500 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.417000 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.432000 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.462500 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.461000 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.475500 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.445000 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.465500 | 0.242 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.464000 | 0.242 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.426621 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.414500 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.391000 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.423000 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.441500 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.452500 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.449000 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.446000 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.477500 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.482000 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.449500 | 0.242 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.033 | Tree loss: 1.629 | Accuracy: 0.460751 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.412500 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.412500 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.438500 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.436000 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.444500 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.473000 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.454000 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.439000 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.475500 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.453000 | 0.242 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.484642 | 0.242 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.408000 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.426500 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.441000 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.442000 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.431500 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.443500 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.460000 | 0.242 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.461000 | 0.241 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.476000 | 0.241 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.461000 | 0.241 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.034 | Tree loss: 1.660 | Accuracy: 0.426621 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.408000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.411500 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.429000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.450000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.442000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.453000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.442500 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.468500 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.459000 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.476500 | 0.241 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.436860 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 1.790 | Reg loss: 0.033 | Tree loss: 1.790 | Accuracy: 0.399000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.404000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.431000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.419000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.415500 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.452000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.477500 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.469000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.481000 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.460500 | 0.241 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.436860 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.424000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.428000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.427000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.462500 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.436500 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.453000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.448500 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.469000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.461500 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.034 | Tree loss: 1.686 | Accuracy: 0.451000 | 0.241 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 1.736 | Reg loss: 0.034 | Tree loss: 1.736 | Accuracy: 0.440273 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.413000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.424500 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.428000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.413000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.436000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.455000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.445000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.474500 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.456000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.454000 | 0.241 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 1.608 | Reg loss: 0.034 | Tree loss: 1.608 | Accuracy: 0.535836 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.420500 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.440500 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.426000 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.426500 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.440500 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.431000 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.473000 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.455500 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.465000 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 1.652 | Reg loss: 0.034 | Tree loss: 1.652 | Accuracy: 0.483000 | 0.241 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.034 | Tree loss: 1.707 | Accuracy: 0.457338 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.420500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.418500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.423000 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.423500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.433500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.475500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.454000 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.444000 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.472500 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.034 | Tree loss: 1.672 | Accuracy: 0.468000 | 0.241 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 1.718 | Reg loss: 0.034 | Tree loss: 1.718 | Accuracy: 0.430034 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.418500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.413500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.432500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.437000 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.459500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.442000 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.458500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.467500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.448500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.472500 | 0.241 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.034 | Tree loss: 1.740 | Accuracy: 0.467577 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.440000 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.420500 | 0.241 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.420500 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.416500 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.454000 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.448000 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.464000 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.459500 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.456500 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.034 | Tree loss: 1.673 | Accuracy: 0.467000 | 0.241 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 1.627 | Reg loss: 0.034 | Tree loss: 1.627 | Accuracy: 0.494881 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.408500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.433500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.417500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.434000 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.454000 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.443500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.449500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.469500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.456500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.034 | Tree loss: 1.664 | Accuracy: 0.467500 | 0.241 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.034 | Tree loss: 1.694 | Accuracy: 0.453925 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.431000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.430000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.427000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.427500 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.447500 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.441000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.453000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.470500 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.444000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.034 | Tree loss: 1.669 | Accuracy: 0.453000 | 0.241 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.474403 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.401000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.414500 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.435000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.451000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.442500 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.429500 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.468500 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.456000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.475000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.465000 | 0.241 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.034 | Tree loss: 1.661 | Accuracy: 0.470990 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.427000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.415500 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.425000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.441000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.469000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.443000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.454500 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.446500 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.459500 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.460000 | 0.241 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.034 | Tree loss: 1.683 | Accuracy: 0.419795 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.423500 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.425500 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.430000 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.427000 | 0.241 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.429000 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.450500 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.440000 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.480000 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.452500 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.477500 | 0.241 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.034 | Tree loss: 1.642 | Accuracy: 0.501706 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.425000 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.434500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.417500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.423000 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.445000 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.444500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.464500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.456000 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.479500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.459500 | 0.241 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.399317 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 1.781 | Reg loss: 0.033 | Tree loss: 1.781 | Accuracy: 0.417500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.422500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.443500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.432500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.413000 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.450500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.450000 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.470000 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.473000 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.456500 | 0.241 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.498294 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.415500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.427000 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.437500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.437000 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.439500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.432500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.434500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.465500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.470500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.463500 | 0.241 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.477816 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.428500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.429500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.406500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.430500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.439000 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.469000 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.454500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.454000 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.462500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.476500 | 0.241 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.453925 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.417500 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.421500 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.404000 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.442500 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.434500 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.442500 | 0.241 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.466000 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.456000 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.470000 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.465500 | 0.241 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 1.596 | Reg loss: 0.033 | Tree loss: 1.596 | Accuracy: 0.491468 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.416000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.416000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.434000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.442500 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.449000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.456500 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.442000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.459000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.449500 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.449000 | 0.241 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.474403 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.426500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.421000 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.420000 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.422500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.454500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.462500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.466000 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.448500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.460500 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.467000 | 0.241 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.409556 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.427500 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.423500 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.441000 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.430000 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.444000 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.451000 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.438500 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.433500 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.472500 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.460000 | 0.241 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 1.604 | Reg loss: 0.033 | Tree loss: 1.604 | Accuracy: 0.467577 | 0.241 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.418500 | 0.241 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.427000 | 0.241 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.417000 | 0.241 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.417500 | 0.241 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.439000 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.449000 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.470500 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.485500 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.463000 | 0.24 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.453925 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.424500 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.436000 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.406000 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.425500 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.420500 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.459000 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.459500 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.472000 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.471500 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.459000 | 0.24 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.426621 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.409500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.434500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.436000 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.427500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.447000 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.458000 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.450500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.444500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.471000 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.472500 | 0.24 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 1.608 | Reg loss: 0.033 | Tree loss: 1.608 | Accuracy: 0.494881 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.411500 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.415000 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.405000 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.432500 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.439500 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.465000 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.471500 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.461000 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.462000 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.468500 | 0.24 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.416382 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.424000 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.409500 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.440000 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.428000 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.420000 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.442500 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.454500 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.477500 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.472000 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.487500 | 0.24 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.450512 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.417000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.424000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.420000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.438500 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.457500 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.458500 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.450000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.455500 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.468000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.468000 | 0.24 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.481229 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.429500 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.413000 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.423000 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.437000 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.444500 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.445500 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.454500 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.459500 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 1.620 | Reg loss: 0.033 | Tree loss: 1.620 | Accuracy: 0.482000 | 0.24 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.464000 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.460751 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.399500 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.405000 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.426000 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.453500 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.455500 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.458500 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.458500 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.484000 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.478000 | 0.24 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.416382 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.407500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.440500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.415500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.440500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.433000 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.459500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.463500 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.468000 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.466000 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.473000 | 0.24 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.457338 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.435000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.414000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.417500 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.446500 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.433500 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.434500 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.460000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.469000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.452000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.478000 | 0.24 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 1.620 | Reg loss: 0.033 | Tree loss: 1.620 | Accuracy: 0.460751 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.426000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.424000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.410500 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.432000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.451500 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.464500 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.468000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.472000 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.491500 | 0.24 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.447099 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.426500 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.429000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.422000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.421000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.452500 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.450500 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.470000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.464500 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.446000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.451000 | 0.24 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.453925 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.423500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.422000 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.428500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.434500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.452500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.452000 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.448500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.455500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.475500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.459500 | 0.24 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.453925 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.426500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.418500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.429500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.431000 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.437500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.440000 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.455500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.481000 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.453500 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.463000 | 0.24 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.395904 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.405000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.440000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.438500 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.442000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.461000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.450000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.449000 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.464500 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.463500 | 0.24 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.477816 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 1.785 | Reg loss: 0.033 | Tree loss: 1.785 | Accuracy: 0.404500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.420500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.417000 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.445500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.469500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.450500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.456000 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.460500 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.455000 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.445000 | 0.24 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.477816 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.408000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.413000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.422500 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.440500 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.432000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.459000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.471500 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.444000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.467500 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.467000 | 0.24 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.457338 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.429500 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.429500 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.404500 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.413500 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.446000 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.440000 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.450000 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.459000 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.457000 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.475500 | 0.24 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 1.578 | Reg loss: 0.033 | Tree loss: 1.578 | Accuracy: 0.529010 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.426000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.431500 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.417500 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.422000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.428000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.460000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.461000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.455500 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.463500 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.471000 | 0.24 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.436860 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.407500 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.420500 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.424500 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.452000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.444000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.464000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.448000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.468000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.465000 | 0.24 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.474403 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.403500 | 0.24 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.436500 | 0.24 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.427500 | 0.24 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.437000 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.449500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.458500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.456500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.452500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.462500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.462500 | 0.239 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.464164 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.419500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.426500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.410500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.424500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.449000 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.460500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.459000 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.476500 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.475000 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.462000 | 0.24 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.484642 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.429000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.420000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.464000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.408000 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.442000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.447000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.449000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.472000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.464500 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.481000 | 0.24 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.447099 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.418000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.429500 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.427000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.438000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.454500 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.457000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.452000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.461000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.453000 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.472500 | 0.24 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.443686 | 0.24 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.421500 | 0.24 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.428500 | 0.24 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.427500 | 0.24 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.439500 | 0.24 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.432500 | 0.24 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.468000 | 0.239 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.473000 | 0.239 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.454000 | 0.239 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.423000 | 0.239 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.471500 | 0.239 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.474403 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.426000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.412000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.427000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.435500 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.439500 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.458500 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.451000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.475000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.463000 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.476500 | 0.239 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.460751 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.033 | Tree loss: 1.806 | Accuracy: 0.401000 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.442500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.419000 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.433000 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.463500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.461500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.472500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.464500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.449500 | 0.239 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.457338 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.415000 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.430500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.424500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.451000 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.447000 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.443000 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.453500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.463500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.456500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.473500 | 0.239 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.467577 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.416500 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.410500 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.416000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.437000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.450000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.457000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.464000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.459500 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.461500 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.464000 | 0.239 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.494881 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.417000 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.413000 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.411500 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.446000 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.451000 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.439000 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.459500 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.487500 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.463500 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.474500 | 0.239 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.457338 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.410500 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.430000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.433500 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.434500 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.436000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.467000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.458000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.459500 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.483000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.448000 | 0.239 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.470990 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.420500 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.417000 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.439500 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.433000 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.455500 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.423000 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.466500 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.466000 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.474000 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.470500 | 0.239 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.460751 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.438000 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.431000 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.414500 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.420000 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.416000 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.465500 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.481500 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.460500 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.442500 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.475500 | 0.239 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.443686 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.033 | Tree loss: 1.791 | Accuracy: 0.407500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.424500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.422500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.445500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.472500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.462000 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.465500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.458000 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.451000 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.466500 | 0.239 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.464164 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.395500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.416500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.436500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.436500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.456000 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.467500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.473500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.472000 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.491500 | 0.239 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.474403 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.414500 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.399000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.433000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.420000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.480500 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.439500 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.463000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.477000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.485000 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.467500 | 0.239 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.508532 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.425500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.421500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.427500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.406000 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.448000 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.458000 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.452500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.465500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.482000 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.480500 | 0.239 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.467577 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.424500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.430500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.423000 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.433000 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.445500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.450500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.461500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.462500 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.486000 | 0.239 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.478500 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.477816 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.434000 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.411500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.423500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.446000 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.450500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.455500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.440500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.465000 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.474000 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.470500 | 0.239 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.460751 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.424500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.428500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.435000 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.438500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.452500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.467500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.454000 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.432500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.470500 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.472000 | 0.239 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.481229 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.427000 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.419500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.427500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.446500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.456500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.457000 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.451500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.463000 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.473500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.461500 | 0.239 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 1.577 | Reg loss: 0.033 | Tree loss: 1.577 | Accuracy: 0.522184 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.421500 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.420000 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.422000 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.452000 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.453000 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.456500 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.468500 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.468500 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.469500 | 0.239 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.467577 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.410500 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.400500 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.435500 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.424000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.449000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.471000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.459000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.469500 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.465000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 1.619 | Reg loss: 0.033 | Tree loss: 1.619 | Accuracy: 0.503000 | 0.239 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.477816 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.430000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.420000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.420500 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.426000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.438000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.457000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.443000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.473000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.462500 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.488000 | 0.239 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.430034 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.410500 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.444000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.432500 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.415000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.459000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.448500 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.453500 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.447000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.479000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.498000 | 0.239 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.522184 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 1.780 | Reg loss: 0.033 | Tree loss: 1.780 | Accuracy: 0.408000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.430500 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.419500 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.448500 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.467000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.461000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.465500 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.469000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.472000 | 0.239 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.484642 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.400500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.421500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.446000 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.458000 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.454500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.467500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.452500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.482000 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.478500 | 0.239 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.464164 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.403000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.419500 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.434500 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.422000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.460000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.451000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.451500 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.466500 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.455000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.473000 | 0.239 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.436860 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.443500 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.413500 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.430500 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.432000 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.447000 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.452500 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.464000 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.453500 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.466500 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.473000 | 0.239 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.501706 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.428000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.436000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.427500 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.442000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.461500 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.457000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.445000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.463500 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.445000 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.479500 | 0.239 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.467577 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.434500 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.415000 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.440500 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.438000 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.444500 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.428000 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.456500 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.474000 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.476500 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.479000 | 0.239 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.392491 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.408000 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.437500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.422000 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.428500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.432500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.446500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.468000 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.471500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.442500 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.474000 | 0.239 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.491468 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.422500 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.438000 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.440000 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.427000 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.426000 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.460000 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.460500 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.463500 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.478500 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.454500 | 0.239 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.464164 | 0.239 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.420500 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.435000 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.407500 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.440500 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.443000 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.423000 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.439000 | 0.239 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.471500 | 0.238 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.480500 | 0.238 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.473000 | 0.238 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 1.629 | Reg loss: 0.033 | Tree loss: 1.629 | Accuracy: 0.488055 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.421500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.413500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.451000 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.444000 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.435500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.469500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.450000 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.481500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.471500 | 0.238 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 1.622 | Reg loss: 0.033 | Tree loss: 1.622 | Accuracy: 0.488055 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.427500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.422500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.424000 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.432500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.455500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.464000 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.458500 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.465000 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.491000 | 0.238 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 1.594 | Reg loss: 0.033 | Tree loss: 1.594 | Accuracy: 0.477816 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.411000 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.417500 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.425500 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.450000 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.461500 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.441500 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.460000 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.459500 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.467000 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.471000 | 0.238 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.457338 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.426000 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.421500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.408500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.443500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.440000 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.446000 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.467500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.478500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.456500 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.479000 | 0.238 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 1.611 | Reg loss: 0.033 | Tree loss: 1.611 | Accuracy: 0.467577 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.425000 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.416000 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.425000 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.434500 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.428000 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.456000 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.463500 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.483000 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.459500 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.483500 | 0.238 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.474403 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.410500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.427500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.429000 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.439000 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.440000 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.459500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.474500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.490500 | 0.238 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.467577 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.415000 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.429500 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.431500 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.424000 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.456000 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.458500 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.471000 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.455500 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.484000 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.462500 | 0.238 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 1.607 | Reg loss: 0.033 | Tree loss: 1.607 | Accuracy: 0.494881 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.413500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.435000 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.425500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.414000 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.451000 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.463500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.430500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.478000 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.479500 | 0.238 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.457338 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.438500 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.410000 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.408000 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.436000 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.474500 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.456500 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.456000 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.467500 | 0.238 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.460751 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.416500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.396500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.421500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.468500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.454500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.470000 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.447000 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.458500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.481500 | 0.238 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 1.552 | Reg loss: 0.033 | Tree loss: 1.552 | Accuracy: 0.494881 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.423000 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.442000 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.400500 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.422000 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.445500 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.461500 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.453000 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.461000 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.468500 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.467500 | 0.238 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 1.606 | Reg loss: 0.033 | Tree loss: 1.606 | Accuracy: 0.481229 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.408000 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.422500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.429500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.447500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.455000 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.443500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.458000 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.469000 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.470500 | 0.238 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.423208 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.418500 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.417500 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.437000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.425500 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.455000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.456000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.437000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.459000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.485000 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.479500 | 0.238 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.488055 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.435000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.420000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.433000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.423000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.438000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.435000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.453500 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.454000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.483500 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.461000 | 0.238 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.474403 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.397500 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.425000 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.433500 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.437500 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.470000 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.455000 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.465000 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.478500 | 0.238 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.480000 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.453925 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.406000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.423500 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.426000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.425500 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.451000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.475500 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.447000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.470000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.444500 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.485000 | 0.238 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.470990 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 1.786 | Reg loss: 0.033 | Tree loss: 1.786 | Accuracy: 0.393500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.412000 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.443500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.422000 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.448500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.482500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.466500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.462000 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.462500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.472500 | 0.238 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 1.605 | Reg loss: 0.033 | Tree loss: 1.605 | Accuracy: 0.508532 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.428500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.392500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.414000 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.449000 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.469000 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.479500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.460000 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.470500 | 0.238 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.470990 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.421000 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.422000 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.419500 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.456500 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.441500 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.446500 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.448000 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.461500 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.455000 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.453000 | 0.238 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 1.573 | Reg loss: 0.033 | Tree loss: 1.573 | Accuracy: 0.511945 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.434000 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.427500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.418500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.432500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.462500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.449000 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 1.626 | Reg loss: 0.033 | Tree loss: 1.626 | Accuracy: 0.482000 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.457500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.447500 | 0.238 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 1.623 | Reg loss: 0.033 | Tree loss: 1.623 | Accuracy: 0.511945 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.443000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.423000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.451500 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.438000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.422000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.444500 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.459500 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.451500 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.462000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.473000 | 0.238 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.436860 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.418500 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.399500 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.442500 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.433500 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.448000 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.457500 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.448000 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.469000 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.472000 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.482000 | 0.238 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.460751 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.410500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.427500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.417500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.437500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.466500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.463000 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.434500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.456000 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.466000 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.499500 | 0.238 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.385666 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.422000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.412500 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.420500 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.437000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.459000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.461000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.458000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.495000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.443000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.472000 | 0.238 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.443686 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.427000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.418500 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.436000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.424000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.446000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.428000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.469000 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.456500 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.469500 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.452500 | 0.238 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.494881 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.423000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.429000 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.429000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.445000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.443500 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.446000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.458000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.478000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.474000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.465000 | 0.238 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.474403 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.403500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.416000 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.423500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.437500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.456000 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.449500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.455500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.476500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.463500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.466500 | 0.238 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 1.621 | Reg loss: 0.033 | Tree loss: 1.621 | Accuracy: 0.508532 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.436500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.408500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.426000 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.424500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.456500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.450500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.444500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.453500 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.472000 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.473000 | 0.238 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.457338 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.419000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.420500 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.421500 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.421000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.461000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.463000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.461000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.462000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.465000 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.471500 | 0.238 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.443686 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.411500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.417500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.430500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.434500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.427000 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.471500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.458000 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.457500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.480000 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.477500 | 0.238 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.447099 | 0.238 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.422500 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.452500 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.447500 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.428000 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.438000 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.446000 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.439000 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.436000 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.458000 | 0.238 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.480500 | 0.237 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 1.612 | Reg loss: 0.033 | Tree loss: 1.612 | Accuracy: 0.460751 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.442000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.422000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.425000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.439500 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.440000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.492500 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.459500 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.475000 | 0.237 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.467577 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.410000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.425000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.451500 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.434500 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.443500 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.454000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.464500 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.457000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.462000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.453000 | 0.237 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.436860 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.424000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.420000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.411000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.462500 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.457000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.453000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.444000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.483000 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.472500 | 0.237 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.501706 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.418000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.424000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.438000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.424500 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.451500 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.454000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.467000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.461500 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.474500 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.461000 | 0.237 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.457338 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.410000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.425500 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.442500 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.443000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.428000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.457000 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.464000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.444500 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.472000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.466000 | 0.237 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.477816 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.434500 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.434000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.426000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.426500 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.447000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.455500 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.436500 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.473000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.447099 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 1.782 | Reg loss: 0.033 | Tree loss: 1.782 | Accuracy: 0.406000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.411500 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.455000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.451000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.468000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.461500 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.457500 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.449000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 1.628 | Reg loss: 0.033 | Tree loss: 1.628 | Accuracy: 0.447099 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.415500 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.420500 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.433000 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.439000 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.460000 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.447500 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.469500 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.467000 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.444500 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.469000 | 0.237 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.440273 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.426500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.435000 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.412500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.423000 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.452500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.460000 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.455500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.458500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.481500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.460500 | 0.237 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 1.589 | Reg loss: 0.033 | Tree loss: 1.589 | Accuracy: 0.470990 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.433500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.424000 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.424500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.412500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.448500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.439500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.448000 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.445500 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.493500 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.490000 | 0.237 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 1.586 | Reg loss: 0.033 | Tree loss: 1.586 | Accuracy: 0.477816 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.417500 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.440000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.417000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.439000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.436000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.456500 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.469000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.465000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.451000 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.459500 | 0.237 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.457338 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.445000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.413000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.437000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.410500 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.447500 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.462500 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.449000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.461000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.452000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.489000 | 0.237 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.460751 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 1.784 | Reg loss: 0.033 | Tree loss: 1.784 | Accuracy: 0.400500 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.427000 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.437000 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.448000 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.435500 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.450500 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.470000 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.485500 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.479000 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.455500 | 0.237 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.447099 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.435000 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.409000 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.437500 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.464000 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.460000 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.452500 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.464000 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.457500 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.457500 | 0.237 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.498294 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.423500 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.435500 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.436000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.447000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.441500 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.449000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.466000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.463000 | 0.237 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.457500 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 1.616 | Reg loss: 0.033 | Tree loss: 1.616 | Accuracy: 0.474403 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.408500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.434000 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.442500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.456500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.460500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.478000 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.467000 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.467500 | 0.237 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 1.605 | Reg loss: 0.033 | Tree loss: 1.605 | Accuracy: 0.525597 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.419500 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.404000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.404000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.449000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.457000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 1.611 | Reg loss: 0.033 | Tree loss: 1.611 | Accuracy: 0.484000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.468500 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.478000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.467000 | 0.237 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 1.589 | Reg loss: 0.033 | Tree loss: 1.589 | Accuracy: 0.501706 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.421500 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.399000 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.436000 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.442000 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.455000 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.484500 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.474500 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.464500 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.460500 | 0.237 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.460751 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.404000 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.421500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.418500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.447500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.447500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.460000 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.473500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 1.627 | Reg loss: 0.033 | Tree loss: 1.627 | Accuracy: 0.480500 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.474000 | 0.237 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 1.803 | Reg loss: 0.033 | Tree loss: 1.803 | Accuracy: 0.433447 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.436000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.440500 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.434000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.447000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.465000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.449500 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.443000 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.443500 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.449500 | 0.237 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.460751 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 1.783 | Reg loss: 0.033 | Tree loss: 1.783 | Accuracy: 0.412500 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.426500 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.433000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.410000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.440000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.468500 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.439000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.468000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.469500 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.467000 | 0.237 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.457338 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.431500 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.424000 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.429500 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.423500 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.444500 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.451500 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.462000 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.465000 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.465000 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.472000 | 0.237 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.470990 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.422000 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.409500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.433500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.442000 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.444500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.439500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 1.629 | Reg loss: 0.033 | Tree loss: 1.629 | Accuracy: 0.469000 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.460500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.486000 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.450500 | 0.237 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.433447 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.423000 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 1.742 | Reg loss: 0.033 | Tree loss: 1.742 | Accuracy: 0.433000 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.427000 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.440500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.444500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.437500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.445000 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.457500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.461500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.456500 | 0.237 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.505119 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.420500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.445500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.429500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.434000 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.442500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.445500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.434000 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 1.621 | Reg loss: 0.033 | Tree loss: 1.621 | Accuracy: 0.483500 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.491468 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.411000 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.438000 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.420500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.428000 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.438000 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.457500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.459500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.463500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.480500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.450500 | 0.237 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 1.605 | Reg loss: 0.033 | Tree loss: 1.605 | Accuracy: 0.532423 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.411000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.420000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.436500 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.445000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.445500 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.455000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.465000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.475500 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.474000 | 0.237 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.450512 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.421500 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.421500 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.418000 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.432500 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.423000 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.451500 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.452000 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.476500 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.473000 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.481000 | 0.237 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.460751 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.410000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.449000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.448000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.458000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.441000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.458500 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.467500 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.455000 | 0.237 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 1.620 | Reg loss: 0.033 | Tree loss: 1.620 | Accuracy: 0.484642 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.420500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.429500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.438500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.443000 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.468000 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.454000 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.451500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.456500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.456000 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.459500 | 0.237 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.470990 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.429500 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.430500 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.452500 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.459000 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.443000 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.473500 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.459000 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.474000 | 0.237 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.481229 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.412500 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.433000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.410500 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.438000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.448000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.469500 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.459000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.472000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.464000 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.477500 | 0.237 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 1.620 | Reg loss: 0.033 | Tree loss: 1.620 | Accuracy: 0.433447 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.419500 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.438000 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.417000 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.442500 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.442000 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.470000 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.464500 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.474500 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.471000 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.462500 | 0.237 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.477816 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.413000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.426000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.438000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.429000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.459500 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.470000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.474500 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.468500 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.462000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.463000 | 0.237 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.464164 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.436500 | 0.237 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.427500 | 0.237 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.423000 | 0.237 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.449500 | 0.237 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.468000 | 0.237 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.473500 | 0.236 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.455500 | 0.236 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.456500 | 0.236 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.472500 | 0.236 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 1.615 | Reg loss: 0.033 | Tree loss: 1.615 | Accuracy: 0.508532 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.420500 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.422000 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.430500 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.444000 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.436500 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.455000 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.447000 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.472000 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.482500 | 0.236 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.508532 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.417500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.429500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.419500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.433500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.454500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.451500 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.473000 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.483000 | 0.236 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.443686 | 0.237 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.421500 | 0.237 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.420000 | 0.237 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.407500 | 0.237 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.426000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.436000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.477000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.440000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.471500 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.475000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.473000 | 0.236 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.440273 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.033 | Tree loss: 1.772 | Accuracy: 0.396000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.425500 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.449500 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.431500 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.458000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.462000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.465000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.491500 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.482000 | 0.236 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.477816 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.426000 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.429500 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.449500 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.440500 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.456500 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.478000 | 0.236 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.467577 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.426000 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.429500 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.442000 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.471000 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.461000 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.448000 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.469500 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.463500 | 0.236 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 1.580 | Reg loss: 0.033 | Tree loss: 1.580 | Accuracy: 0.484642 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.421000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.404500 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.422500 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.421500 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.433000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.467000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.450000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.454000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 1.628 | Reg loss: 0.033 | Tree loss: 1.628 | Accuracy: 0.477000 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.478500 | 0.236 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.447099 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.412500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 1.730 | Reg loss: 0.033 | Tree loss: 1.730 | Accuracy: 0.422000 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.440500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.437500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.438000 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.484000 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.462500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.479500 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.457338 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.431500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.430500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.441000 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.471500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.449500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.459500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.450500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.468500 | 0.236 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.477816 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.428000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.033 | Tree loss: 1.731 | Accuracy: 0.425500 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.425000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.443000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.452000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.460000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.466000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.487000 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.480500 | 0.236 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 1.611 | Reg loss: 0.033 | Tree loss: 1.611 | Accuracy: 0.488055 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.416000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.446000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.454000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.444500 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.482000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.466000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.469000 | 0.236 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.444500 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.467577 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.411500 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.427000 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.449000 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.427500 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.448500 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.456500 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.474000 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.478000 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.463500 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.467000 | 0.236 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.443686 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.420000 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.420000 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.412500 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.444500 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.470000 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.462500 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.451500 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.465000 | 0.236 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.498294 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.426000 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.433500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.440500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.460000 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.440000 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.462500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.477500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.457500 | 0.236 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.464164 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.418500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.413500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.432500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.448000 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.463000 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.456500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.464500 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.465000 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.469000 | 0.236 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 1.591 | Reg loss: 0.033 | Tree loss: 1.591 | Accuracy: 0.488055 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.406500 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.438000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.451500 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.467500 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.469000 | 0.236 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 1.595 | Reg loss: 0.033 | Tree loss: 1.595 | Accuracy: 0.522184 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.438500 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.420500 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.442000 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.432500 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.455500 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.463000 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.459000 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.464500 | 0.236 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.494881 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 1.761 | Reg loss: 0.033 | Tree loss: 1.761 | Accuracy: 0.413000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 1.744 | Reg loss: 0.033 | Tree loss: 1.744 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.447000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.461000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.460500 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.467500 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.464000 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 1.596 | Reg loss: 0.033 | Tree loss: 1.596 | Accuracy: 0.491468 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.411500 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.419000 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.418000 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.473500 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.454500 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.461500 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.477000 | 0.236 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.522184 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.410000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.433000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.450500 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.459500 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.450000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.466000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.460000 | 0.236 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.470990 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.033 | Tree loss: 1.773 | Accuracy: 0.424500 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.408000 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.427000 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.433000 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.476500 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.468500 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.455500 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.481500 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.464000 | 0.236 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.467577 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.418500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.403500 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.426500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.439000 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.446500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.450500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.459000 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.479000 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.459500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.452500 | 0.236 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.470990 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 1.763 | Reg loss: 0.033 | Tree loss: 1.763 | Accuracy: 0.406000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.414500 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.427500 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.449000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.459000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.445000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.448000 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.478500 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.457500 | 0.236 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.470990 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.417000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.421500 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.421500 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.431000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.446000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.446000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.468500 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.468000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.469500 | 0.236 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.457338 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.434000 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.427500 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.422500 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.429000 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.441000 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.447500 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.463000 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.465500 | 0.236 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.457338 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.419500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.423500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.417000 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.412500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.443500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.466500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.454000 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 1.630 | Reg loss: 0.033 | Tree loss: 1.630 | Accuracy: 0.466500 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.484000 | 0.236 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.033 | Tree loss: 1.713 | Accuracy: 0.443686 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.422000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.410500 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.433500 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.429500 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.466000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.456000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.460000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.459000 | 0.236 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.481229 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.428500 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.404500 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.456000 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.454500 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.447500 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 1.623 | Reg loss: 0.033 | Tree loss: 1.623 | Accuracy: 0.487000 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.463500 | 0.236 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.467577 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.408000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.421000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.419000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 1.690 | Reg loss: 0.033 | Tree loss: 1.690 | Accuracy: 0.440500 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.448000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.467000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.463500 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.445500 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.468000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.465000 | 0.236 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.498294 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 1.776 | Reg loss: 0.033 | Tree loss: 1.776 | Accuracy: 0.404500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.428000 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.441500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.444000 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.432500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.457500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.453500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.465500 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 1.624 | Reg loss: 0.033 | Tree loss: 1.624 | Accuracy: 0.481000 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.458000 | 0.236 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.474403 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.424000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.410000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 1.721 | Reg loss: 0.033 | Tree loss: 1.721 | Accuracy: 0.416000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 1.678 | Reg loss: 0.033 | Tree loss: 1.678 | Accuracy: 0.444000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.445000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.463000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.473000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.475500 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.471000 | 0.236 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.412969 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 1.774 | Reg loss: 0.033 | Tree loss: 1.774 | Accuracy: 0.418000 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.438500 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.418500 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.442500 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.442000 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.454000 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.459500 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.477000 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.477500 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.485000 | 0.236 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 1.628 | Reg loss: 0.033 | Tree loss: 1.628 | Accuracy: 0.477816 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 1.766 | Reg loss: 0.033 | Tree loss: 1.766 | Accuracy: 0.411500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.410500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.416500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.425500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.442500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.445000 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.448500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 1.626 | Reg loss: 0.033 | Tree loss: 1.626 | Accuracy: 0.501000 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.494500 | 0.236 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.457338 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 1.771 | Reg loss: 0.033 | Tree loss: 1.771 | Accuracy: 0.411000 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.425000 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.447500 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.437000 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.433500 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.457000 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.460500 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 1.627 | Reg loss: 0.033 | Tree loss: 1.627 | Accuracy: 0.476000 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.465500 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.479500 | 0.236 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.491468 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.432000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.403500 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.427000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.464000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.440000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.434000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.473000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.464000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.463000 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.485500 | 0.236 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 1.619 | Reg loss: 0.033 | Tree loss: 1.619 | Accuracy: 0.481229 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.420500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.430500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.445000 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.454500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.457500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.474500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.467500 | 0.236 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.488055 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.033 | Tree loss: 1.758 | Accuracy: 0.426000 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.432500 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.418500 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.455500 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.459500 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.488000 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.465500 | 0.236 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.443686 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 1.767 | Reg loss: 0.033 | Tree loss: 1.767 | Accuracy: 0.419000 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 1.712 | Reg loss: 0.033 | Tree loss: 1.712 | Accuracy: 0.419000 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.434500 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 1.716 | Reg loss: 0.033 | Tree loss: 1.716 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.433000 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.451000 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.470500 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.468500 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.475500 | 0.236 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 1.624 | Reg loss: 0.033 | Tree loss: 1.624 | Accuracy: 0.460751 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.033 | Tree loss: 1.778 | Accuracy: 0.423000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.430000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.412000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.436500 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.431500 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.453000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.468000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.478000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.466000 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.466500 | 0.236 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 1.583 | Reg loss: 0.033 | Tree loss: 1.583 | Accuracy: 0.522184 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.414000 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.421500 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.439500 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.418000 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.460000 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.476000 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.455000 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.473500 | 0.236 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.458000 | 0.235 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.463000 | 0.235 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.433447 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.418000 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.404000 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.434000 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.437500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.457500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.450000 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.452500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.465500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 1.642 | Reg loss: 0.033 | Tree loss: 1.642 | Accuracy: 0.475500 | 0.235 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.460751 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.033 | Tree loss: 1.757 | Accuracy: 0.415000 | 0.235 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.421500 | 0.235 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.425500 | 0.235 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.419500 | 0.235 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.453500 | 0.235 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.471000 | 0.236 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 1.622 | Reg loss: 0.033 | Tree loss: 1.622 | Accuracy: 0.475000 | 0.236 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.460500 | 0.236 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.463500 | 0.236 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.459000 | 0.236 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.453925 | 0.236 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.418000 | 0.236 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.429500 | 0.236 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.413000 | 0.236 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.455500 | 0.236 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.433000 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.445500 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.471500 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.476000 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.466500 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.460500 | 0.235 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.423208 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.424500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.418500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.404500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.432500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.438000 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 1.610 | Reg loss: 0.033 | Tree loss: 1.610 | Accuracy: 0.483000 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.467500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.481500 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.463000 | 0.235 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.464164 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.404500 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 1.722 | Reg loss: 0.033 | Tree loss: 1.722 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.433500 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.446000 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.439500 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.434000 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 1.637 | Reg loss: 0.033 | Tree loss: 1.637 | Accuracy: 0.473500 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.469000 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.481000 | 0.235 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.453925 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.428500 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.417000 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.423000 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.446500 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.430500 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.487000 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.444500 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.033 | Tree loss: 1.697 | Accuracy: 0.442000 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.033 | Tree loss: 1.720 | Accuracy: 0.467577 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 1.746 | Reg loss: 0.033 | Tree loss: 1.746 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.425500 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.434000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.462000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.439000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.448000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.465500 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 1.594 | Reg loss: 0.033 | Tree loss: 1.594 | Accuracy: 0.508532 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.033 | Tree loss: 1.777 | Accuracy: 0.411500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.437500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.421000 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.452000 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.441500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.461500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.447500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.465500 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.469000 | 0.235 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.470990 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.425500 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.411500 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.442000 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.033 | Tree loss: 1.677 | Accuracy: 0.441500 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.448000 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.456000 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.465000 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.459500 | 0.235 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.474403 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.413000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.410000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.434500 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 1.693 | Reg loss: 0.033 | Tree loss: 1.693 | Accuracy: 0.433500 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.453000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.471000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.460500 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.464500 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.456000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.450512 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.433500 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 1.754 | Reg loss: 0.033 | Tree loss: 1.754 | Accuracy: 0.428000 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.420500 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.432000 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.435500 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 1.640 | Reg loss: 0.033 | Tree loss: 1.640 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.467500 | 0.235 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.474403 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.424000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.429000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.435000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.438000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.441000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.447000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.468000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.462000 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.472500 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.464500 | 0.235 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.457338 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.417500 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.424000 | 0.235 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.418000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.437000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.452500 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.438000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.475500 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.467000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.481000 | 0.235 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.450512 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.419500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.416500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.434500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.437500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.440000 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.469500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.468000 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.471000 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.470500 | 0.235 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.416382 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.409500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.420500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 1.728 | Reg loss: 0.033 | Tree loss: 1.728 | Accuracy: 0.422000 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.440500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.453500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.464500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.459000 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.468500 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.481000 | 0.235 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.426621 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.410000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 1.745 | Reg loss: 0.033 | Tree loss: 1.745 | Accuracy: 0.436000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.425000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.440000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.449000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.479500 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.464000 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.033 | Tree loss: 1.683 | Accuracy: 0.458500 | 0.235 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.498294 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 1.769 | Reg loss: 0.033 | Tree loss: 1.769 | Accuracy: 0.430500 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 1.739 | Reg loss: 0.033 | Tree loss: 1.739 | Accuracy: 0.430500 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.420000 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.440500 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.445000 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.446000 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.454500 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 1.631 | Reg loss: 0.033 | Tree loss: 1.631 | Accuracy: 0.471000 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.479500 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.476000 | 0.235 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.419795 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 1.764 | Reg loss: 0.033 | Tree loss: 1.764 | Accuracy: 0.412000 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.436500 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.434500 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.420500 | 0.235 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.460000 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.458000 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.460000 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.443500 | 0.235 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 1.636 | Reg loss: 0.033 | Tree loss: 1.636 | Accuracy: 0.501706 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 1.779 | Reg loss: 0.033 | Tree loss: 1.779 | Accuracy: 0.402500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.428500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.422500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.413500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.450000 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 1.684 | Reg loss: 0.033 | Tree loss: 1.684 | Accuracy: 0.453000 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.459000 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.455000 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 1.623 | Reg loss: 0.033 | Tree loss: 1.623 | Accuracy: 0.487500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 1.624 | Reg loss: 0.033 | Tree loss: 1.624 | Accuracy: 0.483500 | 0.235 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 1.572 | Reg loss: 0.033 | Tree loss: 1.572 | Accuracy: 0.515358 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 1.762 | Reg loss: 0.033 | Tree loss: 1.762 | Accuracy: 0.419500 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 1.727 | Reg loss: 0.033 | Tree loss: 1.727 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 1.724 | Reg loss: 0.033 | Tree loss: 1.724 | Accuracy: 0.424000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 1.680 | Reg loss: 0.033 | Tree loss: 1.680 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.441500 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.440000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.446000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.458500 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.471000 | 0.235 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.436860 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.422000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.440000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.426000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.442000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.460500 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.433500 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.033 | Tree loss: 1.672 | Accuracy: 0.455000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.468000 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.474500 | 0.235 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.474403 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.407500 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 1.681 | Reg loss: 0.033 | Tree loss: 1.681 | Accuracy: 0.427500 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.450000 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.469000 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.468500 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.462500 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.458500 | 0.235 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.552901 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.438000 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.421500 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.435000 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.420000 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 1.652 | Reg loss: 0.033 | Tree loss: 1.652 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.454000 | 0.235 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.457000 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.462500 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.476000 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.454500 | 0.235 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.033 | Tree loss: 1.703 | Accuracy: 0.447099 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.430500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 1.733 | Reg loss: 0.033 | Tree loss: 1.733 | Accuracy: 0.429500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 1.715 | Reg loss: 0.033 | Tree loss: 1.715 | Accuracy: 0.433000 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.432500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.443500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.475000 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.452000 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.457500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.460500 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.480000 | 0.235 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 1.585 | Reg loss: 0.033 | Tree loss: 1.585 | Accuracy: 0.525597 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.427500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 1.714 | Reg loss: 0.033 | Tree loss: 1.714 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.440500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.442500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.457500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.471500 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.469000 | 0.235 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 1.608 | Reg loss: 0.033 | Tree loss: 1.608 | Accuracy: 0.488055 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.033 | Tree loss: 1.753 | Accuracy: 0.418500 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 1.741 | Reg loss: 0.033 | Tree loss: 1.741 | Accuracy: 0.431000 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 1.704 | Reg loss: 0.033 | Tree loss: 1.704 | Accuracy: 0.443500 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 1.694 | Reg loss: 0.033 | Tree loss: 1.694 | Accuracy: 0.439000 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.445000 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.473500 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.448000 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 1.669 | Reg loss: 0.033 | Tree loss: 1.669 | Accuracy: 0.444000 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.477500 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.473500 | 0.235 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.457338 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.428000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.417500 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.441000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 1.679 | Reg loss: 0.033 | Tree loss: 1.679 | Accuracy: 0.425500 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.436000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.463000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.456500 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.466000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.474000 | 0.235 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 1.633 | Reg loss: 0.033 | Tree loss: 1.633 | Accuracy: 0.470990 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.426500 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.413500 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 1.699 | Reg loss: 0.033 | Tree loss: 1.699 | Accuracy: 0.441500 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 1.709 | Reg loss: 0.033 | Tree loss: 1.709 | Accuracy: 0.430500 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 1.695 | Reg loss: 0.033 | Tree loss: 1.695 | Accuracy: 0.437500 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 1.660 | Reg loss: 0.033 | Tree loss: 1.660 | Accuracy: 0.454000 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.445000 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.479500 | 0.235 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 1.626 | Reg loss: 0.033 | Tree loss: 1.626 | Accuracy: 0.482000 | 0.235 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 1.607 | Reg loss: 0.033 | Tree loss: 1.607 | Accuracy: 0.491468 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.422000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.431000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.443000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.033 | Tree loss: 1.692 | Accuracy: 0.455000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 1.661 | Reg loss: 0.033 | Tree loss: 1.661 | Accuracy: 0.439500 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 1.622 | Reg loss: 0.033 | Tree loss: 1.622 | Accuracy: 0.476000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.432000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.477000 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.467500 | 0.235 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 1.647 | Reg loss: 0.033 | Tree loss: 1.647 | Accuracy: 0.518771 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.422500 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 1.751 | Reg loss: 0.033 | Tree loss: 1.751 | Accuracy: 0.420500 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.432000 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.451500 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 1.685 | Reg loss: 0.033 | Tree loss: 1.685 | Accuracy: 0.442500 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 1.676 | Reg loss: 0.033 | Tree loss: 1.676 | Accuracy: 0.445000 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.033 | Tree loss: 1.688 | Accuracy: 0.448000 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.453500 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.456000 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.470000 | 0.235 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 1.621 | Reg loss: 0.033 | Tree loss: 1.621 | Accuracy: 0.464164 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.423000 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 1.732 | Reg loss: 0.033 | Tree loss: 1.732 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.439000 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.452500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.430000 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 1.668 | Reg loss: 0.033 | Tree loss: 1.668 | Accuracy: 0.449500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 1.648 | Reg loss: 0.033 | Tree loss: 1.648 | Accuracy: 0.452500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 1.641 | Reg loss: 0.033 | Tree loss: 1.641 | Accuracy: 0.444500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.470500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 1.529 | Reg loss: 0.033 | Tree loss: 1.529 | Accuracy: 0.511945 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 1.734 | Reg loss: 0.033 | Tree loss: 1.734 | Accuracy: 0.419500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.427000 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.033 | Tree loss: 1.706 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.425000 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 1.650 | Reg loss: 0.033 | Tree loss: 1.650 | Accuracy: 0.450500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.033 | Tree loss: 1.686 | Accuracy: 0.435500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.471000 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 1.653 | Reg loss: 0.033 | Tree loss: 1.653 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 1.632 | Reg loss: 0.033 | Tree loss: 1.632 | Accuracy: 0.473500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.033 | Tree loss: 1.691 | Accuracy: 0.458500 | 0.235 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 1.655 | Reg loss: 0.033 | Tree loss: 1.655 | Accuracy: 0.412969 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.421000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.417500 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.424000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.033 | Tree loss: 1.701 | Accuracy: 0.418500 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.463000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.468000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 1.659 | Reg loss: 0.033 | Tree loss: 1.659 | Accuracy: 0.467000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.453000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 1.645 | Reg loss: 0.033 | Tree loss: 1.645 | Accuracy: 0.465000 | 0.235 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.033 | Tree loss: 1.666 | Accuracy: 0.458000 | 0.235 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 1.605 | Reg loss: 0.033 | Tree loss: 1.605 | Accuracy: 0.477816 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 1.756 | Reg loss: 0.033 | Tree loss: 1.756 | Accuracy: 0.416000 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 1.735 | Reg loss: 0.033 | Tree loss: 1.735 | Accuracy: 0.426500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.443000 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 1.702 | Reg loss: 0.033 | Tree loss: 1.702 | Accuracy: 0.429500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.444000 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 1.663 | Reg loss: 0.033 | Tree loss: 1.663 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 1.667 | Reg loss: 0.033 | Tree loss: 1.667 | Accuracy: 0.451000 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 1.651 | Reg loss: 0.033 | Tree loss: 1.651 | Accuracy: 0.471500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.033 | Tree loss: 1.654 | Accuracy: 0.463500 | 0.235 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 1.658 | Reg loss: 0.033 | Tree loss: 1.658 | Accuracy: 0.453925 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 1.775 | Reg loss: 0.033 | Tree loss: 1.775 | Accuracy: 0.420000 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.444500 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 1.689 | Reg loss: 0.033 | Tree loss: 1.689 | Accuracy: 0.438000 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 1.726 | Reg loss: 0.033 | Tree loss: 1.726 | Accuracy: 0.414500 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 1.662 | Reg loss: 0.033 | Tree loss: 1.662 | Accuracy: 0.463000 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.441000 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 1.644 | Reg loss: 0.033 | Tree loss: 1.644 | Accuracy: 0.461000 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.033 | Tree loss: 1.664 | Accuracy: 0.451500 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.455500 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.033 | Tree loss: 1.643 | Accuracy: 0.467500 | 0.235 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 1.578 | Reg loss: 0.033 | Tree loss: 1.578 | Accuracy: 0.460751 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 1.765 | Reg loss: 0.033 | Tree loss: 1.765 | Accuracy: 0.409000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.033 | Tree loss: 1.740 | Accuracy: 0.424000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 1.696 | Reg loss: 0.033 | Tree loss: 1.696 | Accuracy: 0.439000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 1.674 | Reg loss: 0.033 | Tree loss: 1.674 | Accuracy: 0.440000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 1.671 | Reg loss: 0.033 | Tree loss: 1.671 | Accuracy: 0.456000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.033 | Tree loss: 1.670 | Accuracy: 0.438500 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 1.638 | Reg loss: 0.033 | Tree loss: 1.638 | Accuracy: 0.468000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.033 | Tree loss: 1.673 | Accuracy: 0.470000 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 1.639 | Reg loss: 0.033 | Tree loss: 1.639 | Accuracy: 0.477500 | 0.235 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 1.575 | Reg loss: 0.033 | Tree loss: 1.575 | Accuracy: 0.505119 | 0.235 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.033 | Tree loss: 1.759 | Accuracy: 0.425000 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.033 | Tree loss: 1.738 | Accuracy: 0.412000 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 1.700 | Reg loss: 0.033 | Tree loss: 1.700 | Accuracy: 0.418500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 1.687 | Reg loss: 0.033 | Tree loss: 1.687 | Accuracy: 0.450500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 1.675 | Reg loss: 0.033 | Tree loss: 1.675 | Accuracy: 0.457500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 1.665 | Reg loss: 0.033 | Tree loss: 1.665 | Accuracy: 0.464500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 1.656 | Reg loss: 0.033 | Tree loss: 1.656 | Accuracy: 0.447000 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.033 | Tree loss: 1.682 | Accuracy: 0.453500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.465000 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.033 | Tree loss: 1.649 | Accuracy: 0.459500 | 0.235 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 1.634 | Reg loss: 0.033 | Tree loss: 1.634 | Accuracy: 0.488055 | 0.235 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.182926829268292\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n",
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "25\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "111\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "16526\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "2729\n",
      "============== Pattern 55 ==============\n",
      "570\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "Average comprehensibility: 67.17073170731707\n",
      "std comprehensibility: 11.914128605996082\n",
      "var comprehensibility: 141.94646044021414\n",
      "minimum comprehensibility: 26\n",
      "maximum comprehensibility: 80\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
