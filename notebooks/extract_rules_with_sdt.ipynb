{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/models/epoch_100.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/models/epoch_100.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/data/test_data.file'\n",
    "batch_size = 32\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "    \n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e261015fd88144d494398b594f79d345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76265ceda73545de8f0f38a5c335ac26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 20))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0d74b5ca44481e8903801c5d4acebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309dfab114ee4c3abad5c1f8479aa0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1c48472f16432886a6855b728cd02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a828341d4557403e95b7e54ca9904827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "layer 11: 0.0\n",
      "layer 12: 0.0\n",
      "Epoch: 00 | Batch: 000 / 013 | Total loss: 1.436 | Reg loss: 0.031 | Tree loss: 1.436 | Accuracy: 0.471500 | 108.009 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 013 | Total loss: 1.391 | Reg loss: 0.031 | Tree loss: 1.391 | Accuracy: 0.489000 | 107.817 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 013 | Total loss: 1.372 | Reg loss: 0.031 | Tree loss: 1.372 | Accuracy: 0.497000 | 107.721 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 013 | Total loss: 1.347 | Reg loss: 0.031 | Tree loss: 1.347 | Accuracy: 0.497500 | 107.737 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 013 | Total loss: 1.368 | Reg loss: 0.031 | Tree loss: 1.368 | Accuracy: 0.502500 | 107.741 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 013 | Total loss: 1.322 | Reg loss: 0.031 | Tree loss: 1.322 | Accuracy: 0.512500 | 107.765 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 013 | Total loss: 1.338 | Reg loss: 0.031 | Tree loss: 1.338 | Accuracy: 0.529000 | 107.778 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 013 | Total loss: 1.317 | Reg loss: 0.031 | Tree loss: 1.317 | Accuracy: 0.519000 | 107.799 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 013 | Total loss: 1.320 | Reg loss: 0.031 | Tree loss: 1.320 | Accuracy: 0.526500 | 107.812 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 013 | Total loss: 1.288 | Reg loss: 0.032 | Tree loss: 1.288 | Accuracy: 0.522500 | 107.829 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 013 | Total loss: 1.301 | Reg loss: 0.032 | Tree loss: 1.301 | Accuracy: 0.532000 | 107.833 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 013 | Total loss: 1.286 | Reg loss: 0.032 | Tree loss: 1.286 | Accuracy: 0.515000 | 107.85 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 013 | Total loss: 1.292 | Reg loss: 0.032 | Tree loss: 1.292 | Accuracy: 0.498171 | 105.418 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 013 | Total loss: 1.740 | Reg loss: 0.031 | Tree loss: 1.740 | Accuracy: 0.486500 | 105.776 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 013 | Total loss: 1.690 | Reg loss: 0.031 | Tree loss: 1.690 | Accuracy: 0.491000 | 105.895 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 013 | Total loss: 1.659 | Reg loss: 0.031 | Tree loss: 1.659 | Accuracy: 0.494500 | 105.998 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 013 | Total loss: 1.615 | Reg loss: 0.031 | Tree loss: 1.615 | Accuracy: 0.467500 | 106.117 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 013 | Total loss: 1.576 | Reg loss: 0.031 | Tree loss: 1.576 | Accuracy: 0.474500 | 106.208 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 013 | Total loss: 1.536 | Reg loss: 0.031 | Tree loss: 1.536 | Accuracy: 0.462500 | 106.296 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 013 | Total loss: 1.501 | Reg loss: 0.031 | Tree loss: 1.501 | Accuracy: 0.452000 | 106.376 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 013 | Total loss: 1.438 | Reg loss: 0.031 | Tree loss: 1.438 | Accuracy: 0.477500 | 106.451 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 013 | Total loss: 1.415 | Reg loss: 0.031 | Tree loss: 1.415 | Accuracy: 0.474000 | 106.528 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 013 | Total loss: 1.376 | Reg loss: 0.031 | Tree loss: 1.376 | Accuracy: 0.487000 | 106.592 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 013 | Total loss: 1.371 | Reg loss: 0.031 | Tree loss: 1.371 | Accuracy: 0.494000 | 106.65 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 013 | Total loss: 1.349 | Reg loss: 0.031 | Tree loss: 1.349 | Accuracy: 0.515000 | 106.704 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 013 | Total loss: 1.320 | Reg loss: 0.031 | Tree loss: 1.320 | Accuracy: 0.493782 | 105.529 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 013 | Total loss: 1.731 | Reg loss: 0.031 | Tree loss: 1.731 | Accuracy: 0.487000 | 105.711 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 013 | Total loss: 1.672 | Reg loss: 0.031 | Tree loss: 1.672 | Accuracy: 0.500500 | 105.778 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 013 | Total loss: 1.596 | Reg loss: 0.031 | Tree loss: 1.596 | Accuracy: 0.476000 | 105.837 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 013 | Total loss: 1.539 | Reg loss: 0.031 | Tree loss: 1.539 | Accuracy: 0.481000 | 105.901 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 013 | Total loss: 1.491 | Reg loss: 0.031 | Tree loss: 1.491 | Accuracy: 0.482500 | 105.972 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 013 | Total loss: 1.442 | Reg loss: 0.031 | Tree loss: 1.442 | Accuracy: 0.488500 | 106.032 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 013 | Total loss: 1.398 | Reg loss: 0.031 | Tree loss: 1.398 | Accuracy: 0.495500 | 106.088 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 013 | Total loss: 1.406 | Reg loss: 0.031 | Tree loss: 1.406 | Accuracy: 0.479000 | 106.143 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 013 | Total loss: 1.373 | Reg loss: 0.031 | Tree loss: 1.373 | Accuracy: 0.483500 | 106.194 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 013 | Total loss: 1.303 | Reg loss: 0.031 | Tree loss: 1.303 | Accuracy: 0.520000 | 106.245 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 013 | Total loss: 1.358 | Reg loss: 0.031 | Tree loss: 1.358 | Accuracy: 0.495000 | 106.289 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 013 | Total loss: 1.340 | Reg loss: 0.032 | Tree loss: 1.340 | Accuracy: 0.496500 | 106.334 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 013 | Total loss: 1.305 | Reg loss: 0.032 | Tree loss: 1.305 | Accuracy: 0.520117 | 105.559 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 013 | Total loss: 1.719 | Reg loss: 0.031 | Tree loss: 1.719 | Accuracy: 0.482500 | 105.68 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 013 | Total loss: 1.672 | Reg loss: 0.031 | Tree loss: 1.672 | Accuracy: 0.475500 | 105.728 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 013 | Total loss: 1.599 | Reg loss: 0.031 | Tree loss: 1.599 | Accuracy: 0.500500 | 105.77 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 013 | Total loss: 1.560 | Reg loss: 0.031 | Tree loss: 1.560 | Accuracy: 0.478500 | 105.816 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 013 | Total loss: 1.505 | Reg loss: 0.031 | Tree loss: 1.505 | Accuracy: 0.483000 | 105.861 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 013 | Total loss: 1.461 | Reg loss: 0.031 | Tree loss: 1.461 | Accuracy: 0.457500 | 105.905 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 013 | Total loss: 1.433 | Reg loss: 0.031 | Tree loss: 1.433 | Accuracy: 0.475000 | 105.948 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 013 | Total loss: 1.394 | Reg loss: 0.031 | Tree loss: 1.394 | Accuracy: 0.470500 | 105.991 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 013 | Total loss: 1.339 | Reg loss: 0.032 | Tree loss: 1.339 | Accuracy: 0.503500 | 106.031 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 013 | Total loss: 1.346 | Reg loss: 0.032 | Tree loss: 1.346 | Accuracy: 0.513500 | 106.071 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 013 | Total loss: 1.314 | Reg loss: 0.032 | Tree loss: 1.314 | Accuracy: 0.518500 | 106.107 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 013 | Total loss: 1.299 | Reg loss: 0.032 | Tree loss: 1.299 | Accuracy: 0.521000 | 106.143 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 013 | Total loss: 1.323 | Reg loss: 0.032 | Tree loss: 1.323 | Accuracy: 0.495977 | 105.565 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 12: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 013 | Total loss: 1.711 | Reg loss: 0.031 | Tree loss: 1.711 | Accuracy: 0.496000 | 105.657 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 013 | Total loss: 1.659 | Reg loss: 0.031 | Tree loss: 1.659 | Accuracy: 0.475000 | 105.692 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 013 | Total loss: 1.596 | Reg loss: 0.031 | Tree loss: 1.596 | Accuracy: 0.474000 | 105.724 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 013 | Total loss: 1.520 | Reg loss: 0.031 | Tree loss: 1.520 | Accuracy: 0.496000 | 105.761 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 013 | Total loss: 1.484 | Reg loss: 0.031 | Tree loss: 1.484 | Accuracy: 0.481500 | 105.796 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 013 | Total loss: 1.426 | Reg loss: 0.032 | Tree loss: 1.426 | Accuracy: 0.489000 | 105.832 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 013 | Total loss: 1.427 | Reg loss: 0.032 | Tree loss: 1.427 | Accuracy: 0.469500 | 105.867 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 013 | Total loss: 1.364 | Reg loss: 0.032 | Tree loss: 1.364 | Accuracy: 0.506000 | 105.901 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 013 | Total loss: 1.340 | Reg loss: 0.032 | Tree loss: 1.340 | Accuracy: 0.489500 | 105.935 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 013 | Total loss: 1.331 | Reg loss: 0.032 | Tree loss: 1.331 | Accuracy: 0.501000 | 105.968 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 013 | Total loss: 1.328 | Reg loss: 0.032 | Tree loss: 1.328 | Accuracy: 0.491500 | 105.999 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 013 | Total loss: 1.328 | Reg loss: 0.032 | Tree loss: 1.328 | Accuracy: 0.516500 | 106.029 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 013 | Total loss: 1.319 | Reg loss: 0.032 | Tree loss: 1.319 | Accuracy: 0.521580 | 105.568 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 013 | Total loss: 1.706 | Reg loss: 0.032 | Tree loss: 1.706 | Accuracy: 0.468000 | 105.642 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 013 | Total loss: 1.643 | Reg loss: 0.032 | Tree loss: 1.643 | Accuracy: 0.484000 | 105.67 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 013 | Total loss: 1.592 | Reg loss: 0.032 | Tree loss: 1.592 | Accuracy: 0.484500 | 105.697 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 013 | Total loss: 1.518 | Reg loss: 0.032 | Tree loss: 1.518 | Accuracy: 0.497500 | 105.726 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 013 | Total loss: 1.470 | Reg loss: 0.032 | Tree loss: 1.470 | Accuracy: 0.477500 | 105.755 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 013 | Total loss: 1.427 | Reg loss: 0.032 | Tree loss: 1.427 | Accuracy: 0.497500 | 105.785 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 013 | Total loss: 1.421 | Reg loss: 0.032 | Tree loss: 1.421 | Accuracy: 0.464000 | 105.815 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 013 | Total loss: 1.362 | Reg loss: 0.032 | Tree loss: 1.362 | Accuracy: 0.483000 | 105.843 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 013 | Total loss: 1.368 | Reg loss: 0.032 | Tree loss: 1.368 | Accuracy: 0.488000 | 105.872 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 013 | Total loss: 1.334 | Reg loss: 0.032 | Tree loss: 1.334 | Accuracy: 0.498500 | 105.9 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 013 | Total loss: 1.316 | Reg loss: 0.032 | Tree loss: 1.316 | Accuracy: 0.500000 | 105.925 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 013 | Total loss: 1.305 | Reg loss: 0.032 | Tree loss: 1.305 | Accuracy: 0.515500 | 105.951 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 013 | Total loss: 1.305 | Reg loss: 0.032 | Tree loss: 1.305 | Accuracy: 0.517922 | 105.568 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 013 | Total loss: 1.694 | Reg loss: 0.032 | Tree loss: 1.694 | Accuracy: 0.476000 | 105.63 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 013 | Total loss: 1.624 | Reg loss: 0.032 | Tree loss: 1.624 | Accuracy: 0.518500 | 105.654 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 013 | Total loss: 1.589 | Reg loss: 0.032 | Tree loss: 1.589 | Accuracy: 0.477500 | 105.677 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 013 | Total loss: 1.518 | Reg loss: 0.032 | Tree loss: 1.518 | Accuracy: 0.494000 | 105.702 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 013 | Total loss: 1.475 | Reg loss: 0.032 | Tree loss: 1.475 | Accuracy: 0.491500 | 105.727 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 013 | Total loss: 1.442 | Reg loss: 0.032 | Tree loss: 1.442 | Accuracy: 0.486000 | 105.753 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 013 | Total loss: 1.413 | Reg loss: 0.032 | Tree loss: 1.413 | Accuracy: 0.474500 | 105.778 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 013 | Total loss: 1.368 | Reg loss: 0.032 | Tree loss: 1.368 | Accuracy: 0.491500 | 105.803 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 013 | Total loss: 1.335 | Reg loss: 0.032 | Tree loss: 1.335 | Accuracy: 0.491000 | 105.828 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 013 | Total loss: 1.317 | Reg loss: 0.032 | Tree loss: 1.317 | Accuracy: 0.512000 | 105.852 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 013 | Total loss: 1.303 | Reg loss: 0.032 | Tree loss: 1.303 | Accuracy: 0.509500 | 105.876 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 013 | Total loss: 1.318 | Reg loss: 0.032 | Tree loss: 1.318 | Accuracy: 0.510000 | 105.901 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 013 | Total loss: 1.274 | Reg loss: 0.032 | Tree loss: 1.274 | Accuracy: 0.521580 | 105.573 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 013 | Total loss: 1.668 | Reg loss: 0.032 | Tree loss: 1.668 | Accuracy: 0.497500 | 105.625 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 013 | Total loss: 1.623 | Reg loss: 0.032 | Tree loss: 1.623 | Accuracy: 0.498000 | 105.646 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 013 | Total loss: 1.569 | Reg loss: 0.032 | Tree loss: 1.569 | Accuracy: 0.472500 | 105.666 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 013 | Total loss: 1.509 | Reg loss: 0.032 | Tree loss: 1.509 | Accuracy: 0.498000 | 105.688 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 013 | Total loss: 1.463 | Reg loss: 0.032 | Tree loss: 1.463 | Accuracy: 0.487500 | 105.709 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 013 | Total loss: 1.425 | Reg loss: 0.032 | Tree loss: 1.425 | Accuracy: 0.495000 | 105.732 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 013 | Total loss: 1.410 | Reg loss: 0.032 | Tree loss: 1.410 | Accuracy: 0.468500 | 105.753 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 013 | Total loss: 1.334 | Reg loss: 0.032 | Tree loss: 1.334 | Accuracy: 0.484500 | 105.775 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 013 | Total loss: 1.367 | Reg loss: 0.032 | Tree loss: 1.367 | Accuracy: 0.474500 | 105.798 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 013 | Total loss: 1.328 | Reg loss: 0.032 | Tree loss: 1.328 | Accuracy: 0.506000 | 105.819 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 013 | Total loss: 1.323 | Reg loss: 0.032 | Tree loss: 1.323 | Accuracy: 0.505500 | 105.84 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 013 | Total loss: 1.313 | Reg loss: 0.032 | Tree loss: 1.313 | Accuracy: 0.508000 | 105.861 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 013 | Total loss: 1.315 | Reg loss: 0.032 | Tree loss: 1.315 | Accuracy: 0.512070 | 105.575 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 000 / 013 | Total loss: 1.662 | Reg loss: 0.032 | Tree loss: 1.662 | Accuracy: 0.487500 | 105.621 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 013 | Total loss: 1.636 | Reg loss: 0.032 | Tree loss: 1.636 | Accuracy: 0.483000 | 105.639 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 013 | Total loss: 1.557 | Reg loss: 0.032 | Tree loss: 1.557 | Accuracy: 0.501500 | 105.657 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 013 | Total loss: 1.517 | Reg loss: 0.032 | Tree loss: 1.517 | Accuracy: 0.483000 | 105.676 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 013 | Total loss: 1.457 | Reg loss: 0.032 | Tree loss: 1.457 | Accuracy: 0.508000 | 105.695 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 013 | Total loss: 1.440 | Reg loss: 0.032 | Tree loss: 1.440 | Accuracy: 0.485000 | 105.715 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 013 | Total loss: 1.394 | Reg loss: 0.032 | Tree loss: 1.394 | Accuracy: 0.479000 | 105.735 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 013 | Total loss: 1.355 | Reg loss: 0.032 | Tree loss: 1.355 | Accuracy: 0.487500 | 105.755 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 013 | Total loss: 1.315 | Reg loss: 0.032 | Tree loss: 1.315 | Accuracy: 0.522500 | 105.775 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 013 | Total loss: 1.322 | Reg loss: 0.032 | Tree loss: 1.322 | Accuracy: 0.484000 | 105.794 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 013 | Total loss: 1.328 | Reg loss: 0.032 | Tree loss: 1.328 | Accuracy: 0.487500 | 105.812 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 013 | Total loss: 1.307 | Reg loss: 0.032 | Tree loss: 1.307 | Accuracy: 0.517000 | 105.83 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 013 | Total loss: 1.299 | Reg loss: 0.032 | Tree loss: 1.299 | Accuracy: 0.520117 | 105.576 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 013 | Total loss: 1.672 | Reg loss: 0.032 | Tree loss: 1.672 | Accuracy: 0.478500 | 105.617 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 013 | Total loss: 1.609 | Reg loss: 0.032 | Tree loss: 1.609 | Accuracy: 0.497500 | 105.633 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 013 | Total loss: 1.559 | Reg loss: 0.032 | Tree loss: 1.559 | Accuracy: 0.487000 | 105.649 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 013 | Total loss: 1.503 | Reg loss: 0.032 | Tree loss: 1.503 | Accuracy: 0.501500 | 105.667 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 013 | Total loss: 1.459 | Reg loss: 0.032 | Tree loss: 1.459 | Accuracy: 0.493500 | 105.684 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 013 | Total loss: 1.422 | Reg loss: 0.032 | Tree loss: 1.422 | Accuracy: 0.481500 | 105.702 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 013 | Total loss: 1.373 | Reg loss: 0.032 | Tree loss: 1.373 | Accuracy: 0.484500 | 105.72 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 013 | Total loss: 1.368 | Reg loss: 0.032 | Tree loss: 1.368 | Accuracy: 0.494000 | 105.738 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 013 | Total loss: 1.323 | Reg loss: 0.032 | Tree loss: 1.323 | Accuracy: 0.505000 | 105.756 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 013 | Total loss: 1.320 | Reg loss: 0.032 | Tree loss: 1.320 | Accuracy: 0.502500 | 105.773 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 013 | Total loss: 1.321 | Reg loss: 0.032 | Tree loss: 1.321 | Accuracy: 0.513500 | 105.789 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 013 | Total loss: 1.294 | Reg loss: 0.032 | Tree loss: 1.294 | Accuracy: 0.524000 | 105.806 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 013 | Total loss: 1.300 | Reg loss: 0.032 | Tree loss: 1.300 | Accuracy: 0.498903 | 105.578 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 013 | Total loss: 1.664 | Reg loss: 0.032 | Tree loss: 1.664 | Accuracy: 0.486500 | 105.614 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 013 | Total loss: 1.592 | Reg loss: 0.032 | Tree loss: 1.592 | Accuracy: 0.480500 | 105.629 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 013 | Total loss: 1.537 | Reg loss: 0.032 | Tree loss: 1.537 | Accuracy: 0.489500 | 105.644 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 013 | Total loss: 1.511 | Reg loss: 0.032 | Tree loss: 1.511 | Accuracy: 0.483000 | 105.659 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 013 | Total loss: 1.451 | Reg loss: 0.032 | Tree loss: 1.451 | Accuracy: 0.496500 | 105.676 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 013 | Total loss: 1.434 | Reg loss: 0.032 | Tree loss: 1.434 | Accuracy: 0.468500 | 105.692 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 013 | Total loss: 1.376 | Reg loss: 0.032 | Tree loss: 1.376 | Accuracy: 0.491000 | 105.708 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 013 | Total loss: 1.383 | Reg loss: 0.032 | Tree loss: 1.383 | Accuracy: 0.473500 | 105.725 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 013 | Total loss: 1.314 | Reg loss: 0.032 | Tree loss: 1.314 | Accuracy: 0.516000 | 105.741 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 013 | Total loss: 1.317 | Reg loss: 0.032 | Tree loss: 1.317 | Accuracy: 0.502500 | 105.757 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 013 | Total loss: 1.300 | Reg loss: 0.032 | Tree loss: 1.300 | Accuracy: 0.499000 | 105.772 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 013 | Total loss: 1.322 | Reg loss: 0.033 | Tree loss: 1.322 | Accuracy: 0.490000 | 105.788 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 013 | Total loss: 1.292 | Reg loss: 0.033 | Tree loss: 1.292 | Accuracy: 0.547915 | 105.581 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 013 | Total loss: 1.641 | Reg loss: 0.032 | Tree loss: 1.641 | Accuracy: 0.493500 | 105.614 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 013 | Total loss: 1.590 | Reg loss: 0.032 | Tree loss: 1.590 | Accuracy: 0.496000 | 105.628 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 013 | Total loss: 1.549 | Reg loss: 0.032 | Tree loss: 1.549 | Accuracy: 0.498000 | 105.641 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 013 | Total loss: 1.483 | Reg loss: 0.032 | Tree loss: 1.483 | Accuracy: 0.490500 | 105.655 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 013 | Total loss: 1.450 | Reg loss: 0.032 | Tree loss: 1.450 | Accuracy: 0.486000 | 105.669 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 013 | Total loss: 1.425 | Reg loss: 0.032 | Tree loss: 1.425 | Accuracy: 0.486500 | 105.684 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 013 | Total loss: 1.379 | Reg loss: 0.032 | Tree loss: 1.379 | Accuracy: 0.490500 | 105.698 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 013 | Total loss: 1.353 | Reg loss: 0.032 | Tree loss: 1.353 | Accuracy: 0.481500 | 105.713 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 013 | Total loss: 1.340 | Reg loss: 0.032 | Tree loss: 1.340 | Accuracy: 0.504500 | 105.728 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 013 | Total loss: 1.297 | Reg loss: 0.033 | Tree loss: 1.297 | Accuracy: 0.514000 | 105.743 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 013 | Total loss: 1.338 | Reg loss: 0.033 | Tree loss: 1.338 | Accuracy: 0.488500 | 105.756 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 013 | Total loss: 1.281 | Reg loss: 0.033 | Tree loss: 1.281 | Accuracy: 0.530000 | 105.771 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 013 | Total loss: 1.293 | Reg loss: 0.033 | Tree loss: 1.293 | Accuracy: 0.518654 | 105.58 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 013 | Total loss: 1.631 | Reg loss: 0.032 | Tree loss: 1.631 | Accuracy: 0.490500 | 105.611 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 001 / 013 | Total loss: 1.598 | Reg loss: 0.032 | Tree loss: 1.598 | Accuracy: 0.494000 | 105.624 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 013 | Total loss: 1.563 | Reg loss: 0.032 | Tree loss: 1.563 | Accuracy: 0.468500 | 105.635 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 013 | Total loss: 1.496 | Reg loss: 0.032 | Tree loss: 1.496 | Accuracy: 0.505000 | 105.648 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 013 | Total loss: 1.417 | Reg loss: 0.032 | Tree loss: 1.417 | Accuracy: 0.507000 | 105.662 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 013 | Total loss: 1.385 | Reg loss: 0.032 | Tree loss: 1.385 | Accuracy: 0.497500 | 105.675 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 013 | Total loss: 1.388 | Reg loss: 0.033 | Tree loss: 1.388 | Accuracy: 0.486500 | 105.689 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 013 | Total loss: 1.378 | Reg loss: 0.033 | Tree loss: 1.378 | Accuracy: 0.477500 | 105.703 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 013 | Total loss: 1.296 | Reg loss: 0.033 | Tree loss: 1.296 | Accuracy: 0.515500 | 105.716 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 013 | Total loss: 1.311 | Reg loss: 0.033 | Tree loss: 1.311 | Accuracy: 0.502500 | 105.73 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 013 | Total loss: 1.323 | Reg loss: 0.033 | Tree loss: 1.323 | Accuracy: 0.499000 | 105.743 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 013 | Total loss: 1.281 | Reg loss: 0.033 | Tree loss: 1.281 | Accuracy: 0.516000 | 105.756 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 013 | Total loss: 1.270 | Reg loss: 0.033 | Tree loss: 1.270 | Accuracy: 0.520117 | 105.581 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 013 | Total loss: 1.646 | Reg loss: 0.032 | Tree loss: 1.646 | Accuracy: 0.480000 | 105.609 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 013 | Total loss: 1.583 | Reg loss: 0.032 | Tree loss: 1.583 | Accuracy: 0.488000 | 105.621 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 013 | Total loss: 1.529 | Reg loss: 0.032 | Tree loss: 1.529 | Accuracy: 0.499000 | 105.632 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 013 | Total loss: 1.497 | Reg loss: 0.033 | Tree loss: 1.497 | Accuracy: 0.490000 | 105.644 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 013 | Total loss: 1.438 | Reg loss: 0.033 | Tree loss: 1.438 | Accuracy: 0.496500 | 105.656 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 013 | Total loss: 1.415 | Reg loss: 0.033 | Tree loss: 1.415 | Accuracy: 0.483000 | 105.669 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 013 | Total loss: 1.378 | Reg loss: 0.033 | Tree loss: 1.378 | Accuracy: 0.491500 | 105.682 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 013 | Total loss: 1.342 | Reg loss: 0.033 | Tree loss: 1.342 | Accuracy: 0.525500 | 105.694 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 013 | Total loss: 1.321 | Reg loss: 0.033 | Tree loss: 1.321 | Accuracy: 0.509000 | 105.707 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 013 | Total loss: 1.315 | Reg loss: 0.033 | Tree loss: 1.315 | Accuracy: 0.492000 | 105.72 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 013 | Total loss: 1.302 | Reg loss: 0.033 | Tree loss: 1.302 | Accuracy: 0.502500 | 105.732 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 013 | Total loss: 1.283 | Reg loss: 0.033 | Tree loss: 1.283 | Accuracy: 0.523000 | 105.745 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 013 | Total loss: 1.275 | Reg loss: 0.033 | Tree loss: 1.275 | Accuracy: 0.529627 | 105.582 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 013 | Total loss: 1.646 | Reg loss: 0.033 | Tree loss: 1.646 | Accuracy: 0.468000 | 105.608 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 013 | Total loss: 1.599 | Reg loss: 0.033 | Tree loss: 1.599 | Accuracy: 0.476500 | 105.619 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 013 | Total loss: 1.537 | Reg loss: 0.033 | Tree loss: 1.537 | Accuracy: 0.481000 | 105.629 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 013 | Total loss: 1.455 | Reg loss: 0.033 | Tree loss: 1.455 | Accuracy: 0.516000 | 105.64 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 013 | Total loss: 1.457 | Reg loss: 0.033 | Tree loss: 1.457 | Accuracy: 0.484000 | 105.652 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 013 | Total loss: 1.387 | Reg loss: 0.033 | Tree loss: 1.387 | Accuracy: 0.508000 | 105.664 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 013 | Total loss: 1.382 | Reg loss: 0.033 | Tree loss: 1.382 | Accuracy: 0.485500 | 105.675 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 013 | Total loss: 1.333 | Reg loss: 0.033 | Tree loss: 1.333 | Accuracy: 0.487500 | 105.687 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 013 | Total loss: 1.304 | Reg loss: 0.033 | Tree loss: 1.304 | Accuracy: 0.515500 | 105.699 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 013 | Total loss: 1.312 | Reg loss: 0.033 | Tree loss: 1.312 | Accuracy: 0.505000 | 105.711 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 013 | Total loss: 1.288 | Reg loss: 0.033 | Tree loss: 1.288 | Accuracy: 0.526500 | 105.722 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 013 | Total loss: 1.296 | Reg loss: 0.033 | Tree loss: 1.296 | Accuracy: 0.519500 | 105.734 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 013 | Total loss: 1.278 | Reg loss: 0.033 | Tree loss: 1.278 | Accuracy: 0.523043 | 105.582 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 013 | Total loss: 1.635 | Reg loss: 0.033 | Tree loss: 1.635 | Accuracy: 0.497000 | 105.607 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 013 | Total loss: 1.567 | Reg loss: 0.033 | Tree loss: 1.567 | Accuracy: 0.494000 | 105.617 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 013 | Total loss: 1.526 | Reg loss: 0.033 | Tree loss: 1.526 | Accuracy: 0.489500 | 105.626 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 013 | Total loss: 1.464 | Reg loss: 0.033 | Tree loss: 1.464 | Accuracy: 0.488500 | 105.637 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 013 | Total loss: 1.425 | Reg loss: 0.033 | Tree loss: 1.425 | Accuracy: 0.493500 | 105.647 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 013 | Total loss: 1.385 | Reg loss: 0.033 | Tree loss: 1.385 | Accuracy: 0.503500 | 105.658 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 013 | Total loss: 1.361 | Reg loss: 0.033 | Tree loss: 1.361 | Accuracy: 0.490000 | 105.67 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 013 | Total loss: 1.350 | Reg loss: 0.033 | Tree loss: 1.350 | Accuracy: 0.501500 | 105.681 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 013 | Total loss: 1.327 | Reg loss: 0.033 | Tree loss: 1.327 | Accuracy: 0.508500 | 105.692 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 013 | Total loss: 1.337 | Reg loss: 0.033 | Tree loss: 1.337 | Accuracy: 0.478500 | 105.703 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 013 | Total loss: 1.284 | Reg loss: 0.033 | Tree loss: 1.284 | Accuracy: 0.511000 | 105.714 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 013 | Total loss: 1.289 | Reg loss: 0.033 | Tree loss: 1.289 | Accuracy: 0.512000 | 105.725 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 013 | Total loss: 1.309 | Reg loss: 0.033 | Tree loss: 1.309 | Accuracy: 0.501829 | 105.583 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 013 | Total loss: 1.625 | Reg loss: 0.033 | Tree loss: 1.625 | Accuracy: 0.491500 | 105.606 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 013 | Total loss: 1.590 | Reg loss: 0.033 | Tree loss: 1.590 | Accuracy: 0.477000 | 105.615 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 002 / 013 | Total loss: 1.518 | Reg loss: 0.033 | Tree loss: 1.518 | Accuracy: 0.511000 | 105.624 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 013 | Total loss: 1.488 | Reg loss: 0.033 | Tree loss: 1.488 | Accuracy: 0.473500 | 105.634 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 013 | Total loss: 1.430 | Reg loss: 0.033 | Tree loss: 1.430 | Accuracy: 0.486000 | 105.644 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 013 | Total loss: 1.383 | Reg loss: 0.033 | Tree loss: 1.383 | Accuracy: 0.507000 | 105.655 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 013 | Total loss: 1.364 | Reg loss: 0.033 | Tree loss: 1.364 | Accuracy: 0.486000 | 105.665 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 013 | Total loss: 1.339 | Reg loss: 0.033 | Tree loss: 1.339 | Accuracy: 0.495000 | 105.677 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 013 | Total loss: 1.304 | Reg loss: 0.033 | Tree loss: 1.304 | Accuracy: 0.508000 | 105.687 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 013 | Total loss: 1.296 | Reg loss: 0.033 | Tree loss: 1.296 | Accuracy: 0.509000 | 105.697 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 013 | Total loss: 1.296 | Reg loss: 0.033 | Tree loss: 1.296 | Accuracy: 0.517500 | 105.707 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 013 | Total loss: 1.264 | Reg loss: 0.033 | Tree loss: 1.264 | Accuracy: 0.535500 | 105.717 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 013 | Total loss: 1.302 | Reg loss: 0.033 | Tree loss: 1.302 | Accuracy: 0.525969 | 105.583 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 013 | Total loss: 1.619 | Reg loss: 0.033 | Tree loss: 1.619 | Accuracy: 0.502000 | 105.605 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 013 | Total loss: 1.562 | Reg loss: 0.033 | Tree loss: 1.562 | Accuracy: 0.503500 | 105.614 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 013 | Total loss: 1.512 | Reg loss: 0.033 | Tree loss: 1.512 | Accuracy: 0.493500 | 105.622 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 013 | Total loss: 1.474 | Reg loss: 0.033 | Tree loss: 1.474 | Accuracy: 0.483500 | 105.632 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 013 | Total loss: 1.425 | Reg loss: 0.033 | Tree loss: 1.425 | Accuracy: 0.497000 | 105.641 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 013 | Total loss: 1.378 | Reg loss: 0.033 | Tree loss: 1.378 | Accuracy: 0.495500 | 105.652 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 013 | Total loss: 1.344 | Reg loss: 0.033 | Tree loss: 1.344 | Accuracy: 0.488000 | 105.661 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 013 | Total loss: 1.322 | Reg loss: 0.033 | Tree loss: 1.322 | Accuracy: 0.511000 | 105.671 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 013 | Total loss: 1.323 | Reg loss: 0.033 | Tree loss: 1.323 | Accuracy: 0.502000 | 105.681 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 013 | Total loss: 1.309 | Reg loss: 0.033 | Tree loss: 1.309 | Accuracy: 0.514500 | 105.69 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 013 | Total loss: 1.311 | Reg loss: 0.033 | Tree loss: 1.311 | Accuracy: 0.486500 | 105.7 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 013 | Total loss: 1.281 | Reg loss: 0.033 | Tree loss: 1.281 | Accuracy: 0.511000 | 105.71 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 013 | Total loss: 1.289 | Reg loss: 0.033 | Tree loss: 1.289 | Accuracy: 0.509876 | 105.584 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 013 | Total loss: 1.608 | Reg loss: 0.033 | Tree loss: 1.608 | Accuracy: 0.500500 | 105.604 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 013 | Total loss: 1.569 | Reg loss: 0.033 | Tree loss: 1.569 | Accuracy: 0.498000 | 105.612 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 013 | Total loss: 1.523 | Reg loss: 0.033 | Tree loss: 1.523 | Accuracy: 0.494500 | 105.621 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 013 | Total loss: 1.466 | Reg loss: 0.033 | Tree loss: 1.466 | Accuracy: 0.487500 | 105.63 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 013 | Total loss: 1.417 | Reg loss: 0.033 | Tree loss: 1.417 | Accuracy: 0.496000 | 105.639 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 013 | Total loss: 1.388 | Reg loss: 0.033 | Tree loss: 1.388 | Accuracy: 0.487500 | 105.648 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 013 | Total loss: 1.346 | Reg loss: 0.033 | Tree loss: 1.346 | Accuracy: 0.498500 | 105.657 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 013 | Total loss: 1.350 | Reg loss: 0.033 | Tree loss: 1.350 | Accuracy: 0.490000 | 105.666 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 013 | Total loss: 1.323 | Reg loss: 0.033 | Tree loss: 1.323 | Accuracy: 0.500500 | 105.676 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 013 | Total loss: 1.299 | Reg loss: 0.033 | Tree loss: 1.299 | Accuracy: 0.491000 | 105.685 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 013 | Total loss: 1.297 | Reg loss: 0.033 | Tree loss: 1.297 | Accuracy: 0.517000 | 105.695 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 013 | Total loss: 1.262 | Reg loss: 0.033 | Tree loss: 1.262 | Accuracy: 0.536000 | 105.704 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 013 | Total loss: 1.249 | Reg loss: 0.033 | Tree loss: 1.249 | Accuracy: 0.523043 | 105.584 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 013 | Total loss: 1.608 | Reg loss: 0.033 | Tree loss: 1.608 | Accuracy: 0.491500 | 105.604 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 013 | Total loss: 1.565 | Reg loss: 0.033 | Tree loss: 1.565 | Accuracy: 0.487500 | 105.612 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 013 | Total loss: 1.490 | Reg loss: 0.033 | Tree loss: 1.490 | Accuracy: 0.516000 | 105.619 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 013 | Total loss: 1.464 | Reg loss: 0.033 | Tree loss: 1.464 | Accuracy: 0.483000 | 105.628 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 013 | Total loss: 1.404 | Reg loss: 0.033 | Tree loss: 1.404 | Accuracy: 0.493000 | 105.636 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 013 | Total loss: 1.387 | Reg loss: 0.033 | Tree loss: 1.387 | Accuracy: 0.498000 | 105.645 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 013 | Total loss: 1.351 | Reg loss: 0.033 | Tree loss: 1.351 | Accuracy: 0.491000 | 105.654 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 013 | Total loss: 1.342 | Reg loss: 0.033 | Tree loss: 1.342 | Accuracy: 0.489000 | 105.663 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 013 | Total loss: 1.309 | Reg loss: 0.033 | Tree loss: 1.309 | Accuracy: 0.507000 | 105.672 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 013 | Total loss: 1.317 | Reg loss: 0.033 | Tree loss: 1.317 | Accuracy: 0.507500 | 105.681 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 013 | Total loss: 1.268 | Reg loss: 0.033 | Tree loss: 1.268 | Accuracy: 0.512000 | 105.689 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 013 | Total loss: 1.283 | Reg loss: 0.033 | Tree loss: 1.283 | Accuracy: 0.510500 | 105.698 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 013 | Total loss: 1.278 | Reg loss: 0.033 | Tree loss: 1.278 | Accuracy: 0.520117 | 105.584 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "layer 9: 0.9723756906077345\n",
      "layer 10: 0.9723756906077345\n",
      "layer 11: 0.9723756906077345\n",
      "layer 12: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 013 | Total loss: 1.618 | Reg loss: 0.033 | Tree loss: 1.618 | Accuracy: 0.473500 | 105.603 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 013 | Total loss: 1.554 | Reg loss: 0.033 | Tree loss: 1.554 | Accuracy: 0.499500 | 105.61 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 013 | Total loss: 1.502 | Reg loss: 0.033 | Tree loss: 1.502 | Accuracy: 0.496000 | 105.618 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 003 / 013 | Total loss: 1.468 | Reg loss: 0.033 | Tree loss: 1.468 | Accuracy: 0.499500 | 105.626 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 013 | Total loss: 1.419 | Reg loss: 0.033 | Tree loss: 1.419 | Accuracy: 0.493000 | 105.634 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 013 | Total loss: 1.381 | Reg loss: 0.033 | Tree loss: 1.381 | Accuracy: 0.493000 | 105.643 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 013 | Total loss: 1.344 | Reg loss: 0.033 | Tree loss: 1.344 | Accuracy: 0.476500 | 105.651 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 013 | Total loss: 1.320 | Reg loss: 0.033 | Tree loss: 1.320 | Accuracy: 0.515500 | 105.659 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 013 | Total loss: 1.296 | Reg loss: 0.033 | Tree loss: 1.296 | Accuracy: 0.512000 | 105.668 sec/iter\n"
     ]
    }
   ],
   "source": [
    "tree = tree.train()\n",
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_tree(tree, factor=1.5)\n",
    "correct = 0\n",
    "tree = tree.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sparseness: {sparseness(tree.inner_nodes.weight)}\")\n",
    "layer = 0\n",
    "sps = []\n",
    "for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "    cur_layer = np.floor(np.log2(i+1))\n",
    "    if cur_layer != layer:\n",
    "        print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "        sps = []\n",
    "        layer = cur_layer\n",
    "    \n",
    "    x_ = tree.inner_nodes.weight[i, :]\n",
    "    sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "    sps.append(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\\n Kurtosis: {kurtosis(weights_layer)}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the accuracy didn't change too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tree_copy = tree_copy.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree_copy.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree_copy.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "attr_names = ['bias']\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "\n",
    "# print(attr_names)\n",
    "stack = LifoQueue()\n",
    "edge_stack = LifoQueue()\n",
    "stack.put(root)\n",
    "rule_counter = 0\n",
    "root.reset()\n",
    "while not stack.empty():\n",
    "    node = stack.get()\n",
    "    if node.is_leaf():\n",
    "        print(f\"============== Rule {rule_counter} ==============\")\n",
    "        for stack_node, cond in zip(stack.queue, edge_stack.queue[1:]):\n",
    "            print(repr(stack_node.get_condition(attr_names)) + cond)\n",
    "            print()\n",
    "        \n",
    "        rule_counter += 1\n",
    "        edge_stack.get()\n",
    "        continue\n",
    "          \n",
    "    if node.left is not None and not node.left.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.left)\n",
    "        node.left.visited = True\n",
    "        edge_stack.put(' < 0')\n",
    "        continue\n",
    "        \n",
    "    if node.right is not None and not node.right.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.right)\n",
    "        node.right.visited = True\n",
    "        edge_stack.put(' > 0')\n",
    "        continue\n",
    "        \n",
    "    if node is not root:\n",
    "        edge_stack.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
