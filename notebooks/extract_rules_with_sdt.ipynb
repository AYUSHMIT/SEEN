{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/models/epoch_100.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/models/epoch_100.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32_min_max_normalization/data/test_data.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab3d7089a864202a46141bc09c16b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a2941e49744218bd5a864bb50e77d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 20))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba1f9b2facf4272a7df7af6afcb38bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a410d2f36ac84fc5bbca54fc56706643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b7eab20f9043fab1d683bdb46b0baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de7502af28141fab23f5eb39b5b1c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=d).fit(samples_f, clusters)\n",
    "tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 00 | Batch: 000 / 013 | Total loss: 1.324 | Reg loss: 0.037 | Tree loss: 1.324 | Accuracy: 0.513000 | 0.902 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.037 | Tree loss: 1.252 | Accuracy: 0.547000 | 0.868 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.037 | Tree loss: 1.229 | Accuracy: 0.560500 | 0.859 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.037 | Tree loss: 1.213 | Accuracy: 0.555500 | 0.855 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.037 | Tree loss: 1.183 | Accuracy: 0.564500 | 0.854 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.037 | Tree loss: 1.136 | Accuracy: 0.591500 | 0.853 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.037 | Tree loss: 1.150 | Accuracy: 0.583000 | 0.855 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.627000 | 0.855 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.643000 | 0.855 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.651061 | 0.849 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 013 | Total loss: 1.313 | Reg loss: 0.037 | Tree loss: 1.313 | Accuracy: 0.525500 | 0.858 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.037 | Tree loss: 1.273 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 013 | Total loss: 1.262 | Reg loss: 0.037 | Tree loss: 1.262 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 013 | Total loss: 1.182 | Reg loss: 0.037 | Tree loss: 1.182 | Accuracy: 0.576000 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.037 | Tree loss: 1.193 | Accuracy: 0.564000 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 013 | Total loss: 1.168 | Reg loss: 0.037 | Tree loss: 1.168 | Accuracy: 0.570000 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.037 | Tree loss: 1.154 | Accuracy: 0.585000 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.610500 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.616679 | 0.854 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.037 | Tree loss: 1.277 | Accuracy: 0.554000 | 0.858 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.037 | Tree loss: 1.261 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.037 | Tree loss: 1.240 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.037 | Tree loss: 1.206 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.037 | Tree loss: 1.169 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.037 | Tree loss: 1.137 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 013 | Total loss: 1.160 | Reg loss: 0.037 | Tree loss: 1.160 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.644000 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.641000 | 0.858 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.624000 | 0.859 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 013 | Total loss: 1.309 | Reg loss: 0.037 | Tree loss: 1.309 | Accuracy: 0.517500 | 0.859 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.037 | Tree loss: 1.253 | Accuracy: 0.532500 | 0.859 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.037 | Tree loss: 1.243 | Accuracy: 0.548500 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.037 | Tree loss: 1.219 | Accuracy: 0.550500 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 013 | Total loss: 1.195 | Reg loss: 0.037 | Tree loss: 1.195 | Accuracy: 0.572000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.037 | Tree loss: 1.138 | Accuracy: 0.580500 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.586000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.607500 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.610000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.619000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.607000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.609000 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.610095 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.037 | Tree loss: 1.283 | Accuracy: 0.542000 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.037 | Tree loss: 1.278 | Accuracy: 0.515500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.037 | Tree loss: 1.248 | Accuracy: 0.550000 | 0.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 003 / 013 | Total loss: 1.226 | Reg loss: 0.037 | Tree loss: 1.226 | Accuracy: 0.552000 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.037 | Tree loss: 1.184 | Accuracy: 0.585000 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.037 | Tree loss: 1.150 | Accuracy: 0.588000 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.613500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.624500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.629500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.617000 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.629500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.610095 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.037 | Tree loss: 1.280 | Accuracy: 0.534000 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.037 | Tree loss: 1.259 | Accuracy: 0.546500 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.037 | Tree loss: 1.237 | Accuracy: 0.546000 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 013 | Total loss: 1.230 | Reg loss: 0.037 | Tree loss: 1.230 | Accuracy: 0.544000 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.037 | Tree loss: 1.175 | Accuracy: 0.586500 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 013 | Total loss: 1.179 | Reg loss: 0.037 | Tree loss: 1.179 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.607000 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.616500 | 0.858 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.619605 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.037 | Tree loss: 1.293 | Accuracy: 0.534000 | 0.858 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.037 | Tree loss: 1.264 | Accuracy: 0.554500 | 0.858 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.037 | Tree loss: 1.225 | Accuracy: 0.539500 | 0.858 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 013 | Total loss: 1.225 | Reg loss: 0.037 | Tree loss: 1.225 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 013 | Total loss: 1.199 | Reg loss: 0.037 | Tree loss: 1.199 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.037 | Tree loss: 1.138 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.037 | Tree loss: 1.285 | Accuracy: 0.544000 | 0.858 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.037 | Tree loss: 1.262 | Accuracy: 0.522000 | 0.858 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.037 | Tree loss: 1.248 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.037 | Tree loss: 1.199 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.037 | Tree loss: 1.188 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.037 | Tree loss: 1.286 | Accuracy: 0.544000 | 0.858 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.037 | Tree loss: 1.271 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.037 | Tree loss: 1.236 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.037 | Tree loss: 1.204 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.037 | Tree loss: 1.196 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.617000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.621068 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.037 | Tree loss: 1.305 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.037 | Tree loss: 1.282 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.037 | Tree loss: 1.238 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.037 | Tree loss: 1.208 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.037 | Tree loss: 1.183 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.610000 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.643014 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.037 | Tree loss: 1.295 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.037 | Tree loss: 1.253 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.037 | Tree loss: 1.238 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.037 | Tree loss: 1.201 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 013 | Total loss: 1.204 | Reg loss: 0.037 | Tree loss: 1.204 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.037 | Tree loss: 1.284 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 013 | Total loss: 1.289 | Reg loss: 0.037 | Tree loss: 1.289 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 013 | Total loss: 1.216 | Reg loss: 0.037 | Tree loss: 1.216 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.037 | Tree loss: 1.197 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.037 | Tree loss: 1.176 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 013 | Total loss: 1.311 | Reg loss: 0.037 | Tree loss: 1.311 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.037 | Tree loss: 1.259 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.037 | Tree loss: 1.223 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.037 | Tree loss: 1.202 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 013 | Total loss: 1.201 | Reg loss: 0.037 | Tree loss: 1.201 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.621800 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.037 | Tree loss: 1.303 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.037 | Tree loss: 1.248 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.037 | Tree loss: 1.232 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.037 | Tree loss: 1.197 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.037 | Tree loss: 1.291 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.037 | Tree loss: 1.263 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.037 | Tree loss: 1.229 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.037 | Tree loss: 1.200 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.629846 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.037 | Tree loss: 1.283 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 013 | Total loss: 1.289 | Reg loss: 0.037 | Tree loss: 1.289 | Accuracy: 0.517000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 013 | Total loss: 1.264 | Reg loss: 0.037 | Tree loss: 1.264 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.037 | Tree loss: 1.204 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.037 | Tree loss: 1.173 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.653500 | 0.857 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.037 | Tree loss: 1.289 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.037 | Tree loss: 1.251 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.037 | Tree loss: 1.237 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 013 | Total loss: 1.224 | Reg loss: 0.037 | Tree loss: 1.224 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.618142 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.037 | Tree loss: 1.281 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.037 | Tree loss: 1.264 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.037 | Tree loss: 1.242 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.037 | Tree loss: 1.193 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.575000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 005 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.629846 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 013 | Total loss: 1.271 | Reg loss: 0.037 | Tree loss: 1.271 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.037 | Tree loss: 1.274 | Accuracy: 0.518500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.037 | Tree loss: 1.235 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.037 | Tree loss: 1.216 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.590500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.638625 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.037 | Tree loss: 1.282 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 013 | Total loss: 1.275 | Reg loss: 0.037 | Tree loss: 1.275 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 013 | Total loss: 1.267 | Reg loss: 0.037 | Tree loss: 1.267 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.037 | Tree loss: 1.212 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.613000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.629115 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 013 | Total loss: 1.307 | Reg loss: 0.037 | Tree loss: 1.307 | Accuracy: 0.519000 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.037 | Tree loss: 1.247 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.037 | Tree loss: 1.239 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 013 | Total loss: 1.230 | Reg loss: 0.037 | Tree loss: 1.230 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.644477 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 013 | Total loss: 1.310 | Reg loss: 0.037 | Tree loss: 1.310 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.037 | Tree loss: 1.261 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.037 | Tree loss: 1.233 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.638000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.615216 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.037 | Tree loss: 1.280 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 013 | Total loss: 1.281 | Reg loss: 0.037 | Tree loss: 1.281 | Accuracy: 0.516000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.037 | Tree loss: 1.244 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.037 | Tree loss: 1.291 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 013 | Total loss: 1.298 | Reg loss: 0.037 | Tree loss: 1.298 | Accuracy: 0.500500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.037 | Tree loss: 1.245 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.037 | Tree loss: 1.292 | Accuracy: 0.525000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.037 | Tree loss: 1.277 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 013 | Total loss: 1.260 | Reg loss: 0.037 | Tree loss: 1.260 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.037 | Tree loss: 1.212 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.621068 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.037 | Tree loss: 1.276 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.037 | Tree loss: 1.278 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.037 | Tree loss: 1.231 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.646672 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 000 / 013 | Total loss: 1.260 | Reg loss: 0.037 | Tree loss: 1.260 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.037 | Tree loss: 1.263 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 013 | Total loss: 1.251 | Reg loss: 0.037 | Tree loss: 1.251 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.610000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.608632 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.037 | Tree loss: 1.301 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.037 | Tree loss: 1.268 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.037 | Tree loss: 1.224 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.610000 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.037 | Tree loss: 1.290 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 013 | Total loss: 1.288 | Reg loss: 0.037 | Tree loss: 1.288 | Accuracy: 0.505000 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.037 | Tree loss: 1.229 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.625457 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.037 | Tree loss: 1.282 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.037 | Tree loss: 1.242 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.538500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.569000 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.579500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.570500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.593500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.620000 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.623500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.637500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.652500 | 0.858 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.654718 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.037 | Tree loss: 1.292 | Accuracy: 0.543500 | 0.858 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.037 | Tree loss: 1.262 | Accuracy: 0.538500 | 0.858 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.037 | Tree loss: 1.235 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.557500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.647500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.038 | Tree loss: 1.305 | Accuracy: 0.515500 | 0.858 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.603511 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.557000 | 0.858 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.633504 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.592500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.655000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.647500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.599854 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.624726 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.623994 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.610000 | 0.857 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.613021 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.623994 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.649000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.626920 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 013 | Total loss: 1.302 | Reg loss: 0.038 | Tree loss: 1.302 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.576000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 013 | Total loss: 1.072 | Reg loss: 0.038 | Tree loss: 1.072 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.660500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.591807 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.586686 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.038 | Tree loss: 1.305 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.598500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.618142 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.633504 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 50 | Batch: 000 / 013 | Total loss: 1.306 | Reg loss: 0.038 | Tree loss: 1.306 | Accuracy: 0.519000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.588500 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.641551 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.621068 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 000 / 013 | Total loss: 1.306 | Reg loss: 0.038 | Tree loss: 1.306 | Accuracy: 0.515500 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618142 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.653500 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.609364 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.546500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.588000 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.606437 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.588000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.645000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.619500 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.647403 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.563500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.600000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.604500 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.573000 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.604500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.600000 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.521000 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.600000 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.594500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.604243 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.598000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.648866 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.038 | Tree loss: 1.305 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.651792 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.515500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.655000 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.588000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.653255 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.038 | Tree loss: 1.301 | Accuracy: 0.517500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.572500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.622500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.658000 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.644000 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.627652 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.571000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.623000 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.599854 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.648500 | 0.856 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.604500 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.626189 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.522000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.510000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.572500 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.634967 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.579500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.653000 | 0.856 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.622531 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.545500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.516000 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.553500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.649000 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 70 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.579500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.577000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.563500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.600000 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.606437 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.514000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.667500 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.618142 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.583500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.621068 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.515000 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.563500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.571000 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.642000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.639356 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.523500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.593500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.610827 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.518000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.655500 | 0.856 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.652524 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.550000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.602000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.593500 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.596928 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.517000 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.652500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.633504 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.612000 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.636430 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.547000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 80 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.653000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.610827 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.566000 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.593270 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.521000 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.545500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.576500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.613021 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.512500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.553500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.580000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.601000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.648500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.629115 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.516000 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.639500 | 0.856 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.643745 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 85 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.566000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.637893 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.509500 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.516000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.649000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.654000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.668617 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.647000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.640000 | 0.856 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.650329 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.601000 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.639500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.594000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.612000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.648135 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.661302 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.525000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.576000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.527500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.598000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.615216 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.601000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.596196 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.524000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.546500 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.571000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.645208 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 95 | Batch: 000 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.526000 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.585500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.583500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.611500 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.623000 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.600585 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.519500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.648500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.628383 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.574000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.576000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.613021 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.636430 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.517500 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.606000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.640819 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 100 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.603500 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.650000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 100 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.640088 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.546500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.603500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.582500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 101 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.524000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 011 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 102 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.626189 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.614500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 103 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.658500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 104 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.586500 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.599000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 011 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.612000 | 0.856 sec/iter\n",
      "Epoch: 105 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.636430 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.612000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 106 | Batch: 012 / 013 | Total loss: 1.059 | Reg loss: 0.038 | Tree loss: 1.059 | Accuracy: 0.645940 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.582500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 107 | Batch: 012 / 013 | Total loss: 1.050 | Reg loss: 0.038 | Tree loss: 1.050 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.565000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108 | Batch: 005 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 108 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.639356 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.038 | Tree loss: 1.295 | Accuracy: 0.513500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.597500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 109 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.617410 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 110 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.545500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.599500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.646500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.649000 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 110 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.575500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.599000 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 011 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 111 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.038 | Tree loss: 1.301 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.643000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 112 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.573000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 113 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.576000 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.623000 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.652500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 114 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.608632 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.610500 | 0.856 sec/iter\n",
      "Epoch: 115 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.618142 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.649000 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 116 | Batch: 012 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.653255 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 117 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.632772 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.524500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 118 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.617410 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.613000 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 119 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 119 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.631309 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 120 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.629115 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.572500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.599500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 011 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 121 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.577000 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.574000 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.656500 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 122 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 122 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.632772 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 011 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 123 | Batch: 012 / 013 | Total loss: 1.061 | Reg loss: 0.038 | Tree loss: 1.061 | Accuracy: 0.651061 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 124 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.649598 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 125 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 125 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 126 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.628383 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 127 | Batch: 012 / 013 | Total loss: 1.064 | Reg loss: 0.038 | Tree loss: 1.064 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 013 | Total loss: 1.307 | Reg loss: 0.038 | Tree loss: 1.307 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.652500 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 128 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.632772 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.613000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 011 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 129 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.662000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 130 | Batch: 012 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.638625 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 131 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.629115 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.517000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 132 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.637893 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.525000 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.592500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 011 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 133 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.637162 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.552000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 134 | Batch: 012 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.637893 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 135 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 135 | Batch: 012 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.640819 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 136 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.524500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 137 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.613753 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.638500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 138 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.632772 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 139 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.610095 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 140 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.658000 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 140 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 141 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.621068 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 013 | Total loss: 1.312 | Reg loss: 0.038 | Tree loss: 1.312 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.663500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 142 | Batch: 012 / 013 | Total loss: 1.061 | Reg loss: 0.038 | Tree loss: 1.061 | Accuracy: 0.637893 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 143 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.623263 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.038 | Tree loss: 1.305 | Accuracy: 0.509500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.613000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.647500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.647500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 144 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 145 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 145 | Batch: 012 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.625457 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 146 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.624726 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.553500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 147 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.588500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 148 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.648135 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 149 | Batch: 012 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.614484 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 150 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.652500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 150 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.626920 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.506500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.651000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 151 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 152 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.633504 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.583500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 153 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.617410 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 154 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.660500 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 154 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 155 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 155 | Batch: 012 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.603511 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.657000 | 0.857 sec/iter\n",
      "Epoch: 156 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 013 | Total loss: 1.306 | Reg loss: 0.038 | Tree loss: 1.306 | Accuracy: 0.517000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.652500 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 157 | Batch: 012 / 013 | Total loss: 1.072 | Reg loss: 0.038 | Tree loss: 1.072 | Accuracy: 0.653987 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 013 | Total loss: 1.315 | Reg loss: 0.038 | Tree loss: 1.315 | Accuracy: 0.514000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.588500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 158 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.633504 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 159 | Batch: 012 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.606437 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 160 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.552500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 160 | Batch: 012 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 161 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.626920 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.578000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 162 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 163 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.613753 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.645500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 164 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.618142 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.660500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 165 | Batch: 012 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.662034 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 011 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 166 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.606437 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 011 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 167 | Batch: 012 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 011 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 168 | Batch: 012 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.599122 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169 | Batch: 000 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.520500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.655500 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 169 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 170 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 171 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.655450 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.647500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 172 | Batch: 012 / 013 | Total loss: 1.066 | Reg loss: 0.038 | Tree loss: 1.066 | Accuracy: 0.659839 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.572000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 173 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 173 | Batch: 012 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 174 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.607000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 011 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 174 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.669349 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 175 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.652500 | 0.857 sec/iter\n",
      "Epoch: 175 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.645208 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.509000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 176 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.626189 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.644500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.658500 | 0.857 sec/iter\n",
      "Epoch: 177 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 178 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 179 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.649598 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 011 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.668000 | 0.857 sec/iter\n",
      "Epoch: 180 | Batch: 012 / 013 | Total loss: 1.074 | Reg loss: 0.038 | Tree loss: 1.074 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 181 | Batch: 012 / 013 | Total loss: 1.066 | Reg loss: 0.038 | Tree loss: 1.066 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 182 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.626920 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.668500 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 183 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.660571 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 184 | Batch: 012 / 013 | Total loss: 1.070 | Reg loss: 0.038 | Tree loss: 1.070 | Accuracy: 0.662034 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 185 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.517000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 011 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.663500 | 0.857 sec/iter\n",
      "Epoch: 185 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.569500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 186 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.664500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.661500 | 0.857 sec/iter\n",
      "Epoch: 187 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.616679 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 188 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.646672 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 013 | Total loss: 1.309 | Reg loss: 0.038 | Tree loss: 1.309 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 189 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.628383 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 190 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.633000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 190 | Batch: 012 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.520500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 191 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.632772 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 192 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 193 | Batch: 012 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.652524 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.038 | Tree loss: 1.301 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 194 | Batch: 012 / 013 | Total loss: 1.075 | Reg loss: 0.038 | Tree loss: 1.075 | Accuracy: 0.658376 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Batch: 000 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.590500 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 195 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 196 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.626920 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 197 | Batch: 012 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.648866 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 198 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 199 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.539000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.649000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 011 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.655000 | 0.857 sec/iter\n",
      "Epoch: 199 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 200 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.607000 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 200 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 201 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.593500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 202 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.613021 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.520500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.630500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 203 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.038 | Tree loss: 1.295 | Accuracy: 0.518500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 204 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 205 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 205 | Batch: 012 / 013 | Total loss: 1.076 | Reg loss: 0.038 | Tree loss: 1.076 | Accuracy: 0.637162 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 013 | Total loss: 1.302 | Reg loss: 0.038 | Tree loss: 1.302 | Accuracy: 0.504000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 206 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.624726 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 207 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.634967 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.590500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 208 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 013 | Total loss: 1.315 | Reg loss: 0.038 | Tree loss: 1.315 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.593500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 209 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 210 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 210 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.628383 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.515500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 211 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.629846 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.519000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.567500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 212 | Batch: 012 / 013 | Total loss: 1.066 | Reg loss: 0.038 | Tree loss: 1.066 | Accuracy: 0.667154 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.522500 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.525000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 213 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.648866 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 214 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 214 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.633504 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 215 | Batch: 000 / 013 | Total loss: 1.313 | Reg loss: 0.038 | Tree loss: 1.313 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 215 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.641551 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.638500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 216 | Batch: 012 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.614484 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.607000 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 217 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 013 | Total loss: 1.312 | Reg loss: 0.038 | Tree loss: 1.312 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 218 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 219 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.645940 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 220 | Batch: 000 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 220 | Batch: 012 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.610827 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 221 | Batch: 012 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 222 | Batch: 012 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.583500 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.656500 | 0.857 sec/iter\n",
      "Epoch: 223 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.509500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.667500 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 011 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 224 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.629846 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 225 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.514000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.577500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 | Batch: 005 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 225 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.634967 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 226 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.519000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.590500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 227 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.610095 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 011 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.660500 | 0.857 sec/iter\n",
      "Epoch: 228 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640819 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 229 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 013 | Total loss: 1.072 | Reg loss: 0.038 | Tree loss: 1.072 | Accuracy: 0.658500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 229 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634967 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 230 | Batch: 000 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 230 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.512500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 011 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 231 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.625457 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 232 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.641551 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 233 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 234 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 235 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.516500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 235 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.613753 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.650000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 011 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 236 | Batch: 012 / 013 | Total loss: 1.056 | Reg loss: 0.038 | Tree loss: 1.056 | Accuracy: 0.661302 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 011 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 237 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.550000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 238 | Batch: 012 / 013 | Total loss: 1.059 | Reg loss: 0.038 | Tree loss: 1.059 | Accuracy: 0.648866 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 239 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 011 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 239 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 240 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.662500 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 240 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.516500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 241 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.644477 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.640000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 242 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.616679 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.593500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 243 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.608632 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 244 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 244 | Batch: 012 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 245 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 245 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.610827 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.514500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.578000 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.664500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 246 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.646672 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 247 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.637162 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.646000 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 248 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 249 | Batch: 000 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 249 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 250 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 250 | Batch: 012 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.642282 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.561500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 251 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.657500 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 252 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 253 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.598391 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 254 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 254 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.628383 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 255 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.515500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.603500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 255 | Batch: 010 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 255 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.612290 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 013 | Total loss: 1.306 | Reg loss: 0.038 | Tree loss: 1.306 | Accuracy: 0.516000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 256 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.640819 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.525000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 257 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 258 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.629846 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.610000 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 259 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 260 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.652524 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.518500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 011 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 261 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.621800 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.655500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 262 | Batch: 012 / 013 | Total loss: 1.076 | Reg loss: 0.038 | Tree loss: 1.076 | Accuracy: 0.651061 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.038 | Tree loss: 1.301 | Accuracy: 0.512000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 263 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.557000 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.562000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264 | Batch: 005 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 264 | Batch: 012 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.644477 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.578000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 265 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.651061 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 013 | Total loss: 1.308 | Reg loss: 0.038 | Tree loss: 1.308 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 266 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.648135 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 267 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.629115 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.567500 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.646500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 268 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.614484 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 269 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 011 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 269 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.514000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.649000 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 011 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 270 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.629846 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.516000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.619000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 271 | Batch: 012 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.623263 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.520000 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 272 | Batch: 012 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.607901 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 273 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.622531 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 274 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.585500 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.613000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 274 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 275 | Batch: 000 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.524500 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 011 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 275 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.640088 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 276 | Batch: 012 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.642282 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.506000 | 0.857 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.578000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 277 | Batch: 012 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.648866 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.546500 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.593500 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.654500 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 278 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 279 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 279 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.607901 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 280 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 280 | Batch: 012 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.598391 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 013 | Total loss: 1.310 | Reg loss: 0.038 | Tree loss: 1.310 | Accuracy: 0.522500 | 0.857 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 011 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 281 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.613021 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.514500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.576000 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 282 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.637162 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.512000 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.550000 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.588000 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 283 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.618142 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 284 | Batch: 000 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.571000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.650000 | 0.856 sec/iter\n",
      "Epoch: 284 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.643014 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 285 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.505000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.579000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 285 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.643014 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286 | Batch: 000 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 286 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.570500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.593500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 287 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.629115 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.623000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.645000 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.653500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 288 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.646672 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 289 | Batch: 000 / 013 | Total loss: 1.302 | Reg loss: 0.038 | Tree loss: 1.302 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.574000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.582500 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 011 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 289 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 290 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.567500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.649000 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.639500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 290 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.638625 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.522000 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.527500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.611500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 291 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.646672 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.545500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 292 | Batch: 012 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.610095 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.515000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.586500 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 293 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.645940 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 294 | Batch: 000 / 013 | Total loss: 1.322 | Reg loss: 0.038 | Tree loss: 1.322 | Accuracy: 0.518500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.574500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.633500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 011 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.653500 | 0.856 sec/iter\n",
      "Epoch: 294 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.637893 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 295 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 295 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 296 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.622531 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 013 | Total loss: 1.227 | Reg loss: 0.038 | Tree loss: 1.227 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.574000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.594500 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 297 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.640088 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.550000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.511500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 298 | Batch: 012 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.637893 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 000 / 013 | Total loss: 1.301 | Reg loss: 0.038 | Tree loss: 1.301 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.509000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.588000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 011 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 299 | Batch: 012 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.653987 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 300 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.545500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 300 | Batch: 012 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.658376 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.519500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.520500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.563500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.619500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 301 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.625457 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 013 | Total loss: 1.304 | Reg loss: 0.038 | Tree loss: 1.304 | Accuracy: 0.521000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.572500 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 302 | Batch: 012 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.602048 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.567500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 303 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.598500 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 303 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 304 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 304 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.627652 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 305 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.575500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.583500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 305 | Batch: 012 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.665691 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.526000 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.586500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 306 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 013 | Total loss: 1.298 | Reg loss: 0.038 | Tree loss: 1.298 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.579000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.613500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 307 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 307 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.620337 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.521000 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.648500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.657500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 308 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.621068 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.523500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.566000 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 309 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.611558 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 310 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.619500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.650500 | 0.856 sec/iter\n",
      "Epoch: 310 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.617410 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.577000 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.640000 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 311 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 312 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.599500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 011 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 312 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.619605 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.537500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.579000 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.611500 | 0.856 sec/iter\n",
      "Epoch: 313 | Batch: 012 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.664228 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 314 | Batch: 000 / 013 | Total loss: 1.305 | Reg loss: 0.038 | Tree loss: 1.305 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.597500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.603500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 314 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 315 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 315 | Batch: 012 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.627652 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.519500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.550500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.594500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.650000 | 0.856 sec/iter\n",
      "Epoch: 316 | Batch: 012 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.520000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.599000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.606000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 317 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.517000 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.516500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.604500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.631500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.639500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.637500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 318 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.631309 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 319 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.517000 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.612000 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 319 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 320 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.626500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 320 | Batch: 012 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.612290 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.512500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.594000 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 321 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.603511 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.519000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.525000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.573000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.642500 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 322 | Batch: 012 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.628383 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.579000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 011 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 323 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 324 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.519500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.512500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.580000 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.651000 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 011 / 013 | Total loss: 1.076 | Reg loss: 0.038 | Tree loss: 1.076 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 324 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.603500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 325 | Batch: 012 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.622531 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.580000 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.574000 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.645000 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 326 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 013 | Total loss: 1.303 | Reg loss: 0.038 | Tree loss: 1.303 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.585000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.594500 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.619500 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 327 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.653255 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.553500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.574500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.639500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.650000 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 328 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.649598 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 329 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.514000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 013 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.573000 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.597500 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.610500 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 011 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 329 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.613753 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 330 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.520500 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.575500 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.579500 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.649000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.661000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 011 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 330 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.638625 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.569500 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.587000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 331 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 011 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 332 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.563500 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.639500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 333 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.658376 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 334 | Batch: 000 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 334 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.614484 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 335 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.520500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.574500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.650500 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.651000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 335 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.623000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 011 / 013 | Total loss: 1.076 | Reg loss: 0.038 | Tree loss: 1.076 | Accuracy: 0.656500 | 0.856 sec/iter\n",
      "Epoch: 336 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.644477 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.519500 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.541500 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.566000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.645000 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 011 / 013 | Total loss: 1.075 | Reg loss: 0.038 | Tree loss: 1.075 | Accuracy: 0.654500 | 0.856 sec/iter\n",
      "Epoch: 337 | Batch: 012 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.608632 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 338 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.525000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.526000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.575500 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.593000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.647000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 011 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.664000 | 0.856 sec/iter\n",
      "Epoch: 338 | Batch: 012 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 339 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.619500 | 0.856 sec/iter\n",
      "Epoch: 339 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.526500 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.595500 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.598000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 340 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.612290 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 013 | Total loss: 1.304 | Reg loss: 0.038 | Tree loss: 1.304 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.552500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.581000 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 011 / 013 | Total loss: 1.071 | Reg loss: 0.038 | Tree loss: 1.071 | Accuracy: 0.665500 | 0.856 sec/iter\n",
      "Epoch: 341 | Batch: 012 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.629115 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.524000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.584500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 342 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.577000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.644000 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.614500 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 342 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.636430 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 013 | Total loss: 1.306 | Reg loss: 0.038 | Tree loss: 1.306 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.564000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.649000 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 343 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.655450 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 344 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.523500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.518000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.652500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 344 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 345 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 345 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.645940 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.627500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 346 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 346 | Batch: 012 / 013 | Total loss: 1.080 | Reg loss: 0.038 | Tree loss: 1.080 | Accuracy: 0.637893 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.519000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.566000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.614500 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 011 / 013 | Total loss: 1.063 | Reg loss: 0.038 | Tree loss: 1.063 | Accuracy: 0.653000 | 0.856 sec/iter\n",
      "Epoch: 347 | Batch: 012 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.645940 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.578000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 011 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 348 | Batch: 012 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.640819 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 349 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.536000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.568500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.578000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.600000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 349 | Batch: 012 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.633504 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 350 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.612500 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.657000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.650000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 350 | Batch: 012 / 013 | Total loss: 1.068 | Reg loss: 0.038 | Tree loss: 1.068 | Accuracy: 0.656181 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 351 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.548000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.523000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.579500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.590000 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.622500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 351 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.621068 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.517000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.528500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.626500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 352 | Batch: 012 / 013 | Total loss: 1.075 | Reg loss: 0.038 | Tree loss: 1.075 | Accuracy: 0.639356 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.583000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.581500 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.578000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.625000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 353 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.634967 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 354 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.583500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.618000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 354 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.642282 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 355 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.517000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.534500 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.576500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 355 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.565000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.597000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.641500 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 355 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.523500 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.519000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 356 | Batch: 012 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.629115 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.550000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.578000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.594500 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.608000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.643000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.644500 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.629000 | 0.856 sec/iter\n",
      "Epoch: 357 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.610827 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.517500 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.632000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.646000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 358 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.614484 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 359 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 013 | Total loss: 1.221 | Reg loss: 0.038 | Tree loss: 1.221 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.598000 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.638500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.616500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 359 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.613021 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 360 | Batch: 000 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.620500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 360 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.634236 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.523500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.535500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.540500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.577500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.572500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.589000 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.592000 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 361 | Batch: 012 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.601317 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.530500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.595000 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.641000 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 011 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 362 | Batch: 012 / 013 | Total loss: 1.056 | Reg loss: 0.038 | Tree loss: 1.056 | Accuracy: 0.659839 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.544500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.531500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.604000 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.625500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 363 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.621800 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 364 | Batch: 000 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.517500 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.545000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.604500 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.603500 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.657000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 364 | Batch: 012 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.653987 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 365 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.539500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.613500 | 0.856 sec/iter\n",
      "Epoch: 365 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.571500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.588500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 011 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 366 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.629846 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.586000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.593500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.609000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 367 | Batch: 012 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.609364 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.038 | Tree loss: 1.295 | Accuracy: 0.527500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.548500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.557500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 368 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.596500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.611000 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.653500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.623500 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 368 | Batch: 012 / 013 | Total loss: 1.072 | Reg loss: 0.038 | Tree loss: 1.072 | Accuracy: 0.627652 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 369 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.529500 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.560500 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.573500 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.615000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.636000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 369 | Batch: 012 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.611558 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 370 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 013 | Total loss: 1.226 | Reg loss: 0.038 | Tree loss: 1.226 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.567500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.591500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.631000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.635000 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.636500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.621500 | 0.856 sec/iter\n",
      "Epoch: 370 | Batch: 012 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.633504 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.551500 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.570000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.594000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.600500 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.620000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.626000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 371 | Batch: 012 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.634967 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.554500 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.563000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.605500 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.635500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 372 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.660000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 372 | Batch: 012 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.526000 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.534000 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 013 | Total loss: 1.256 | Reg loss: 0.038 | Tree loss: 1.256 | Accuracy: 0.522000 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.617000 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.615500 | 0.856 sec/iter\n",
      "Epoch: 373 | Batch: 012 / 013 | Total loss: 1.075 | Reg loss: 0.038 | Tree loss: 1.075 | Accuracy: 0.628383 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 374 | Batch: 000 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.505000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.560000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.585500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.582500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.640000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.638000 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 011 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.630500 | 0.856 sec/iter\n",
      "Epoch: 374 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.631309 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 375 | Batch: 000 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.553000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.565500 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.601500 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.613000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.630000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 375 | Batch: 012 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.607169 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.538500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.555500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.562500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.558500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.578500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.627000 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 376 | Batch: 012 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.623994 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377 | Batch: 000 / 013 | Total loss: 1.318 | Reg loss: 0.038 | Tree loss: 1.318 | Accuracy: 0.508500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.546500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.542000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.575000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.576500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.584000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.610000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.607500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.611500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 377 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.630578 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.038 | Tree loss: 1.295 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.547000 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.574500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.572000 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.589500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.608500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.642000 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 011 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 378 | Batch: 012 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.632041 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 379 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.549000 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.532500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.510500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.547500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.557500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.584500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.609500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.633500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.634000 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 379 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.634967 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 380 | Batch: 000 / 013 | Total loss: 1.309 | Reg loss: 0.038 | Tree loss: 1.309 | Accuracy: 0.519000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.525500 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.528000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.552000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.556000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.567000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.645000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.661500 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.621000 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.649500 | 0.856 sec/iter\n",
      "Epoch: 380 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.613753 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.511500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.536500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.550500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.555500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.564500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.594000 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.605000 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.622500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.618500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.641500 | 0.856 sec/iter\n",
      "Epoch: 381 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.640819 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.527000 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.555000 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.606500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.632500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.638500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.645500 | 0.856 sec/iter\n",
      "Epoch: 382 | Batch: 012 / 013 | Total loss: 1.079 | Reg loss: 0.038 | Tree loss: 1.079 | Accuracy: 0.629846 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 013 | Total loss: 1.299 | Reg loss: 0.038 | Tree loss: 1.299 | Accuracy: 0.522000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.549500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.541000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.559500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.571500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.580500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.607000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.619000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.651500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 011 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 383 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.626920 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 384 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.537000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.531000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.543000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.543500 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.558000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.603000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.590500 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.634500 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.639000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 384 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.618873 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 385 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.538000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.546000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.557000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.568000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.569000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.582000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.592500 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.614000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.645500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 385 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.614500 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.624000 | 0.856 sec/iter\n",
      "Epoch: 385 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.607901 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.525000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.533000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.533500 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.551000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.591000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.616000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.628500 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.644000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 013 | Total loss: 1.080 | Reg loss: 0.038 | Tree loss: 1.080 | Accuracy: 0.658000 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.650500 | 0.856 sec/iter\n",
      "Epoch: 386 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.645940 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 013 | Total loss: 1.309 | Reg loss: 0.038 | Tree loss: 1.309 | Accuracy: 0.520500 | 0.857 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.539000 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.535000 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 013 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.561000 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.566500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.587500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.594000 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.633000 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.629500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 011 / 013 | Total loss: 1.073 | Reg loss: 0.038 | Tree loss: 1.073 | Accuracy: 0.647500 | 0.856 sec/iter\n",
      "Epoch: 387 | Batch: 012 / 013 | Total loss: 1.076 | Reg loss: 0.038 | Tree loss: 1.076 | Accuracy: 0.624726 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.530000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 013 | Total loss: 1.302 | Reg loss: 0.038 | Tree loss: 1.302 | Accuracy: 0.512000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.521000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.521500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.562000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.586500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.602500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.635500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.648000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.627500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.661500 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.637000 | 0.856 sec/iter\n",
      "Epoch: 388 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.634967 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 389 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.524500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.524000 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.544000 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.542500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.559000 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.577000 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.561500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.579500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.610500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.610500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.617500 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.628000 | 0.856 sec/iter\n",
      "Epoch: 389 | Batch: 012 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 390 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.532000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.529000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.540000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.556500 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.554000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.596000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.576000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.643500 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.624500 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.622000 | 0.856 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 390 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 390 | Batch: 012 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.519000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.554500 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.598500 | 0.856 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 391 | Batch: 012 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.601317 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.655000 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 392 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.635699 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 013 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 013 | Total loss: 1.236 | Reg loss: 0.038 | Tree loss: 1.236 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 013 | Total loss: 1.203 | Reg loss: 0.038 | Tree loss: 1.203 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 393 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.615216 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 394 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.575000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 394 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.657500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 394 | Batch: 012 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.640819 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.517000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 395 | Batch: 012 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.622531 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.647500 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 396 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.618873 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 011 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 397 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.603511 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.570500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.620000 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.635500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 398 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 398 | Batch: 012 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.614484 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 399 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.574500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 399 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.632772 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 400 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.576500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 400 | Batch: 012 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.596000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 401 | Batch: 012 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.608632 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 013 | Total loss: 1.315 | Reg loss: 0.038 | Tree loss: 1.315 | Accuracy: 0.511000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 402 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 403 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.548000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 403 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.603511 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 404 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 011 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 404 | Batch: 012 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.653987 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 405 | Batch: 000 / 013 | Total loss: 1.304 | Reg loss: 0.038 | Tree loss: 1.304 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 013 | Total loss: 1.164 | Reg loss: 0.038 | Tree loss: 1.164 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.603500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 011 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 405 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.645208 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 406 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.626189 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.528000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 013 | Total loss: 1.171 | Reg loss: 0.038 | Tree loss: 1.171 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 013 | Total loss: 1.188 | Reg loss: 0.038 | Tree loss: 1.188 | Accuracy: 0.552000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 407 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.618500 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 011 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 407 | Batch: 012 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.519500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 011 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 408 | Batch: 012 / 013 | Total loss: 1.078 | Reg loss: 0.038 | Tree loss: 1.078 | Accuracy: 0.643745 | 0.856 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 409 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 013 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.583500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 011 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 409 | Batch: 012 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.649598 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 410 | Batch: 000 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 013 | Total loss: 1.213 | Reg loss: 0.038 | Tree loss: 1.213 | Accuracy: 0.545500 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 013 | Total loss: 1.184 | Reg loss: 0.038 | Tree loss: 1.184 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 410 | Batch: 012 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.631309 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.521500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.607000 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.615000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 411 | Batch: 010 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 411 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 013 | Total loss: 1.309 | Reg loss: 0.038 | Tree loss: 1.309 | Accuracy: 0.507000 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 412 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.647403 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.625500 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 413 | Batch: 012 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 414 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.606500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 011 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 414 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.617410 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 415 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.566000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.609500 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 415 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 416 | Batch: 000 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 013 | Total loss: 1.283 | Reg loss: 0.038 | Tree loss: 1.283 | Accuracy: 0.512500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.649500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 011 / 013 | Total loss: 1.084 | Reg loss: 0.038 | Tree loss: 1.084 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 416 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.662765 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.594000 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 417 | Batch: 012 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 013 | Total loss: 1.247 | Reg loss: 0.038 | Tree loss: 1.247 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 013 | Total loss: 1.191 | Reg loss: 0.038 | Tree loss: 1.191 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 011 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 418 | Batch: 012 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.614484 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 419 | Batch: 000 / 013 | Total loss: 1.310 | Reg loss: 0.038 | Tree loss: 1.310 | Accuracy: 0.511500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 013 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 013 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 419 | Batch: 012 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.623994 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 420 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.540000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 013 | Total loss: 1.205 | Reg loss: 0.038 | Tree loss: 1.205 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.569000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 420 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.611000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 420 | Batch: 012 / 013 | Total loss: 1.075 | Reg loss: 0.038 | Tree loss: 1.075 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 421 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.640819 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 013 | Total loss: 1.276 | Reg loss: 0.038 | Tree loss: 1.276 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 013 | Total loss: 1.225 | Reg loss: 0.038 | Tree loss: 1.225 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.596500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.653500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.609000 | 0.857 sec/iter\n",
      "Epoch: 422 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.658376 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 011 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 423 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 424 | Batch: 000 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.511500 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 013 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 013 | Total loss: 1.151 | Reg loss: 0.038 | Tree loss: 1.151 | Accuracy: 0.578000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.603000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.657000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.629000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424 | Batch: 010 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 424 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.630578 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 425 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 013 | Total loss: 1.233 | Reg loss: 0.038 | Tree loss: 1.233 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.558000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.611500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 011 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 425 | Batch: 012 / 013 | Total loss: 1.089 | Reg loss: 0.038 | Tree loss: 1.089 | Accuracy: 0.641551 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 013 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.623000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 011 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.622500 | 0.857 sec/iter\n",
      "Epoch: 426 | Batch: 012 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.546000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.628000 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 427 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.623994 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.524500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.544500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 013 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 428 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.649598 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429 | Batch: 000 / 013 | Total loss: 1.295 | Reg loss: 0.038 | Tree loss: 1.295 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 013 | Total loss: 1.284 | Reg loss: 0.038 | Tree loss: 1.284 | Accuracy: 0.524000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 013 | Total loss: 1.208 | Reg loss: 0.038 | Tree loss: 1.208 | Accuracy: 0.551500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.565500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.607000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.616000 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 429 | Batch: 012 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.621068 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 430 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 013 | Total loss: 1.296 | Reg loss: 0.038 | Tree loss: 1.296 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 013 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.587000 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.638500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 011 / 013 | Total loss: 1.081 | Reg loss: 0.038 | Tree loss: 1.081 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 430 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.625457 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 013 | Total loss: 1.288 | Reg loss: 0.038 | Tree loss: 1.288 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 013 | Total loss: 1.245 | Reg loss: 0.038 | Tree loss: 1.245 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 013 | Total loss: 1.214 | Reg loss: 0.038 | Tree loss: 1.214 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 013 | Total loss: 1.144 | Reg loss: 0.038 | Tree loss: 1.144 | Accuracy: 0.588500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.594500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 011 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.642500 | 0.857 sec/iter\n",
      "Epoch: 431 | Batch: 012 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.613753 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 013 | Total loss: 1.242 | Reg loss: 0.038 | Tree loss: 1.242 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.622000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.626500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 432 | Batch: 012 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.637893 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 013 | Total loss: 1.223 | Reg loss: 0.038 | Tree loss: 1.223 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 013 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.576500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 433 | Batch: 005 / 013 | Total loss: 1.154 | Reg loss: 0.038 | Tree loss: 1.154 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.615500 | 0.857 sec/iter\n",
      "Epoch: 433 | Batch: 012 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.636430 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 434 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.527000 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 013 | Total loss: 1.179 | Reg loss: 0.038 | Tree loss: 1.179 | Accuracy: 0.569500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.589000 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 011 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 434 | Batch: 012 / 013 | Total loss: 1.085 | Reg loss: 0.038 | Tree loss: 1.085 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 435 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 013 | Total loss: 1.253 | Reg loss: 0.038 | Tree loss: 1.253 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.553500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 013 | Total loss: 1.170 | Reg loss: 0.038 | Tree loss: 1.170 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.604500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 011 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 435 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621068 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 013 | Total loss: 1.251 | Reg loss: 0.038 | Tree loss: 1.251 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.538000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 013 | Total loss: 1.175 | Reg loss: 0.038 | Tree loss: 1.175 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 013 | Total loss: 1.091 | Reg loss: 0.038 | Tree loss: 1.091 | Accuracy: 0.654500 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 011 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 436 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.637162 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 013 | Total loss: 1.218 | Reg loss: 0.038 | Tree loss: 1.218 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.571000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.554000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.584000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.602500 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.621000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 437 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 011 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 437 | Batch: 012 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.643014 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 013 | Total loss: 1.241 | Reg loss: 0.038 | Tree loss: 1.241 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 013 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.641000 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 438 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.644477 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 439 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 013 | Total loss: 1.080 | Reg loss: 0.038 | Tree loss: 1.080 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.623500 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 011 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 439 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 440 | Batch: 000 / 013 | Total loss: 1.289 | Reg loss: 0.038 | Tree loss: 1.289 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 013 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.563000 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.588000 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 011 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 440 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.624726 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.523500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 013 | Total loss: 1.220 | Reg loss: 0.038 | Tree loss: 1.220 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.598000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 013 | Total loss: 1.083 | Reg loss: 0.038 | Tree loss: 1.083 | Accuracy: 0.648000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 011 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.661000 | 0.857 sec/iter\n",
      "Epoch: 441 | Batch: 012 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.638625 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 442 | Batch: 000 / 013 | Total loss: 1.278 | Reg loss: 0.038 | Tree loss: 1.278 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 013 | Total loss: 1.250 | Reg loss: 0.038 | Tree loss: 1.250 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.530500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 013 | Total loss: 1.187 | Reg loss: 0.038 | Tree loss: 1.187 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 013 | Total loss: 1.178 | Reg loss: 0.038 | Tree loss: 1.178 | Accuracy: 0.559000 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.600500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.633500 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 011 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 442 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.618142 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 013 | Total loss: 1.304 | Reg loss: 0.038 | Tree loss: 1.304 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 013 | Total loss: 1.232 | Reg loss: 0.038 | Tree loss: 1.232 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 013 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.604000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 011 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 443 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 444 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 013 | Total loss: 1.267 | Reg loss: 0.038 | Tree loss: 1.267 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 013 | Total loss: 1.210 | Reg loss: 0.038 | Tree loss: 1.210 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.621000 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.653500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.645500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 011 / 013 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 444 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.623994 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 445 | Batch: 000 / 013 | Total loss: 1.290 | Reg loss: 0.038 | Tree loss: 1.290 | Accuracy: 0.525000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.564500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 013 | Total loss: 1.240 | Reg loss: 0.038 | Tree loss: 1.240 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.549500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 013 | Total loss: 1.166 | Reg loss: 0.038 | Tree loss: 1.166 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.583500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.605500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 013 | Total loss: 1.135 | Reg loss: 0.038 | Tree loss: 1.135 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.634500 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 011 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 445 | Batch: 012 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.642282 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.532500 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 013 | Total loss: 1.231 | Reg loss: 0.038 | Tree loss: 1.231 | Accuracy: 0.558500 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 013 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 013 | Total loss: 1.206 | Reg loss: 0.038 | Tree loss: 1.206 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.572500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.577000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.647000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.645000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 011 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.640500 | 0.857 sec/iter\n",
      "Epoch: 446 | Batch: 012 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.610095 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 013 | Total loss: 1.273 | Reg loss: 0.038 | Tree loss: 1.273 | Accuracy: 0.522000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 013 | Total loss: 1.239 | Reg loss: 0.038 | Tree loss: 1.239 | Accuracy: 0.550000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.560000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.582000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.597000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 011 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.632000 | 0.857 sec/iter\n",
      "Epoch: 447 | Batch: 012 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.608632 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.549000 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.552000 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 013 | Total loss: 1.169 | Reg loss: 0.038 | Tree loss: 1.169 | Accuracy: 0.562000 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.631500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.650500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.648500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.639500 | 0.857 sec/iter\n",
      "Epoch: 448 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.640088 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 449 | Batch: 000 / 013 | Total loss: 1.271 | Reg loss: 0.038 | Tree loss: 1.271 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 013 | Total loss: 1.186 | Reg loss: 0.038 | Tree loss: 1.186 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 013 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.569000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 013 | Total loss: 1.118 | Reg loss: 0.038 | Tree loss: 1.118 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.619500 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 011 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.628500 | 0.857 sec/iter\n",
      "Epoch: 449 | Batch: 012 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 450 | Batch: 000 / 013 | Total loss: 1.285 | Reg loss: 0.038 | Tree loss: 1.285 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.547000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 013 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.599000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 013 | Total loss: 1.129 | Reg loss: 0.038 | Tree loss: 1.129 | Accuracy: 0.608500 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.638000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.639500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450 | Batch: 010 / 013 | Total loss: 1.088 | Reg loss: 0.038 | Tree loss: 1.088 | Accuracy: 0.651500 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 450 | Batch: 012 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.618873 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 013 | Total loss: 1.287 | Reg loss: 0.038 | Tree loss: 1.287 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 013 | Total loss: 1.265 | Reg loss: 0.038 | Tree loss: 1.265 | Accuracy: 0.534000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 013 | Total loss: 1.222 | Reg loss: 0.038 | Tree loss: 1.222 | Accuracy: 0.531000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 013 | Total loss: 1.161 | Reg loss: 0.038 | Tree loss: 1.161 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.564000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 013 | Total loss: 1.148 | Reg loss: 0.038 | Tree loss: 1.148 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.630500 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.652000 | 0.857 sec/iter\n",
      "Epoch: 451 | Batch: 012 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.619605 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 013 | Total loss: 1.274 | Reg loss: 0.038 | Tree loss: 1.274 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 013 | Total loss: 1.258 | Reg loss: 0.038 | Tree loss: 1.258 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 013 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.544000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 013 | Total loss: 1.147 | Reg loss: 0.038 | Tree loss: 1.147 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.591500 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.637500 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.644000 | 0.857 sec/iter\n",
      "Epoch: 452 | Batch: 012 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.662034 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 013 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.517500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 013 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.565000 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.567000 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.579500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.586000 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 011 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 453 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 454 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.533000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 013 | Total loss: 1.270 | Reg loss: 0.038 | Tree loss: 1.270 | Accuracy: 0.526500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.556000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 013 | Total loss: 1.168 | Reg loss: 0.038 | Tree loss: 1.168 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 013 | Total loss: 1.146 | Reg loss: 0.038 | Tree loss: 1.146 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.624000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.662500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.632500 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 011 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.651000 | 0.857 sec/iter\n",
      "Epoch: 454 | Batch: 012 / 013 | Total loss: 1.138 | Reg loss: 0.038 | Tree loss: 1.138 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455 | Batch: 000 / 013 | Total loss: 1.282 | Reg loss: 0.038 | Tree loss: 1.282 | Accuracy: 0.545000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 013 | Total loss: 1.252 | Reg loss: 0.038 | Tree loss: 1.252 | Accuracy: 0.540500 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 013 | Total loss: 1.192 | Reg loss: 0.038 | Tree loss: 1.192 | Accuracy: 0.570000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 013 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 013 | Total loss: 1.160 | Reg loss: 0.038 | Tree loss: 1.160 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.593000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.608000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 013 | Total loss: 1.094 | Reg loss: 0.038 | Tree loss: 1.094 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 011 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.610500 | 0.857 sec/iter\n",
      "Epoch: 455 | Batch: 012 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.637893 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.552500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 013 | Total loss: 1.264 | Reg loss: 0.038 | Tree loss: 1.264 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.561500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.583500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 013 | Total loss: 1.183 | Reg loss: 0.038 | Tree loss: 1.183 | Accuracy: 0.571500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.589500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 013 | Total loss: 1.133 | Reg loss: 0.038 | Tree loss: 1.133 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 013 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.658000 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.617500 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 013 | Total loss: 1.131 | Reg loss: 0.038 | Tree loss: 1.131 | Accuracy: 0.617000 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 011 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 456 | Batch: 012 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.586686 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 013 | Total loss: 1.261 | Reg loss: 0.038 | Tree loss: 1.261 | Accuracy: 0.539000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.542000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.566500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.607500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 013 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 013 | Total loss: 1.101 | Reg loss: 0.038 | Tree loss: 1.101 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.618000 | 0.857 sec/iter\n",
      "Epoch: 457 | Batch: 012 / 013 | Total loss: 1.098 | Reg loss: 0.038 | Tree loss: 1.098 | Accuracy: 0.627652 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.539500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.526000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 013 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.560500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.573000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 013 | Total loss: 1.150 | Reg loss: 0.038 | Tree loss: 1.150 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.612500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 013 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.635500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 013 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.658000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.629000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 011 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.631000 | 0.857 sec/iter\n",
      "Epoch: 458 | Batch: 012 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.618873 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 013 | Total loss: 1.279 | Reg loss: 0.038 | Tree loss: 1.279 | Accuracy: 0.535000 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 013 | Total loss: 1.268 | Reg loss: 0.038 | Tree loss: 1.268 | Accuracy: 0.529500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 013 | Total loss: 1.243 | Reg loss: 0.038 | Tree loss: 1.243 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 013 | Total loss: 1.207 | Reg loss: 0.038 | Tree loss: 1.207 | Accuracy: 0.541500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 013 | Total loss: 1.173 | Reg loss: 0.038 | Tree loss: 1.173 | Accuracy: 0.591500 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 459 | Batch: 005 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.587500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.606000 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.602000 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 011 / 013 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.613500 | 0.857 sec/iter\n",
      "Epoch: 459 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.616679 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 460 | Batch: 000 / 013 | Total loss: 1.275 | Reg loss: 0.038 | Tree loss: 1.275 | Accuracy: 0.537000 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 013 | Total loss: 1.234 | Reg loss: 0.038 | Tree loss: 1.234 | Accuracy: 0.533500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 013 | Total loss: 1.202 | Reg loss: 0.038 | Tree loss: 1.202 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.535500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 013 | Total loss: 1.111 | Reg loss: 0.038 | Tree loss: 1.111 | Accuracy: 0.646500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.629500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 013 | Total loss: 1.113 | Reg loss: 0.038 | Tree loss: 1.113 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 011 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 460 | Batch: 012 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 013 | Total loss: 1.291 | Reg loss: 0.038 | Tree loss: 1.291 | Accuracy: 0.518500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 013 | Total loss: 1.257 | Reg loss: 0.038 | Tree loss: 1.257 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 013 | Total loss: 1.248 | Reg loss: 0.038 | Tree loss: 1.248 | Accuracy: 0.534500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 013 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 013 | Total loss: 1.162 | Reg loss: 0.038 | Tree loss: 1.162 | Accuracy: 0.580000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.580500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.585000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 013 | Total loss: 1.109 | Reg loss: 0.038 | Tree loss: 1.109 | Accuracy: 0.612000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 013 | Total loss: 1.143 | Reg loss: 0.038 | Tree loss: 1.143 | Accuracy: 0.592000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.601000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.601500 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 011 / 013 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 461 | Batch: 012 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.635699 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 013 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.537500 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.527500 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.548500 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 013 | Total loss: 1.198 | Reg loss: 0.038 | Tree loss: 1.198 | Accuracy: 0.553000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 013 | Total loss: 1.167 | Reg loss: 0.038 | Tree loss: 1.167 | Accuracy: 0.581500 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 013 | Total loss: 1.156 | Reg loss: 0.038 | Tree loss: 1.156 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 013 | Total loss: 1.153 | Reg loss: 0.038 | Tree loss: 1.153 | Accuracy: 0.579000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.637000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.639000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.653000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 011 / 013 | Total loss: 1.105 | Reg loss: 0.038 | Tree loss: 1.105 | Accuracy: 0.633000 | 0.857 sec/iter\n",
      "Epoch: 462 | Batch: 012 / 013 | Total loss: 1.082 | Reg loss: 0.038 | Tree loss: 1.082 | Accuracy: 0.655450 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 013 | Total loss: 1.281 | Reg loss: 0.038 | Tree loss: 1.281 | Accuracy: 0.531500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 013 | Total loss: 1.249 | Reg loss: 0.038 | Tree loss: 1.249 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 013 | Total loss: 1.228 | Reg loss: 0.038 | Tree loss: 1.228 | Accuracy: 0.530000 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 013 | Total loss: 1.216 | Reg loss: 0.038 | Tree loss: 1.216 | Accuracy: 0.550500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 013 | Total loss: 1.182 | Reg loss: 0.038 | Tree loss: 1.182 | Accuracy: 0.568000 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 013 | Total loss: 1.149 | Reg loss: 0.038 | Tree loss: 1.149 | Accuracy: 0.572500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.588500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.582500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 013 | Total loss: 1.125 | Reg loss: 0.038 | Tree loss: 1.125 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 013 | Total loss: 1.123 | Reg loss: 0.038 | Tree loss: 1.123 | Accuracy: 0.608000 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 463 | Batch: 010 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.627000 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 011 / 013 | Total loss: 1.087 | Reg loss: 0.038 | Tree loss: 1.087 | Accuracy: 0.644500 | 0.857 sec/iter\n",
      "Epoch: 463 | Batch: 012 / 013 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.620337 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 464 | Batch: 000 / 013 | Total loss: 1.300 | Reg loss: 0.038 | Tree loss: 1.300 | Accuracy: 0.514500 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 013 | Total loss: 1.269 | Reg loss: 0.038 | Tree loss: 1.269 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.521000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 013 | Total loss: 1.194 | Reg loss: 0.038 | Tree loss: 1.194 | Accuracy: 0.557500 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 013 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.575500 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 013 | Total loss: 1.159 | Reg loss: 0.038 | Tree loss: 1.159 | Accuracy: 0.597500 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 013 | Total loss: 1.092 | Reg loss: 0.038 | Tree loss: 1.092 | Accuracy: 0.626000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.615000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 013 | Total loss: 1.112 | Reg loss: 0.038 | Tree loss: 1.112 | Accuracy: 0.636000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.625000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 011 / 013 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.643000 | 0.857 sec/iter\n",
      "Epoch: 464 | Batch: 012 / 013 | Total loss: 1.074 | Reg loss: 0.038 | Tree loss: 1.074 | Accuracy: 0.654718 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 465 | Batch: 000 / 013 | Total loss: 1.266 | Reg loss: 0.038 | Tree loss: 1.266 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 013 | Total loss: 1.260 | Reg loss: 0.038 | Tree loss: 1.260 | Accuracy: 0.538500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 013 | Total loss: 1.219 | Reg loss: 0.038 | Tree loss: 1.219 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.542500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 013 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.556500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.578500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 013 | Total loss: 1.140 | Reg loss: 0.038 | Tree loss: 1.140 | Accuracy: 0.574000 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.583000 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.605000 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 013 | Total loss: 1.124 | Reg loss: 0.038 | Tree loss: 1.124 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.654000 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 465 | Batch: 012 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.639356 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 013 | Total loss: 1.294 | Reg loss: 0.038 | Tree loss: 1.294 | Accuracy: 0.524500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.523000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 013 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.536000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.563500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 013 | Total loss: 1.180 | Reg loss: 0.038 | Tree loss: 1.180 | Accuracy: 0.572000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.584500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 013 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.590000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 013 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.635000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 013 | Total loss: 1.121 | Reg loss: 0.038 | Tree loss: 1.121 | Accuracy: 0.630000 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.641500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 011 / 013 | Total loss: 1.128 | Reg loss: 0.038 | Tree loss: 1.128 | Accuracy: 0.624500 | 0.857 sec/iter\n",
      "Epoch: 466 | Batch: 012 / 013 | Total loss: 1.097 | Reg loss: 0.038 | Tree loss: 1.097 | Accuracy: 0.637162 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 013 | Total loss: 1.293 | Reg loss: 0.038 | Tree loss: 1.293 | Accuracy: 0.528500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 013 | Total loss: 1.263 | Reg loss: 0.038 | Tree loss: 1.263 | Accuracy: 0.547500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.543000 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 013 | Total loss: 1.189 | Reg loss: 0.038 | Tree loss: 1.189 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 013 | Total loss: 1.185 | Reg loss: 0.038 | Tree loss: 1.185 | Accuracy: 0.577500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 013 | Total loss: 1.145 | Reg loss: 0.038 | Tree loss: 1.145 | Accuracy: 0.573500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.586500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 013 | Total loss: 1.134 | Reg loss: 0.038 | Tree loss: 1.134 | Accuracy: 0.576000 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.599500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.598500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 013 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.614500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 011 / 013 | Total loss: 1.100 | Reg loss: 0.038 | Tree loss: 1.100 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 467 | Batch: 012 / 013 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.643745 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 468 | Batch: 000 / 013 | Total loss: 1.297 | Reg loss: 0.038 | Tree loss: 1.297 | Accuracy: 0.520500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 013 | Total loss: 1.259 | Reg loss: 0.038 | Tree loss: 1.259 | Accuracy: 0.536500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 013 | Total loss: 1.212 | Reg loss: 0.038 | Tree loss: 1.212 | Accuracy: 0.555000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 013 | Total loss: 1.193 | Reg loss: 0.038 | Tree loss: 1.193 | Accuracy: 0.555500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 013 | Total loss: 1.163 | Reg loss: 0.038 | Tree loss: 1.163 | Accuracy: 0.575000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.591000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 013 | Total loss: 1.130 | Reg loss: 0.038 | Tree loss: 1.130 | Accuracy: 0.621500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 013 | Total loss: 1.137 | Reg loss: 0.038 | Tree loss: 1.137 | Accuracy: 0.642000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 013 | Total loss: 1.093 | Reg loss: 0.038 | Tree loss: 1.093 | Accuracy: 0.655000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 013 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.627500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 013 | Total loss: 1.115 | Reg loss: 0.038 | Tree loss: 1.115 | Accuracy: 0.636500 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 011 / 013 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.634000 | 0.857 sec/iter\n",
      "Epoch: 468 | Batch: 012 / 013 | Total loss: 1.120 | Reg loss: 0.038 | Tree loss: 1.120 | Accuracy: 0.611558 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 469 | Batch: 000 / 013 | Total loss: 1.286 | Reg loss: 0.038 | Tree loss: 1.286 | Accuracy: 0.525500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 013 | Total loss: 1.246 | Reg loss: 0.038 | Tree loss: 1.246 | Accuracy: 0.559500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 013 | Total loss: 1.229 | Reg loss: 0.038 | Tree loss: 1.229 | Accuracy: 0.543500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 013 | Total loss: 1.196 | Reg loss: 0.038 | Tree loss: 1.196 | Accuracy: 0.561000 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 013 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.551000 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.562500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 013 | Total loss: 1.136 | Reg loss: 0.038 | Tree loss: 1.136 | Accuracy: 0.581000 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 013 | Total loss: 1.127 | Reg loss: 0.038 | Tree loss: 1.127 | Accuracy: 0.600000 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 013 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.614000 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 013 | Total loss: 1.117 | Reg loss: 0.038 | Tree loss: 1.117 | Accuracy: 0.616500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 011 / 013 | Total loss: 1.103 | Reg loss: 0.038 | Tree loss: 1.103 | Accuracy: 0.620500 | 0.857 sec/iter\n",
      "Epoch: 469 | Batch: 012 / 013 | Total loss: 1.102 | Reg loss: 0.038 | Tree loss: 1.102 | Accuracy: 0.634236 | 0.857 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 470 | Batch: 000 / 013 | Total loss: 1.292 | Reg loss: 0.038 | Tree loss: 1.292 | Accuracy: 0.529000 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 013 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.532000 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 013 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.541000 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 013 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.568500 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 013 | Total loss: 1.177 | Reg loss: 0.038 | Tree loss: 1.177 | Accuracy: 0.546500 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 013 | Total loss: 1.152 | Reg loss: 0.038 | Tree loss: 1.152 | Accuracy: 0.595000 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 013 | Total loss: 1.142 | Reg loss: 0.038 | Tree loss: 1.142 | Accuracy: 0.595500 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 013 | Total loss: 1.116 | Reg loss: 0.038 | Tree loss: 1.116 | Accuracy: 0.640000 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 013 | Total loss: 1.099 | Reg loss: 0.038 | Tree loss: 1.099 | Accuracy: 0.643500 | 0.857 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 013 | Total loss: 1.096 | Reg loss: 0.038 | Tree loss: 1.096 | Accuracy: 0.654500 | 0.857 sec/iter\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6a62b69bad11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mavg_sp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_sparseness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msparsity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_sp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2acff909ed4f>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, loader, device, log_interval, losses, accs, epoch, iteration)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eitan.k/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eitan.k/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tree = tree.train()\n",
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 5 | score: 0.8127882682224938\n",
      "depth: 6 | score: 0.8932865533961446\n",
      "depth: 7 | score: 0.9375566681121141\n",
      "depth: 8 | score: 0.9539165056963772\n",
      "depth: 9 | score: 0.9731540978436551\n",
      "depth: 10 | score: 0.9798162967635117\n",
      "depth: 11 | score: 0.9896716206094532\n",
      "depth: 12 | score: 0.9942839121693539\n",
      "depth: 13 | score: 0.9963338195293098\n",
      "depth: 14 | score: 0.9973587732092877\n",
      "depth: 15 | score: 0.9979106713446604\n",
      "depth: 16 | score: 0.998147199116963\n",
      "depth: 17 | score: 0.9986596759569519\n",
      "depth: 18 | score: 0.9987779398431033\n",
      "depth: 19 | score: 0.9992115740923246\n",
      "depth: 20 | score: 0.9994086805692435\n",
      "depth: 21 | score: 0.9995663657507786\n",
      "depth: 22 | score: 0.9996846296369298\n",
      "depth: 23 | score: 0.9997634722276973\n",
      "depth: 24 | score: 0.9999211574092325\n",
      "depth: 25 | score: 1.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for d in range(5, 26):\n",
    "    clf = DecisionTreeClassifier(max_depth=d).fit(samples_f, clusters)\n",
    "    s = clf.score(samples_f, clusters)\n",
    "    scores.append(s)\n",
    "    print(f\"depth: {d} | score: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "54\n",
      "0.8932865533961446\n"
     ]
    }
   ],
   "source": [
    "print(clf.get_depth())\n",
    "print(clf.get_n_leaves())\n",
    "print(clf.score(samples_f, clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 149)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "clf.tree_.feature[n], clf.tree_.feature[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_tree(tree, factor=1.5)\n",
    "correct = 0\n",
    "tree = tree.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sparseness: {sparseness(tree.inner_nodes.weight)}\")\n",
    "layer = 0\n",
    "sps = []\n",
    "for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "    cur_layer = np.floor(np.log2(i+1))\n",
    "    if cur_layer != layer:\n",
    "        print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "        sps = []\n",
    "        layer = cur_layer\n",
    "    \n",
    "    x_ = tree.inner_nodes.weight[i, :]\n",
    "    sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "    sps.append(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\\n Kurtosis: {kurtosis(weights_layer)}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the accuracy didn't change too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tree_copy = tree_copy.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree_copy.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree_copy.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    normalizers = torch.cat([dataset.dataset.sensor_maxs[signal_name] for _ in range(sampled.shape[-1])])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "for leaf in leaves:\n",
    "    leaf.reset_path()\n",
    "    leaf.update_path_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Rule {rule_counter} ==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack = LifoQueue()\n",
    "edge_stack = LifoQueue()\n",
    "stack.put(root)\n",
    "rule_counter = 0\n",
    "root.reset()\n",
    "while not stack.empty():\n",
    "    node = stack.get()\n",
    "    if node.is_leaf():\n",
    "        print(f\"============== Rule {rule_counter} ==============\")\n",
    "        for stack_node, cond in zip(stack.queue, edge_stack.queue[1:]):\n",
    "            print(repr(stack_node.get_condition(attr_names)) + cond)\n",
    "            print()\n",
    "        \n",
    "        rule_counter += 1\n",
    "        edge_stack.get()\n",
    "        continue\n",
    "          \n",
    "    if node.left is not None and not node.left.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.left)\n",
    "        node.left.visited = True\n",
    "        edge_stack.put(' < 0')\n",
    "        continue\n",
    "        \n",
    "    if node.right is not None and not node.right.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.right)\n",
    "        node.right.visited = True\n",
    "        edge_stack.put(' > 0')\n",
    "        continue\n",
    "        \n",
    "    if node is not root:\n",
    "        edge_stack.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
