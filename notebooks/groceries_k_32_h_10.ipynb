{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.133718490600586 | KNN Loss: 6.226861000061035 | BCE Loss: 1.9068574905395508\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.137260437011719 | KNN Loss: 6.226616859436035 | BCE Loss: 1.9106433391571045\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.188157081604004 | KNN Loss: 6.226138114929199 | BCE Loss: 1.9620192050933838\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.124198913574219 | KNN Loss: 6.226280212402344 | BCE Loss: 1.8979182243347168\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.152647018432617 | KNN Loss: 6.226075172424316 | BCE Loss: 1.9265716075897217\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.139322280883789 | KNN Loss: 6.2255659103393555 | BCE Loss: 1.913756012916565\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.129941940307617 | KNN Loss: 6.225582599639893 | BCE Loss: 1.9043588638305664\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.077471733093262 | KNN Loss: 6.2255635261535645 | BCE Loss: 1.8519079685211182\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.092585563659668 | KNN Loss: 6.225566387176514 | BCE Loss: 1.8670192956924438\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.129924774169922 | KNN Loss: 6.224828720092773 | BCE Loss: 1.9050958156585693\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.103008270263672 | KNN Loss: 6.224802017211914 | BCE Loss: 1.8782066106796265\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.117267608642578 | KNN Loss: 6.224084854125977 | BCE Loss: 1.8931832313537598\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.137149810791016 | KNN Loss: 6.223583221435547 | BCE Loss: 1.9135667085647583\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.072366714477539 | KNN Loss: 6.2237982749938965 | BCE Loss: 1.8485679626464844\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.058802604675293 | KNN Loss: 6.222825527191162 | BCE Loss: 1.8359767198562622\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.078664779663086 | KNN Loss: 6.222851276397705 | BCE Loss: 1.8558135032653809\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.014839172363281 | KNN Loss: 6.223230361938477 | BCE Loss: 1.791608452796936\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.059126853942871 | KNN Loss: 6.222644329071045 | BCE Loss: 1.836482286453247\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.073165893554688 | KNN Loss: 6.222244739532471 | BCE Loss: 1.8509211540222168\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.012185096740723 | KNN Loss: 6.221795082092285 | BCE Loss: 1.7903900146484375\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.02244758605957 | KNN Loss: 6.220424652099609 | BCE Loss: 1.80202317237854\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.005974769592285 | KNN Loss: 6.220493316650391 | BCE Loss: 1.7854812145233154\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.9771881103515625 | KNN Loss: 6.21923303604126 | BCE Loss: 1.7579549551010132\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.005210876464844 | KNN Loss: 6.2194294929504395 | BCE Loss: 1.7857813835144043\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.037556648254395 | KNN Loss: 6.218379497528076 | BCE Loss: 1.819177269935608\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.970513343811035 | KNN Loss: 6.218464374542236 | BCE Loss: 1.7520489692687988\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 8.00041389465332 | KNN Loss: 6.216684341430664 | BCE Loss: 1.7837300300598145\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.980104446411133 | KNN Loss: 6.217034339904785 | BCE Loss: 1.763069987297058\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.932190418243408 | KNN Loss: 6.216141700744629 | BCE Loss: 1.7160487174987793\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.910072326660156 | KNN Loss: 6.21376371383667 | BCE Loss: 1.6963086128234863\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.931334972381592 | KNN Loss: 6.213469505310059 | BCE Loss: 1.7178654670715332\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.9190754890441895 | KNN Loss: 6.211972236633301 | BCE Loss: 1.7071031332015991\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.901410102844238 | KNN Loss: 6.209949016571045 | BCE Loss: 1.6914608478546143\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.875959396362305 | KNN Loss: 6.208963871002197 | BCE Loss: 1.6669955253601074\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.864973545074463 | KNN Loss: 6.205075263977051 | BCE Loss: 1.659898281097412\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.85327672958374 | KNN Loss: 6.203278064727783 | BCE Loss: 1.649998664855957\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.854536056518555 | KNN Loss: 6.20039701461792 | BCE Loss: 1.6541389226913452\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.771772384643555 | KNN Loss: 6.1948089599609375 | BCE Loss: 1.5769635438919067\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.804625988006592 | KNN Loss: 6.196599960327148 | BCE Loss: 1.6080260276794434\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.774175643920898 | KNN Loss: 6.1932501792907715 | BCE Loss: 1.5809255838394165\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.735767364501953 | KNN Loss: 6.1880645751953125 | BCE Loss: 1.5477027893066406\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.7285871505737305 | KNN Loss: 6.180646896362305 | BCE Loss: 1.5479402542114258\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.729580879211426 | KNN Loss: 6.173324108123779 | BCE Loss: 1.5562567710876465\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.705489635467529 | KNN Loss: 6.167610168457031 | BCE Loss: 1.537879467010498\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.693655967712402 | KNN Loss: 6.156147480010986 | BCE Loss: 1.537508249282837\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.637024402618408 | KNN Loss: 6.153566837310791 | BCE Loss: 1.4834576845169067\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.64780855178833 | KNN Loss: 6.151968479156494 | BCE Loss: 1.4958401918411255\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.56368350982666 | KNN Loss: 6.11815071105957 | BCE Loss: 1.4455327987670898\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.547089099884033 | KNN Loss: 6.108872890472412 | BCE Loss: 1.4382163286209106\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.540518283843994 | KNN Loss: 6.104055404663086 | BCE Loss: 1.4364628791809082\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.467073440551758 | KNN Loss: 6.079890251159668 | BCE Loss: 1.387183427810669\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.4288554191589355 | KNN Loss: 6.061941623687744 | BCE Loss: 1.3669137954711914\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.385983467102051 | KNN Loss: 6.027460098266602 | BCE Loss: 1.3585233688354492\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.31722354888916 | KNN Loss: 5.994771957397461 | BCE Loss: 1.3224517107009888\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.283099174499512 | KNN Loss: 5.9728546142578125 | BCE Loss: 1.3102443218231201\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.230544567108154 | KNN Loss: 5.944033145904541 | BCE Loss: 1.2865115404129028\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.205049514770508 | KNN Loss: 5.915677070617676 | BCE Loss: 1.289372205734253\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.090904712677002 | KNN Loss: 5.838033199310303 | BCE Loss: 1.2528715133666992\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.045956611633301 | KNN Loss: 5.793121337890625 | BCE Loss: 1.2528350353240967\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.9300432205200195 | KNN Loss: 5.725693702697754 | BCE Loss: 1.2043492794036865\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.865896224975586 | KNN Loss: 5.677393436431885 | BCE Loss: 1.188502550125122\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.7504563331604 | KNN Loss: 5.585760116577148 | BCE Loss: 1.164696216583252\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.675670623779297 | KNN Loss: 5.496864318847656 | BCE Loss: 1.178806185722351\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.585823059082031 | KNN Loss: 5.423928260803223 | BCE Loss: 1.1618950366973877\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.525257110595703 | KNN Loss: 5.344659805297852 | BCE Loss: 1.1805970668792725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 6.2970476150512695 | KNN Loss: 5.1975531578063965 | BCE Loss: 1.0994946956634521\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 6.278271198272705 | KNN Loss: 5.143256664276123 | BCE Loss: 1.135014533996582\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 6.123623847961426 | KNN Loss: 5.0088276863098145 | BCE Loss: 1.1147961616516113\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.025326251983643 | KNN Loss: 4.92909574508667 | BCE Loss: 1.0962305068969727\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.927366256713867 | KNN Loss: 4.832589149475098 | BCE Loss: 1.0947771072387695\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.831093788146973 | KNN Loss: 4.722562313079834 | BCE Loss: 1.1085315942764282\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.719992637634277 | KNN Loss: 4.626026630401611 | BCE Loss: 1.0939661264419556\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.601950168609619 | KNN Loss: 4.527592182159424 | BCE Loss: 1.0743578672409058\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.570174694061279 | KNN Loss: 4.468252182006836 | BCE Loss: 1.1019223928451538\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.524017333984375 | KNN Loss: 4.413023948669434 | BCE Loss: 1.1109936237335205\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.446160316467285 | KNN Loss: 4.337626934051514 | BCE Loss: 1.1085333824157715\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.356048583984375 | KNN Loss: 4.261194705963135 | BCE Loss: 1.0948537588119507\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.317276954650879 | KNN Loss: 4.227077007293701 | BCE Loss: 1.0902001857757568\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.31874942779541 | KNN Loss: 4.19055700302124 | BCE Loss: 1.1281921863555908\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.224751949310303 | KNN Loss: 4.128123760223389 | BCE Loss: 1.096628189086914\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.213113307952881 | KNN Loss: 4.125190734863281 | BCE Loss: 1.0879226922988892\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.124781608581543 | KNN Loss: 4.074338912963867 | BCE Loss: 1.0504429340362549\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.149942874908447 | KNN Loss: 4.05812406539917 | BCE Loss: 1.0918188095092773\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.094085693359375 | KNN Loss: 4.023499011993408 | BCE Loss: 1.070586919784546\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.093493938446045 | KNN Loss: 4.000213146209717 | BCE Loss: 1.0932809114456177\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.054958343505859 | KNN Loss: 3.9851231575012207 | BCE Loss: 1.0698350667953491\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.028932571411133 | KNN Loss: 3.959679365158081 | BCE Loss: 1.0692529678344727\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.048856735229492 | KNN Loss: 3.9738669395446777 | BCE Loss: 1.0749895572662354\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.979846000671387 | KNN Loss: 3.950676441192627 | BCE Loss: 1.0291697978973389\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.046960830688477 | KNN Loss: 3.9393467903137207 | BCE Loss: 1.1076138019561768\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.004806995391846 | KNN Loss: 3.9060072898864746 | BCE Loss: 1.098799705505371\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.977624893188477 | KNN Loss: 3.9010932445526123 | BCE Loss: 1.0765318870544434\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 5.031789779663086 | KNN Loss: 3.937546491622925 | BCE Loss: 1.0942435264587402\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.956814765930176 | KNN Loss: 3.9001100063323975 | BCE Loss: 1.0567047595977783\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.037088394165039 | KNN Loss: 3.936940908432007 | BCE Loss: 1.1001473665237427\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.956228256225586 | KNN Loss: 3.888150930404663 | BCE Loss: 1.0680773258209229\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.976168632507324 | KNN Loss: 3.901738405227661 | BCE Loss: 1.074429988861084\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.934091091156006 | KNN Loss: 3.8641018867492676 | BCE Loss: 1.0699890851974487\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.948387622833252 | KNN Loss: 3.8921926021575928 | BCE Loss: 1.0561951398849487\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.944800853729248 | KNN Loss: 3.8982644081115723 | BCE Loss: 1.0465364456176758\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.904728412628174 | KNN Loss: 3.8405866622924805 | BCE Loss: 1.0641416311264038\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.915231704711914 | KNN Loss: 3.8418643474578857 | BCE Loss: 1.0733671188354492\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.910012245178223 | KNN Loss: 3.866189479827881 | BCE Loss: 1.043823003768921\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.91497802734375 | KNN Loss: 3.8487656116485596 | BCE Loss: 1.0662121772766113\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.961361885070801 | KNN Loss: 3.875595808029175 | BCE Loss: 1.085766315460205\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.94981575012207 | KNN Loss: 3.8488523960113525 | BCE Loss: 1.1009631156921387\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.896374702453613 | KNN Loss: 3.838672637939453 | BCE Loss: 1.0577021837234497\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.9612717628479 | KNN Loss: 3.8775532245635986 | BCE Loss: 1.0837186574935913\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.904706001281738 | KNN Loss: 3.8340864181518555 | BCE Loss: 1.0706197023391724\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.926974296569824 | KNN Loss: 3.862576961517334 | BCE Loss: 1.0643972158432007\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.8850202560424805 | KNN Loss: 3.8390448093414307 | BCE Loss: 1.0459752082824707\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.91021728515625 | KNN Loss: 3.861398935317993 | BCE Loss: 1.0488181114196777\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.911138534545898 | KNN Loss: 3.853501319885254 | BCE Loss: 1.0576369762420654\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.892584800720215 | KNN Loss: 3.85109281539917 | BCE Loss: 1.041492223739624\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.875498294830322 | KNN Loss: 3.843799352645874 | BCE Loss: 1.0316989421844482\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.895102500915527 | KNN Loss: 3.824305295944214 | BCE Loss: 1.0707972049713135\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.907104969024658 | KNN Loss: 3.832549571990967 | BCE Loss: 1.0745553970336914\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.931394577026367 | KNN Loss: 3.8440937995910645 | BCE Loss: 1.0873010158538818\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.89281702041626 | KNN Loss: 3.8153040409088135 | BCE Loss: 1.0775128602981567\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.893592834472656 | KNN Loss: 3.830653429031372 | BCE Loss: 1.0629394054412842\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.931446075439453 | KNN Loss: 3.871955633163452 | BCE Loss: 1.05949068069458\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.950326919555664 | KNN Loss: 3.913165330886841 | BCE Loss: 1.0371617078781128\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.889518737792969 | KNN Loss: 3.8242790699005127 | BCE Loss: 1.065239667892456\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.965680122375488 | KNN Loss: 3.8920626640319824 | BCE Loss: 1.0736172199249268\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.896854400634766 | KNN Loss: 3.823063850402832 | BCE Loss: 1.0737905502319336\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.877542495727539 | KNN Loss: 3.8290724754333496 | BCE Loss: 1.0484700202941895\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.8973798751831055 | KNN Loss: 3.8510351181030273 | BCE Loss: 1.0463449954986572\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.888158798217773 | KNN Loss: 3.839181423187256 | BCE Loss: 1.0489771366119385\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.915949821472168 | KNN Loss: 3.8618063926696777 | BCE Loss: 1.0541434288024902\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.870028495788574 | KNN Loss: 3.834526300430298 | BCE Loss: 1.0355024337768555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.895739555358887 | KNN Loss: 3.8490068912506104 | BCE Loss: 1.046732783317566\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.902154922485352 | KNN Loss: 3.8195180892944336 | BCE Loss: 1.082637071609497\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.926777362823486 | KNN Loss: 3.8551180362701416 | BCE Loss: 1.0716592073440552\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.889044284820557 | KNN Loss: 3.8556411266326904 | BCE Loss: 1.0334031581878662\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.888866901397705 | KNN Loss: 3.8489649295806885 | BCE Loss: 1.0399020910263062\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.8839311599731445 | KNN Loss: 3.8232359886169434 | BCE Loss: 1.0606951713562012\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.8758344650268555 | KNN Loss: 3.8018832206726074 | BCE Loss: 1.0739511251449585\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.916973114013672 | KNN Loss: 3.8316996097564697 | BCE Loss: 1.0852733850479126\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.915508270263672 | KNN Loss: 3.8622450828552246 | BCE Loss: 1.0532633066177368\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.853957176208496 | KNN Loss: 3.8023521900177 | BCE Loss: 1.051604986190796\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.8782958984375 | KNN Loss: 3.835003614425659 | BCE Loss: 1.0432922840118408\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.871964454650879 | KNN Loss: 3.819030523300171 | BCE Loss: 1.052933931350708\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.859551429748535 | KNN Loss: 3.805922269821167 | BCE Loss: 1.0536293983459473\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.83369255065918 | KNN Loss: 3.795562505722046 | BCE Loss: 1.0381300449371338\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.880520820617676 | KNN Loss: 3.8199853897094727 | BCE Loss: 1.0605353116989136\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.867386817932129 | KNN Loss: 3.8233730792999268 | BCE Loss: 1.0440139770507812\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.877436637878418 | KNN Loss: 3.8232483863830566 | BCE Loss: 1.0541880130767822\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.866643905639648 | KNN Loss: 3.821903944015503 | BCE Loss: 1.0447402000427246\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.868406295776367 | KNN Loss: 3.8157219886779785 | BCE Loss: 1.0526840686798096\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.841878890991211 | KNN Loss: 3.803271532058716 | BCE Loss: 1.0386075973510742\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.890986442565918 | KNN Loss: 3.8222057819366455 | BCE Loss: 1.0687804222106934\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.898019790649414 | KNN Loss: 3.849612236022949 | BCE Loss: 1.0484074354171753\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.894721508026123 | KNN Loss: 3.812755823135376 | BCE Loss: 1.0819655656814575\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.865149021148682 | KNN Loss: 3.8314497470855713 | BCE Loss: 1.0336993932724\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.851417541503906 | KNN Loss: 3.8036155700683594 | BCE Loss: 1.0478017330169678\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.853663444519043 | KNN Loss: 3.7984349727630615 | BCE Loss: 1.0552287101745605\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.853200912475586 | KNN Loss: 3.802428960800171 | BCE Loss: 1.050771951675415\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.900486946105957 | KNN Loss: 3.8349595069885254 | BCE Loss: 1.0655276775360107\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.857769966125488 | KNN Loss: 3.7953433990478516 | BCE Loss: 1.0624265670776367\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.823574066162109 | KNN Loss: 3.789215326309204 | BCE Loss: 1.0343585014343262\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.871298313140869 | KNN Loss: 3.811803102493286 | BCE Loss: 1.059495210647583\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.858609199523926 | KNN Loss: 3.797623872756958 | BCE Loss: 1.0609853267669678\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.839956283569336 | KNN Loss: 3.790295362472534 | BCE Loss: 1.0496611595153809\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.8477325439453125 | KNN Loss: 3.8189046382904053 | BCE Loss: 1.0288276672363281\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.868290424346924 | KNN Loss: 3.807832717895508 | BCE Loss: 1.060457706451416\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.829219818115234 | KNN Loss: 3.7845842838287354 | BCE Loss: 1.04463529586792\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.884790897369385 | KNN Loss: 3.8096821308135986 | BCE Loss: 1.0751086473464966\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.812519073486328 | KNN Loss: 3.7728164196014404 | BCE Loss: 1.0397025346755981\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.827787399291992 | KNN Loss: 3.785397529602051 | BCE Loss: 1.042389988899231\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.864596366882324 | KNN Loss: 3.80256986618042 | BCE Loss: 1.0620263814926147\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.85327672958374 | KNN Loss: 3.8056468963623047 | BCE Loss: 1.0476298332214355\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.816858768463135 | KNN Loss: 3.7752606868743896 | BCE Loss: 1.0415980815887451\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.8334832191467285 | KNN Loss: 3.7849924564361572 | BCE Loss: 1.0484906435012817\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.855991363525391 | KNN Loss: 3.785351037979126 | BCE Loss: 1.0706405639648438\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.906964302062988 | KNN Loss: 3.8414745330810547 | BCE Loss: 1.0654895305633545\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.899322986602783 | KNN Loss: 3.826169013977051 | BCE Loss: 1.0731539726257324\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.8376545906066895 | KNN Loss: 3.7984087467193604 | BCE Loss: 1.0392457246780396\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.8922600746154785 | KNN Loss: 3.8094780445098877 | BCE Loss: 1.0827820301055908\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.845581531524658 | KNN Loss: 3.7953176498413086 | BCE Loss: 1.05026376247406\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.824365615844727 | KNN Loss: 3.7698910236358643 | BCE Loss: 1.0544748306274414\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.839541912078857 | KNN Loss: 3.7957074642181396 | BCE Loss: 1.0438344478607178\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.844684600830078 | KNN Loss: 3.791767120361328 | BCE Loss: 1.052917242050171\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.82156229019165 | KNN Loss: 3.809142589569092 | BCE Loss: 1.0124197006225586\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.865028381347656 | KNN Loss: 3.8078858852386475 | BCE Loss: 1.0571422576904297\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.847615718841553 | KNN Loss: 3.795619487762451 | BCE Loss: 1.0519962310791016\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.836416721343994 | KNN Loss: 3.783841133117676 | BCE Loss: 1.0525754690170288\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.823065757751465 | KNN Loss: 3.7935047149658203 | BCE Loss: 1.0295610427856445\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.826745510101318 | KNN Loss: 3.776014804840088 | BCE Loss: 1.0507307052612305\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.80570650100708 | KNN Loss: 3.7834885120391846 | BCE Loss: 1.022217869758606\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.842715740203857 | KNN Loss: 3.7900707721710205 | BCE Loss: 1.0526448488235474\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.857227325439453 | KNN Loss: 3.810391664505005 | BCE Loss: 1.0468354225158691\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.852120399475098 | KNN Loss: 3.7855286598205566 | BCE Loss: 1.0665919780731201\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.846216201782227 | KNN Loss: 3.790454149246216 | BCE Loss: 1.0557622909545898\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.822646617889404 | KNN Loss: 3.77789306640625 | BCE Loss: 1.0447535514831543\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.792433738708496 | KNN Loss: 3.775866985321045 | BCE Loss: 1.0165667533874512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.830670356750488 | KNN Loss: 3.7763350009918213 | BCE Loss: 1.0543352365493774\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.85447883605957 | KNN Loss: 3.7854175567626953 | BCE Loss: 1.069061279296875\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.841700077056885 | KNN Loss: 3.8065178394317627 | BCE Loss: 1.035182237625122\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.8088531494140625 | KNN Loss: 3.7778830528259277 | BCE Loss: 1.0309698581695557\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.791323184967041 | KNN Loss: 3.7804763317108154 | BCE Loss: 1.0108469724655151\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.834181785583496 | KNN Loss: 3.7814431190490723 | BCE Loss: 1.0527385473251343\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.873087406158447 | KNN Loss: 3.8162765502929688 | BCE Loss: 1.056810736656189\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.836986541748047 | KNN Loss: 3.7850193977355957 | BCE Loss: 1.0519670248031616\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.814211845397949 | KNN Loss: 3.76576828956604 | BCE Loss: 1.0484435558319092\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.873615741729736 | KNN Loss: 3.8323147296905518 | BCE Loss: 1.0413010120391846\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.83000373840332 | KNN Loss: 3.7946228981018066 | BCE Loss: 1.0353808403015137\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.811777114868164 | KNN Loss: 3.793130874633789 | BCE Loss: 1.018646240234375\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.80983304977417 | KNN Loss: 3.7875568866729736 | BCE Loss: 1.0222761631011963\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.820352077484131 | KNN Loss: 3.786529541015625 | BCE Loss: 1.0338226556777954\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.847846031188965 | KNN Loss: 3.784031629562378 | BCE Loss: 1.0638141632080078\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.843002796173096 | KNN Loss: 3.800917863845825 | BCE Loss: 1.04208505153656\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.839656352996826 | KNN Loss: 3.8113136291503906 | BCE Loss: 1.0283427238464355\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.810041904449463 | KNN Loss: 3.754998207092285 | BCE Loss: 1.0550436973571777\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.843344211578369 | KNN Loss: 3.774653196334839 | BCE Loss: 1.0686910152435303\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.866580009460449 | KNN Loss: 3.8226633071899414 | BCE Loss: 1.0439164638519287\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.826586723327637 | KNN Loss: 3.7715039253234863 | BCE Loss: 1.0550830364227295\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.806140899658203 | KNN Loss: 3.782569408416748 | BCE Loss: 1.0235716104507446\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.841012477874756 | KNN Loss: 3.787785053253174 | BCE Loss: 1.053227424621582\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.844683647155762 | KNN Loss: 3.760833501815796 | BCE Loss: 1.0838499069213867\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.777509689331055 | KNN Loss: 3.7786996364593506 | BCE Loss: 0.9988102912902832\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.796781539916992 | KNN Loss: 3.7653427124023438 | BCE Loss: 1.0314388275146484\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.8076653480529785 | KNN Loss: 3.7730398178100586 | BCE Loss: 1.03462553024292\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.841056823730469 | KNN Loss: 3.774937391281128 | BCE Loss: 1.0661191940307617\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.841202735900879 | KNN Loss: 3.7859549522399902 | BCE Loss: 1.0552476644515991\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.835633754730225 | KNN Loss: 3.789491653442383 | BCE Loss: 1.0461421012878418\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.8481903076171875 | KNN Loss: 3.7878785133361816 | BCE Loss: 1.060312032699585\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.784951210021973 | KNN Loss: 3.7598209381103516 | BCE Loss: 1.0251305103302002\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.813719272613525 | KNN Loss: 3.749753475189209 | BCE Loss: 1.0639657974243164\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.824878215789795 | KNN Loss: 3.7609307765960693 | BCE Loss: 1.0639474391937256\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.840030670166016 | KNN Loss: 3.775182008743286 | BCE Loss: 1.06484854221344\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.825969696044922 | KNN Loss: 3.797649621963501 | BCE Loss: 1.0283203125\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.830492973327637 | KNN Loss: 3.795375108718872 | BCE Loss: 1.0351179838180542\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.80172061920166 | KNN Loss: 3.7659378051757812 | BCE Loss: 1.035783052444458\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.7983598709106445 | KNN Loss: 3.7515125274658203 | BCE Loss: 1.0468473434448242\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.821906089782715 | KNN Loss: 3.789320707321167 | BCE Loss: 1.0325852632522583\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.854011058807373 | KNN Loss: 3.7979416847229004 | BCE Loss: 1.0560693740844727\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.838468074798584 | KNN Loss: 3.7860569953918457 | BCE Loss: 1.0524111986160278\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.8154473304748535 | KNN Loss: 3.777151346206665 | BCE Loss: 1.038295865058899\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.8273162841796875 | KNN Loss: 3.772528648376465 | BCE Loss: 1.0547873973846436\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.811215400695801 | KNN Loss: 3.7720253467559814 | BCE Loss: 1.0391901731491089\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.841367244720459 | KNN Loss: 3.798750877380371 | BCE Loss: 1.042616367340088\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.828027248382568 | KNN Loss: 3.7808899879455566 | BCE Loss: 1.0471371412277222\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.825839519500732 | KNN Loss: 3.7839393615722656 | BCE Loss: 1.0419001579284668\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.827375411987305 | KNN Loss: 3.788161277770996 | BCE Loss: 1.0392141342163086\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.789017677307129 | KNN Loss: 3.7706334590911865 | BCE Loss: 1.0183844566345215\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.804978847503662 | KNN Loss: 3.7647385597229004 | BCE Loss: 1.0402402877807617\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.791200637817383 | KNN Loss: 3.7562918663024902 | BCE Loss: 1.0349085330963135\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.776404857635498 | KNN Loss: 3.752683401107788 | BCE Loss: 1.0237213373184204\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.824159145355225 | KNN Loss: 3.785203456878662 | BCE Loss: 1.038955807685852\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.775432586669922 | KNN Loss: 3.764554023742676 | BCE Loss: 1.010878562927246\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.773591995239258 | KNN Loss: 3.744767189025879 | BCE Loss: 1.028824806213379\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.8057475090026855 | KNN Loss: 3.7702977657318115 | BCE Loss: 1.035449743270874\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.783661365509033 | KNN Loss: 3.7627339363098145 | BCE Loss: 1.0209273099899292\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.7923970222473145 | KNN Loss: 3.7618134021759033 | BCE Loss: 1.0305836200714111\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.828240394592285 | KNN Loss: 3.76033878326416 | BCE Loss: 1.0679017305374146\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.778738498687744 | KNN Loss: 3.7501649856567383 | BCE Loss: 1.0285735130310059\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.7759857177734375 | KNN Loss: 3.752258062362671 | BCE Loss: 1.0237274169921875\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.812477111816406 | KNN Loss: 3.7816314697265625 | BCE Loss: 1.0308455228805542\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.817651748657227 | KNN Loss: 3.7906458377838135 | BCE Loss: 1.027005910873413\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.834056854248047 | KNN Loss: 3.7884087562561035 | BCE Loss: 1.0456480979919434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.8183817863464355 | KNN Loss: 3.7670352458953857 | BCE Loss: 1.0513464212417603\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.799330711364746 | KNN Loss: 3.7650609016418457 | BCE Loss: 1.0342698097229004\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.820438861846924 | KNN Loss: 3.7949063777923584 | BCE Loss: 1.0255324840545654\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.801769733428955 | KNN Loss: 3.762892723083496 | BCE Loss: 1.038877010345459\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.841640472412109 | KNN Loss: 3.7723915576934814 | BCE Loss: 1.0692486763000488\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.806844711303711 | KNN Loss: 3.7636656761169434 | BCE Loss: 1.0431790351867676\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.792999267578125 | KNN Loss: 3.770460605621338 | BCE Loss: 1.0225389003753662\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.766674041748047 | KNN Loss: 3.7495651245117188 | BCE Loss: 1.0171089172363281\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.804955959320068 | KNN Loss: 3.7903060913085938 | BCE Loss: 1.0146499872207642\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.801969528198242 | KNN Loss: 3.751951217651367 | BCE Loss: 1.050018548965454\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.802322864532471 | KNN Loss: 3.753603935241699 | BCE Loss: 1.048719048500061\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.79030704498291 | KNN Loss: 3.766413927078247 | BCE Loss: 1.0238932371139526\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.756970405578613 | KNN Loss: 3.7454419136047363 | BCE Loss: 1.011528491973877\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.835320472717285 | KNN Loss: 3.775193452835083 | BCE Loss: 1.0601270198822021\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.789442539215088 | KNN Loss: 3.767582654953003 | BCE Loss: 1.021859884262085\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.771825790405273 | KNN Loss: 3.7527658939361572 | BCE Loss: 1.019059658050537\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.813755512237549 | KNN Loss: 3.7574408054351807 | BCE Loss: 1.0563148260116577\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.826617240905762 | KNN Loss: 3.7685232162475586 | BCE Loss: 1.0580940246582031\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.762082099914551 | KNN Loss: 3.7440388202667236 | BCE Loss: 1.0180435180664062\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.781109809875488 | KNN Loss: 3.743506908416748 | BCE Loss: 1.0376029014587402\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.75063419342041 | KNN Loss: 3.736482858657837 | BCE Loss: 1.0141510963439941\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.757114887237549 | KNN Loss: 3.724370241165161 | BCE Loss: 1.0327445268630981\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.752167701721191 | KNN Loss: 3.7264280319213867 | BCE Loss: 1.0257396697998047\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.783556938171387 | KNN Loss: 3.7687971591949463 | BCE Loss: 1.0147597789764404\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.788242816925049 | KNN Loss: 3.772780179977417 | BCE Loss: 1.0154626369476318\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.777732849121094 | KNN Loss: 3.7595372200012207 | BCE Loss: 1.0181958675384521\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.785959243774414 | KNN Loss: 3.7704548835754395 | BCE Loss: 1.015504240989685\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.809422969818115 | KNN Loss: 3.7702090740203857 | BCE Loss: 1.0392138957977295\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.825209140777588 | KNN Loss: 3.799656629562378 | BCE Loss: 1.0255526304244995\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.825949668884277 | KNN Loss: 3.77736496925354 | BCE Loss: 1.0485849380493164\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.771645545959473 | KNN Loss: 3.7469685077667236 | BCE Loss: 1.02467679977417\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.7814412117004395 | KNN Loss: 3.7428462505340576 | BCE Loss: 1.0385949611663818\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.8080902099609375 | KNN Loss: 3.7433719635009766 | BCE Loss: 1.0647183656692505\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.779809951782227 | KNN Loss: 3.7317590713500977 | BCE Loss: 1.0480507612228394\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.8149614334106445 | KNN Loss: 3.741199254989624 | BCE Loss: 1.0737619400024414\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.807210922241211 | KNN Loss: 3.7536346912384033 | BCE Loss: 1.0535759925842285\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.789364814758301 | KNN Loss: 3.7572250366210938 | BCE Loss: 1.032139778137207\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.828577995300293 | KNN Loss: 3.80290150642395 | BCE Loss: 1.0256762504577637\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.758689880371094 | KNN Loss: 3.7407889366149902 | BCE Loss: 1.0179009437561035\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.823055267333984 | KNN Loss: 3.790114641189575 | BCE Loss: 1.03294038772583\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.818857192993164 | KNN Loss: 3.779170036315918 | BCE Loss: 1.0396873950958252\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.779654502868652 | KNN Loss: 3.7588346004486084 | BCE Loss: 1.0208200216293335\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.798372745513916 | KNN Loss: 3.742617607116699 | BCE Loss: 1.0557551383972168\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.7650275230407715 | KNN Loss: 3.7360355854034424 | BCE Loss: 1.028991937637329\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.77857780456543 | KNN Loss: 3.741677761077881 | BCE Loss: 1.0369000434875488\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.735651016235352 | KNN Loss: 3.7379941940307617 | BCE Loss: 0.9976565837860107\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.779180526733398 | KNN Loss: 3.754847764968872 | BCE Loss: 1.0243325233459473\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.79409646987915 | KNN Loss: 3.7354378700256348 | BCE Loss: 1.0586585998535156\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.767616271972656 | KNN Loss: 3.7367985248565674 | BCE Loss: 1.0308176279067993\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.776076316833496 | KNN Loss: 3.7255587577819824 | BCE Loss: 1.0505176782608032\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.815225124359131 | KNN Loss: 3.7717607021331787 | BCE Loss: 1.0434644222259521\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.771867752075195 | KNN Loss: 3.771426200866699 | BCE Loss: 1.000441551208496\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.819093704223633 | KNN Loss: 3.7530717849731445 | BCE Loss: 1.0660216808319092\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.766582489013672 | KNN Loss: 3.756113052368164 | BCE Loss: 1.0104693174362183\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.80455207824707 | KNN Loss: 3.761249542236328 | BCE Loss: 1.043302297592163\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.784246921539307 | KNN Loss: 3.749101161956787 | BCE Loss: 1.03514564037323\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.801122665405273 | KNN Loss: 3.7417564392089844 | BCE Loss: 1.059366226196289\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.777756690979004 | KNN Loss: 3.7500524520874023 | BCE Loss: 1.0277040004730225\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.782598495483398 | KNN Loss: 3.745696783065796 | BCE Loss: 1.0369019508361816\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.766664505004883 | KNN Loss: 3.7529995441436768 | BCE Loss: 1.013664722442627\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.766634941101074 | KNN Loss: 3.75189471244812 | BCE Loss: 1.0147404670715332\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.732025146484375 | KNN Loss: 3.7197682857513428 | BCE Loss: 1.0122569799423218\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.7785725593566895 | KNN Loss: 3.7752251625061035 | BCE Loss: 1.0033475160598755\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.7782087326049805 | KNN Loss: 3.744284152984619 | BCE Loss: 1.0339246988296509\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.746156215667725 | KNN Loss: 3.753638982772827 | BCE Loss: 0.9925171136856079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.746033668518066 | KNN Loss: 3.7426271438598633 | BCE Loss: 1.0034065246582031\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.759019374847412 | KNN Loss: 3.7413265705108643 | BCE Loss: 1.0176926851272583\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.778705596923828 | KNN Loss: 3.761247396469116 | BCE Loss: 1.017458438873291\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.833542823791504 | KNN Loss: 3.795400857925415 | BCE Loss: 1.0381418466567993\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.789176940917969 | KNN Loss: 3.7545673847198486 | BCE Loss: 1.0346095561981201\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.811786651611328 | KNN Loss: 3.773365020751953 | BCE Loss: 1.038421630859375\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.768449306488037 | KNN Loss: 3.762960195541382 | BCE Loss: 1.0054891109466553\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.768016815185547 | KNN Loss: 3.727332830429077 | BCE Loss: 1.0406839847564697\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.79170560836792 | KNN Loss: 3.7532050609588623 | BCE Loss: 1.038500428199768\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.773981094360352 | KNN Loss: 3.7547495365142822 | BCE Loss: 1.0192315578460693\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.753345966339111 | KNN Loss: 3.7398786544799805 | BCE Loss: 1.0134671926498413\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.778946876525879 | KNN Loss: 3.7234537601470947 | BCE Loss: 1.0554931163787842\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.733229637145996 | KNN Loss: 3.735802412033081 | BCE Loss: 0.997427225112915\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.736665725708008 | KNN Loss: 3.7314441204071045 | BCE Loss: 1.0052217245101929\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.773255348205566 | KNN Loss: 3.7312066555023193 | BCE Loss: 1.042048454284668\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.7625274658203125 | KNN Loss: 3.736037492752075 | BCE Loss: 1.0264899730682373\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.754840850830078 | KNN Loss: 3.7287046909332275 | BCE Loss: 1.0261363983154297\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.777108192443848 | KNN Loss: 3.751270055770874 | BCE Loss: 1.0258383750915527\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.756253242492676 | KNN Loss: 3.743744373321533 | BCE Loss: 1.0125091075897217\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.77570104598999 | KNN Loss: 3.748096227645874 | BCE Loss: 1.0276046991348267\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.81285285949707 | KNN Loss: 3.759037971496582 | BCE Loss: 1.0538151264190674\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.7583723068237305 | KNN Loss: 3.742964029312134 | BCE Loss: 1.0154082775115967\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.814374923706055 | KNN Loss: 3.762666702270508 | BCE Loss: 1.0517083406448364\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.796337604522705 | KNN Loss: 3.767164468765259 | BCE Loss: 1.0291730165481567\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.787938117980957 | KNN Loss: 3.760741949081421 | BCE Loss: 1.0271960496902466\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.76249885559082 | KNN Loss: 3.749666452407837 | BCE Loss: 1.0128321647644043\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.762735366821289 | KNN Loss: 3.721386432647705 | BCE Loss: 1.0413486957550049\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.762908458709717 | KNN Loss: 3.760786533355713 | BCE Loss: 1.002121925354004\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.808795928955078 | KNN Loss: 3.745443105697632 | BCE Loss: 1.0633528232574463\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.784813404083252 | KNN Loss: 3.781827926635742 | BCE Loss: 1.0029854774475098\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.788149833679199 | KNN Loss: 3.7651312351226807 | BCE Loss: 1.0230185985565186\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.779361724853516 | KNN Loss: 3.7484917640686035 | BCE Loss: 1.0308701992034912\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.781541347503662 | KNN Loss: 3.751934289932251 | BCE Loss: 1.0296070575714111\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.786242485046387 | KNN Loss: 3.7458837032318115 | BCE Loss: 1.040358543395996\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.770400524139404 | KNN Loss: 3.741990089416504 | BCE Loss: 1.0284103155136108\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.816300868988037 | KNN Loss: 3.762394666671753 | BCE Loss: 1.0539062023162842\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.772866725921631 | KNN Loss: 3.7425894737243652 | BCE Loss: 1.0302773714065552\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.795863151550293 | KNN Loss: 3.7416422367095947 | BCE Loss: 1.0542207956314087\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.732792854309082 | KNN Loss: 3.737208127975464 | BCE Loss: 0.9955844879150391\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.722363471984863 | KNN Loss: 3.721055507659912 | BCE Loss: 1.0013080835342407\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.734808921813965 | KNN Loss: 3.6949892044067383 | BCE Loss: 1.0398199558258057\n",
      "Epoch    61: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.773179531097412 | KNN Loss: 3.749782085418701 | BCE Loss: 1.0233973264694214\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.818324565887451 | KNN Loss: 3.808384418487549 | BCE Loss: 1.009940266609192\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.766240119934082 | KNN Loss: 3.7357635498046875 | BCE Loss: 1.0304763317108154\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.798979759216309 | KNN Loss: 3.7544751167297363 | BCE Loss: 1.0445046424865723\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.776544570922852 | KNN Loss: 3.731612205505371 | BCE Loss: 1.0449321269989014\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.764861106872559 | KNN Loss: 3.718079090118408 | BCE Loss: 1.0467822551727295\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.772513389587402 | KNN Loss: 3.7333545684814453 | BCE Loss: 1.039158821105957\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.766804218292236 | KNN Loss: 3.728896379470825 | BCE Loss: 1.0379078388214111\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.781806945800781 | KNN Loss: 3.736067295074463 | BCE Loss: 1.0457395315170288\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.7478413581848145 | KNN Loss: 3.7298147678375244 | BCE Loss: 1.01802659034729\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.7634477615356445 | KNN Loss: 3.7372496128082275 | BCE Loss: 1.026198148727417\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.747626304626465 | KNN Loss: 3.7376723289489746 | BCE Loss: 1.0099539756774902\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.802831172943115 | KNN Loss: 3.754760980606079 | BCE Loss: 1.0480701923370361\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.7827019691467285 | KNN Loss: 3.728214740753174 | BCE Loss: 1.0544871091842651\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.77204704284668 | KNN Loss: 3.729656219482422 | BCE Loss: 1.042391061782837\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.7680134773254395 | KNN Loss: 3.7492458820343018 | BCE Loss: 1.0187674760818481\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.725284099578857 | KNN Loss: 3.725639820098877 | BCE Loss: 0.9996441602706909\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.755992889404297 | KNN Loss: 3.722808837890625 | BCE Loss: 1.0331840515136719\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.731517314910889 | KNN Loss: 3.7050554752349854 | BCE Loss: 1.0264619588851929\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.743995189666748 | KNN Loss: 3.7205464839935303 | BCE Loss: 1.0234487056732178\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.7401885986328125 | KNN Loss: 3.7103841304779053 | BCE Loss: 1.0298045873641968\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.761066436767578 | KNN Loss: 3.745986223220825 | BCE Loss: 1.0150800943374634\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.743494510650635 | KNN Loss: 3.712418556213379 | BCE Loss: 1.0310758352279663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.804323673248291 | KNN Loss: 3.7712912559509277 | BCE Loss: 1.0330322980880737\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.743203163146973 | KNN Loss: 3.724619150161743 | BCE Loss: 1.0185837745666504\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.767424583435059 | KNN Loss: 3.7458789348602295 | BCE Loss: 1.021545648574829\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.785834312438965 | KNN Loss: 3.766901731491089 | BCE Loss: 1.018932580947876\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.73980188369751 | KNN Loss: 3.7337684631347656 | BCE Loss: 1.0060333013534546\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.798725605010986 | KNN Loss: 3.7558059692382812 | BCE Loss: 1.042919635772705\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.806766510009766 | KNN Loss: 3.750889301300049 | BCE Loss: 1.0558772087097168\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.716067790985107 | KNN Loss: 3.7115399837493896 | BCE Loss: 1.0045279264450073\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.786219120025635 | KNN Loss: 3.7709269523620605 | BCE Loss: 1.0152921676635742\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.796674728393555 | KNN Loss: 3.754329204559326 | BCE Loss: 1.0423452854156494\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.764847278594971 | KNN Loss: 3.720151662826538 | BCE Loss: 1.0446957349777222\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.780677795410156 | KNN Loss: 3.7558581829071045 | BCE Loss: 1.0248196125030518\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.779468536376953 | KNN Loss: 3.730297088623047 | BCE Loss: 1.0491716861724854\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.7507405281066895 | KNN Loss: 3.730180025100708 | BCE Loss: 1.020560383796692\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.778964042663574 | KNN Loss: 3.7577390670776367 | BCE Loss: 1.0212249755859375\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.772499084472656 | KNN Loss: 3.7322583198547363 | BCE Loss: 1.040241003036499\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.797705173492432 | KNN Loss: 3.7655773162841797 | BCE Loss: 1.0321279764175415\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.7658233642578125 | KNN Loss: 3.750152587890625 | BCE Loss: 1.0156710147857666\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.763579368591309 | KNN Loss: 3.7146530151367188 | BCE Loss: 1.0489262342453003\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.802045822143555 | KNN Loss: 3.7659859657287598 | BCE Loss: 1.0360597372055054\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.783505439758301 | KNN Loss: 3.7669777870178223 | BCE Loss: 1.0165278911590576\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.7896881103515625 | KNN Loss: 3.7389681339263916 | BCE Loss: 1.0507197380065918\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.7389631271362305 | KNN Loss: 3.7033498287200928 | BCE Loss: 1.0356130599975586\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.783176898956299 | KNN Loss: 3.7336082458496094 | BCE Loss: 1.049568772315979\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.7194061279296875 | KNN Loss: 3.723897933959961 | BCE Loss: 0.995508074760437\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.7786126136779785 | KNN Loss: 3.7541840076446533 | BCE Loss: 1.0244286060333252\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.767178535461426 | KNN Loss: 3.728520393371582 | BCE Loss: 1.0386579036712646\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.783166408538818 | KNN Loss: 3.7554268836975098 | BCE Loss: 1.0277395248413086\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.757077217102051 | KNN Loss: 3.7349941730499268 | BCE Loss: 1.022082805633545\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.755035400390625 | KNN Loss: 3.7209370136260986 | BCE Loss: 1.0340983867645264\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.725981712341309 | KNN Loss: 3.689403772354126 | BCE Loss: 1.0365779399871826\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.758418560028076 | KNN Loss: 3.74945330619812 | BCE Loss: 1.008965253829956\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.759151458740234 | KNN Loss: 3.722212314605713 | BCE Loss: 1.0369393825531006\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.774356365203857 | KNN Loss: 3.7213010787963867 | BCE Loss: 1.0530551671981812\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.787976264953613 | KNN Loss: 3.736631155014038 | BCE Loss: 1.0513451099395752\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.7985687255859375 | KNN Loss: 3.7525217533111572 | BCE Loss: 1.0460472106933594\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.748556137084961 | KNN Loss: 3.727562665939331 | BCE Loss: 1.0209935903549194\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.807476997375488 | KNN Loss: 3.776264190673828 | BCE Loss: 1.0312129259109497\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.739786148071289 | KNN Loss: 3.7267959117889404 | BCE Loss: 1.0129902362823486\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.760781288146973 | KNN Loss: 3.719043016433716 | BCE Loss: 1.0417381525039673\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.780642986297607 | KNN Loss: 3.730767250061035 | BCE Loss: 1.0498758554458618\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.813617706298828 | KNN Loss: 3.755892515182495 | BCE Loss: 1.0577250719070435\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.76694917678833 | KNN Loss: 3.738281488418579 | BCE Loss: 1.028667688369751\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.742795944213867 | KNN Loss: 3.7457220554351807 | BCE Loss: 0.997073769569397\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.769798278808594 | KNN Loss: 3.74540638923645 | BCE Loss: 1.0243921279907227\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.751720905303955 | KNN Loss: 3.736063241958618 | BCE Loss: 1.015657663345337\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.75788688659668 | KNN Loss: 3.7306220531463623 | BCE Loss: 1.0272648334503174\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.740344524383545 | KNN Loss: 3.724668502807617 | BCE Loss: 1.0156760215759277\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.767390251159668 | KNN Loss: 3.7194576263427734 | BCE Loss: 1.0479326248168945\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.756529808044434 | KNN Loss: 3.7497127056121826 | BCE Loss: 1.0068168640136719\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.789158821105957 | KNN Loss: 3.730950117111206 | BCE Loss: 1.0582084655761719\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.756075859069824 | KNN Loss: 3.7081918716430664 | BCE Loss: 1.0478841066360474\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.742124557495117 | KNN Loss: 3.7254884243011475 | BCE Loss: 1.0166360139846802\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.768711090087891 | KNN Loss: 3.7327170372009277 | BCE Loss: 1.0359938144683838\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.747193336486816 | KNN Loss: 3.7274012565612793 | BCE Loss: 1.019792079925537\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.723460674285889 | KNN Loss: 3.715982437133789 | BCE Loss: 1.0074782371520996\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.763133525848389 | KNN Loss: 3.7293291091918945 | BCE Loss: 1.0338045358657837\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.749814033508301 | KNN Loss: 3.7102773189544678 | BCE Loss: 1.0395365953445435\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.744329452514648 | KNN Loss: 3.731179714202881 | BCE Loss: 1.0131494998931885\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.730859756469727 | KNN Loss: 3.707312822341919 | BCE Loss: 1.0235470533370972\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.751429557800293 | KNN Loss: 3.720613718032837 | BCE Loss: 1.030815601348877\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.833011627197266 | KNN Loss: 3.7622904777526855 | BCE Loss: 1.07072114944458\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.775859832763672 | KNN Loss: 3.76676869392395 | BCE Loss: 1.0090912580490112\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.742213726043701 | KNN Loss: 3.7339134216308594 | BCE Loss: 1.0083001852035522\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.779267311096191 | KNN Loss: 3.7331197261810303 | BCE Loss: 1.0461475849151611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.778682231903076 | KNN Loss: 3.7395260334014893 | BCE Loss: 1.0391563177108765\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.757467746734619 | KNN Loss: 3.7363107204437256 | BCE Loss: 1.021157145500183\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.747154235839844 | KNN Loss: 3.7119104862213135 | BCE Loss: 1.0352437496185303\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.7187180519104 | KNN Loss: 3.7200770378112793 | BCE Loss: 0.9986411929130554\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.768254280090332 | KNN Loss: 3.735257148742676 | BCE Loss: 1.0329973697662354\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.743958950042725 | KNN Loss: 3.7258899211883545 | BCE Loss: 1.0180689096450806\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.759041786193848 | KNN Loss: 3.7442972660064697 | BCE Loss: 1.0147442817687988\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.7481584548950195 | KNN Loss: 3.71254563331604 | BCE Loss: 1.0356125831604004\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.721792697906494 | KNN Loss: 3.7274868488311768 | BCE Loss: 0.9943058490753174\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.75217866897583 | KNN Loss: 3.7264819145202637 | BCE Loss: 1.025696873664856\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.742249488830566 | KNN Loss: 3.718775510787964 | BCE Loss: 1.0234742164611816\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.756003379821777 | KNN Loss: 3.752098321914673 | BCE Loss: 1.0039048194885254\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.768261909484863 | KNN Loss: 3.7387948036193848 | BCE Loss: 1.0294671058654785\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.804832458496094 | KNN Loss: 3.7542331218719482 | BCE Loss: 1.0505990982055664\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.766571998596191 | KNN Loss: 3.7317748069763184 | BCE Loss: 1.034796953201294\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.763584136962891 | KNN Loss: 3.7366421222686768 | BCE Loss: 1.0269418954849243\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.780120849609375 | KNN Loss: 3.766753673553467 | BCE Loss: 1.0133671760559082\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.787964820861816 | KNN Loss: 3.722499132156372 | BCE Loss: 1.0654656887054443\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.773737907409668 | KNN Loss: 3.739633798599243 | BCE Loss: 1.0341041088104248\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.7707414627075195 | KNN Loss: 3.728468656539917 | BCE Loss: 1.0422728061676025\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.756439208984375 | KNN Loss: 3.749809741973877 | BCE Loss: 1.0066293478012085\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.776185989379883 | KNN Loss: 3.7408835887908936 | BCE Loss: 1.0353025197982788\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.773770809173584 | KNN Loss: 3.7425668239593506 | BCE Loss: 1.031204104423523\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.738818645477295 | KNN Loss: 3.7355244159698486 | BCE Loss: 1.0032942295074463\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.730315208435059 | KNN Loss: 3.723454475402832 | BCE Loss: 1.0068609714508057\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.737188339233398 | KNN Loss: 3.7305142879486084 | BCE Loss: 1.0066742897033691\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.747509956359863 | KNN Loss: 3.7340962886810303 | BCE Loss: 1.0134137868881226\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.770639419555664 | KNN Loss: 3.7395894527435303 | BCE Loss: 1.0310497283935547\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.753677845001221 | KNN Loss: 3.725105047225952 | BCE Loss: 1.028572678565979\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.792194366455078 | KNN Loss: 3.754917621612549 | BCE Loss: 1.0372769832611084\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.7606048583984375 | KNN Loss: 3.749516010284424 | BCE Loss: 1.0110887289047241\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.740311622619629 | KNN Loss: 3.7231533527374268 | BCE Loss: 1.0171583890914917\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.764063358306885 | KNN Loss: 3.7233879566192627 | BCE Loss: 1.040675401687622\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.7787885665893555 | KNN Loss: 3.7695751190185547 | BCE Loss: 1.0092133283615112\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.732320308685303 | KNN Loss: 3.706835985183716 | BCE Loss: 1.025484323501587\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.7449846267700195 | KNN Loss: 3.7362546920776367 | BCE Loss: 1.008730173110962\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.748613357543945 | KNN Loss: 3.7461514472961426 | BCE Loss: 1.0024621486663818\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.736827850341797 | KNN Loss: 3.7315869331359863 | BCE Loss: 1.0052411556243896\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.793249130249023 | KNN Loss: 3.73071551322937 | BCE Loss: 1.0625333786010742\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.785523414611816 | KNN Loss: 3.769892454147339 | BCE Loss: 1.0156309604644775\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.7896013259887695 | KNN Loss: 3.7665534019470215 | BCE Loss: 1.0230481624603271\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.809115409851074 | KNN Loss: 3.775019645690918 | BCE Loss: 1.0340956449508667\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.739370346069336 | KNN Loss: 3.725353956222534 | BCE Loss: 1.0140166282653809\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.713789939880371 | KNN Loss: 3.7037925720214844 | BCE Loss: 1.0099976062774658\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.740607738494873 | KNN Loss: 3.7155556678771973 | BCE Loss: 1.0250521898269653\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.775660991668701 | KNN Loss: 3.744293212890625 | BCE Loss: 1.0313677787780762\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.7296319007873535 | KNN Loss: 3.7213947772979736 | BCE Loss: 1.0082371234893799\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.745985507965088 | KNN Loss: 3.7223010063171387 | BCE Loss: 1.0236845016479492\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.713440418243408 | KNN Loss: 3.689744234085083 | BCE Loss: 1.0236963033676147\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.741645336151123 | KNN Loss: 3.7207887172698975 | BCE Loss: 1.0208566188812256\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.803339958190918 | KNN Loss: 3.730154275894165 | BCE Loss: 1.073185682296753\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.785470962524414 | KNN Loss: 3.771122694015503 | BCE Loss: 1.0143482685089111\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.739619731903076 | KNN Loss: 3.7182071208953857 | BCE Loss: 1.0214126110076904\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.772195816040039 | KNN Loss: 3.719310998916626 | BCE Loss: 1.0528846979141235\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.775802135467529 | KNN Loss: 3.7524890899658203 | BCE Loss: 1.023313045501709\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.768298149108887 | KNN Loss: 3.74828839302063 | BCE Loss: 1.020009994506836\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.743022918701172 | KNN Loss: 3.7235145568847656 | BCE Loss: 1.0195081233978271\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.739839553833008 | KNN Loss: 3.731549024581909 | BCE Loss: 1.0082902908325195\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.768990993499756 | KNN Loss: 3.739976644515991 | BCE Loss: 1.0290143489837646\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.757375717163086 | KNN Loss: 3.7378134727478027 | BCE Loss: 1.019562005996704\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.7289628982543945 | KNN Loss: 3.7001070976257324 | BCE Loss: 1.028855562210083\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.782423496246338 | KNN Loss: 3.7323949337005615 | BCE Loss: 1.0500285625457764\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.728941440582275 | KNN Loss: 3.7010676860809326 | BCE Loss: 1.0278737545013428\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.75920295715332 | KNN Loss: 3.732827663421631 | BCE Loss: 1.0263752937316895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.732298374176025 | KNN Loss: 3.727149248123169 | BCE Loss: 1.0051491260528564\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.73263692855835 | KNN Loss: 3.7160277366638184 | BCE Loss: 1.0166091918945312\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.771571636199951 | KNN Loss: 3.7526919841766357 | BCE Loss: 1.0188796520233154\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.807049751281738 | KNN Loss: 3.767599105834961 | BCE Loss: 1.039450764656067\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.786619186401367 | KNN Loss: 3.753934860229492 | BCE Loss: 1.032684564590454\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.76007080078125 | KNN Loss: 3.7363476753234863 | BCE Loss: 1.0237228870391846\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.695275783538818 | KNN Loss: 3.698026180267334 | BCE Loss: 0.9972495436668396\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.7546281814575195 | KNN Loss: 3.723904848098755 | BCE Loss: 1.0307235717773438\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.739515781402588 | KNN Loss: 3.700157642364502 | BCE Loss: 1.039358139038086\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.732002258300781 | KNN Loss: 3.7359607219696045 | BCE Loss: 0.9960415363311768\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.730198860168457 | KNN Loss: 3.7272579669952393 | BCE Loss: 1.0029407739639282\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.740054607391357 | KNN Loss: 3.721526861190796 | BCE Loss: 1.0185277462005615\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.789487838745117 | KNN Loss: 3.7592196464538574 | BCE Loss: 1.0302681922912598\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.820390701293945 | KNN Loss: 3.781477928161621 | BCE Loss: 1.0389125347137451\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.734956741333008 | KNN Loss: 3.72121000289917 | BCE Loss: 1.013746738433838\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.76697301864624 | KNN Loss: 3.754742383956909 | BCE Loss: 1.012230634689331\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.80009651184082 | KNN Loss: 3.755420207977295 | BCE Loss: 1.0446763038635254\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.765118598937988 | KNN Loss: 3.751643180847168 | BCE Loss: 1.0134754180908203\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.781866073608398 | KNN Loss: 3.754549503326416 | BCE Loss: 1.027316689491272\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.7706451416015625 | KNN Loss: 3.7476377487182617 | BCE Loss: 1.0230075120925903\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.773928165435791 | KNN Loss: 3.7447509765625 | BCE Loss: 1.0291773080825806\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.751322269439697 | KNN Loss: 3.729247570037842 | BCE Loss: 1.0220746994018555\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.794460773468018 | KNN Loss: 3.7570700645446777 | BCE Loss: 1.0373907089233398\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.7535529136657715 | KNN Loss: 3.7242448329925537 | BCE Loss: 1.0293081998825073\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.737239837646484 | KNN Loss: 3.729945421218872 | BCE Loss: 1.0072944164276123\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.728888511657715 | KNN Loss: 3.715646743774414 | BCE Loss: 1.0132417678833008\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.713113784790039 | KNN Loss: 3.710705280303955 | BCE Loss: 1.0024082660675049\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.773003578186035 | KNN Loss: 3.7364683151245117 | BCE Loss: 1.0365352630615234\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.775806427001953 | KNN Loss: 3.7369072437286377 | BCE Loss: 1.0388994216918945\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.722574234008789 | KNN Loss: 3.7031567096710205 | BCE Loss: 1.0194175243377686\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.7719316482543945 | KNN Loss: 3.73146915435791 | BCE Loss: 1.0404622554779053\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.7874755859375 | KNN Loss: 3.744553565979004 | BCE Loss: 1.042922019958496\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.753551006317139 | KNN Loss: 3.737800359725952 | BCE Loss: 1.0157506465911865\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.787631988525391 | KNN Loss: 3.7308733463287354 | BCE Loss: 1.0567588806152344\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.744242191314697 | KNN Loss: 3.7131965160369873 | BCE Loss: 1.0310457944869995\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.753702640533447 | KNN Loss: 3.7419755458831787 | BCE Loss: 1.011727213859558\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.728109359741211 | KNN Loss: 3.7099173069000244 | BCE Loss: 1.0181918144226074\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.726810455322266 | KNN Loss: 3.717633008956909 | BCE Loss: 1.0091774463653564\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.772432804107666 | KNN Loss: 3.7133066654205322 | BCE Loss: 1.0591261386871338\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.7510199546813965 | KNN Loss: 3.716428518295288 | BCE Loss: 1.0345914363861084\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.763674736022949 | KNN Loss: 3.7406539916992188 | BCE Loss: 1.0230205059051514\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.728514671325684 | KNN Loss: 3.7085678577423096 | BCE Loss: 1.019946575164795\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.761749267578125 | KNN Loss: 3.7261850833892822 | BCE Loss: 1.0355639457702637\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.786083221435547 | KNN Loss: 3.7340786457061768 | BCE Loss: 1.0520045757293701\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.760236740112305 | KNN Loss: 3.7259180545806885 | BCE Loss: 1.0343185663223267\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.775924205780029 | KNN Loss: 3.721055030822754 | BCE Loss: 1.0548691749572754\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.71332311630249 | KNN Loss: 3.6990342140197754 | BCE Loss: 1.0142889022827148\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.783143997192383 | KNN Loss: 3.7374441623687744 | BCE Loss: 1.0456995964050293\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.802812099456787 | KNN Loss: 3.7704226970672607 | BCE Loss: 1.0323894023895264\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.776756763458252 | KNN Loss: 3.738194704055786 | BCE Loss: 1.0385620594024658\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.785216808319092 | KNN Loss: 3.7068049907684326 | BCE Loss: 1.0784119367599487\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.748049259185791 | KNN Loss: 3.7279419898986816 | BCE Loss: 1.020107388496399\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.800778388977051 | KNN Loss: 3.7500133514404297 | BCE Loss: 1.0507651567459106\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.733171463012695 | KNN Loss: 3.708256959915161 | BCE Loss: 1.0249147415161133\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.764941692352295 | KNN Loss: 3.7273287773132324 | BCE Loss: 1.0376129150390625\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.760727882385254 | KNN Loss: 3.7510809898376465 | BCE Loss: 1.0096471309661865\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.6991658210754395 | KNN Loss: 3.7118616104125977 | BCE Loss: 0.9873040914535522\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.738499641418457 | KNN Loss: 3.726247549057007 | BCE Loss: 1.012251853942871\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.72645902633667 | KNN Loss: 3.7038862705230713 | BCE Loss: 1.022572636604309\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.7855730056762695 | KNN Loss: 3.75705623626709 | BCE Loss: 1.0285170078277588\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.739072799682617 | KNN Loss: 3.740710973739624 | BCE Loss: 0.9983619451522827\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.790316581726074 | KNN Loss: 3.7483255863189697 | BCE Loss: 1.0419912338256836\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.764555931091309 | KNN Loss: 3.7186546325683594 | BCE Loss: 1.0459012985229492\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.706221580505371 | KNN Loss: 3.6915457248687744 | BCE Loss: 1.0146760940551758\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.757816314697266 | KNN Loss: 3.738823652267456 | BCE Loss: 1.0189927816390991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.744203567504883 | KNN Loss: 3.728470802307129 | BCE Loss: 1.0157325267791748\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.791128158569336 | KNN Loss: 3.7581899166107178 | BCE Loss: 1.0329383611679077\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.785099506378174 | KNN Loss: 3.7327120304107666 | BCE Loss: 1.0523874759674072\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.823698043823242 | KNN Loss: 3.756364107131958 | BCE Loss: 1.067333698272705\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.748905181884766 | KNN Loss: 3.7328269481658936 | BCE Loss: 1.0160784721374512\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.755220413208008 | KNN Loss: 3.7157833576202393 | BCE Loss: 1.039436936378479\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.740978717803955 | KNN Loss: 3.7173399925231934 | BCE Loss: 1.0236387252807617\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.73416805267334 | KNN Loss: 3.7425291538238525 | BCE Loss: 0.991638720035553\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.737909317016602 | KNN Loss: 3.7066049575805664 | BCE Loss: 1.0313045978546143\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.7379865646362305 | KNN Loss: 3.7186598777770996 | BCE Loss: 1.0193268060684204\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.768622398376465 | KNN Loss: 3.7321834564208984 | BCE Loss: 1.0364387035369873\n",
      "Epoch    99: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.7495317459106445 | KNN Loss: 3.72168231010437 | BCE Loss: 1.0278494358062744\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.766763687133789 | KNN Loss: 3.739156484603882 | BCE Loss: 1.0276069641113281\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.700745582580566 | KNN Loss: 3.7220568656921387 | BCE Loss: 0.9786884784698486\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.764732360839844 | KNN Loss: 3.718522787094116 | BCE Loss: 1.0462098121643066\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.746075630187988 | KNN Loss: 3.727691173553467 | BCE Loss: 1.018384337425232\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.770464897155762 | KNN Loss: 3.738255739212036 | BCE Loss: 1.0322093963623047\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.752161979675293 | KNN Loss: 3.7418651580810547 | BCE Loss: 1.0102967023849487\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.77625036239624 | KNN Loss: 3.7189550399780273 | BCE Loss: 1.057295322418213\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.727829456329346 | KNN Loss: 3.719900131225586 | BCE Loss: 1.0079294443130493\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.762781143188477 | KNN Loss: 3.7367939949035645 | BCE Loss: 1.0259873867034912\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.740159034729004 | KNN Loss: 3.7333788871765137 | BCE Loss: 1.0067801475524902\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.766918659210205 | KNN Loss: 3.73856782913208 | BCE Loss: 1.0283507108688354\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.760422706604004 | KNN Loss: 3.738137722015381 | BCE Loss: 1.022284984588623\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.780123710632324 | KNN Loss: 3.7512001991271973 | BCE Loss: 1.028923511505127\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.7694292068481445 | KNN Loss: 3.7472434043884277 | BCE Loss: 1.0221855640411377\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.749924659729004 | KNN Loss: 3.7227694988250732 | BCE Loss: 1.0271550416946411\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.788054466247559 | KNN Loss: 3.762382745742798 | BCE Loss: 1.0256717205047607\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.753267288208008 | KNN Loss: 3.7310616970062256 | BCE Loss: 1.0222055912017822\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.74245023727417 | KNN Loss: 3.706768035888672 | BCE Loss: 1.0356820821762085\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.775025367736816 | KNN Loss: 3.752678632736206 | BCE Loss: 1.0223467350006104\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.743545055389404 | KNN Loss: 3.7349417209625244 | BCE Loss: 1.0086034536361694\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.752382278442383 | KNN Loss: 3.7154366970062256 | BCE Loss: 1.0369457006454468\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.76423454284668 | KNN Loss: 3.7394893169403076 | BCE Loss: 1.024745225906372\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.73369836807251 | KNN Loss: 3.7289609909057617 | BCE Loss: 1.0047374963760376\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.73604679107666 | KNN Loss: 3.6970503330230713 | BCE Loss: 1.038996696472168\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.7281413078308105 | KNN Loss: 3.7241814136505127 | BCE Loss: 1.0039598941802979\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.719565391540527 | KNN Loss: 3.7164788246154785 | BCE Loss: 1.0030863285064697\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.735966682434082 | KNN Loss: 3.716853141784668 | BCE Loss: 1.0191137790679932\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.771090507507324 | KNN Loss: 3.7240028381347656 | BCE Loss: 1.047087550163269\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.7350993156433105 | KNN Loss: 3.7201688289642334 | BCE Loss: 1.0149306058883667\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.771822452545166 | KNN Loss: 3.7383012771606445 | BCE Loss: 1.033521056175232\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.787259578704834 | KNN Loss: 3.728890895843506 | BCE Loss: 1.0583686828613281\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.713016510009766 | KNN Loss: 3.6940929889678955 | BCE Loss: 1.0189235210418701\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.827305316925049 | KNN Loss: 3.73606014251709 | BCE Loss: 1.091245174407959\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.7605438232421875 | KNN Loss: 3.729938507080078 | BCE Loss: 1.0306053161621094\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.751786231994629 | KNN Loss: 3.7412450313568115 | BCE Loss: 1.0105414390563965\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.723130226135254 | KNN Loss: 3.702420473098755 | BCE Loss: 1.0207096338272095\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.793797492980957 | KNN Loss: 3.7289140224456787 | BCE Loss: 1.0648837089538574\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.7389607429504395 | KNN Loss: 3.7155802249908447 | BCE Loss: 1.0233805179595947\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.735706329345703 | KNN Loss: 3.72113037109375 | BCE Loss: 1.0145759582519531\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.7058796882629395 | KNN Loss: 3.71051025390625 | BCE Loss: 0.9953692555427551\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.751133441925049 | KNN Loss: 3.731539726257324 | BCE Loss: 1.0195937156677246\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.755880355834961 | KNN Loss: 3.73860239982605 | BCE Loss: 1.0172778367996216\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.744326591491699 | KNN Loss: 3.7063663005828857 | BCE Loss: 1.0379605293273926\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.78364896774292 | KNN Loss: 3.752650260925293 | BCE Loss: 1.0309988260269165\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.709445953369141 | KNN Loss: 3.6889383792877197 | BCE Loss: 1.0205073356628418\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.732876300811768 | KNN Loss: 3.6994564533233643 | BCE Loss: 1.0334199666976929\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.749286651611328 | KNN Loss: 3.7026541233062744 | BCE Loss: 1.0466326475143433\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.7594218254089355 | KNN Loss: 3.738881826400757 | BCE Loss: 1.0205398797988892\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.735204696655273 | KNN Loss: 3.7155420780181885 | BCE Loss: 1.019662857055664\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.751226425170898 | KNN Loss: 3.6947360038757324 | BCE Loss: 1.056490182876587\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.701210975646973 | KNN Loss: 3.694023847579956 | BCE Loss: 1.0071872472763062\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.741571426391602 | KNN Loss: 3.7130630016326904 | BCE Loss: 1.0285084247589111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.766383171081543 | KNN Loss: 3.721853256225586 | BCE Loss: 1.0445301532745361\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.790277481079102 | KNN Loss: 3.7404749393463135 | BCE Loss: 1.049802541732788\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.7837700843811035 | KNN Loss: 3.7410778999328613 | BCE Loss: 1.0426921844482422\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.756117820739746 | KNN Loss: 3.7103755474090576 | BCE Loss: 1.045742392539978\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.707571983337402 | KNN Loss: 3.7029592990875244 | BCE Loss: 1.0046124458312988\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.696730136871338 | KNN Loss: 3.7123231887817383 | BCE Loss: 0.9844069480895996\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.7585577964782715 | KNN Loss: 3.6969218254089355 | BCE Loss: 1.0616360902786255\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.741058826446533 | KNN Loss: 3.7382378578186035 | BCE Loss: 1.0028209686279297\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.754125118255615 | KNN Loss: 3.7068066596984863 | BCE Loss: 1.047318458557129\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.733245849609375 | KNN Loss: 3.7140045166015625 | BCE Loss: 1.0192413330078125\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.766834259033203 | KNN Loss: 3.733088493347168 | BCE Loss: 1.0337456464767456\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.784296989440918 | KNN Loss: 3.7648777961730957 | BCE Loss: 1.0194189548492432\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.696333885192871 | KNN Loss: 3.6909677982330322 | BCE Loss: 1.0053658485412598\n",
      "Epoch   110: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.742398262023926 | KNN Loss: 3.727060556411743 | BCE Loss: 1.0153374671936035\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.715702056884766 | KNN Loss: 3.702824831008911 | BCE Loss: 1.0128774642944336\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.711606502532959 | KNN Loss: 3.6984260082244873 | BCE Loss: 1.0131804943084717\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.747063636779785 | KNN Loss: 3.7356300354003906 | BCE Loss: 1.0114333629608154\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.796529293060303 | KNN Loss: 3.7650651931762695 | BCE Loss: 1.0314639806747437\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.753359317779541 | KNN Loss: 3.7207510471343994 | BCE Loss: 1.0326082706451416\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.694467544555664 | KNN Loss: 3.692204475402832 | BCE Loss: 1.002262830734253\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.75272798538208 | KNN Loss: 3.7121424674987793 | BCE Loss: 1.0405855178833008\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.782544136047363 | KNN Loss: 3.7251524925231934 | BCE Loss: 1.057391881942749\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.751482963562012 | KNN Loss: 3.7278525829315186 | BCE Loss: 1.0236303806304932\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.722206115722656 | KNN Loss: 3.6973979473114014 | BCE Loss: 1.0248081684112549\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.7308759689331055 | KNN Loss: 3.720454692840576 | BCE Loss: 1.0104213953018188\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.730007171630859 | KNN Loss: 3.7074947357177734 | BCE Loss: 1.0225121974945068\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.778922080993652 | KNN Loss: 3.7295987606048584 | BCE Loss: 1.049323558807373\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.7542524337768555 | KNN Loss: 3.7330305576324463 | BCE Loss: 1.02122163772583\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.787514686584473 | KNN Loss: 3.7645184993743896 | BCE Loss: 1.022995948791504\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.787930488586426 | KNN Loss: 3.734480857849121 | BCE Loss: 1.0534496307373047\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.766225814819336 | KNN Loss: 3.7456789016723633 | BCE Loss: 1.020546793937683\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.743594646453857 | KNN Loss: 3.720475912094116 | BCE Loss: 1.0231187343597412\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.810140609741211 | KNN Loss: 3.7549872398376465 | BCE Loss: 1.0551533699035645\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.723890781402588 | KNN Loss: 3.7080230712890625 | BCE Loss: 1.0158677101135254\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.70268440246582 | KNN Loss: 3.729343891143799 | BCE Loss: 0.9733403921127319\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.735387325286865 | KNN Loss: 3.713041067123413 | BCE Loss: 1.0223462581634521\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.712327480316162 | KNN Loss: 3.7021868228912354 | BCE Loss: 1.0101406574249268\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.744746685028076 | KNN Loss: 3.7213191986083984 | BCE Loss: 1.0234274864196777\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.720780849456787 | KNN Loss: 3.727141857147217 | BCE Loss: 0.9936388731002808\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.730978965759277 | KNN Loss: 3.714653968811035 | BCE Loss: 1.0163251161575317\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.751481056213379 | KNN Loss: 3.722778081893921 | BCE Loss: 1.028703212738037\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.7840375900268555 | KNN Loss: 3.722644090652466 | BCE Loss: 1.0613936185836792\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.789872169494629 | KNN Loss: 3.7423503398895264 | BCE Loss: 1.0475220680236816\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.700267791748047 | KNN Loss: 3.688124895095825 | BCE Loss: 1.0121431350708008\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.7452898025512695 | KNN Loss: 3.73925518989563 | BCE Loss: 1.0060346126556396\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.731478214263916 | KNN Loss: 3.706178665161133 | BCE Loss: 1.0252996683120728\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.758610248565674 | KNN Loss: 3.733751058578491 | BCE Loss: 1.0248593091964722\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.742929458618164 | KNN Loss: 3.7189443111419678 | BCE Loss: 1.0239851474761963\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.748922348022461 | KNN Loss: 3.7299611568450928 | BCE Loss: 1.018960952758789\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.719099998474121 | KNN Loss: 3.7052063941955566 | BCE Loss: 1.0138938426971436\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.733610153198242 | KNN Loss: 3.719714879989624 | BCE Loss: 1.0138951539993286\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.725166320800781 | KNN Loss: 3.6917338371276855 | BCE Loss: 1.0334324836730957\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.678268909454346 | KNN Loss: 3.6767237186431885 | BCE Loss: 1.0015450716018677\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.75656270980835 | KNN Loss: 3.703439235687256 | BCE Loss: 1.0531234741210938\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.720414161682129 | KNN Loss: 3.69818115234375 | BCE Loss: 1.0222327709197998\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.7403154373168945 | KNN Loss: 3.725470781326294 | BCE Loss: 1.014844536781311\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.707011699676514 | KNN Loss: 3.6842870712280273 | BCE Loss: 1.0227245092391968\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.754179954528809 | KNN Loss: 3.725472927093506 | BCE Loss: 1.0287070274353027\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.765387535095215 | KNN Loss: 3.7177748680114746 | BCE Loss: 1.0476126670837402\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.724535942077637 | KNN Loss: 3.7149312496185303 | BCE Loss: 1.0096046924591064\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.767043113708496 | KNN Loss: 3.748854875564575 | BCE Loss: 1.0181884765625\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.769189357757568 | KNN Loss: 3.727778911590576 | BCE Loss: 1.0414104461669922\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.748459815979004 | KNN Loss: 3.7219057083129883 | BCE Loss: 1.0265538692474365\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.7940521240234375 | KNN Loss: 3.7284929752349854 | BCE Loss: 1.0655593872070312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.726959705352783 | KNN Loss: 3.7319440841674805 | BCE Loss: 0.9950157999992371\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.769983291625977 | KNN Loss: 3.7198033332824707 | BCE Loss: 1.050180196762085\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.776616096496582 | KNN Loss: 3.710921049118042 | BCE Loss: 1.0656951665878296\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.756170749664307 | KNN Loss: 3.717165231704712 | BCE Loss: 1.0390053987503052\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.712133884429932 | KNN Loss: 3.7022809982299805 | BCE Loss: 1.0098527669906616\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.762120246887207 | KNN Loss: 3.7132728099823 | BCE Loss: 1.0488471984863281\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.71661376953125 | KNN Loss: 3.6954264640808105 | BCE Loss: 1.0211875438690186\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.768210411071777 | KNN Loss: 3.756453037261963 | BCE Loss: 1.0117576122283936\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.717744827270508 | KNN Loss: 3.7145440578460693 | BCE Loss: 1.0032005310058594\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.748493194580078 | KNN Loss: 3.7279114723205566 | BCE Loss: 1.020581841468811\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.780298233032227 | KNN Loss: 3.7339584827423096 | BCE Loss: 1.046339511871338\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.725944995880127 | KNN Loss: 3.6907565593719482 | BCE Loss: 1.0351884365081787\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.72864294052124 | KNN Loss: 3.691345691680908 | BCE Loss: 1.037297248840332\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.726914882659912 | KNN Loss: 3.7038426399230957 | BCE Loss: 1.0230721235275269\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.74980354309082 | KNN Loss: 3.749650478363037 | BCE Loss: 1.000152826309204\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.782310485839844 | KNN Loss: 3.7326412200927734 | BCE Loss: 1.0496691465377808\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.767414093017578 | KNN Loss: 3.733917713165283 | BCE Loss: 1.033496618270874\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.773580551147461 | KNN Loss: 3.761949062347412 | BCE Loss: 1.0116312503814697\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.729034900665283 | KNN Loss: 3.718731641769409 | BCE Loss: 1.010303258895874\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.7133469581604 | KNN Loss: 3.7007062435150146 | BCE Loss: 1.0126407146453857\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.756345748901367 | KNN Loss: 3.727282762527466 | BCE Loss: 1.0290628671646118\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.724615573883057 | KNN Loss: 3.6854116916656494 | BCE Loss: 1.0392038822174072\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.7679924964904785 | KNN Loss: 3.730323076248169 | BCE Loss: 1.0376694202423096\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.762946605682373 | KNN Loss: 3.7365074157714844 | BCE Loss: 1.0264391899108887\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.7715067863464355 | KNN Loss: 3.7394206523895264 | BCE Loss: 1.0320861339569092\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.723500728607178 | KNN Loss: 3.723536252975464 | BCE Loss: 0.9999645352363586\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.742566108703613 | KNN Loss: 3.7371931076049805 | BCE Loss: 1.0053728818893433\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.766026020050049 | KNN Loss: 3.731074333190918 | BCE Loss: 1.0349515676498413\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.714143753051758 | KNN Loss: 3.6821835041046143 | BCE Loss: 1.031960129737854\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.716155529022217 | KNN Loss: 3.7052674293518066 | BCE Loss: 1.0108879804611206\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.734609603881836 | KNN Loss: 3.7135138511657715 | BCE Loss: 1.021095871925354\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.713895797729492 | KNN Loss: 3.6851894855499268 | BCE Loss: 1.0287060737609863\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.717296600341797 | KNN Loss: 3.7156972885131836 | BCE Loss: 1.0015990734100342\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.763608455657959 | KNN Loss: 3.741105556488037 | BCE Loss: 1.0225030183792114\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.719313621520996 | KNN Loss: 3.6991536617279053 | BCE Loss: 1.02016019821167\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.755240440368652 | KNN Loss: 3.724900960922241 | BCE Loss: 1.0303397178649902\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.733078956604004 | KNN Loss: 3.707242250442505 | BCE Loss: 1.02583646774292\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.731107711791992 | KNN Loss: 3.712663173675537 | BCE Loss: 1.018444538116455\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.735306739807129 | KNN Loss: 3.715367078781128 | BCE Loss: 1.0199397802352905\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.738300800323486 | KNN Loss: 3.7105302810668945 | BCE Loss: 1.0277705192565918\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.748184680938721 | KNN Loss: 3.7358837127685547 | BCE Loss: 1.0123008489608765\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.704084396362305 | KNN Loss: 3.6961350440979004 | BCE Loss: 1.0079493522644043\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.7733964920043945 | KNN Loss: 3.708646297454834 | BCE Loss: 1.0647499561309814\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.781668186187744 | KNN Loss: 3.740553855895996 | BCE Loss: 1.041114330291748\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.755753517150879 | KNN Loss: 3.7137327194213867 | BCE Loss: 1.0420209169387817\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.711969375610352 | KNN Loss: 3.7223379611968994 | BCE Loss: 0.9896314144134521\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.7351226806640625 | KNN Loss: 3.716543197631836 | BCE Loss: 1.0185792446136475\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.751216888427734 | KNN Loss: 3.7333507537841797 | BCE Loss: 1.0178663730621338\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.765536308288574 | KNN Loss: 3.738072633743286 | BCE Loss: 1.0274639129638672\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.722598075866699 | KNN Loss: 3.694535493850708 | BCE Loss: 1.0280624628067017\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.714826583862305 | KNN Loss: 3.715543746948242 | BCE Loss: 0.999282956123352\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.718709468841553 | KNN Loss: 3.7055392265319824 | BCE Loss: 1.0131702423095703\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.711668014526367 | KNN Loss: 3.7093358039855957 | BCE Loss: 1.002332329750061\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.738947868347168 | KNN Loss: 3.706655979156494 | BCE Loss: 1.0322918891906738\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.7763142585754395 | KNN Loss: 3.745558738708496 | BCE Loss: 1.0307555198669434\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.6841583251953125 | KNN Loss: 3.676628589630127 | BCE Loss: 1.0075299739837646\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.753762245178223 | KNN Loss: 3.7241530418395996 | BCE Loss: 1.0296090841293335\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.742735385894775 | KNN Loss: 3.679811716079712 | BCE Loss: 1.062923789024353\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.768598556518555 | KNN Loss: 3.7320518493652344 | BCE Loss: 1.0365469455718994\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.775578498840332 | KNN Loss: 3.731487512588501 | BCE Loss: 1.044090986251831\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.759364604949951 | KNN Loss: 3.747159004211426 | BCE Loss: 1.0122056007385254\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.7291717529296875 | KNN Loss: 3.7209954261779785 | BCE Loss: 1.008176565170288\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.7076287269592285 | KNN Loss: 3.698857545852661 | BCE Loss: 1.008771300315857\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.704570293426514 | KNN Loss: 3.6902365684509277 | BCE Loss: 1.0143338441848755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.81531286239624 | KNN Loss: 3.7517008781433105 | BCE Loss: 1.0636119842529297\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.734622001647949 | KNN Loss: 3.7043774127960205 | BCE Loss: 1.0302447080612183\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.729386329650879 | KNN Loss: 3.694829225540161 | BCE Loss: 1.0345571041107178\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.717040538787842 | KNN Loss: 3.720607042312622 | BCE Loss: 0.9964333772659302\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.73325252532959 | KNN Loss: 3.745354652404785 | BCE Loss: 0.9878976345062256\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.732753753662109 | KNN Loss: 3.714066743850708 | BCE Loss: 1.0186870098114014\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.747666835784912 | KNN Loss: 3.690275192260742 | BCE Loss: 1.05739164352417\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.709174156188965 | KNN Loss: 3.696916103363037 | BCE Loss: 1.0122582912445068\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.701415538787842 | KNN Loss: 3.6955759525299072 | BCE Loss: 1.005839467048645\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.711177825927734 | KNN Loss: 3.712257146835327 | BCE Loss: 0.9989207983016968\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.744938850402832 | KNN Loss: 3.7407214641571045 | BCE Loss: 1.0042173862457275\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.736778259277344 | KNN Loss: 3.7115378379821777 | BCE Loss: 1.0252405405044556\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.734389305114746 | KNN Loss: 3.7153568267822266 | BCE Loss: 1.0190322399139404\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.74575138092041 | KNN Loss: 3.7213337421417236 | BCE Loss: 1.0244178771972656\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.728269577026367 | KNN Loss: 3.685551643371582 | BCE Loss: 1.0427181720733643\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.752553939819336 | KNN Loss: 3.7259292602539062 | BCE Loss: 1.0266244411468506\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.719254493713379 | KNN Loss: 3.6989455223083496 | BCE Loss: 1.0203092098236084\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.71998405456543 | KNN Loss: 3.702577590942383 | BCE Loss: 1.0174065828323364\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.747833728790283 | KNN Loss: 3.72441029548645 | BCE Loss: 1.023423433303833\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.7739973068237305 | KNN Loss: 3.7536325454711914 | BCE Loss: 1.02036452293396\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.720276832580566 | KNN Loss: 3.7073285579681396 | BCE Loss: 1.0129483938217163\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.776212692260742 | KNN Loss: 3.7509233951568604 | BCE Loss: 1.0252891778945923\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.71666145324707 | KNN Loss: 3.699557304382324 | BCE Loss: 1.017104148864746\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.728531360626221 | KNN Loss: 3.7116806507110596 | BCE Loss: 1.0168507099151611\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.729452133178711 | KNN Loss: 3.7193515300750732 | BCE Loss: 1.0101008415222168\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.714946746826172 | KNN Loss: 3.6881980895996094 | BCE Loss: 1.0267486572265625\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.7467498779296875 | KNN Loss: 3.727626323699951 | BCE Loss: 1.0191236734390259\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.73478889465332 | KNN Loss: 3.715437412261963 | BCE Loss: 1.0193512439727783\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.762112617492676 | KNN Loss: 3.7159790992736816 | BCE Loss: 1.0461335182189941\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.748892784118652 | KNN Loss: 3.734651803970337 | BCE Loss: 1.0142409801483154\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.705024719238281 | KNN Loss: 3.6702747344970703 | BCE Loss: 1.0347497463226318\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.709013938903809 | KNN Loss: 3.696575164794922 | BCE Loss: 1.0124388933181763\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.698256969451904 | KNN Loss: 3.7027671337127686 | BCE Loss: 0.9954899549484253\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.699816703796387 | KNN Loss: 3.7132115364074707 | BCE Loss: 0.9866049885749817\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.761235237121582 | KNN Loss: 3.730653762817383 | BCE Loss: 1.0305814743041992\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.711750507354736 | KNN Loss: 3.692622661590576 | BCE Loss: 1.0191278457641602\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.772227764129639 | KNN Loss: 3.736454963684082 | BCE Loss: 1.035772681236267\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.727769374847412 | KNN Loss: 3.715148448944092 | BCE Loss: 1.0126210451126099\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.824304580688477 | KNN Loss: 3.7402472496032715 | BCE Loss: 1.084057092666626\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.761992931365967 | KNN Loss: 3.7342262268066406 | BCE Loss: 1.0277665853500366\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.73495626449585 | KNN Loss: 3.702317237854004 | BCE Loss: 1.0326391458511353\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.744296073913574 | KNN Loss: 3.7060294151306152 | BCE Loss: 1.038266658782959\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.778897285461426 | KNN Loss: 3.7380917072296143 | BCE Loss: 1.0408055782318115\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.766974449157715 | KNN Loss: 3.7279138565063477 | BCE Loss: 1.0390608310699463\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.759940147399902 | KNN Loss: 3.7236528396606445 | BCE Loss: 1.0362874269485474\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.719570636749268 | KNN Loss: 3.7127015590667725 | BCE Loss: 1.0068689584732056\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.759374618530273 | KNN Loss: 3.7263429164886475 | BCE Loss: 1.0330314636230469\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.723888874053955 | KNN Loss: 3.7043521404266357 | BCE Loss: 1.0195367336273193\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.689490795135498 | KNN Loss: 3.6996524333953857 | BCE Loss: 0.989838182926178\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.76408576965332 | KNN Loss: 3.74831485748291 | BCE Loss: 1.0157710313796997\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.705105781555176 | KNN Loss: 3.7000973224639893 | BCE Loss: 1.0050084590911865\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.766571521759033 | KNN Loss: 3.7263903617858887 | BCE Loss: 1.040181040763855\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.714580535888672 | KNN Loss: 3.69805908203125 | BCE Loss: 1.0165212154388428\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.750690937042236 | KNN Loss: 3.749067783355713 | BCE Loss: 1.001623272895813\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.781461715698242 | KNN Loss: 3.735194683074951 | BCE Loss: 1.046267032623291\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.741559028625488 | KNN Loss: 3.7236812114715576 | BCE Loss: 1.0178775787353516\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.712279319763184 | KNN Loss: 3.6939709186553955 | BCE Loss: 1.018308401107788\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.774758338928223 | KNN Loss: 3.7382798194885254 | BCE Loss: 1.0364787578582764\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.730850696563721 | KNN Loss: 3.7269697189331055 | BCE Loss: 1.0038809776306152\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.739093780517578 | KNN Loss: 3.710301160812378 | BCE Loss: 1.0287925004959106\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.76624059677124 | KNN Loss: 3.734320640563965 | BCE Loss: 1.0319199562072754\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.704226493835449 | KNN Loss: 3.681224822998047 | BCE Loss: 1.0230015516281128\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.755023002624512 | KNN Loss: 3.70171856880188 | BCE Loss: 1.0533044338226318\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.725216388702393 | KNN Loss: 3.70440936088562 | BCE Loss: 1.0208070278167725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.719339370727539 | KNN Loss: 3.6985154151916504 | BCE Loss: 1.0208237171173096\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.711606502532959 | KNN Loss: 3.7011332511901855 | BCE Loss: 1.0104732513427734\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.741427421569824 | KNN Loss: 3.7226762771606445 | BCE Loss: 1.0187511444091797\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.689827919006348 | KNN Loss: 3.6908771991729736 | BCE Loss: 0.9989506602287292\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.698273658752441 | KNN Loss: 3.6775872707366943 | BCE Loss: 1.0206865072250366\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.690425872802734 | KNN Loss: 3.721332311630249 | BCE Loss: 0.9690937995910645\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.759272575378418 | KNN Loss: 3.7386832237243652 | BCE Loss: 1.0205894708633423\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.714505195617676 | KNN Loss: 3.6979339122772217 | BCE Loss: 1.016571283340454\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.729985237121582 | KNN Loss: 3.696258306503296 | BCE Loss: 1.033726692199707\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.727475166320801 | KNN Loss: 3.7196900844573975 | BCE Loss: 1.0077848434448242\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.691031455993652 | KNN Loss: 3.679037094116211 | BCE Loss: 1.0119946002960205\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.698400974273682 | KNN Loss: 3.6883506774902344 | BCE Loss: 1.0100501775741577\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.745710372924805 | KNN Loss: 3.7229418754577637 | BCE Loss: 1.022768497467041\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.7215166091918945 | KNN Loss: 3.702392816543579 | BCE Loss: 1.0191240310668945\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.7062177658081055 | KNN Loss: 3.6965091228485107 | BCE Loss: 1.0097084045410156\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.766659736633301 | KNN Loss: 3.7257611751556396 | BCE Loss: 1.040898323059082\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.731559753417969 | KNN Loss: 3.7001495361328125 | BCE Loss: 1.0314099788665771\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.763598442077637 | KNN Loss: 3.726397752761841 | BCE Loss: 1.037200689315796\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.796051502227783 | KNN Loss: 3.738450527191162 | BCE Loss: 1.0576010942459106\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.74212646484375 | KNN Loss: 3.7109413146972656 | BCE Loss: 1.0311849117279053\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.754368782043457 | KNN Loss: 3.717869520187378 | BCE Loss: 1.0364995002746582\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.719038963317871 | KNN Loss: 3.6854307651519775 | BCE Loss: 1.0336079597473145\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.740131378173828 | KNN Loss: 3.707460880279541 | BCE Loss: 1.0326707363128662\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.7716851234436035 | KNN Loss: 3.7337429523468018 | BCE Loss: 1.0379421710968018\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.750586032867432 | KNN Loss: 3.728092670440674 | BCE Loss: 1.0224933624267578\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.753205299377441 | KNN Loss: 3.7018492221832275 | BCE Loss: 1.0513560771942139\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.778354644775391 | KNN Loss: 3.734829902648926 | BCE Loss: 1.043524980545044\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.676363468170166 | KNN Loss: 3.6806278228759766 | BCE Loss: 0.9957356452941895\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.725953102111816 | KNN Loss: 3.7038421630859375 | BCE Loss: 1.022110939025879\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.786780834197998 | KNN Loss: 3.7526004314422607 | BCE Loss: 1.0341804027557373\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.725019454956055 | KNN Loss: 3.7268192768096924 | BCE Loss: 0.9982001781463623\n",
      "Epoch   145: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.728645324707031 | KNN Loss: 3.705853223800659 | BCE Loss: 1.022791862487793\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.714967727661133 | KNN Loss: 3.7000789642333984 | BCE Loss: 1.0148890018463135\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.732728004455566 | KNN Loss: 3.7151987552642822 | BCE Loss: 1.017529010772705\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.755871295928955 | KNN Loss: 3.7332189083099365 | BCE Loss: 1.0226523876190186\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.698282718658447 | KNN Loss: 3.725977897644043 | BCE Loss: 0.9723047614097595\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.705944061279297 | KNN Loss: 3.6960911750793457 | BCE Loss: 1.009852647781372\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.809067249298096 | KNN Loss: 3.7655370235443115 | BCE Loss: 1.0435303449630737\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.7428507804870605 | KNN Loss: 3.7154572010040283 | BCE Loss: 1.0273935794830322\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.68508243560791 | KNN Loss: 3.670515537261963 | BCE Loss: 1.0145666599273682\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.739289283752441 | KNN Loss: 3.695260524749756 | BCE Loss: 1.0440289974212646\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.709856033325195 | KNN Loss: 3.6953117847442627 | BCE Loss: 1.0145443677902222\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.6917829513549805 | KNN Loss: 3.697361946105957 | BCE Loss: 0.9944210052490234\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.721707344055176 | KNN Loss: 3.718371629714966 | BCE Loss: 1.0033355951309204\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.785604476928711 | KNN Loss: 3.7104368209838867 | BCE Loss: 1.0751675367355347\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.713377952575684 | KNN Loss: 3.6967215538024902 | BCE Loss: 1.0166566371917725\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.711334228515625 | KNN Loss: 3.690880537033081 | BCE Loss: 1.020453691482544\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.7209014892578125 | KNN Loss: 3.6909871101379395 | BCE Loss: 1.029914379119873\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.728569030761719 | KNN Loss: 3.7228004932403564 | BCE Loss: 1.0057685375213623\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.754786491394043 | KNN Loss: 3.7460618019104004 | BCE Loss: 1.0087249279022217\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.709433555603027 | KNN Loss: 3.700622797012329 | BCE Loss: 1.0088107585906982\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.698668003082275 | KNN Loss: 3.6906964778900146 | BCE Loss: 1.0079715251922607\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.807168006896973 | KNN Loss: 3.7276852130889893 | BCE Loss: 1.0794825553894043\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.704763412475586 | KNN Loss: 3.6989076137542725 | BCE Loss: 1.0058560371398926\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.748303413391113 | KNN Loss: 3.706556558609009 | BCE Loss: 1.041746973991394\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.7175798416137695 | KNN Loss: 3.6966638565063477 | BCE Loss: 1.0209157466888428\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.720072269439697 | KNN Loss: 3.710364580154419 | BCE Loss: 1.0097076892852783\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.728476524353027 | KNN Loss: 3.7128171920776367 | BCE Loss: 1.0156594514846802\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.755159378051758 | KNN Loss: 3.728337287902832 | BCE Loss: 1.0268219709396362\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.7066144943237305 | KNN Loss: 3.709352970123291 | BCE Loss: 0.9972615242004395\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.757818698883057 | KNN Loss: 3.718724250793457 | BCE Loss: 1.0390944480895996\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.780694007873535 | KNN Loss: 3.7070558071136475 | BCE Loss: 1.0736380815505981\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.729399681091309 | KNN Loss: 3.688734531402588 | BCE Loss: 1.0406649112701416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.756929397583008 | KNN Loss: 3.7393643856048584 | BCE Loss: 1.0175647735595703\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.754549980163574 | KNN Loss: 3.719489097595215 | BCE Loss: 1.0350606441497803\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.734681129455566 | KNN Loss: 3.716872453689575 | BCE Loss: 1.017808437347412\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.71866512298584 | KNN Loss: 3.683283567428589 | BCE Loss: 1.0353814363479614\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.7091569900512695 | KNN Loss: 3.7152724266052246 | BCE Loss: 0.9938847422599792\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.7798943519592285 | KNN Loss: 3.7471213340759277 | BCE Loss: 1.0327730178833008\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.720847129821777 | KNN Loss: 3.71753191947937 | BCE Loss: 1.0033149719238281\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.769082069396973 | KNN Loss: 3.740628719329834 | BCE Loss: 1.0284533500671387\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.793918609619141 | KNN Loss: 3.747382164001465 | BCE Loss: 1.0465365648269653\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.762117385864258 | KNN Loss: 3.711359739303589 | BCE Loss: 1.0507577657699585\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.691401481628418 | KNN Loss: 3.666814088821411 | BCE Loss: 1.024587631225586\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.716992378234863 | KNN Loss: 3.6843044757843018 | BCE Loss: 1.0326881408691406\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.733092784881592 | KNN Loss: 3.711914300918579 | BCE Loss: 1.0211783647537231\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.706323623657227 | KNN Loss: 3.697568416595459 | BCE Loss: 1.0087549686431885\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.7311553955078125 | KNN Loss: 3.7014431953430176 | BCE Loss: 1.0297119617462158\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.734828472137451 | KNN Loss: 3.7093498706817627 | BCE Loss: 1.0254786014556885\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.7708587646484375 | KNN Loss: 3.7363829612731934 | BCE Loss: 1.034475564956665\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.739727020263672 | KNN Loss: 3.7014410495758057 | BCE Loss: 1.0382862091064453\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.686802387237549 | KNN Loss: 3.6954286098480225 | BCE Loss: 0.9913737177848816\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.70877742767334 | KNN Loss: 3.7132463455200195 | BCE Loss: 0.9955313205718994\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.702287673950195 | KNN Loss: 3.682981491088867 | BCE Loss: 1.0193063020706177\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.714113235473633 | KNN Loss: 3.6873250007629395 | BCE Loss: 1.0267882347106934\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.711982727050781 | KNN Loss: 3.690541982650757 | BCE Loss: 1.0214406251907349\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.723424434661865 | KNN Loss: 3.717198371887207 | BCE Loss: 1.0062260627746582\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.761853218078613 | KNN Loss: 3.701897621154785 | BCE Loss: 1.059955358505249\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.736478805541992 | KNN Loss: 3.7182273864746094 | BCE Loss: 1.018251657485962\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.744974613189697 | KNN Loss: 3.7191803455352783 | BCE Loss: 1.0257941484451294\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.703419208526611 | KNN Loss: 3.723846673965454 | BCE Loss: 0.9795726537704468\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.7256903648376465 | KNN Loss: 3.7219631671905518 | BCE Loss: 1.0037270784378052\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.755695343017578 | KNN Loss: 3.7124526500701904 | BCE Loss: 1.0432429313659668\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.720904350280762 | KNN Loss: 3.7051031589508057 | BCE Loss: 1.0158014297485352\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.728703498840332 | KNN Loss: 3.690580129623413 | BCE Loss: 1.0381234884262085\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.707457065582275 | KNN Loss: 3.6915106773376465 | BCE Loss: 1.0159462690353394\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.733046531677246 | KNN Loss: 3.7029590606689453 | BCE Loss: 1.0300875902175903\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.746987342834473 | KNN Loss: 3.7082924842834473 | BCE Loss: 1.0386948585510254\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.69438362121582 | KNN Loss: 3.678149938583374 | BCE Loss: 1.0162338018417358\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.746615886688232 | KNN Loss: 3.7048192024230957 | BCE Loss: 1.0417965650558472\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.723297595977783 | KNN Loss: 3.7014520168304443 | BCE Loss: 1.0218455791473389\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.738391876220703 | KNN Loss: 3.7081847190856934 | BCE Loss: 1.0302069187164307\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.759877681732178 | KNN Loss: 3.7129616737365723 | BCE Loss: 1.046916127204895\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.729989051818848 | KNN Loss: 3.705179214477539 | BCE Loss: 1.0248095989227295\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.718924045562744 | KNN Loss: 3.7141740322113037 | BCE Loss: 1.0047498941421509\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.694248199462891 | KNN Loss: 3.708592653274536 | BCE Loss: 0.9856553077697754\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.77010440826416 | KNN Loss: 3.7449541091918945 | BCE Loss: 1.0251502990722656\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.728776931762695 | KNN Loss: 3.7277743816375732 | BCE Loss: 1.0010024309158325\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.759666442871094 | KNN Loss: 3.70967173576355 | BCE Loss: 1.049994707107544\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.699705123901367 | KNN Loss: 3.6913414001464844 | BCE Loss: 1.0083638429641724\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.741998672485352 | KNN Loss: 3.6901869773864746 | BCE Loss: 1.0518114566802979\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.799284934997559 | KNN Loss: 3.7844200134277344 | BCE Loss: 1.0148649215698242\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.722047805786133 | KNN Loss: 3.6885414123535156 | BCE Loss: 1.033506155014038\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.698810577392578 | KNN Loss: 3.6862242221832275 | BCE Loss: 1.0125861167907715\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.725459098815918 | KNN Loss: 3.7075178623199463 | BCE Loss: 1.0179413557052612\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.783624649047852 | KNN Loss: 3.7384896278381348 | BCE Loss: 1.045135259628296\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.729863166809082 | KNN Loss: 3.692324161529541 | BCE Loss: 1.037539005279541\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.7117600440979 | KNN Loss: 3.721917152404785 | BCE Loss: 0.9898427128791809\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.6838698387146 | KNN Loss: 3.6874544620513916 | BCE Loss: 0.996415376663208\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.773881912231445 | KNN Loss: 3.7612147331237793 | BCE Loss: 1.0126674175262451\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.787100791931152 | KNN Loss: 3.739856004714966 | BCE Loss: 1.0472445487976074\n",
      "Epoch   160: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.733933925628662 | KNN Loss: 3.700385332107544 | BCE Loss: 1.0335485935211182\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.731963157653809 | KNN Loss: 3.7233774662017822 | BCE Loss: 1.0085854530334473\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.739093780517578 | KNN Loss: 3.7247111797332764 | BCE Loss: 1.0143823623657227\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.6914544105529785 | KNN Loss: 3.6957168579101562 | BCE Loss: 0.9957374334335327\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.783331871032715 | KNN Loss: 3.745882511138916 | BCE Loss: 1.0374494791030884\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.701567649841309 | KNN Loss: 3.692683458328247 | BCE Loss: 1.0088841915130615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.753200531005859 | KNN Loss: 3.727186918258667 | BCE Loss: 1.0260136127471924\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.717901706695557 | KNN Loss: 3.688567876815796 | BCE Loss: 1.0293339490890503\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.754419326782227 | KNN Loss: 3.728102445602417 | BCE Loss: 1.02631676197052\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.766294002532959 | KNN Loss: 3.7201483249664307 | BCE Loss: 1.0461457967758179\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.765169143676758 | KNN Loss: 3.7180089950561523 | BCE Loss: 1.047160267829895\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.73515510559082 | KNN Loss: 3.7042880058288574 | BCE Loss: 1.0308669805526733\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.720388889312744 | KNN Loss: 3.6718897819519043 | BCE Loss: 1.0484991073608398\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.717018127441406 | KNN Loss: 3.710132122039795 | BCE Loss: 1.0068860054016113\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.731673717498779 | KNN Loss: 3.7433712482452393 | BCE Loss: 0.9883025884628296\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.723539352416992 | KNN Loss: 3.728062629699707 | BCE Loss: 0.9954767823219299\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.721569061279297 | KNN Loss: 3.7100677490234375 | BCE Loss: 1.0115015506744385\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.704291343688965 | KNN Loss: 3.676563262939453 | BCE Loss: 1.0277281999588013\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.743769645690918 | KNN Loss: 3.7066738605499268 | BCE Loss: 1.0370957851409912\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.72409725189209 | KNN Loss: 3.699852466583252 | BCE Loss: 1.0242445468902588\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.737668037414551 | KNN Loss: 3.7177419662475586 | BCE Loss: 1.0199263095855713\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.719038009643555 | KNN Loss: 3.698389768600464 | BCE Loss: 1.0206480026245117\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.721020221710205 | KNN Loss: 3.6824827194213867 | BCE Loss: 1.0385375022888184\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.667970657348633 | KNN Loss: 3.670525074005127 | BCE Loss: 0.9974454641342163\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.751162528991699 | KNN Loss: 3.7191545963287354 | BCE Loss: 1.032008171081543\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.686707496643066 | KNN Loss: 3.675821542739868 | BCE Loss: 1.0108859539031982\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.680874824523926 | KNN Loss: 3.676560401916504 | BCE Loss: 1.0043141841888428\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.725249767303467 | KNN Loss: 3.7009880542755127 | BCE Loss: 1.024261713027954\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.784520626068115 | KNN Loss: 3.733963966369629 | BCE Loss: 1.0505567789077759\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.705188274383545 | KNN Loss: 3.688769578933716 | BCE Loss: 1.0164188146591187\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.7438578605651855 | KNN Loss: 3.7169814109802246 | BCE Loss: 1.026876449584961\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.7062859535217285 | KNN Loss: 3.6945300102233887 | BCE Loss: 1.0117560625076294\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.748983383178711 | KNN Loss: 3.7149298191070557 | BCE Loss: 1.0340538024902344\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.721752643585205 | KNN Loss: 3.687842607498169 | BCE Loss: 1.0339100360870361\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.759291648864746 | KNN Loss: 3.7245841026306152 | BCE Loss: 1.0347073078155518\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.733310699462891 | KNN Loss: 3.7268762588500977 | BCE Loss: 1.006434440612793\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.727999687194824 | KNN Loss: 3.695805311203003 | BCE Loss: 1.0321941375732422\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.682295799255371 | KNN Loss: 3.686232328414917 | BCE Loss: 0.9960636496543884\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.757286548614502 | KNN Loss: 3.70536732673645 | BCE Loss: 1.0519191026687622\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.7719340324401855 | KNN Loss: 3.7313358783721924 | BCE Loss: 1.0405981540679932\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.755216598510742 | KNN Loss: 3.7172200679779053 | BCE Loss: 1.037996768951416\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.739084720611572 | KNN Loss: 3.711477279663086 | BCE Loss: 1.0276074409484863\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.776590347290039 | KNN Loss: 3.71738338470459 | BCE Loss: 1.0592069625854492\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.702174186706543 | KNN Loss: 3.6903223991394043 | BCE Loss: 1.0118517875671387\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.725682735443115 | KNN Loss: 3.709712505340576 | BCE Loss: 1.015970230102539\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.697813987731934 | KNN Loss: 3.703394889831543 | BCE Loss: 0.994419276714325\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.725388526916504 | KNN Loss: 3.710754871368408 | BCE Loss: 1.0146336555480957\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.687973499298096 | KNN Loss: 3.6965081691741943 | BCE Loss: 0.991465151309967\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.72044563293457 | KNN Loss: 3.6802151203155518 | BCE Loss: 1.0402307510375977\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.711304664611816 | KNN Loss: 3.684692859649658 | BCE Loss: 1.0266118049621582\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.744812488555908 | KNN Loss: 3.725440502166748 | BCE Loss: 1.0193719863891602\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.740363597869873 | KNN Loss: 3.716482400894165 | BCE Loss: 1.023881196975708\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.7636003494262695 | KNN Loss: 3.7224249839782715 | BCE Loss: 1.041175127029419\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.702977180480957 | KNN Loss: 3.707094192504883 | BCE Loss: 0.9958827495574951\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.676486968994141 | KNN Loss: 3.6815850734710693 | BCE Loss: 0.9949020147323608\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.733936309814453 | KNN Loss: 3.7213289737701416 | BCE Loss: 1.0126075744628906\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.716910362243652 | KNN Loss: 3.7075650691986084 | BCE Loss: 1.009345293045044\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.711820602416992 | KNN Loss: 3.7035140991210938 | BCE Loss: 1.0083062648773193\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.755154609680176 | KNN Loss: 3.7358036041259766 | BCE Loss: 1.0193507671356201\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.753535270690918 | KNN Loss: 3.7235374450683594 | BCE Loss: 1.0299975872039795\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.764315128326416 | KNN Loss: 3.746434450149536 | BCE Loss: 1.0178806781768799\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.781214237213135 | KNN Loss: 3.719214677810669 | BCE Loss: 1.0619996786117554\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.68729305267334 | KNN Loss: 3.6988093852996826 | BCE Loss: 0.9884837865829468\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.709452152252197 | KNN Loss: 3.700054883956909 | BCE Loss: 1.009397268295288\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.727873802185059 | KNN Loss: 3.6913304328918457 | BCE Loss: 1.0365431308746338\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.710392951965332 | KNN Loss: 3.714848518371582 | BCE Loss: 0.99554443359375\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.704102516174316 | KNN Loss: 3.685917377471924 | BCE Loss: 1.0181849002838135\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.7343034744262695 | KNN Loss: 3.728214740753174 | BCE Loss: 1.0060887336730957\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.748194694519043 | KNN Loss: 3.7218613624572754 | BCE Loss: 1.0263335704803467\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.671290874481201 | KNN Loss: 3.677879810333252 | BCE Loss: 0.9934110045433044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.780730724334717 | KNN Loss: 3.729328155517578 | BCE Loss: 1.0514025688171387\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.683350086212158 | KNN Loss: 3.679535388946533 | BCE Loss: 1.0038148164749146\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.708678245544434 | KNN Loss: 3.717698335647583 | BCE Loss: 0.9909801483154297\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.731808662414551 | KNN Loss: 3.6848747730255127 | BCE Loss: 1.0469340085983276\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.716100215911865 | KNN Loss: 3.6944806575775146 | BCE Loss: 1.0216196775436401\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.724859237670898 | KNN Loss: 3.6820766925811768 | BCE Loss: 1.0427826642990112\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.738103866577148 | KNN Loss: 3.7123820781707764 | BCE Loss: 1.025721549987793\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.715538501739502 | KNN Loss: 3.7161967754364014 | BCE Loss: 0.9993417859077454\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.737636089324951 | KNN Loss: 3.7039318084716797 | BCE Loss: 1.0337042808532715\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.773970603942871 | KNN Loss: 3.75836181640625 | BCE Loss: 1.015608787536621\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.7427191734313965 | KNN Loss: 3.7345452308654785 | BCE Loss: 1.008173942565918\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.710113525390625 | KNN Loss: 3.6933822631835938 | BCE Loss: 1.0167310237884521\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.698136329650879 | KNN Loss: 3.678529977798462 | BCE Loss: 1.019606590270996\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.729068279266357 | KNN Loss: 3.7176382541656494 | BCE Loss: 1.011430025100708\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.709410667419434 | KNN Loss: 3.687669038772583 | BCE Loss: 1.0217417478561401\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.716978073120117 | KNN Loss: 3.6941235065460205 | BCE Loss: 1.0228548049926758\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.747715950012207 | KNN Loss: 3.7042949199676514 | BCE Loss: 1.0434211492538452\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.698122978210449 | KNN Loss: 3.679567575454712 | BCE Loss: 1.0185556411743164\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.732781410217285 | KNN Loss: 3.711982250213623 | BCE Loss: 1.020798921585083\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.789035320281982 | KNN Loss: 3.7677996158599854 | BCE Loss: 1.0212358236312866\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.724919319152832 | KNN Loss: 3.686178684234619 | BCE Loss: 1.038740873336792\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.717288017272949 | KNN Loss: 3.6942079067230225 | BCE Loss: 1.0230802297592163\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.746086597442627 | KNN Loss: 3.731879234313965 | BCE Loss: 1.0142074823379517\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.764194011688232 | KNN Loss: 3.7367005348205566 | BCE Loss: 1.0274935960769653\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.753340721130371 | KNN Loss: 3.7276248931884766 | BCE Loss: 1.025715708732605\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.761665344238281 | KNN Loss: 3.7390620708465576 | BCE Loss: 1.0226030349731445\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.71114444732666 | KNN Loss: 3.715179920196533 | BCE Loss: 0.9959644079208374\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.734493255615234 | KNN Loss: 3.729032039642334 | BCE Loss: 1.0054609775543213\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.756129264831543 | KNN Loss: 3.720384359359741 | BCE Loss: 1.0357451438903809\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.706715106964111 | KNN Loss: 3.6978542804718018 | BCE Loss: 1.0088609457015991\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.732996463775635 | KNN Loss: 3.71576189994812 | BCE Loss: 1.017234444618225\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.685465335845947 | KNN Loss: 3.6935532093048096 | BCE Loss: 0.9919120073318481\n",
      "Epoch   177: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.736752986907959 | KNN Loss: 3.731020212173462 | BCE Loss: 1.0057326555252075\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.735993385314941 | KNN Loss: 3.708716869354248 | BCE Loss: 1.0272762775421143\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.749932289123535 | KNN Loss: 3.714900493621826 | BCE Loss: 1.035032033920288\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.690026760101318 | KNN Loss: 3.674844980239868 | BCE Loss: 1.0151817798614502\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.710281848907471 | KNN Loss: 3.699514389038086 | BCE Loss: 1.0107674598693848\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.731932640075684 | KNN Loss: 3.7238221168518066 | BCE Loss: 1.008110523223877\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.730223178863525 | KNN Loss: 3.713547945022583 | BCE Loss: 1.0166751146316528\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.7392048835754395 | KNN Loss: 3.7056031227111816 | BCE Loss: 1.0336016416549683\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.713399410247803 | KNN Loss: 3.696993827819824 | BCE Loss: 1.016405463218689\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.740013122558594 | KNN Loss: 3.709092855453491 | BCE Loss: 1.0309205055236816\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.763904571533203 | KNN Loss: 3.738729953765869 | BCE Loss: 1.0251743793487549\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.695697784423828 | KNN Loss: 3.68698787689209 | BCE Loss: 1.0087100267410278\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.714988708496094 | KNN Loss: 3.700376272201538 | BCE Loss: 1.0146124362945557\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.726917743682861 | KNN Loss: 3.699812650680542 | BCE Loss: 1.0271052122116089\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.690584182739258 | KNN Loss: 3.674018621444702 | BCE Loss: 1.0165657997131348\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.666328430175781 | KNN Loss: 3.685966968536377 | BCE Loss: 0.9803614020347595\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.758909702301025 | KNN Loss: 3.7399179935455322 | BCE Loss: 1.0189917087554932\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.782634735107422 | KNN Loss: 3.741804361343384 | BCE Loss: 1.040830373764038\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.710884094238281 | KNN Loss: 3.6882307529449463 | BCE Loss: 1.022653341293335\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.678549289703369 | KNN Loss: 3.675881862640381 | BCE Loss: 1.0026674270629883\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.759974479675293 | KNN Loss: 3.724604845046997 | BCE Loss: 1.035369634628296\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.709929466247559 | KNN Loss: 3.698432207107544 | BCE Loss: 1.0114970207214355\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.70622444152832 | KNN Loss: 3.7015297412872314 | BCE Loss: 1.0046944618225098\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.761407852172852 | KNN Loss: 3.7351326942443848 | BCE Loss: 1.0262752771377563\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.720692157745361 | KNN Loss: 3.7244820594787598 | BCE Loss: 0.9962100982666016\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.725836753845215 | KNN Loss: 3.706874132156372 | BCE Loss: 1.0189623832702637\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.69924259185791 | KNN Loss: 3.682093381881714 | BCE Loss: 1.0171489715576172\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.71684455871582 | KNN Loss: 3.692324638366699 | BCE Loss: 1.0245200395584106\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.71088981628418 | KNN Loss: 3.70615291595459 | BCE Loss: 1.0047369003295898\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.737033843994141 | KNN Loss: 3.7317938804626465 | BCE Loss: 1.005239725112915\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.72869348526001 | KNN Loss: 3.7158312797546387 | BCE Loss: 1.0128620862960815\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.728973388671875 | KNN Loss: 3.7269840240478516 | BCE Loss: 1.0019893646240234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.716207027435303 | KNN Loss: 3.7061922550201416 | BCE Loss: 1.0100146532058716\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.746191024780273 | KNN Loss: 3.7029364109039307 | BCE Loss: 1.0432543754577637\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.789287567138672 | KNN Loss: 3.7146055698394775 | BCE Loss: 1.0746822357177734\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.7031474113464355 | KNN Loss: 3.6847736835479736 | BCE Loss: 1.0183738470077515\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.7702484130859375 | KNN Loss: 3.7433109283447266 | BCE Loss: 1.026937484741211\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.699399471282959 | KNN Loss: 3.6949753761291504 | BCE Loss: 1.0044242143630981\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.704554080963135 | KNN Loss: 3.7164793014526367 | BCE Loss: 0.9880748987197876\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.745429515838623 | KNN Loss: 3.7175729274749756 | BCE Loss: 1.027856469154358\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.726168155670166 | KNN Loss: 3.706906318664551 | BCE Loss: 1.0192617177963257\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.775594711303711 | KNN Loss: 3.729311466217041 | BCE Loss: 1.046283483505249\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.709362983703613 | KNN Loss: 3.7032411098480225 | BCE Loss: 1.0061216354370117\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.7143354415893555 | KNN Loss: 3.701962471008301 | BCE Loss: 1.0123732089996338\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.735800743103027 | KNN Loss: 3.711665630340576 | BCE Loss: 1.0241353511810303\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.724569320678711 | KNN Loss: 3.707686424255371 | BCE Loss: 1.0168827772140503\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.799410820007324 | KNN Loss: 3.747988700866699 | BCE Loss: 1.051421880722046\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.760892391204834 | KNN Loss: 3.7347586154937744 | BCE Loss: 1.0261338949203491\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.762853145599365 | KNN Loss: 3.7284815311431885 | BCE Loss: 1.0343714952468872\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.715251445770264 | KNN Loss: 3.674834966659546 | BCE Loss: 1.0404164791107178\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.713530540466309 | KNN Loss: 3.6998322010040283 | BCE Loss: 1.0136983394622803\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.750820159912109 | KNN Loss: 3.723228693008423 | BCE Loss: 1.0275912284851074\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.741156578063965 | KNN Loss: 3.705282211303711 | BCE Loss: 1.0358742475509644\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.705226898193359 | KNN Loss: 3.6929774284362793 | BCE Loss: 1.0122493505477905\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.761541366577148 | KNN Loss: 3.7145097255706787 | BCE Loss: 1.0470316410064697\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.752740383148193 | KNN Loss: 3.744382381439209 | BCE Loss: 1.0083580017089844\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.813820838928223 | KNN Loss: 3.7676327228546143 | BCE Loss: 1.0461878776550293\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.750842094421387 | KNN Loss: 3.7291905879974365 | BCE Loss: 1.0216517448425293\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.740029335021973 | KNN Loss: 3.7187671661376953 | BCE Loss: 1.0212624073028564\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.703800678253174 | KNN Loss: 3.6867926120758057 | BCE Loss: 1.0170081853866577\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.732579231262207 | KNN Loss: 3.702346086502075 | BCE Loss: 1.0302329063415527\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.7213053703308105 | KNN Loss: 3.7190048694610596 | BCE Loss: 1.0023006200790405\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.751655578613281 | KNN Loss: 3.718052864074707 | BCE Loss: 1.0336029529571533\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.778595924377441 | KNN Loss: 3.7386269569396973 | BCE Loss: 1.0399692058563232\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.742612838745117 | KNN Loss: 3.7399837970733643 | BCE Loss: 1.002629280090332\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.784749984741211 | KNN Loss: 3.724731683731079 | BCE Loss: 1.060018539428711\n",
      "Epoch   188: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.718120574951172 | KNN Loss: 3.694751501083374 | BCE Loss: 1.0233690738677979\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.739819526672363 | KNN Loss: 3.7127418518066406 | BCE Loss: 1.0270779132843018\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.763084411621094 | KNN Loss: 3.731494665145874 | BCE Loss: 1.0315897464752197\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.716870307922363 | KNN Loss: 3.7117104530334473 | BCE Loss: 1.005159616470337\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.704156875610352 | KNN Loss: 3.71041202545166 | BCE Loss: 0.9937446117401123\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.733204364776611 | KNN Loss: 3.696342945098877 | BCE Loss: 1.0368614196777344\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.756401062011719 | KNN Loss: 3.7091376781463623 | BCE Loss: 1.047263503074646\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.692289352416992 | KNN Loss: 3.6869912147521973 | BCE Loss: 1.005298376083374\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.6635661125183105 | KNN Loss: 3.6689984798431396 | BCE Loss: 0.9945675134658813\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.733695030212402 | KNN Loss: 3.7292160987854004 | BCE Loss: 1.0044790506362915\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.74354362487793 | KNN Loss: 3.731382131576538 | BCE Loss: 1.0121617317199707\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.699269771575928 | KNN Loss: 3.7053842544555664 | BCE Loss: 0.9938854575157166\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.731754779815674 | KNN Loss: 3.7050044536590576 | BCE Loss: 1.0267504453659058\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.758889198303223 | KNN Loss: 3.738481044769287 | BCE Loss: 1.0204081535339355\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.719989776611328 | KNN Loss: 3.7142605781555176 | BCE Loss: 1.005729079246521\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.735156059265137 | KNN Loss: 3.701878786087036 | BCE Loss: 1.0332775115966797\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.72930908203125 | KNN Loss: 3.720958948135376 | BCE Loss: 1.008349895477295\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.692424774169922 | KNN Loss: 3.6826443672180176 | BCE Loss: 1.0097806453704834\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.752496242523193 | KNN Loss: 3.7214651107788086 | BCE Loss: 1.0310311317443848\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.761303901672363 | KNN Loss: 3.7185957431793213 | BCE Loss: 1.042708396911621\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.680710792541504 | KNN Loss: 3.6965267658233643 | BCE Loss: 0.9841840863227844\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.704519271850586 | KNN Loss: 3.688480854034424 | BCE Loss: 1.0160382986068726\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.719663143157959 | KNN Loss: 3.6902894973754883 | BCE Loss: 1.0293737649917603\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.757130146026611 | KNN Loss: 3.7618634700775146 | BCE Loss: 0.9952667951583862\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.697263240814209 | KNN Loss: 3.6928162574768066 | BCE Loss: 1.004447102546692\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.721049785614014 | KNN Loss: 3.6993770599365234 | BCE Loss: 1.0216728448867798\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.70289421081543 | KNN Loss: 3.706169366836548 | BCE Loss: 0.9967246055603027\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.710013389587402 | KNN Loss: 3.712008237838745 | BCE Loss: 0.9980053901672363\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.7133941650390625 | KNN Loss: 3.695833921432495 | BCE Loss: 1.0175601243972778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.713300704956055 | KNN Loss: 3.688751697540283 | BCE Loss: 1.0245490074157715\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.723952293395996 | KNN Loss: 3.714909076690674 | BCE Loss: 1.0090430974960327\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.708863258361816 | KNN Loss: 3.7277944087982178 | BCE Loss: 0.981069028377533\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.756234645843506 | KNN Loss: 3.720839262008667 | BCE Loss: 1.0353952646255493\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.751707077026367 | KNN Loss: 3.7351186275482178 | BCE Loss: 1.0165886878967285\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.728169918060303 | KNN Loss: 3.7104337215423584 | BCE Loss: 1.0177360773086548\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.722468376159668 | KNN Loss: 3.699815034866333 | BCE Loss: 1.022653341293335\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.726489543914795 | KNN Loss: 3.7234554290771484 | BCE Loss: 1.003033995628357\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.765325546264648 | KNN Loss: 3.7482104301452637 | BCE Loss: 1.0171152353286743\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.695046901702881 | KNN Loss: 3.690094232559204 | BCE Loss: 1.0049526691436768\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.734023094177246 | KNN Loss: 3.702882766723633 | BCE Loss: 1.0311400890350342\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.748968601226807 | KNN Loss: 3.726405143737793 | BCE Loss: 1.0225633382797241\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.726210594177246 | KNN Loss: 3.7145698070526123 | BCE Loss: 1.011641025543213\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.754368782043457 | KNN Loss: 3.724843978881836 | BCE Loss: 1.029524803161621\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.755221366882324 | KNN Loss: 3.743345260620117 | BCE Loss: 1.0118759870529175\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.732659339904785 | KNN Loss: 3.7354140281677246 | BCE Loss: 0.997245192527771\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.744684219360352 | KNN Loss: 3.7288525104522705 | BCE Loss: 1.015831708908081\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.715896129608154 | KNN Loss: 3.686922788619995 | BCE Loss: 1.0289733409881592\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.734116077423096 | KNN Loss: 3.7171316146850586 | BCE Loss: 1.0169845819473267\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.713374137878418 | KNN Loss: 3.7010886669158936 | BCE Loss: 1.012285590171814\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.720037460327148 | KNN Loss: 3.692028045654297 | BCE Loss: 1.0280094146728516\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.699374675750732 | KNN Loss: 3.697666645050049 | BCE Loss: 1.0017080307006836\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.764178276062012 | KNN Loss: 3.7386908531188965 | BCE Loss: 1.0254876613616943\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.780834674835205 | KNN Loss: 3.741710901260376 | BCE Loss: 1.0391238927841187\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.736905097961426 | KNN Loss: 3.727494955062866 | BCE Loss: 1.0094102621078491\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.770488739013672 | KNN Loss: 3.7287585735321045 | BCE Loss: 1.0417301654815674\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.742331504821777 | KNN Loss: 3.723778486251831 | BCE Loss: 1.0185532569885254\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.702688217163086 | KNN Loss: 3.6844797134399414 | BCE Loss: 1.0182087421417236\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.750682830810547 | KNN Loss: 3.7092559337615967 | BCE Loss: 1.0414268970489502\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.724774360656738 | KNN Loss: 3.7307534217834473 | BCE Loss: 0.9940208196640015\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.744524955749512 | KNN Loss: 3.7011659145355225 | BCE Loss: 1.0433592796325684\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.687236785888672 | KNN Loss: 3.7013351917266846 | BCE Loss: 0.9859017729759216\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.740438938140869 | KNN Loss: 3.6971688270568848 | BCE Loss: 1.0432701110839844\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.740570068359375 | KNN Loss: 3.708561420440674 | BCE Loss: 1.0320088863372803\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.729935646057129 | KNN Loss: 3.735553026199341 | BCE Loss: 0.9943827986717224\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.761077880859375 | KNN Loss: 3.7236242294311523 | BCE Loss: 1.0374537706375122\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.769275188446045 | KNN Loss: 3.727224588394165 | BCE Loss: 1.0420504808425903\n",
      "Epoch   199: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.713168144226074 | KNN Loss: 3.6934633255004883 | BCE Loss: 1.019705057144165\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.777478218078613 | KNN Loss: 3.7429420948028564 | BCE Loss: 1.0345361232757568\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.802471160888672 | KNN Loss: 3.7633256912231445 | BCE Loss: 1.0391452312469482\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.749504089355469 | KNN Loss: 3.7342662811279297 | BCE Loss: 1.0152376890182495\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.752514362335205 | KNN Loss: 3.7321290969848633 | BCE Loss: 1.0203851461410522\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.741389274597168 | KNN Loss: 3.703190326690674 | BCE Loss: 1.0381989479064941\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.739434242248535 | KNN Loss: 3.719510793685913 | BCE Loss: 1.019923448562622\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.684562683105469 | KNN Loss: 3.6623504161834717 | BCE Loss: 1.022212266921997\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.736696243286133 | KNN Loss: 3.7136447429656982 | BCE Loss: 1.023051381111145\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.715781211853027 | KNN Loss: 3.6974759101867676 | BCE Loss: 1.0183050632476807\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.71208381652832 | KNN Loss: 3.692960023880005 | BCE Loss: 1.0191237926483154\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.734987258911133 | KNN Loss: 3.7136383056640625 | BCE Loss: 1.0213489532470703\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.742420196533203 | KNN Loss: 3.715311288833618 | BCE Loss: 1.0271090269088745\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.744927406311035 | KNN Loss: 3.7071280479431152 | BCE Loss: 1.0377994775772095\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.689296245574951 | KNN Loss: 3.697819948196411 | BCE Loss: 0.9914761781692505\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.738944053649902 | KNN Loss: 3.7134742736816406 | BCE Loss: 1.0254695415496826\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.7463836669921875 | KNN Loss: 3.7236881256103516 | BCE Loss: 1.0226953029632568\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.7313618659973145 | KNN Loss: 3.7125377655029297 | BCE Loss: 1.0188241004943848\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.767327785491943 | KNN Loss: 3.711745262145996 | BCE Loss: 1.0555825233459473\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.697449684143066 | KNN Loss: 3.694321870803833 | BCE Loss: 1.0031275749206543\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.74277400970459 | KNN Loss: 3.741698980331421 | BCE Loss: 1.0010747909545898\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.708374977111816 | KNN Loss: 3.6716701984405518 | BCE Loss: 1.0367045402526855\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.768210411071777 | KNN Loss: 3.711606025695801 | BCE Loss: 1.056604266166687\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.72863245010376 | KNN Loss: 3.7078263759613037 | BCE Loss: 1.020806074142456\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.697099208831787 | KNN Loss: 3.6708216667175293 | BCE Loss: 1.0262775421142578\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.751562118530273 | KNN Loss: 3.7102460861206055 | BCE Loss: 1.0413157939910889\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.80470085144043 | KNN Loss: 3.769874095916748 | BCE Loss: 1.0348269939422607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.708405017852783 | KNN Loss: 3.7081358432769775 | BCE Loss: 1.0002692937850952\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.768553733825684 | KNN Loss: 3.729435682296753 | BCE Loss: 1.0391178131103516\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.720483779907227 | KNN Loss: 3.7037880420684814 | BCE Loss: 1.016695499420166\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.730031967163086 | KNN Loss: 3.7115371227264404 | BCE Loss: 1.018494963645935\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.7294230461120605 | KNN Loss: 3.693521738052368 | BCE Loss: 1.0359013080596924\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.683966159820557 | KNN Loss: 3.6786420345306396 | BCE Loss: 1.005324125289917\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.739405632019043 | KNN Loss: 3.7060437202453613 | BCE Loss: 1.0333621501922607\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.713843822479248 | KNN Loss: 3.7043344974517822 | BCE Loss: 1.0095093250274658\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.778728485107422 | KNN Loss: 3.7400012016296387 | BCE Loss: 1.038727045059204\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.674932956695557 | KNN Loss: 3.6920578479766846 | BCE Loss: 0.9828751087188721\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.764476299285889 | KNN Loss: 3.7431910037994385 | BCE Loss: 1.0212851762771606\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.756391525268555 | KNN Loss: 3.6991524696350098 | BCE Loss: 1.057239055633545\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.739588260650635 | KNN Loss: 3.709479331970215 | BCE Loss: 1.03010892868042\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.7275390625 | KNN Loss: 3.699676752090454 | BCE Loss: 1.0278624296188354\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.703314304351807 | KNN Loss: 3.6862165927886963 | BCE Loss: 1.0170977115631104\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.730413913726807 | KNN Loss: 3.695481777191162 | BCE Loss: 1.034932017326355\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.70494270324707 | KNN Loss: 3.68761944770813 | BCE Loss: 1.0173232555389404\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.695633411407471 | KNN Loss: 3.701885223388672 | BCE Loss: 0.9937483072280884\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.676543235778809 | KNN Loss: 3.666727066040039 | BCE Loss: 1.009816288948059\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.744925022125244 | KNN Loss: 3.731950044631958 | BCE Loss: 1.0129749774932861\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.754279136657715 | KNN Loss: 3.7296197414398193 | BCE Loss: 1.0246596336364746\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.755695343017578 | KNN Loss: 3.743384599685669 | BCE Loss: 1.0123108625411987\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.7468671798706055 | KNN Loss: 3.726118564605713 | BCE Loss: 1.020748496055603\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.698835372924805 | KNN Loss: 3.6943562030792236 | BCE Loss: 1.004479169845581\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.725469589233398 | KNN Loss: 3.7020576000213623 | BCE Loss: 1.023411750793457\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.730264186859131 | KNN Loss: 3.730095863342285 | BCE Loss: 1.0001683235168457\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.756852626800537 | KNN Loss: 3.7247490882873535 | BCE Loss: 1.0321035385131836\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.7439093589782715 | KNN Loss: 3.707429885864258 | BCE Loss: 1.0364793539047241\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.674466133117676 | KNN Loss: 3.6774868965148926 | BCE Loss: 0.9969793558120728\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.71859073638916 | KNN Loss: 3.7015743255615234 | BCE Loss: 1.0170161724090576\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.7179155349731445 | KNN Loss: 3.6877498626708984 | BCE Loss: 1.0301659107208252\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.679533958435059 | KNN Loss: 3.675081491470337 | BCE Loss: 1.0044524669647217\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.692303657531738 | KNN Loss: 3.679471731185913 | BCE Loss: 1.0128321647644043\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.707590103149414 | KNN Loss: 3.7084805965423584 | BCE Loss: 0.9991095066070557\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.67559814453125 | KNN Loss: 3.6680245399475098 | BCE Loss: 1.0075734853744507\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.690274715423584 | KNN Loss: 3.692335844039917 | BCE Loss: 0.9979387521743774\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.701801300048828 | KNN Loss: 3.6962225437164307 | BCE Loss: 1.0055787563323975\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.754734516143799 | KNN Loss: 3.6858763694763184 | BCE Loss: 1.0688581466674805\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.760585784912109 | KNN Loss: 3.727275848388672 | BCE Loss: 1.033309817314148\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.716302871704102 | KNN Loss: 3.6983983516693115 | BCE Loss: 1.01790452003479\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.697859764099121 | KNN Loss: 3.690075397491455 | BCE Loss: 1.007784128189087\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.70599365234375 | KNN Loss: 3.6762261390686035 | BCE Loss: 1.0297677516937256\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.702476501464844 | KNN Loss: 3.7006733417510986 | BCE Loss: 1.001802921295166\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.727547645568848 | KNN Loss: 3.7110800743103027 | BCE Loss: 1.0164674520492554\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.720758438110352 | KNN Loss: 3.706834316253662 | BCE Loss: 1.0139240026474\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.713247299194336 | KNN Loss: 3.7094674110412598 | BCE Loss: 1.0037798881530762\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.726840019226074 | KNN Loss: 3.715261697769165 | BCE Loss: 1.01157808303833\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.74681282043457 | KNN Loss: 3.7386958599090576 | BCE Loss: 1.0081168413162231\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.739250659942627 | KNN Loss: 3.7140660285949707 | BCE Loss: 1.0251847505569458\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.742785930633545 | KNN Loss: 3.71262788772583 | BCE Loss: 1.0301579236984253\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.739959716796875 | KNN Loss: 3.7255263328552246 | BCE Loss: 1.0144331455230713\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.6952643394470215 | KNN Loss: 3.6832022666931152 | BCE Loss: 1.0120619535446167\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.757296562194824 | KNN Loss: 3.7328813076019287 | BCE Loss: 1.0244154930114746\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.724421977996826 | KNN Loss: 3.680250644683838 | BCE Loss: 1.0441712141036987\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.690858364105225 | KNN Loss: 3.6912524700164795 | BCE Loss: 0.9996060132980347\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.732532024383545 | KNN Loss: 3.68750262260437 | BCE Loss: 1.0450294017791748\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.727411270141602 | KNN Loss: 3.7243332862854004 | BCE Loss: 1.0030782222747803\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.764857292175293 | KNN Loss: 3.710451364517212 | BCE Loss: 1.0544060468673706\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.703760147094727 | KNN Loss: 3.6876354217529297 | BCE Loss: 1.0161247253417969\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.7144341468811035 | KNN Loss: 3.7220523357391357 | BCE Loss: 0.9923819303512573\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.743274211883545 | KNN Loss: 3.728519916534424 | BCE Loss: 1.014754295349121\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.7243804931640625 | KNN Loss: 3.6925272941589355 | BCE Loss: 1.0318530797958374\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.73691463470459 | KNN Loss: 3.7043704986572266 | BCE Loss: 1.0325442552566528\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.729413032531738 | KNN Loss: 3.7178640365600586 | BCE Loss: 1.0115488767623901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.764874458312988 | KNN Loss: 3.7142367362976074 | BCE Loss: 1.0506376028060913\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.754358768463135 | KNN Loss: 3.727163791656494 | BCE Loss: 1.0271949768066406\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.7194318771362305 | KNN Loss: 3.703477382659912 | BCE Loss: 1.015954613685608\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.709132671356201 | KNN Loss: 3.6900923252105713 | BCE Loss: 1.0190404653549194\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.708712577819824 | KNN Loss: 3.6976027488708496 | BCE Loss: 1.0111100673675537\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.748821258544922 | KNN Loss: 3.741931200027466 | BCE Loss: 1.0068902969360352\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.7399187088012695 | KNN Loss: 3.728058099746704 | BCE Loss: 1.0118608474731445\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.717723846435547 | KNN Loss: 3.7034571170806885 | BCE Loss: 1.0142664909362793\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.715348243713379 | KNN Loss: 3.6964669227600098 | BCE Loss: 1.0188813209533691\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.7718424797058105 | KNN Loss: 3.7384254932403564 | BCE Loss: 1.0334171056747437\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.708583831787109 | KNN Loss: 3.7127981185913086 | BCE Loss: 0.9957858324050903\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.779697418212891 | KNN Loss: 3.7464187145233154 | BCE Loss: 1.0332787036895752\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.691554546356201 | KNN Loss: 3.6928317546844482 | BCE Loss: 0.9987226724624634\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.712850093841553 | KNN Loss: 3.724393129348755 | BCE Loss: 0.9884570837020874\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.73674201965332 | KNN Loss: 3.70064640045166 | BCE Loss: 1.0360958576202393\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.725210189819336 | KNN Loss: 3.701707363128662 | BCE Loss: 1.0235025882720947\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.768915176391602 | KNN Loss: 3.74782133102417 | BCE Loss: 1.0210938453674316\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.69675350189209 | KNN Loss: 3.697361946105957 | BCE Loss: 0.9993917942047119\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.666426181793213 | KNN Loss: 3.676072359085083 | BCE Loss: 0.9903538823127747\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.697751998901367 | KNN Loss: 3.6936020851135254 | BCE Loss: 1.0041499137878418\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.747297763824463 | KNN Loss: 3.723051071166992 | BCE Loss: 1.0242466926574707\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.776243209838867 | KNN Loss: 3.733412027359009 | BCE Loss: 1.0428310632705688\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.687851428985596 | KNN Loss: 3.6915509700775146 | BCE Loss: 0.9963003396987915\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.725770950317383 | KNN Loss: 3.6969528198242188 | BCE Loss: 1.028818130493164\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.730979919433594 | KNN Loss: 3.7200915813446045 | BCE Loss: 1.0108883380889893\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.783363342285156 | KNN Loss: 3.732774496078491 | BCE Loss: 1.0505890846252441\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.710813999176025 | KNN Loss: 3.688640832901001 | BCE Loss: 1.0221731662750244\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.717746734619141 | KNN Loss: 3.7169101238250732 | BCE Loss: 1.0008363723754883\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.748869895935059 | KNN Loss: 3.720000982284546 | BCE Loss: 1.0288691520690918\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.699554443359375 | KNN Loss: 3.663825750350952 | BCE Loss: 1.035728931427002\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.722064018249512 | KNN Loss: 3.7062535285949707 | BCE Loss: 1.0158107280731201\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.68105411529541 | KNN Loss: 3.6817047595977783 | BCE Loss: 0.9993494749069214\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.716445446014404 | KNN Loss: 3.67846417427063 | BCE Loss: 1.037981390953064\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.7153215408325195 | KNN Loss: 3.6798648834228516 | BCE Loss: 1.0354567766189575\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.791467189788818 | KNN Loss: 3.7289929389953613 | BCE Loss: 1.062474250793457\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.724091053009033 | KNN Loss: 3.6938538551330566 | BCE Loss: 1.030237078666687\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.753443717956543 | KNN Loss: 3.7481775283813477 | BCE Loss: 1.0052661895751953\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.7441725730896 | KNN Loss: 3.7345564365386963 | BCE Loss: 1.0096161365509033\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.728996753692627 | KNN Loss: 3.7438406944274902 | BCE Loss: 0.9851561784744263\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.728940010070801 | KNN Loss: 3.6944782733917236 | BCE Loss: 1.034461498260498\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.732765197753906 | KNN Loss: 3.6955695152282715 | BCE Loss: 1.0371954441070557\n",
      "Epoch   221: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.670926094055176 | KNN Loss: 3.670102834701538 | BCE Loss: 1.0008232593536377\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.7674078941345215 | KNN Loss: 3.737278461456299 | BCE Loss: 1.0301294326782227\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.73438024520874 | KNN Loss: 3.717609405517578 | BCE Loss: 1.0167707204818726\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.6948723793029785 | KNN Loss: 3.6877126693725586 | BCE Loss: 1.0071595907211304\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.772669792175293 | KNN Loss: 3.736187696456909 | BCE Loss: 1.0364822149276733\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.713830947875977 | KNN Loss: 3.6941990852355957 | BCE Loss: 1.0196317434310913\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.776412010192871 | KNN Loss: 3.760510206222534 | BCE Loss: 1.0159015655517578\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.738786220550537 | KNN Loss: 3.7000670433044434 | BCE Loss: 1.0387191772460938\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.782515525817871 | KNN Loss: 3.75748610496521 | BCE Loss: 1.0250294208526611\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.730678558349609 | KNN Loss: 3.7009353637695312 | BCE Loss: 1.0297434329986572\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.727385520935059 | KNN Loss: 3.7088570594787598 | BCE Loss: 1.018528699874878\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.7268500328063965 | KNN Loss: 3.692843437194824 | BCE Loss: 1.0340064764022827\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.663683891296387 | KNN Loss: 3.6674437522888184 | BCE Loss: 0.9962401986122131\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.687945365905762 | KNN Loss: 3.676506757736206 | BCE Loss: 1.0114387273788452\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.715718746185303 | KNN Loss: 3.7104580402374268 | BCE Loss: 1.005260705947876\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.744141578674316 | KNN Loss: 3.7120425701141357 | BCE Loss: 1.0320992469787598\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.770215034484863 | KNN Loss: 3.7320070266723633 | BCE Loss: 1.038208246231079\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.684075355529785 | KNN Loss: 3.6864125728607178 | BCE Loss: 0.9976626634597778\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.675838470458984 | KNN Loss: 3.6818809509277344 | BCE Loss: 0.9939574003219604\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.733824729919434 | KNN Loss: 3.722400426864624 | BCE Loss: 1.0114245414733887\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.719888687133789 | KNN Loss: 3.709338903427124 | BCE Loss: 1.010549783706665\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.747819900512695 | KNN Loss: 3.7412543296813965 | BCE Loss: 1.0065655708312988\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.7516584396362305 | KNN Loss: 3.72835636138916 | BCE Loss: 1.0233021974563599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.706443786621094 | KNN Loss: 3.6974732875823975 | BCE Loss: 1.0089704990386963\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.730773448944092 | KNN Loss: 3.704136848449707 | BCE Loss: 1.0266367197036743\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.755996227264404 | KNN Loss: 3.7244274616241455 | BCE Loss: 1.0315686464309692\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.7377753257751465 | KNN Loss: 3.7246792316436768 | BCE Loss: 1.0130960941314697\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.77932596206665 | KNN Loss: 3.7532176971435547 | BCE Loss: 1.0261081457138062\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.741094589233398 | KNN Loss: 3.700617551803589 | BCE Loss: 1.0404767990112305\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.732032299041748 | KNN Loss: 3.705249071121216 | BCE Loss: 1.0267832279205322\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.762996673583984 | KNN Loss: 3.7364537715911865 | BCE Loss: 1.0265426635742188\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.784043312072754 | KNN Loss: 3.742339611053467 | BCE Loss: 1.0417039394378662\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.740572929382324 | KNN Loss: 3.7401275634765625 | BCE Loss: 1.0004454851150513\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.731882095336914 | KNN Loss: 3.691169500350952 | BCE Loss: 1.040712833404541\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.703482627868652 | KNN Loss: 3.6755259037017822 | BCE Loss: 1.0279567241668701\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.743074893951416 | KNN Loss: 3.702817916870117 | BCE Loss: 1.0402570962905884\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.701059818267822 | KNN Loss: 3.6953394412994385 | BCE Loss: 1.0057202577590942\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.699678897857666 | KNN Loss: 3.6843323707580566 | BCE Loss: 1.015346646308899\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.7511091232299805 | KNN Loss: 3.7452332973480225 | BCE Loss: 1.005875825881958\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.714972019195557 | KNN Loss: 3.7293179035186768 | BCE Loss: 0.9856541156768799\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.716628551483154 | KNN Loss: 3.684084415435791 | BCE Loss: 1.0325442552566528\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.736728668212891 | KNN Loss: 3.710432529449463 | BCE Loss: 1.0262961387634277\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.670780658721924 | KNN Loss: 3.6767876148223877 | BCE Loss: 0.9939930438995361\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.760525703430176 | KNN Loss: 3.720738172531128 | BCE Loss: 1.039787769317627\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.719935417175293 | KNN Loss: 3.6945414543151855 | BCE Loss: 1.0253937244415283\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.758991241455078 | KNN Loss: 3.7268471717834473 | BCE Loss: 1.0321439504623413\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.702878952026367 | KNN Loss: 3.6990890502929688 | BCE Loss: 1.0037897825241089\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.7619218826293945 | KNN Loss: 3.7370622158050537 | BCE Loss: 1.02485990524292\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.697318077087402 | KNN Loss: 3.6648244857788086 | BCE Loss: 1.0324935913085938\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.722757339477539 | KNN Loss: 3.6965930461883545 | BCE Loss: 1.0261640548706055\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.728893280029297 | KNN Loss: 3.72060227394104 | BCE Loss: 1.0082907676696777\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.761899471282959 | KNN Loss: 3.7384274005889893 | BCE Loss: 1.0234720706939697\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.7183332443237305 | KNN Loss: 3.7150650024414062 | BCE Loss: 1.0032682418823242\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.730710983276367 | KNN Loss: 3.734241008758545 | BCE Loss: 0.9964697957038879\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.710204124450684 | KNN Loss: 3.7101833820343018 | BCE Loss: 1.0000207424163818\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.790153503417969 | KNN Loss: 3.742344617843628 | BCE Loss: 1.04780912399292\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.717398643493652 | KNN Loss: 3.6939733028411865 | BCE Loss: 1.023425579071045\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.745698928833008 | KNN Loss: 3.7019906044006348 | BCE Loss: 1.043708324432373\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.717659950256348 | KNN Loss: 3.735565423965454 | BCE Loss: 0.9820947051048279\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.743302345275879 | KNN Loss: 3.730616331100464 | BCE Loss: 1.012685775756836\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.73736572265625 | KNN Loss: 3.6906559467315674 | BCE Loss: 1.0467095375061035\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.776077747344971 | KNN Loss: 3.742614269256592 | BCE Loss: 1.0334633588790894\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.74522066116333 | KNN Loss: 3.696824073791504 | BCE Loss: 1.0483965873718262\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.730527877807617 | KNN Loss: 3.6989684104919434 | BCE Loss: 1.0315592288970947\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.706068515777588 | KNN Loss: 3.6969501972198486 | BCE Loss: 1.0091184377670288\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.728447914123535 | KNN Loss: 3.717791795730591 | BCE Loss: 1.0106561183929443\n",
      "Epoch   232: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.719630241394043 | KNN Loss: 3.7009174823760986 | BCE Loss: 1.0187127590179443\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.731686115264893 | KNN Loss: 3.7026233673095703 | BCE Loss: 1.0290627479553223\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.757948398590088 | KNN Loss: 3.716326951980591 | BCE Loss: 1.041621446609497\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.710577011108398 | KNN Loss: 3.7045295238494873 | BCE Loss: 1.0060476064682007\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.709887981414795 | KNN Loss: 3.6990342140197754 | BCE Loss: 1.0108537673950195\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.7674760818481445 | KNN Loss: 3.7326416969299316 | BCE Loss: 1.034834623336792\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.723785400390625 | KNN Loss: 3.7175207138061523 | BCE Loss: 1.0062648057937622\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.700155258178711 | KNN Loss: 3.67488431930542 | BCE Loss: 1.0252708196640015\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.722537994384766 | KNN Loss: 3.7091822624206543 | BCE Loss: 1.0133554935455322\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.715599536895752 | KNN Loss: 3.6962544918060303 | BCE Loss: 1.0193450450897217\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.7544355392456055 | KNN Loss: 3.7191460132598877 | BCE Loss: 1.0352897644042969\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.734651565551758 | KNN Loss: 3.708312511444092 | BCE Loss: 1.026338815689087\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.758900165557861 | KNN Loss: 3.7376534938812256 | BCE Loss: 1.0212466716766357\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.711691856384277 | KNN Loss: 3.698732852935791 | BCE Loss: 1.0129587650299072\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.697977542877197 | KNN Loss: 3.666004180908203 | BCE Loss: 1.0319732427597046\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.780337810516357 | KNN Loss: 3.732221841812134 | BCE Loss: 1.0481159687042236\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.711938381195068 | KNN Loss: 3.693772315979004 | BCE Loss: 1.018165946006775\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.727358341217041 | KNN Loss: 3.7176666259765625 | BCE Loss: 1.009691834449768\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.732283592224121 | KNN Loss: 3.691925048828125 | BCE Loss: 1.0403587818145752\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.768912315368652 | KNN Loss: 3.732208490371704 | BCE Loss: 1.0367040634155273\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.72315788269043 | KNN Loss: 3.719747304916382 | BCE Loss: 1.0034106969833374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.765722274780273 | KNN Loss: 3.724414825439453 | BCE Loss: 1.0413073301315308\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.672757148742676 | KNN Loss: 3.668394088745117 | BCE Loss: 1.0043628215789795\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.731295585632324 | KNN Loss: 3.7123425006866455 | BCE Loss: 1.0189533233642578\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.731834888458252 | KNN Loss: 3.6939644813537598 | BCE Loss: 1.0378704071044922\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.758278846740723 | KNN Loss: 3.7176353931427 | BCE Loss: 1.0406434535980225\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.75576114654541 | KNN Loss: 3.7350966930389404 | BCE Loss: 1.0206644535064697\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.7541961669921875 | KNN Loss: 3.7285399436950684 | BCE Loss: 1.0256564617156982\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.684233665466309 | KNN Loss: 3.6861777305603027 | BCE Loss: 0.9980558753013611\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.751011848449707 | KNN Loss: 3.7015366554260254 | BCE Loss: 1.049475073814392\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.7440996170043945 | KNN Loss: 3.734574317932129 | BCE Loss: 1.0095252990722656\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.670352458953857 | KNN Loss: 3.6745831966400146 | BCE Loss: 0.9957691431045532\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.737507343292236 | KNN Loss: 3.720771551132202 | BCE Loss: 1.0167356729507446\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.768511772155762 | KNN Loss: 3.7273948192596436 | BCE Loss: 1.0411169528961182\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.730591773986816 | KNN Loss: 3.6941921710968018 | BCE Loss: 1.0363998413085938\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.710629940032959 | KNN Loss: 3.694359540939331 | BCE Loss: 1.0162705183029175\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.752533912658691 | KNN Loss: 3.7130682468414307 | BCE Loss: 1.0394659042358398\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.721648693084717 | KNN Loss: 3.7160909175872803 | BCE Loss: 1.0055577754974365\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.720272064208984 | KNN Loss: 3.701179027557373 | BCE Loss: 1.0190927982330322\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.7054443359375 | KNN Loss: 3.694401741027832 | BCE Loss: 1.011042833328247\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.709616661071777 | KNN Loss: 3.6904444694519043 | BCE Loss: 1.019171953201294\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.71888542175293 | KNN Loss: 3.6986401081085205 | BCE Loss: 1.02024507522583\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.755824565887451 | KNN Loss: 3.715771436691284 | BCE Loss: 1.0400532484054565\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.769554138183594 | KNN Loss: 3.739489793777466 | BCE Loss: 1.030064344406128\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.731514930725098 | KNN Loss: 3.715757369995117 | BCE Loss: 1.0157573223114014\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.696966648101807 | KNN Loss: 3.697512626647949 | BCE Loss: 0.9994540810585022\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.7454023361206055 | KNN Loss: 3.727022886276245 | BCE Loss: 1.0183793306350708\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.761321544647217 | KNN Loss: 3.7272260189056396 | BCE Loss: 1.0340955257415771\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.728799819946289 | KNN Loss: 3.6908581256866455 | BCE Loss: 1.037941813468933\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.725785732269287 | KNN Loss: 3.7063844203948975 | BCE Loss: 1.0194013118743896\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.723064422607422 | KNN Loss: 3.7152297496795654 | BCE Loss: 1.0078344345092773\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.688378810882568 | KNN Loss: 3.6944122314453125 | BCE Loss: 0.9939664602279663\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.74854850769043 | KNN Loss: 3.722825288772583 | BCE Loss: 1.0257233381271362\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.76403284072876 | KNN Loss: 3.731628894805908 | BCE Loss: 1.0324040651321411\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.724026679992676 | KNN Loss: 3.7141692638397217 | BCE Loss: 1.0098572969436646\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.696502685546875 | KNN Loss: 3.6930935382843018 | BCE Loss: 1.0034093856811523\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.663090229034424 | KNN Loss: 3.676682472229004 | BCE Loss: 0.9864079356193542\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.77620792388916 | KNN Loss: 3.7334988117218018 | BCE Loss: 1.0427088737487793\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.722189903259277 | KNN Loss: 3.709421157836914 | BCE Loss: 1.0127689838409424\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.739989280700684 | KNN Loss: 3.7370874881744385 | BCE Loss: 1.0029019117355347\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.719359397888184 | KNN Loss: 3.684654951095581 | BCE Loss: 1.0347042083740234\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.713165760040283 | KNN Loss: 3.6867010593414307 | BCE Loss: 1.026464581489563\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.72725772857666 | KNN Loss: 3.724459171295166 | BCE Loss: 1.0027987957000732\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.699267387390137 | KNN Loss: 3.6900315284729004 | BCE Loss: 1.0092356204986572\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.742028713226318 | KNN Loss: 3.70815372467041 | BCE Loss: 1.0338749885559082\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.736760139465332 | KNN Loss: 3.724031686782837 | BCE Loss: 1.0127283334732056\n",
      "Epoch   243: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.717552661895752 | KNN Loss: 3.737227201461792 | BCE Loss: 0.98032546043396\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.736178398132324 | KNN Loss: 3.736701011657715 | BCE Loss: 0.9994772672653198\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.7108612060546875 | KNN Loss: 3.6881823539733887 | BCE Loss: 1.0226787328720093\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.721552848815918 | KNN Loss: 3.694249391555786 | BCE Loss: 1.0273032188415527\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.722423553466797 | KNN Loss: 3.7087063789367676 | BCE Loss: 1.0137171745300293\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.681364059448242 | KNN Loss: 3.6874494552612305 | BCE Loss: 0.9939148426055908\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.713217735290527 | KNN Loss: 3.689964532852173 | BCE Loss: 1.0232529640197754\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.748047828674316 | KNN Loss: 3.7058629989624023 | BCE Loss: 1.042184591293335\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.713919162750244 | KNN Loss: 3.702045440673828 | BCE Loss: 1.011873722076416\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.72643518447876 | KNN Loss: 3.6965980529785156 | BCE Loss: 1.0298371315002441\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.750915050506592 | KNN Loss: 3.705185651779175 | BCE Loss: 1.045729398727417\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.728178024291992 | KNN Loss: 3.705204725265503 | BCE Loss: 1.0229731798171997\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.76110315322876 | KNN Loss: 3.746601104736328 | BCE Loss: 1.0145020484924316\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.76370906829834 | KNN Loss: 3.7220754623413086 | BCE Loss: 1.0416338443756104\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.727897644042969 | KNN Loss: 3.704982280731201 | BCE Loss: 1.0229156017303467\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.7413177490234375 | KNN Loss: 3.6992223262786865 | BCE Loss: 1.042095422744751\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.761035442352295 | KNN Loss: 3.7365992069244385 | BCE Loss: 1.024436116218567\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.712126731872559 | KNN Loss: 3.6854963302612305 | BCE Loss: 1.026630163192749\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.7301225662231445 | KNN Loss: 3.7137300968170166 | BCE Loss: 1.016392707824707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.711285591125488 | KNN Loss: 3.690167188644409 | BCE Loss: 1.0211186408996582\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.708611965179443 | KNN Loss: 3.6842329502105713 | BCE Loss: 1.0243788957595825\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.726962089538574 | KNN Loss: 3.7061986923217773 | BCE Loss: 1.0207631587982178\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.72654390335083 | KNN Loss: 3.7096428871154785 | BCE Loss: 1.016900897026062\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.762096405029297 | KNN Loss: 3.719595432281494 | BCE Loss: 1.0425007343292236\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.755941867828369 | KNN Loss: 3.7365715503692627 | BCE Loss: 1.019370436668396\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.743181228637695 | KNN Loss: 3.689335823059082 | BCE Loss: 1.0538454055786133\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.727602958679199 | KNN Loss: 3.694179058074951 | BCE Loss: 1.033423900604248\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.747245788574219 | KNN Loss: 3.698653221130371 | BCE Loss: 1.0485925674438477\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.72041654586792 | KNN Loss: 3.7011425495147705 | BCE Loss: 1.0192738771438599\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.733075141906738 | KNN Loss: 3.7140309810638428 | BCE Loss: 1.0190441608428955\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.733685493469238 | KNN Loss: 3.6987876892089844 | BCE Loss: 1.0348975658416748\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.730788230895996 | KNN Loss: 3.7128939628601074 | BCE Loss: 1.0178942680358887\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.683312892913818 | KNN Loss: 3.690884590148926 | BCE Loss: 0.9924283027648926\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.74225378036499 | KNN Loss: 3.7444727420806885 | BCE Loss: 0.9977811574935913\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.7641191482543945 | KNN Loss: 3.728663921356201 | BCE Loss: 1.0354549884796143\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.770593166351318 | KNN Loss: 3.728034019470215 | BCE Loss: 1.0425591468811035\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.7368035316467285 | KNN Loss: 3.728248119354248 | BCE Loss: 1.0085554122924805\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.711007118225098 | KNN Loss: 3.715958595275879 | BCE Loss: 0.995048463344574\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.735143661499023 | KNN Loss: 3.68573260307312 | BCE Loss: 1.0494108200073242\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.737551689147949 | KNN Loss: 3.71256685256958 | BCE Loss: 1.0249848365783691\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.748337745666504 | KNN Loss: 3.7044448852539062 | BCE Loss: 1.0438928604125977\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.741575241088867 | KNN Loss: 3.714630365371704 | BCE Loss: 1.026944637298584\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.7303876876831055 | KNN Loss: 3.7067625522613525 | BCE Loss: 1.0236252546310425\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.70578670501709 | KNN Loss: 3.6916909217834473 | BCE Loss: 1.0140959024429321\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.667140483856201 | KNN Loss: 3.6749210357666016 | BCE Loss: 0.9922192692756653\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.7336745262146 | KNN Loss: 3.714797258377075 | BCE Loss: 1.0188772678375244\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.727150917053223 | KNN Loss: 3.7464168071746826 | BCE Loss: 0.9807340502738953\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.745168685913086 | KNN Loss: 3.69287109375 | BCE Loss: 1.0522973537445068\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.715457916259766 | KNN Loss: 3.7010486125946045 | BCE Loss: 1.0144093036651611\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.728047847747803 | KNN Loss: 3.688713312149048 | BCE Loss: 1.0393344163894653\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.688058853149414 | KNN Loss: 3.71283221244812 | BCE Loss: 0.9752267599105835\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.724128723144531 | KNN Loss: 3.678346872329712 | BCE Loss: 1.0457816123962402\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.704570293426514 | KNN Loss: 3.723788261413574 | BCE Loss: 0.9807820320129395\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.7405877113342285 | KNN Loss: 3.6870007514953613 | BCE Loss: 1.0535869598388672\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.722263336181641 | KNN Loss: 3.6997480392456055 | BCE Loss: 1.022515058517456\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.726604461669922 | KNN Loss: 3.724860668182373 | BCE Loss: 1.0017436742782593\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.768867015838623 | KNN Loss: 3.7210826873779297 | BCE Loss: 1.047784447669983\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.755614280700684 | KNN Loss: 3.7087948322296143 | BCE Loss: 1.0468196868896484\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.707200050354004 | KNN Loss: 3.6879067420959473 | BCE Loss: 1.0192934274673462\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.7272443771362305 | KNN Loss: 3.716386318206787 | BCE Loss: 1.0108582973480225\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.734537124633789 | KNN Loss: 3.7147226333618164 | BCE Loss: 1.0198147296905518\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.707378387451172 | KNN Loss: 3.7211670875549316 | BCE Loss: 0.9862111806869507\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.741725921630859 | KNN Loss: 3.7282700538635254 | BCE Loss: 1.013456106185913\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.729317665100098 | KNN Loss: 3.6947202682495117 | BCE Loss: 1.034597396850586\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.731713771820068 | KNN Loss: 3.69796085357666 | BCE Loss: 1.0337529182434082\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.706650733947754 | KNN Loss: 3.6931822299957275 | BCE Loss: 1.0134685039520264\n",
      "Epoch   254: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.680026054382324 | KNN Loss: 3.6883835792541504 | BCE Loss: 0.9916423559188843\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.733122825622559 | KNN Loss: 3.724536895751953 | BCE Loss: 1.0085861682891846\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.732255935668945 | KNN Loss: 3.7135531902313232 | BCE Loss: 1.018702507019043\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.711442947387695 | KNN Loss: 3.712941884994507 | BCE Loss: 0.9985010623931885\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.7189154624938965 | KNN Loss: 3.6943552494049072 | BCE Loss: 1.0245603322982788\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.753740310668945 | KNN Loss: 3.709887981414795 | BCE Loss: 1.0438520908355713\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.6851396560668945 | KNN Loss: 3.6805737018585205 | BCE Loss: 1.004565954208374\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.739377021789551 | KNN Loss: 3.6994407176971436 | BCE Loss: 1.0399364233016968\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.725494384765625 | KNN Loss: 3.711393356323242 | BCE Loss: 1.0141009092330933\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.792479991912842 | KNN Loss: 3.7691407203674316 | BCE Loss: 1.0233391523361206\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.700351238250732 | KNN Loss: 3.6911826133728027 | BCE Loss: 1.0091686248779297\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.709291934967041 | KNN Loss: 3.676297664642334 | BCE Loss: 1.032994270324707\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.718027114868164 | KNN Loss: 3.70405912399292 | BCE Loss: 1.0139679908752441\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.698739051818848 | KNN Loss: 3.697706699371338 | BCE Loss: 1.0010323524475098\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.709402561187744 | KNN Loss: 3.7011075019836426 | BCE Loss: 1.008294939994812\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.688380241394043 | KNN Loss: 3.6746304035186768 | BCE Loss: 1.013749599456787\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.682876110076904 | KNN Loss: 3.680023431777954 | BCE Loss: 1.0028526782989502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.723313331604004 | KNN Loss: 3.6996958255767822 | BCE Loss: 1.0236172676086426\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.7329888343811035 | KNN Loss: 3.7090611457824707 | BCE Loss: 1.0239276885986328\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.743885040283203 | KNN Loss: 3.693159580230713 | BCE Loss: 1.0507256984710693\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.718471527099609 | KNN Loss: 3.7119100093841553 | BCE Loss: 1.0065617561340332\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.800650596618652 | KNN Loss: 3.725152015686035 | BCE Loss: 1.075498342514038\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.682872295379639 | KNN Loss: 3.700822591781616 | BCE Loss: 0.982049822807312\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.744189739227295 | KNN Loss: 3.7414488792419434 | BCE Loss: 1.0027409791946411\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.725719451904297 | KNN Loss: 3.68992018699646 | BCE Loss: 1.0357990264892578\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.756832599639893 | KNN Loss: 3.7189111709594727 | BCE Loss: 1.03792142868042\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.763556003570557 | KNN Loss: 3.7188398838043213 | BCE Loss: 1.0447161197662354\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.70001220703125 | KNN Loss: 3.675687074661255 | BCE Loss: 1.0243253707885742\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.729031085968018 | KNN Loss: 3.7150826454162598 | BCE Loss: 1.0139484405517578\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.6907501220703125 | KNN Loss: 3.676903009414673 | BCE Loss: 1.01384699344635\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.728130340576172 | KNN Loss: 3.691701650619507 | BCE Loss: 1.0364289283752441\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.708229064941406 | KNN Loss: 3.6761088371276855 | BCE Loss: 1.0321204662322998\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.715987205505371 | KNN Loss: 3.7082080841064453 | BCE Loss: 1.0077792406082153\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.770054817199707 | KNN Loss: 3.724015474319458 | BCE Loss: 1.046039342880249\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.749443054199219 | KNN Loss: 3.7203292846679688 | BCE Loss: 1.029114007949829\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.752887725830078 | KNN Loss: 3.7185583114624023 | BCE Loss: 1.0343295335769653\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.764545917510986 | KNN Loss: 3.736875057220459 | BCE Loss: 1.0276708602905273\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.712522506713867 | KNN Loss: 3.697218656539917 | BCE Loss: 1.0153037309646606\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.7285566329956055 | KNN Loss: 3.707671642303467 | BCE Loss: 1.0208847522735596\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.749689102172852 | KNN Loss: 3.7154839038848877 | BCE Loss: 1.0342049598693848\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.707693576812744 | KNN Loss: 3.6957616806030273 | BCE Loss: 1.0119318962097168\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.7322001457214355 | KNN Loss: 3.6985247135162354 | BCE Loss: 1.0336753129959106\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.668137550354004 | KNN Loss: 3.664398431777954 | BCE Loss: 1.0037388801574707\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.749423027038574 | KNN Loss: 3.7419562339782715 | BCE Loss: 1.0074665546417236\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.745608806610107 | KNN Loss: 3.7093827724456787 | BCE Loss: 1.0362261533737183\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.701058864593506 | KNN Loss: 3.686553478240967 | BCE Loss: 1.014505386352539\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.722038269042969 | KNN Loss: 3.6894562244415283 | BCE Loss: 1.03258216381073\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.716390132904053 | KNN Loss: 3.70147967338562 | BCE Loss: 1.0149104595184326\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.724514961242676 | KNN Loss: 3.727339267730713 | BCE Loss: 0.997175931930542\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.7311296463012695 | KNN Loss: 3.7229881286621094 | BCE Loss: 1.0081415176391602\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.722106456756592 | KNN Loss: 3.689608335494995 | BCE Loss: 1.0324981212615967\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.748165607452393 | KNN Loss: 3.7037713527679443 | BCE Loss: 1.0443942546844482\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.712626934051514 | KNN Loss: 3.6903116703033447 | BCE Loss: 1.0223151445388794\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.704180717468262 | KNN Loss: 3.712578535079956 | BCE Loss: 0.9916023015975952\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.769045829772949 | KNN Loss: 3.7235422134399414 | BCE Loss: 1.045503854751587\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.727202415466309 | KNN Loss: 3.720795154571533 | BCE Loss: 1.0064074993133545\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.747153282165527 | KNN Loss: 3.7061383724212646 | BCE Loss: 1.0410149097442627\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.737939834594727 | KNN Loss: 3.7156198024749756 | BCE Loss: 1.0223197937011719\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.752552509307861 | KNN Loss: 3.7097480297088623 | BCE Loss: 1.042804479598999\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.711252689361572 | KNN Loss: 3.702172040939331 | BCE Loss: 1.0090805292129517\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.724636554718018 | KNN Loss: 3.7132108211517334 | BCE Loss: 1.0114258527755737\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.719552516937256 | KNN Loss: 3.694023609161377 | BCE Loss: 1.025528907775879\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.681704998016357 | KNN Loss: 3.693063974380493 | BCE Loss: 0.9886410236358643\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.7040228843688965 | KNN Loss: 3.7109830379486084 | BCE Loss: 0.9930399060249329\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.725260257720947 | KNN Loss: 3.7281315326690674 | BCE Loss: 0.9971288442611694\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.718924522399902 | KNN Loss: 3.7181155681610107 | BCE Loss: 1.0008090734481812\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.703666687011719 | KNN Loss: 3.6856846809387207 | BCE Loss: 1.017981767654419\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.70904541015625 | KNN Loss: 3.6904494762420654 | BCE Loss: 1.0185956954956055\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.725025653839111 | KNN Loss: 3.7129721641540527 | BCE Loss: 1.0120534896850586\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.719122409820557 | KNN Loss: 3.7127509117126465 | BCE Loss: 1.0063714981079102\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.728935241699219 | KNN Loss: 3.7038841247558594 | BCE Loss: 1.0250508785247803\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.796645641326904 | KNN Loss: 3.7699878215789795 | BCE Loss: 1.0266577005386353\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.7812042236328125 | KNN Loss: 3.746070146560669 | BCE Loss: 1.035134196281433\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.740816116333008 | KNN Loss: 3.6870408058166504 | BCE Loss: 1.0537753105163574\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.758940696716309 | KNN Loss: 3.7226402759552 | BCE Loss: 1.036300539970398\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.807765007019043 | KNN Loss: 3.748148202896118 | BCE Loss: 1.0596168041229248\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.728631019592285 | KNN Loss: 3.7227492332458496 | BCE Loss: 1.0058820247650146\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.788678169250488 | KNN Loss: 3.7618908882141113 | BCE Loss: 1.026787519454956\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.740880489349365 | KNN Loss: 3.7185513973236084 | BCE Loss: 1.0223289728164673\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.6894378662109375 | KNN Loss: 3.6905243396759033 | BCE Loss: 0.9989135265350342\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.707627773284912 | KNN Loss: 3.6717660427093506 | BCE Loss: 1.0358617305755615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.713976860046387 | KNN Loss: 3.717313766479492 | BCE Loss: 0.996662974357605\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.707190036773682 | KNN Loss: 3.684222936630249 | BCE Loss: 1.0229671001434326\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.704684257507324 | KNN Loss: 3.6773910522460938 | BCE Loss: 1.0272934436798096\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.7339863777160645 | KNN Loss: 3.6897499561309814 | BCE Loss: 1.044236421585083\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.736303806304932 | KNN Loss: 3.729484796524048 | BCE Loss: 1.0068188905715942\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.690225124359131 | KNN Loss: 3.691316604614258 | BCE Loss: 0.9989083409309387\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.743015289306641 | KNN Loss: 3.716688394546509 | BCE Loss: 1.026327133178711\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.698496341705322 | KNN Loss: 3.699834108352661 | BCE Loss: 0.9986621141433716\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.735480785369873 | KNN Loss: 3.6945037841796875 | BCE Loss: 1.040976881980896\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.750516891479492 | KNN Loss: 3.716951847076416 | BCE Loss: 1.0335650444030762\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.725120544433594 | KNN Loss: 3.7039082050323486 | BCE Loss: 1.0212122201919556\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.7290849685668945 | KNN Loss: 3.69269061088562 | BCE Loss: 1.0363941192626953\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.726412773132324 | KNN Loss: 3.7124176025390625 | BCE Loss: 1.0139954090118408\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.761798858642578 | KNN Loss: 3.7063844203948975 | BCE Loss: 1.0554144382476807\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.700381278991699 | KNN Loss: 3.6878185272216797 | BCE Loss: 1.0125627517700195\n",
      "Epoch   270: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.784276008605957 | KNN Loss: 3.7401766777038574 | BCE Loss: 1.0440993309020996\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.74026346206665 | KNN Loss: 3.706956148147583 | BCE Loss: 1.033307433128357\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.677080154418945 | KNN Loss: 3.6989994049072266 | BCE Loss: 0.9780809879302979\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.770817756652832 | KNN Loss: 3.741485118865967 | BCE Loss: 1.0293326377868652\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.712968826293945 | KNN Loss: 3.688450813293457 | BCE Loss: 1.0245182514190674\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.6673994064331055 | KNN Loss: 3.688089609146118 | BCE Loss: 0.9793098568916321\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.728193283081055 | KNN Loss: 3.723504066467285 | BCE Loss: 1.0046889781951904\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.722557544708252 | KNN Loss: 3.6894121170043945 | BCE Loss: 1.0331453084945679\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.682033061981201 | KNN Loss: 3.6852259635925293 | BCE Loss: 0.9968070387840271\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.735984802246094 | KNN Loss: 3.7123289108276367 | BCE Loss: 1.0236557722091675\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.7199883460998535 | KNN Loss: 3.6739256381988525 | BCE Loss: 1.0460628271102905\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.766952037811279 | KNN Loss: 3.71675443649292 | BCE Loss: 1.050197720527649\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.720142364501953 | KNN Loss: 3.7182741165161133 | BCE Loss: 1.001868486404419\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.694782257080078 | KNN Loss: 3.68864107131958 | BCE Loss: 1.0061414241790771\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.728228569030762 | KNN Loss: 3.680448055267334 | BCE Loss: 1.0477805137634277\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.71432638168335 | KNN Loss: 3.6893370151519775 | BCE Loss: 1.0249894857406616\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.770631313323975 | KNN Loss: 3.7389016151428223 | BCE Loss: 1.031729817390442\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.671594142913818 | KNN Loss: 3.6824426651000977 | BCE Loss: 0.9891514778137207\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.692870140075684 | KNN Loss: 3.6864352226257324 | BCE Loss: 1.006434679031372\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.723272800445557 | KNN Loss: 3.696697473526001 | BCE Loss: 1.0265754461288452\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.730722427368164 | KNN Loss: 3.723015069961548 | BCE Loss: 1.0077073574066162\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.781838417053223 | KNN Loss: 3.755019187927246 | BCE Loss: 1.0268192291259766\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.748284339904785 | KNN Loss: 3.7091877460479736 | BCE Loss: 1.0390968322753906\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.706500053405762 | KNN Loss: 3.7066924571990967 | BCE Loss: 0.9998077750205994\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.724450588226318 | KNN Loss: 3.7221686840057373 | BCE Loss: 1.0022817850112915\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.741386890411377 | KNN Loss: 3.7231106758117676 | BCE Loss: 1.0182762145996094\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.766416549682617 | KNN Loss: 3.719844102859497 | BCE Loss: 1.046572208404541\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.719960689544678 | KNN Loss: 3.7031662464141846 | BCE Loss: 1.0167944431304932\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.748557090759277 | KNN Loss: 3.7394049167633057 | BCE Loss: 1.0091521739959717\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.748605251312256 | KNN Loss: 3.7314960956573486 | BCE Loss: 1.0171091556549072\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.740893363952637 | KNN Loss: 3.713074207305908 | BCE Loss: 1.0278191566467285\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.755920886993408 | KNN Loss: 3.717158794403076 | BCE Loss: 1.038762092590332\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.695180416107178 | KNN Loss: 3.671178102493286 | BCE Loss: 1.0240023136138916\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.670692443847656 | KNN Loss: 3.691359281539917 | BCE Loss: 0.9793329238891602\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.747790336608887 | KNN Loss: 3.7082700729370117 | BCE Loss: 1.039520502090454\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.724830627441406 | KNN Loss: 3.698673963546753 | BCE Loss: 1.0261566638946533\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.707361221313477 | KNN Loss: 3.6795742511749268 | BCE Loss: 1.027787208557129\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.705034255981445 | KNN Loss: 3.7005646228790283 | BCE Loss: 1.004469633102417\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.701066017150879 | KNN Loss: 3.6875712871551514 | BCE Loss: 1.0134944915771484\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.696669578552246 | KNN Loss: 3.674437999725342 | BCE Loss: 1.0222318172454834\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.704127788543701 | KNN Loss: 3.6912107467651367 | BCE Loss: 1.012916922569275\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.7531890869140625 | KNN Loss: 3.7341394424438477 | BCE Loss: 1.0190495252609253\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.703311920166016 | KNN Loss: 3.695326566696167 | BCE Loss: 1.0079854726791382\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.722278118133545 | KNN Loss: 3.7055702209472656 | BCE Loss: 1.0167078971862793\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.755693435668945 | KNN Loss: 3.7080490589141846 | BCE Loss: 1.0476441383361816\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.780045032501221 | KNN Loss: 3.7663538455963135 | BCE Loss: 1.0136911869049072\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.7304182052612305 | KNN Loss: 3.7076151371002197 | BCE Loss: 1.0228028297424316\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.740771293640137 | KNN Loss: 3.7199299335479736 | BCE Loss: 1.020841121673584\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.708767890930176 | KNN Loss: 3.6888256072998047 | BCE Loss: 1.019942283630371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.7282891273498535 | KNN Loss: 3.6897709369659424 | BCE Loss: 1.0385181903839111\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.727675437927246 | KNN Loss: 3.684317111968994 | BCE Loss: 1.0433580875396729\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.756896495819092 | KNN Loss: 3.726398468017578 | BCE Loss: 1.0304981470108032\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.740072727203369 | KNN Loss: 3.694629430770874 | BCE Loss: 1.0454431772232056\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.71905517578125 | KNN Loss: 3.6993558406829834 | BCE Loss: 1.0196993350982666\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.702778339385986 | KNN Loss: 3.6962029933929443 | BCE Loss: 1.006575345993042\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.701593399047852 | KNN Loss: 3.7081174850463867 | BCE Loss: 0.9934757947921753\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.713642120361328 | KNN Loss: 3.7055482864379883 | BCE Loss: 1.0080939531326294\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.763524055480957 | KNN Loss: 3.7388570308685303 | BCE Loss: 1.0246669054031372\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.705445289611816 | KNN Loss: 3.6744701862335205 | BCE Loss: 1.0309752225875854\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.695747375488281 | KNN Loss: 3.6820802688598633 | BCE Loss: 1.013667345046997\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.750765800476074 | KNN Loss: 3.7030179500579834 | BCE Loss: 1.0477479696273804\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.708550453186035 | KNN Loss: 3.702176332473755 | BCE Loss: 1.0063741207122803\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.772152423858643 | KNN Loss: 3.7166872024536133 | BCE Loss: 1.0554653406143188\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.7204742431640625 | KNN Loss: 3.713214635848999 | BCE Loss: 1.0072598457336426\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.7587714195251465 | KNN Loss: 3.734273910522461 | BCE Loss: 1.024497389793396\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.7459588050842285 | KNN Loss: 3.7139110565185547 | BCE Loss: 1.0320477485656738\n",
      "Epoch   281: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.706583499908447 | KNN Loss: 3.6922054290771484 | BCE Loss: 1.0143780708312988\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.699460983276367 | KNN Loss: 3.689328670501709 | BCE Loss: 1.0101323127746582\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.757590293884277 | KNN Loss: 3.7458090782165527 | BCE Loss: 1.0117812156677246\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.7380146980285645 | KNN Loss: 3.7084147930145264 | BCE Loss: 1.0296000242233276\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.746342182159424 | KNN Loss: 3.7243683338165283 | BCE Loss: 1.021973729133606\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.699043273925781 | KNN Loss: 3.6724510192871094 | BCE Loss: 1.0265922546386719\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.734241008758545 | KNN Loss: 3.7162246704101562 | BCE Loss: 1.0180163383483887\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.742428302764893 | KNN Loss: 3.71955943107605 | BCE Loss: 1.0228689908981323\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.739572048187256 | KNN Loss: 3.6957778930664062 | BCE Loss: 1.04379403591156\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.744602203369141 | KNN Loss: 3.700676202774048 | BCE Loss: 1.0439260005950928\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.727294921875 | KNN Loss: 3.6795480251312256 | BCE Loss: 1.0477471351623535\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.744289398193359 | KNN Loss: 3.719571352005005 | BCE Loss: 1.0247178077697754\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.703168869018555 | KNN Loss: 3.714401960372925 | BCE Loss: 0.9887666702270508\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.758363723754883 | KNN Loss: 3.7186341285705566 | BCE Loss: 1.039729356765747\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.754146099090576 | KNN Loss: 3.72043514251709 | BCE Loss: 1.0337108373641968\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.740696907043457 | KNN Loss: 3.7079765796661377 | BCE Loss: 1.0327202081680298\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.720709800720215 | KNN Loss: 3.7138559818267822 | BCE Loss: 1.0068539381027222\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.698644638061523 | KNN Loss: 3.687838315963745 | BCE Loss: 1.0108060836791992\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.747677803039551 | KNN Loss: 3.6941025257110596 | BCE Loss: 1.0535755157470703\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.709715843200684 | KNN Loss: 3.698740005493164 | BCE Loss: 1.0109760761260986\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.746384143829346 | KNN Loss: 3.7084105014801025 | BCE Loss: 1.0379736423492432\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.72653341293335 | KNN Loss: 3.7195587158203125 | BCE Loss: 1.006974697113037\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.721441268920898 | KNN Loss: 3.694291114807129 | BCE Loss: 1.0271499156951904\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.720093250274658 | KNN Loss: 3.717923402786255 | BCE Loss: 1.0021698474884033\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.770728588104248 | KNN Loss: 3.7092573642730713 | BCE Loss: 1.0614711046218872\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.655541896820068 | KNN Loss: 3.672518253326416 | BCE Loss: 0.9830236434936523\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.774532318115234 | KNN Loss: 3.710855007171631 | BCE Loss: 1.063677430152893\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.704791069030762 | KNN Loss: 3.6874136924743652 | BCE Loss: 1.017377495765686\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.791391372680664 | KNN Loss: 3.7388670444488525 | BCE Loss: 1.0525240898132324\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.751646995544434 | KNN Loss: 3.7224864959716797 | BCE Loss: 1.0291606187820435\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.732820987701416 | KNN Loss: 3.7089478969573975 | BCE Loss: 1.0238730907440186\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.739676475524902 | KNN Loss: 3.7258424758911133 | BCE Loss: 1.01383376121521\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.746736526489258 | KNN Loss: 3.712554931640625 | BCE Loss: 1.0341817140579224\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.766432762145996 | KNN Loss: 3.7452807426452637 | BCE Loss: 1.0211522579193115\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.697668075561523 | KNN Loss: 3.69760799407959 | BCE Loss: 1.0000603199005127\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.741879463195801 | KNN Loss: 3.7141785621643066 | BCE Loss: 1.0277010202407837\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.710912704467773 | KNN Loss: 3.6922693252563477 | BCE Loss: 1.0186431407928467\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.700631618499756 | KNN Loss: 3.6889328956604004 | BCE Loss: 1.0116987228393555\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.725012302398682 | KNN Loss: 3.715296506881714 | BCE Loss: 1.0097159147262573\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.713586807250977 | KNN Loss: 3.6906285285949707 | BCE Loss: 1.0229582786560059\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.7517900466918945 | KNN Loss: 3.727694511413574 | BCE Loss: 1.0240957736968994\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.6906232833862305 | KNN Loss: 3.6843020915985107 | BCE Loss: 1.0063210725784302\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.723422527313232 | KNN Loss: 3.7058608531951904 | BCE Loss: 1.017561674118042\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.695996284484863 | KNN Loss: 3.692934274673462 | BCE Loss: 1.0030617713928223\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.7564826011657715 | KNN Loss: 3.717339515686035 | BCE Loss: 1.0391432046890259\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.746476650238037 | KNN Loss: 3.7189674377441406 | BCE Loss: 1.0275092124938965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.7249908447265625 | KNN Loss: 3.6939847469329834 | BCE Loss: 1.031006097793579\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.720367431640625 | KNN Loss: 3.721616744995117 | BCE Loss: 0.9987505674362183\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.660045623779297 | KNN Loss: 3.6917829513549805 | BCE Loss: 0.9682624340057373\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.712893962860107 | KNN Loss: 3.700392246246338 | BCE Loss: 1.0125017166137695\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.739830493927002 | KNN Loss: 3.7237322330474854 | BCE Loss: 1.0160982608795166\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.728182315826416 | KNN Loss: 3.687965154647827 | BCE Loss: 1.0402171611785889\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.708847999572754 | KNN Loss: 3.687863349914551 | BCE Loss: 1.0209846496582031\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.700455665588379 | KNN Loss: 3.6586804389953613 | BCE Loss: 1.0417754650115967\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.711047172546387 | KNN Loss: 3.7121734619140625 | BCE Loss: 0.9988735914230347\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.703310012817383 | KNN Loss: 3.683758020401001 | BCE Loss: 1.0195519924163818\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.7087788581848145 | KNN Loss: 3.6969387531280518 | BCE Loss: 1.0118399858474731\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.739307403564453 | KNN Loss: 3.713787078857422 | BCE Loss: 1.0255200862884521\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.713608741760254 | KNN Loss: 3.695871114730835 | BCE Loss: 1.0177373886108398\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.75242805480957 | KNN Loss: 3.7277684211730957 | BCE Loss: 1.0246593952178955\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.774577617645264 | KNN Loss: 3.7537667751312256 | BCE Loss: 1.020810842514038\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.732245922088623 | KNN Loss: 3.695775032043457 | BCE Loss: 1.036470890045166\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.722835540771484 | KNN Loss: 3.697167158126831 | BCE Loss: 1.0256681442260742\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.694095611572266 | KNN Loss: 3.7026562690734863 | BCE Loss: 0.9914393424987793\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.736739635467529 | KNN Loss: 3.7339766025543213 | BCE Loss: 1.0027631521224976\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.757148265838623 | KNN Loss: 3.717648506164551 | BCE Loss: 1.0394996404647827\n",
      "Epoch   292: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.704948425292969 | KNN Loss: 3.6766247749328613 | BCE Loss: 1.0283234119415283\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.715195178985596 | KNN Loss: 3.6938459873199463 | BCE Loss: 1.0213491916656494\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.716324806213379 | KNN Loss: 3.6953749656677246 | BCE Loss: 1.0209496021270752\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.750467777252197 | KNN Loss: 3.7319071292877197 | BCE Loss: 1.018560528755188\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.717214584350586 | KNN Loss: 3.707139253616333 | BCE Loss: 1.0100752115249634\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.762101650238037 | KNN Loss: 3.7233598232269287 | BCE Loss: 1.0387417078018188\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.732887268066406 | KNN Loss: 3.7154510021209717 | BCE Loss: 1.017436146736145\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.737896919250488 | KNN Loss: 3.713458776473999 | BCE Loss: 1.0244379043579102\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.710661888122559 | KNN Loss: 3.696977138519287 | BCE Loss: 1.0136847496032715\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.7309746742248535 | KNN Loss: 3.7330524921417236 | BCE Loss: 0.9979221224784851\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.76017951965332 | KNN Loss: 3.6917293071746826 | BCE Loss: 1.0684504508972168\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.742387771606445 | KNN Loss: 3.7075557708740234 | BCE Loss: 1.034832239151001\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.732832908630371 | KNN Loss: 3.71087908744812 | BCE Loss: 1.0219537019729614\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.730351448059082 | KNN Loss: 3.7108328342437744 | BCE Loss: 1.0195183753967285\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.756106376647949 | KNN Loss: 3.738434076309204 | BCE Loss: 1.017672061920166\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.760300159454346 | KNN Loss: 3.714747667312622 | BCE Loss: 1.0455526113510132\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.700714111328125 | KNN Loss: 3.675163745880127 | BCE Loss: 1.0255506038665771\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.729082107543945 | KNN Loss: 3.7115983963012695 | BCE Loss: 1.0174837112426758\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.760365009307861 | KNN Loss: 3.7203493118286133 | BCE Loss: 1.040015697479248\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.7098588943481445 | KNN Loss: 3.7011330127716064 | BCE Loss: 1.0087260007858276\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.684469223022461 | KNN Loss: 3.6874606609344482 | BCE Loss: 0.997008740901947\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.773391246795654 | KNN Loss: 3.7273523807525635 | BCE Loss: 1.0460388660430908\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.738852024078369 | KNN Loss: 3.7015860080718994 | BCE Loss: 1.0372658967971802\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.792239189147949 | KNN Loss: 3.7443699836730957 | BCE Loss: 1.0478692054748535\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.711177349090576 | KNN Loss: 3.6930794715881348 | BCE Loss: 1.0180977582931519\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.773015022277832 | KNN Loss: 3.754950761795044 | BCE Loss: 1.018064260482788\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.726839065551758 | KNN Loss: 3.7067408561706543 | BCE Loss: 1.0200979709625244\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.701412200927734 | KNN Loss: 3.704075336456299 | BCE Loss: 0.9973368644714355\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.762106418609619 | KNN Loss: 3.7184956073760986 | BCE Loss: 1.04361093044281\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.743902206420898 | KNN Loss: 3.7053558826446533 | BCE Loss: 1.0385465621948242\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.734065055847168 | KNN Loss: 3.710040807723999 | BCE Loss: 1.024024248123169\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.746446132659912 | KNN Loss: 3.7131361961364746 | BCE Loss: 1.033309817314148\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.75228214263916 | KNN Loss: 3.7143256664276123 | BCE Loss: 1.0379562377929688\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.785512924194336 | KNN Loss: 3.7422595024108887 | BCE Loss: 1.0432536602020264\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.738277912139893 | KNN Loss: 3.7436764240264893 | BCE Loss: 0.994601309299469\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.737753391265869 | KNN Loss: 3.7306931018829346 | BCE Loss: 1.0070604085922241\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.77329683303833 | KNN Loss: 3.7269015312194824 | BCE Loss: 1.0463953018188477\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.708528995513916 | KNN Loss: 3.709806442260742 | BCE Loss: 0.998722493648529\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.729611396789551 | KNN Loss: 3.723327875137329 | BCE Loss: 1.0062837600708008\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.7463884353637695 | KNN Loss: 3.6993675231933594 | BCE Loss: 1.0470211505889893\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.733044624328613 | KNN Loss: 3.7126669883728027 | BCE Loss: 1.0203778743743896\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.721589088439941 | KNN Loss: 3.6994738578796387 | BCE Loss: 1.0221149921417236\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.732895851135254 | KNN Loss: 3.7165687084198 | BCE Loss: 1.0163272619247437\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.722003936767578 | KNN Loss: 3.6810669898986816 | BCE Loss: 1.040937066078186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.705644130706787 | KNN Loss: 3.704404592514038 | BCE Loss: 1.001239538192749\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.730780601501465 | KNN Loss: 3.71010684967041 | BCE Loss: 1.0206736326217651\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.734591007232666 | KNN Loss: 3.7141847610473633 | BCE Loss: 1.0204062461853027\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.731437683105469 | KNN Loss: 3.7091715335845947 | BCE Loss: 1.0222663879394531\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.754538536071777 | KNN Loss: 3.751600503921509 | BCE Loss: 1.002937912940979\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.721158981323242 | KNN Loss: 3.690776824951172 | BCE Loss: 1.0303820371627808\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.759917736053467 | KNN Loss: 3.7397727966308594 | BCE Loss: 1.0201449394226074\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.750723361968994 | KNN Loss: 3.702622890472412 | BCE Loss: 1.0481003522872925\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.756314754486084 | KNN Loss: 3.738961696624756 | BCE Loss: 1.0173531770706177\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.696993827819824 | KNN Loss: 3.685030698776245 | BCE Loss: 1.0119633674621582\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.751536846160889 | KNN Loss: 3.7488017082214355 | BCE Loss: 1.0027350187301636\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.774742126464844 | KNN Loss: 3.756610870361328 | BCE Loss: 1.0181312561035156\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.717492580413818 | KNN Loss: 3.7154173851013184 | BCE Loss: 1.0020751953125\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.716170787811279 | KNN Loss: 3.7180685997009277 | BCE Loss: 0.998102068901062\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.705857276916504 | KNN Loss: 3.694669723510742 | BCE Loss: 1.0111874341964722\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.731093406677246 | KNN Loss: 3.7074248790740967 | BCE Loss: 1.0236684083938599\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.721242904663086 | KNN Loss: 3.716992139816284 | BCE Loss: 1.0042510032653809\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.735755920410156 | KNN Loss: 3.692005157470703 | BCE Loss: 1.0437507629394531\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.683505058288574 | KNN Loss: 3.6747891902923584 | BCE Loss: 1.008716106414795\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.700699806213379 | KNN Loss: 3.661231756210327 | BCE Loss: 1.0394680500030518\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.686166763305664 | KNN Loss: 3.676038980484009 | BCE Loss: 1.0101276636123657\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.730443000793457 | KNN Loss: 3.7282979488372803 | BCE Loss: 1.0021450519561768\n",
      "Epoch   303: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.754375457763672 | KNN Loss: 3.7370481491088867 | BCE Loss: 1.0173273086547852\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.734776496887207 | KNN Loss: 3.7154781818389893 | BCE Loss: 1.0192980766296387\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.718171119689941 | KNN Loss: 3.693002939224243 | BCE Loss: 1.0251680612564087\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.691497325897217 | KNN Loss: 3.694220781326294 | BCE Loss: 0.9972767233848572\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.691080570220947 | KNN Loss: 3.6934449672698975 | BCE Loss: 0.9976354837417603\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.758946418762207 | KNN Loss: 3.718629837036133 | BCE Loss: 1.0403163433074951\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.71868896484375 | KNN Loss: 3.696748971939087 | BCE Loss: 1.021939992904663\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.716830253601074 | KNN Loss: 3.683187246322632 | BCE Loss: 1.0336427688598633\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.7493577003479 | KNN Loss: 3.7091736793518066 | BCE Loss: 1.0401841402053833\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.6919097900390625 | KNN Loss: 3.708070755004883 | BCE Loss: 0.9838389158248901\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.718505859375 | KNN Loss: 3.701080083847046 | BCE Loss: 1.017425537109375\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.703843116760254 | KNN Loss: 3.699418544769287 | BCE Loss: 1.004424810409546\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.752443313598633 | KNN Loss: 3.7385458946228027 | BCE Loss: 1.0138976573944092\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.712294101715088 | KNN Loss: 3.6897642612457275 | BCE Loss: 1.0225298404693604\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.770835876464844 | KNN Loss: 3.722022294998169 | BCE Loss: 1.0488133430480957\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.7343974113464355 | KNN Loss: 3.707162380218506 | BCE Loss: 1.0272349119186401\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.742910861968994 | KNN Loss: 3.70082426071167 | BCE Loss: 1.0420866012573242\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.727753162384033 | KNN Loss: 3.7113025188446045 | BCE Loss: 1.0164505243301392\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.730163097381592 | KNN Loss: 3.7169604301452637 | BCE Loss: 1.0132027864456177\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.695253372192383 | KNN Loss: 3.677337169647217 | BCE Loss: 1.017916202545166\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.714053153991699 | KNN Loss: 3.6966428756713867 | BCE Loss: 1.0174100399017334\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.6960978507995605 | KNN Loss: 3.6768620014190674 | BCE Loss: 1.0192358493804932\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.7389421463012695 | KNN Loss: 3.7016284465789795 | BCE Loss: 1.037313461303711\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.7397894859313965 | KNN Loss: 3.701540946960449 | BCE Loss: 1.0382486581802368\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.734730243682861 | KNN Loss: 3.6776621341705322 | BCE Loss: 1.0570682287216187\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.692169666290283 | KNN Loss: 3.6888396739959717 | BCE Loss: 1.0033299922943115\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.718401908874512 | KNN Loss: 3.681748867034912 | BCE Loss: 1.0366530418395996\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.74510383605957 | KNN Loss: 3.726677179336548 | BCE Loss: 1.018426537513733\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.744465351104736 | KNN Loss: 3.70615553855896 | BCE Loss: 1.0383096933364868\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.73372745513916 | KNN Loss: 3.7362821102142334 | BCE Loss: 0.9974454641342163\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.755772113800049 | KNN Loss: 3.707106590270996 | BCE Loss: 1.0486655235290527\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.767596244812012 | KNN Loss: 3.7290470600128174 | BCE Loss: 1.0385494232177734\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.678432464599609 | KNN Loss: 3.680617332458496 | BCE Loss: 0.9978150129318237\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.683134078979492 | KNN Loss: 3.686087131500244 | BCE Loss: 0.9970467686653137\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.7298455238342285 | KNN Loss: 3.6944069862365723 | BCE Loss: 1.0354386568069458\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.793818473815918 | KNN Loss: 3.723825693130493 | BCE Loss: 1.0699927806854248\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.76392126083374 | KNN Loss: 3.7550175189971924 | BCE Loss: 1.0089037418365479\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.715141296386719 | KNN Loss: 3.6875171661376953 | BCE Loss: 1.0276243686676025\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.704695701599121 | KNN Loss: 3.699002742767334 | BCE Loss: 1.005692720413208\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.731925010681152 | KNN Loss: 3.739542007446289 | BCE Loss: 0.9923828840255737\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.720828533172607 | KNN Loss: 3.7080018520355225 | BCE Loss: 1.0128268003463745\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.73931884765625 | KNN Loss: 3.7221615314483643 | BCE Loss: 1.0171575546264648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.710790634155273 | KNN Loss: 3.684206485748291 | BCE Loss: 1.0265841484069824\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.718181610107422 | KNN Loss: 3.703930139541626 | BCE Loss: 1.014251470565796\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.72463846206665 | KNN Loss: 3.7226040363311768 | BCE Loss: 1.0020344257354736\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.705843448638916 | KNN Loss: 3.7026607990264893 | BCE Loss: 1.0031825304031372\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.711874008178711 | KNN Loss: 3.6819443702697754 | BCE Loss: 1.029929518699646\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.721822261810303 | KNN Loss: 3.6949875354766846 | BCE Loss: 1.0268347263336182\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.758835792541504 | KNN Loss: 3.6966676712036133 | BCE Loss: 1.0621683597564697\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.779362678527832 | KNN Loss: 3.741718292236328 | BCE Loss: 1.0376445055007935\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.743358612060547 | KNN Loss: 3.726909637451172 | BCE Loss: 1.016448974609375\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.710771560668945 | KNN Loss: 3.6869001388549805 | BCE Loss: 1.0238711833953857\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.7207536697387695 | KNN Loss: 3.7006492614746094 | BCE Loss: 1.0201046466827393\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.736773490905762 | KNN Loss: 3.723788022994995 | BCE Loss: 1.0129857063293457\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.753276824951172 | KNN Loss: 3.7170958518981934 | BCE Loss: 1.0361812114715576\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.796197891235352 | KNN Loss: 3.732105255126953 | BCE Loss: 1.0640928745269775\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.677160263061523 | KNN Loss: 3.6706418991088867 | BCE Loss: 1.0065183639526367\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.79538106918335 | KNN Loss: 3.750119924545288 | BCE Loss: 1.0452611446380615\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.695533752441406 | KNN Loss: 3.70202374458313 | BCE Loss: 0.9935102462768555\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.751473426818848 | KNN Loss: 3.7183547019958496 | BCE Loss: 1.0331188440322876\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.746567726135254 | KNN Loss: 3.7363553047180176 | BCE Loss: 1.0102126598358154\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.781336784362793 | KNN Loss: 3.732344150543213 | BCE Loss: 1.048992395401001\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.769314765930176 | KNN Loss: 3.748873233795166 | BCE Loss: 1.0204417705535889\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.727159023284912 | KNN Loss: 3.712946653366089 | BCE Loss: 1.0142124891281128\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.739347457885742 | KNN Loss: 3.7032341957092285 | BCE Loss: 1.0361131429672241\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.698681831359863 | KNN Loss: 3.6914873123168945 | BCE Loss: 1.0071943998336792\n",
      "Epoch   314: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.6999101638793945 | KNN Loss: 3.674152374267578 | BCE Loss: 1.0257580280303955\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.744531631469727 | KNN Loss: 3.725564479827881 | BCE Loss: 1.0189672708511353\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.722592830657959 | KNN Loss: 3.7236363887786865 | BCE Loss: 0.998956561088562\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.737534999847412 | KNN Loss: 3.7167954444885254 | BCE Loss: 1.0207394361495972\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.715803623199463 | KNN Loss: 3.694899320602417 | BCE Loss: 1.0209044218063354\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.744090557098389 | KNN Loss: 3.7209384441375732 | BCE Loss: 1.0231519937515259\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.719273567199707 | KNN Loss: 3.714179277420044 | BCE Loss: 1.005094051361084\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.770649433135986 | KNN Loss: 3.728641986846924 | BCE Loss: 1.042007565498352\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.722599029541016 | KNN Loss: 3.6974241733551025 | BCE Loss: 1.025174856185913\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.746566295623779 | KNN Loss: 3.7250704765319824 | BCE Loss: 1.0214958190917969\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.732806205749512 | KNN Loss: 3.7315750122070312 | BCE Loss: 1.0012309551239014\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.681103229522705 | KNN Loss: 3.6781654357910156 | BCE Loss: 1.0029377937316895\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.720174789428711 | KNN Loss: 3.710297107696533 | BCE Loss: 1.0098779201507568\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.745011806488037 | KNN Loss: 3.7150075435638428 | BCE Loss: 1.0300042629241943\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.67983341217041 | KNN Loss: 3.6803879737854004 | BCE Loss: 0.9994451999664307\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.774426460266113 | KNN Loss: 3.7093405723571777 | BCE Loss: 1.0650861263275146\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.73773193359375 | KNN Loss: 3.715707302093506 | BCE Loss: 1.022024393081665\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.711462020874023 | KNN Loss: 3.683527946472168 | BCE Loss: 1.0279343128204346\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.7310566902160645 | KNN Loss: 3.71921706199646 | BCE Loss: 1.011839509010315\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.7448296546936035 | KNN Loss: 3.715740203857422 | BCE Loss: 1.0290894508361816\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.751291751861572 | KNN Loss: 3.716719388961792 | BCE Loss: 1.0345724821090698\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.672837257385254 | KNN Loss: 3.683889865875244 | BCE Loss: 0.9889472723007202\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.762409210205078 | KNN Loss: 3.73626971244812 | BCE Loss: 1.026139736175537\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.779259204864502 | KNN Loss: 3.7508466243743896 | BCE Loss: 1.0284124612808228\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.692586898803711 | KNN Loss: 3.6777822971343994 | BCE Loss: 1.0148043632507324\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.762270927429199 | KNN Loss: 3.718165159225464 | BCE Loss: 1.0441057682037354\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.73131799697876 | KNN Loss: 3.701535940170288 | BCE Loss: 1.0297821760177612\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.714200496673584 | KNN Loss: 3.7057430744171143 | BCE Loss: 1.0084574222564697\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.668729782104492 | KNN Loss: 3.6754682064056396 | BCE Loss: 0.993261456489563\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.680535793304443 | KNN Loss: 3.6786723136901855 | BCE Loss: 1.0018634796142578\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.71793794631958 | KNN Loss: 3.683609962463379 | BCE Loss: 1.0343278646469116\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.714417457580566 | KNN Loss: 3.6922473907470703 | BCE Loss: 1.022170066833496\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.677195072174072 | KNN Loss: 3.677492141723633 | BCE Loss: 0.9997028112411499\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.70769739151001 | KNN Loss: 3.6875691413879395 | BCE Loss: 1.0201282501220703\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.728260517120361 | KNN Loss: 3.73504638671875 | BCE Loss: 0.9932140111923218\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.758913993835449 | KNN Loss: 3.7337143421173096 | BCE Loss: 1.0251994132995605\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.701163291931152 | KNN Loss: 3.6844756603240967 | BCE Loss: 1.0166873931884766\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.71958065032959 | KNN Loss: 3.7071304321289062 | BCE Loss: 1.0124502182006836\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.7483415603637695 | KNN Loss: 3.7244415283203125 | BCE Loss: 1.023899793624878\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.758203506469727 | KNN Loss: 3.744854211807251 | BCE Loss: 1.013349175453186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.736855983734131 | KNN Loss: 3.724478244781494 | BCE Loss: 1.0123776197433472\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.745788097381592 | KNN Loss: 3.7305126190185547 | BCE Loss: 1.015275478363037\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.706618309020996 | KNN Loss: 3.687767505645752 | BCE Loss: 1.018850564956665\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.713393688201904 | KNN Loss: 3.687021017074585 | BCE Loss: 1.0263726711273193\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.737730979919434 | KNN Loss: 3.7323317527770996 | BCE Loss: 1.005399465560913\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.754436016082764 | KNN Loss: 3.7122859954833984 | BCE Loss: 1.0421500205993652\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.700850963592529 | KNN Loss: 3.6889078617095947 | BCE Loss: 1.0119431018829346\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.694737911224365 | KNN Loss: 3.6849381923675537 | BCE Loss: 1.009799599647522\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.704558849334717 | KNN Loss: 3.7002861499786377 | BCE Loss: 1.0042728185653687\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.705008506774902 | KNN Loss: 3.693028211593628 | BCE Loss: 1.011980414390564\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.725352764129639 | KNN Loss: 3.7033276557922363 | BCE Loss: 1.0220251083374023\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.693538665771484 | KNN Loss: 3.7077949047088623 | BCE Loss: 0.9857436418533325\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.746466636657715 | KNN Loss: 3.7219765186309814 | BCE Loss: 1.0244899988174438\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.741769313812256 | KNN Loss: 3.6981253623962402 | BCE Loss: 1.043643832206726\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.788350582122803 | KNN Loss: 3.7421765327453613 | BCE Loss: 1.0461740493774414\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.726099014282227 | KNN Loss: 3.6991333961486816 | BCE Loss: 1.0269653797149658\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.6932477951049805 | KNN Loss: 3.679009437561035 | BCE Loss: 1.0142383575439453\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.7149658203125 | KNN Loss: 3.693341016769409 | BCE Loss: 1.0216246843338013\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.803110122680664 | KNN Loss: 3.779614210128784 | BCE Loss: 1.023496150970459\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.7527031898498535 | KNN Loss: 3.7176032066345215 | BCE Loss: 1.0351001024246216\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.7160234451293945 | KNN Loss: 3.699714422225952 | BCE Loss: 1.0163092613220215\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.711094856262207 | KNN Loss: 3.711848735809326 | BCE Loss: 0.9992462396621704\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.762113571166992 | KNN Loss: 3.7304108142852783 | BCE Loss: 1.0317027568817139\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.705617904663086 | KNN Loss: 3.712458372116089 | BCE Loss: 0.9931597709655762\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.7741899490356445 | KNN Loss: 3.723125457763672 | BCE Loss: 1.0510644912719727\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.724766731262207 | KNN Loss: 3.7000739574432373 | BCE Loss: 1.0246925354003906\n",
      "Epoch   325: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.731587886810303 | KNN Loss: 3.71356201171875 | BCE Loss: 1.0180258750915527\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.741806507110596 | KNN Loss: 3.716529369354248 | BCE Loss: 1.025277018547058\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.687638282775879 | KNN Loss: 3.661456823348999 | BCE Loss: 1.0261814594268799\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.753633975982666 | KNN Loss: 3.7136762142181396 | BCE Loss: 1.0399576425552368\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.701845169067383 | KNN Loss: 3.6919169425964355 | BCE Loss: 1.0099284648895264\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.762975215911865 | KNN Loss: 3.734104871749878 | BCE Loss: 1.0288703441619873\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.674624919891357 | KNN Loss: 3.664533853530884 | BCE Loss: 1.0100910663604736\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.724488735198975 | KNN Loss: 3.7060351371765137 | BCE Loss: 1.018453598022461\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.728377819061279 | KNN Loss: 3.7181918621063232 | BCE Loss: 1.010185956954956\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.7260942459106445 | KNN Loss: 3.7166404724121094 | BCE Loss: 1.0094537734985352\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.706393241882324 | KNN Loss: 3.6946191787719727 | BCE Loss: 1.0117738246917725\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.766595840454102 | KNN Loss: 3.7305774688720703 | BCE Loss: 1.0360186100006104\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.7578325271606445 | KNN Loss: 3.721402168273926 | BCE Loss: 1.0364302396774292\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.701727390289307 | KNN Loss: 3.6876707077026367 | BCE Loss: 1.01405668258667\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.757303714752197 | KNN Loss: 3.7123775482177734 | BCE Loss: 1.0449260473251343\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.762113571166992 | KNN Loss: 3.696986436843872 | BCE Loss: 1.0651270151138306\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.7390875816345215 | KNN Loss: 3.7017617225646973 | BCE Loss: 1.0373258590698242\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.694060325622559 | KNN Loss: 3.6878979206085205 | BCE Loss: 1.006162166595459\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.724462509155273 | KNN Loss: 3.698413372039795 | BCE Loss: 1.026049256324768\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.703066825866699 | KNN Loss: 3.6983706951141357 | BCE Loss: 1.0046963691711426\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.703362464904785 | KNN Loss: 3.687391996383667 | BCE Loss: 1.0159703493118286\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.6777143478393555 | KNN Loss: 3.6635830402374268 | BCE Loss: 1.0141310691833496\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.7166876792907715 | KNN Loss: 3.6814749240875244 | BCE Loss: 1.035212755203247\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.698050498962402 | KNN Loss: 3.707387685775757 | BCE Loss: 0.9906630516052246\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.728335380554199 | KNN Loss: 3.708122491836548 | BCE Loss: 1.0202126502990723\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.715266227722168 | KNN Loss: 3.682511329650879 | BCE Loss: 1.0327547788619995\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.725482940673828 | KNN Loss: 3.6939034461975098 | BCE Loss: 1.0315792560577393\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.754552364349365 | KNN Loss: 3.7254390716552734 | BCE Loss: 1.0291134119033813\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.726945877075195 | KNN Loss: 3.7008471488952637 | BCE Loss: 1.0260989665985107\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.708495140075684 | KNN Loss: 3.674736976623535 | BCE Loss: 1.0337580442428589\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.719170570373535 | KNN Loss: 3.681678533554077 | BCE Loss: 1.037492036819458\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.749139785766602 | KNN Loss: 3.7011168003082275 | BCE Loss: 1.0480231046676636\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.739603042602539 | KNN Loss: 3.714829206466675 | BCE Loss: 1.0247737169265747\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.720678329467773 | KNN Loss: 3.7267115116119385 | BCE Loss: 0.9939665794372559\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.729586601257324 | KNN Loss: 3.6987855434417725 | BCE Loss: 1.0308008193969727\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.795576095581055 | KNN Loss: 3.750469207763672 | BCE Loss: 1.0451068878173828\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.726935863494873 | KNN Loss: 3.709113836288452 | BCE Loss: 1.0178219079971313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.747918605804443 | KNN Loss: 3.704313039779663 | BCE Loss: 1.0436055660247803\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.689865589141846 | KNN Loss: 3.6858856678009033 | BCE Loss: 1.0039799213409424\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.693680286407471 | KNN Loss: 3.6991167068481445 | BCE Loss: 0.9945634007453918\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.7407026290893555 | KNN Loss: 3.7134084701538086 | BCE Loss: 1.0272942781448364\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.706449508666992 | KNN Loss: 3.6962218284606934 | BCE Loss: 1.0102277994155884\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.723599910736084 | KNN Loss: 3.6896913051605225 | BCE Loss: 1.033908724784851\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.70393180847168 | KNN Loss: 3.7126567363739014 | BCE Loss: 0.9912748336791992\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.7195258140563965 | KNN Loss: 3.6981565952301025 | BCE Loss: 1.021369218826294\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.725622177124023 | KNN Loss: 3.7020468711853027 | BCE Loss: 1.0235755443572998\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.782342433929443 | KNN Loss: 3.724666118621826 | BCE Loss: 1.0576764345169067\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.745356559753418 | KNN Loss: 3.723177433013916 | BCE Loss: 1.022179365158081\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.717623710632324 | KNN Loss: 3.706824541091919 | BCE Loss: 1.0107989311218262\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.759268283843994 | KNN Loss: 3.7386345863342285 | BCE Loss: 1.0206336975097656\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.715078353881836 | KNN Loss: 3.6989362239837646 | BCE Loss: 1.0161422491073608\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.719715118408203 | KNN Loss: 3.6973907947540283 | BCE Loss: 1.0223243236541748\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.751292705535889 | KNN Loss: 3.7170326709747314 | BCE Loss: 1.0342601537704468\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.742992877960205 | KNN Loss: 3.700425386428833 | BCE Loss: 1.0425676107406616\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.708465576171875 | KNN Loss: 3.695992946624756 | BCE Loss: 1.01247239112854\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.744327545166016 | KNN Loss: 3.726487636566162 | BCE Loss: 1.0178399085998535\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.700798511505127 | KNN Loss: 3.6874969005584717 | BCE Loss: 1.0133016109466553\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.740382194519043 | KNN Loss: 3.7204527854919434 | BCE Loss: 1.0199296474456787\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.742534160614014 | KNN Loss: 3.7444565296173096 | BCE Loss: 0.9980775117874146\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.724729061126709 | KNN Loss: 3.7104647159576416 | BCE Loss: 1.014264464378357\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.727326393127441 | KNN Loss: 3.6904940605163574 | BCE Loss: 1.036832571029663\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.698613166809082 | KNN Loss: 3.6962358951568604 | BCE Loss: 1.0023775100708008\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.738308906555176 | KNN Loss: 3.7306833267211914 | BCE Loss: 1.0076258182525635\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.706127166748047 | KNN Loss: 3.735114336013794 | BCE Loss: 0.9710127115249634\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.649784088134766 | KNN Loss: 3.657839298248291 | BCE Loss: 0.9919446110725403\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.735750198364258 | KNN Loss: 3.7008116245269775 | BCE Loss: 1.0349385738372803\n",
      "Epoch   336: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.719020366668701 | KNN Loss: 3.701655149459839 | BCE Loss: 1.0173652172088623\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.687521934509277 | KNN Loss: 3.6890692710876465 | BCE Loss: 0.9984527826309204\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.731222152709961 | KNN Loss: 3.6903538703918457 | BCE Loss: 1.0408685207366943\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.730673789978027 | KNN Loss: 3.7026329040527344 | BCE Loss: 1.028041124343872\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.733970642089844 | KNN Loss: 3.710514783859253 | BCE Loss: 1.0234558582305908\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.757256031036377 | KNN Loss: 3.710653781890869 | BCE Loss: 1.0466022491455078\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.728878021240234 | KNN Loss: 3.7245495319366455 | BCE Loss: 1.0043284893035889\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.768160820007324 | KNN Loss: 3.6943612098693848 | BCE Loss: 1.0737993717193604\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.697081565856934 | KNN Loss: 3.6889212131500244 | BCE Loss: 1.0081605911254883\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.79940128326416 | KNN Loss: 3.7638254165649414 | BCE Loss: 1.0355757474899292\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.781625747680664 | KNN Loss: 3.7294459342956543 | BCE Loss: 1.0521795749664307\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.734165668487549 | KNN Loss: 3.717536687850952 | BCE Loss: 1.0166288614273071\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.776796340942383 | KNN Loss: 3.7688729763031006 | BCE Loss: 1.0079231262207031\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.70197057723999 | KNN Loss: 3.7021279335021973 | BCE Loss: 0.9998427629470825\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.760656356811523 | KNN Loss: 3.6908018589019775 | BCE Loss: 1.069854497909546\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.705573558807373 | KNN Loss: 3.6716034412384033 | BCE Loss: 1.0339702367782593\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.7042341232299805 | KNN Loss: 3.6862246990203857 | BCE Loss: 1.0180094242095947\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.693464279174805 | KNN Loss: 3.6937973499298096 | BCE Loss: 0.9996670484542847\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.735234260559082 | KNN Loss: 3.7166402339935303 | BCE Loss: 1.0185939073562622\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.748542785644531 | KNN Loss: 3.7201998233795166 | BCE Loss: 1.0283432006835938\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.720321178436279 | KNN Loss: 3.7112841606140137 | BCE Loss: 1.0090370178222656\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.723492622375488 | KNN Loss: 3.703197479248047 | BCE Loss: 1.0202951431274414\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.74608850479126 | KNN Loss: 3.718374729156494 | BCE Loss: 1.0277137756347656\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.735990524291992 | KNN Loss: 3.699838638305664 | BCE Loss: 1.0361518859863281\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.714602947235107 | KNN Loss: 3.7275173664093018 | BCE Loss: 0.9870854616165161\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.7050323486328125 | KNN Loss: 3.6954505443573 | BCE Loss: 1.0095818042755127\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.752070426940918 | KNN Loss: 3.7149267196655273 | BCE Loss: 1.0371437072753906\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.75990104675293 | KNN Loss: 3.7139065265655518 | BCE Loss: 1.045994758605957\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.762389183044434 | KNN Loss: 3.745940923690796 | BCE Loss: 1.0164480209350586\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.74766731262207 | KNN Loss: 3.719233989715576 | BCE Loss: 1.0284335613250732\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.746564865112305 | KNN Loss: 3.70656156539917 | BCE Loss: 1.0400030612945557\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.705610275268555 | KNN Loss: 3.6987686157226562 | BCE Loss: 1.0068416595458984\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.738277912139893 | KNN Loss: 3.702327013015747 | BCE Loss: 1.035950779914856\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.766120910644531 | KNN Loss: 3.7286694049835205 | BCE Loss: 1.0374517440795898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.7226881980896 | KNN Loss: 3.708848714828491 | BCE Loss: 1.0138394832611084\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.672100067138672 | KNN Loss: 3.6779356002807617 | BCE Loss: 0.9941645860671997\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.723851680755615 | KNN Loss: 3.7013914585113525 | BCE Loss: 1.0224602222442627\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.720301151275635 | KNN Loss: 3.671569585800171 | BCE Loss: 1.0487314462661743\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.747027397155762 | KNN Loss: 3.750065803527832 | BCE Loss: 0.9969615936279297\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.733222961425781 | KNN Loss: 3.701648473739624 | BCE Loss: 1.0315744876861572\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.701747894287109 | KNN Loss: 3.689814567565918 | BCE Loss: 1.0119333267211914\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.699629783630371 | KNN Loss: 3.7087562084198 | BCE Loss: 0.9908735752105713\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.711835861206055 | KNN Loss: 3.706132173538208 | BCE Loss: 1.0057038068771362\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.708996295928955 | KNN Loss: 3.6849653720855713 | BCE Loss: 1.0240309238433838\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.749405860900879 | KNN Loss: 3.697216272354126 | BCE Loss: 1.0521893501281738\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.722732067108154 | KNN Loss: 3.7178151607513428 | BCE Loss: 1.0049169063568115\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.697115898132324 | KNN Loss: 3.7107608318328857 | BCE Loss: 0.9863550662994385\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.752320289611816 | KNN Loss: 3.707118034362793 | BCE Loss: 1.0452024936676025\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.724264144897461 | KNN Loss: 3.6889379024505615 | BCE Loss: 1.0353262424468994\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.749734878540039 | KNN Loss: 3.7289795875549316 | BCE Loss: 1.0207555294036865\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.73265266418457 | KNN Loss: 3.7355897426605225 | BCE Loss: 0.9970627427101135\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.726161003112793 | KNN Loss: 3.72052264213562 | BCE Loss: 1.005638599395752\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.699759483337402 | KNN Loss: 3.702667713165283 | BCE Loss: 0.9970916509628296\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.759402751922607 | KNN Loss: 3.735414981842041 | BCE Loss: 1.0239877700805664\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.732894420623779 | KNN Loss: 3.728802442550659 | BCE Loss: 1.0040918588638306\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.718448638916016 | KNN Loss: 3.6801867485046387 | BCE Loss: 1.038261890411377\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.7406086921691895 | KNN Loss: 3.7346229553222656 | BCE Loss: 1.0059857368469238\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.727116107940674 | KNN Loss: 3.723597764968872 | BCE Loss: 1.0035183429718018\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.735960960388184 | KNN Loss: 3.664930582046509 | BCE Loss: 1.0710303783416748\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.755679607391357 | KNN Loss: 3.7232930660247803 | BCE Loss: 1.0323866605758667\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.758169174194336 | KNN Loss: 3.728071451187134 | BCE Loss: 1.0300979614257812\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.736157417297363 | KNN Loss: 3.7134037017822266 | BCE Loss: 1.0227535963058472\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.766124248504639 | KNN Loss: 3.764242172241211 | BCE Loss: 1.0018819570541382\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.714428901672363 | KNN Loss: 3.7345614433288574 | BCE Loss: 0.9798673391342163\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.730210304260254 | KNN Loss: 3.704026937484741 | BCE Loss: 1.0261833667755127\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.752209663391113 | KNN Loss: 3.713474750518799 | BCE Loss: 1.038734793663025\n",
      "Epoch   347: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.715669631958008 | KNN Loss: 3.6921920776367188 | BCE Loss: 1.023477554321289\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.726291656494141 | KNN Loss: 3.698725461959839 | BCE Loss: 1.0275661945343018\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.756793975830078 | KNN Loss: 3.7231366634368896 | BCE Loss: 1.0336570739746094\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.823366165161133 | KNN Loss: 3.78995418548584 | BCE Loss: 1.0334117412567139\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.699349880218506 | KNN Loss: 3.672633409500122 | BCE Loss: 1.0267163515090942\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.74941873550415 | KNN Loss: 3.714231252670288 | BCE Loss: 1.0351874828338623\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.743207931518555 | KNN Loss: 3.719136953353882 | BCE Loss: 1.0240707397460938\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.7173309326171875 | KNN Loss: 3.709774971008301 | BCE Loss: 1.0075562000274658\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.759944915771484 | KNN Loss: 3.733295440673828 | BCE Loss: 1.0266495943069458\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.722633361816406 | KNN Loss: 3.698659658432007 | BCE Loss: 1.0239739418029785\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.736223220825195 | KNN Loss: 3.704427480697632 | BCE Loss: 1.0317955017089844\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.7565741539001465 | KNN Loss: 3.7095842361450195 | BCE Loss: 1.046989917755127\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.711498260498047 | KNN Loss: 3.680109977722168 | BCE Loss: 1.0313881635665894\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.703961372375488 | KNN Loss: 3.6876041889190674 | BCE Loss: 1.016357183456421\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.776150703430176 | KNN Loss: 3.759589433670044 | BCE Loss: 1.0165610313415527\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.758710861206055 | KNN Loss: 3.7024788856506348 | BCE Loss: 1.0562320947647095\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.724867820739746 | KNN Loss: 3.7077062129974365 | BCE Loss: 1.0171613693237305\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.7414021492004395 | KNN Loss: 3.7011775970458984 | BCE Loss: 1.040224552154541\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.719785213470459 | KNN Loss: 3.7198712825775146 | BCE Loss: 0.9999139308929443\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.671937942504883 | KNN Loss: 3.663353681564331 | BCE Loss: 1.0085840225219727\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.711385726928711 | KNN Loss: 3.6941092014312744 | BCE Loss: 1.0172767639160156\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.735102653503418 | KNN Loss: 3.7022552490234375 | BCE Loss: 1.032847285270691\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.711179733276367 | KNN Loss: 3.7112176418304443 | BCE Loss: 0.9999620914459229\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.721981048583984 | KNN Loss: 3.7079339027404785 | BCE Loss: 1.0140469074249268\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.750997543334961 | KNN Loss: 3.7282633781433105 | BCE Loss: 1.0227339267730713\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.722293376922607 | KNN Loss: 3.7163965702056885 | BCE Loss: 1.005896806716919\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.699489116668701 | KNN Loss: 3.679929494857788 | BCE Loss: 1.019559621810913\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.725437164306641 | KNN Loss: 3.711806535720825 | BCE Loss: 1.0136306285858154\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.7115397453308105 | KNN Loss: 3.6973016262054443 | BCE Loss: 1.0142382383346558\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.729340553283691 | KNN Loss: 3.6921775341033936 | BCE Loss: 1.037163257598877\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.685787200927734 | KNN Loss: 3.681469678878784 | BCE Loss: 1.004317283630371\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.729238986968994 | KNN Loss: 3.712097644805908 | BCE Loss: 1.017141342163086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.6947760581970215 | KNN Loss: 3.6749894618988037 | BCE Loss: 1.0197867155075073\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.760902404785156 | KNN Loss: 3.74523663520813 | BCE Loss: 1.015665888786316\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.774825096130371 | KNN Loss: 3.7355313301086426 | BCE Loss: 1.0392935276031494\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.720942497253418 | KNN Loss: 3.7027907371520996 | BCE Loss: 1.0181517601013184\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.737308979034424 | KNN Loss: 3.710371732711792 | BCE Loss: 1.0269372463226318\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.728839874267578 | KNN Loss: 3.7028276920318604 | BCE Loss: 1.0260121822357178\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.686656951904297 | KNN Loss: 3.6773147583007812 | BCE Loss: 1.0093424320220947\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.700539588928223 | KNN Loss: 3.682980537414551 | BCE Loss: 1.0175590515136719\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.789599418640137 | KNN Loss: 3.717099666595459 | BCE Loss: 1.0724999904632568\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.737285614013672 | KNN Loss: 3.697141408920288 | BCE Loss: 1.040144443511963\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.735423564910889 | KNN Loss: 3.7083165645599365 | BCE Loss: 1.0271071195602417\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.739013671875 | KNN Loss: 3.7142856121063232 | BCE Loss: 1.0247281789779663\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.732564926147461 | KNN Loss: 3.709540367126465 | BCE Loss: 1.023024320602417\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.727884769439697 | KNN Loss: 3.7247910499572754 | BCE Loss: 1.0030936002731323\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.730991363525391 | KNN Loss: 3.684162139892578 | BCE Loss: 1.046829104423523\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.726787090301514 | KNN Loss: 3.7004013061523438 | BCE Loss: 1.02638578414917\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.683088302612305 | KNN Loss: 3.678414821624756 | BCE Loss: 1.004673719406128\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.784974098205566 | KNN Loss: 3.7742738723754883 | BCE Loss: 1.0107004642486572\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.699063301086426 | KNN Loss: 3.682380199432373 | BCE Loss: 1.0166828632354736\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.695785999298096 | KNN Loss: 3.6860289573669434 | BCE Loss: 1.0097570419311523\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.7425150871276855 | KNN Loss: 3.714388608932495 | BCE Loss: 1.02812659740448\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.7607622146606445 | KNN Loss: 3.720756769180298 | BCE Loss: 1.0400056838989258\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.718845844268799 | KNN Loss: 3.7032787799835205 | BCE Loss: 1.0155670642852783\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.718226909637451 | KNN Loss: 3.696503162384033 | BCE Loss: 1.0217238664627075\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.703385353088379 | KNN Loss: 3.682832956314087 | BCE Loss: 1.020552396774292\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.7150421142578125 | KNN Loss: 3.69822359085083 | BCE Loss: 1.0168185234069824\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.7023606300354 | KNN Loss: 3.6942179203033447 | BCE Loss: 1.0081427097320557\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.694758415222168 | KNN Loss: 3.6728556156158447 | BCE Loss: 1.0219030380249023\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.68906831741333 | KNN Loss: 3.6925411224365234 | BCE Loss: 0.9965270757675171\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.665240287780762 | KNN Loss: 3.6701793670654297 | BCE Loss: 0.9950611591339111\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.675009727478027 | KNN Loss: 3.670060396194458 | BCE Loss: 1.0049493312835693\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.7586140632629395 | KNN Loss: 3.7187392711639404 | BCE Loss: 1.0398746728897095\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.667520999908447 | KNN Loss: 3.678860664367676 | BCE Loss: 0.988660454750061\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.757336616516113 | KNN Loss: 3.7453041076660156 | BCE Loss: 1.0120322704315186\n",
      "Epoch   358: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.69056510925293 | KNN Loss: 3.6814725399017334 | BCE Loss: 1.0090923309326172\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.698922157287598 | KNN Loss: 3.6923577785491943 | BCE Loss: 1.0065642595291138\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.704697132110596 | KNN Loss: 3.6850035190582275 | BCE Loss: 1.0196936130523682\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.69036865234375 | KNN Loss: 3.699275016784668 | BCE Loss: 0.991093635559082\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.765019416809082 | KNN Loss: 3.701298475265503 | BCE Loss: 1.0637208223342896\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.721884250640869 | KNN Loss: 3.7018513679504395 | BCE Loss: 1.0200328826904297\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.690515518188477 | KNN Loss: 3.690021276473999 | BCE Loss: 1.0004944801330566\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.717988967895508 | KNN Loss: 3.676427125930786 | BCE Loss: 1.0415616035461426\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.756196022033691 | KNN Loss: 3.7195560932159424 | BCE Loss: 1.03663969039917\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.724761962890625 | KNN Loss: 3.7232396602630615 | BCE Loss: 1.0015223026275635\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.693910598754883 | KNN Loss: 3.6798274517059326 | BCE Loss: 1.0140831470489502\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.721553802490234 | KNN Loss: 3.694718599319458 | BCE Loss: 1.0268354415893555\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.72752046585083 | KNN Loss: 3.68695068359375 | BCE Loss: 1.0405696630477905\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.729880332946777 | KNN Loss: 3.7123217582702637 | BCE Loss: 1.0175584554672241\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.710038661956787 | KNN Loss: 3.7039904594421387 | BCE Loss: 1.0060480833053589\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.675239562988281 | KNN Loss: 3.6770308017730713 | BCE Loss: 0.9982089400291443\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.737813949584961 | KNN Loss: 3.7033820152282715 | BCE Loss: 1.034432053565979\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.723728656768799 | KNN Loss: 3.710265874862671 | BCE Loss: 1.013462781906128\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.758233070373535 | KNN Loss: 3.7273759841918945 | BCE Loss: 1.0308568477630615\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.747884750366211 | KNN Loss: 3.733780860900879 | BCE Loss: 1.0141041278839111\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.66123104095459 | KNN Loss: 3.6565091609954834 | BCE Loss: 1.0047218799591064\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.705322265625 | KNN Loss: 3.683652400970459 | BCE Loss: 1.0216699838638306\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.698666572570801 | KNN Loss: 3.6805953979492188 | BCE Loss: 1.018071174621582\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.701923847198486 | KNN Loss: 3.678359031677246 | BCE Loss: 1.0235648155212402\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.724275588989258 | KNN Loss: 3.6939611434936523 | BCE Loss: 1.0303144454956055\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.696483612060547 | KNN Loss: 3.7166080474853516 | BCE Loss: 0.9798756837844849\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.705738544464111 | KNN Loss: 3.6858396530151367 | BCE Loss: 1.0198988914489746\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.72233772277832 | KNN Loss: 3.7164511680603027 | BCE Loss: 1.0058866739273071\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.731950759887695 | KNN Loss: 3.7037265300750732 | BCE Loss: 1.028224229812622\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.6909379959106445 | KNN Loss: 3.6691246032714844 | BCE Loss: 1.021813154220581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.722850322723389 | KNN Loss: 3.708188533782959 | BCE Loss: 1.0146616697311401\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.7519073486328125 | KNN Loss: 3.7114431858062744 | BCE Loss: 1.0404644012451172\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.716756820678711 | KNN Loss: 3.7218017578125 | BCE Loss: 0.9949550628662109\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.7560319900512695 | KNN Loss: 3.720256805419922 | BCE Loss: 1.0357754230499268\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.779790878295898 | KNN Loss: 3.7272849082946777 | BCE Loss: 1.0525059700012207\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.745134353637695 | KNN Loss: 3.7289514541625977 | BCE Loss: 1.0161831378936768\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.747990608215332 | KNN Loss: 3.7096035480499268 | BCE Loss: 1.0383872985839844\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.780076026916504 | KNN Loss: 3.7505900859832764 | BCE Loss: 1.029485821723938\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.735238075256348 | KNN Loss: 3.701597213745117 | BCE Loss: 1.033640742301941\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.74393892288208 | KNN Loss: 3.7289934158325195 | BCE Loss: 1.0149455070495605\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.706638336181641 | KNN Loss: 3.6956701278686523 | BCE Loss: 1.0109684467315674\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.684184551239014 | KNN Loss: 3.692570924758911 | BCE Loss: 0.991613507270813\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.72086238861084 | KNN Loss: 3.7017903327941895 | BCE Loss: 1.0190720558166504\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.69224739074707 | KNN Loss: 3.7009575366973877 | BCE Loss: 0.9912896156311035\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.707135200500488 | KNN Loss: 3.6870522499084473 | BCE Loss: 1.0200831890106201\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.741022109985352 | KNN Loss: 3.70218563079834 | BCE Loss: 1.0388367176055908\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.738863945007324 | KNN Loss: 3.706838369369507 | BCE Loss: 1.0320253372192383\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.717391490936279 | KNN Loss: 3.7187016010284424 | BCE Loss: 0.9986897706985474\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.747725486755371 | KNN Loss: 3.727965831756592 | BCE Loss: 1.0197596549987793\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.713801383972168 | KNN Loss: 3.7068283557891846 | BCE Loss: 1.0069732666015625\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.7203898429870605 | KNN Loss: 3.709524631500244 | BCE Loss: 1.0108652114868164\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.746589660644531 | KNN Loss: 3.7638161182403564 | BCE Loss: 0.9827735424041748\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.724452495574951 | KNN Loss: 3.7021803855895996 | BCE Loss: 1.0222721099853516\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.724250316619873 | KNN Loss: 3.697713613510132 | BCE Loss: 1.0265367031097412\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.695354461669922 | KNN Loss: 3.6766083240509033 | BCE Loss: 1.0187458992004395\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.702394962310791 | KNN Loss: 3.6767897605895996 | BCE Loss: 1.0256050825119019\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.771807670593262 | KNN Loss: 3.737896680831909 | BCE Loss: 1.0339112281799316\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.774529457092285 | KNN Loss: 3.7401154041290283 | BCE Loss: 1.0344140529632568\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.724940776824951 | KNN Loss: 3.696500062942505 | BCE Loss: 1.0284408330917358\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.744588851928711 | KNN Loss: 3.694133758544922 | BCE Loss: 1.0504549741744995\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.7423176765441895 | KNN Loss: 3.713045358657837 | BCE Loss: 1.0292723178863525\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.7807159423828125 | KNN Loss: 3.720146417617798 | BCE Loss: 1.0605696439743042\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.696831703186035 | KNN Loss: 3.693833589553833 | BCE Loss: 1.0029979944229126\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.72274112701416 | KNN Loss: 3.6985514163970947 | BCE Loss: 1.0241897106170654\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.733294486999512 | KNN Loss: 3.7078001499176025 | BCE Loss: 1.02549409866333\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.712629795074463 | KNN Loss: 3.6966333389282227 | BCE Loss: 1.0159965753555298\n",
      "Epoch   369: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.741227626800537 | KNN Loss: 3.726630926132202 | BCE Loss: 1.0145965814590454\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.682850360870361 | KNN Loss: 3.694181442260742 | BCE Loss: 0.9886689186096191\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.707066059112549 | KNN Loss: 3.7019574642181396 | BCE Loss: 1.0051085948944092\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.733188629150391 | KNN Loss: 3.708505630493164 | BCE Loss: 1.0246832370758057\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.695483207702637 | KNN Loss: 3.6889240741729736 | BCE Loss: 1.006558895111084\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.722055435180664 | KNN Loss: 3.687239170074463 | BCE Loss: 1.0348165035247803\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.73949670791626 | KNN Loss: 3.712627649307251 | BCE Loss: 1.0268691778182983\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.734509468078613 | KNN Loss: 3.7056918144226074 | BCE Loss: 1.0288174152374268\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.688070774078369 | KNN Loss: 3.673593044281006 | BCE Loss: 1.0144778490066528\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.732583999633789 | KNN Loss: 3.706077814102173 | BCE Loss: 1.0265061855316162\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.746842861175537 | KNN Loss: 3.697296380996704 | BCE Loss: 1.049546480178833\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.70139217376709 | KNN Loss: 3.6866562366485596 | BCE Loss: 1.0147358179092407\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.762179374694824 | KNN Loss: 3.7133684158325195 | BCE Loss: 1.0488109588623047\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.7124128341674805 | KNN Loss: 3.6922948360443115 | BCE Loss: 1.0201177597045898\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.77528715133667 | KNN Loss: 3.7286133766174316 | BCE Loss: 1.0466736555099487\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.7042436599731445 | KNN Loss: 3.6884377002716064 | BCE Loss: 1.0158058404922485\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.750602722167969 | KNN Loss: 3.7162716388702393 | BCE Loss: 1.034331202507019\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.74822998046875 | KNN Loss: 3.7413442134857178 | BCE Loss: 1.0068856477737427\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.735530853271484 | KNN Loss: 3.696002244949341 | BCE Loss: 1.0395288467407227\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.7235612869262695 | KNN Loss: 3.7118449211120605 | BCE Loss: 1.011716365814209\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.728862285614014 | KNN Loss: 3.701906681060791 | BCE Loss: 1.0269556045532227\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.767073154449463 | KNN Loss: 3.717087984085083 | BCE Loss: 1.0499851703643799\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.7159743309021 | KNN Loss: 3.6993815898895264 | BCE Loss: 1.0165926218032837\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.746214866638184 | KNN Loss: 3.7117881774902344 | BCE Loss: 1.0344266891479492\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.740835666656494 | KNN Loss: 3.718140125274658 | BCE Loss: 1.022695541381836\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.7158966064453125 | KNN Loss: 3.7298853397369385 | BCE Loss: 0.9860110282897949\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.719759941101074 | KNN Loss: 3.7166948318481445 | BCE Loss: 1.0030648708343506\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.73824405670166 | KNN Loss: 3.7310755252838135 | BCE Loss: 1.0071687698364258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.708643913269043 | KNN Loss: 3.6894748210906982 | BCE Loss: 1.0191688537597656\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.73189640045166 | KNN Loss: 3.7173259258270264 | BCE Loss: 1.0145704746246338\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.745631217956543 | KNN Loss: 3.7286622524261475 | BCE Loss: 1.0169689655303955\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.734236717224121 | KNN Loss: 3.730241537094116 | BCE Loss: 1.0039949417114258\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.695277214050293 | KNN Loss: 3.68479323387146 | BCE Loss: 1.0104838609695435\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.719077110290527 | KNN Loss: 3.708599328994751 | BCE Loss: 1.0104777812957764\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.7434916496276855 | KNN Loss: 3.7338762283325195 | BCE Loss: 1.0096155405044556\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.736710548400879 | KNN Loss: 3.704347610473633 | BCE Loss: 1.032362699508667\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.738536357879639 | KNN Loss: 3.7044763565063477 | BCE Loss: 1.034060001373291\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.694474220275879 | KNN Loss: 3.6880273818969727 | BCE Loss: 1.0064468383789062\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.699924468994141 | KNN Loss: 3.700188398361206 | BCE Loss: 0.9997360706329346\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.720124244689941 | KNN Loss: 3.677061080932617 | BCE Loss: 1.0430630445480347\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.748396873474121 | KNN Loss: 3.7201595306396484 | BCE Loss: 1.0282375812530518\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.713571548461914 | KNN Loss: 3.698359251022339 | BCE Loss: 1.015212059020996\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.721792221069336 | KNN Loss: 3.6898319721221924 | BCE Loss: 1.0319604873657227\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.7397685050964355 | KNN Loss: 3.7037904262542725 | BCE Loss: 1.0359779596328735\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.703128814697266 | KNN Loss: 3.688413143157959 | BCE Loss: 1.0147157907485962\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.7108330726623535 | KNN Loss: 3.6956586837768555 | BCE Loss: 1.0151742696762085\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.7269182205200195 | KNN Loss: 3.697223424911499 | BCE Loss: 1.02969491481781\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.6974005699157715 | KNN Loss: 3.6747515201568604 | BCE Loss: 1.0226490497589111\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.728271484375 | KNN Loss: 3.6956136226654053 | BCE Loss: 1.0326581001281738\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.702144622802734 | KNN Loss: 3.685389280319214 | BCE Loss: 1.0167551040649414\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.734128952026367 | KNN Loss: 3.7178382873535156 | BCE Loss: 1.0162907838821411\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.725979804992676 | KNN Loss: 3.7068958282470703 | BCE Loss: 1.0190839767456055\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.7088942527771 | KNN Loss: 3.690981149673462 | BCE Loss: 1.0179131031036377\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.7105817794799805 | KNN Loss: 3.698148250579834 | BCE Loss: 1.0124335289001465\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.717439651489258 | KNN Loss: 3.721550941467285 | BCE Loss: 0.9958888292312622\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.738405227661133 | KNN Loss: 3.699023723602295 | BCE Loss: 1.039381742477417\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.739332675933838 | KNN Loss: 3.715641736984253 | BCE Loss: 1.0236910581588745\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.718672752380371 | KNN Loss: 3.710176944732666 | BCE Loss: 1.0084956884384155\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.716085910797119 | KNN Loss: 3.699380397796631 | BCE Loss: 1.0167053937911987\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.7610673904418945 | KNN Loss: 3.7066736221313477 | BCE Loss: 1.0543936491012573\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.751694679260254 | KNN Loss: 3.7030293941497803 | BCE Loss: 1.0486654043197632\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.738580226898193 | KNN Loss: 3.698958158493042 | BCE Loss: 1.039622187614441\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.698007583618164 | KNN Loss: 3.7080793380737305 | BCE Loss: 0.9899280071258545\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.713397979736328 | KNN Loss: 3.7014362812042236 | BCE Loss: 1.0119614601135254\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.754189491271973 | KNN Loss: 3.7249205112457275 | BCE Loss: 1.0292692184448242\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.695550918579102 | KNN Loss: 3.696561098098755 | BCE Loss: 0.9989897012710571\n",
      "Epoch   380: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.696577072143555 | KNN Loss: 3.6909565925598145 | BCE Loss: 1.0056203603744507\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.710532188415527 | KNN Loss: 3.687199592590332 | BCE Loss: 1.0233328342437744\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.719511032104492 | KNN Loss: 3.701857805252075 | BCE Loss: 1.017653226852417\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.721964359283447 | KNN Loss: 3.692272186279297 | BCE Loss: 1.02969229221344\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.722072601318359 | KNN Loss: 3.6990954875946045 | BCE Loss: 1.022977352142334\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.719559192657471 | KNN Loss: 3.708721160888672 | BCE Loss: 1.0108380317687988\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.7489333152771 | KNN Loss: 3.698645830154419 | BCE Loss: 1.0502876043319702\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.701045513153076 | KNN Loss: 3.685991048812866 | BCE Loss: 1.01505446434021\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.735945701599121 | KNN Loss: 3.7193498611450195 | BCE Loss: 1.0165959596633911\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.683365821838379 | KNN Loss: 3.6820390224456787 | BCE Loss: 1.0013269186019897\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.742408275604248 | KNN Loss: 3.694345474243164 | BCE Loss: 1.048062801361084\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.708497047424316 | KNN Loss: 3.718506336212158 | BCE Loss: 0.9899904727935791\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.7161455154418945 | KNN Loss: 3.71150541305542 | BCE Loss: 1.0046398639678955\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.734009265899658 | KNN Loss: 3.7025258541107178 | BCE Loss: 1.0314834117889404\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.729855537414551 | KNN Loss: 3.7385828495025635 | BCE Loss: 0.9912729263305664\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.714535713195801 | KNN Loss: 3.693288803100586 | BCE Loss: 1.0212470293045044\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.754392623901367 | KNN Loss: 3.728186845779419 | BCE Loss: 1.0262060165405273\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.688067436218262 | KNN Loss: 3.6769368648529053 | BCE Loss: 1.0111308097839355\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.729859828948975 | KNN Loss: 3.7263896465301514 | BCE Loss: 1.0034701824188232\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.732799530029297 | KNN Loss: 3.7073028087615967 | BCE Loss: 1.0254966020584106\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.679740905761719 | KNN Loss: 3.6660373210906982 | BCE Loss: 1.0137033462524414\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.726432800292969 | KNN Loss: 3.7183375358581543 | BCE Loss: 1.0080950260162354\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.7229485511779785 | KNN Loss: 3.722288131713867 | BCE Loss: 1.0006603002548218\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.701911926269531 | KNN Loss: 3.677933692932129 | BCE Loss: 1.0239779949188232\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.728171348571777 | KNN Loss: 3.714024066925049 | BCE Loss: 1.0141475200653076\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.7343244552612305 | KNN Loss: 3.7494192123413086 | BCE Loss: 0.9849053025245667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.725707054138184 | KNN Loss: 3.6938588619232178 | BCE Loss: 1.0318479537963867\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.705301761627197 | KNN Loss: 3.6808857917785645 | BCE Loss: 1.0244159698486328\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.7588348388671875 | KNN Loss: 3.7512500286102295 | BCE Loss: 1.007584810256958\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.716248512268066 | KNN Loss: 3.687664747238159 | BCE Loss: 1.0285835266113281\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.756243705749512 | KNN Loss: 3.7247273921966553 | BCE Loss: 1.0315163135528564\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.699306964874268 | KNN Loss: 3.697979211807251 | BCE Loss: 1.0013277530670166\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.739497184753418 | KNN Loss: 3.703273057937622 | BCE Loss: 1.0362238883972168\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.73333215713501 | KNN Loss: 3.7068867683410645 | BCE Loss: 1.0264453887939453\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.781588554382324 | KNN Loss: 3.7190592288970947 | BCE Loss: 1.0625290870666504\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.726619720458984 | KNN Loss: 3.724252939224243 | BCE Loss: 1.0023667812347412\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.665459156036377 | KNN Loss: 3.662933349609375 | BCE Loss: 1.002525806427002\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.7313408851623535 | KNN Loss: 3.71234130859375 | BCE Loss: 1.018999457359314\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.671595573425293 | KNN Loss: 3.6793951988220215 | BCE Loss: 0.992200493812561\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.724987030029297 | KNN Loss: 3.6905357837677 | BCE Loss: 1.0344510078430176\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.741845607757568 | KNN Loss: 3.678224802017212 | BCE Loss: 1.0636208057403564\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.6777849197387695 | KNN Loss: 3.689675807952881 | BCE Loss: 0.9881089925765991\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.710256099700928 | KNN Loss: 3.7063653469085693 | BCE Loss: 1.0038907527923584\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.738096237182617 | KNN Loss: 3.676088571548462 | BCE Loss: 1.0620074272155762\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.712939262390137 | KNN Loss: 3.6963562965393066 | BCE Loss: 1.01658296585083\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.710525035858154 | KNN Loss: 3.6779024600982666 | BCE Loss: 1.0326224565505981\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.749904632568359 | KNN Loss: 3.707530975341797 | BCE Loss: 1.042373776435852\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.73218297958374 | KNN Loss: 3.704780101776123 | BCE Loss: 1.0274029970169067\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.724145412445068 | KNN Loss: 3.6938066482543945 | BCE Loss: 1.0303386449813843\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.684797286987305 | KNN Loss: 3.673128843307495 | BCE Loss: 1.0116684436798096\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.729203224182129 | KNN Loss: 3.7039005756378174 | BCE Loss: 1.0253028869628906\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.74625301361084 | KNN Loss: 3.7287497520446777 | BCE Loss: 1.0175034999847412\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.740667343139648 | KNN Loss: 3.723896026611328 | BCE Loss: 1.0167715549468994\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.6810455322265625 | KNN Loss: 3.66670298576355 | BCE Loss: 1.0143425464630127\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.721480369567871 | KNN Loss: 3.7110142707824707 | BCE Loss: 1.0104660987854004\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.7470784187316895 | KNN Loss: 3.7044358253479004 | BCE Loss: 1.0426424741744995\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.698853015899658 | KNN Loss: 3.693596124649048 | BCE Loss: 1.0052570104599\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.7298784255981445 | KNN Loss: 3.6956632137298584 | BCE Loss: 1.034214973449707\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.72257137298584 | KNN Loss: 3.7165379524230957 | BCE Loss: 1.0060336589813232\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.6957831382751465 | KNN Loss: 3.6788015365600586 | BCE Loss: 1.016981601715088\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.685372352600098 | KNN Loss: 3.696364164352417 | BCE Loss: 0.9890080690383911\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.715765476226807 | KNN Loss: 3.69980788230896 | BCE Loss: 1.0159577131271362\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.741762161254883 | KNN Loss: 3.7174599170684814 | BCE Loss: 1.0243022441864014\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.737802982330322 | KNN Loss: 3.7097280025482178 | BCE Loss: 1.028075098991394\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.735289573669434 | KNN Loss: 3.714369058609009 | BCE Loss: 1.0209206342697144\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.771765232086182 | KNN Loss: 3.763228178024292 | BCE Loss: 1.0085370540618896\n",
      "Epoch   391: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.727222442626953 | KNN Loss: 3.72157883644104 | BCE Loss: 1.005643367767334\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.73618745803833 | KNN Loss: 3.70858097076416 | BCE Loss: 1.0276066064834595\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.705848693847656 | KNN Loss: 3.6741671562194824 | BCE Loss: 1.0316815376281738\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.7452287673950195 | KNN Loss: 3.7228822708129883 | BCE Loss: 1.0223464965820312\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.716620922088623 | KNN Loss: 3.686955690383911 | BCE Loss: 1.0296651124954224\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.727532863616943 | KNN Loss: 3.7066609859466553 | BCE Loss: 1.020871877670288\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.791788101196289 | KNN Loss: 3.7273943424224854 | BCE Loss: 1.0643935203552246\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.723747730255127 | KNN Loss: 3.715275287628174 | BCE Loss: 1.0084725618362427\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.716414451599121 | KNN Loss: 3.6908864974975586 | BCE Loss: 1.0255277156829834\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.688348770141602 | KNN Loss: 3.709379196166992 | BCE Loss: 0.9789693355560303\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.726700782775879 | KNN Loss: 3.7143964767456055 | BCE Loss: 1.0123040676116943\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.734649658203125 | KNN Loss: 3.7120208740234375 | BCE Loss: 1.0226285457611084\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.7667236328125 | KNN Loss: 3.730156421661377 | BCE Loss: 1.036566972732544\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.72876501083374 | KNN Loss: 3.695897102355957 | BCE Loss: 1.0328677892684937\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.673372268676758 | KNN Loss: 3.6725995540618896 | BCE Loss: 1.0007728338241577\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.793231010437012 | KNN Loss: 3.754718780517578 | BCE Loss: 1.0385122299194336\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.713735580444336 | KNN Loss: 3.70595383644104 | BCE Loss: 1.0077815055847168\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.742645740509033 | KNN Loss: 3.7149925231933594 | BCE Loss: 1.0276532173156738\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.698894500732422 | KNN Loss: 3.703795909881592 | BCE Loss: 0.9950985908508301\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.76027250289917 | KNN Loss: 3.7255733013153076 | BCE Loss: 1.0346992015838623\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.7671589851379395 | KNN Loss: 3.7256886959075928 | BCE Loss: 1.0414704084396362\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.793405532836914 | KNN Loss: 3.7548606395721436 | BCE Loss: 1.0385451316833496\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.682272911071777 | KNN Loss: 3.6714107990264893 | BCE Loss: 1.010862112045288\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.732343673706055 | KNN Loss: 3.682774066925049 | BCE Loss: 1.0495697259902954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.728349685668945 | KNN Loss: 3.71309232711792 | BCE Loss: 1.0152573585510254\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.735073566436768 | KNN Loss: 3.713737726211548 | BCE Loss: 1.0213358402252197\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.696706295013428 | KNN Loss: 3.681657314300537 | BCE Loss: 1.0150489807128906\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.7346320152282715 | KNN Loss: 3.7257397174835205 | BCE Loss: 1.0088921785354614\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.738533020019531 | KNN Loss: 3.7023513317108154 | BCE Loss: 1.0361816883087158\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.736053943634033 | KNN Loss: 3.693506956100464 | BCE Loss: 1.0425468683242798\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.716411113739014 | KNN Loss: 3.685304641723633 | BCE Loss: 1.0311065912246704\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.726442337036133 | KNN Loss: 3.7217354774475098 | BCE Loss: 1.0047070980072021\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.711135387420654 | KNN Loss: 3.6920065879821777 | BCE Loss: 1.019128680229187\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.727778434753418 | KNN Loss: 3.7231366634368896 | BCE Loss: 1.0046415328979492\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.726521968841553 | KNN Loss: 3.7135791778564453 | BCE Loss: 1.0129427909851074\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.728510856628418 | KNN Loss: 3.6912682056427 | BCE Loss: 1.0372424125671387\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.720221519470215 | KNN Loss: 3.70064115524292 | BCE Loss: 1.0195802450180054\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.728923797607422 | KNN Loss: 3.6970043182373047 | BCE Loss: 1.031919240951538\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.723934173583984 | KNN Loss: 3.716273784637451 | BCE Loss: 1.0076605081558228\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.746471405029297 | KNN Loss: 3.7329633235931396 | BCE Loss: 1.0135083198547363\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.750784397125244 | KNN Loss: 3.7049601078033447 | BCE Loss: 1.0458242893218994\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.814685821533203 | KNN Loss: 3.758671998977661 | BCE Loss: 1.056014060974121\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.725128173828125 | KNN Loss: 3.7031219005584717 | BCE Loss: 1.0220062732696533\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.748164176940918 | KNN Loss: 3.722986936569214 | BCE Loss: 1.025177240371704\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.751379489898682 | KNN Loss: 3.7305378913879395 | BCE Loss: 1.0208417177200317\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.659989833831787 | KNN Loss: 3.6815128326416016 | BCE Loss: 0.9784771800041199\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.763216972351074 | KNN Loss: 3.694599151611328 | BCE Loss: 1.0686177015304565\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.694792747497559 | KNN Loss: 3.6899352073669434 | BCE Loss: 1.0048575401306152\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.693292140960693 | KNN Loss: 3.691995143890381 | BCE Loss: 1.001297116279602\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.710034370422363 | KNN Loss: 3.690735340118408 | BCE Loss: 1.019299030303955\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.659367084503174 | KNN Loss: 3.6605687141418457 | BCE Loss: 0.9987983107566833\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.703354835510254 | KNN Loss: 3.7052841186523438 | BCE Loss: 0.9980705976486206\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.71363639831543 | KNN Loss: 3.698178768157959 | BCE Loss: 1.0154578685760498\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.75667667388916 | KNN Loss: 3.730787515640259 | BCE Loss: 1.0258893966674805\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.696558952331543 | KNN Loss: 3.6666126251220703 | BCE Loss: 1.0299460887908936\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.7393951416015625 | KNN Loss: 3.7125706672668457 | BCE Loss: 1.0268242359161377\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.733384132385254 | KNN Loss: 3.6999311447143555 | BCE Loss: 1.033453106880188\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.732047080993652 | KNN Loss: 3.7121496200561523 | BCE Loss: 1.019897699356079\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.697478294372559 | KNN Loss: 3.694870948791504 | BCE Loss: 1.0026074647903442\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.760685920715332 | KNN Loss: 3.699524402618408 | BCE Loss: 1.061161756515503\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.686201095581055 | KNN Loss: 3.692718744277954 | BCE Loss: 0.9934821724891663\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.690613746643066 | KNN Loss: 3.6786935329437256 | BCE Loss: 1.0119199752807617\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.692708969116211 | KNN Loss: 3.6875176429748535 | BCE Loss: 1.0051910877227783\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.7296013832092285 | KNN Loss: 3.7110424041748047 | BCE Loss: 1.0185588598251343\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.740680694580078 | KNN Loss: 3.715569496154785 | BCE Loss: 1.0251110792160034\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.74500036239624 | KNN Loss: 3.7163333892822266 | BCE Loss: 1.0286669731140137\n",
      "Epoch   402: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.734189033508301 | KNN Loss: 3.732677698135376 | BCE Loss: 1.0015110969543457\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.761387825012207 | KNN Loss: 3.7062227725982666 | BCE Loss: 1.0551648139953613\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.725997447967529 | KNN Loss: 3.6742055416107178 | BCE Loss: 1.051791787147522\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.722085952758789 | KNN Loss: 3.702150583267212 | BCE Loss: 1.0199356079101562\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.701005935668945 | KNN Loss: 3.691594362258911 | BCE Loss: 1.0094115734100342\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.7623395919799805 | KNN Loss: 3.7269835472106934 | BCE Loss: 1.035355806350708\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.7438554763793945 | KNN Loss: 3.7246713638305664 | BCE Loss: 1.019183874130249\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.6920270919799805 | KNN Loss: 3.6833958625793457 | BCE Loss: 1.0086309909820557\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.7459282875061035 | KNN Loss: 3.713954448699951 | BCE Loss: 1.0319737195968628\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.707579612731934 | KNN Loss: 3.6845483779907227 | BCE Loss: 1.0230313539505005\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.714076042175293 | KNN Loss: 3.6852986812591553 | BCE Loss: 1.0287771224975586\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.744567394256592 | KNN Loss: 3.721501350402832 | BCE Loss: 1.0230659246444702\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.740421772003174 | KNN Loss: 3.72379207611084 | BCE Loss: 1.016629695892334\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.751465797424316 | KNN Loss: 3.7096543312072754 | BCE Loss: 1.041811227798462\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.713540077209473 | KNN Loss: 3.691936731338501 | BCE Loss: 1.0216032266616821\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.759072303771973 | KNN Loss: 3.7233405113220215 | BCE Loss: 1.0357317924499512\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.778847694396973 | KNN Loss: 3.7401957511901855 | BCE Loss: 1.038651943206787\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.713374137878418 | KNN Loss: 3.6856892108917236 | BCE Loss: 1.0276849269866943\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.735118389129639 | KNN Loss: 3.707526445388794 | BCE Loss: 1.0275918245315552\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.7407026290893555 | KNN Loss: 3.7266016006469727 | BCE Loss: 1.0141010284423828\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.704127311706543 | KNN Loss: 3.6920979022979736 | BCE Loss: 1.0120294094085693\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.744664669036865 | KNN Loss: 3.714695930480957 | BCE Loss: 1.0299687385559082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.747786998748779 | KNN Loss: 3.698589324951172 | BCE Loss: 1.0491976737976074\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.723831653594971 | KNN Loss: 3.726553440093994 | BCE Loss: 0.9972783327102661\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.7312517166137695 | KNN Loss: 3.7239890098571777 | BCE Loss: 1.0072624683380127\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.723222255706787 | KNN Loss: 3.719123601913452 | BCE Loss: 1.0040987730026245\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.71256160736084 | KNN Loss: 3.6877942085266113 | BCE Loss: 1.0247673988342285\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.734351634979248 | KNN Loss: 3.7214672565460205 | BCE Loss: 1.012884497642517\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.70643424987793 | KNN Loss: 3.68967342376709 | BCE Loss: 1.0167605876922607\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.746150493621826 | KNN Loss: 3.7100167274475098 | BCE Loss: 1.0361337661743164\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.72859525680542 | KNN Loss: 3.71201491355896 | BCE Loss: 1.0165802240371704\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.771071434020996 | KNN Loss: 3.743952751159668 | BCE Loss: 1.027118444442749\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.73563289642334 | KNN Loss: 3.7147347927093506 | BCE Loss: 1.0208981037139893\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.696327209472656 | KNN Loss: 3.701749801635742 | BCE Loss: 0.9945774078369141\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.733802795410156 | KNN Loss: 3.719665288925171 | BCE Loss: 1.0141372680664062\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.715893268585205 | KNN Loss: 3.725395679473877 | BCE Loss: 0.9904975295066833\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.750186920166016 | KNN Loss: 3.7103445529937744 | BCE Loss: 1.0398422479629517\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.703132152557373 | KNN Loss: 3.6936442852020264 | BCE Loss: 1.0094878673553467\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.713861465454102 | KNN Loss: 3.6865553855895996 | BCE Loss: 1.027306318283081\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.7476606369018555 | KNN Loss: 3.7236509323120117 | BCE Loss: 1.0240097045898438\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.764832019805908 | KNN Loss: 3.7379250526428223 | BCE Loss: 1.0269068479537964\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.6930341720581055 | KNN Loss: 3.679830312728882 | BCE Loss: 1.0132038593292236\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.711254119873047 | KNN Loss: 3.690836191177368 | BCE Loss: 1.0204176902770996\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.734781742095947 | KNN Loss: 3.7088990211486816 | BCE Loss: 1.0258827209472656\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.704911231994629 | KNN Loss: 3.6765472888946533 | BCE Loss: 1.0283637046813965\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.684535026550293 | KNN Loss: 3.6896963119506836 | BCE Loss: 0.9948386549949646\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.706940650939941 | KNN Loss: 3.6987786293029785 | BCE Loss: 1.0081621408462524\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.739312171936035 | KNN Loss: 3.7215936183929443 | BCE Loss: 1.0177184343338013\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.6705708503723145 | KNN Loss: 3.684054136276245 | BCE Loss: 0.986516535282135\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.677346229553223 | KNN Loss: 3.6954474449157715 | BCE Loss: 0.9818989038467407\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.70613431930542 | KNN Loss: 3.6977338790893555 | BCE Loss: 1.0084004402160645\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.726742267608643 | KNN Loss: 3.694624185562134 | BCE Loss: 1.0321180820465088\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.6962995529174805 | KNN Loss: 3.710444688796997 | BCE Loss: 0.9858551025390625\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.763027191162109 | KNN Loss: 3.7167043685913086 | BCE Loss: 1.0463225841522217\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.681722640991211 | KNN Loss: 3.678727388381958 | BCE Loss: 1.002995252609253\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.75823974609375 | KNN Loss: 3.7010066509246826 | BCE Loss: 1.0572328567504883\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.713544845581055 | KNN Loss: 3.7191803455352783 | BCE Loss: 0.9943647384643555\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.746295928955078 | KNN Loss: 3.7117607593536377 | BCE Loss: 1.0345354080200195\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.71813440322876 | KNN Loss: 3.719376802444458 | BCE Loss: 0.9987577199935913\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.717088222503662 | KNN Loss: 3.706000804901123 | BCE Loss: 1.011087417602539\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.723177433013916 | KNN Loss: 3.7162232398986816 | BCE Loss: 1.0069541931152344\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.778547763824463 | KNN Loss: 3.7419474124908447 | BCE Loss: 1.0366003513336182\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.748305320739746 | KNN Loss: 3.712125301361084 | BCE Loss: 1.036180019378662\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.709990501403809 | KNN Loss: 3.720203399658203 | BCE Loss: 0.989787220954895\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.720746040344238 | KNN Loss: 3.7181384563446045 | BCE Loss: 1.0026073455810547\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.774299144744873 | KNN Loss: 3.7090940475463867 | BCE Loss: 1.0652049779891968\n",
      "Epoch   413: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.735664367675781 | KNN Loss: 3.7183873653411865 | BCE Loss: 1.0172770023345947\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.738292217254639 | KNN Loss: 3.713676929473877 | BCE Loss: 1.0246152877807617\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.724578857421875 | KNN Loss: 3.727815866470337 | BCE Loss: 0.9967632293701172\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.708256721496582 | KNN Loss: 3.697425127029419 | BCE Loss: 1.0108314752578735\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.737715244293213 | KNN Loss: 3.729218006134033 | BCE Loss: 1.0084971189498901\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.704921722412109 | KNN Loss: 3.6907870769500732 | BCE Loss: 1.0141348838806152\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.726613521575928 | KNN Loss: 3.708584785461426 | BCE Loss: 1.0180288553237915\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.736993789672852 | KNN Loss: 3.718919277191162 | BCE Loss: 1.0180745124816895\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.766269683837891 | KNN Loss: 3.731292724609375 | BCE Loss: 1.0349769592285156\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.728338718414307 | KNN Loss: 3.7090892791748047 | BCE Loss: 1.019249439239502\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.764794826507568 | KNN Loss: 3.7330734729766846 | BCE Loss: 1.0317212343215942\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.761821746826172 | KNN Loss: 3.7200634479522705 | BCE Loss: 1.0417582988739014\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.769475936889648 | KNN Loss: 3.7467734813690186 | BCE Loss: 1.0227025747299194\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.713551998138428 | KNN Loss: 3.7142348289489746 | BCE Loss: 0.9993172883987427\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.762336254119873 | KNN Loss: 3.726637125015259 | BCE Loss: 1.0356991291046143\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.771440505981445 | KNN Loss: 3.746619462966919 | BCE Loss: 1.0248208045959473\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.714111804962158 | KNN Loss: 3.70507550239563 | BCE Loss: 1.0090364217758179\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.705333232879639 | KNN Loss: 3.6814284324645996 | BCE Loss: 1.0239046812057495\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.6850128173828125 | KNN Loss: 3.6822872161865234 | BCE Loss: 1.0027258396148682\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.755674362182617 | KNN Loss: 3.7206602096557617 | BCE Loss: 1.0350143909454346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.705761909484863 | KNN Loss: 3.7020103931427 | BCE Loss: 1.003751516342163\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.718288421630859 | KNN Loss: 3.6987667083740234 | BCE Loss: 1.019521951675415\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.730375289916992 | KNN Loss: 3.707319974899292 | BCE Loss: 1.0230554342269897\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.7313642501831055 | KNN Loss: 3.7257070541381836 | BCE Loss: 1.005657434463501\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.709545612335205 | KNN Loss: 3.7183635234832764 | BCE Loss: 0.9911822080612183\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.6713361740112305 | KNN Loss: 3.6820507049560547 | BCE Loss: 0.9892855882644653\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.73178243637085 | KNN Loss: 3.692059278488159 | BCE Loss: 1.0397231578826904\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.707244396209717 | KNN Loss: 3.694640874862671 | BCE Loss: 1.0126036405563354\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.729675769805908 | KNN Loss: 3.7152645587921143 | BCE Loss: 1.0144110918045044\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.658383369445801 | KNN Loss: 3.6605896949768066 | BCE Loss: 0.9977935552597046\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.7356977462768555 | KNN Loss: 3.703821897506714 | BCE Loss: 1.0318760871887207\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.728918075561523 | KNN Loss: 3.710597038269043 | BCE Loss: 1.0183210372924805\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.714053153991699 | KNN Loss: 3.698019027709961 | BCE Loss: 1.0160338878631592\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.75723934173584 | KNN Loss: 3.7252025604248047 | BCE Loss: 1.0320367813110352\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.74356746673584 | KNN Loss: 3.734017848968506 | BCE Loss: 1.0095493793487549\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.724367618560791 | KNN Loss: 3.6997432708740234 | BCE Loss: 1.0246243476867676\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.737340927124023 | KNN Loss: 3.706118106842041 | BCE Loss: 1.0312225818634033\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.722589015960693 | KNN Loss: 3.6912147998809814 | BCE Loss: 1.031374216079712\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.660706520080566 | KNN Loss: 3.6950466632843018 | BCE Loss: 0.9656600952148438\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.725734710693359 | KNN Loss: 3.6943132877349854 | BCE Loss: 1.0314213037490845\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.7212066650390625 | KNN Loss: 3.7203118801116943 | BCE Loss: 1.000894546508789\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.724856376647949 | KNN Loss: 3.727125406265259 | BCE Loss: 0.9977307915687561\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.698220252990723 | KNN Loss: 3.6868956089019775 | BCE Loss: 1.0113248825073242\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.742751121520996 | KNN Loss: 3.6984236240386963 | BCE Loss: 1.0443273782730103\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.721206188201904 | KNN Loss: 3.6773629188537598 | BCE Loss: 1.0438432693481445\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.774202823638916 | KNN Loss: 3.7358391284942627 | BCE Loss: 1.0383635759353638\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.700801849365234 | KNN Loss: 3.7044897079467773 | BCE Loss: 0.996312141418457\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.730175018310547 | KNN Loss: 3.7218332290649414 | BCE Loss: 1.0083415508270264\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.762004375457764 | KNN Loss: 3.750149965286255 | BCE Loss: 1.0118544101715088\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.7132792472839355 | KNN Loss: 3.716235399246216 | BCE Loss: 0.997044026851654\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.722398281097412 | KNN Loss: 3.706652879714966 | BCE Loss: 1.0157455205917358\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.733642101287842 | KNN Loss: 3.703324794769287 | BCE Loss: 1.0303173065185547\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.720776081085205 | KNN Loss: 3.7070624828338623 | BCE Loss: 1.0137135982513428\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.6963067054748535 | KNN Loss: 3.674428939819336 | BCE Loss: 1.0218777656555176\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.686291694641113 | KNN Loss: 3.688112497329712 | BCE Loss: 0.9981789588928223\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.715037822723389 | KNN Loss: 3.6827588081359863 | BCE Loss: 1.0322790145874023\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.709197521209717 | KNN Loss: 3.6921844482421875 | BCE Loss: 1.0170130729675293\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.743216514587402 | KNN Loss: 3.71679949760437 | BCE Loss: 1.0264167785644531\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.732013702392578 | KNN Loss: 3.7059149742126465 | BCE Loss: 1.0260988473892212\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.755970001220703 | KNN Loss: 3.7276222705841064 | BCE Loss: 1.0283474922180176\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.7510247230529785 | KNN Loss: 3.707913875579834 | BCE Loss: 1.0431108474731445\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.743076324462891 | KNN Loss: 3.737713098526001 | BCE Loss: 1.0053631067276\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.712576389312744 | KNN Loss: 3.7136242389678955 | BCE Loss: 0.9989521503448486\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.738394260406494 | KNN Loss: 3.683025360107422 | BCE Loss: 1.0553689002990723\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.753311634063721 | KNN Loss: 3.7128970623016357 | BCE Loss: 1.040414571762085\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.751230239868164 | KNN Loss: 3.73540997505188 | BCE Loss: 1.015820026397705\n",
      "Epoch   424: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.743931293487549 | KNN Loss: 3.73053240776062 | BCE Loss: 1.0133987665176392\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.74215030670166 | KNN Loss: 3.70320987701416 | BCE Loss: 1.0389405488967896\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.757007122039795 | KNN Loss: 3.719388484954834 | BCE Loss: 1.037618637084961\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.699052333831787 | KNN Loss: 3.685807228088379 | BCE Loss: 1.0132449865341187\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.690740585327148 | KNN Loss: 3.6966583728790283 | BCE Loss: 0.9940823912620544\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.736681938171387 | KNN Loss: 3.6914987564086914 | BCE Loss: 1.0451831817626953\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.704282283782959 | KNN Loss: 3.6959080696105957 | BCE Loss: 1.0083740949630737\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.7331624031066895 | KNN Loss: 3.722956895828247 | BCE Loss: 1.0102055072784424\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.697475910186768 | KNN Loss: 3.6887598037719727 | BCE Loss: 1.0087159872055054\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.746612548828125 | KNN Loss: 3.705604314804077 | BCE Loss: 1.0410081148147583\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.681631088256836 | KNN Loss: 3.701120138168335 | BCE Loss: 0.9805108308792114\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.715554237365723 | KNN Loss: 3.7005865573883057 | BCE Loss: 1.0149677991867065\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.736032962799072 | KNN Loss: 3.6819121837615967 | BCE Loss: 1.0541207790374756\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.740199565887451 | KNN Loss: 3.696948289871216 | BCE Loss: 1.043251395225525\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.758795738220215 | KNN Loss: 3.7049312591552734 | BCE Loss: 1.0538647174835205\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.7099199295043945 | KNN Loss: 3.684182643890381 | BCE Loss: 1.0257372856140137\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.676446437835693 | KNN Loss: 3.6843323707580566 | BCE Loss: 0.9921139478683472\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.737120151519775 | KNN Loss: 3.7110445499420166 | BCE Loss: 1.0260756015777588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.7531418800354 | KNN Loss: 3.7138781547546387 | BCE Loss: 1.0392636060714722\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.743902683258057 | KNN Loss: 3.710149049758911 | BCE Loss: 1.033753514289856\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.715070724487305 | KNN Loss: 3.7111356258392334 | BCE Loss: 1.0039349794387817\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.768191814422607 | KNN Loss: 3.745206594467163 | BCE Loss: 1.0229852199554443\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.698960781097412 | KNN Loss: 3.6769661903381348 | BCE Loss: 1.0219945907592773\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.7117390632629395 | KNN Loss: 3.7186484336853027 | BCE Loss: 0.9930906891822815\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.717439651489258 | KNN Loss: 3.697937250137329 | BCE Loss: 1.0195026397705078\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.7236833572387695 | KNN Loss: 3.7015960216522217 | BCE Loss: 1.0220870971679688\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.725886821746826 | KNN Loss: 3.7012746334075928 | BCE Loss: 1.0246121883392334\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.745204448699951 | KNN Loss: 3.7480669021606445 | BCE Loss: 0.9971374869346619\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.738903999328613 | KNN Loss: 3.7222535610198975 | BCE Loss: 1.0166504383087158\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.689919471740723 | KNN Loss: 3.67425799369812 | BCE Loss: 1.0156617164611816\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.750024795532227 | KNN Loss: 3.692166566848755 | BCE Loss: 1.0578579902648926\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.719198703765869 | KNN Loss: 3.711284875869751 | BCE Loss: 1.0079139471054077\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.725228786468506 | KNN Loss: 3.7065656185150146 | BCE Loss: 1.0186631679534912\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.746323585510254 | KNN Loss: 3.705920934677124 | BCE Loss: 1.0404024124145508\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.710007190704346 | KNN Loss: 3.710277557373047 | BCE Loss: 0.9997296333312988\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.724387168884277 | KNN Loss: 3.698838949203491 | BCE Loss: 1.0255484580993652\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.686978340148926 | KNN Loss: 3.673283338546753 | BCE Loss: 1.0136947631835938\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.752048969268799 | KNN Loss: 3.7287490367889404 | BCE Loss: 1.0232999324798584\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.756213188171387 | KNN Loss: 3.7076025009155273 | BCE Loss: 1.0486106872558594\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.728879928588867 | KNN Loss: 3.69490122795105 | BCE Loss: 1.033978819847107\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.717197418212891 | KNN Loss: 3.7132411003112793 | BCE Loss: 1.0039560794830322\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.768094062805176 | KNN Loss: 3.6925058364868164 | BCE Loss: 1.0755884647369385\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.751865386962891 | KNN Loss: 3.732933759689331 | BCE Loss: 1.0189313888549805\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.748648643493652 | KNN Loss: 3.721176862716675 | BCE Loss: 1.0274720191955566\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.750367164611816 | KNN Loss: 3.728154182434082 | BCE Loss: 1.0222129821777344\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.729881286621094 | KNN Loss: 3.717613458633423 | BCE Loss: 1.0122679471969604\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.693211078643799 | KNN Loss: 3.684541940689087 | BCE Loss: 1.008669137954712\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.717836380004883 | KNN Loss: 3.6886284351348877 | BCE Loss: 1.0292079448699951\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.763973712921143 | KNN Loss: 3.747823715209961 | BCE Loss: 1.016149878501892\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.738992691040039 | KNN Loss: 3.7187414169311523 | BCE Loss: 1.0202512741088867\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.765254974365234 | KNN Loss: 3.752281904220581 | BCE Loss: 1.0129729509353638\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.73901891708374 | KNN Loss: 3.7018914222717285 | BCE Loss: 1.0371274948120117\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.735901355743408 | KNN Loss: 3.710660934448242 | BCE Loss: 1.025240421295166\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.761240005493164 | KNN Loss: 3.7151105403900146 | BCE Loss: 1.0461294651031494\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.711372375488281 | KNN Loss: 3.6969637870788574 | BCE Loss: 1.0144083499908447\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.7663445472717285 | KNN Loss: 3.744953155517578 | BCE Loss: 1.0213913917541504\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.702509880065918 | KNN Loss: 3.7051122188568115 | BCE Loss: 0.9973974823951721\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.727684020996094 | KNN Loss: 3.694633722305298 | BCE Loss: 1.033050298690796\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.688432216644287 | KNN Loss: 3.6992342472076416 | BCE Loss: 0.9891981482505798\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.767329216003418 | KNN Loss: 3.735220432281494 | BCE Loss: 1.032109022140503\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.721705913543701 | KNN Loss: 3.690009117126465 | BCE Loss: 1.0316967964172363\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.749210357666016 | KNN Loss: 3.7150979042053223 | BCE Loss: 1.034112572669983\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.722055912017822 | KNN Loss: 3.708690643310547 | BCE Loss: 1.0133652687072754\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.701421737670898 | KNN Loss: 3.678292751312256 | BCE Loss: 1.0231292247772217\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.693093776702881 | KNN Loss: 3.6611862182617188 | BCE Loss: 1.0319074392318726\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.7145538330078125 | KNN Loss: 3.6969621181488037 | BCE Loss: 1.0175914764404297\n",
      "Epoch   435: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.762326240539551 | KNN Loss: 3.7242939472198486 | BCE Loss: 1.0380325317382812\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.775463104248047 | KNN Loss: 3.7675600051879883 | BCE Loss: 1.007902979850769\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.696678638458252 | KNN Loss: 3.6988492012023926 | BCE Loss: 0.9978294372558594\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.677721977233887 | KNN Loss: 3.6723573207855225 | BCE Loss: 1.0053647756576538\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.736003875732422 | KNN Loss: 3.7068564891815186 | BCE Loss: 1.0291471481323242\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.664093494415283 | KNN Loss: 3.6773834228515625 | BCE Loss: 0.9867100715637207\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.7674560546875 | KNN Loss: 3.724191188812256 | BCE Loss: 1.043264627456665\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.72174072265625 | KNN Loss: 3.685260772705078 | BCE Loss: 1.0364799499511719\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.755172252655029 | KNN Loss: 3.7317192554473877 | BCE Loss: 1.0234529972076416\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.691963195800781 | KNN Loss: 3.712224245071411 | BCE Loss: 0.9797387719154358\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.716446399688721 | KNN Loss: 3.6987311840057373 | BCE Loss: 1.0177152156829834\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.800002574920654 | KNN Loss: 3.717341899871826 | BCE Loss: 1.0826606750488281\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.773495197296143 | KNN Loss: 3.7627246379852295 | BCE Loss: 1.0107706785202026\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.7075090408325195 | KNN Loss: 3.7063543796539307 | BCE Loss: 1.0011547803878784\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.737398147583008 | KNN Loss: 3.733332872390747 | BCE Loss: 1.0040652751922607\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.704257488250732 | KNN Loss: 3.703317165374756 | BCE Loss: 1.0009404420852661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.741335868835449 | KNN Loss: 3.726243257522583 | BCE Loss: 1.0150928497314453\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.785909175872803 | KNN Loss: 3.7275731563568115 | BCE Loss: 1.0583359003067017\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.708998203277588 | KNN Loss: 3.6945319175720215 | BCE Loss: 1.0144662857055664\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.715358734130859 | KNN Loss: 3.6989035606384277 | BCE Loss: 1.0164551734924316\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.741302967071533 | KNN Loss: 3.7196907997131348 | BCE Loss: 1.0216121673583984\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.79486608505249 | KNN Loss: 3.7653303146362305 | BCE Loss: 1.0295357704162598\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.73509407043457 | KNN Loss: 3.7317440509796143 | BCE Loss: 1.0033502578735352\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.703238010406494 | KNN Loss: 3.684605121612549 | BCE Loss: 1.0186328887939453\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.711285591125488 | KNN Loss: 3.7060608863830566 | BCE Loss: 1.0052249431610107\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.715819358825684 | KNN Loss: 3.699073076248169 | BCE Loss: 1.0167464017868042\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.720983505249023 | KNN Loss: 3.698321580886841 | BCE Loss: 1.0226619243621826\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.731990814208984 | KNN Loss: 3.710134267807007 | BCE Loss: 1.0218563079833984\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.741822719573975 | KNN Loss: 3.708472728729248 | BCE Loss: 1.0333499908447266\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.736991882324219 | KNN Loss: 3.713484525680542 | BCE Loss: 1.0235075950622559\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.812711715698242 | KNN Loss: 3.7294037342071533 | BCE Loss: 1.0833079814910889\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.737944602966309 | KNN Loss: 3.7105443477630615 | BCE Loss: 1.027400016784668\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.734941005706787 | KNN Loss: 3.698579788208008 | BCE Loss: 1.0363613367080688\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.746121406555176 | KNN Loss: 3.7106893062591553 | BCE Loss: 1.0354321002960205\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.717189788818359 | KNN Loss: 3.715496063232422 | BCE Loss: 1.001693606376648\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.7287187576293945 | KNN Loss: 3.707533836364746 | BCE Loss: 1.0211849212646484\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.7326202392578125 | KNN Loss: 3.693883180618286 | BCE Loss: 1.0387372970581055\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.707952976226807 | KNN Loss: 3.7078092098236084 | BCE Loss: 1.0001438856124878\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.706545829772949 | KNN Loss: 3.67718505859375 | BCE Loss: 1.0293607711791992\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.701525688171387 | KNN Loss: 3.6987595558166504 | BCE Loss: 1.0027661323547363\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.732404708862305 | KNN Loss: 3.716022253036499 | BCE Loss: 1.0163824558258057\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.717727184295654 | KNN Loss: 3.6808090209960938 | BCE Loss: 1.0369181632995605\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.73149299621582 | KNN Loss: 3.7137396335601807 | BCE Loss: 1.0177531242370605\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.710067272186279 | KNN Loss: 3.710981845855713 | BCE Loss: 0.999085545539856\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.699660778045654 | KNN Loss: 3.6961135864257812 | BCE Loss: 1.0035470724105835\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.750246047973633 | KNN Loss: 3.731156826019287 | BCE Loss: 1.0190892219543457\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.722224235534668 | KNN Loss: 3.716536521911621 | BCE Loss: 1.005687952041626\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.726335048675537 | KNN Loss: 3.7118027210235596 | BCE Loss: 1.014532208442688\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.734555244445801 | KNN Loss: 3.707827091217041 | BCE Loss: 1.0267281532287598\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.7194976806640625 | KNN Loss: 3.6922967433929443 | BCE Loss: 1.0272008180618286\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.721007823944092 | KNN Loss: 3.698505163192749 | BCE Loss: 1.0225025415420532\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.716836452484131 | KNN Loss: 3.7119948863983154 | BCE Loss: 1.0048415660858154\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.744524002075195 | KNN Loss: 3.6937694549560547 | BCE Loss: 1.0507546663284302\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.712462425231934 | KNN Loss: 3.6948747634887695 | BCE Loss: 1.0175877809524536\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.729428291320801 | KNN Loss: 3.7024929523468018 | BCE Loss: 1.026935338973999\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.694704055786133 | KNN Loss: 3.6811935901641846 | BCE Loss: 1.0135102272033691\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.72217321395874 | KNN Loss: 3.6919775009155273 | BCE Loss: 1.030195713043213\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.74806547164917 | KNN Loss: 3.724181890487671 | BCE Loss: 1.023883581161499\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.780157089233398 | KNN Loss: 3.740708112716675 | BCE Loss: 1.0394492149353027\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.709505558013916 | KNN Loss: 3.6849191188812256 | BCE Loss: 1.0245864391326904\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.776741027832031 | KNN Loss: 3.75301194190979 | BCE Loss: 1.0237290859222412\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.679300308227539 | KNN Loss: 3.6902053356170654 | BCE Loss: 0.9890952110290527\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.739598274230957 | KNN Loss: 3.707275867462158 | BCE Loss: 1.0323224067687988\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.744273662567139 | KNN Loss: 3.7168033123016357 | BCE Loss: 1.0274702310562134\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.697846412658691 | KNN Loss: 3.7022159099578857 | BCE Loss: 0.9956302642822266\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.723560810089111 | KNN Loss: 3.696091413497925 | BCE Loss: 1.0274693965911865\n",
      "Epoch   446: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.7308807373046875 | KNN Loss: 3.704155921936035 | BCE Loss: 1.0267245769500732\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.770315170288086 | KNN Loss: 3.7228217124938965 | BCE Loss: 1.0474934577941895\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.718661308288574 | KNN Loss: 3.704472303390503 | BCE Loss: 1.0141888856887817\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.719470024108887 | KNN Loss: 3.6978182792663574 | BCE Loss: 1.0216515064239502\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.757782459259033 | KNN Loss: 3.7073659896850586 | BCE Loss: 1.0504164695739746\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.724036693572998 | KNN Loss: 3.7147438526153564 | BCE Loss: 1.0092929601669312\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.720083713531494 | KNN Loss: 3.689314842224121 | BCE Loss: 1.030768871307373\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.731830596923828 | KNN Loss: 3.712642192840576 | BCE Loss: 1.019188642501831\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.713572025299072 | KNN Loss: 3.7077083587646484 | BCE Loss: 1.0058637857437134\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.75622034072876 | KNN Loss: 3.7208058834075928 | BCE Loss: 1.0354145765304565\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.695103645324707 | KNN Loss: 3.683875322341919 | BCE Loss: 1.011228084564209\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.7377424240112305 | KNN Loss: 3.7262163162231445 | BCE Loss: 1.0115258693695068\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.769758701324463 | KNN Loss: 3.7347028255462646 | BCE Loss: 1.0350558757781982\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.739136695861816 | KNN Loss: 3.7064623832702637 | BCE Loss: 1.0326745510101318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.7324347496032715 | KNN Loss: 3.7232372760772705 | BCE Loss: 1.009197473526001\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.746771812438965 | KNN Loss: 3.7235639095306396 | BCE Loss: 1.0232077836990356\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.696274280548096 | KNN Loss: 3.687047004699707 | BCE Loss: 1.0092272758483887\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.720311641693115 | KNN Loss: 3.698303699493408 | BCE Loss: 1.0220078229904175\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.745702743530273 | KNN Loss: 3.7228331565856934 | BCE Loss: 1.022869348526001\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.699822425842285 | KNN Loss: 3.6966190338134766 | BCE Loss: 1.0032036304473877\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.670045375823975 | KNN Loss: 3.6735451221466064 | BCE Loss: 0.9965003728866577\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.763278961181641 | KNN Loss: 3.7159793376922607 | BCE Loss: 1.0472993850708008\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.829549789428711 | KNN Loss: 3.781986951828003 | BCE Loss: 1.047562837600708\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.72845983505249 | KNN Loss: 3.7097859382629395 | BCE Loss: 1.0186738967895508\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.743461608886719 | KNN Loss: 3.728041410446167 | BCE Loss: 1.0154204368591309\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.716982841491699 | KNN Loss: 3.698984384536743 | BCE Loss: 1.0179985761642456\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.742208003997803 | KNN Loss: 3.6962101459503174 | BCE Loss: 1.0459977388381958\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.718922138214111 | KNN Loss: 3.725525379180908 | BCE Loss: 0.9933968186378479\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.744487762451172 | KNN Loss: 3.723098039627075 | BCE Loss: 1.0213896036148071\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.797691345214844 | KNN Loss: 3.7622430324554443 | BCE Loss: 1.0354485511779785\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.7655415534973145 | KNN Loss: 3.728827953338623 | BCE Loss: 1.0367134809494019\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.735365867614746 | KNN Loss: 3.6872894763946533 | BCE Loss: 1.0480761528015137\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.707118511199951 | KNN Loss: 3.6723222732543945 | BCE Loss: 1.0347962379455566\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.741629600524902 | KNN Loss: 3.6973750591278076 | BCE Loss: 1.0442543029785156\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.718084812164307 | KNN Loss: 3.7030088901519775 | BCE Loss: 1.0150758028030396\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.705474853515625 | KNN Loss: 3.696148633956909 | BCE Loss: 1.009326457977295\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.7273712158203125 | KNN Loss: 3.6895506381988525 | BCE Loss: 1.037820816040039\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.7450032234191895 | KNN Loss: 3.7113523483276367 | BCE Loss: 1.0336508750915527\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.724599361419678 | KNN Loss: 3.7316510677337646 | BCE Loss: 0.9929484128952026\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.743369102478027 | KNN Loss: 3.7337584495544434 | BCE Loss: 1.0096107721328735\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.668696403503418 | KNN Loss: 3.6658308506011963 | BCE Loss: 1.0028655529022217\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.750541687011719 | KNN Loss: 3.7215373516082764 | BCE Loss: 1.0290040969848633\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.72764253616333 | KNN Loss: 3.7116427421569824 | BCE Loss: 1.0159999132156372\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.739282608032227 | KNN Loss: 3.7200229167938232 | BCE Loss: 1.0192594528198242\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.725146293640137 | KNN Loss: 3.693751096725464 | BCE Loss: 1.0313949584960938\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.711435317993164 | KNN Loss: 3.689957857131958 | BCE Loss: 1.021477460861206\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.708373546600342 | KNN Loss: 3.71940279006958 | BCE Loss: 0.9889706373214722\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.775354385375977 | KNN Loss: 3.7458832263946533 | BCE Loss: 1.0294712781906128\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.724059104919434 | KNN Loss: 3.6990268230438232 | BCE Loss: 1.0250320434570312\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.703753471374512 | KNN Loss: 3.7020976543426514 | BCE Loss: 1.00165593624115\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.752041816711426 | KNN Loss: 3.7132954597473145 | BCE Loss: 1.0387465953826904\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.692540168762207 | KNN Loss: 3.6700222492218018 | BCE Loss: 1.0225180387496948\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.72155237197876 | KNN Loss: 3.7202556133270264 | BCE Loss: 1.0012966394424438\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.784022331237793 | KNN Loss: 3.7443535327911377 | BCE Loss: 1.0396690368652344\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.744492053985596 | KNN Loss: 3.7159948348999023 | BCE Loss: 1.0284972190856934\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.724096298217773 | KNN Loss: 3.7090461254119873 | BCE Loss: 1.0150504112243652\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.718099594116211 | KNN Loss: 3.707306385040283 | BCE Loss: 1.0107934474945068\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.751626491546631 | KNN Loss: 3.716923952102661 | BCE Loss: 1.0347025394439697\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.779980659484863 | KNN Loss: 3.7251644134521484 | BCE Loss: 1.0548162460327148\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.737649917602539 | KNN Loss: 3.7144360542297363 | BCE Loss: 1.0232141017913818\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.7286882400512695 | KNN Loss: 3.676990270614624 | BCE Loss: 1.0516982078552246\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.721275806427002 | KNN Loss: 3.7227461338043213 | BCE Loss: 0.9985297918319702\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.735439777374268 | KNN Loss: 3.714972734451294 | BCE Loss: 1.0204670429229736\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.710213661193848 | KNN Loss: 3.6992027759552 | BCE Loss: 1.0110106468200684\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.741582870483398 | KNN Loss: 3.724313974380493 | BCE Loss: 1.0172690153121948\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.788073539733887 | KNN Loss: 3.7381577491760254 | BCE Loss: 1.0499155521392822\n",
      "Epoch   457: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.732187271118164 | KNN Loss: 3.6852729320526123 | BCE Loss: 1.0469144582748413\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.768174171447754 | KNN Loss: 3.7294490337371826 | BCE Loss: 1.0387253761291504\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.745107173919678 | KNN Loss: 3.711437463760376 | BCE Loss: 1.0336697101593018\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.725006580352783 | KNN Loss: 3.70729398727417 | BCE Loss: 1.0177125930786133\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.716895580291748 | KNN Loss: 3.6869559288024902 | BCE Loss: 1.0299396514892578\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.727728843688965 | KNN Loss: 3.7001962661743164 | BCE Loss: 1.0275325775146484\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.721215724945068 | KNN Loss: 3.720869302749634 | BCE Loss: 1.0003464221954346\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.752284049987793 | KNN Loss: 3.733522415161133 | BCE Loss: 1.0187616348266602\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.702641487121582 | KNN Loss: 3.711005687713623 | BCE Loss: 0.9916360378265381\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.702943801879883 | KNN Loss: 3.699352264404297 | BCE Loss: 1.0035912990570068\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.716179847717285 | KNN Loss: 3.707784414291382 | BCE Loss: 1.0083951950073242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.690435409545898 | KNN Loss: 3.6791489124298096 | BCE Loss: 1.011286735534668\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.689745903015137 | KNN Loss: 3.6812632083892822 | BCE Loss: 1.0084829330444336\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.755011558532715 | KNN Loss: 3.717414140701294 | BCE Loss: 1.037597417831421\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.73530387878418 | KNN Loss: 3.7053306102752686 | BCE Loss: 1.0299732685089111\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.7699384689331055 | KNN Loss: 3.739673614501953 | BCE Loss: 1.0302648544311523\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.737393379211426 | KNN Loss: 3.72377610206604 | BCE Loss: 1.0136171579360962\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.752344131469727 | KNN Loss: 3.733727216720581 | BCE Loss: 1.0186166763305664\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.694576740264893 | KNN Loss: 3.697068929672241 | BCE Loss: 0.9975079298019409\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.698614120483398 | KNN Loss: 3.6688759326934814 | BCE Loss: 1.0297383069992065\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.780183792114258 | KNN Loss: 3.7432117462158203 | BCE Loss: 1.0369722843170166\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.7377471923828125 | KNN Loss: 3.707991361618042 | BCE Loss: 1.0297558307647705\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.687379837036133 | KNN Loss: 3.691114664077759 | BCE Loss: 0.9962654113769531\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.728862762451172 | KNN Loss: 3.700079917907715 | BCE Loss: 1.0287829637527466\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.6943488121032715 | KNN Loss: 3.680144786834717 | BCE Loss: 1.0142040252685547\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.720843315124512 | KNN Loss: 3.718991279602051 | BCE Loss: 1.0018517971038818\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.752098560333252 | KNN Loss: 3.6957125663757324 | BCE Loss: 1.05638587474823\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.738927841186523 | KNN Loss: 3.711362600326538 | BCE Loss: 1.0275650024414062\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.776604175567627 | KNN Loss: 3.738039493560791 | BCE Loss: 1.038564682006836\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.694829940795898 | KNN Loss: 3.6696741580963135 | BCE Loss: 1.0251555442810059\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.684677600860596 | KNN Loss: 3.685701370239258 | BCE Loss: 0.9989761710166931\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.71744966506958 | KNN Loss: 3.726097822189331 | BCE Loss: 0.991351842880249\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.791234493255615 | KNN Loss: 3.74483060836792 | BCE Loss: 1.0464038848876953\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.749044895172119 | KNN Loss: 3.709805965423584 | BCE Loss: 1.0392389297485352\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.7024455070495605 | KNN Loss: 3.71336030960083 | BCE Loss: 0.9890850782394409\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.754512310028076 | KNN Loss: 3.724043846130371 | BCE Loss: 1.0304685831069946\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.717001438140869 | KNN Loss: 3.701093912124634 | BCE Loss: 1.0159075260162354\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.763306617736816 | KNN Loss: 3.7164883613586426 | BCE Loss: 1.046818494796753\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.7150678634643555 | KNN Loss: 3.710047960281372 | BCE Loss: 1.0050201416015625\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.735455513000488 | KNN Loss: 3.6997783184051514 | BCE Loss: 1.035677433013916\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.683549404144287 | KNN Loss: 3.6828866004943848 | BCE Loss: 1.000662922859192\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.721591949462891 | KNN Loss: 3.71124529838562 | BCE Loss: 1.01034677028656\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.7317047119140625 | KNN Loss: 3.706912040710449 | BCE Loss: 1.0247927904129028\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.772844314575195 | KNN Loss: 3.7235827445983887 | BCE Loss: 1.0492615699768066\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.72719669342041 | KNN Loss: 3.6916611194610596 | BCE Loss: 1.0355353355407715\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.758811950683594 | KNN Loss: 3.7126922607421875 | BCE Loss: 1.0461196899414062\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.7445807456970215 | KNN Loss: 3.7294108867645264 | BCE Loss: 1.0151698589324951\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.768113613128662 | KNN Loss: 3.729656219482422 | BCE Loss: 1.0384573936462402\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.740687370300293 | KNN Loss: 3.7184464931488037 | BCE Loss: 1.0222406387329102\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.727839469909668 | KNN Loss: 3.7152085304260254 | BCE Loss: 1.012630820274353\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.742526054382324 | KNN Loss: 3.6983299255371094 | BCE Loss: 1.044196367263794\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.754935264587402 | KNN Loss: 3.7406351566314697 | BCE Loss: 1.0143002271652222\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.729547500610352 | KNN Loss: 3.729403495788574 | BCE Loss: 1.0001442432403564\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.738944053649902 | KNN Loss: 3.7279212474823 | BCE Loss: 1.0110230445861816\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.724310398101807 | KNN Loss: 3.6895947456359863 | BCE Loss: 1.0347155332565308\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.72989559173584 | KNN Loss: 3.7093632221221924 | BCE Loss: 1.0205326080322266\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.7280497550964355 | KNN Loss: 3.700766086578369 | BCE Loss: 1.0272836685180664\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.686069488525391 | KNN Loss: 3.671882152557373 | BCE Loss: 1.0141870975494385\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.668685436248779 | KNN Loss: 3.688147783279419 | BCE Loss: 0.9805375933647156\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.71828556060791 | KNN Loss: 3.672696590423584 | BCE Loss: 1.045588731765747\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.7273359298706055 | KNN Loss: 3.745445728302002 | BCE Loss: 0.9818902015686035\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.731767177581787 | KNN Loss: 3.686708688735962 | BCE Loss: 1.0450586080551147\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.716849327087402 | KNN Loss: 3.68855357170105 | BCE Loss: 1.0282959938049316\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.6890153884887695 | KNN Loss: 3.6671979427337646 | BCE Loss: 1.0218175649642944\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.742908954620361 | KNN Loss: 3.7274911403656006 | BCE Loss: 1.0154176950454712\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.694059371948242 | KNN Loss: 3.6762192249298096 | BCE Loss: 1.0178402662277222\n",
      "Epoch   468: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.771542549133301 | KNN Loss: 3.74210786819458 | BCE Loss: 1.0294346809387207\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.733092308044434 | KNN Loss: 3.684675693511963 | BCE Loss: 1.0484167337417603\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.721183776855469 | KNN Loss: 3.700392007827759 | BCE Loss: 1.020792007446289\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.71503210067749 | KNN Loss: 3.7104387283325195 | BCE Loss: 1.0045933723449707\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.706921577453613 | KNN Loss: 3.704897403717041 | BCE Loss: 1.0020241737365723\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.765320777893066 | KNN Loss: 3.7322964668273926 | BCE Loss: 1.033024549484253\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.715632438659668 | KNN Loss: 3.703214645385742 | BCE Loss: 1.0124180316925049\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.720433235168457 | KNN Loss: 3.6821508407592773 | BCE Loss: 1.0382825136184692\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.72720193862915 | KNN Loss: 3.7293789386749268 | BCE Loss: 0.9978228211402893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.790313720703125 | KNN Loss: 3.750507116317749 | BCE Loss: 1.0398067235946655\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.7207794189453125 | KNN Loss: 3.701083183288574 | BCE Loss: 1.0196962356567383\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.785818099975586 | KNN Loss: 3.729259490966797 | BCE Loss: 1.0565588474273682\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.736263275146484 | KNN Loss: 3.694779872894287 | BCE Loss: 1.0414836406707764\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.717832565307617 | KNN Loss: 3.720715284347534 | BCE Loss: 0.9971174597740173\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.718566417694092 | KNN Loss: 3.7001214027404785 | BCE Loss: 1.0184450149536133\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.682604789733887 | KNN Loss: 3.677612781524658 | BCE Loss: 1.004992127418518\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.745893478393555 | KNN Loss: 3.726593255996704 | BCE Loss: 1.0193002223968506\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.7119622230529785 | KNN Loss: 3.6781113147735596 | BCE Loss: 1.0338510274887085\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.742931365966797 | KNN Loss: 3.6814959049224854 | BCE Loss: 1.0614354610443115\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.766573905944824 | KNN Loss: 3.7312393188476562 | BCE Loss: 1.0353343486785889\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.708146572113037 | KNN Loss: 3.70637845993042 | BCE Loss: 1.0017679929733276\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.704355239868164 | KNN Loss: 3.6859724521636963 | BCE Loss: 1.0183825492858887\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.718279838562012 | KNN Loss: 3.710439443588257 | BCE Loss: 1.0078403949737549\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.749466419219971 | KNN Loss: 3.7243940830230713 | BCE Loss: 1.0250723361968994\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.714954853057861 | KNN Loss: 3.7153818607330322 | BCE Loss: 0.9995729923248291\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.7553815841674805 | KNN Loss: 3.744684934616089 | BCE Loss: 1.0106966495513916\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.723593711853027 | KNN Loss: 3.7023189067840576 | BCE Loss: 1.0212745666503906\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.70383358001709 | KNN Loss: 3.714369297027588 | BCE Loss: 0.9894644618034363\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.748304843902588 | KNN Loss: 3.6969311237335205 | BCE Loss: 1.0513737201690674\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.748456954956055 | KNN Loss: 3.7137088775634766 | BCE Loss: 1.034747838973999\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.726994037628174 | KNN Loss: 3.6784539222717285 | BCE Loss: 1.0485399961471558\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.6909308433532715 | KNN Loss: 3.6636619567871094 | BCE Loss: 1.0272687673568726\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.731578350067139 | KNN Loss: 3.727768898010254 | BCE Loss: 1.0038095712661743\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.715837001800537 | KNN Loss: 3.6880600452423096 | BCE Loss: 1.0277769565582275\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.736109256744385 | KNN Loss: 3.7297773361206055 | BCE Loss: 1.0063320398330688\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.722874641418457 | KNN Loss: 3.710883378982544 | BCE Loss: 1.0119915008544922\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.709592819213867 | KNN Loss: 3.6826205253601074 | BCE Loss: 1.0269722938537598\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.738344669342041 | KNN Loss: 3.7355926036834717 | BCE Loss: 1.0027521848678589\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.749298095703125 | KNN Loss: 3.7223050594329834 | BCE Loss: 1.0269930362701416\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.742276191711426 | KNN Loss: 3.690776824951172 | BCE Loss: 1.051499366760254\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.739221096038818 | KNN Loss: 3.715715169906616 | BCE Loss: 1.0235059261322021\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.730765342712402 | KNN Loss: 3.718907594680786 | BCE Loss: 1.011857509613037\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.72287654876709 | KNN Loss: 3.701141119003296 | BCE Loss: 1.021735429763794\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.713046073913574 | KNN Loss: 3.7047083377838135 | BCE Loss: 1.0083378553390503\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.737046718597412 | KNN Loss: 3.6751389503479004 | BCE Loss: 1.0619078874588013\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.741444110870361 | KNN Loss: 3.7089178562164307 | BCE Loss: 1.0325262546539307\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.6771745681762695 | KNN Loss: 3.6878952980041504 | BCE Loss: 0.98927903175354\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.678452491760254 | KNN Loss: 3.700233221054077 | BCE Loss: 0.9782193899154663\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.717824935913086 | KNN Loss: 3.6794896125793457 | BCE Loss: 1.0383350849151611\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.703587532043457 | KNN Loss: 3.6845815181732178 | BCE Loss: 1.0190060138702393\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.722305774688721 | KNN Loss: 3.707326650619507 | BCE Loss: 1.0149792432785034\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.722177505493164 | KNN Loss: 3.694849967956543 | BCE Loss: 1.027327299118042\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.791156768798828 | KNN Loss: 3.7566134929656982 | BCE Loss: 1.0345433950424194\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.724522590637207 | KNN Loss: 3.701911449432373 | BCE Loss: 1.022611379623413\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.7083821296691895 | KNN Loss: 3.6918437480926514 | BCE Loss: 1.016538381576538\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.721804618835449 | KNN Loss: 3.690669059753418 | BCE Loss: 1.0311353206634521\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.707708835601807 | KNN Loss: 3.6816744804382324 | BCE Loss: 1.0260343551635742\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.733150482177734 | KNN Loss: 3.686237335205078 | BCE Loss: 1.0469129085540771\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.709475994110107 | KNN Loss: 3.6827991008758545 | BCE Loss: 1.026676893234253\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.741334915161133 | KNN Loss: 3.7211644649505615 | BCE Loss: 1.0201702117919922\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.768086910247803 | KNN Loss: 3.729111909866333 | BCE Loss: 1.0389750003814697\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.726349353790283 | KNN Loss: 3.733086109161377 | BCE Loss: 0.9932631254196167\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.704365253448486 | KNN Loss: 3.7043142318725586 | BCE Loss: 1.0000509023666382\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.719949722290039 | KNN Loss: 3.6970438957214355 | BCE Loss: 1.0229055881500244\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.722232818603516 | KNN Loss: 3.7123255729675293 | BCE Loss: 1.0099074840545654\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.8233642578125 | KNN Loss: 3.788926839828491 | BCE Loss: 1.0344374179840088\n",
      "Epoch   479: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.692089080810547 | KNN Loss: 3.698845624923706 | BCE Loss: 0.9932436347007751\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.710113525390625 | KNN Loss: 3.6685986518859863 | BCE Loss: 1.0415148735046387\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.732523441314697 | KNN Loss: 3.6911277770996094 | BCE Loss: 1.0413955450057983\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.683398246765137 | KNN Loss: 3.6968140602111816 | BCE Loss: 0.9865841865539551\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.708080768585205 | KNN Loss: 3.697174072265625 | BCE Loss: 1.01090669631958\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.678106784820557 | KNN Loss: 3.680579900741577 | BCE Loss: 0.997527003288269\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.741245746612549 | KNN Loss: 3.7015798091888428 | BCE Loss: 1.0396660566329956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.699853897094727 | KNN Loss: 3.69313383102417 | BCE Loss: 1.0067198276519775\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.660924911499023 | KNN Loss: 3.6677608489990234 | BCE Loss: 0.9931640625\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.731444358825684 | KNN Loss: 3.7100045680999756 | BCE Loss: 1.021439552307129\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.725133419036865 | KNN Loss: 3.7023613452911377 | BCE Loss: 1.0227720737457275\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.721109390258789 | KNN Loss: 3.695420742034912 | BCE Loss: 1.0256887674331665\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.707658767700195 | KNN Loss: 3.6958277225494385 | BCE Loss: 1.011831283569336\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.735317230224609 | KNN Loss: 3.6926376819610596 | BCE Loss: 1.0426793098449707\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.760739803314209 | KNN Loss: 3.712620496749878 | BCE Loss: 1.0481191873550415\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.772215366363525 | KNN Loss: 3.7207462787628174 | BCE Loss: 1.0514692068099976\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.701416969299316 | KNN Loss: 3.679363965988159 | BCE Loss: 1.0220530033111572\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.819027423858643 | KNN Loss: 3.7643518447875977 | BCE Loss: 1.0546756982803345\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.697724342346191 | KNN Loss: 3.6904470920562744 | BCE Loss: 1.007277011871338\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.742223739624023 | KNN Loss: 3.7208962440490723 | BCE Loss: 1.0213274955749512\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.679210186004639 | KNN Loss: 3.693605661392212 | BCE Loss: 0.9856044054031372\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.75168514251709 | KNN Loss: 3.759136199951172 | BCE Loss: 0.9925490617752075\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.733804702758789 | KNN Loss: 3.7104196548461914 | BCE Loss: 1.0233852863311768\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.683715343475342 | KNN Loss: 3.6754798889160156 | BCE Loss: 1.0082353353500366\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.717870712280273 | KNN Loss: 3.716066360473633 | BCE Loss: 1.0018045902252197\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.734546184539795 | KNN Loss: 3.703249931335449 | BCE Loss: 1.0312962532043457\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.761136054992676 | KNN Loss: 3.709221363067627 | BCE Loss: 1.0519148111343384\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.726741313934326 | KNN Loss: 3.7090823650360107 | BCE Loss: 1.0176589488983154\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.68931245803833 | KNN Loss: 3.67746639251709 | BCE Loss: 1.0118461847305298\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.760970115661621 | KNN Loss: 3.7121741771698 | BCE Loss: 1.0487958192825317\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.717653274536133 | KNN Loss: 3.6965742111206055 | BCE Loss: 1.0210789442062378\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.737950325012207 | KNN Loss: 3.702392816543579 | BCE Loss: 1.035557746887207\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.730842590332031 | KNN Loss: 3.7011399269104004 | BCE Loss: 1.02970290184021\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.756531238555908 | KNN Loss: 3.713300943374634 | BCE Loss: 1.043230414390564\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.742095947265625 | KNN Loss: 3.697126865386963 | BCE Loss: 1.044968843460083\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.710292816162109 | KNN Loss: 3.68605637550354 | BCE Loss: 1.0242364406585693\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.737790107727051 | KNN Loss: 3.693552017211914 | BCE Loss: 1.0442383289337158\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.728618621826172 | KNN Loss: 3.707353115081787 | BCE Loss: 1.0212652683258057\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.713659286499023 | KNN Loss: 3.7000460624694824 | BCE Loss: 1.0136131048202515\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.715983867645264 | KNN Loss: 3.6798388957977295 | BCE Loss: 1.0361448526382446\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.678009986877441 | KNN Loss: 3.6859188079833984 | BCE Loss: 0.9920910596847534\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.718473434448242 | KNN Loss: 3.68864107131958 | BCE Loss: 1.029832363128662\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.704753875732422 | KNN Loss: 3.718252658843994 | BCE Loss: 0.9865012764930725\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.733156681060791 | KNN Loss: 3.735515832901001 | BCE Loss: 0.9976406693458557\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.763027667999268 | KNN Loss: 3.716479539871216 | BCE Loss: 1.0465481281280518\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.695935249328613 | KNN Loss: 3.6824634075164795 | BCE Loss: 1.0134716033935547\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.758744239807129 | KNN Loss: 3.712062358856201 | BCE Loss: 1.0466820001602173\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.700833320617676 | KNN Loss: 3.7083873748779297 | BCE Loss: 0.9924459457397461\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.659797668457031 | KNN Loss: 3.68127179145813 | BCE Loss: 0.978525698184967\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.73486328125 | KNN Loss: 3.715909719467163 | BCE Loss: 1.018953800201416\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.730395317077637 | KNN Loss: 3.7213573455810547 | BCE Loss: 1.0090382099151611\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.706930160522461 | KNN Loss: 3.6846985816955566 | BCE Loss: 1.0222315788269043\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.75060510635376 | KNN Loss: 3.733868360519409 | BCE Loss: 1.016736626625061\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.760834217071533 | KNN Loss: 3.7281274795532227 | BCE Loss: 1.0327067375183105\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.741729259490967 | KNN Loss: 3.696380376815796 | BCE Loss: 1.045348882675171\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.733166694641113 | KNN Loss: 3.7289206981658936 | BCE Loss: 1.0042462348937988\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.749969482421875 | KNN Loss: 3.720874071121216 | BCE Loss: 1.0290954113006592\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.713129997253418 | KNN Loss: 3.6869754791259766 | BCE Loss: 1.0261542797088623\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.718774795532227 | KNN Loss: 3.7076685428619385 | BCE Loss: 1.0111063718795776\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.752583026885986 | KNN Loss: 3.7283129692077637 | BCE Loss: 1.0242700576782227\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.6851630210876465 | KNN Loss: 3.7010302543640137 | BCE Loss: 0.9841326475143433\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.718016624450684 | KNN Loss: 3.6970055103302 | BCE Loss: 1.0210111141204834\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.705303192138672 | KNN Loss: 3.699345350265503 | BCE Loss: 1.005958080291748\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.719900131225586 | KNN Loss: 3.7094109058380127 | BCE Loss: 1.0104892253875732\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.732069969177246 | KNN Loss: 3.714423894882202 | BCE Loss: 1.017646312713623\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.795260906219482 | KNN Loss: 3.7499868869781494 | BCE Loss: 1.0452741384506226\n",
      "Epoch   490: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.740113258361816 | KNN Loss: 3.718019485473633 | BCE Loss: 1.0220938920974731\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.7642822265625 | KNN Loss: 3.7259116172790527 | BCE Loss: 1.0383706092834473\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.766770839691162 | KNN Loss: 3.728804588317871 | BCE Loss: 1.037966251373291\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.6864705085754395 | KNN Loss: 3.6798274517059326 | BCE Loss: 1.0066430568695068\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.7495012283325195 | KNN Loss: 3.7387990951538086 | BCE Loss: 1.010702133178711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.691200256347656 | KNN Loss: 3.7076048851013184 | BCE Loss: 0.9835952520370483\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.730219841003418 | KNN Loss: 3.7243521213531494 | BCE Loss: 1.0058677196502686\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.709720134735107 | KNN Loss: 3.6959407329559326 | BCE Loss: 1.0137794017791748\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.751674175262451 | KNN Loss: 3.746224880218506 | BCE Loss: 1.0054491758346558\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.745136260986328 | KNN Loss: 3.7450082302093506 | BCE Loss: 1.0001280307769775\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.755199432373047 | KNN Loss: 3.7198448181152344 | BCE Loss: 1.0353548526763916\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.71691370010376 | KNN Loss: 3.7100090980529785 | BCE Loss: 1.0069046020507812\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.70306921005249 | KNN Loss: 3.6778745651245117 | BCE Loss: 1.025194525718689\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.74629545211792 | KNN Loss: 3.698829412460327 | BCE Loss: 1.0474659204483032\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.696194171905518 | KNN Loss: 3.6907668113708496 | BCE Loss: 1.0054274797439575\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.731021404266357 | KNN Loss: 3.6993470191955566 | BCE Loss: 1.0316743850708008\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.702940940856934 | KNN Loss: 3.689410448074341 | BCE Loss: 1.0135304927825928\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.724457740783691 | KNN Loss: 3.7107889652252197 | BCE Loss: 1.0136687755584717\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.757930755615234 | KNN Loss: 3.7292397022247314 | BCE Loss: 1.0286908149719238\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.752359390258789 | KNN Loss: 3.7451119422912598 | BCE Loss: 1.0072475671768188\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.693227291107178 | KNN Loss: 3.6878185272216797 | BCE Loss: 1.0054086446762085\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.778220176696777 | KNN Loss: 3.738781452178955 | BCE Loss: 1.0394384860992432\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.738903045654297 | KNN Loss: 3.707756519317627 | BCE Loss: 1.0311466455459595\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.744163513183594 | KNN Loss: 3.711552858352661 | BCE Loss: 1.0326106548309326\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.765122890472412 | KNN Loss: 3.741682291030884 | BCE Loss: 1.0234405994415283\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.744576454162598 | KNN Loss: 3.7172698974609375 | BCE Loss: 1.027306318283081\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.708187103271484 | KNN Loss: 3.7123146057128906 | BCE Loss: 0.9958726167678833\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.772964954376221 | KNN Loss: 3.7378523349761963 | BCE Loss: 1.0351126194000244\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.746582984924316 | KNN Loss: 3.7056052684783936 | BCE Loss: 1.0409777164459229\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.726522445678711 | KNN Loss: 3.715975761413574 | BCE Loss: 1.0105464458465576\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.740379333496094 | KNN Loss: 3.692484140396118 | BCE Loss: 1.047895073890686\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.710991859436035 | KNN Loss: 3.703559637069702 | BCE Loss: 1.007431983947754\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.704689025878906 | KNN Loss: 3.7017829418182373 | BCE Loss: 1.002906084060669\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.747884750366211 | KNN Loss: 3.7138891220092773 | BCE Loss: 1.0339958667755127\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.730770111083984 | KNN Loss: 3.72517728805542 | BCE Loss: 1.005592942237854\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.731527328491211 | KNN Loss: 3.702003002166748 | BCE Loss: 1.0295240879058838\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.758467674255371 | KNN Loss: 3.7486984729766846 | BCE Loss: 1.0097692012786865\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.744546890258789 | KNN Loss: 3.712603807449341 | BCE Loss: 1.0319433212280273\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.702930450439453 | KNN Loss: 3.6698970794677734 | BCE Loss: 1.0330333709716797\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.7450408935546875 | KNN Loss: 3.708723545074463 | BCE Loss: 1.0363173484802246\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.701353073120117 | KNN Loss: 3.712630271911621 | BCE Loss: 0.9887230396270752\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.775228023529053 | KNN Loss: 3.734699249267578 | BCE Loss: 1.040528655052185\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.705760955810547 | KNN Loss: 3.7110660076141357 | BCE Loss: 0.994694709777832\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.714217185974121 | KNN Loss: 3.6801767349243164 | BCE Loss: 1.0340405702590942\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.732211112976074 | KNN Loss: 3.7206177711486816 | BCE Loss: 1.0115935802459717\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.715107440948486 | KNN Loss: 3.69219708442688 | BCE Loss: 1.022910475730896\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.741611957550049 | KNN Loss: 3.7303690910339355 | BCE Loss: 1.0112428665161133\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.755963325500488 | KNN Loss: 3.7242817878723145 | BCE Loss: 1.0316812992095947\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.705421447753906 | KNN Loss: 3.6818339824676514 | BCE Loss: 1.0235872268676758\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.743129730224609 | KNN Loss: 3.715179204940796 | BCE Loss: 1.0279507637023926\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.730956077575684 | KNN Loss: 3.725285530090332 | BCE Loss: 1.0056706666946411\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.698433876037598 | KNN Loss: 3.6988027095794678 | BCE Loss: 0.9996309280395508\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.732439041137695 | KNN Loss: 3.6989071369171143 | BCE Loss: 1.033531904220581\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.736021518707275 | KNN Loss: 3.703655242919922 | BCE Loss: 1.0323662757873535\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.702450275421143 | KNN Loss: 3.6922507286071777 | BCE Loss: 1.0101996660232544\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.704306125640869 | KNN Loss: 3.6772937774658203 | BCE Loss: 1.0270124673843384\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.731109619140625 | KNN Loss: 3.704800605773926 | BCE Loss: 1.0263091325759888\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.725761413574219 | KNN Loss: 3.7063281536102295 | BCE Loss: 1.0194334983825684\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.7098917961120605 | KNN Loss: 3.7044143676757812 | BCE Loss: 1.0054774284362793\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.741281509399414 | KNN Loss: 3.725865364074707 | BCE Loss: 1.015416145324707\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0376,  4.0441,  2.9325,  2.2331,  2.3031,  0.7054,  2.1160,  1.6226,\n",
      "          2.6057,  1.9758,  1.6559,  1.7841,  0.4641,  1.8709,  1.3173,  1.0579,\n",
      "          3.1501,  2.2467,  3.1790,  2.6883,  1.7776,  2.4152,  1.8910,  3.0578,\n",
      "          2.8368,  1.8914,  2.1541,  1.4339,  1.7862,  0.6679, -0.0071,  0.7831,\n",
      "          0.5321,  0.9999,  1.7067,  1.2602,  1.1641,  3.7554,  1.0405,  1.4261,\n",
      "          0.7441, -1.0527, -0.0267,  2.4635,  2.0952,  0.8401, -0.3758,  0.2380,\n",
      "          1.6674,  2.5574,  1.9121,  0.4660,  1.3182,  0.2007, -0.6321,  1.3695,\n",
      "          1.4160,  1.5192,  1.3526,  1.9552,  0.1368,  1.1053,  0.3451,  1.2740,\n",
      "          1.1374,  2.0587, -1.5341,  0.5228,  2.6743,  1.9066,  2.9133,  0.1733,\n",
      "          1.4021,  2.7667,  1.9263,  1.1434,  0.5951,  0.6298,  0.0815,  1.9243,\n",
      "          0.1050,  0.4397,  1.9906, -0.2688,  0.1968, -1.3471, -2.4413, -0.0647,\n",
      "          0.7598, -1.8507,  0.6403,  0.1251, -0.5934, -1.3275,  0.4164,  1.4477,\n",
      "         -0.3175, -0.4011,  0.1844,  0.8603,  0.5998, -0.9640,  0.9727,  1.4557,\n",
      "         -1.3310, -1.2451, -0.0425,  0.0091, -0.7219, -1.9601, -0.0826, -3.1392,\n",
      "         -0.4077,  1.8691,  1.6199, -0.5339, -0.5642, -0.0220,  1.6120, -2.7763,\n",
      "          0.2745, -0.1065,  0.4580, -0.7848,  0.0701, -0.8626, -0.8804,  1.0338,\n",
      "          0.2630, -0.7367,  0.4152, -0.8661, -1.7161, -0.5908, -0.4715,  1.1902,\n",
      "         -0.2877,  0.1579, -1.8040, -1.4989, -1.5565,  0.7910, -1.7124, -0.9036,\n",
      "         -0.8090, -0.4954, -1.8953, -1.6313, -2.2189, -0.9647, -0.9229, -0.3064,\n",
      "         -1.7549,  0.6367, -1.2372, -0.7782, -3.9071,  0.2803, -0.0520, -0.5705,\n",
      "         -2.1726, -1.6979, -1.1223, -1.1569, -2.6137, -2.6403, -3.6574]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.9071, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.0441, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897059fbc0f4ba1bb7a8a285c5c5484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 99.09it/s] \n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab49339b94a1476597248ef38786afd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740dc7a8efce482f8836ec3579521551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266578e963734c42b56ea7e9ded224b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 026 | Total loss: 9.618 | Reg loss: 0.012 | Tree loss: 9.618 | Accuracy: 0.000000 | 0.887 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 026 | Total loss: 9.614 | Reg loss: 0.011 | Tree loss: 9.614 | Accuracy: 0.000000 | 0.824 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 026 | Total loss: 9.612 | Reg loss: 0.010 | Tree loss: 9.612 | Accuracy: 0.000000 | 0.808 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 026 | Total loss: 9.608 | Reg loss: 0.009 | Tree loss: 9.608 | Accuracy: 0.000000 | 0.798 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 026 | Total loss: 9.605 | Reg loss: 0.009 | Tree loss: 9.605 | Accuracy: 0.000000 | 0.792 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 026 | Total loss: 9.602 | Reg loss: 0.008 | Tree loss: 9.602 | Accuracy: 0.000000 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 026 | Total loss: 9.598 | Reg loss: 0.008 | Tree loss: 9.598 | Accuracy: 0.000000 | 0.788 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 026 | Total loss: 9.594 | Reg loss: 0.007 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.788 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 026 | Total loss: 9.592 | Reg loss: 0.007 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.786 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 026 | Total loss: 9.589 | Reg loss: 0.007 | Tree loss: 9.589 | Accuracy: 0.000000 | 0.786 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 026 | Total loss: 9.584 | Reg loss: 0.007 | Tree loss: 9.584 | Accuracy: 0.000000 | 0.787 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 026 | Total loss: 9.582 | Reg loss: 0.007 | Tree loss: 9.582 | Accuracy: 0.000000 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 026 | Total loss: 9.578 | Reg loss: 0.007 | Tree loss: 9.578 | Accuracy: 0.000000 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 026 | Total loss: 9.578 | Reg loss: 0.007 | Tree loss: 9.578 | Accuracy: 0.001953 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 026 | Total loss: 9.573 | Reg loss: 0.007 | Tree loss: 9.573 | Accuracy: 0.001953 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 026 | Total loss: 9.571 | Reg loss: 0.007 | Tree loss: 9.571 | Accuracy: 0.005859 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 026 | Total loss: 9.567 | Reg loss: 0.007 | Tree loss: 9.567 | Accuracy: 0.007812 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 026 | Total loss: 9.565 | Reg loss: 0.007 | Tree loss: 9.565 | Accuracy: 0.005859 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 026 | Total loss: 9.563 | Reg loss: 0.008 | Tree loss: 9.563 | Accuracy: 0.005859 | 0.789 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 026 | Total loss: 9.560 | Reg loss: 0.008 | Tree loss: 9.560 | Accuracy: 0.015625 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 026 | Total loss: 9.559 | Reg loss: 0.008 | Tree loss: 9.559 | Accuracy: 0.021484 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 026 | Total loss: 9.556 | Reg loss: 0.008 | Tree loss: 9.556 | Accuracy: 0.042969 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 026 | Total loss: 9.553 | Reg loss: 0.008 | Tree loss: 9.553 | Accuracy: 0.044922 | 0.79 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 026 | Total loss: 9.550 | Reg loss: 0.008 | Tree loss: 9.550 | Accuracy: 0.046875 | 0.791 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 026 | Total loss: 9.547 | Reg loss: 0.009 | Tree loss: 9.547 | Accuracy: 0.089844 | 0.791 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 026 | Total loss: 9.545 | Reg loss: 0.009 | Tree loss: 9.545 | Accuracy: 0.087719 | 0.791 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 026 | Total loss: 9.571 | Reg loss: 0.004 | Tree loss: 9.571 | Accuracy: 0.000000 | 0.801 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 026 | Total loss: 9.569 | Reg loss: 0.004 | Tree loss: 9.569 | Accuracy: 0.001953 | 0.799 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 026 | Total loss: 9.567 | Reg loss: 0.004 | Tree loss: 9.567 | Accuracy: 0.001953 | 0.798 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 026 | Total loss: 9.562 | Reg loss: 0.005 | Tree loss: 9.562 | Accuracy: 0.009766 | 0.797 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 026 | Total loss: 9.562 | Reg loss: 0.005 | Tree loss: 9.562 | Accuracy: 0.017578 | 0.797 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 026 | Total loss: 9.559 | Reg loss: 0.005 | Tree loss: 9.559 | Accuracy: 0.025391 | 0.796 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 026 | Total loss: 9.554 | Reg loss: 0.005 | Tree loss: 9.554 | Accuracy: 0.027344 | 0.796 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 026 | Total loss: 9.551 | Reg loss: 0.005 | Tree loss: 9.551 | Accuracy: 0.035156 | 0.797 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 026 | Total loss: 9.550 | Reg loss: 0.006 | Tree loss: 9.550 | Accuracy: 0.068359 | 0.798 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 026 | Total loss: 9.547 | Reg loss: 0.006 | Tree loss: 9.547 | Accuracy: 0.082031 | 0.799 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 026 | Total loss: 9.543 | Reg loss: 0.006 | Tree loss: 9.543 | Accuracy: 0.095703 | 0.799 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 026 | Total loss: 9.542 | Reg loss: 0.006 | Tree loss: 9.542 | Accuracy: 0.119141 | 0.8 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 026 | Total loss: 9.537 | Reg loss: 0.007 | Tree loss: 9.537 | Accuracy: 0.132812 | 0.801 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 026 | Total loss: 9.535 | Reg loss: 0.007 | Tree loss: 9.535 | Accuracy: 0.132812 | 0.801 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 026 | Total loss: 9.535 | Reg loss: 0.007 | Tree loss: 9.535 | Accuracy: 0.121094 | 0.802 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 026 | Total loss: 9.529 | Reg loss: 0.007 | Tree loss: 9.529 | Accuracy: 0.150391 | 0.802 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 026 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.142578 | 0.802 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 026 | Total loss: 9.523 | Reg loss: 0.008 | Tree loss: 9.523 | Accuracy: 0.167969 | 0.803 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 026 | Total loss: 9.524 | Reg loss: 0.008 | Tree loss: 9.524 | Accuracy: 0.128906 | 0.803 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 026 | Total loss: 9.523 | Reg loss: 0.009 | Tree loss: 9.523 | Accuracy: 0.150391 | 0.804 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 026 | Total loss: 9.517 | Reg loss: 0.009 | Tree loss: 9.517 | Accuracy: 0.166016 | 0.804 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 026 | Total loss: 9.515 | Reg loss: 0.009 | Tree loss: 9.515 | Accuracy: 0.150391 | 0.805 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 026 | Total loss: 9.513 | Reg loss: 0.009 | Tree loss: 9.513 | Accuracy: 0.154297 | 0.807 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 026 | Total loss: 9.510 | Reg loss: 0.010 | Tree loss: 9.510 | Accuracy: 0.162109 | 0.808 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 026 | Total loss: 9.505 | Reg loss: 0.010 | Tree loss: 9.505 | Accuracy: 0.162109 | 0.809 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 026 | Total loss: 9.505 | Reg loss: 0.010 | Tree loss: 9.505 | Accuracy: 0.122807 | 0.809 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 026 | Total loss: 9.535 | Reg loss: 0.006 | Tree loss: 9.535 | Accuracy: 0.138672 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 026 | Total loss: 9.533 | Reg loss: 0.006 | Tree loss: 9.533 | Accuracy: 0.144531 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 026 | Total loss: 9.529 | Reg loss: 0.006 | Tree loss: 9.529 | Accuracy: 0.132812 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 026 | Total loss: 9.528 | Reg loss: 0.007 | Tree loss: 9.528 | Accuracy: 0.142578 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 026 | Total loss: 9.525 | Reg loss: 0.007 | Tree loss: 9.525 | Accuracy: 0.150391 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 026 | Total loss: 9.523 | Reg loss: 0.007 | Tree loss: 9.523 | Accuracy: 0.130859 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 026 | Total loss: 9.519 | Reg loss: 0.007 | Tree loss: 9.519 | Accuracy: 0.144531 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 026 | Total loss: 9.515 | Reg loss: 0.007 | Tree loss: 9.515 | Accuracy: 0.156250 | 0.828 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 026 | Total loss: 9.511 | Reg loss: 0.008 | Tree loss: 9.511 | Accuracy: 0.181641 | 0.828 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 009 / 026 | Total loss: 9.509 | Reg loss: 0.008 | Tree loss: 9.509 | Accuracy: 0.136719 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 026 | Total loss: 9.508 | Reg loss: 0.008 | Tree loss: 9.508 | Accuracy: 0.123047 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 026 | Total loss: 9.501 | Reg loss: 0.008 | Tree loss: 9.501 | Accuracy: 0.181641 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 026 | Total loss: 9.503 | Reg loss: 0.009 | Tree loss: 9.503 | Accuracy: 0.150391 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 026 | Total loss: 9.497 | Reg loss: 0.009 | Tree loss: 9.497 | Accuracy: 0.181641 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 026 | Total loss: 9.494 | Reg loss: 0.009 | Tree loss: 9.494 | Accuracy: 0.156250 | 0.827 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 026 | Total loss: 9.491 | Reg loss: 0.010 | Tree loss: 9.491 | Accuracy: 0.132812 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 026 | Total loss: 9.490 | Reg loss: 0.010 | Tree loss: 9.490 | Accuracy: 0.152344 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 026 | Total loss: 9.485 | Reg loss: 0.010 | Tree loss: 9.485 | Accuracy: 0.169922 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 026 | Total loss: 9.485 | Reg loss: 0.011 | Tree loss: 9.485 | Accuracy: 0.128906 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 026 | Total loss: 9.482 | Reg loss: 0.011 | Tree loss: 9.482 | Accuracy: 0.160156 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 026 | Total loss: 9.478 | Reg loss: 0.011 | Tree loss: 9.478 | Accuracy: 0.144531 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 026 | Total loss: 9.480 | Reg loss: 0.011 | Tree loss: 9.480 | Accuracy: 0.132812 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 026 | Total loss: 9.471 | Reg loss: 0.012 | Tree loss: 9.471 | Accuracy: 0.156250 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 026 | Total loss: 9.470 | Reg loss: 0.012 | Tree loss: 9.470 | Accuracy: 0.123047 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 026 | Total loss: 9.460 | Reg loss: 0.012 | Tree loss: 9.460 | Accuracy: 0.175781 | 0.826 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 026 | Total loss: 9.454 | Reg loss: 0.013 | Tree loss: 9.454 | Accuracy: 0.175439 | 0.825 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 026 | Total loss: 9.495 | Reg loss: 0.008 | Tree loss: 9.495 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 026 | Total loss: 9.497 | Reg loss: 0.008 | Tree loss: 9.497 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 026 | Total loss: 9.493 | Reg loss: 0.009 | Tree loss: 9.493 | Accuracy: 0.146484 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 026 | Total loss: 9.487 | Reg loss: 0.009 | Tree loss: 9.487 | Accuracy: 0.128906 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 026 | Total loss: 9.486 | Reg loss: 0.009 | Tree loss: 9.486 | Accuracy: 0.152344 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 026 | Total loss: 9.484 | Reg loss: 0.009 | Tree loss: 9.484 | Accuracy: 0.126953 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 026 | Total loss: 9.483 | Reg loss: 0.009 | Tree loss: 9.483 | Accuracy: 0.148438 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 026 | Total loss: 9.473 | Reg loss: 0.010 | Tree loss: 9.473 | Accuracy: 0.158203 | 0.838 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 026 | Total loss: 9.471 | Reg loss: 0.010 | Tree loss: 9.471 | Accuracy: 0.144531 | 0.837 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 026 | Total loss: 9.469 | Reg loss: 0.010 | Tree loss: 9.469 | Accuracy: 0.132812 | 0.837 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 026 | Total loss: 9.462 | Reg loss: 0.010 | Tree loss: 9.462 | Accuracy: 0.152344 | 0.836 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 026 | Total loss: 9.459 | Reg loss: 0.011 | Tree loss: 9.459 | Accuracy: 0.134766 | 0.836 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 026 | Total loss: 9.452 | Reg loss: 0.011 | Tree loss: 9.452 | Accuracy: 0.177734 | 0.835 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 026 | Total loss: 9.451 | Reg loss: 0.011 | Tree loss: 9.451 | Accuracy: 0.152344 | 0.835 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 026 | Total loss: 9.448 | Reg loss: 0.012 | Tree loss: 9.448 | Accuracy: 0.152344 | 0.834 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 026 | Total loss: 9.442 | Reg loss: 0.012 | Tree loss: 9.442 | Accuracy: 0.132812 | 0.834 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 026 | Total loss: 9.436 | Reg loss: 0.012 | Tree loss: 9.436 | Accuracy: 0.162109 | 0.834 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 026 | Total loss: 9.432 | Reg loss: 0.013 | Tree loss: 9.432 | Accuracy: 0.150391 | 0.834 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 026 | Total loss: 9.430 | Reg loss: 0.013 | Tree loss: 9.430 | Accuracy: 0.164062 | 0.834 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 026 | Total loss: 9.426 | Reg loss: 0.013 | Tree loss: 9.426 | Accuracy: 0.134766 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 026 | Total loss: 9.419 | Reg loss: 0.014 | Tree loss: 9.419 | Accuracy: 0.148438 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 026 | Total loss: 9.414 | Reg loss: 0.014 | Tree loss: 9.414 | Accuracy: 0.144531 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 026 | Total loss: 9.407 | Reg loss: 0.014 | Tree loss: 9.407 | Accuracy: 0.162109 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 026 | Total loss: 9.405 | Reg loss: 0.015 | Tree loss: 9.405 | Accuracy: 0.156250 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 026 | Total loss: 9.400 | Reg loss: 0.015 | Tree loss: 9.400 | Accuracy: 0.150391 | 0.833 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 026 | Total loss: 9.369 | Reg loss: 0.015 | Tree loss: 9.369 | Accuracy: 0.228070 | 0.833 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 026 | Total loss: 9.449 | Reg loss: 0.011 | Tree loss: 9.449 | Accuracy: 0.150391 | 0.843 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 026 | Total loss: 9.441 | Reg loss: 0.011 | Tree loss: 9.441 | Accuracy: 0.154297 | 0.842 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 026 | Total loss: 9.448 | Reg loss: 0.011 | Tree loss: 9.448 | Accuracy: 0.113281 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 026 | Total loss: 9.431 | Reg loss: 0.011 | Tree loss: 9.431 | Accuracy: 0.195312 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 026 | Total loss: 9.432 | Reg loss: 0.011 | Tree loss: 9.432 | Accuracy: 0.150391 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 026 | Total loss: 9.420 | Reg loss: 0.011 | Tree loss: 9.420 | Accuracy: 0.179688 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 026 | Total loss: 9.426 | Reg loss: 0.012 | Tree loss: 9.426 | Accuracy: 0.128906 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 026 | Total loss: 9.413 | Reg loss: 0.012 | Tree loss: 9.413 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 026 | Total loss: 9.413 | Reg loss: 0.012 | Tree loss: 9.413 | Accuracy: 0.128906 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 026 | Total loss: 9.398 | Reg loss: 0.012 | Tree loss: 9.398 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 026 | Total loss: 9.395 | Reg loss: 0.013 | Tree loss: 9.395 | Accuracy: 0.146484 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 026 | Total loss: 9.389 | Reg loss: 0.013 | Tree loss: 9.389 | Accuracy: 0.169922 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 026 | Total loss: 9.383 | Reg loss: 0.013 | Tree loss: 9.383 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 026 | Total loss: 9.380 | Reg loss: 0.014 | Tree loss: 9.380 | Accuracy: 0.136719 | 0.84 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 026 | Total loss: 9.370 | Reg loss: 0.014 | Tree loss: 9.370 | Accuracy: 0.138672 | 0.84 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 026 | Total loss: 9.358 | Reg loss: 0.014 | Tree loss: 9.358 | Accuracy: 0.148438 | 0.84 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 026 | Total loss: 9.354 | Reg loss: 0.015 | Tree loss: 9.354 | Accuracy: 0.140625 | 0.839 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 026 | Total loss: 9.341 | Reg loss: 0.015 | Tree loss: 9.341 | Accuracy: 0.164062 | 0.839 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 026 | Total loss: 9.342 | Reg loss: 0.016 | Tree loss: 9.342 | Accuracy: 0.130859 | 0.839 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 019 / 026 | Total loss: 9.335 | Reg loss: 0.016 | Tree loss: 9.335 | Accuracy: 0.130859 | 0.839 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 026 | Total loss: 9.322 | Reg loss: 0.016 | Tree loss: 9.322 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 026 | Total loss: 9.312 | Reg loss: 0.017 | Tree loss: 9.312 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 026 | Total loss: 9.302 | Reg loss: 0.017 | Tree loss: 9.302 | Accuracy: 0.154297 | 0.838 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 026 | Total loss: 9.292 | Reg loss: 0.018 | Tree loss: 9.292 | Accuracy: 0.162109 | 0.838 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 026 | Total loss: 9.287 | Reg loss: 0.018 | Tree loss: 9.287 | Accuracy: 0.150391 | 0.838 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 026 | Total loss: 9.235 | Reg loss: 0.018 | Tree loss: 9.235 | Accuracy: 0.210526 | 0.838 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 026 | Total loss: 9.382 | Reg loss: 0.013 | Tree loss: 9.382 | Accuracy: 0.103516 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 026 | Total loss: 9.367 | Reg loss: 0.013 | Tree loss: 9.367 | Accuracy: 0.152344 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 026 | Total loss: 9.357 | Reg loss: 0.013 | Tree loss: 9.357 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 026 | Total loss: 9.354 | Reg loss: 0.013 | Tree loss: 9.354 | Accuracy: 0.160156 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 026 | Total loss: 9.339 | Reg loss: 0.013 | Tree loss: 9.339 | Accuracy: 0.162109 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 026 | Total loss: 9.339 | Reg loss: 0.014 | Tree loss: 9.339 | Accuracy: 0.132812 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 026 | Total loss: 9.324 | Reg loss: 0.014 | Tree loss: 9.324 | Accuracy: 0.138672 | 0.845 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 026 | Total loss: 9.307 | Reg loss: 0.014 | Tree loss: 9.307 | Accuracy: 0.164062 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 026 | Total loss: 9.306 | Reg loss: 0.014 | Tree loss: 9.306 | Accuracy: 0.140625 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 026 | Total loss: 9.295 | Reg loss: 0.015 | Tree loss: 9.295 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 026 | Total loss: 9.283 | Reg loss: 0.015 | Tree loss: 9.283 | Accuracy: 0.154297 | 0.846 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 026 | Total loss: 9.266 | Reg loss: 0.015 | Tree loss: 9.266 | Accuracy: 0.162109 | 0.845 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 026 | Total loss: 9.269 | Reg loss: 0.016 | Tree loss: 9.269 | Accuracy: 0.125000 | 0.845 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 026 | Total loss: 9.252 | Reg loss: 0.016 | Tree loss: 9.252 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 026 | Total loss: 9.236 | Reg loss: 0.016 | Tree loss: 9.236 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 026 | Total loss: 9.222 | Reg loss: 0.017 | Tree loss: 9.222 | Accuracy: 0.136719 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 026 | Total loss: 9.215 | Reg loss: 0.017 | Tree loss: 9.215 | Accuracy: 0.162109 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 026 | Total loss: 9.200 | Reg loss: 0.018 | Tree loss: 9.200 | Accuracy: 0.158203 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 026 | Total loss: 9.181 | Reg loss: 0.018 | Tree loss: 9.181 | Accuracy: 0.187500 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 026 | Total loss: 9.179 | Reg loss: 0.018 | Tree loss: 9.179 | Accuracy: 0.169922 | 0.843 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 026 | Total loss: 9.172 | Reg loss: 0.019 | Tree loss: 9.172 | Accuracy: 0.103516 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 026 | Total loss: 9.160 | Reg loss: 0.019 | Tree loss: 9.160 | Accuracy: 0.144531 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 026 | Total loss: 9.133 | Reg loss: 0.020 | Tree loss: 9.133 | Accuracy: 0.164062 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 026 | Total loss: 9.115 | Reg loss: 0.020 | Tree loss: 9.115 | Accuracy: 0.152344 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 026 | Total loss: 9.110 | Reg loss: 0.020 | Tree loss: 9.110 | Accuracy: 0.156250 | 0.844 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 026 | Total loss: 9.113 | Reg loss: 0.021 | Tree loss: 9.113 | Accuracy: 0.122807 | 0.843 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 026 | Total loss: 9.258 | Reg loss: 0.015 | Tree loss: 9.258 | Accuracy: 0.154297 | 0.849 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 026 | Total loss: 9.249 | Reg loss: 0.015 | Tree loss: 9.249 | Accuracy: 0.156250 | 0.849 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 026 | Total loss: 9.230 | Reg loss: 0.015 | Tree loss: 9.230 | Accuracy: 0.140625 | 0.849 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 026 | Total loss: 9.233 | Reg loss: 0.015 | Tree loss: 9.233 | Accuracy: 0.126953 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 026 | Total loss: 9.203 | Reg loss: 0.016 | Tree loss: 9.203 | Accuracy: 0.132812 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 026 | Total loss: 9.188 | Reg loss: 0.016 | Tree loss: 9.188 | Accuracy: 0.160156 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 026 | Total loss: 9.181 | Reg loss: 0.016 | Tree loss: 9.181 | Accuracy: 0.156250 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 026 | Total loss: 9.159 | Reg loss: 0.016 | Tree loss: 9.159 | Accuracy: 0.138672 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 026 | Total loss: 9.138 | Reg loss: 0.016 | Tree loss: 9.138 | Accuracy: 0.132812 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 026 | Total loss: 9.120 | Reg loss: 0.017 | Tree loss: 9.120 | Accuracy: 0.154297 | 0.847 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 026 | Total loss: 9.107 | Reg loss: 0.017 | Tree loss: 9.107 | Accuracy: 0.158203 | 0.847 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 026 | Total loss: 9.079 | Reg loss: 0.017 | Tree loss: 9.079 | Accuracy: 0.167969 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 026 | Total loss: 9.084 | Reg loss: 0.018 | Tree loss: 9.084 | Accuracy: 0.117188 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 026 | Total loss: 9.069 | Reg loss: 0.018 | Tree loss: 9.069 | Accuracy: 0.175781 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 026 | Total loss: 9.049 | Reg loss: 0.018 | Tree loss: 9.049 | Accuracy: 0.175781 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 026 | Total loss: 9.041 | Reg loss: 0.019 | Tree loss: 9.041 | Accuracy: 0.136719 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 026 | Total loss: 9.005 | Reg loss: 0.019 | Tree loss: 9.005 | Accuracy: 0.158203 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 026 | Total loss: 9.001 | Reg loss: 0.020 | Tree loss: 9.001 | Accuracy: 0.128906 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 026 | Total loss: 8.975 | Reg loss: 0.020 | Tree loss: 8.975 | Accuracy: 0.160156 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 026 | Total loss: 8.962 | Reg loss: 0.020 | Tree loss: 8.962 | Accuracy: 0.136719 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 026 | Total loss: 8.948 | Reg loss: 0.021 | Tree loss: 8.948 | Accuracy: 0.132812 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 026 | Total loss: 8.929 | Reg loss: 0.021 | Tree loss: 8.929 | Accuracy: 0.166016 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 026 | Total loss: 8.903 | Reg loss: 0.022 | Tree loss: 8.903 | Accuracy: 0.138672 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 026 | Total loss: 8.909 | Reg loss: 0.022 | Tree loss: 8.909 | Accuracy: 0.099609 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 026 | Total loss: 8.894 | Reg loss: 0.022 | Tree loss: 8.894 | Accuracy: 0.121094 | 0.848 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 026 | Total loss: 8.839 | Reg loss: 0.023 | Tree loss: 8.839 | Accuracy: 0.210526 | 0.848 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 026 | Total loss: 9.084 | Reg loss: 0.017 | Tree loss: 9.084 | Accuracy: 0.130859 | 0.852 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 001 / 026 | Total loss: 9.075 | Reg loss: 0.017 | Tree loss: 9.075 | Accuracy: 0.123047 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 026 | Total loss: 9.068 | Reg loss: 0.017 | Tree loss: 9.068 | Accuracy: 0.134766 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 026 | Total loss: 9.035 | Reg loss: 0.017 | Tree loss: 9.035 | Accuracy: 0.160156 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 026 | Total loss: 9.012 | Reg loss: 0.018 | Tree loss: 9.012 | Accuracy: 0.167969 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 026 | Total loss: 8.997 | Reg loss: 0.018 | Tree loss: 8.997 | Accuracy: 0.140625 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 026 | Total loss: 8.985 | Reg loss: 0.018 | Tree loss: 8.985 | Accuracy: 0.152344 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 026 | Total loss: 8.958 | Reg loss: 0.018 | Tree loss: 8.958 | Accuracy: 0.150391 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 026 | Total loss: 8.948 | Reg loss: 0.018 | Tree loss: 8.948 | Accuracy: 0.125000 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 026 | Total loss: 8.909 | Reg loss: 0.019 | Tree loss: 8.909 | Accuracy: 0.130859 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 026 | Total loss: 8.888 | Reg loss: 0.019 | Tree loss: 8.888 | Accuracy: 0.148438 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 026 | Total loss: 8.884 | Reg loss: 0.019 | Tree loss: 8.884 | Accuracy: 0.125000 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 026 | Total loss: 8.855 | Reg loss: 0.019 | Tree loss: 8.855 | Accuracy: 0.171875 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 026 | Total loss: 8.840 | Reg loss: 0.020 | Tree loss: 8.840 | Accuracy: 0.123047 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 026 | Total loss: 8.808 | Reg loss: 0.020 | Tree loss: 8.808 | Accuracy: 0.150391 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 026 | Total loss: 8.789 | Reg loss: 0.020 | Tree loss: 8.789 | Accuracy: 0.130859 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 026 | Total loss: 8.767 | Reg loss: 0.021 | Tree loss: 8.767 | Accuracy: 0.152344 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 026 | Total loss: 8.754 | Reg loss: 0.021 | Tree loss: 8.754 | Accuracy: 0.154297 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 026 | Total loss: 8.753 | Reg loss: 0.021 | Tree loss: 8.753 | Accuracy: 0.115234 | 0.852 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 026 | Total loss: 8.723 | Reg loss: 0.022 | Tree loss: 8.723 | Accuracy: 0.105469 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 026 | Total loss: 8.698 | Reg loss: 0.022 | Tree loss: 8.698 | Accuracy: 0.107422 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 026 | Total loss: 8.700 | Reg loss: 0.022 | Tree loss: 8.700 | Accuracy: 0.130859 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 026 | Total loss: 8.642 | Reg loss: 0.023 | Tree loss: 8.642 | Accuracy: 0.130859 | 0.851 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 026 | Total loss: 8.630 | Reg loss: 0.023 | Tree loss: 8.630 | Accuracy: 0.119141 | 0.85 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 026 | Total loss: 8.610 | Reg loss: 0.023 | Tree loss: 8.610 | Accuracy: 0.146484 | 0.85 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 026 | Total loss: 8.693 | Reg loss: 0.024 | Tree loss: 8.693 | Accuracy: 0.122807 | 0.85 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 026 | Total loss: 8.869 | Reg loss: 0.019 | Tree loss: 8.869 | Accuracy: 0.156250 | 0.854 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 026 | Total loss: 8.864 | Reg loss: 0.019 | Tree loss: 8.864 | Accuracy: 0.142578 | 0.854 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 026 | Total loss: 8.811 | Reg loss: 0.019 | Tree loss: 8.811 | Accuracy: 0.173828 | 0.854 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 026 | Total loss: 8.809 | Reg loss: 0.019 | Tree loss: 8.809 | Accuracy: 0.154297 | 0.854 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 026 | Total loss: 8.800 | Reg loss: 0.019 | Tree loss: 8.800 | Accuracy: 0.126953 | 0.854 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 026 | Total loss: 8.769 | Reg loss: 0.019 | Tree loss: 8.769 | Accuracy: 0.152344 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 026 | Total loss: 8.754 | Reg loss: 0.019 | Tree loss: 8.754 | Accuracy: 0.134766 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 026 | Total loss: 8.733 | Reg loss: 0.020 | Tree loss: 8.733 | Accuracy: 0.123047 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 026 | Total loss: 8.728 | Reg loss: 0.020 | Tree loss: 8.728 | Accuracy: 0.144531 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 026 | Total loss: 8.704 | Reg loss: 0.020 | Tree loss: 8.704 | Accuracy: 0.115234 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 026 | Total loss: 8.654 | Reg loss: 0.020 | Tree loss: 8.654 | Accuracy: 0.111328 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 026 | Total loss: 8.642 | Reg loss: 0.020 | Tree loss: 8.642 | Accuracy: 0.162109 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 026 | Total loss: 8.569 | Reg loss: 0.021 | Tree loss: 8.569 | Accuracy: 0.148438 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 026 | Total loss: 8.584 | Reg loss: 0.021 | Tree loss: 8.584 | Accuracy: 0.125000 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 026 | Total loss: 8.582 | Reg loss: 0.021 | Tree loss: 8.582 | Accuracy: 0.125000 | 0.853 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 026 | Total loss: 8.560 | Reg loss: 0.022 | Tree loss: 8.560 | Accuracy: 0.115234 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 026 | Total loss: 8.527 | Reg loss: 0.022 | Tree loss: 8.527 | Accuracy: 0.134766 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 026 | Total loss: 8.504 | Reg loss: 0.022 | Tree loss: 8.504 | Accuracy: 0.136719 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 026 | Total loss: 8.487 | Reg loss: 0.022 | Tree loss: 8.487 | Accuracy: 0.138672 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 026 | Total loss: 8.461 | Reg loss: 0.023 | Tree loss: 8.461 | Accuracy: 0.142578 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 026 | Total loss: 8.465 | Reg loss: 0.023 | Tree loss: 8.465 | Accuracy: 0.103516 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 026 | Total loss: 8.436 | Reg loss: 0.023 | Tree loss: 8.436 | Accuracy: 0.107422 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 026 | Total loss: 8.381 | Reg loss: 0.024 | Tree loss: 8.381 | Accuracy: 0.125000 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 026 | Total loss: 8.385 | Reg loss: 0.024 | Tree loss: 8.385 | Accuracy: 0.115234 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 026 | Total loss: 8.339 | Reg loss: 0.024 | Tree loss: 8.339 | Accuracy: 0.136719 | 0.852 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 026 | Total loss: 8.340 | Reg loss: 0.024 | Tree loss: 8.340 | Accuracy: 0.122807 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 026 | Total loss: 8.640 | Reg loss: 0.020 | Tree loss: 8.640 | Accuracy: 0.136719 | 0.855 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 026 | Total loss: 8.615 | Reg loss: 0.020 | Tree loss: 8.615 | Accuracy: 0.146484 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 026 | Total loss: 8.586 | Reg loss: 0.020 | Tree loss: 8.586 | Accuracy: 0.126953 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 026 | Total loss: 8.562 | Reg loss: 0.020 | Tree loss: 8.562 | Accuracy: 0.136719 | 0.855 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 026 | Total loss: 8.566 | Reg loss: 0.020 | Tree loss: 8.566 | Accuracy: 0.136719 | 0.855 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 026 | Total loss: 8.538 | Reg loss: 0.021 | Tree loss: 8.538 | Accuracy: 0.132812 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 026 | Total loss: 8.511 | Reg loss: 0.021 | Tree loss: 8.511 | Accuracy: 0.130859 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 026 | Total loss: 8.469 | Reg loss: 0.021 | Tree loss: 8.469 | Accuracy: 0.121094 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 026 | Total loss: 8.460 | Reg loss: 0.021 | Tree loss: 8.460 | Accuracy: 0.150391 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 026 | Total loss: 8.439 | Reg loss: 0.021 | Tree loss: 8.439 | Accuracy: 0.130859 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 026 | Total loss: 8.406 | Reg loss: 0.021 | Tree loss: 8.406 | Accuracy: 0.164062 | 0.854 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 011 / 026 | Total loss: 8.390 | Reg loss: 0.022 | Tree loss: 8.390 | Accuracy: 0.119141 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 026 | Total loss: 8.336 | Reg loss: 0.022 | Tree loss: 8.336 | Accuracy: 0.123047 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 026 | Total loss: 8.322 | Reg loss: 0.022 | Tree loss: 8.322 | Accuracy: 0.152344 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 026 | Total loss: 8.319 | Reg loss: 0.022 | Tree loss: 8.319 | Accuracy: 0.117188 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 026 | Total loss: 8.288 | Reg loss: 0.022 | Tree loss: 8.288 | Accuracy: 0.154297 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 026 | Total loss: 8.255 | Reg loss: 0.023 | Tree loss: 8.255 | Accuracy: 0.136719 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 026 | Total loss: 8.270 | Reg loss: 0.023 | Tree loss: 8.270 | Accuracy: 0.123047 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 026 | Total loss: 8.241 | Reg loss: 0.023 | Tree loss: 8.241 | Accuracy: 0.128906 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 026 | Total loss: 8.209 | Reg loss: 0.023 | Tree loss: 8.209 | Accuracy: 0.130859 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 026 | Total loss: 8.203 | Reg loss: 0.024 | Tree loss: 8.203 | Accuracy: 0.099609 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 026 | Total loss: 8.165 | Reg loss: 0.024 | Tree loss: 8.165 | Accuracy: 0.121094 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 026 | Total loss: 8.142 | Reg loss: 0.024 | Tree loss: 8.142 | Accuracy: 0.115234 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 026 | Total loss: 8.118 | Reg loss: 0.024 | Tree loss: 8.118 | Accuracy: 0.103516 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 026 | Total loss: 8.105 | Reg loss: 0.025 | Tree loss: 8.105 | Accuracy: 0.119141 | 0.854 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 026 | Total loss: 8.034 | Reg loss: 0.025 | Tree loss: 8.034 | Accuracy: 0.087719 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 026 | Total loss: 8.394 | Reg loss: 0.021 | Tree loss: 8.394 | Accuracy: 0.128906 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 026 | Total loss: 8.369 | Reg loss: 0.021 | Tree loss: 8.369 | Accuracy: 0.130859 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 026 | Total loss: 8.336 | Reg loss: 0.021 | Tree loss: 8.336 | Accuracy: 0.152344 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 026 | Total loss: 8.288 | Reg loss: 0.021 | Tree loss: 8.288 | Accuracy: 0.162109 | 0.857 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 026 | Total loss: 8.345 | Reg loss: 0.021 | Tree loss: 8.345 | Accuracy: 0.117188 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 026 | Total loss: 8.276 | Reg loss: 0.022 | Tree loss: 8.276 | Accuracy: 0.125000 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 026 | Total loss: 8.235 | Reg loss: 0.022 | Tree loss: 8.235 | Accuracy: 0.138672 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 026 | Total loss: 8.215 | Reg loss: 0.022 | Tree loss: 8.215 | Accuracy: 0.125000 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 026 | Total loss: 8.190 | Reg loss: 0.022 | Tree loss: 8.190 | Accuracy: 0.111328 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 026 | Total loss: 8.187 | Reg loss: 0.022 | Tree loss: 8.187 | Accuracy: 0.130859 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 026 | Total loss: 8.136 | Reg loss: 0.022 | Tree loss: 8.136 | Accuracy: 0.142578 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 026 | Total loss: 8.134 | Reg loss: 0.022 | Tree loss: 8.134 | Accuracy: 0.123047 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 026 | Total loss: 8.103 | Reg loss: 0.022 | Tree loss: 8.103 | Accuracy: 0.138672 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 026 | Total loss: 8.079 | Reg loss: 0.023 | Tree loss: 8.079 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 026 | Total loss: 8.060 | Reg loss: 0.023 | Tree loss: 8.060 | Accuracy: 0.128906 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 026 | Total loss: 8.011 | Reg loss: 0.023 | Tree loss: 8.011 | Accuracy: 0.111328 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 026 | Total loss: 8.031 | Reg loss: 0.023 | Tree loss: 8.031 | Accuracy: 0.126953 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 026 | Total loss: 8.006 | Reg loss: 0.023 | Tree loss: 8.006 | Accuracy: 0.117188 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 026 | Total loss: 7.988 | Reg loss: 0.024 | Tree loss: 7.988 | Accuracy: 0.138672 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 026 | Total loss: 7.957 | Reg loss: 0.024 | Tree loss: 7.957 | Accuracy: 0.146484 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 026 | Total loss: 7.938 | Reg loss: 0.024 | Tree loss: 7.938 | Accuracy: 0.146484 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 026 | Total loss: 7.919 | Reg loss: 0.024 | Tree loss: 7.919 | Accuracy: 0.101562 | 0.855 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 026 | Total loss: 7.856 | Reg loss: 0.024 | Tree loss: 7.856 | Accuracy: 0.125000 | 0.854 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 026 | Total loss: 7.875 | Reg loss: 0.025 | Tree loss: 7.875 | Accuracy: 0.154297 | 0.854 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 026 | Total loss: 7.843 | Reg loss: 0.025 | Tree loss: 7.843 | Accuracy: 0.134766 | 0.854 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 026 | Total loss: 7.753 | Reg loss: 0.025 | Tree loss: 7.753 | Accuracy: 0.122807 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 026 | Total loss: 8.132 | Reg loss: 0.022 | Tree loss: 8.132 | Accuracy: 0.123047 | 0.858 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 026 | Total loss: 8.096 | Reg loss: 0.022 | Tree loss: 8.096 | Accuracy: 0.140625 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 026 | Total loss: 8.085 | Reg loss: 0.022 | Tree loss: 8.085 | Accuracy: 0.158203 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 026 | Total loss: 8.060 | Reg loss: 0.022 | Tree loss: 8.060 | Accuracy: 0.121094 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 026 | Total loss: 8.005 | Reg loss: 0.022 | Tree loss: 8.005 | Accuracy: 0.130859 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 026 | Total loss: 8.010 | Reg loss: 0.022 | Tree loss: 8.010 | Accuracy: 0.123047 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 026 | Total loss: 7.984 | Reg loss: 0.022 | Tree loss: 7.984 | Accuracy: 0.162109 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 026 | Total loss: 7.966 | Reg loss: 0.022 | Tree loss: 7.966 | Accuracy: 0.119141 | 0.857 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 026 | Total loss: 7.958 | Reg loss: 0.023 | Tree loss: 7.958 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 026 | Total loss: 7.902 | Reg loss: 0.023 | Tree loss: 7.902 | Accuracy: 0.156250 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 026 | Total loss: 7.897 | Reg loss: 0.023 | Tree loss: 7.897 | Accuracy: 0.117188 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 026 | Total loss: 7.883 | Reg loss: 0.023 | Tree loss: 7.883 | Accuracy: 0.111328 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 026 | Total loss: 7.824 | Reg loss: 0.023 | Tree loss: 7.824 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 026 | Total loss: 7.841 | Reg loss: 0.023 | Tree loss: 7.841 | Accuracy: 0.123047 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 026 | Total loss: 7.810 | Reg loss: 0.023 | Tree loss: 7.810 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 026 | Total loss: 7.791 | Reg loss: 0.023 | Tree loss: 7.791 | Accuracy: 0.152344 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 026 | Total loss: 7.761 | Reg loss: 0.024 | Tree loss: 7.761 | Accuracy: 0.125000 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 026 | Total loss: 7.735 | Reg loss: 0.024 | Tree loss: 7.735 | Accuracy: 0.136719 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 026 | Total loss: 7.701 | Reg loss: 0.024 | Tree loss: 7.701 | Accuracy: 0.119141 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 026 | Total loss: 7.744 | Reg loss: 0.024 | Tree loss: 7.744 | Accuracy: 0.111328 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 026 | Total loss: 7.666 | Reg loss: 0.024 | Tree loss: 7.666 | Accuracy: 0.132812 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 021 / 026 | Total loss: 7.673 | Reg loss: 0.024 | Tree loss: 7.673 | Accuracy: 0.138672 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 026 | Total loss: 7.636 | Reg loss: 0.025 | Tree loss: 7.636 | Accuracy: 0.105469 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 026 | Total loss: 7.593 | Reg loss: 0.025 | Tree loss: 7.593 | Accuracy: 0.144531 | 0.856 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 026 | Total loss: 7.598 | Reg loss: 0.025 | Tree loss: 7.598 | Accuracy: 0.093750 | 0.855 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 026 | Total loss: 7.499 | Reg loss: 0.025 | Tree loss: 7.499 | Accuracy: 0.192982 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 026 | Total loss: 7.839 | Reg loss: 0.023 | Tree loss: 7.839 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 026 | Total loss: 7.853 | Reg loss: 0.023 | Tree loss: 7.853 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 026 | Total loss: 7.799 | Reg loss: 0.023 | Tree loss: 7.799 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 026 | Total loss: 7.795 | Reg loss: 0.023 | Tree loss: 7.795 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 026 | Total loss: 7.783 | Reg loss: 0.023 | Tree loss: 7.783 | Accuracy: 0.117188 | 0.858 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 026 | Total loss: 7.751 | Reg loss: 0.023 | Tree loss: 7.751 | Accuracy: 0.128906 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 026 | Total loss: 7.734 | Reg loss: 0.023 | Tree loss: 7.734 | Accuracy: 0.126953 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 026 | Total loss: 7.707 | Reg loss: 0.023 | Tree loss: 7.707 | Accuracy: 0.126953 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 026 | Total loss: 7.668 | Reg loss: 0.023 | Tree loss: 7.668 | Accuracy: 0.125000 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 026 | Total loss: 7.662 | Reg loss: 0.023 | Tree loss: 7.662 | Accuracy: 0.126953 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 026 | Total loss: 7.636 | Reg loss: 0.023 | Tree loss: 7.636 | Accuracy: 0.132812 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 026 | Total loss: 7.611 | Reg loss: 0.023 | Tree loss: 7.611 | Accuracy: 0.128906 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 026 | Total loss: 7.611 | Reg loss: 0.024 | Tree loss: 7.611 | Accuracy: 0.140625 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 026 | Total loss: 7.588 | Reg loss: 0.024 | Tree loss: 7.588 | Accuracy: 0.144531 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 026 | Total loss: 7.573 | Reg loss: 0.024 | Tree loss: 7.573 | Accuracy: 0.130859 | 0.857 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 026 | Total loss: 7.556 | Reg loss: 0.024 | Tree loss: 7.556 | Accuracy: 0.109375 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 026 | Total loss: 7.506 | Reg loss: 0.024 | Tree loss: 7.506 | Accuracy: 0.117188 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 026 | Total loss: 7.487 | Reg loss: 0.024 | Tree loss: 7.487 | Accuracy: 0.121094 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 026 | Total loss: 7.441 | Reg loss: 0.024 | Tree loss: 7.441 | Accuracy: 0.136719 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 026 | Total loss: 7.432 | Reg loss: 0.024 | Tree loss: 7.432 | Accuracy: 0.150391 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 026 | Total loss: 7.429 | Reg loss: 0.025 | Tree loss: 7.429 | Accuracy: 0.123047 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 026 | Total loss: 7.426 | Reg loss: 0.025 | Tree loss: 7.426 | Accuracy: 0.111328 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 026 | Total loss: 7.358 | Reg loss: 0.025 | Tree loss: 7.358 | Accuracy: 0.142578 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 026 | Total loss: 7.401 | Reg loss: 0.025 | Tree loss: 7.401 | Accuracy: 0.119141 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 026 | Total loss: 7.331 | Reg loss: 0.025 | Tree loss: 7.331 | Accuracy: 0.144531 | 0.856 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 026 | Total loss: 7.456 | Reg loss: 0.025 | Tree loss: 7.456 | Accuracy: 0.070175 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 026 | Total loss: 7.619 | Reg loss: 0.023 | Tree loss: 7.619 | Accuracy: 0.117188 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 026 | Total loss: 7.571 | Reg loss: 0.023 | Tree loss: 7.571 | Accuracy: 0.138672 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 026 | Total loss: 7.533 | Reg loss: 0.023 | Tree loss: 7.533 | Accuracy: 0.109375 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 026 | Total loss: 7.507 | Reg loss: 0.023 | Tree loss: 7.507 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 026 | Total loss: 7.515 | Reg loss: 0.023 | Tree loss: 7.515 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 026 | Total loss: 7.543 | Reg loss: 0.023 | Tree loss: 7.543 | Accuracy: 0.107422 | 0.858 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 026 | Total loss: 7.446 | Reg loss: 0.023 | Tree loss: 7.446 | Accuracy: 0.156250 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 026 | Total loss: 7.475 | Reg loss: 0.023 | Tree loss: 7.475 | Accuracy: 0.115234 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 026 | Total loss: 7.455 | Reg loss: 0.024 | Tree loss: 7.455 | Accuracy: 0.123047 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 026 | Total loss: 7.426 | Reg loss: 0.024 | Tree loss: 7.426 | Accuracy: 0.126953 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 026 | Total loss: 7.358 | Reg loss: 0.024 | Tree loss: 7.358 | Accuracy: 0.136719 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 026 | Total loss: 7.347 | Reg loss: 0.024 | Tree loss: 7.347 | Accuracy: 0.130859 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 026 | Total loss: 7.335 | Reg loss: 0.024 | Tree loss: 7.335 | Accuracy: 0.142578 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 026 | Total loss: 7.339 | Reg loss: 0.024 | Tree loss: 7.339 | Accuracy: 0.132812 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 026 | Total loss: 7.324 | Reg loss: 0.024 | Tree loss: 7.324 | Accuracy: 0.138672 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 026 | Total loss: 7.330 | Reg loss: 0.024 | Tree loss: 7.330 | Accuracy: 0.121094 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 026 | Total loss: 7.252 | Reg loss: 0.024 | Tree loss: 7.252 | Accuracy: 0.130859 | 0.857 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 026 | Total loss: 7.244 | Reg loss: 0.024 | Tree loss: 7.244 | Accuracy: 0.148438 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 026 | Total loss: 7.268 | Reg loss: 0.025 | Tree loss: 7.268 | Accuracy: 0.103516 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 026 | Total loss: 7.220 | Reg loss: 0.025 | Tree loss: 7.220 | Accuracy: 0.128906 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 026 | Total loss: 7.195 | Reg loss: 0.025 | Tree loss: 7.195 | Accuracy: 0.125000 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 026 | Total loss: 7.170 | Reg loss: 0.025 | Tree loss: 7.170 | Accuracy: 0.150391 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 026 | Total loss: 7.136 | Reg loss: 0.025 | Tree loss: 7.136 | Accuracy: 0.121094 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 026 | Total loss: 7.075 | Reg loss: 0.025 | Tree loss: 7.075 | Accuracy: 0.156250 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 026 | Total loss: 7.094 | Reg loss: 0.025 | Tree loss: 7.094 | Accuracy: 0.128906 | 0.856 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 026 | Total loss: 7.122 | Reg loss: 0.025 | Tree loss: 7.122 | Accuracy: 0.122807 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 026 | Total loss: 7.335 | Reg loss: 0.024 | Tree loss: 7.335 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 026 | Total loss: 7.375 | Reg loss: 0.024 | Tree loss: 7.375 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 026 | Total loss: 7.315 | Reg loss: 0.024 | Tree loss: 7.315 | Accuracy: 0.140625 | 0.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 003 / 026 | Total loss: 7.317 | Reg loss: 0.024 | Tree loss: 7.317 | Accuracy: 0.115234 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 026 | Total loss: 7.277 | Reg loss: 0.024 | Tree loss: 7.277 | Accuracy: 0.132812 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 026 | Total loss: 7.256 | Reg loss: 0.024 | Tree loss: 7.256 | Accuracy: 0.134766 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 026 | Total loss: 7.215 | Reg loss: 0.024 | Tree loss: 7.215 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 026 | Total loss: 7.202 | Reg loss: 0.024 | Tree loss: 7.202 | Accuracy: 0.117188 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 026 | Total loss: 7.183 | Reg loss: 0.024 | Tree loss: 7.183 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 026 | Total loss: 7.164 | Reg loss: 0.024 | Tree loss: 7.164 | Accuracy: 0.128906 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 026 | Total loss: 7.135 | Reg loss: 0.024 | Tree loss: 7.135 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 026 | Total loss: 7.113 | Reg loss: 0.024 | Tree loss: 7.113 | Accuracy: 0.099609 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 026 | Total loss: 7.117 | Reg loss: 0.024 | Tree loss: 7.117 | Accuracy: 0.142578 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 026 | Total loss: 7.075 | Reg loss: 0.024 | Tree loss: 7.075 | Accuracy: 0.113281 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 026 | Total loss: 7.079 | Reg loss: 0.024 | Tree loss: 7.079 | Accuracy: 0.109375 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 026 | Total loss: 7.060 | Reg loss: 0.024 | Tree loss: 7.060 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 026 | Total loss: 6.993 | Reg loss: 0.025 | Tree loss: 6.993 | Accuracy: 0.132812 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 026 | Total loss: 7.012 | Reg loss: 0.025 | Tree loss: 7.012 | Accuracy: 0.128906 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 026 | Total loss: 6.993 | Reg loss: 0.025 | Tree loss: 6.993 | Accuracy: 0.109375 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 026 | Total loss: 6.957 | Reg loss: 0.025 | Tree loss: 6.957 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 026 | Total loss: 6.916 | Reg loss: 0.025 | Tree loss: 6.916 | Accuracy: 0.144531 | 0.858 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 026 | Total loss: 6.932 | Reg loss: 0.025 | Tree loss: 6.932 | Accuracy: 0.119141 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 026 | Total loss: 6.934 | Reg loss: 0.025 | Tree loss: 6.934 | Accuracy: 0.132812 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 026 | Total loss: 6.896 | Reg loss: 0.025 | Tree loss: 6.896 | Accuracy: 0.126953 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 026 | Total loss: 6.910 | Reg loss: 0.025 | Tree loss: 6.910 | Accuracy: 0.144531 | 0.857 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 026 | Total loss: 6.912 | Reg loss: 0.025 | Tree loss: 6.912 | Accuracy: 0.105263 | 0.857 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 026 | Total loss: 7.129 | Reg loss: 0.024 | Tree loss: 7.129 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 026 | Total loss: 7.095 | Reg loss: 0.024 | Tree loss: 7.095 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 026 | Total loss: 7.070 | Reg loss: 0.024 | Tree loss: 7.070 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 026 | Total loss: 7.025 | Reg loss: 0.024 | Tree loss: 7.025 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 026 | Total loss: 7.030 | Reg loss: 0.024 | Tree loss: 7.030 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 026 | Total loss: 6.989 | Reg loss: 0.024 | Tree loss: 6.989 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 026 | Total loss: 6.985 | Reg loss: 0.024 | Tree loss: 6.985 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 026 | Total loss: 6.982 | Reg loss: 0.024 | Tree loss: 6.982 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 026 | Total loss: 6.962 | Reg loss: 0.024 | Tree loss: 6.962 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 026 | Total loss: 6.926 | Reg loss: 0.024 | Tree loss: 6.926 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 026 | Total loss: 6.907 | Reg loss: 0.024 | Tree loss: 6.907 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 026 | Total loss: 6.905 | Reg loss: 0.024 | Tree loss: 6.905 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 026 | Total loss: 6.868 | Reg loss: 0.024 | Tree loss: 6.868 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 026 | Total loss: 6.820 | Reg loss: 0.024 | Tree loss: 6.820 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 026 | Total loss: 6.807 | Reg loss: 0.025 | Tree loss: 6.807 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 026 | Total loss: 6.800 | Reg loss: 0.025 | Tree loss: 6.800 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 026 | Total loss: 6.822 | Reg loss: 0.025 | Tree loss: 6.822 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 026 | Total loss: 6.759 | Reg loss: 0.025 | Tree loss: 6.759 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 026 | Total loss: 6.784 | Reg loss: 0.025 | Tree loss: 6.784 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 026 | Total loss: 6.750 | Reg loss: 0.025 | Tree loss: 6.750 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 026 | Total loss: 6.733 | Reg loss: 0.025 | Tree loss: 6.733 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 026 | Total loss: 6.723 | Reg loss: 0.025 | Tree loss: 6.723 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 026 | Total loss: 6.698 | Reg loss: 0.025 | Tree loss: 6.698 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 026 | Total loss: 6.687 | Reg loss: 0.025 | Tree loss: 6.687 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 026 | Total loss: 6.674 | Reg loss: 0.025 | Tree loss: 6.674 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 026 | Total loss: 6.671 | Reg loss: 0.025 | Tree loss: 6.671 | Accuracy: 0.105263 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 026 | Total loss: 6.867 | Reg loss: 0.024 | Tree loss: 6.867 | Accuracy: 0.123047 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 026 | Total loss: 6.857 | Reg loss: 0.024 | Tree loss: 6.857 | Accuracy: 0.109375 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 026 | Total loss: 6.823 | Reg loss: 0.024 | Tree loss: 6.823 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 026 | Total loss: 6.821 | Reg loss: 0.024 | Tree loss: 6.821 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 026 | Total loss: 6.840 | Reg loss: 0.024 | Tree loss: 6.840 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 026 | Total loss: 6.775 | Reg loss: 0.024 | Tree loss: 6.775 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 026 | Total loss: 6.769 | Reg loss: 0.024 | Tree loss: 6.769 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 026 | Total loss: 6.733 | Reg loss: 0.024 | Tree loss: 6.733 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 026 | Total loss: 6.752 | Reg loss: 0.024 | Tree loss: 6.752 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 026 | Total loss: 6.694 | Reg loss: 0.024 | Tree loss: 6.694 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 026 | Total loss: 6.660 | Reg loss: 0.024 | Tree loss: 6.660 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 026 | Total loss: 6.658 | Reg loss: 0.025 | Tree loss: 6.658 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 026 | Total loss: 6.665 | Reg loss: 0.025 | Tree loss: 6.665 | Accuracy: 0.117188 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 013 / 026 | Total loss: 6.596 | Reg loss: 0.025 | Tree loss: 6.596 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 026 | Total loss: 6.629 | Reg loss: 0.025 | Tree loss: 6.629 | Accuracy: 0.113281 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 026 | Total loss: 6.588 | Reg loss: 0.025 | Tree loss: 6.588 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 026 | Total loss: 6.616 | Reg loss: 0.025 | Tree loss: 6.616 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 026 | Total loss: 6.534 | Reg loss: 0.025 | Tree loss: 6.534 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 026 | Total loss: 6.534 | Reg loss: 0.025 | Tree loss: 6.534 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 026 | Total loss: 6.545 | Reg loss: 0.025 | Tree loss: 6.545 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 026 | Total loss: 6.524 | Reg loss: 0.025 | Tree loss: 6.524 | Accuracy: 0.103516 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 026 | Total loss: 6.490 | Reg loss: 0.025 | Tree loss: 6.490 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 026 | Total loss: 6.455 | Reg loss: 0.025 | Tree loss: 6.455 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 026 | Total loss: 6.447 | Reg loss: 0.025 | Tree loss: 6.447 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 026 | Total loss: 6.427 | Reg loss: 0.025 | Tree loss: 6.427 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 026 | Total loss: 6.384 | Reg loss: 0.026 | Tree loss: 6.384 | Accuracy: 0.070175 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 026 | Total loss: 6.628 | Reg loss: 0.024 | Tree loss: 6.628 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 026 | Total loss: 6.618 | Reg loss: 0.024 | Tree loss: 6.618 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 026 | Total loss: 6.573 | Reg loss: 0.024 | Tree loss: 6.573 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 026 | Total loss: 6.592 | Reg loss: 0.024 | Tree loss: 6.592 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 026 | Total loss: 6.541 | Reg loss: 0.024 | Tree loss: 6.541 | Accuracy: 0.123047 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 026 | Total loss: 6.573 | Reg loss: 0.024 | Tree loss: 6.573 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 026 | Total loss: 6.516 | Reg loss: 0.024 | Tree loss: 6.516 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 026 | Total loss: 6.546 | Reg loss: 0.025 | Tree loss: 6.546 | Accuracy: 0.115234 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 026 | Total loss: 6.495 | Reg loss: 0.025 | Tree loss: 6.495 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 026 | Total loss: 6.461 | Reg loss: 0.025 | Tree loss: 6.461 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 026 | Total loss: 6.484 | Reg loss: 0.025 | Tree loss: 6.484 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 026 | Total loss: 6.450 | Reg loss: 0.025 | Tree loss: 6.450 | Accuracy: 0.111328 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 026 | Total loss: 6.411 | Reg loss: 0.025 | Tree loss: 6.411 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 026 | Total loss: 6.437 | Reg loss: 0.025 | Tree loss: 6.437 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 026 | Total loss: 6.428 | Reg loss: 0.025 | Tree loss: 6.428 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 026 | Total loss: 6.367 | Reg loss: 0.025 | Tree loss: 6.367 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 026 | Total loss: 6.341 | Reg loss: 0.025 | Tree loss: 6.341 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 026 | Total loss: 6.347 | Reg loss: 0.025 | Tree loss: 6.347 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 026 | Total loss: 6.341 | Reg loss: 0.025 | Tree loss: 6.341 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 026 | Total loss: 6.303 | Reg loss: 0.025 | Tree loss: 6.303 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 026 | Total loss: 6.280 | Reg loss: 0.025 | Tree loss: 6.280 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 026 | Total loss: 6.321 | Reg loss: 0.025 | Tree loss: 6.321 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 026 | Total loss: 6.296 | Reg loss: 0.025 | Tree loss: 6.296 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 026 | Total loss: 6.271 | Reg loss: 0.025 | Tree loss: 6.271 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 026 | Total loss: 6.234 | Reg loss: 0.026 | Tree loss: 6.234 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 026 | Total loss: 6.363 | Reg loss: 0.026 | Tree loss: 6.363 | Accuracy: 0.105263 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 026 | Total loss: 6.392 | Reg loss: 0.025 | Tree loss: 6.392 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 026 | Total loss: 6.407 | Reg loss: 0.025 | Tree loss: 6.407 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 026 | Total loss: 6.377 | Reg loss: 0.025 | Tree loss: 6.377 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 026 | Total loss: 6.340 | Reg loss: 0.025 | Tree loss: 6.340 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 026 | Total loss: 6.319 | Reg loss: 0.025 | Tree loss: 6.319 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 026 | Total loss: 6.381 | Reg loss: 0.025 | Tree loss: 6.381 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 026 | Total loss: 6.319 | Reg loss: 0.025 | Tree loss: 6.319 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 026 | Total loss: 6.294 | Reg loss: 0.025 | Tree loss: 6.294 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 026 | Total loss: 6.264 | Reg loss: 0.025 | Tree loss: 6.264 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 026 | Total loss: 6.270 | Reg loss: 0.025 | Tree loss: 6.270 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 026 | Total loss: 6.266 | Reg loss: 0.025 | Tree loss: 6.266 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 026 | Total loss: 6.239 | Reg loss: 0.025 | Tree loss: 6.239 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 026 | Total loss: 6.217 | Reg loss: 0.025 | Tree loss: 6.217 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 026 | Total loss: 6.210 | Reg loss: 0.025 | Tree loss: 6.210 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 026 | Total loss: 6.219 | Reg loss: 0.025 | Tree loss: 6.219 | Accuracy: 0.111328 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 026 | Total loss: 6.172 | Reg loss: 0.025 | Tree loss: 6.172 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 026 | Total loss: 6.156 | Reg loss: 0.025 | Tree loss: 6.156 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 026 | Total loss: 6.131 | Reg loss: 0.025 | Tree loss: 6.131 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 026 | Total loss: 6.132 | Reg loss: 0.025 | Tree loss: 6.132 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 026 | Total loss: 6.118 | Reg loss: 0.025 | Tree loss: 6.118 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 026 | Total loss: 6.081 | Reg loss: 0.025 | Tree loss: 6.081 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 026 | Total loss: 6.130 | Reg loss: 0.025 | Tree loss: 6.130 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 026 | Total loss: 6.103 | Reg loss: 0.025 | Tree loss: 6.103 | Accuracy: 0.125000 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 023 / 026 | Total loss: 6.064 | Reg loss: 0.025 | Tree loss: 6.064 | Accuracy: 0.125000 | 0.858 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 026 | Total loss: 6.053 | Reg loss: 0.026 | Tree loss: 6.053 | Accuracy: 0.123047 | 0.858 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 026 | Total loss: 5.975 | Reg loss: 0.026 | Tree loss: 5.975 | Accuracy: 0.140351 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 026 | Total loss: 6.178 | Reg loss: 0.025 | Tree loss: 6.178 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 026 | Total loss: 6.174 | Reg loss: 0.025 | Tree loss: 6.174 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 026 | Total loss: 6.174 | Reg loss: 0.025 | Tree loss: 6.174 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 026 | Total loss: 6.174 | Reg loss: 0.025 | Tree loss: 6.174 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 026 | Total loss: 6.162 | Reg loss: 0.025 | Tree loss: 6.162 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 026 | Total loss: 6.126 | Reg loss: 0.025 | Tree loss: 6.126 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 026 | Total loss: 6.103 | Reg loss: 0.025 | Tree loss: 6.103 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 026 | Total loss: 6.082 | Reg loss: 0.025 | Tree loss: 6.082 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 026 | Total loss: 6.123 | Reg loss: 0.025 | Tree loss: 6.123 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 026 | Total loss: 6.086 | Reg loss: 0.025 | Tree loss: 6.086 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 026 | Total loss: 6.020 | Reg loss: 0.025 | Tree loss: 6.020 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 026 | Total loss: 5.999 | Reg loss: 0.025 | Tree loss: 5.999 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 026 | Total loss: 5.995 | Reg loss: 0.025 | Tree loss: 5.995 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 026 | Total loss: 6.016 | Reg loss: 0.025 | Tree loss: 6.016 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 026 | Total loss: 5.989 | Reg loss: 0.025 | Tree loss: 5.989 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 026 | Total loss: 5.963 | Reg loss: 0.025 | Tree loss: 5.963 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 026 | Total loss: 5.986 | Reg loss: 0.025 | Tree loss: 5.986 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 026 | Total loss: 5.961 | Reg loss: 0.025 | Tree loss: 5.961 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 026 | Total loss: 5.957 | Reg loss: 0.025 | Tree loss: 5.957 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 026 | Total loss: 5.941 | Reg loss: 0.025 | Tree loss: 5.941 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 026 | Total loss: 5.909 | Reg loss: 0.025 | Tree loss: 5.909 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 026 | Total loss: 5.929 | Reg loss: 0.025 | Tree loss: 5.929 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 026 | Total loss: 5.896 | Reg loss: 0.025 | Tree loss: 5.896 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 026 | Total loss: 5.875 | Reg loss: 0.025 | Tree loss: 5.875 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 026 | Total loss: 5.827 | Reg loss: 0.026 | Tree loss: 5.827 | Accuracy: 0.132812 | 0.858 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 026 | Total loss: 5.769 | Reg loss: 0.026 | Tree loss: 5.769 | Accuracy: 0.192982 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 026 | Total loss: 5.978 | Reg loss: 0.025 | Tree loss: 5.978 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 026 | Total loss: 5.979 | Reg loss: 0.025 | Tree loss: 5.979 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 026 | Total loss: 5.974 | Reg loss: 0.025 | Tree loss: 5.974 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 026 | Total loss: 5.945 | Reg loss: 0.025 | Tree loss: 5.945 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 026 | Total loss: 5.951 | Reg loss: 0.025 | Tree loss: 5.951 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 026 | Total loss: 5.919 | Reg loss: 0.025 | Tree loss: 5.919 | Accuracy: 0.111328 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 026 | Total loss: 5.900 | Reg loss: 0.025 | Tree loss: 5.900 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 026 | Total loss: 5.887 | Reg loss: 0.025 | Tree loss: 5.887 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 026 | Total loss: 5.877 | Reg loss: 0.025 | Tree loss: 5.877 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 026 | Total loss: 5.819 | Reg loss: 0.025 | Tree loss: 5.819 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 026 | Total loss: 5.852 | Reg loss: 0.025 | Tree loss: 5.852 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 026 | Total loss: 5.874 | Reg loss: 0.025 | Tree loss: 5.874 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 026 | Total loss: 5.824 | Reg loss: 0.025 | Tree loss: 5.824 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 026 | Total loss: 5.834 | Reg loss: 0.025 | Tree loss: 5.834 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 026 | Total loss: 5.807 | Reg loss: 0.025 | Tree loss: 5.807 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 026 | Total loss: 5.817 | Reg loss: 0.025 | Tree loss: 5.817 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 026 | Total loss: 5.750 | Reg loss: 0.025 | Tree loss: 5.750 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 026 | Total loss: 5.742 | Reg loss: 0.025 | Tree loss: 5.742 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 026 | Total loss: 5.760 | Reg loss: 0.025 | Tree loss: 5.760 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 026 | Total loss: 5.724 | Reg loss: 0.025 | Tree loss: 5.724 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 026 | Total loss: 5.731 | Reg loss: 0.025 | Tree loss: 5.731 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 026 | Total loss: 5.773 | Reg loss: 0.025 | Tree loss: 5.773 | Accuracy: 0.111328 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 026 | Total loss: 5.710 | Reg loss: 0.025 | Tree loss: 5.710 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 026 | Total loss: 5.714 | Reg loss: 0.026 | Tree loss: 5.714 | Accuracy: 0.095703 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 026 | Total loss: 5.699 | Reg loss: 0.026 | Tree loss: 5.699 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 026 | Total loss: 5.745 | Reg loss: 0.026 | Tree loss: 5.745 | Accuracy: 0.122807 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 026 | Total loss: 5.774 | Reg loss: 0.025 | Tree loss: 5.774 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 026 | Total loss: 5.776 | Reg loss: 0.025 | Tree loss: 5.776 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 026 | Total loss: 5.800 | Reg loss: 0.025 | Tree loss: 5.800 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 026 | Total loss: 5.775 | Reg loss: 0.025 | Tree loss: 5.775 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 026 | Total loss: 5.749 | Reg loss: 0.025 | Tree loss: 5.749 | Accuracy: 0.121094 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 005 / 026 | Total loss: 5.753 | Reg loss: 0.025 | Tree loss: 5.753 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 026 | Total loss: 5.716 | Reg loss: 0.025 | Tree loss: 5.716 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 026 | Total loss: 5.686 | Reg loss: 0.025 | Tree loss: 5.686 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 026 | Total loss: 5.703 | Reg loss: 0.025 | Tree loss: 5.703 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 026 | Total loss: 5.681 | Reg loss: 0.025 | Tree loss: 5.681 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 026 | Total loss: 5.677 | Reg loss: 0.025 | Tree loss: 5.677 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 026 | Total loss: 5.637 | Reg loss: 0.025 | Tree loss: 5.637 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 026 | Total loss: 5.653 | Reg loss: 0.025 | Tree loss: 5.653 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 026 | Total loss: 5.640 | Reg loss: 0.025 | Tree loss: 5.640 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 026 | Total loss: 5.637 | Reg loss: 0.025 | Tree loss: 5.637 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 026 | Total loss: 5.601 | Reg loss: 0.025 | Tree loss: 5.601 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 026 | Total loss: 5.641 | Reg loss: 0.025 | Tree loss: 5.641 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 026 | Total loss: 5.560 | Reg loss: 0.025 | Tree loss: 5.560 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 026 | Total loss: 5.591 | Reg loss: 0.025 | Tree loss: 5.591 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 026 | Total loss: 5.557 | Reg loss: 0.025 | Tree loss: 5.557 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 026 | Total loss: 5.592 | Reg loss: 0.025 | Tree loss: 5.592 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 026 | Total loss: 5.539 | Reg loss: 0.025 | Tree loss: 5.539 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 026 | Total loss: 5.510 | Reg loss: 0.025 | Tree loss: 5.510 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 026 | Total loss: 5.521 | Reg loss: 0.026 | Tree loss: 5.521 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 026 | Total loss: 5.491 | Reg loss: 0.026 | Tree loss: 5.491 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 026 | Total loss: 5.487 | Reg loss: 0.026 | Tree loss: 5.487 | Accuracy: 0.070175 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 026 | Total loss: 5.623 | Reg loss: 0.025 | Tree loss: 5.623 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 026 | Total loss: 5.621 | Reg loss: 0.025 | Tree loss: 5.621 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 026 | Total loss: 5.589 | Reg loss: 0.025 | Tree loss: 5.589 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 026 | Total loss: 5.582 | Reg loss: 0.025 | Tree loss: 5.582 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 026 | Total loss: 5.551 | Reg loss: 0.025 | Tree loss: 5.551 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 026 | Total loss: 5.554 | Reg loss: 0.025 | Tree loss: 5.554 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 026 | Total loss: 5.530 | Reg loss: 0.025 | Tree loss: 5.530 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 026 | Total loss: 5.533 | Reg loss: 0.025 | Tree loss: 5.533 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 026 | Total loss: 5.513 | Reg loss: 0.025 | Tree loss: 5.513 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 026 | Total loss: 5.536 | Reg loss: 0.025 | Tree loss: 5.536 | Accuracy: 0.107422 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 026 | Total loss: 5.562 | Reg loss: 0.025 | Tree loss: 5.562 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 026 | Total loss: 5.495 | Reg loss: 0.025 | Tree loss: 5.495 | Accuracy: 0.105469 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 026 | Total loss: 5.479 | Reg loss: 0.025 | Tree loss: 5.479 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 026 | Total loss: 5.476 | Reg loss: 0.025 | Tree loss: 5.476 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 026 | Total loss: 5.449 | Reg loss: 0.025 | Tree loss: 5.449 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 026 | Total loss: 5.392 | Reg loss: 0.025 | Tree loss: 5.392 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 026 | Total loss: 5.421 | Reg loss: 0.025 | Tree loss: 5.421 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 026 | Total loss: 5.451 | Reg loss: 0.025 | Tree loss: 5.451 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 026 | Total loss: 5.402 | Reg loss: 0.025 | Tree loss: 5.402 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 026 | Total loss: 5.435 | Reg loss: 0.025 | Tree loss: 5.435 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 026 | Total loss: 5.342 | Reg loss: 0.025 | Tree loss: 5.342 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 026 | Total loss: 5.352 | Reg loss: 0.025 | Tree loss: 5.352 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 026 | Total loss: 5.340 | Reg loss: 0.025 | Tree loss: 5.340 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 026 | Total loss: 5.325 | Reg loss: 0.025 | Tree loss: 5.325 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 026 | Total loss: 5.324 | Reg loss: 0.026 | Tree loss: 5.324 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 026 | Total loss: 5.344 | Reg loss: 0.026 | Tree loss: 5.344 | Accuracy: 0.175439 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 026 | Total loss: 5.435 | Reg loss: 0.025 | Tree loss: 5.435 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 026 | Total loss: 5.436 | Reg loss: 0.025 | Tree loss: 5.436 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 026 | Total loss: 5.478 | Reg loss: 0.025 | Tree loss: 5.478 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 026 | Total loss: 5.418 | Reg loss: 0.025 | Tree loss: 5.418 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 026 | Total loss: 5.386 | Reg loss: 0.025 | Tree loss: 5.386 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 026 | Total loss: 5.380 | Reg loss: 0.025 | Tree loss: 5.380 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 026 | Total loss: 5.376 | Reg loss: 0.025 | Tree loss: 5.376 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 026 | Total loss: 5.360 | Reg loss: 0.025 | Tree loss: 5.360 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 026 | Total loss: 5.369 | Reg loss: 0.025 | Tree loss: 5.369 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 026 | Total loss: 5.320 | Reg loss: 0.025 | Tree loss: 5.320 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 026 | Total loss: 5.320 | Reg loss: 0.025 | Tree loss: 5.320 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 026 | Total loss: 5.377 | Reg loss: 0.025 | Tree loss: 5.377 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 026 | Total loss: 5.292 | Reg loss: 0.025 | Tree loss: 5.292 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 026 | Total loss: 5.274 | Reg loss: 0.025 | Tree loss: 5.274 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 026 | Total loss: 5.272 | Reg loss: 0.025 | Tree loss: 5.272 | Accuracy: 0.119141 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 015 / 026 | Total loss: 5.263 | Reg loss: 0.025 | Tree loss: 5.263 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 026 | Total loss: 5.276 | Reg loss: 0.025 | Tree loss: 5.276 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 026 | Total loss: 5.225 | Reg loss: 0.025 | Tree loss: 5.225 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 026 | Total loss: 5.245 | Reg loss: 0.025 | Tree loss: 5.245 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 026 | Total loss: 5.255 | Reg loss: 0.025 | Tree loss: 5.255 | Accuracy: 0.107422 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 026 | Total loss: 5.239 | Reg loss: 0.025 | Tree loss: 5.239 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 026 | Total loss: 5.200 | Reg loss: 0.025 | Tree loss: 5.200 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 026 | Total loss: 5.163 | Reg loss: 0.025 | Tree loss: 5.163 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 026 | Total loss: 5.170 | Reg loss: 0.025 | Tree loss: 5.170 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 026 | Total loss: 5.163 | Reg loss: 0.026 | Tree loss: 5.163 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 026 | Total loss: 5.261 | Reg loss: 0.026 | Tree loss: 5.261 | Accuracy: 0.157895 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 026 | Total loss: 5.302 | Reg loss: 0.025 | Tree loss: 5.302 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 026 | Total loss: 5.252 | Reg loss: 0.025 | Tree loss: 5.252 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 026 | Total loss: 5.255 | Reg loss: 0.025 | Tree loss: 5.255 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 026 | Total loss: 5.271 | Reg loss: 0.025 | Tree loss: 5.271 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 026 | Total loss: 5.255 | Reg loss: 0.025 | Tree loss: 5.255 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 026 | Total loss: 5.203 | Reg loss: 0.025 | Tree loss: 5.203 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 026 | Total loss: 5.199 | Reg loss: 0.025 | Tree loss: 5.199 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 026 | Total loss: 5.222 | Reg loss: 0.025 | Tree loss: 5.222 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 026 | Total loss: 5.179 | Reg loss: 0.025 | Tree loss: 5.179 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 026 | Total loss: 5.113 | Reg loss: 0.025 | Tree loss: 5.113 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 026 | Total loss: 5.165 | Reg loss: 0.025 | Tree loss: 5.165 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 026 | Total loss: 5.132 | Reg loss: 0.025 | Tree loss: 5.132 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 026 | Total loss: 5.182 | Reg loss: 0.025 | Tree loss: 5.182 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 026 | Total loss: 5.164 | Reg loss: 0.025 | Tree loss: 5.164 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 026 | Total loss: 5.097 | Reg loss: 0.025 | Tree loss: 5.097 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 026 | Total loss: 5.101 | Reg loss: 0.025 | Tree loss: 5.101 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 026 | Total loss: 5.153 | Reg loss: 0.025 | Tree loss: 5.153 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 026 | Total loss: 5.143 | Reg loss: 0.025 | Tree loss: 5.143 | Accuracy: 0.126953 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 026 | Total loss: 5.048 | Reg loss: 0.025 | Tree loss: 5.048 | Accuracy: 0.128906 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 026 | Total loss: 5.046 | Reg loss: 0.025 | Tree loss: 5.046 | Accuracy: 0.156250 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 026 | Total loss: 5.065 | Reg loss: 0.025 | Tree loss: 5.065 | Accuracy: 0.119141 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 026 | Total loss: 5.088 | Reg loss: 0.025 | Tree loss: 5.088 | Accuracy: 0.105469 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 026 | Total loss: 5.042 | Reg loss: 0.025 | Tree loss: 5.042 | Accuracy: 0.142578 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 026 | Total loss: 5.025 | Reg loss: 0.025 | Tree loss: 5.025 | Accuracy: 0.121094 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 026 | Total loss: 5.032 | Reg loss: 0.025 | Tree loss: 5.032 | Accuracy: 0.128906 | 0.858 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 026 | Total loss: 5.019 | Reg loss: 0.026 | Tree loss: 5.019 | Accuracy: 0.122807 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 026 | Total loss: 5.105 | Reg loss: 0.025 | Tree loss: 5.105 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 026 | Total loss: 5.101 | Reg loss: 0.025 | Tree loss: 5.101 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 026 | Total loss: 5.095 | Reg loss: 0.025 | Tree loss: 5.095 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 026 | Total loss: 5.100 | Reg loss: 0.025 | Tree loss: 5.100 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 026 | Total loss: 5.148 | Reg loss: 0.025 | Tree loss: 5.148 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 026 | Total loss: 5.045 | Reg loss: 0.025 | Tree loss: 5.045 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 026 | Total loss: 5.079 | Reg loss: 0.025 | Tree loss: 5.079 | Accuracy: 0.109375 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 026 | Total loss: 5.043 | Reg loss: 0.025 | Tree loss: 5.043 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 026 | Total loss: 5.031 | Reg loss: 0.025 | Tree loss: 5.031 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 026 | Total loss: 5.013 | Reg loss: 0.025 | Tree loss: 5.013 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 026 | Total loss: 5.004 | Reg loss: 0.025 | Tree loss: 5.004 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 026 | Total loss: 4.980 | Reg loss: 0.025 | Tree loss: 4.980 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 026 | Total loss: 4.977 | Reg loss: 0.025 | Tree loss: 4.977 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 026 | Total loss: 4.975 | Reg loss: 0.025 | Tree loss: 4.975 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 026 | Total loss: 4.958 | Reg loss: 0.025 | Tree loss: 4.958 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 026 | Total loss: 4.973 | Reg loss: 0.025 | Tree loss: 4.973 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 026 | Total loss: 4.967 | Reg loss: 0.025 | Tree loss: 4.967 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 026 | Total loss: 4.933 | Reg loss: 0.025 | Tree loss: 4.933 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 026 | Total loss: 4.996 | Reg loss: 0.025 | Tree loss: 4.996 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 026 | Total loss: 4.917 | Reg loss: 0.025 | Tree loss: 4.917 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 026 | Total loss: 4.907 | Reg loss: 0.025 | Tree loss: 4.907 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 026 | Total loss: 4.902 | Reg loss: 0.025 | Tree loss: 4.902 | Accuracy: 0.113281 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 026 | Total loss: 4.926 | Reg loss: 0.025 | Tree loss: 4.926 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 026 | Total loss: 4.885 | Reg loss: 0.025 | Tree loss: 4.885 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 026 | Total loss: 4.890 | Reg loss: 0.025 | Tree loss: 4.890 | Accuracy: 0.148438 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 025 / 026 | Total loss: 4.840 | Reg loss: 0.025 | Tree loss: 4.840 | Accuracy: 0.122807 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 026 | Total loss: 4.955 | Reg loss: 0.025 | Tree loss: 4.955 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 026 | Total loss: 4.945 | Reg loss: 0.025 | Tree loss: 4.945 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 026 | Total loss: 4.927 | Reg loss: 0.025 | Tree loss: 4.927 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 026 | Total loss: 4.958 | Reg loss: 0.025 | Tree loss: 4.958 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 026 | Total loss: 4.939 | Reg loss: 0.025 | Tree loss: 4.939 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 026 | Total loss: 4.933 | Reg loss: 0.025 | Tree loss: 4.933 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 026 | Total loss: 4.833 | Reg loss: 0.025 | Tree loss: 4.833 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 026 | Total loss: 4.903 | Reg loss: 0.025 | Tree loss: 4.903 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 026 | Total loss: 4.931 | Reg loss: 0.025 | Tree loss: 4.931 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 026 | Total loss: 4.903 | Reg loss: 0.025 | Tree loss: 4.903 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 026 | Total loss: 4.873 | Reg loss: 0.025 | Tree loss: 4.873 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 026 | Total loss: 4.850 | Reg loss: 0.025 | Tree loss: 4.850 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 026 | Total loss: 4.849 | Reg loss: 0.025 | Tree loss: 4.849 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 026 | Total loss: 4.846 | Reg loss: 0.025 | Tree loss: 4.846 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 026 | Total loss: 4.878 | Reg loss: 0.025 | Tree loss: 4.878 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 026 | Total loss: 4.802 | Reg loss: 0.025 | Tree loss: 4.802 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 026 | Total loss: 4.826 | Reg loss: 0.025 | Tree loss: 4.826 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 026 | Total loss: 4.780 | Reg loss: 0.025 | Tree loss: 4.780 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 026 | Total loss: 4.797 | Reg loss: 0.025 | Tree loss: 4.797 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 026 | Total loss: 4.820 | Reg loss: 0.025 | Tree loss: 4.820 | Accuracy: 0.105469 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 026 | Total loss: 4.726 | Reg loss: 0.025 | Tree loss: 4.726 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 026 | Total loss: 4.846 | Reg loss: 0.025 | Tree loss: 4.846 | Accuracy: 0.103516 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 026 | Total loss: 4.763 | Reg loss: 0.025 | Tree loss: 4.763 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 026 | Total loss: 4.754 | Reg loss: 0.025 | Tree loss: 4.754 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 026 | Total loss: 4.750 | Reg loss: 0.025 | Tree loss: 4.750 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 026 | Total loss: 4.635 | Reg loss: 0.025 | Tree loss: 4.635 | Accuracy: 0.105263 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 026 | Total loss: 4.848 | Reg loss: 0.025 | Tree loss: 4.848 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 026 | Total loss: 4.757 | Reg loss: 0.025 | Tree loss: 4.757 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 026 | Total loss: 4.792 | Reg loss: 0.025 | Tree loss: 4.792 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 026 | Total loss: 4.800 | Reg loss: 0.025 | Tree loss: 4.800 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 026 | Total loss: 4.801 | Reg loss: 0.025 | Tree loss: 4.801 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 026 | Total loss: 4.775 | Reg loss: 0.025 | Tree loss: 4.775 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 026 | Total loss: 4.787 | Reg loss: 0.025 | Tree loss: 4.787 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 026 | Total loss: 4.727 | Reg loss: 0.025 | Tree loss: 4.727 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 026 | Total loss: 4.741 | Reg loss: 0.025 | Tree loss: 4.741 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 026 | Total loss: 4.705 | Reg loss: 0.025 | Tree loss: 4.705 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 026 | Total loss: 4.771 | Reg loss: 0.025 | Tree loss: 4.771 | Accuracy: 0.109375 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 026 | Total loss: 4.748 | Reg loss: 0.025 | Tree loss: 4.748 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 026 | Total loss: 4.687 | Reg loss: 0.025 | Tree loss: 4.687 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 026 | Total loss: 4.684 | Reg loss: 0.025 | Tree loss: 4.684 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 026 | Total loss: 4.748 | Reg loss: 0.025 | Tree loss: 4.748 | Accuracy: 0.109375 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 026 | Total loss: 4.705 | Reg loss: 0.025 | Tree loss: 4.705 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 026 | Total loss: 4.730 | Reg loss: 0.025 | Tree loss: 4.730 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 026 | Total loss: 4.696 | Reg loss: 0.025 | Tree loss: 4.696 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 026 | Total loss: 4.710 | Reg loss: 0.025 | Tree loss: 4.710 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 026 | Total loss: 4.655 | Reg loss: 0.025 | Tree loss: 4.655 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 026 | Total loss: 4.595 | Reg loss: 0.025 | Tree loss: 4.595 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 026 | Total loss: 4.658 | Reg loss: 0.025 | Tree loss: 4.658 | Accuracy: 0.113281 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 026 | Total loss: 4.648 | Reg loss: 0.025 | Tree loss: 4.648 | Accuracy: 0.111328 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 026 | Total loss: 4.627 | Reg loss: 0.025 | Tree loss: 4.627 | Accuracy: 0.107422 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 026 | Total loss: 4.594 | Reg loss: 0.025 | Tree loss: 4.594 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 026 | Total loss: 4.475 | Reg loss: 0.025 | Tree loss: 4.475 | Accuracy: 0.140351 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 026 | Total loss: 4.692 | Reg loss: 0.025 | Tree loss: 4.692 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 026 | Total loss: 4.653 | Reg loss: 0.025 | Tree loss: 4.653 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 026 | Total loss: 4.658 | Reg loss: 0.025 | Tree loss: 4.658 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 026 | Total loss: 4.662 | Reg loss: 0.025 | Tree loss: 4.662 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 026 | Total loss: 4.671 | Reg loss: 0.025 | Tree loss: 4.671 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 026 | Total loss: 4.652 | Reg loss: 0.025 | Tree loss: 4.652 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 026 | Total loss: 4.585 | Reg loss: 0.025 | Tree loss: 4.585 | Accuracy: 0.187500 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 007 / 026 | Total loss: 4.683 | Reg loss: 0.025 | Tree loss: 4.683 | Accuracy: 0.107422 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 026 | Total loss: 4.611 | Reg loss: 0.025 | Tree loss: 4.611 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 026 | Total loss: 4.604 | Reg loss: 0.025 | Tree loss: 4.604 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 026 | Total loss: 4.621 | Reg loss: 0.025 | Tree loss: 4.621 | Accuracy: 0.113281 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 026 | Total loss: 4.596 | Reg loss: 0.025 | Tree loss: 4.596 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 026 | Total loss: 4.557 | Reg loss: 0.025 | Tree loss: 4.557 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 026 | Total loss: 4.564 | Reg loss: 0.025 | Tree loss: 4.564 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 026 | Total loss: 4.585 | Reg loss: 0.025 | Tree loss: 4.585 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 026 | Total loss: 4.577 | Reg loss: 0.025 | Tree loss: 4.577 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 026 | Total loss: 4.505 | Reg loss: 0.025 | Tree loss: 4.505 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 026 | Total loss: 4.595 | Reg loss: 0.025 | Tree loss: 4.595 | Accuracy: 0.109375 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 026 | Total loss: 4.556 | Reg loss: 0.025 | Tree loss: 4.556 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 026 | Total loss: 4.528 | Reg loss: 0.025 | Tree loss: 4.528 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 026 | Total loss: 4.538 | Reg loss: 0.025 | Tree loss: 4.538 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 026 | Total loss: 4.555 | Reg loss: 0.025 | Tree loss: 4.555 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 026 | Total loss: 4.471 | Reg loss: 0.025 | Tree loss: 4.471 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 026 | Total loss: 4.518 | Reg loss: 0.025 | Tree loss: 4.518 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 026 | Total loss: 4.498 | Reg loss: 0.025 | Tree loss: 4.498 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 026 | Total loss: 4.574 | Reg loss: 0.025 | Tree loss: 4.574 | Accuracy: 0.157895 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 026 | Total loss: 4.599 | Reg loss: 0.025 | Tree loss: 4.599 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 026 | Total loss: 4.545 | Reg loss: 0.025 | Tree loss: 4.545 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 026 | Total loss: 4.593 | Reg loss: 0.025 | Tree loss: 4.593 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 026 | Total loss: 4.567 | Reg loss: 0.025 | Tree loss: 4.567 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 026 | Total loss: 4.529 | Reg loss: 0.025 | Tree loss: 4.529 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 026 | Total loss: 4.506 | Reg loss: 0.025 | Tree loss: 4.506 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 026 | Total loss: 4.500 | Reg loss: 0.025 | Tree loss: 4.500 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 026 | Total loss: 4.490 | Reg loss: 0.025 | Tree loss: 4.490 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 026 | Total loss: 4.497 | Reg loss: 0.025 | Tree loss: 4.497 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 026 | Total loss: 4.505 | Reg loss: 0.025 | Tree loss: 4.505 | Accuracy: 0.115234 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 026 | Total loss: 4.472 | Reg loss: 0.025 | Tree loss: 4.472 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 026 | Total loss: 4.433 | Reg loss: 0.025 | Tree loss: 4.433 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 026 | Total loss: 4.477 | Reg loss: 0.025 | Tree loss: 4.477 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 026 | Total loss: 4.460 | Reg loss: 0.025 | Tree loss: 4.460 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 026 | Total loss: 4.443 | Reg loss: 0.025 | Tree loss: 4.443 | Accuracy: 0.101562 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 026 | Total loss: 4.425 | Reg loss: 0.025 | Tree loss: 4.425 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 026 | Total loss: 4.457 | Reg loss: 0.025 | Tree loss: 4.457 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 026 | Total loss: 4.416 | Reg loss: 0.025 | Tree loss: 4.416 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 026 | Total loss: 4.423 | Reg loss: 0.025 | Tree loss: 4.423 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 026 | Total loss: 4.368 | Reg loss: 0.025 | Tree loss: 4.368 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 026 | Total loss: 4.412 | Reg loss: 0.025 | Tree loss: 4.412 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 026 | Total loss: 4.417 | Reg loss: 0.025 | Tree loss: 4.417 | Accuracy: 0.107422 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 026 | Total loss: 4.367 | Reg loss: 0.025 | Tree loss: 4.367 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 026 | Total loss: 4.426 | Reg loss: 0.025 | Tree loss: 4.426 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 026 | Total loss: 4.359 | Reg loss: 0.025 | Tree loss: 4.359 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 026 | Total loss: 4.362 | Reg loss: 0.025 | Tree loss: 4.362 | Accuracy: 0.175439 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 026 | Total loss: 4.405 | Reg loss: 0.025 | Tree loss: 4.405 | Accuracy: 0.154297 | 0.862 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 026 | Total loss: 4.407 | Reg loss: 0.025 | Tree loss: 4.407 | Accuracy: 0.136719 | 0.862 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 026 | Total loss: 4.399 | Reg loss: 0.025 | Tree loss: 4.399 | Accuracy: 0.169922 | 0.862 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 026 | Total loss: 4.436 | Reg loss: 0.025 | Tree loss: 4.436 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 026 | Total loss: 4.412 | Reg loss: 0.025 | Tree loss: 4.412 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 026 | Total loss: 4.372 | Reg loss: 0.025 | Tree loss: 4.372 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 026 | Total loss: 4.417 | Reg loss: 0.025 | Tree loss: 4.417 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 026 | Total loss: 4.341 | Reg loss: 0.025 | Tree loss: 4.341 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 026 | Total loss: 4.376 | Reg loss: 0.025 | Tree loss: 4.376 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 026 | Total loss: 4.372 | Reg loss: 0.025 | Tree loss: 4.372 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 026 | Total loss: 4.391 | Reg loss: 0.025 | Tree loss: 4.391 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 026 | Total loss: 4.313 | Reg loss: 0.025 | Tree loss: 4.313 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 026 | Total loss: 4.337 | Reg loss: 0.025 | Tree loss: 4.337 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 026 | Total loss: 4.337 | Reg loss: 0.025 | Tree loss: 4.337 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 026 | Total loss: 4.311 | Reg loss: 0.025 | Tree loss: 4.311 | Accuracy: 0.146484 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 026 | Total loss: 4.313 | Reg loss: 0.025 | Tree loss: 4.313 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 026 | Total loss: 4.396 | Reg loss: 0.025 | Tree loss: 4.396 | Accuracy: 0.111328 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 017 / 026 | Total loss: 4.339 | Reg loss: 0.025 | Tree loss: 4.339 | Accuracy: 0.109375 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 026 | Total loss: 4.312 | Reg loss: 0.025 | Tree loss: 4.312 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 026 | Total loss: 4.326 | Reg loss: 0.025 | Tree loss: 4.326 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 026 | Total loss: 4.311 | Reg loss: 0.025 | Tree loss: 4.311 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 026 | Total loss: 4.283 | Reg loss: 0.025 | Tree loss: 4.283 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 026 | Total loss: 4.277 | Reg loss: 0.025 | Tree loss: 4.277 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 026 | Total loss: 4.298 | Reg loss: 0.025 | Tree loss: 4.298 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 026 | Total loss: 4.262 | Reg loss: 0.025 | Tree loss: 4.262 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 026 | Total loss: 4.288 | Reg loss: 0.025 | Tree loss: 4.288 | Accuracy: 0.140351 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 026 | Total loss: 4.311 | Reg loss: 0.025 | Tree loss: 4.311 | Accuracy: 0.128906 | 0.862 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 026 | Total loss: 4.300 | Reg loss: 0.025 | Tree loss: 4.300 | Accuracy: 0.164062 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 026 | Total loss: 4.314 | Reg loss: 0.025 | Tree loss: 4.314 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 026 | Total loss: 4.295 | Reg loss: 0.025 | Tree loss: 4.295 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 026 | Total loss: 4.289 | Reg loss: 0.025 | Tree loss: 4.289 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 026 | Total loss: 4.274 | Reg loss: 0.025 | Tree loss: 4.274 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 026 | Total loss: 4.300 | Reg loss: 0.025 | Tree loss: 4.300 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 026 | Total loss: 4.253 | Reg loss: 0.025 | Tree loss: 4.253 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 026 | Total loss: 4.249 | Reg loss: 0.025 | Tree loss: 4.249 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 026 | Total loss: 4.236 | Reg loss: 0.025 | Tree loss: 4.236 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 026 | Total loss: 4.214 | Reg loss: 0.025 | Tree loss: 4.214 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 026 | Total loss: 4.239 | Reg loss: 0.025 | Tree loss: 4.239 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 026 | Total loss: 4.266 | Reg loss: 0.025 | Tree loss: 4.266 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 026 | Total loss: 4.211 | Reg loss: 0.025 | Tree loss: 4.211 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 026 | Total loss: 4.268 | Reg loss: 0.025 | Tree loss: 4.268 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 026 | Total loss: 4.206 | Reg loss: 0.025 | Tree loss: 4.206 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 026 | Total loss: 4.175 | Reg loss: 0.025 | Tree loss: 4.175 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 026 | Total loss: 4.229 | Reg loss: 0.025 | Tree loss: 4.229 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 026 | Total loss: 4.152 | Reg loss: 0.025 | Tree loss: 4.152 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 026 | Total loss: 4.225 | Reg loss: 0.025 | Tree loss: 4.225 | Accuracy: 0.113281 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 026 | Total loss: 4.196 | Reg loss: 0.025 | Tree loss: 4.196 | Accuracy: 0.091797 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 026 | Total loss: 4.176 | Reg loss: 0.025 | Tree loss: 4.176 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 026 | Total loss: 4.149 | Reg loss: 0.025 | Tree loss: 4.149 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 026 | Total loss: 4.154 | Reg loss: 0.025 | Tree loss: 4.154 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 026 | Total loss: 4.159 | Reg loss: 0.025 | Tree loss: 4.159 | Accuracy: 0.107422 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 026 | Total loss: 4.126 | Reg loss: 0.025 | Tree loss: 4.126 | Accuracy: 0.140351 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 026 | Total loss: 4.251 | Reg loss: 0.025 | Tree loss: 4.251 | Accuracy: 0.136719 | 0.862 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 026 | Total loss: 4.208 | Reg loss: 0.025 | Tree loss: 4.208 | Accuracy: 0.128906 | 0.862 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 026 | Total loss: 4.221 | Reg loss: 0.025 | Tree loss: 4.221 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 026 | Total loss: 4.207 | Reg loss: 0.025 | Tree loss: 4.207 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 026 | Total loss: 4.197 | Reg loss: 0.025 | Tree loss: 4.197 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 026 | Total loss: 4.204 | Reg loss: 0.025 | Tree loss: 4.204 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 026 | Total loss: 4.147 | Reg loss: 0.025 | Tree loss: 4.147 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 026 | Total loss: 4.170 | Reg loss: 0.025 | Tree loss: 4.170 | Accuracy: 0.164062 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 026 | Total loss: 4.156 | Reg loss: 0.025 | Tree loss: 4.156 | Accuracy: 0.113281 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 026 | Total loss: 4.135 | Reg loss: 0.025 | Tree loss: 4.135 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 026 | Total loss: 4.155 | Reg loss: 0.025 | Tree loss: 4.155 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 026 | Total loss: 4.107 | Reg loss: 0.025 | Tree loss: 4.107 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 026 | Total loss: 4.113 | Reg loss: 0.025 | Tree loss: 4.113 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 026 | Total loss: 4.108 | Reg loss: 0.025 | Tree loss: 4.108 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 026 | Total loss: 4.123 | Reg loss: 0.025 | Tree loss: 4.123 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 026 | Total loss: 4.100 | Reg loss: 0.025 | Tree loss: 4.100 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 026 | Total loss: 4.112 | Reg loss: 0.025 | Tree loss: 4.112 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 026 | Total loss: 4.051 | Reg loss: 0.025 | Tree loss: 4.051 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 026 | Total loss: 4.052 | Reg loss: 0.025 | Tree loss: 4.052 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 026 | Total loss: 4.052 | Reg loss: 0.025 | Tree loss: 4.052 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 026 | Total loss: 4.051 | Reg loss: 0.025 | Tree loss: 4.051 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 026 | Total loss: 4.073 | Reg loss: 0.025 | Tree loss: 4.073 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 026 | Total loss: 4.069 | Reg loss: 0.025 | Tree loss: 4.069 | Accuracy: 0.146484 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 026 | Total loss: 3.957 | Reg loss: 0.026 | Tree loss: 3.957 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 026 | Total loss: 4.041 | Reg loss: 0.026 | Tree loss: 4.041 | Accuracy: 0.107422 | 0.861 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 026 | Total loss: 3.987 | Reg loss: 0.026 | Tree loss: 3.987 | Accuracy: 0.122807 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 026 | Total loss: 4.063 | Reg loss: 0.025 | Tree loss: 4.063 | Accuracy: 0.169922 | 0.862 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 026 | Total loss: 4.126 | Reg loss: 0.025 | Tree loss: 4.126 | Accuracy: 0.125000 | 0.862 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 026 | Total loss: 4.068 | Reg loss: 0.025 | Tree loss: 4.068 | Accuracy: 0.144531 | 0.862 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 026 | Total loss: 4.131 | Reg loss: 0.025 | Tree loss: 4.131 | Accuracy: 0.142578 | 0.862 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 026 | Total loss: 4.073 | Reg loss: 0.025 | Tree loss: 4.073 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 026 | Total loss: 4.094 | Reg loss: 0.025 | Tree loss: 4.094 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 026 | Total loss: 4.095 | Reg loss: 0.025 | Tree loss: 4.095 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 026 | Total loss: 4.093 | Reg loss: 0.025 | Tree loss: 4.093 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 026 | Total loss: 4.024 | Reg loss: 0.025 | Tree loss: 4.024 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 026 | Total loss: 4.037 | Reg loss: 0.025 | Tree loss: 4.037 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 026 | Total loss: 4.004 | Reg loss: 0.025 | Tree loss: 4.004 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 026 | Total loss: 3.999 | Reg loss: 0.025 | Tree loss: 3.999 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 026 | Total loss: 4.051 | Reg loss: 0.025 | Tree loss: 4.051 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 026 | Total loss: 3.988 | Reg loss: 0.025 | Tree loss: 3.988 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 026 | Total loss: 4.027 | Reg loss: 0.025 | Tree loss: 4.027 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 026 | Total loss: 3.998 | Reg loss: 0.025 | Tree loss: 3.998 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 026 | Total loss: 3.937 | Reg loss: 0.025 | Tree loss: 3.937 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 026 | Total loss: 3.990 | Reg loss: 0.025 | Tree loss: 3.990 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 026 | Total loss: 4.000 | Reg loss: 0.025 | Tree loss: 4.000 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 026 | Total loss: 3.952 | Reg loss: 0.026 | Tree loss: 3.952 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 026 | Total loss: 3.926 | Reg loss: 0.026 | Tree loss: 3.926 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 026 | Total loss: 3.942 | Reg loss: 0.026 | Tree loss: 3.942 | Accuracy: 0.109375 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 026 | Total loss: 3.912 | Reg loss: 0.026 | Tree loss: 3.912 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 026 | Total loss: 3.982 | Reg loss: 0.026 | Tree loss: 3.982 | Accuracy: 0.111328 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 026 | Total loss: 3.898 | Reg loss: 0.026 | Tree loss: 3.898 | Accuracy: 0.164062 | 0.861 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 026 | Total loss: 3.970 | Reg loss: 0.026 | Tree loss: 3.970 | Accuracy: 0.210526 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 026 | Total loss: 4.017 | Reg loss: 0.025 | Tree loss: 4.017 | Accuracy: 0.152344 | 0.862 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 026 | Total loss: 4.016 | Reg loss: 0.025 | Tree loss: 4.016 | Accuracy: 0.150391 | 0.862 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 026 | Total loss: 4.010 | Reg loss: 0.025 | Tree loss: 4.010 | Accuracy: 0.146484 | 0.862 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 026 | Total loss: 3.994 | Reg loss: 0.025 | Tree loss: 3.994 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 026 | Total loss: 3.981 | Reg loss: 0.025 | Tree loss: 3.981 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 026 | Total loss: 3.957 | Reg loss: 0.025 | Tree loss: 3.957 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 026 | Total loss: 3.952 | Reg loss: 0.025 | Tree loss: 3.952 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 026 | Total loss: 3.963 | Reg loss: 0.025 | Tree loss: 3.963 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 026 | Total loss: 3.969 | Reg loss: 0.025 | Tree loss: 3.969 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 026 | Total loss: 3.899 | Reg loss: 0.025 | Tree loss: 3.899 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 026 | Total loss: 3.943 | Reg loss: 0.025 | Tree loss: 3.943 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 026 | Total loss: 3.914 | Reg loss: 0.025 | Tree loss: 3.914 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 026 | Total loss: 3.943 | Reg loss: 0.025 | Tree loss: 3.943 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 026 | Total loss: 3.916 | Reg loss: 0.025 | Tree loss: 3.916 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 026 | Total loss: 3.887 | Reg loss: 0.026 | Tree loss: 3.887 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 026 | Total loss: 3.877 | Reg loss: 0.026 | Tree loss: 3.877 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 026 | Total loss: 3.947 | Reg loss: 0.026 | Tree loss: 3.947 | Accuracy: 0.115234 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 026 | Total loss: 3.896 | Reg loss: 0.026 | Tree loss: 3.896 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 026 | Total loss: 3.851 | Reg loss: 0.026 | Tree loss: 3.851 | Accuracy: 0.146484 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 026 | Total loss: 3.826 | Reg loss: 0.026 | Tree loss: 3.826 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 026 | Total loss: 3.868 | Reg loss: 0.026 | Tree loss: 3.868 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 026 | Total loss: 3.879 | Reg loss: 0.026 | Tree loss: 3.879 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 026 | Total loss: 3.790 | Reg loss: 0.026 | Tree loss: 3.790 | Accuracy: 0.175781 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 026 | Total loss: 3.831 | Reg loss: 0.026 | Tree loss: 3.831 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 026 | Total loss: 3.825 | Reg loss: 0.026 | Tree loss: 3.825 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 026 | Total loss: 3.664 | Reg loss: 0.026 | Tree loss: 3.664 | Accuracy: 0.228070 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 026 | Total loss: 3.914 | Reg loss: 0.025 | Tree loss: 3.914 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 026 | Total loss: 3.950 | Reg loss: 0.025 | Tree loss: 3.950 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 026 | Total loss: 3.918 | Reg loss: 0.025 | Tree loss: 3.918 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 026 | Total loss: 3.890 | Reg loss: 0.025 | Tree loss: 3.890 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 026 | Total loss: 3.838 | Reg loss: 0.025 | Tree loss: 3.838 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 026 | Total loss: 3.896 | Reg loss: 0.025 | Tree loss: 3.896 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 026 | Total loss: 3.882 | Reg loss: 0.025 | Tree loss: 3.882 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 026 | Total loss: 3.963 | Reg loss: 0.025 | Tree loss: 3.963 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 026 | Total loss: 3.854 | Reg loss: 0.025 | Tree loss: 3.854 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 026 | Total loss: 3.838 | Reg loss: 0.026 | Tree loss: 3.838 | Accuracy: 0.164062 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 010 / 026 | Total loss: 3.834 | Reg loss: 0.026 | Tree loss: 3.834 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 026 | Total loss: 3.842 | Reg loss: 0.026 | Tree loss: 3.842 | Accuracy: 0.123047 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 026 | Total loss: 3.810 | Reg loss: 0.026 | Tree loss: 3.810 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 026 | Total loss: 3.803 | Reg loss: 0.026 | Tree loss: 3.803 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 026 | Total loss: 3.796 | Reg loss: 0.026 | Tree loss: 3.796 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 026 | Total loss: 3.794 | Reg loss: 0.026 | Tree loss: 3.794 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 026 | Total loss: 3.776 | Reg loss: 0.026 | Tree loss: 3.776 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 026 | Total loss: 3.839 | Reg loss: 0.026 | Tree loss: 3.839 | Accuracy: 0.125000 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 026 | Total loss: 3.741 | Reg loss: 0.026 | Tree loss: 3.741 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 026 | Total loss: 3.745 | Reg loss: 0.026 | Tree loss: 3.745 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 026 | Total loss: 3.750 | Reg loss: 0.026 | Tree loss: 3.750 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 026 | Total loss: 3.731 | Reg loss: 0.026 | Tree loss: 3.731 | Accuracy: 0.167969 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 026 | Total loss: 3.750 | Reg loss: 0.027 | Tree loss: 3.750 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 026 | Total loss: 3.732 | Reg loss: 0.027 | Tree loss: 3.732 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 026 | Total loss: 3.689 | Reg loss: 0.027 | Tree loss: 3.689 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 026 | Total loss: 3.875 | Reg loss: 0.027 | Tree loss: 3.875 | Accuracy: 0.105263 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 026 | Total loss: 3.866 | Reg loss: 0.026 | Tree loss: 3.866 | Accuracy: 0.167969 | 0.862 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 026 | Total loss: 3.883 | Reg loss: 0.026 | Tree loss: 3.883 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 026 | Total loss: 3.878 | Reg loss: 0.026 | Tree loss: 3.878 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 026 | Total loss: 3.825 | Reg loss: 0.026 | Tree loss: 3.825 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 026 | Total loss: 3.821 | Reg loss: 0.026 | Tree loss: 3.821 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 026 | Total loss: 3.845 | Reg loss: 0.026 | Tree loss: 3.845 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 026 | Total loss: 3.787 | Reg loss: 0.026 | Tree loss: 3.787 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 026 | Total loss: 3.799 | Reg loss: 0.026 | Tree loss: 3.799 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 026 | Total loss: 3.816 | Reg loss: 0.026 | Tree loss: 3.816 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 026 | Total loss: 3.748 | Reg loss: 0.026 | Tree loss: 3.748 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 026 | Total loss: 3.708 | Reg loss: 0.026 | Tree loss: 3.708 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 026 | Total loss: 3.752 | Reg loss: 0.026 | Tree loss: 3.752 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 026 | Total loss: 3.714 | Reg loss: 0.026 | Tree loss: 3.714 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 026 | Total loss: 3.702 | Reg loss: 0.026 | Tree loss: 3.702 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 026 | Total loss: 3.737 | Reg loss: 0.026 | Tree loss: 3.737 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 026 | Total loss: 3.686 | Reg loss: 0.026 | Tree loss: 3.686 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 026 | Total loss: 3.716 | Reg loss: 0.026 | Tree loss: 3.716 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 026 | Total loss: 3.676 | Reg loss: 0.026 | Tree loss: 3.676 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 026 | Total loss: 3.669 | Reg loss: 0.027 | Tree loss: 3.669 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 026 | Total loss: 3.666 | Reg loss: 0.027 | Tree loss: 3.666 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 026 | Total loss: 3.647 | Reg loss: 0.027 | Tree loss: 3.647 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 026 | Total loss: 3.653 | Reg loss: 0.027 | Tree loss: 3.653 | Accuracy: 0.146484 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 026 | Total loss: 3.625 | Reg loss: 0.027 | Tree loss: 3.625 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 026 | Total loss: 3.678 | Reg loss: 0.027 | Tree loss: 3.678 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 026 | Total loss: 3.623 | Reg loss: 0.027 | Tree loss: 3.623 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 026 | Total loss: 3.588 | Reg loss: 0.027 | Tree loss: 3.588 | Accuracy: 0.210526 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 026 | Total loss: 3.793 | Reg loss: 0.026 | Tree loss: 3.793 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 026 | Total loss: 3.781 | Reg loss: 0.026 | Tree loss: 3.781 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 026 | Total loss: 3.766 | Reg loss: 0.026 | Tree loss: 3.766 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 026 | Total loss: 3.750 | Reg loss: 0.026 | Tree loss: 3.750 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 026 | Total loss: 3.738 | Reg loss: 0.026 | Tree loss: 3.738 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 026 | Total loss: 3.711 | Reg loss: 0.026 | Tree loss: 3.711 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 026 | Total loss: 3.705 | Reg loss: 0.026 | Tree loss: 3.705 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 026 | Total loss: 3.745 | Reg loss: 0.026 | Tree loss: 3.745 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 026 | Total loss: 3.641 | Reg loss: 0.026 | Tree loss: 3.641 | Accuracy: 0.175781 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 026 | Total loss: 3.688 | Reg loss: 0.026 | Tree loss: 3.688 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 026 | Total loss: 3.685 | Reg loss: 0.026 | Tree loss: 3.685 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 026 | Total loss: 3.768 | Reg loss: 0.026 | Tree loss: 3.768 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 026 | Total loss: 3.666 | Reg loss: 0.026 | Tree loss: 3.666 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 026 | Total loss: 3.659 | Reg loss: 0.026 | Tree loss: 3.659 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 026 | Total loss: 3.649 | Reg loss: 0.027 | Tree loss: 3.649 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 026 | Total loss: 3.614 | Reg loss: 0.027 | Tree loss: 3.614 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 026 | Total loss: 3.650 | Reg loss: 0.027 | Tree loss: 3.650 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 026 | Total loss: 3.604 | Reg loss: 0.027 | Tree loss: 3.604 | Accuracy: 0.113281 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 026 | Total loss: 3.602 | Reg loss: 0.027 | Tree loss: 3.602 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 026 | Total loss: 3.595 | Reg loss: 0.027 | Tree loss: 3.595 | Accuracy: 0.152344 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 020 / 026 | Total loss: 3.594 | Reg loss: 0.027 | Tree loss: 3.594 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 026 | Total loss: 3.602 | Reg loss: 0.027 | Tree loss: 3.602 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 026 | Total loss: 3.575 | Reg loss: 0.027 | Tree loss: 3.575 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 026 | Total loss: 3.549 | Reg loss: 0.027 | Tree loss: 3.549 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 026 | Total loss: 3.508 | Reg loss: 0.027 | Tree loss: 3.508 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 026 | Total loss: 3.585 | Reg loss: 0.027 | Tree loss: 3.585 | Accuracy: 0.228070 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 026 | Total loss: 3.696 | Reg loss: 0.026 | Tree loss: 3.696 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 026 | Total loss: 3.679 | Reg loss: 0.026 | Tree loss: 3.679 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 026 | Total loss: 3.703 | Reg loss: 0.026 | Tree loss: 3.703 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 026 | Total loss: 3.640 | Reg loss: 0.026 | Tree loss: 3.640 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 026 | Total loss: 3.671 | Reg loss: 0.026 | Tree loss: 3.671 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 026 | Total loss: 3.641 | Reg loss: 0.026 | Tree loss: 3.641 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 026 | Total loss: 3.645 | Reg loss: 0.026 | Tree loss: 3.645 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 026 | Total loss: 3.665 | Reg loss: 0.026 | Tree loss: 3.665 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 026 | Total loss: 3.620 | Reg loss: 0.026 | Tree loss: 3.620 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 026 | Total loss: 3.689 | Reg loss: 0.027 | Tree loss: 3.689 | Accuracy: 0.101562 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 026 | Total loss: 3.587 | Reg loss: 0.027 | Tree loss: 3.587 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 026 | Total loss: 3.628 | Reg loss: 0.027 | Tree loss: 3.628 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 026 | Total loss: 3.610 | Reg loss: 0.027 | Tree loss: 3.610 | Accuracy: 0.105469 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 026 | Total loss: 3.619 | Reg loss: 0.027 | Tree loss: 3.619 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 026 | Total loss: 3.570 | Reg loss: 0.027 | Tree loss: 3.570 | Accuracy: 0.123047 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 026 | Total loss: 3.622 | Reg loss: 0.027 | Tree loss: 3.622 | Accuracy: 0.167969 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 026 | Total loss: 3.500 | Reg loss: 0.027 | Tree loss: 3.500 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 026 | Total loss: 3.606 | Reg loss: 0.027 | Tree loss: 3.606 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 026 | Total loss: 3.556 | Reg loss: 0.027 | Tree loss: 3.556 | Accuracy: 0.113281 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 026 | Total loss: 3.483 | Reg loss: 0.027 | Tree loss: 3.483 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 026 | Total loss: 3.571 | Reg loss: 0.027 | Tree loss: 3.571 | Accuracy: 0.121094 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 026 | Total loss: 3.547 | Reg loss: 0.027 | Tree loss: 3.547 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 026 | Total loss: 3.518 | Reg loss: 0.027 | Tree loss: 3.518 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 026 | Total loss: 3.493 | Reg loss: 0.027 | Tree loss: 3.493 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 026 | Total loss: 3.465 | Reg loss: 0.027 | Tree loss: 3.465 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 026 | Total loss: 3.395 | Reg loss: 0.028 | Tree loss: 3.395 | Accuracy: 0.245614 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 026 | Total loss: 3.604 | Reg loss: 0.027 | Tree loss: 3.604 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 026 | Total loss: 3.696 | Reg loss: 0.027 | Tree loss: 3.696 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 026 | Total loss: 3.683 | Reg loss: 0.027 | Tree loss: 3.683 | Accuracy: 0.103516 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 026 | Total loss: 3.603 | Reg loss: 0.027 | Tree loss: 3.603 | Accuracy: 0.167969 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 026 | Total loss: 3.645 | Reg loss: 0.027 | Tree loss: 3.645 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 026 | Total loss: 3.567 | Reg loss: 0.027 | Tree loss: 3.567 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 026 | Total loss: 3.615 | Reg loss: 0.027 | Tree loss: 3.615 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 026 | Total loss: 3.603 | Reg loss: 0.027 | Tree loss: 3.603 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 026 | Total loss: 3.600 | Reg loss: 0.027 | Tree loss: 3.600 | Accuracy: 0.117188 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 026 | Total loss: 3.614 | Reg loss: 0.027 | Tree loss: 3.614 | Accuracy: 0.119141 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 026 | Total loss: 3.542 | Reg loss: 0.027 | Tree loss: 3.542 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 026 | Total loss: 3.528 | Reg loss: 0.027 | Tree loss: 3.528 | Accuracy: 0.150391 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 026 | Total loss: 3.541 | Reg loss: 0.027 | Tree loss: 3.541 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 026 | Total loss: 3.549 | Reg loss: 0.027 | Tree loss: 3.549 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 026 | Total loss: 3.537 | Reg loss: 0.027 | Tree loss: 3.537 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 026 | Total loss: 3.525 | Reg loss: 0.027 | Tree loss: 3.525 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 026 | Total loss: 3.502 | Reg loss: 0.027 | Tree loss: 3.502 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 026 | Total loss: 3.454 | Reg loss: 0.027 | Tree loss: 3.454 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 026 | Total loss: 3.497 | Reg loss: 0.027 | Tree loss: 3.497 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 026 | Total loss: 3.466 | Reg loss: 0.027 | Tree loss: 3.466 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 026 | Total loss: 3.462 | Reg loss: 0.027 | Tree loss: 3.462 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 026 | Total loss: 3.449 | Reg loss: 0.027 | Tree loss: 3.449 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 026 | Total loss: 3.434 | Reg loss: 0.028 | Tree loss: 3.434 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 026 | Total loss: 3.433 | Reg loss: 0.028 | Tree loss: 3.433 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 026 | Total loss: 3.396 | Reg loss: 0.028 | Tree loss: 3.396 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 026 | Total loss: 3.533 | Reg loss: 0.028 | Tree loss: 3.533 | Accuracy: 0.157895 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 026 | Total loss: 3.577 | Reg loss: 0.027 | Tree loss: 3.577 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 026 | Total loss: 3.583 | Reg loss: 0.027 | Tree loss: 3.583 | Accuracy: 0.136719 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 002 / 026 | Total loss: 3.588 | Reg loss: 0.027 | Tree loss: 3.588 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 026 | Total loss: 3.580 | Reg loss: 0.027 | Tree loss: 3.580 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 026 | Total loss: 3.553 | Reg loss: 0.027 | Tree loss: 3.553 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 026 | Total loss: 3.539 | Reg loss: 0.027 | Tree loss: 3.539 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 026 | Total loss: 3.554 | Reg loss: 0.027 | Tree loss: 3.554 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 026 | Total loss: 3.547 | Reg loss: 0.027 | Tree loss: 3.547 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 026 | Total loss: 3.523 | Reg loss: 0.027 | Tree loss: 3.523 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 026 | Total loss: 3.444 | Reg loss: 0.027 | Tree loss: 3.444 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 026 | Total loss: 3.553 | Reg loss: 0.027 | Tree loss: 3.553 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 026 | Total loss: 3.527 | Reg loss: 0.027 | Tree loss: 3.527 | Accuracy: 0.107422 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 026 | Total loss: 3.481 | Reg loss: 0.027 | Tree loss: 3.481 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 026 | Total loss: 3.508 | Reg loss: 0.027 | Tree loss: 3.508 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 026 | Total loss: 3.454 | Reg loss: 0.027 | Tree loss: 3.454 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 026 | Total loss: 3.412 | Reg loss: 0.027 | Tree loss: 3.412 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 026 | Total loss: 3.481 | Reg loss: 0.027 | Tree loss: 3.481 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 026 | Total loss: 3.408 | Reg loss: 0.027 | Tree loss: 3.408 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 026 | Total loss: 3.457 | Reg loss: 0.027 | Tree loss: 3.457 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 026 | Total loss: 3.440 | Reg loss: 0.028 | Tree loss: 3.440 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 026 | Total loss: 3.436 | Reg loss: 0.028 | Tree loss: 3.436 | Accuracy: 0.113281 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 026 | Total loss: 3.420 | Reg loss: 0.028 | Tree loss: 3.420 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 026 | Total loss: 3.411 | Reg loss: 0.028 | Tree loss: 3.411 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 026 | Total loss: 3.392 | Reg loss: 0.028 | Tree loss: 3.392 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 026 | Total loss: 3.396 | Reg loss: 0.028 | Tree loss: 3.396 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 026 | Total loss: 3.288 | Reg loss: 0.028 | Tree loss: 3.288 | Accuracy: 0.122807 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 026 | Total loss: 3.549 | Reg loss: 0.027 | Tree loss: 3.549 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 026 | Total loss: 3.567 | Reg loss: 0.027 | Tree loss: 3.567 | Accuracy: 0.156250 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 026 | Total loss: 3.524 | Reg loss: 0.027 | Tree loss: 3.524 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 026 | Total loss: 3.502 | Reg loss: 0.027 | Tree loss: 3.502 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 026 | Total loss: 3.532 | Reg loss: 0.027 | Tree loss: 3.532 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 026 | Total loss: 3.496 | Reg loss: 0.027 | Tree loss: 3.496 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 026 | Total loss: 3.471 | Reg loss: 0.027 | Tree loss: 3.471 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 026 | Total loss: 3.490 | Reg loss: 0.027 | Tree loss: 3.490 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 026 | Total loss: 3.524 | Reg loss: 0.027 | Tree loss: 3.524 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 026 | Total loss: 3.486 | Reg loss: 0.027 | Tree loss: 3.486 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 026 | Total loss: 3.437 | Reg loss: 0.027 | Tree loss: 3.437 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 026 | Total loss: 3.417 | Reg loss: 0.027 | Tree loss: 3.417 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 026 | Total loss: 3.429 | Reg loss: 0.027 | Tree loss: 3.429 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 026 | Total loss: 3.443 | Reg loss: 0.027 | Tree loss: 3.443 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 026 | Total loss: 3.399 | Reg loss: 0.027 | Tree loss: 3.399 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 026 | Total loss: 3.404 | Reg loss: 0.027 | Tree loss: 3.404 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 026 | Total loss: 3.416 | Reg loss: 0.028 | Tree loss: 3.416 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 026 | Total loss: 3.402 | Reg loss: 0.028 | Tree loss: 3.402 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 026 | Total loss: 3.419 | Reg loss: 0.028 | Tree loss: 3.419 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 026 | Total loss: 3.367 | Reg loss: 0.028 | Tree loss: 3.367 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 026 | Total loss: 3.411 | Reg loss: 0.028 | Tree loss: 3.411 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 026 | Total loss: 3.409 | Reg loss: 0.028 | Tree loss: 3.409 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 026 | Total loss: 3.350 | Reg loss: 0.028 | Tree loss: 3.350 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 026 | Total loss: 3.334 | Reg loss: 0.028 | Tree loss: 3.334 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 026 | Total loss: 3.344 | Reg loss: 0.028 | Tree loss: 3.344 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 026 | Total loss: 3.225 | Reg loss: 0.028 | Tree loss: 3.225 | Accuracy: 0.245614 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 026 | Total loss: 3.502 | Reg loss: 0.027 | Tree loss: 3.502 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 026 | Total loss: 3.507 | Reg loss: 0.027 | Tree loss: 3.507 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 026 | Total loss: 3.469 | Reg loss: 0.027 | Tree loss: 3.469 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 026 | Total loss: 3.487 | Reg loss: 0.027 | Tree loss: 3.487 | Accuracy: 0.136719 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 026 | Total loss: 3.568 | Reg loss: 0.027 | Tree loss: 3.568 | Accuracy: 0.138672 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 026 | Total loss: 3.454 | Reg loss: 0.027 | Tree loss: 3.454 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 026 | Total loss: 3.463 | Reg loss: 0.027 | Tree loss: 3.463 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 026 | Total loss: 3.419 | Reg loss: 0.027 | Tree loss: 3.419 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 026 | Total loss: 3.458 | Reg loss: 0.027 | Tree loss: 3.458 | Accuracy: 0.130859 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 026 | Total loss: 3.391 | Reg loss: 0.027 | Tree loss: 3.391 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 026 | Total loss: 3.413 | Reg loss: 0.027 | Tree loss: 3.413 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 026 | Total loss: 3.413 | Reg loss: 0.027 | Tree loss: 3.413 | Accuracy: 0.140625 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 012 / 026 | Total loss: 3.352 | Reg loss: 0.027 | Tree loss: 3.352 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 026 | Total loss: 3.373 | Reg loss: 0.028 | Tree loss: 3.373 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 026 | Total loss: 3.394 | Reg loss: 0.028 | Tree loss: 3.394 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 026 | Total loss: 3.396 | Reg loss: 0.028 | Tree loss: 3.396 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 026 | Total loss: 3.355 | Reg loss: 0.028 | Tree loss: 3.355 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 026 | Total loss: 3.306 | Reg loss: 0.028 | Tree loss: 3.306 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 026 | Total loss: 3.360 | Reg loss: 0.028 | Tree loss: 3.360 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 026 | Total loss: 3.385 | Reg loss: 0.028 | Tree loss: 3.385 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 026 | Total loss: 3.359 | Reg loss: 0.028 | Tree loss: 3.359 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 026 | Total loss: 3.344 | Reg loss: 0.028 | Tree loss: 3.344 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 026 | Total loss: 3.323 | Reg loss: 0.028 | Tree loss: 3.323 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 026 | Total loss: 3.304 | Reg loss: 0.028 | Tree loss: 3.304 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 026 | Total loss: 3.281 | Reg loss: 0.028 | Tree loss: 3.281 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 026 | Total loss: 3.338 | Reg loss: 0.028 | Tree loss: 3.338 | Accuracy: 0.070175 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 026 | Total loss: 3.431 | Reg loss: 0.027 | Tree loss: 3.431 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 026 | Total loss: 3.499 | Reg loss: 0.027 | Tree loss: 3.499 | Accuracy: 0.111328 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 026 | Total loss: 3.536 | Reg loss: 0.027 | Tree loss: 3.536 | Accuracy: 0.140625 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 026 | Total loss: 3.439 | Reg loss: 0.027 | Tree loss: 3.439 | Accuracy: 0.164062 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 026 | Total loss: 3.445 | Reg loss: 0.027 | Tree loss: 3.445 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 026 | Total loss: 3.371 | Reg loss: 0.027 | Tree loss: 3.371 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 026 | Total loss: 3.410 | Reg loss: 0.027 | Tree loss: 3.410 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 026 | Total loss: 3.336 | Reg loss: 0.027 | Tree loss: 3.336 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 026 | Total loss: 3.412 | Reg loss: 0.028 | Tree loss: 3.412 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 026 | Total loss: 3.386 | Reg loss: 0.028 | Tree loss: 3.386 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 026 | Total loss: 3.359 | Reg loss: 0.028 | Tree loss: 3.359 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 026 | Total loss: 3.370 | Reg loss: 0.028 | Tree loss: 3.370 | Accuracy: 0.126953 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 026 | Total loss: 3.397 | Reg loss: 0.028 | Tree loss: 3.397 | Accuracy: 0.144531 | 0.861 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 026 | Total loss: 3.382 | Reg loss: 0.028 | Tree loss: 3.382 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 026 | Total loss: 3.335 | Reg loss: 0.028 | Tree loss: 3.335 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 026 | Total loss: 3.364 | Reg loss: 0.028 | Tree loss: 3.364 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 026 | Total loss: 3.293 | Reg loss: 0.028 | Tree loss: 3.293 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 026 | Total loss: 3.331 | Reg loss: 0.028 | Tree loss: 3.331 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 026 | Total loss: 3.313 | Reg loss: 0.028 | Tree loss: 3.313 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 026 | Total loss: 3.288 | Reg loss: 0.028 | Tree loss: 3.288 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 026 | Total loss: 3.288 | Reg loss: 0.028 | Tree loss: 3.288 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 026 | Total loss: 3.315 | Reg loss: 0.028 | Tree loss: 3.315 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 026 | Total loss: 3.330 | Reg loss: 0.028 | Tree loss: 3.330 | Accuracy: 0.109375 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 026 | Total loss: 3.255 | Reg loss: 0.028 | Tree loss: 3.255 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 026 | Total loss: 3.248 | Reg loss: 0.028 | Tree loss: 3.248 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 026 | Total loss: 3.348 | Reg loss: 0.028 | Tree loss: 3.348 | Accuracy: 0.122807 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 026 | Total loss: 3.410 | Reg loss: 0.028 | Tree loss: 3.410 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 026 | Total loss: 3.427 | Reg loss: 0.028 | Tree loss: 3.427 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 026 | Total loss: 3.437 | Reg loss: 0.028 | Tree loss: 3.437 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 026 | Total loss: 3.417 | Reg loss: 0.028 | Tree loss: 3.417 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 026 | Total loss: 3.392 | Reg loss: 0.028 | Tree loss: 3.392 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 026 | Total loss: 3.417 | Reg loss: 0.028 | Tree loss: 3.417 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 026 | Total loss: 3.408 | Reg loss: 0.028 | Tree loss: 3.408 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 026 | Total loss: 3.367 | Reg loss: 0.028 | Tree loss: 3.367 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 026 | Total loss: 3.341 | Reg loss: 0.028 | Tree loss: 3.341 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 026 | Total loss: 3.357 | Reg loss: 0.028 | Tree loss: 3.357 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 026 | Total loss: 3.314 | Reg loss: 0.028 | Tree loss: 3.314 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 026 | Total loss: 3.353 | Reg loss: 0.028 | Tree loss: 3.353 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 026 | Total loss: 3.290 | Reg loss: 0.028 | Tree loss: 3.290 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 026 | Total loss: 3.326 | Reg loss: 0.028 | Tree loss: 3.326 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 026 | Total loss: 3.287 | Reg loss: 0.028 | Tree loss: 3.287 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 026 | Total loss: 3.352 | Reg loss: 0.028 | Tree loss: 3.352 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 026 | Total loss: 3.239 | Reg loss: 0.028 | Tree loss: 3.239 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 026 | Total loss: 3.233 | Reg loss: 0.028 | Tree loss: 3.233 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 026 | Total loss: 3.261 | Reg loss: 0.028 | Tree loss: 3.261 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 026 | Total loss: 3.253 | Reg loss: 0.028 | Tree loss: 3.253 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 026 | Total loss: 3.281 | Reg loss: 0.028 | Tree loss: 3.281 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 026 | Total loss: 3.282 | Reg loss: 0.028 | Tree loss: 3.282 | Accuracy: 0.140625 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 022 / 026 | Total loss: 3.363 | Reg loss: 0.028 | Tree loss: 3.363 | Accuracy: 0.113281 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 026 | Total loss: 3.284 | Reg loss: 0.028 | Tree loss: 3.284 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 026 | Total loss: 3.246 | Reg loss: 0.028 | Tree loss: 3.246 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 026 | Total loss: 3.166 | Reg loss: 0.028 | Tree loss: 3.166 | Accuracy: 0.175439 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 026 | Total loss: 3.381 | Reg loss: 0.028 | Tree loss: 3.381 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 026 | Total loss: 3.395 | Reg loss: 0.028 | Tree loss: 3.395 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 026 | Total loss: 3.384 | Reg loss: 0.028 | Tree loss: 3.384 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 026 | Total loss: 3.368 | Reg loss: 0.028 | Tree loss: 3.368 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 026 | Total loss: 3.368 | Reg loss: 0.028 | Tree loss: 3.368 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 026 | Total loss: 3.405 | Reg loss: 0.028 | Tree loss: 3.405 | Accuracy: 0.115234 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 026 | Total loss: 3.386 | Reg loss: 0.028 | Tree loss: 3.386 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 026 | Total loss: 3.316 | Reg loss: 0.028 | Tree loss: 3.316 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 026 | Total loss: 3.311 | Reg loss: 0.028 | Tree loss: 3.311 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 026 | Total loss: 3.297 | Reg loss: 0.028 | Tree loss: 3.297 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 026 | Total loss: 3.388 | Reg loss: 0.028 | Tree loss: 3.388 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 026 | Total loss: 3.328 | Reg loss: 0.028 | Tree loss: 3.328 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 026 | Total loss: 3.241 | Reg loss: 0.028 | Tree loss: 3.241 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 026 | Total loss: 3.276 | Reg loss: 0.028 | Tree loss: 3.276 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 026 | Total loss: 3.279 | Reg loss: 0.028 | Tree loss: 3.279 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 026 | Total loss: 3.261 | Reg loss: 0.028 | Tree loss: 3.261 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 026 | Total loss: 3.280 | Reg loss: 0.028 | Tree loss: 3.280 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 026 | Total loss: 3.313 | Reg loss: 0.028 | Tree loss: 3.313 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 026 | Total loss: 3.199 | Reg loss: 0.028 | Tree loss: 3.199 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 026 | Total loss: 3.198 | Reg loss: 0.028 | Tree loss: 3.198 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 026 | Total loss: 3.253 | Reg loss: 0.028 | Tree loss: 3.253 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 026 | Total loss: 3.276 | Reg loss: 0.028 | Tree loss: 3.276 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 026 | Total loss: 3.225 | Reg loss: 0.028 | Tree loss: 3.225 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 026 | Total loss: 3.205 | Reg loss: 0.028 | Tree loss: 3.205 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 026 | Total loss: 3.217 | Reg loss: 0.028 | Tree loss: 3.217 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 026 | Total loss: 3.344 | Reg loss: 0.028 | Tree loss: 3.344 | Accuracy: 0.052632 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 026 | Total loss: 3.355 | Reg loss: 0.028 | Tree loss: 3.355 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 026 | Total loss: 3.400 | Reg loss: 0.028 | Tree loss: 3.400 | Accuracy: 0.128906 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 026 | Total loss: 3.317 | Reg loss: 0.028 | Tree loss: 3.317 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 026 | Total loss: 3.408 | Reg loss: 0.028 | Tree loss: 3.408 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 026 | Total loss: 3.319 | Reg loss: 0.028 | Tree loss: 3.319 | Accuracy: 0.152344 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 026 | Total loss: 3.339 | Reg loss: 0.028 | Tree loss: 3.339 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 026 | Total loss: 3.277 | Reg loss: 0.028 | Tree loss: 3.277 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 026 | Total loss: 3.345 | Reg loss: 0.028 | Tree loss: 3.345 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 026 | Total loss: 3.315 | Reg loss: 0.028 | Tree loss: 3.315 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 026 | Total loss: 3.294 | Reg loss: 0.028 | Tree loss: 3.294 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 026 | Total loss: 3.264 | Reg loss: 0.028 | Tree loss: 3.264 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 026 | Total loss: 3.254 | Reg loss: 0.028 | Tree loss: 3.254 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 026 | Total loss: 3.242 | Reg loss: 0.028 | Tree loss: 3.242 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 026 | Total loss: 3.294 | Reg loss: 0.028 | Tree loss: 3.294 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 026 | Total loss: 3.250 | Reg loss: 0.028 | Tree loss: 3.250 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 026 | Total loss: 3.322 | Reg loss: 0.028 | Tree loss: 3.322 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 026 | Total loss: 3.190 | Reg loss: 0.028 | Tree loss: 3.190 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 026 | Total loss: 3.243 | Reg loss: 0.028 | Tree loss: 3.243 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 026 | Total loss: 3.249 | Reg loss: 0.028 | Tree loss: 3.249 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 026 | Total loss: 3.237 | Reg loss: 0.028 | Tree loss: 3.237 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 026 | Total loss: 3.216 | Reg loss: 0.028 | Tree loss: 3.216 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 026 | Total loss: 3.269 | Reg loss: 0.028 | Tree loss: 3.269 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 026 | Total loss: 3.167 | Reg loss: 0.028 | Tree loss: 3.167 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 026 | Total loss: 3.143 | Reg loss: 0.028 | Tree loss: 3.143 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 026 | Total loss: 3.184 | Reg loss: 0.028 | Tree loss: 3.184 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 026 | Total loss: 3.043 | Reg loss: 0.029 | Tree loss: 3.043 | Accuracy: 0.228070 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 026 | Total loss: 3.323 | Reg loss: 0.028 | Tree loss: 3.323 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 026 | Total loss: 3.389 | Reg loss: 0.028 | Tree loss: 3.389 | Accuracy: 0.142578 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 026 | Total loss: 3.361 | Reg loss: 0.028 | Tree loss: 3.361 | Accuracy: 0.148438 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 026 | Total loss: 3.277 | Reg loss: 0.028 | Tree loss: 3.277 | Accuracy: 0.154297 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 004 / 026 | Total loss: 3.293 | Reg loss: 0.028 | Tree loss: 3.293 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 026 | Total loss: 3.335 | Reg loss: 0.028 | Tree loss: 3.335 | Accuracy: 0.132812 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 026 | Total loss: 3.263 | Reg loss: 0.028 | Tree loss: 3.263 | Accuracy: 0.154297 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 026 | Total loss: 3.327 | Reg loss: 0.028 | Tree loss: 3.327 | Accuracy: 0.134766 | 0.861 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 026 | Total loss: 3.338 | Reg loss: 0.028 | Tree loss: 3.338 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 026 | Total loss: 3.295 | Reg loss: 0.028 | Tree loss: 3.295 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 026 | Total loss: 3.209 | Reg loss: 0.028 | Tree loss: 3.209 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 026 | Total loss: 3.286 | Reg loss: 0.028 | Tree loss: 3.286 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 026 | Total loss: 3.258 | Reg loss: 0.028 | Tree loss: 3.258 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 026 | Total loss: 3.252 | Reg loss: 0.028 | Tree loss: 3.252 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 026 | Total loss: 3.237 | Reg loss: 0.028 | Tree loss: 3.237 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 026 | Total loss: 3.285 | Reg loss: 0.028 | Tree loss: 3.285 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 026 | Total loss: 3.225 | Reg loss: 0.028 | Tree loss: 3.225 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 026 | Total loss: 3.200 | Reg loss: 0.028 | Tree loss: 3.200 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 026 | Total loss: 3.191 | Reg loss: 0.028 | Tree loss: 3.191 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 026 | Total loss: 3.153 | Reg loss: 0.028 | Tree loss: 3.153 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 026 | Total loss: 3.166 | Reg loss: 0.028 | Tree loss: 3.166 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 026 | Total loss: 3.171 | Reg loss: 0.028 | Tree loss: 3.171 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 026 | Total loss: 3.176 | Reg loss: 0.028 | Tree loss: 3.176 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 026 | Total loss: 3.158 | Reg loss: 0.029 | Tree loss: 3.158 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 026 | Total loss: 3.104 | Reg loss: 0.029 | Tree loss: 3.104 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 026 | Total loss: 3.123 | Reg loss: 0.029 | Tree loss: 3.123 | Accuracy: 0.228070 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 026 | Total loss: 3.353 | Reg loss: 0.028 | Tree loss: 3.353 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 026 | Total loss: 3.243 | Reg loss: 0.028 | Tree loss: 3.243 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 026 | Total loss: 3.305 | Reg loss: 0.028 | Tree loss: 3.305 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 026 | Total loss: 3.317 | Reg loss: 0.028 | Tree loss: 3.317 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 026 | Total loss: 3.291 | Reg loss: 0.028 | Tree loss: 3.291 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 026 | Total loss: 3.328 | Reg loss: 0.028 | Tree loss: 3.328 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 026 | Total loss: 3.259 | Reg loss: 0.028 | Tree loss: 3.259 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 026 | Total loss: 3.244 | Reg loss: 0.028 | Tree loss: 3.244 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 026 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 026 | Total loss: 3.217 | Reg loss: 0.028 | Tree loss: 3.217 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 026 | Total loss: 3.276 | Reg loss: 0.028 | Tree loss: 3.276 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 026 | Total loss: 3.246 | Reg loss: 0.028 | Tree loss: 3.246 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 026 | Total loss: 3.191 | Reg loss: 0.028 | Tree loss: 3.191 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 026 | Total loss: 3.241 | Reg loss: 0.028 | Tree loss: 3.241 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 026 | Total loss: 3.231 | Reg loss: 0.028 | Tree loss: 3.231 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 026 | Total loss: 3.247 | Reg loss: 0.028 | Tree loss: 3.247 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 026 | Total loss: 3.243 | Reg loss: 0.028 | Tree loss: 3.243 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 026 | Total loss: 3.198 | Reg loss: 0.028 | Tree loss: 3.198 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 026 | Total loss: 3.182 | Reg loss: 0.028 | Tree loss: 3.182 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 026 | Total loss: 3.150 | Reg loss: 0.028 | Tree loss: 3.150 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 026 | Total loss: 3.096 | Reg loss: 0.028 | Tree loss: 3.096 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 026 | Total loss: 3.138 | Reg loss: 0.028 | Tree loss: 3.138 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 026 | Total loss: 3.162 | Reg loss: 0.029 | Tree loss: 3.162 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 026 | Total loss: 3.180 | Reg loss: 0.029 | Tree loss: 3.180 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 026 | Total loss: 3.149 | Reg loss: 0.029 | Tree loss: 3.149 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 026 | Total loss: 3.149 | Reg loss: 0.029 | Tree loss: 3.149 | Accuracy: 0.140351 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 026 | Total loss: 3.302 | Reg loss: 0.028 | Tree loss: 3.302 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 026 | Total loss: 3.300 | Reg loss: 0.028 | Tree loss: 3.300 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 026 | Total loss: 3.294 | Reg loss: 0.028 | Tree loss: 3.294 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 026 | Total loss: 3.280 | Reg loss: 0.028 | Tree loss: 3.280 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 026 | Total loss: 3.246 | Reg loss: 0.028 | Tree loss: 3.246 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 026 | Total loss: 3.300 | Reg loss: 0.028 | Tree loss: 3.300 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 026 | Total loss: 3.294 | Reg loss: 0.028 | Tree loss: 3.294 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 026 | Total loss: 3.274 | Reg loss: 0.028 | Tree loss: 3.274 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 026 | Total loss: 3.215 | Reg loss: 0.028 | Tree loss: 3.215 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 026 | Total loss: 3.214 | Reg loss: 0.028 | Tree loss: 3.214 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 026 | Total loss: 3.200 | Reg loss: 0.028 | Tree loss: 3.200 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 026 | Total loss: 3.173 | Reg loss: 0.028 | Tree loss: 3.173 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 026 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 026 | Total loss: 3.252 | Reg loss: 0.028 | Tree loss: 3.252 | Accuracy: 0.126953 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 014 / 026 | Total loss: 3.187 | Reg loss: 0.028 | Tree loss: 3.187 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 026 | Total loss: 3.154 | Reg loss: 0.028 | Tree loss: 3.154 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 026 | Total loss: 3.213 | Reg loss: 0.028 | Tree loss: 3.213 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 026 | Total loss: 3.167 | Reg loss: 0.028 | Tree loss: 3.167 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 026 | Total loss: 3.147 | Reg loss: 0.028 | Tree loss: 3.147 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 026 | Total loss: 3.123 | Reg loss: 0.028 | Tree loss: 3.123 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 026 | Total loss: 3.144 | Reg loss: 0.029 | Tree loss: 3.144 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 026 | Total loss: 3.126 | Reg loss: 0.029 | Tree loss: 3.126 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 026 | Total loss: 3.113 | Reg loss: 0.029 | Tree loss: 3.113 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 026 | Total loss: 3.141 | Reg loss: 0.029 | Tree loss: 3.141 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 026 | Total loss: 3.107 | Reg loss: 0.029 | Tree loss: 3.107 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 026 | Total loss: 3.137 | Reg loss: 0.029 | Tree loss: 3.137 | Accuracy: 0.157895 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 026 | Total loss: 3.277 | Reg loss: 0.028 | Tree loss: 3.277 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 026 | Total loss: 3.274 | Reg loss: 0.028 | Tree loss: 3.274 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 026 | Total loss: 3.228 | Reg loss: 0.028 | Tree loss: 3.228 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 026 | Total loss: 3.281 | Reg loss: 0.028 | Tree loss: 3.281 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 026 | Total loss: 3.264 | Reg loss: 0.028 | Tree loss: 3.264 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 026 | Total loss: 3.253 | Reg loss: 0.028 | Tree loss: 3.253 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 026 | Total loss: 3.224 | Reg loss: 0.028 | Tree loss: 3.224 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 026 | Total loss: 3.209 | Reg loss: 0.028 | Tree loss: 3.209 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 026 | Total loss: 3.251 | Reg loss: 0.028 | Tree loss: 3.251 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 026 | Total loss: 3.205 | Reg loss: 0.028 | Tree loss: 3.205 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 026 | Total loss: 3.256 | Reg loss: 0.028 | Tree loss: 3.256 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 026 | Total loss: 3.173 | Reg loss: 0.028 | Tree loss: 3.173 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 026 | Total loss: 3.168 | Reg loss: 0.028 | Tree loss: 3.168 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 026 | Total loss: 3.177 | Reg loss: 0.028 | Tree loss: 3.177 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 026 | Total loss: 3.197 | Reg loss: 0.028 | Tree loss: 3.197 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 026 | Total loss: 3.138 | Reg loss: 0.028 | Tree loss: 3.138 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 026 | Total loss: 3.177 | Reg loss: 0.028 | Tree loss: 3.177 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 026 | Total loss: 3.220 | Reg loss: 0.028 | Tree loss: 3.220 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 026 | Total loss: 3.138 | Reg loss: 0.029 | Tree loss: 3.138 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 026 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 026 | Total loss: 3.099 | Reg loss: 0.029 | Tree loss: 3.099 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 026 | Total loss: 3.147 | Reg loss: 0.029 | Tree loss: 3.147 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 026 | Total loss: 3.103 | Reg loss: 0.029 | Tree loss: 3.103 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 026 | Total loss: 3.308 | Reg loss: 0.029 | Tree loss: 3.308 | Accuracy: 0.140351 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 026 | Total loss: 3.280 | Reg loss: 0.028 | Tree loss: 3.280 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 026 | Total loss: 3.220 | Reg loss: 0.028 | Tree loss: 3.220 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 026 | Total loss: 3.257 | Reg loss: 0.028 | Tree loss: 3.257 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 026 | Total loss: 3.248 | Reg loss: 0.028 | Tree loss: 3.248 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 026 | Total loss: 3.249 | Reg loss: 0.028 | Tree loss: 3.249 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 026 | Total loss: 3.237 | Reg loss: 0.028 | Tree loss: 3.237 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 026 | Total loss: 3.256 | Reg loss: 0.028 | Tree loss: 3.256 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 026 | Total loss: 3.254 | Reg loss: 0.028 | Tree loss: 3.254 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 026 | Total loss: 3.185 | Reg loss: 0.028 | Tree loss: 3.185 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 026 | Total loss: 3.209 | Reg loss: 0.028 | Tree loss: 3.209 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 026 | Total loss: 3.201 | Reg loss: 0.028 | Tree loss: 3.201 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 026 | Total loss: 3.170 | Reg loss: 0.028 | Tree loss: 3.170 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 026 | Total loss: 3.156 | Reg loss: 0.028 | Tree loss: 3.156 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 026 | Total loss: 3.091 | Reg loss: 0.028 | Tree loss: 3.091 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 026 | Total loss: 3.182 | Reg loss: 0.028 | Tree loss: 3.182 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 026 | Total loss: 3.163 | Reg loss: 0.028 | Tree loss: 3.163 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 026 | Total loss: 3.089 | Reg loss: 0.029 | Tree loss: 3.089 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 026 | Total loss: 3.098 | Reg loss: 0.029 | Tree loss: 3.098 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 026 | Total loss: 3.139 | Reg loss: 0.029 | Tree loss: 3.139 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 026 | Total loss: 3.126 | Reg loss: 0.029 | Tree loss: 3.126 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 026 | Total loss: 3.114 | Reg loss: 0.029 | Tree loss: 3.114 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.140625 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 024 / 026 | Total loss: 3.123 | Reg loss: 0.029 | Tree loss: 3.123 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 026 | Total loss: 3.112 | Reg loss: 0.029 | Tree loss: 3.112 | Accuracy: 0.175439 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 026 | Total loss: 3.237 | Reg loss: 0.028 | Tree loss: 3.237 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 026 | Total loss: 3.313 | Reg loss: 0.028 | Tree loss: 3.313 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 026 | Total loss: 3.201 | Reg loss: 0.028 | Tree loss: 3.201 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 026 | Total loss: 3.227 | Reg loss: 0.028 | Tree loss: 3.227 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 026 | Total loss: 3.211 | Reg loss: 0.028 | Tree loss: 3.211 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 026 | Total loss: 3.241 | Reg loss: 0.028 | Tree loss: 3.241 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 026 | Total loss: 3.176 | Reg loss: 0.028 | Tree loss: 3.176 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 026 | Total loss: 3.240 | Reg loss: 0.028 | Tree loss: 3.240 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 026 | Total loss: 3.122 | Reg loss: 0.028 | Tree loss: 3.122 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 026 | Total loss: 3.141 | Reg loss: 0.028 | Tree loss: 3.141 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 026 | Total loss: 3.183 | Reg loss: 0.028 | Tree loss: 3.183 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 026 | Total loss: 3.204 | Reg loss: 0.028 | Tree loss: 3.204 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 026 | Total loss: 3.170 | Reg loss: 0.028 | Tree loss: 3.170 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 026 | Total loss: 3.147 | Reg loss: 0.028 | Tree loss: 3.147 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 026 | Total loss: 3.138 | Reg loss: 0.029 | Tree loss: 3.138 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 026 | Total loss: 3.136 | Reg loss: 0.029 | Tree loss: 3.136 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 026 | Total loss: 3.106 | Reg loss: 0.029 | Tree loss: 3.106 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 026 | Total loss: 3.145 | Reg loss: 0.029 | Tree loss: 3.145 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 026 | Total loss: 3.018 | Reg loss: 0.029 | Tree loss: 3.018 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 026 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 026 | Total loss: 3.165 | Reg loss: 0.029 | Tree loss: 3.165 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 026 | Total loss: 3.140 | Reg loss: 0.029 | Tree loss: 3.140 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 026 | Total loss: 3.075 | Reg loss: 0.029 | Tree loss: 3.075 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 026 | Total loss: 3.195 | Reg loss: 0.029 | Tree loss: 3.195 | Accuracy: 0.228070 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 026 | Total loss: 3.221 | Reg loss: 0.028 | Tree loss: 3.221 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 026 | Total loss: 3.250 | Reg loss: 0.028 | Tree loss: 3.250 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 026 | Total loss: 3.211 | Reg loss: 0.028 | Tree loss: 3.211 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 026 | Total loss: 3.182 | Reg loss: 0.028 | Tree loss: 3.182 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 026 | Total loss: 3.189 | Reg loss: 0.028 | Tree loss: 3.189 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 026 | Total loss: 3.205 | Reg loss: 0.028 | Tree loss: 3.205 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 026 | Total loss: 3.201 | Reg loss: 0.028 | Tree loss: 3.201 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 026 | Total loss: 3.181 | Reg loss: 0.028 | Tree loss: 3.181 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 026 | Total loss: 3.165 | Reg loss: 0.028 | Tree loss: 3.165 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 026 | Total loss: 3.168 | Reg loss: 0.028 | Tree loss: 3.168 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 026 | Total loss: 3.180 | Reg loss: 0.028 | Tree loss: 3.180 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 026 | Total loss: 3.153 | Reg loss: 0.028 | Tree loss: 3.153 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 026 | Total loss: 3.150 | Reg loss: 0.029 | Tree loss: 3.150 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 026 | Total loss: 3.118 | Reg loss: 0.029 | Tree loss: 3.118 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 026 | Total loss: 3.130 | Reg loss: 0.029 | Tree loss: 3.130 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 026 | Total loss: 3.096 | Reg loss: 0.029 | Tree loss: 3.096 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 026 | Total loss: 3.180 | Reg loss: 0.029 | Tree loss: 3.180 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 026 | Total loss: 3.096 | Reg loss: 0.029 | Tree loss: 3.096 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 026 | Total loss: 3.125 | Reg loss: 0.029 | Tree loss: 3.125 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 026 | Total loss: 3.071 | Reg loss: 0.029 | Tree loss: 3.071 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 026 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 026 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 026 | Total loss: 3.057 | Reg loss: 0.029 | Tree loss: 3.057 | Accuracy: 0.157895 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 026 | Total loss: 3.192 | Reg loss: 0.028 | Tree loss: 3.192 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 026 | Total loss: 3.215 | Reg loss: 0.028 | Tree loss: 3.215 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 026 | Total loss: 3.196 | Reg loss: 0.028 | Tree loss: 3.196 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 026 | Total loss: 3.181 | Reg loss: 0.028 | Tree loss: 3.181 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 026 | Total loss: 3.270 | Reg loss: 0.028 | Tree loss: 3.270 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 026 | Total loss: 3.178 | Reg loss: 0.028 | Tree loss: 3.178 | Accuracy: 0.144531 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 006 / 026 | Total loss: 3.231 | Reg loss: 0.028 | Tree loss: 3.231 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 026 | Total loss: 3.128 | Reg loss: 0.028 | Tree loss: 3.128 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 026 | Total loss: 3.173 | Reg loss: 0.028 | Tree loss: 3.173 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 026 | Total loss: 3.163 | Reg loss: 0.028 | Tree loss: 3.163 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 026 | Total loss: 3.173 | Reg loss: 0.029 | Tree loss: 3.173 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 026 | Total loss: 3.118 | Reg loss: 0.029 | Tree loss: 3.118 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 026 | Total loss: 3.162 | Reg loss: 0.029 | Tree loss: 3.162 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 026 | Total loss: 3.144 | Reg loss: 0.029 | Tree loss: 3.144 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 026 | Total loss: 3.077 | Reg loss: 0.029 | Tree loss: 3.077 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 026 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 026 | Total loss: 3.055 | Reg loss: 0.029 | Tree loss: 3.055 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 026 | Total loss: 3.057 | Reg loss: 0.029 | Tree loss: 3.057 | Accuracy: 0.175439 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 026 | Total loss: 3.232 | Reg loss: 0.028 | Tree loss: 3.232 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 026 | Total loss: 3.152 | Reg loss: 0.028 | Tree loss: 3.152 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 026 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 026 | Total loss: 3.192 | Reg loss: 0.028 | Tree loss: 3.192 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 026 | Total loss: 3.148 | Reg loss: 0.028 | Tree loss: 3.148 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 026 | Total loss: 3.221 | Reg loss: 0.028 | Tree loss: 3.221 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 026 | Total loss: 3.176 | Reg loss: 0.028 | Tree loss: 3.176 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 026 | Total loss: 3.095 | Reg loss: 0.028 | Tree loss: 3.095 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 026 | Total loss: 3.175 | Reg loss: 0.029 | Tree loss: 3.175 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 026 | Total loss: 3.130 | Reg loss: 0.029 | Tree loss: 3.130 | Accuracy: 0.119141 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 026 | Total loss: 3.146 | Reg loss: 0.029 | Tree loss: 3.146 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 026 | Total loss: 3.191 | Reg loss: 0.029 | Tree loss: 3.191 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 026 | Total loss: 3.068 | Reg loss: 0.029 | Tree loss: 3.068 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 026 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 026 | Total loss: 3.121 | Reg loss: 0.029 | Tree loss: 3.121 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 026 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 026 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 026 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 026 | Total loss: 3.015 | Reg loss: 0.029 | Tree loss: 3.015 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 026 | Total loss: 3.082 | Reg loss: 0.029 | Tree loss: 3.082 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.122807 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 026 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 026 | Total loss: 3.175 | Reg loss: 0.028 | Tree loss: 3.175 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 026 | Total loss: 3.212 | Reg loss: 0.028 | Tree loss: 3.212 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 026 | Total loss: 3.167 | Reg loss: 0.028 | Tree loss: 3.167 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 026 | Total loss: 3.116 | Reg loss: 0.028 | Tree loss: 3.116 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 026 | Total loss: 3.141 | Reg loss: 0.029 | Tree loss: 3.141 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 026 | Total loss: 3.130 | Reg loss: 0.029 | Tree loss: 3.130 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 026 | Total loss: 3.174 | Reg loss: 0.029 | Tree loss: 3.174 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 026 | Total loss: 3.208 | Reg loss: 0.029 | Tree loss: 3.208 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 026 | Total loss: 3.141 | Reg loss: 0.029 | Tree loss: 3.141 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 026 | Total loss: 3.107 | Reg loss: 0.029 | Tree loss: 3.107 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 026 | Total loss: 3.063 | Reg loss: 0.029 | Tree loss: 3.063 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 026 | Total loss: 3.129 | Reg loss: 0.029 | Tree loss: 3.129 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 026 | Total loss: 3.060 | Reg loss: 0.029 | Tree loss: 3.060 | Accuracy: 0.154297 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 016 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.158203 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 026 | Total loss: 3.080 | Reg loss: 0.029 | Tree loss: 3.080 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 026 | Total loss: 3.114 | Reg loss: 0.029 | Tree loss: 3.114 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 026 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 026 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 026 | Total loss: 2.890 | Reg loss: 0.029 | Tree loss: 2.890 | Accuracy: 0.210526 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 026 | Total loss: 3.232 | Reg loss: 0.029 | Tree loss: 3.232 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 026 | Total loss: 3.143 | Reg loss: 0.029 | Tree loss: 3.143 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 026 | Total loss: 3.186 | Reg loss: 0.029 | Tree loss: 3.186 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 026 | Total loss: 3.103 | Reg loss: 0.029 | Tree loss: 3.103 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 026 | Total loss: 3.119 | Reg loss: 0.029 | Tree loss: 3.119 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 026 | Total loss: 3.142 | Reg loss: 0.029 | Tree loss: 3.142 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 026 | Total loss: 3.165 | Reg loss: 0.029 | Tree loss: 3.165 | Accuracy: 0.123047 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 026 | Total loss: 3.214 | Reg loss: 0.029 | Tree loss: 3.214 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 026 | Total loss: 3.115 | Reg loss: 0.029 | Tree loss: 3.115 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 026 | Total loss: 3.109 | Reg loss: 0.029 | Tree loss: 3.109 | Accuracy: 0.117188 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.126953 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 026 | Total loss: 3.097 | Reg loss: 0.029 | Tree loss: 3.097 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 026 | Total loss: 3.084 | Reg loss: 0.029 | Tree loss: 3.084 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 026 | Total loss: 3.102 | Reg loss: 0.029 | Tree loss: 3.102 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 026 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 026 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 026 | Total loss: 3.075 | Reg loss: 0.029 | Tree loss: 3.075 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 026 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.140351 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 026 | Total loss: 3.150 | Reg loss: 0.029 | Tree loss: 3.150 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 026 | Total loss: 3.161 | Reg loss: 0.029 | Tree loss: 3.161 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 026 | Total loss: 3.123 | Reg loss: 0.029 | Tree loss: 3.123 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 026 | Total loss: 3.184 | Reg loss: 0.029 | Tree loss: 3.184 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 026 | Total loss: 3.082 | Reg loss: 0.029 | Tree loss: 3.082 | Accuracy: 0.154297 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 026 | Total loss: 3.132 | Reg loss: 0.029 | Tree loss: 3.132 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 026 | Total loss: 3.172 | Reg loss: 0.029 | Tree loss: 3.172 | Accuracy: 0.134766 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 026 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 026 | Total loss: 3.123 | Reg loss: 0.029 | Tree loss: 3.123 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 026 | Total loss: 3.111 | Reg loss: 0.029 | Tree loss: 3.111 | Accuracy: 0.121094 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 026 | Total loss: 3.125 | Reg loss: 0.029 | Tree loss: 3.125 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 026 | Total loss: 3.099 | Reg loss: 0.029 | Tree loss: 3.099 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 026 | Total loss: 3.093 | Reg loss: 0.029 | Tree loss: 3.093 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 026 | Total loss: 3.059 | Reg loss: 0.029 | Tree loss: 3.059 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 026 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 026 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 026 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 026 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 026 | Total loss: 3.083 | Reg loss: 0.029 | Tree loss: 3.083 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 026 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.158203 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 026 | Total loss: 2.995 | Reg loss: 0.029 | Tree loss: 2.995 | Accuracy: 0.105263 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 026 | Total loss: 3.192 | Reg loss: 0.029 | Tree loss: 3.192 | Accuracy: 0.125000 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 026 | Total loss: 3.167 | Reg loss: 0.029 | Tree loss: 3.167 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 026 | Total loss: 3.150 | Reg loss: 0.029 | Tree loss: 3.150 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 026 | Total loss: 3.166 | Reg loss: 0.029 | Tree loss: 3.166 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 026 | Total loss: 3.088 | Reg loss: 0.029 | Tree loss: 3.088 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 026 | Total loss: 3.126 | Reg loss: 0.029 | Tree loss: 3.126 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 026 | Total loss: 3.153 | Reg loss: 0.029 | Tree loss: 3.153 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 026 | Total loss: 3.097 | Reg loss: 0.029 | Tree loss: 3.097 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 026 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 026 | Total loss: 3.026 | Reg loss: 0.029 | Tree loss: 3.026 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 026 | Total loss: 3.084 | Reg loss: 0.029 | Tree loss: 3.084 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 026 | Total loss: 3.121 | Reg loss: 0.029 | Tree loss: 3.121 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 026 | Total loss: 3.106 | Reg loss: 0.029 | Tree loss: 3.106 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 026 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 026 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 026 | Total loss: 2.990 | Reg loss: 0.029 | Tree loss: 2.990 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 026 | Total loss: 3.015 | Reg loss: 0.029 | Tree loss: 3.015 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 026 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 026 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 026 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 026 | Total loss: 2.867 | Reg loss: 0.029 | Tree loss: 2.867 | Accuracy: 0.210526 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 026 | Total loss: 3.149 | Reg loss: 0.029 | Tree loss: 3.149 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 026 | Total loss: 3.143 | Reg loss: 0.029 | Tree loss: 3.143 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 026 | Total loss: 3.175 | Reg loss: 0.029 | Tree loss: 3.175 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 026 | Total loss: 3.152 | Reg loss: 0.029 | Tree loss: 3.152 | Accuracy: 0.130859 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 026 | Total loss: 3.111 | Reg loss: 0.029 | Tree loss: 3.111 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 026 | Total loss: 3.103 | Reg loss: 0.029 | Tree loss: 3.103 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 026 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 026 | Total loss: 3.059 | Reg loss: 0.029 | Tree loss: 3.059 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 026 | Total loss: 3.096 | Reg loss: 0.029 | Tree loss: 3.096 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.138672 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.152344 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 026 | Total loss: 3.117 | Reg loss: 0.029 | Tree loss: 3.117 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 026 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.140625 | 0.86 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 026 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 026 | Total loss: 3.053 | Reg loss: 0.029 | Tree loss: 3.053 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 026 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 026 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 026 | Total loss: 3.000 | Reg loss: 0.029 | Tree loss: 3.000 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 026 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 026 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 026 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 026 | Total loss: 2.910 | Reg loss: 0.029 | Tree loss: 2.910 | Accuracy: 0.122807 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 026 | Total loss: 3.173 | Reg loss: 0.029 | Tree loss: 3.173 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 026 | Total loss: 3.118 | Reg loss: 0.029 | Tree loss: 3.118 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 026 | Total loss: 3.087 | Reg loss: 0.029 | Tree loss: 3.087 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 026 | Total loss: 3.145 | Reg loss: 0.029 | Tree loss: 3.145 | Accuracy: 0.136719 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.146484 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 026 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.134766 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 008 / 026 | Total loss: 3.136 | Reg loss: 0.029 | Tree loss: 3.136 | Accuracy: 0.144531 | 0.86 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 026 | Total loss: 3.120 | Reg loss: 0.029 | Tree loss: 3.120 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 026 | Total loss: 3.094 | Reg loss: 0.029 | Tree loss: 3.094 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 026 | Total loss: 3.058 | Reg loss: 0.029 | Tree loss: 3.058 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 026 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 026 | Total loss: 3.071 | Reg loss: 0.029 | Tree loss: 3.071 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 026 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 026 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 026 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 026 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 026 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.157895 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 026 | Total loss: 3.104 | Reg loss: 0.029 | Tree loss: 3.104 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 026 | Total loss: 3.159 | Reg loss: 0.029 | Tree loss: 3.159 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 026 | Total loss: 3.147 | Reg loss: 0.029 | Tree loss: 3.147 | Accuracy: 0.132812 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 026 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 0.142578 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.148438 | 0.86 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 026 | Total loss: 3.113 | Reg loss: 0.029 | Tree loss: 3.113 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 026 | Total loss: 3.106 | Reg loss: 0.029 | Tree loss: 3.106 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 026 | Total loss: 3.060 | Reg loss: 0.029 | Tree loss: 3.060 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 026 | Total loss: 3.079 | Reg loss: 0.029 | Tree loss: 3.079 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.126953 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 026 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 026 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 026 | Total loss: 2.980 | Reg loss: 0.029 | Tree loss: 2.980 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 026 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 026 | Total loss: 2.995 | Reg loss: 0.029 | Tree loss: 2.995 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 026 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 026 | Total loss: 2.770 | Reg loss: 0.029 | Tree loss: 2.770 | Accuracy: 0.210526 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 026 | Total loss: 3.124 | Reg loss: 0.029 | Tree loss: 3.124 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 026 | Total loss: 3.132 | Reg loss: 0.029 | Tree loss: 3.132 | Accuracy: 0.128906 | 0.86 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 026 | Total loss: 3.145 | Reg loss: 0.029 | Tree loss: 3.145 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 026 | Total loss: 3.069 | Reg loss: 0.029 | Tree loss: 3.069 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 026 | Total loss: 3.099 | Reg loss: 0.029 | Tree loss: 3.099 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 026 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 026 | Total loss: 3.107 | Reg loss: 0.029 | Tree loss: 3.107 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 026 | Total loss: 3.152 | Reg loss: 0.029 | Tree loss: 3.152 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 026 | Total loss: 3.077 | Reg loss: 0.029 | Tree loss: 3.077 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 026 | Total loss: 3.097 | Reg loss: 0.029 | Tree loss: 3.097 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 026 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 026 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 026 | Total loss: 3.003 | Reg loss: 0.029 | Tree loss: 3.003 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 026 | Total loss: 3.026 | Reg loss: 0.029 | Tree loss: 3.026 | Accuracy: 0.158203 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 026 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 026 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.128906 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 018 / 026 | Total loss: 3.003 | Reg loss: 0.029 | Tree loss: 3.003 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 026 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 026 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 026 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 026 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.192982 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 026 | Total loss: 3.076 | Reg loss: 0.029 | Tree loss: 3.076 | Accuracy: 0.158203 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 026 | Total loss: 3.175 | Reg loss: 0.029 | Tree loss: 3.175 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 026 | Total loss: 3.163 | Reg loss: 0.029 | Tree loss: 3.163 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 026 | Total loss: 3.138 | Reg loss: 0.029 | Tree loss: 3.138 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 026 | Total loss: 3.106 | Reg loss: 0.029 | Tree loss: 3.106 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 026 | Total loss: 3.091 | Reg loss: 0.029 | Tree loss: 3.091 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 026 | Total loss: 3.028 | Reg loss: 0.029 | Tree loss: 3.028 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 026 | Total loss: 3.060 | Reg loss: 0.029 | Tree loss: 3.060 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 026 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 026 | Total loss: 3.073 | Reg loss: 0.029 | Tree loss: 3.073 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 026 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 026 | Total loss: 2.888 | Reg loss: 0.029 | Tree loss: 2.888 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 026 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 026 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 026 | Total loss: 2.903 | Reg loss: 0.029 | Tree loss: 2.903 | Accuracy: 0.087719 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 026 | Total loss: 3.175 | Reg loss: 0.029 | Tree loss: 3.175 | Accuracy: 0.142578 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 026 | Total loss: 3.108 | Reg loss: 0.029 | Tree loss: 3.108 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 026 | Total loss: 3.131 | Reg loss: 0.029 | Tree loss: 3.131 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 026 | Total loss: 3.059 | Reg loss: 0.029 | Tree loss: 3.059 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 026 | Total loss: 3.133 | Reg loss: 0.029 | Tree loss: 3.133 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 026 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 026 | Total loss: 3.042 | Reg loss: 0.029 | Tree loss: 3.042 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 026 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.119141 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 026 | Total loss: 3.018 | Reg loss: 0.029 | Tree loss: 3.018 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 026 | Total loss: 2.995 | Reg loss: 0.029 | Tree loss: 2.995 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 026 | Total loss: 2.958 | Reg loss: 0.029 | Tree loss: 2.958 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 026 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 026 | Total loss: 2.916 | Reg loss: 0.029 | Tree loss: 2.916 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 026 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 026 | Total loss: 2.896 | Reg loss: 0.029 | Tree loss: 2.896 | Accuracy: 0.087719 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 026 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 026 | Total loss: 3.161 | Reg loss: 0.029 | Tree loss: 3.161 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 026 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 026 | Total loss: 3.108 | Reg loss: 0.029 | Tree loss: 3.108 | Accuracy: 0.158203 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 026 | Total loss: 3.108 | Reg loss: 0.029 | Tree loss: 3.108 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 026 | Total loss: 3.042 | Reg loss: 0.029 | Tree loss: 3.042 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 026 | Total loss: 3.079 | Reg loss: 0.029 | Tree loss: 3.079 | Accuracy: 0.128906 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 026 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 026 | Total loss: 3.079 | Reg loss: 0.029 | Tree loss: 3.079 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 026 | Total loss: 3.042 | Reg loss: 0.029 | Tree loss: 3.042 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 026 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.117188 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 026 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 026 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 026 | Total loss: 3.043 | Reg loss: 0.029 | Tree loss: 3.043 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 026 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 026 | Total loss: 2.921 | Reg loss: 0.029 | Tree loss: 2.921 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 026 | Total loss: 2.875 | Reg loss: 0.029 | Tree loss: 2.875 | Accuracy: 0.210526 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 026 | Total loss: 3.167 | Reg loss: 0.029 | Tree loss: 3.167 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 026 | Total loss: 3.091 | Reg loss: 0.029 | Tree loss: 3.091 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 026 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 026 | Total loss: 3.052 | Reg loss: 0.029 | Tree loss: 3.052 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 026 | Total loss: 3.071 | Reg loss: 0.029 | Tree loss: 3.071 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 026 | Total loss: 3.063 | Reg loss: 0.029 | Tree loss: 3.063 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 026 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 026 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 026 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.156250 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 026 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 026 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 026 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 026 | Total loss: 3.043 | Reg loss: 0.029 | Tree loss: 3.043 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.115234 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 026 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 026 | Total loss: 3.000 | Reg loss: 0.029 | Tree loss: 3.000 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.123047 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 026 | Total loss: 2.983 | Reg loss: 0.029 | Tree loss: 2.983 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 026 | Total loss: 2.813 | Reg loss: 0.029 | Tree loss: 2.813 | Accuracy: 0.210526 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 026 | Total loss: 3.122 | Reg loss: 0.029 | Tree loss: 3.122 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 026 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 026 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.150391 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 026 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.121094 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.136719 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 010 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 026 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 026 | Total loss: 3.076 | Reg loss: 0.029 | Tree loss: 3.076 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 026 | Total loss: 2.952 | Reg loss: 0.029 | Tree loss: 2.952 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.125000 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.136719 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 026 | Total loss: 2.925 | Reg loss: 0.029 | Tree loss: 2.925 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 026 | Total loss: 2.987 | Reg loss: 0.029 | Tree loss: 2.987 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 026 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.130859 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 026 | Total loss: 2.882 | Reg loss: 0.029 | Tree loss: 2.882 | Accuracy: 0.171875 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.140625 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 026 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.148438 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 026 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.138672 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 026 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.144531 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 026 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 026 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.192982 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 026 | Total loss: 3.102 | Reg loss: 0.029 | Tree loss: 3.102 | Accuracy: 0.160156 | 0.859 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 026 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.134766 | 0.859 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 026 | Total loss: 3.108 | Reg loss: 0.029 | Tree loss: 3.108 | Accuracy: 0.146484 | 0.859 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.132812 | 0.859 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 026 | Total loss: 3.028 | Reg loss: 0.029 | Tree loss: 3.028 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.169922 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 026 | Total loss: 3.057 | Reg loss: 0.029 | Tree loss: 3.057 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 026 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 026 | Total loss: 3.067 | Reg loss: 0.029 | Tree loss: 3.067 | Accuracy: 0.142578 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 026 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.175781 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 026 | Total loss: 3.055 | Reg loss: 0.029 | Tree loss: 3.055 | Accuracy: 0.158203 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.126953 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.185547 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 026 | Total loss: 2.983 | Reg loss: 0.029 | Tree loss: 2.983 | Accuracy: 0.142578 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 026 | Total loss: 2.920 | Reg loss: 0.029 | Tree loss: 2.920 | Accuracy: 0.160156 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.123047 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 026 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.123047 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 026 | Total loss: 2.965 | Reg loss: 0.029 | Tree loss: 2.965 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.162109 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 026 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 026 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.158203 | 0.858 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 026 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.140351 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 026 | Total loss: 3.081 | Reg loss: 0.029 | Tree loss: 3.081 | Accuracy: 0.154297 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 026 | Total loss: 3.154 | Reg loss: 0.029 | Tree loss: 3.154 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 026 | Total loss: 3.063 | Reg loss: 0.029 | Tree loss: 3.063 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 026 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 026 | Total loss: 3.078 | Reg loss: 0.029 | Tree loss: 3.078 | Accuracy: 0.166016 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 026 | Total loss: 3.035 | Reg loss: 0.029 | Tree loss: 3.035 | Accuracy: 0.123047 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 026 | Total loss: 3.089 | Reg loss: 0.029 | Tree loss: 3.089 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 026 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 026 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 026 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.154297 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.171875 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 026 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.156250 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.164062 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 026 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.113281 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 026 | Total loss: 3.042 | Reg loss: 0.029 | Tree loss: 3.042 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.162109 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.132812 | 0.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 020 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.167969 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.154297 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.138672 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 026 | Total loss: 2.914 | Reg loss: 0.029 | Tree loss: 2.914 | Accuracy: 0.175781 | 0.858 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 026 | Total loss: 2.815 | Reg loss: 0.029 | Tree loss: 2.815 | Accuracy: 0.175439 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 026 | Total loss: 3.136 | Reg loss: 0.029 | Tree loss: 3.136 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 026 | Total loss: 3.117 | Reg loss: 0.029 | Tree loss: 3.117 | Accuracy: 0.148438 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.164062 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 026 | Total loss: 3.058 | Reg loss: 0.029 | Tree loss: 3.058 | Accuracy: 0.156250 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 026 | Total loss: 3.128 | Reg loss: 0.029 | Tree loss: 3.128 | Accuracy: 0.125000 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 026 | Total loss: 3.075 | Reg loss: 0.029 | Tree loss: 3.075 | Accuracy: 0.144531 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.167969 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.177734 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 026 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 026 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.138672 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 026 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.156250 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 026 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.140625 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 026 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.167969 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.169922 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.146484 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 026 | Total loss: 2.898 | Reg loss: 0.029 | Tree loss: 2.898 | Accuracy: 0.144531 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 026 | Total loss: 2.987 | Reg loss: 0.029 | Tree loss: 2.987 | Accuracy: 0.130859 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.162109 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.142578 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.160156 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 026 | Total loss: 2.935 | Reg loss: 0.029 | Tree loss: 2.935 | Accuracy: 0.132812 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.134766 | 0.858 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 026 | Total loss: 2.750 | Reg loss: 0.029 | Tree loss: 2.750 | Accuracy: 0.228070 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 026 | Total loss: 3.106 | Reg loss: 0.029 | Tree loss: 3.106 | Accuracy: 0.132812 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 026 | Total loss: 3.135 | Reg loss: 0.029 | Tree loss: 3.135 | Accuracy: 0.167969 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 026 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.152344 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.128906 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 026 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.175781 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 026 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.136719 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.150391 | 0.858 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 026 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.160156 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.164062 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.123047 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.134766 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 026 | Total loss: 3.032 | Reg loss: 0.029 | Tree loss: 3.032 | Accuracy: 0.140625 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.140625 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 026 | Total loss: 2.972 | Reg loss: 0.029 | Tree loss: 2.972 | Accuracy: 0.169922 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 026 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 0.138672 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.146484 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.156250 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.156250 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 026 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.169922 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.148438 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.152344 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 026 | Total loss: 2.879 | Reg loss: 0.029 | Tree loss: 2.879 | Accuracy: 0.144531 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.146484 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.144531 | 0.857 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 026 | Total loss: 2.859 | Reg loss: 0.029 | Tree loss: 2.859 | Accuracy: 0.140351 | 0.857 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.181641 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.154297 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 002 / 026 | Total loss: 3.076 | Reg loss: 0.029 | Tree loss: 3.076 | Accuracy: 0.136719 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 026 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.121094 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.156250 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.152344 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 026 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.162109 | 0.857 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.144531 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.144531 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.144531 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.164062 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 026 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.158203 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.156250 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 026 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.156250 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 026 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.146484 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 026 | Total loss: 2.885 | Reg loss: 0.029 | Tree loss: 2.885 | Accuracy: 0.166016 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.152344 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 026 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.158203 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.150391 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 026 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.123047 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.146484 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 026 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.158203 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.111328 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 026 | Total loss: 2.898 | Reg loss: 0.029 | Tree loss: 2.898 | Accuracy: 0.158203 | 0.856 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 026 | Total loss: 3.146 | Reg loss: 0.029 | Tree loss: 3.146 | Accuracy: 0.087719 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 026 | Total loss: 3.102 | Reg loss: 0.029 | Tree loss: 3.102 | Accuracy: 0.132812 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 026 | Total loss: 3.053 | Reg loss: 0.029 | Tree loss: 3.053 | Accuracy: 0.177734 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.148438 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 026 | Total loss: 3.043 | Reg loss: 0.029 | Tree loss: 3.043 | Accuracy: 0.134766 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.125000 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 026 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.142578 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 026 | Total loss: 3.112 | Reg loss: 0.029 | Tree loss: 3.112 | Accuracy: 0.136719 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 026 | Total loss: 3.069 | Reg loss: 0.029 | Tree loss: 3.069 | Accuracy: 0.150391 | 0.856 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 026 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.146484 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 026 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.144531 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 026 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.148438 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 026 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.158203 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 026 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.185547 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 026 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.166016 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.142578 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.126953 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.136719 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 026 | Total loss: 2.977 | Reg loss: 0.029 | Tree loss: 2.977 | Accuracy: 0.175781 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 026 | Total loss: 2.968 | Reg loss: 0.029 | Tree loss: 2.968 | Accuracy: 0.134766 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 026 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.162109 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 026 | Total loss: 2.935 | Reg loss: 0.029 | Tree loss: 2.935 | Accuracy: 0.150391 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.142578 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.144531 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 026 | Total loss: 2.923 | Reg loss: 0.029 | Tree loss: 2.923 | Accuracy: 0.142578 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 026 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.175781 | 0.855 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.140351 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.146484 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 026 | Total loss: 3.089 | Reg loss: 0.029 | Tree loss: 3.089 | Accuracy: 0.154297 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.177734 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.134766 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 026 | Total loss: 3.063 | Reg loss: 0.029 | Tree loss: 3.063 | Accuracy: 0.164062 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.130859 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 026 | Total loss: 3.071 | Reg loss: 0.029 | Tree loss: 3.071 | Accuracy: 0.140625 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 026 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.156250 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 026 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.162109 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.162109 | 0.855 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.144531 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 026 | Total loss: 3.028 | Reg loss: 0.029 | Tree loss: 3.028 | Accuracy: 0.126953 | 0.854 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 012 / 026 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.128906 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 026 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.144531 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.142578 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.140625 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.146484 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.148438 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 026 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.140625 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 026 | Total loss: 2.927 | Reg loss: 0.029 | Tree loss: 2.927 | Accuracy: 0.148438 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 026 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.158203 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.142578 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.154297 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.158203 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 026 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.187500 | 0.854 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 026 | Total loss: 2.901 | Reg loss: 0.029 | Tree loss: 2.901 | Accuracy: 0.052632 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 026 | Total loss: 3.078 | Reg loss: 0.029 | Tree loss: 3.078 | Accuracy: 0.140625 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 026 | Total loss: 3.128 | Reg loss: 0.029 | Tree loss: 3.128 | Accuracy: 0.144531 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 026 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.156250 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 026 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.150391 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.134766 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 026 | Total loss: 2.987 | Reg loss: 0.029 | Tree loss: 2.987 | Accuracy: 0.187500 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.167969 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.177734 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.150391 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 026 | Total loss: 2.995 | Reg loss: 0.029 | Tree loss: 2.995 | Accuracy: 0.142578 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.107422 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 026 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.160156 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.171875 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 026 | Total loss: 2.983 | Reg loss: 0.029 | Tree loss: 2.983 | Accuracy: 0.132812 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 026 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.128906 | 0.854 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 026 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.136719 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 026 | Total loss: 2.914 | Reg loss: 0.029 | Tree loss: 2.914 | Accuracy: 0.164062 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.150391 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.160156 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 026 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.119141 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.152344 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 026 | Total loss: 2.864 | Reg loss: 0.029 | Tree loss: 2.864 | Accuracy: 0.154297 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 026 | Total loss: 2.895 | Reg loss: 0.029 | Tree loss: 2.895 | Accuracy: 0.142578 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 026 | Total loss: 2.895 | Reg loss: 0.029 | Tree loss: 2.895 | Accuracy: 0.125000 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.169922 | 0.853 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 026 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.175439 | 0.853 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 026 | Total loss: 3.134 | Reg loss: 0.029 | Tree loss: 3.134 | Accuracy: 0.148438 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 026 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.150391 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.152344 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 026 | Total loss: 3.063 | Reg loss: 0.029 | Tree loss: 3.063 | Accuracy: 0.132812 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 026 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.164062 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 026 | Total loss: 2.968 | Reg loss: 0.029 | Tree loss: 2.968 | Accuracy: 0.183594 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 026 | Total loss: 3.067 | Reg loss: 0.029 | Tree loss: 3.067 | Accuracy: 0.173828 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.136719 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.128906 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.160156 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.156250 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 026 | Total loss: 2.990 | Reg loss: 0.029 | Tree loss: 2.990 | Accuracy: 0.123047 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.138672 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.158203 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 026 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.144531 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.154297 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 026 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.158203 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 026 | Total loss: 2.990 | Reg loss: 0.029 | Tree loss: 2.990 | Accuracy: 0.171875 | 0.853 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.156250 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 026 | Total loss: 2.870 | Reg loss: 0.029 | Tree loss: 2.870 | Accuracy: 0.140625 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 026 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.148438 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.154297 | 0.852 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 022 / 026 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.121094 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 026 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 0.125000 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 026 | Total loss: 2.893 | Reg loss: 0.029 | Tree loss: 2.893 | Accuracy: 0.146484 | 0.852 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 026 | Total loss: 2.824 | Reg loss: 0.029 | Tree loss: 2.824 | Accuracy: 0.175439 | 0.852 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 026 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.128906 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.166016 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 026 | Total loss: 3.093 | Reg loss: 0.029 | Tree loss: 3.093 | Accuracy: 0.152344 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.154297 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 026 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.138672 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 026 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.166016 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.142578 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.150391 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.144531 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.164062 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 026 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.173828 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 026 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.130859 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.160156 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 026 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.125000 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 026 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.146484 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.134766 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 026 | Total loss: 2.977 | Reg loss: 0.029 | Tree loss: 2.977 | Accuracy: 0.130859 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 026 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.144531 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.146484 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 026 | Total loss: 2.891 | Reg loss: 0.029 | Tree loss: 2.891 | Accuracy: 0.134766 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.144531 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 026 | Total loss: 2.875 | Reg loss: 0.029 | Tree loss: 2.875 | Accuracy: 0.169922 | 0.852 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 026 | Total loss: 2.919 | Reg loss: 0.029 | Tree loss: 2.919 | Accuracy: 0.169922 | 0.851 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.160156 | 0.851 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.152344 | 0.851 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.140351 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 026 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.162109 | 0.852 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 026 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 0.125000 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 026 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.167969 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.142578 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 026 | Total loss: 3.068 | Reg loss: 0.029 | Tree loss: 3.068 | Accuracy: 0.167969 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 026 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.160156 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 026 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.154297 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 026 | Total loss: 3.084 | Reg loss: 0.029 | Tree loss: 3.084 | Accuracy: 0.148438 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 026 | Total loss: 2.990 | Reg loss: 0.029 | Tree loss: 2.990 | Accuracy: 0.140625 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.150391 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 026 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 0.142578 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.156250 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 026 | Total loss: 3.018 | Reg loss: 0.029 | Tree loss: 3.018 | Accuracy: 0.146484 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.158203 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.142578 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 026 | Total loss: 2.968 | Reg loss: 0.029 | Tree loss: 2.968 | Accuracy: 0.136719 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.154297 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.144531 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 026 | Total loss: 2.983 | Reg loss: 0.029 | Tree loss: 2.983 | Accuracy: 0.156250 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 026 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.126953 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 026 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.158203 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.148438 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.156250 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 026 | Total loss: 2.883 | Reg loss: 0.029 | Tree loss: 2.883 | Accuracy: 0.138672 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 026 | Total loss: 2.881 | Reg loss: 0.029 | Tree loss: 2.881 | Accuracy: 0.142578 | 0.851 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 026 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.175439 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 026 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.144531 | 0.851 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 026 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.148438 | 0.851 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 026 | Total loss: 2.982 | Reg loss: 0.029 | Tree loss: 2.982 | Accuracy: 0.177734 | 0.851 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.177734 | 0.851 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 004 / 026 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.171875 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 026 | Total loss: 3.000 | Reg loss: 0.029 | Tree loss: 3.000 | Accuracy: 0.142578 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 026 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.154297 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 026 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.150391 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 026 | Total loss: 3.015 | Reg loss: 0.029 | Tree loss: 3.015 | Accuracy: 0.156250 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 026 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.130859 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.154297 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.140625 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.148438 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 026 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.126953 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 026 | Total loss: 3.075 | Reg loss: 0.029 | Tree loss: 3.075 | Accuracy: 0.146484 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 026 | Total loss: 2.935 | Reg loss: 0.029 | Tree loss: 2.935 | Accuracy: 0.154297 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.130859 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 026 | Total loss: 2.907 | Reg loss: 0.029 | Tree loss: 2.907 | Accuracy: 0.154297 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 026 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.142578 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.138672 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.162109 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 026 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.150391 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.154297 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.142578 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.128906 | 0.85 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 026 | Total loss: 2.824 | Reg loss: 0.029 | Tree loss: 2.824 | Accuracy: 0.157895 | 0.85 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 026 | Total loss: 3.170 | Reg loss: 0.029 | Tree loss: 3.170 | Accuracy: 0.132812 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.148438 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 026 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.128906 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 026 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.173828 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.134766 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.152344 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 026 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.140625 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 026 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.160156 | 0.85 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.150391 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.146484 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 026 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.132812 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.177734 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.146484 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 026 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.142578 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 026 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.154297 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 026 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.162109 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.130859 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.162109 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 026 | Total loss: 2.882 | Reg loss: 0.029 | Tree loss: 2.882 | Accuracy: 0.171875 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.146484 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 026 | Total loss: 2.966 | Reg loss: 0.029 | Tree loss: 2.966 | Accuracy: 0.164062 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 026 | Total loss: 2.971 | Reg loss: 0.029 | Tree loss: 2.971 | Accuracy: 0.130859 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 026 | Total loss: 2.865 | Reg loss: 0.029 | Tree loss: 2.865 | Accuracy: 0.140625 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 026 | Total loss: 2.907 | Reg loss: 0.029 | Tree loss: 2.907 | Accuracy: 0.138672 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 026 | Total loss: 2.897 | Reg loss: 0.029 | Tree loss: 2.897 | Accuracy: 0.162109 | 0.849 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 026 | Total loss: 2.811 | Reg loss: 0.029 | Tree loss: 2.811 | Accuracy: 0.140351 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 026 | Total loss: 3.093 | Reg loss: 0.029 | Tree loss: 3.093 | Accuracy: 0.144531 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.136719 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.160156 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 026 | Total loss: 2.965 | Reg loss: 0.029 | Tree loss: 2.965 | Accuracy: 0.167969 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 026 | Total loss: 3.045 | Reg loss: 0.029 | Tree loss: 3.045 | Accuracy: 0.162109 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 026 | Total loss: 3.026 | Reg loss: 0.029 | Tree loss: 3.026 | Accuracy: 0.140625 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 026 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.146484 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 026 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 0.183594 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 026 | Total loss: 3.043 | Reg loss: 0.029 | Tree loss: 3.043 | Accuracy: 0.156250 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 026 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.136719 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.119141 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.150391 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.166016 | 0.849 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 026 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.115234 | 0.849 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 014 / 026 | Total loss: 2.883 | Reg loss: 0.029 | Tree loss: 2.883 | Accuracy: 0.158203 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.173828 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.099609 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 026 | Total loss: 2.905 | Reg loss: 0.029 | Tree loss: 2.905 | Accuracy: 0.148438 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 026 | Total loss: 2.920 | Reg loss: 0.029 | Tree loss: 2.920 | Accuracy: 0.156250 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 026 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.150391 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 026 | Total loss: 2.889 | Reg loss: 0.029 | Tree loss: 2.889 | Accuracy: 0.169922 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 026 | Total loss: 2.933 | Reg loss: 0.029 | Tree loss: 2.933 | Accuracy: 0.140625 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 026 | Total loss: 2.874 | Reg loss: 0.029 | Tree loss: 2.874 | Accuracy: 0.152344 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.134766 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 026 | Total loss: 2.844 | Reg loss: 0.029 | Tree loss: 2.844 | Accuracy: 0.158203 | 0.848 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 026 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 0.175439 | 0.848 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 026 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.144531 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 026 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.175781 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.140625 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.150391 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 026 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.142578 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 026 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.136719 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 026 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.169922 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.150391 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.136719 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 026 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.179688 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 026 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.152344 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.162109 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.162109 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 026 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.175781 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.140625 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 026 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.126953 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 026 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 0.134766 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 026 | Total loss: 2.890 | Reg loss: 0.029 | Tree loss: 2.890 | Accuracy: 0.150391 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 026 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.150391 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.138672 | 0.848 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 026 | Total loss: 2.880 | Reg loss: 0.029 | Tree loss: 2.880 | Accuracy: 0.134766 | 0.847 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.125000 | 0.847 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.160156 | 0.847 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 026 | Total loss: 2.910 | Reg loss: 0.029 | Tree loss: 2.910 | Accuracy: 0.136719 | 0.847 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 026 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.152344 | 0.847 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 026 | Total loss: 3.058 | Reg loss: 0.029 | Tree loss: 3.058 | Accuracy: 0.157895 | 0.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.146484 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.171875 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 026 | Total loss: 3.066 | Reg loss: 0.029 | Tree loss: 3.066 | Accuracy: 0.134766 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 026 | Total loss: 2.980 | Reg loss: 0.029 | Tree loss: 2.980 | Accuracy: 0.140625 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 026 | Total loss: 3.058 | Reg loss: 0.029 | Tree loss: 3.058 | Accuracy: 0.162109 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.121094 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.171875 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 026 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.152344 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 026 | Total loss: 2.925 | Reg loss: 0.029 | Tree loss: 2.925 | Accuracy: 0.187500 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.123047 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.166016 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 026 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.126953 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.169922 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 026 | Total loss: 2.957 | Reg loss: 0.029 | Tree loss: 2.957 | Accuracy: 0.154297 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 026 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.148438 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.128906 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 026 | Total loss: 2.958 | Reg loss: 0.029 | Tree loss: 2.958 | Accuracy: 0.138672 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.138672 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 026 | Total loss: 2.935 | Reg loss: 0.029 | Tree loss: 2.935 | Accuracy: 0.156250 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 026 | Total loss: 2.846 | Reg loss: 0.029 | Tree loss: 2.846 | Accuracy: 0.175781 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.136719 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.138672 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.128906 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.142578 | 0.847 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 024 / 026 | Total loss: 2.860 | Reg loss: 0.029 | Tree loss: 2.860 | Accuracy: 0.167969 | 0.847 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 026 | Total loss: 2.850 | Reg loss: 0.029 | Tree loss: 2.850 | Accuracy: 0.157895 | 0.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.150391 | 0.847 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 026 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.148438 | 0.847 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 026 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.167969 | 0.847 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.156250 | 0.847 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 026 | Total loss: 3.035 | Reg loss: 0.029 | Tree loss: 3.035 | Accuracy: 0.146484 | 0.847 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.138672 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 026 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.150391 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 026 | Total loss: 2.952 | Reg loss: 0.029 | Tree loss: 2.952 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 026 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.140625 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.164062 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.154297 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.156250 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 026 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.169922 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.134766 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 026 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.142578 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.128906 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 026 | Total loss: 2.875 | Reg loss: 0.029 | Tree loss: 2.875 | Accuracy: 0.164062 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 026 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.113281 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.126953 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 026 | Total loss: 2.892 | Reg loss: 0.029 | Tree loss: 2.892 | Accuracy: 0.150391 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.175781 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.150391 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 026 | Total loss: 2.889 | Reg loss: 0.029 | Tree loss: 2.889 | Accuracy: 0.134766 | 0.846 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 026 | Total loss: 3.018 | Reg loss: 0.029 | Tree loss: 3.018 | Accuracy: 0.122807 | 0.846 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 026 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.183594 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.150391 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 026 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.183594 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.134766 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 026 | Total loss: 3.052 | Reg loss: 0.029 | Tree loss: 3.052 | Accuracy: 0.144531 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 026 | Total loss: 3.035 | Reg loss: 0.029 | Tree loss: 3.035 | Accuracy: 0.146484 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 026 | Total loss: 3.067 | Reg loss: 0.029 | Tree loss: 3.067 | Accuracy: 0.125000 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 026 | Total loss: 3.032 | Reg loss: 0.029 | Tree loss: 3.032 | Accuracy: 0.130859 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.142578 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.171875 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 026 | Total loss: 2.972 | Reg loss: 0.029 | Tree loss: 2.972 | Accuracy: 0.148438 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 026 | Total loss: 3.004 | Reg loss: 0.029 | Tree loss: 3.004 | Accuracy: 0.132812 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.136719 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.166016 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.146484 | 0.846 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.130859 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 026 | Total loss: 2.907 | Reg loss: 0.029 | Tree loss: 2.907 | Accuracy: 0.138672 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.173828 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 026 | Total loss: 2.861 | Reg loss: 0.029 | Tree loss: 2.861 | Accuracy: 0.169922 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 026 | Total loss: 2.868 | Reg loss: 0.029 | Tree loss: 2.868 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 026 | Total loss: 2.870 | Reg loss: 0.029 | Tree loss: 2.870 | Accuracy: 0.167969 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.123047 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.130859 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 026 | Total loss: 2.862 | Reg loss: 0.029 | Tree loss: 2.862 | Accuracy: 0.162109 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 026 | Total loss: 2.878 | Reg loss: 0.029 | Tree loss: 2.878 | Accuracy: 0.142578 | 0.845 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 026 | Total loss: 2.883 | Reg loss: 0.029 | Tree loss: 2.883 | Accuracy: 0.175439 | 0.845 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 026 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 026 | Total loss: 3.132 | Reg loss: 0.029 | Tree loss: 3.132 | Accuracy: 0.142578 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.156250 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 026 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 026 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 026 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.158203 | 0.845 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 006 / 026 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.144531 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.142578 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 026 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.130859 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.130859 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 026 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.146484 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.134766 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 026 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.154297 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 026 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 0.146484 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 026 | Total loss: 2.895 | Reg loss: 0.029 | Tree loss: 2.895 | Accuracy: 0.181641 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.148438 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 026 | Total loss: 2.858 | Reg loss: 0.029 | Tree loss: 2.858 | Accuracy: 0.177734 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 026 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 026 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.140625 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 026 | Total loss: 2.855 | Reg loss: 0.029 | Tree loss: 2.855 | Accuracy: 0.154297 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.158203 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 026 | Total loss: 2.820 | Reg loss: 0.029 | Tree loss: 2.820 | Accuracy: 0.160156 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 026 | Total loss: 2.871 | Reg loss: 0.029 | Tree loss: 2.871 | Accuracy: 0.134766 | 0.845 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 026 | Total loss: 2.869 | Reg loss: 0.029 | Tree loss: 2.869 | Accuracy: 0.122807 | 0.845 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 026 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.152344 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 026 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.158203 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 026 | Total loss: 3.094 | Reg loss: 0.029 | Tree loss: 3.094 | Accuracy: 0.134766 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 026 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 026 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.171875 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.150391 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 026 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.140625 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 026 | Total loss: 3.016 | Reg loss: 0.029 | Tree loss: 3.016 | Accuracy: 0.152344 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 026 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.132812 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 026 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.162109 | 0.845 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 026 | Total loss: 2.987 | Reg loss: 0.029 | Tree loss: 2.987 | Accuracy: 0.138672 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 026 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.150391 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 026 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.140625 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 026 | Total loss: 2.965 | Reg loss: 0.029 | Tree loss: 2.965 | Accuracy: 0.150391 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.162109 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 026 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.134766 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 026 | Total loss: 2.966 | Reg loss: 0.029 | Tree loss: 2.966 | Accuracy: 0.148438 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 026 | Total loss: 2.894 | Reg loss: 0.029 | Tree loss: 2.894 | Accuracy: 0.156250 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 026 | Total loss: 2.896 | Reg loss: 0.029 | Tree loss: 2.896 | Accuracy: 0.126953 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 026 | Total loss: 2.887 | Reg loss: 0.029 | Tree loss: 2.887 | Accuracy: 0.158203 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.148438 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 026 | Total loss: 2.846 | Reg loss: 0.029 | Tree loss: 2.846 | Accuracy: 0.150391 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 026 | Total loss: 2.919 | Reg loss: 0.029 | Tree loss: 2.919 | Accuracy: 0.152344 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.162109 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 026 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.140625 | 0.844 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 026 | Total loss: 2.835 | Reg loss: 0.029 | Tree loss: 2.835 | Accuracy: 0.192982 | 0.844 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 026 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.111328 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 026 | Total loss: 3.066 | Reg loss: 0.029 | Tree loss: 3.066 | Accuracy: 0.152344 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 026 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.132812 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.119141 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 026 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.136719 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.156250 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.162109 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.175781 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 026 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.146484 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 026 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.171875 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.148438 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 026 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 0.150391 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 026 | Total loss: 2.876 | Reg loss: 0.029 | Tree loss: 2.876 | Accuracy: 0.169922 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 026 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.144531 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.148438 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.136719 | 0.844 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 016 / 026 | Total loss: 2.928 | Reg loss: 0.029 | Tree loss: 2.928 | Accuracy: 0.136719 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 026 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 0.154297 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 026 | Total loss: 2.887 | Reg loss: 0.029 | Tree loss: 2.887 | Accuracy: 0.177734 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 026 | Total loss: 2.840 | Reg loss: 0.029 | Tree loss: 2.840 | Accuracy: 0.175781 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 026 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.134766 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 026 | Total loss: 2.875 | Reg loss: 0.029 | Tree loss: 2.875 | Accuracy: 0.185547 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 026 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.140625 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 026 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.115234 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 026 | Total loss: 2.879 | Reg loss: 0.029 | Tree loss: 2.879 | Accuracy: 0.138672 | 0.844 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 026 | Total loss: 2.851 | Reg loss: 0.029 | Tree loss: 2.851 | Accuracy: 0.228070 | 0.844 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 026 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.130859 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 026 | Total loss: 3.073 | Reg loss: 0.029 | Tree loss: 3.073 | Accuracy: 0.125000 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.158203 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.177734 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 026 | Total loss: 3.035 | Reg loss: 0.029 | Tree loss: 3.035 | Accuracy: 0.146484 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 026 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.152344 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 026 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.123047 | 0.844 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.150391 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.117188 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 026 | Total loss: 2.906 | Reg loss: 0.029 | Tree loss: 2.906 | Accuracy: 0.181641 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 026 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.164062 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 026 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.146484 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 026 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.154297 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 026 | Total loss: 2.972 | Reg loss: 0.029 | Tree loss: 2.972 | Accuracy: 0.119141 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.144531 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 026 | Total loss: 2.921 | Reg loss: 0.029 | Tree loss: 2.921 | Accuracy: 0.181641 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 026 | Total loss: 2.891 | Reg loss: 0.029 | Tree loss: 2.891 | Accuracy: 0.173828 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.142578 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.132812 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.154297 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 026 | Total loss: 2.923 | Reg loss: 0.029 | Tree loss: 2.923 | Accuracy: 0.166016 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 026 | Total loss: 2.874 | Reg loss: 0.029 | Tree loss: 2.874 | Accuracy: 0.128906 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 026 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.154297 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 026 | Total loss: 2.893 | Reg loss: 0.029 | Tree loss: 2.893 | Accuracy: 0.173828 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.136719 | 0.843 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 026 | Total loss: 2.760 | Reg loss: 0.029 | Tree loss: 2.760 | Accuracy: 0.105263 | 0.843 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 026 | Total loss: 3.041 | Reg loss: 0.028 | Tree loss: 3.041 | Accuracy: 0.173828 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 026 | Total loss: 3.009 | Reg loss: 0.028 | Tree loss: 3.009 | Accuracy: 0.171875 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 026 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.138672 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 026 | Total loss: 3.058 | Reg loss: 0.028 | Tree loss: 3.058 | Accuracy: 0.130859 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 026 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.158203 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.158203 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 026 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.171875 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 026 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.117188 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.160156 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 026 | Total loss: 3.000 | Reg loss: 0.029 | Tree loss: 3.000 | Accuracy: 0.138672 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.130859 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 026 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.121094 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 026 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.138672 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 026 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.144531 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.123047 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 026 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.138672 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 026 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.158203 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 026 | Total loss: 2.901 | Reg loss: 0.029 | Tree loss: 2.901 | Accuracy: 0.142578 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 026 | Total loss: 2.846 | Reg loss: 0.029 | Tree loss: 2.846 | Accuracy: 0.187500 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 026 | Total loss: 2.885 | Reg loss: 0.029 | Tree loss: 2.885 | Accuracy: 0.158203 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 026 | Total loss: 2.864 | Reg loss: 0.029 | Tree loss: 2.864 | Accuracy: 0.146484 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.144531 | 0.843 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 026 | Total loss: 2.862 | Reg loss: 0.029 | Tree loss: 2.862 | Accuracy: 0.138672 | 0.842 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 026 | Total loss: 2.885 | Reg loss: 0.029 | Tree loss: 2.885 | Accuracy: 0.158203 | 0.842 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 026 | Total loss: 2.897 | Reg loss: 0.029 | Tree loss: 2.897 | Accuracy: 0.177734 | 0.842 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 026 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.175439 | 0.842 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 026 | Total loss: 3.022 | Reg loss: 0.028 | Tree loss: 3.022 | Accuracy: 0.138672 | 0.843 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 026 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.146484 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 026 | Total loss: 3.011 | Reg loss: 0.028 | Tree loss: 3.011 | Accuracy: 0.115234 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 026 | Total loss: 2.959 | Reg loss: 0.028 | Tree loss: 2.959 | Accuracy: 0.185547 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 026 | Total loss: 3.092 | Reg loss: 0.028 | Tree loss: 3.092 | Accuracy: 0.121094 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 026 | Total loss: 3.053 | Reg loss: 0.028 | Tree loss: 3.053 | Accuracy: 0.148438 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.128906 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 026 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.150391 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 026 | Total loss: 2.954 | Reg loss: 0.029 | Tree loss: 2.954 | Accuracy: 0.142578 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.130859 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.152344 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.126953 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 026 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.166016 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 026 | Total loss: 2.910 | Reg loss: 0.029 | Tree loss: 2.910 | Accuracy: 0.158203 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 026 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.169922 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 026 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.156250 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 026 | Total loss: 2.927 | Reg loss: 0.029 | Tree loss: 2.927 | Accuracy: 0.144531 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.130859 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 026 | Total loss: 2.906 | Reg loss: 0.029 | Tree loss: 2.906 | Accuracy: 0.183594 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 026 | Total loss: 2.905 | Reg loss: 0.029 | Tree loss: 2.905 | Accuracy: 0.128906 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 026 | Total loss: 2.852 | Reg loss: 0.029 | Tree loss: 2.852 | Accuracy: 0.164062 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 026 | Total loss: 2.887 | Reg loss: 0.029 | Tree loss: 2.887 | Accuracy: 0.152344 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 026 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.128906 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 026 | Total loss: 2.871 | Reg loss: 0.029 | Tree loss: 2.871 | Accuracy: 0.169922 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 026 | Total loss: 2.838 | Reg loss: 0.029 | Tree loss: 2.838 | Accuracy: 0.185547 | 0.842 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 026 | Total loss: 2.920 | Reg loss: 0.029 | Tree loss: 2.920 | Accuracy: 0.192982 | 0.842 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 026 | Total loss: 3.091 | Reg loss: 0.028 | Tree loss: 3.091 | Accuracy: 0.119141 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 026 | Total loss: 3.080 | Reg loss: 0.028 | Tree loss: 3.080 | Accuracy: 0.115234 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 026 | Total loss: 3.035 | Reg loss: 0.028 | Tree loss: 3.035 | Accuracy: 0.171875 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 026 | Total loss: 2.992 | Reg loss: 0.028 | Tree loss: 2.992 | Accuracy: 0.146484 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 026 | Total loss: 3.056 | Reg loss: 0.028 | Tree loss: 3.056 | Accuracy: 0.144531 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 026 | Total loss: 2.983 | Reg loss: 0.028 | Tree loss: 2.983 | Accuracy: 0.156250 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 026 | Total loss: 2.931 | Reg loss: 0.028 | Tree loss: 2.931 | Accuracy: 0.160156 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 026 | Total loss: 2.958 | Reg loss: 0.029 | Tree loss: 2.958 | Accuracy: 0.160156 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.146484 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.156250 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 026 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.140625 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 026 | Total loss: 2.910 | Reg loss: 0.029 | Tree loss: 2.910 | Accuracy: 0.175781 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 026 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.132812 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 026 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.148438 | 0.842 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 026 | Total loss: 2.921 | Reg loss: 0.029 | Tree loss: 2.921 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 026 | Total loss: 2.909 | Reg loss: 0.029 | Tree loss: 2.909 | Accuracy: 0.146484 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.167969 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.166016 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 026 | Total loss: 2.899 | Reg loss: 0.029 | Tree loss: 2.899 | Accuracy: 0.140625 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 026 | Total loss: 2.902 | Reg loss: 0.029 | Tree loss: 2.902 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 026 | Total loss: 2.928 | Reg loss: 0.029 | Tree loss: 2.928 | Accuracy: 0.154297 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 026 | Total loss: 2.852 | Reg loss: 0.029 | Tree loss: 2.852 | Accuracy: 0.154297 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 026 | Total loss: 2.922 | Reg loss: 0.029 | Tree loss: 2.922 | Accuracy: 0.150391 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 026 | Total loss: 2.878 | Reg loss: 0.029 | Tree loss: 2.878 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.136719 | 0.841 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 026 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.140351 | 0.841 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 026 | Total loss: 3.029 | Reg loss: 0.028 | Tree loss: 3.029 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 026 | Total loss: 3.034 | Reg loss: 0.028 | Tree loss: 3.034 | Accuracy: 0.171875 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 026 | Total loss: 3.041 | Reg loss: 0.028 | Tree loss: 3.041 | Accuracy: 0.138672 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 026 | Total loss: 3.023 | Reg loss: 0.028 | Tree loss: 3.023 | Accuracy: 0.152344 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 026 | Total loss: 3.015 | Reg loss: 0.028 | Tree loss: 3.015 | Accuracy: 0.134766 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 026 | Total loss: 3.002 | Reg loss: 0.028 | Tree loss: 3.002 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 026 | Total loss: 3.058 | Reg loss: 0.028 | Tree loss: 3.058 | Accuracy: 0.136719 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 026 | Total loss: 2.948 | Reg loss: 0.028 | Tree loss: 2.948 | Accuracy: 0.146484 | 0.841 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 008 / 026 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.136719 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 026 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.146484 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 026 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.134766 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 026 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.128906 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 026 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.136719 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 026 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.146484 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 026 | Total loss: 2.860 | Reg loss: 0.029 | Tree loss: 2.860 | Accuracy: 0.173828 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 026 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.154297 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.150391 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.162109 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 026 | Total loss: 2.865 | Reg loss: 0.029 | Tree loss: 2.865 | Accuracy: 0.154297 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 026 | Total loss: 2.860 | Reg loss: 0.029 | Tree loss: 2.860 | Accuracy: 0.183594 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 026 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.142578 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 026 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.130859 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 026 | Total loss: 2.858 | Reg loss: 0.029 | Tree loss: 2.858 | Accuracy: 0.160156 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 026 | Total loss: 2.894 | Reg loss: 0.029 | Tree loss: 2.894 | Accuracy: 0.152344 | 0.841 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 026 | Total loss: 2.756 | Reg loss: 0.029 | Tree loss: 2.756 | Accuracy: 0.157895 | 0.841 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 026 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.173828 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 026 | Total loss: 3.044 | Reg loss: 0.028 | Tree loss: 3.044 | Accuracy: 0.125000 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 026 | Total loss: 3.086 | Reg loss: 0.028 | Tree loss: 3.086 | Accuracy: 0.132812 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 026 | Total loss: 2.998 | Reg loss: 0.028 | Tree loss: 2.998 | Accuracy: 0.164062 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 026 | Total loss: 3.053 | Reg loss: 0.028 | Tree loss: 3.053 | Accuracy: 0.158203 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 026 | Total loss: 3.031 | Reg loss: 0.028 | Tree loss: 3.031 | Accuracy: 0.148438 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 026 | Total loss: 2.972 | Reg loss: 0.028 | Tree loss: 2.972 | Accuracy: 0.175781 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 026 | Total loss: 2.973 | Reg loss: 0.028 | Tree loss: 2.973 | Accuracy: 0.144531 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 026 | Total loss: 2.990 | Reg loss: 0.028 | Tree loss: 2.990 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 026 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.128906 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.175781 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.142578 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 026 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.156250 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 026 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.160156 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 026 | Total loss: 2.885 | Reg loss: 0.029 | Tree loss: 2.885 | Accuracy: 0.144531 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 026 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.128906 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 026 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 0.115234 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 026 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.113281 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 026 | Total loss: 2.857 | Reg loss: 0.029 | Tree loss: 2.857 | Accuracy: 0.169922 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 026 | Total loss: 2.844 | Reg loss: 0.029 | Tree loss: 2.844 | Accuracy: 0.150391 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 026 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.121094 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 026 | Total loss: 2.875 | Reg loss: 0.029 | Tree loss: 2.875 | Accuracy: 0.177734 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 026 | Total loss: 2.869 | Reg loss: 0.029 | Tree loss: 2.869 | Accuracy: 0.154297 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 026 | Total loss: 2.847 | Reg loss: 0.029 | Tree loss: 2.847 | Accuracy: 0.177734 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 026 | Total loss: 2.918 | Reg loss: 0.029 | Tree loss: 2.918 | Accuracy: 0.142578 | 0.84 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 026 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.087719 | 0.84 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 026 | Total loss: 3.099 | Reg loss: 0.028 | Tree loss: 3.099 | Accuracy: 0.160156 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 026 | Total loss: 3.037 | Reg loss: 0.028 | Tree loss: 3.037 | Accuracy: 0.130859 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 026 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.197266 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 026 | Total loss: 3.000 | Reg loss: 0.028 | Tree loss: 3.000 | Accuracy: 0.134766 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 026 | Total loss: 3.002 | Reg loss: 0.028 | Tree loss: 3.002 | Accuracy: 0.154297 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 026 | Total loss: 2.965 | Reg loss: 0.028 | Tree loss: 2.965 | Accuracy: 0.158203 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 026 | Total loss: 3.055 | Reg loss: 0.028 | Tree loss: 3.055 | Accuracy: 0.152344 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 026 | Total loss: 2.978 | Reg loss: 0.028 | Tree loss: 2.978 | Accuracy: 0.154297 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 026 | Total loss: 2.980 | Reg loss: 0.028 | Tree loss: 2.980 | Accuracy: 0.152344 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 026 | Total loss: 2.917 | Reg loss: 0.028 | Tree loss: 2.917 | Accuracy: 0.156250 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 026 | Total loss: 2.907 | Reg loss: 0.029 | Tree loss: 2.907 | Accuracy: 0.156250 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 026 | Total loss: 2.906 | Reg loss: 0.029 | Tree loss: 2.906 | Accuracy: 0.181641 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 026 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.121094 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 026 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.142578 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 026 | Total loss: 2.892 | Reg loss: 0.029 | Tree loss: 2.892 | Accuracy: 0.152344 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 026 | Total loss: 2.966 | Reg loss: 0.029 | Tree loss: 2.966 | Accuracy: 0.142578 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 026 | Total loss: 2.914 | Reg loss: 0.029 | Tree loss: 2.914 | Accuracy: 0.140625 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.121094 | 0.84 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 018 / 026 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.152344 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 026 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.142578 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 026 | Total loss: 2.893 | Reg loss: 0.029 | Tree loss: 2.893 | Accuracy: 0.152344 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 026 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.132812 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 026 | Total loss: 2.890 | Reg loss: 0.029 | Tree loss: 2.890 | Accuracy: 0.138672 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 026 | Total loss: 2.899 | Reg loss: 0.029 | Tree loss: 2.899 | Accuracy: 0.158203 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 026 | Total loss: 2.859 | Reg loss: 0.029 | Tree loss: 2.859 | Accuracy: 0.144531 | 0.84 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 026 | Total loss: 2.801 | Reg loss: 0.029 | Tree loss: 2.801 | Accuracy: 0.157895 | 0.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 026 | Total loss: 3.017 | Reg loss: 0.028 | Tree loss: 3.017 | Accuracy: 0.164062 | 0.84 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 026 | Total loss: 3.049 | Reg loss: 0.028 | Tree loss: 3.049 | Accuracy: 0.144531 | 0.84 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 026 | Total loss: 3.087 | Reg loss: 0.028 | Tree loss: 3.087 | Accuracy: 0.146484 | 0.84 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 026 | Total loss: 2.996 | Reg loss: 0.028 | Tree loss: 2.996 | Accuracy: 0.162109 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 026 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 026 | Total loss: 2.987 | Reg loss: 0.028 | Tree loss: 2.987 | Accuracy: 0.158203 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 026 | Total loss: 3.002 | Reg loss: 0.028 | Tree loss: 3.002 | Accuracy: 0.128906 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 026 | Total loss: 2.997 | Reg loss: 0.028 | Tree loss: 2.997 | Accuracy: 0.177734 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 026 | Total loss: 2.934 | Reg loss: 0.028 | Tree loss: 2.934 | Accuracy: 0.164062 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 026 | Total loss: 3.038 | Reg loss: 0.028 | Tree loss: 3.038 | Accuracy: 0.134766 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 026 | Total loss: 2.888 | Reg loss: 0.028 | Tree loss: 2.888 | Accuracy: 0.158203 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 026 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.152344 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 026 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.158203 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 026 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.152344 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 026 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.132812 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 026 | Total loss: 2.876 | Reg loss: 0.029 | Tree loss: 2.876 | Accuracy: 0.166016 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 026 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.156250 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.144531 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 026 | Total loss: 2.894 | Reg loss: 0.029 | Tree loss: 2.894 | Accuracy: 0.148438 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.171875 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 026 | Total loss: 2.900 | Reg loss: 0.029 | Tree loss: 2.900 | Accuracy: 0.115234 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 026 | Total loss: 2.830 | Reg loss: 0.029 | Tree loss: 2.830 | Accuracy: 0.166016 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 026 | Total loss: 2.854 | Reg loss: 0.029 | Tree loss: 2.854 | Accuracy: 0.158203 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 026 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.111328 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 026 | Total loss: 2.909 | Reg loss: 0.029 | Tree loss: 2.909 | Accuracy: 0.113281 | 0.839 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 026 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.070175 | 0.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 026 | Total loss: 3.058 | Reg loss: 0.028 | Tree loss: 3.058 | Accuracy: 0.117188 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 026 | Total loss: 3.041 | Reg loss: 0.028 | Tree loss: 3.041 | Accuracy: 0.152344 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 026 | Total loss: 3.006 | Reg loss: 0.028 | Tree loss: 3.006 | Accuracy: 0.173828 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 026 | Total loss: 2.956 | Reg loss: 0.028 | Tree loss: 2.956 | Accuracy: 0.162109 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 026 | Total loss: 3.004 | Reg loss: 0.028 | Tree loss: 3.004 | Accuracy: 0.144531 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 026 | Total loss: 3.018 | Reg loss: 0.028 | Tree loss: 3.018 | Accuracy: 0.146484 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 026 | Total loss: 2.953 | Reg loss: 0.028 | Tree loss: 2.953 | Accuracy: 0.164062 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 026 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.136719 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 026 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.140625 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 026 | Total loss: 2.969 | Reg loss: 0.028 | Tree loss: 2.969 | Accuracy: 0.140625 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 026 | Total loss: 2.953 | Reg loss: 0.028 | Tree loss: 2.953 | Accuracy: 0.160156 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 026 | Total loss: 2.992 | Reg loss: 0.028 | Tree loss: 2.992 | Accuracy: 0.152344 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 026 | Total loss: 2.895 | Reg loss: 0.029 | Tree loss: 2.895 | Accuracy: 0.169922 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 026 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.136719 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 026 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.171875 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 026 | Total loss: 2.879 | Reg loss: 0.029 | Tree loss: 2.879 | Accuracy: 0.158203 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 026 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.142578 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 026 | Total loss: 2.840 | Reg loss: 0.029 | Tree loss: 2.840 | Accuracy: 0.146484 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 026 | Total loss: 2.895 | Reg loss: 0.029 | Tree loss: 2.895 | Accuracy: 0.150391 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 026 | Total loss: 2.867 | Reg loss: 0.029 | Tree loss: 2.867 | Accuracy: 0.169922 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 026 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 0.134766 | 0.839 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 026 | Total loss: 2.934 | Reg loss: 0.029 | Tree loss: 2.934 | Accuracy: 0.125000 | 0.838 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 026 | Total loss: 2.916 | Reg loss: 0.029 | Tree loss: 2.916 | Accuracy: 0.138672 | 0.838 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 026 | Total loss: 2.899 | Reg loss: 0.029 | Tree loss: 2.899 | Accuracy: 0.158203 | 0.838 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 026 | Total loss: 2.900 | Reg loss: 0.029 | Tree loss: 2.900 | Accuracy: 0.134766 | 0.838 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 026 | Total loss: 3.034 | Reg loss: 0.029 | Tree loss: 3.034 | Accuracy: 0.175439 | 0.838 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 000 / 026 | Total loss: 2.998 | Reg loss: 0.028 | Tree loss: 2.998 | Accuracy: 0.154297 | 0.839 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 026 | Total loss: 3.049 | Reg loss: 0.028 | Tree loss: 3.049 | Accuracy: 0.138672 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 026 | Total loss: 3.094 | Reg loss: 0.028 | Tree loss: 3.094 | Accuracy: 0.152344 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 026 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 0.134766 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 026 | Total loss: 3.029 | Reg loss: 0.028 | Tree loss: 3.029 | Accuracy: 0.119141 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 026 | Total loss: 2.975 | Reg loss: 0.028 | Tree loss: 2.975 | Accuracy: 0.162109 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 026 | Total loss: 2.997 | Reg loss: 0.028 | Tree loss: 2.997 | Accuracy: 0.144531 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 026 | Total loss: 2.950 | Reg loss: 0.028 | Tree loss: 2.950 | Accuracy: 0.166016 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 026 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.158203 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 026 | Total loss: 2.968 | Reg loss: 0.028 | Tree loss: 2.968 | Accuracy: 0.134766 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 026 | Total loss: 2.984 | Reg loss: 0.028 | Tree loss: 2.984 | Accuracy: 0.123047 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 026 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.134766 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 026 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.146484 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 026 | Total loss: 2.874 | Reg loss: 0.029 | Tree loss: 2.874 | Accuracy: 0.162109 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 026 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.144531 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 026 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.171875 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 026 | Total loss: 2.839 | Reg loss: 0.029 | Tree loss: 2.839 | Accuracy: 0.179688 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 026 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.138672 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 026 | Total loss: 2.845 | Reg loss: 0.029 | Tree loss: 2.845 | Accuracy: 0.158203 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 026 | Total loss: 2.878 | Reg loss: 0.029 | Tree loss: 2.878 | Accuracy: 0.140625 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 026 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.136719 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 026 | Total loss: 2.886 | Reg loss: 0.029 | Tree loss: 2.886 | Accuracy: 0.146484 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 026 | Total loss: 2.805 | Reg loss: 0.029 | Tree loss: 2.805 | Accuracy: 0.185547 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 026 | Total loss: 2.865 | Reg loss: 0.029 | Tree loss: 2.865 | Accuracy: 0.154297 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 026 | Total loss: 2.882 | Reg loss: 0.029 | Tree loss: 2.882 | Accuracy: 0.148438 | 0.838 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 026 | Total loss: 2.761 | Reg loss: 0.029 | Tree loss: 2.761 | Accuracy: 0.105263 | 0.838 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654f733f3ebf4fc89b01c72305e70864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376fb95b9f60432785f46a4f16119d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22f3febb4b84767a7f19c0cfc025892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8e06b88a1045d1bdc95c9db6b92cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 9.948453608247423\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 970\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "9675\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "3182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "Average comprehensibility: 47.45154639175258\n",
      "std comprehensibility: 3.1044070238126458\n",
      "var comprehensibility: 9.63734296949729\n",
      "minimum comprehensibility: 36\n",
      "maximum comprehensibility: 56\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
