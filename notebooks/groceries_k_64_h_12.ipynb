{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.212848663330078 | KNN Loss: 6.229985237121582 | BCE Loss: 1.9828636646270752\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.168519973754883 | KNN Loss: 6.229908466339111 | BCE Loss: 1.9386110305786133\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.14419937133789 | KNN Loss: 6.229614734649658 | BCE Loss: 1.9145846366882324\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.14875316619873 | KNN Loss: 6.229379653930664 | BCE Loss: 1.919373631477356\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.138741493225098 | KNN Loss: 6.229327201843262 | BCE Loss: 1.9094146490097046\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.163969039916992 | KNN Loss: 6.229116439819336 | BCE Loss: 1.9348526000976562\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.142416000366211 | KNN Loss: 6.228728771209717 | BCE Loss: 1.9136877059936523\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.133009910583496 | KNN Loss: 6.228733062744141 | BCE Loss: 1.9042772054672241\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.090954780578613 | KNN Loss: 6.228724479675293 | BCE Loss: 1.8622300624847412\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.116913795471191 | KNN Loss: 6.228275299072266 | BCE Loss: 1.8886384963989258\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.108771324157715 | KNN Loss: 6.228553295135498 | BCE Loss: 1.8802183866500854\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.032567977905273 | KNN Loss: 6.227932929992676 | BCE Loss: 1.804634928703308\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.139668464660645 | KNN Loss: 6.227555751800537 | BCE Loss: 1.9121127128601074\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.074206352233887 | KNN Loss: 6.2271728515625 | BCE Loss: 1.8470335006713867\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.07069206237793 | KNN Loss: 6.227368354797363 | BCE Loss: 1.8433237075805664\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.056090354919434 | KNN Loss: 6.226765155792236 | BCE Loss: 1.829325556755066\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.031925201416016 | KNN Loss: 6.226944446563721 | BCE Loss: 1.804980993270874\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.037485122680664 | KNN Loss: 6.226507663726807 | BCE Loss: 1.8109769821166992\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.068265914916992 | KNN Loss: 6.225895881652832 | BCE Loss: 1.8423705101013184\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 7.982265949249268 | KNN Loss: 6.226096153259277 | BCE Loss: 1.7561697959899902\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.006495475769043 | KNN Loss: 6.226150989532471 | BCE Loss: 1.7803444862365723\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 7.997621536254883 | KNN Loss: 6.225345611572266 | BCE Loss: 1.772275686264038\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.967384338378906 | KNN Loss: 6.224903106689453 | BCE Loss: 1.7424814701080322\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.015817642211914 | KNN Loss: 6.224453926086426 | BCE Loss: 1.7913637161254883\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.971815586090088 | KNN Loss: 6.224050045013428 | BCE Loss: 1.7477655410766602\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.928751468658447 | KNN Loss: 6.223540782928467 | BCE Loss: 1.7052106857299805\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.927845001220703 | KNN Loss: 6.223506450653076 | BCE Loss: 1.704338550567627\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.897754192352295 | KNN Loss: 6.222952842712402 | BCE Loss: 1.674801230430603\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.932838439941406 | KNN Loss: 6.222580432891846 | BCE Loss: 1.7102577686309814\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.881655216217041 | KNN Loss: 6.222578048706055 | BCE Loss: 1.6590770483016968\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.913565158843994 | KNN Loss: 6.221623420715332 | BCE Loss: 1.6919416189193726\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.884415626525879 | KNN Loss: 6.2213969230651855 | BCE Loss: 1.6630187034606934\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.848147392272949 | KNN Loss: 6.220667839050293 | BCE Loss: 1.6274795532226562\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.846590042114258 | KNN Loss: 6.220032215118408 | BCE Loss: 1.6265580654144287\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.847685813903809 | KNN Loss: 6.219126224517822 | BCE Loss: 1.6285593509674072\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.788485527038574 | KNN Loss: 6.219078063964844 | BCE Loss: 1.5694074630737305\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.835504055023193 | KNN Loss: 6.217228889465332 | BCE Loss: 1.6182752847671509\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.7940673828125 | KNN Loss: 6.216682434082031 | BCE Loss: 1.5773847103118896\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.762846946716309 | KNN Loss: 6.216444969177246 | BCE Loss: 1.5464019775390625\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.739413261413574 | KNN Loss: 6.214718818664551 | BCE Loss: 1.5246946811676025\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.73658561706543 | KNN Loss: 6.212091445922852 | BCE Loss: 1.5244944095611572\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.710622310638428 | KNN Loss: 6.211867809295654 | BCE Loss: 1.498754620552063\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.720384120941162 | KNN Loss: 6.211704254150391 | BCE Loss: 1.508679986000061\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.684443473815918 | KNN Loss: 6.211142063140869 | BCE Loss: 1.473301649093628\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.67216682434082 | KNN Loss: 6.206372261047363 | BCE Loss: 1.4657946825027466\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.614360332489014 | KNN Loss: 6.205354690551758 | BCE Loss: 1.4090055227279663\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.593764305114746 | KNN Loss: 6.203633785247803 | BCE Loss: 1.3901304006576538\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.572123050689697 | KNN Loss: 6.200310230255127 | BCE Loss: 1.3718128204345703\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.560102462768555 | KNN Loss: 6.19742488861084 | BCE Loss: 1.362677812576294\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.5059285163879395 | KNN Loss: 6.1952033042907715 | BCE Loss: 1.310725212097168\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.526675701141357 | KNN Loss: 6.192440986633301 | BCE Loss: 1.334234595298767\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.499863147735596 | KNN Loss: 6.189671993255615 | BCE Loss: 1.310191035270691\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.434103965759277 | KNN Loss: 6.184381484985352 | BCE Loss: 1.2497227191925049\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.458679676055908 | KNN Loss: 6.180112361907959 | BCE Loss: 1.2785673141479492\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.405264377593994 | KNN Loss: 6.1696457862854 | BCE Loss: 1.2356187105178833\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.4174580574035645 | KNN Loss: 6.170203685760498 | BCE Loss: 1.2472543716430664\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.349226951599121 | KNN Loss: 6.157938003540039 | BCE Loss: 1.1912891864776611\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.359609127044678 | KNN Loss: 6.149692535400391 | BCE Loss: 1.2099164724349976\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.323939800262451 | KNN Loss: 6.133922100067139 | BCE Loss: 1.190017819404602\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.296648979187012 | KNN Loss: 6.125064373016357 | BCE Loss: 1.1715843677520752\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.2758378982543945 | KNN Loss: 6.110327243804932 | BCE Loss: 1.1655104160308838\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.264586448669434 | KNN Loss: 6.093118667602539 | BCE Loss: 1.1714675426483154\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.2423624992370605 | KNN Loss: 6.073263168334961 | BCE Loss: 1.1690994501113892\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.1834940910339355 | KNN Loss: 6.054794788360596 | BCE Loss: 1.1286991834640503\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.162687301635742 | KNN Loss: 6.026739597320557 | BCE Loss: 1.135947823524475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.1136908531188965 | KNN Loss: 6.006924152374268 | BCE Loss: 1.106766700744629\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.062594890594482 | KNN Loss: 5.958533763885498 | BCE Loss: 1.1040611267089844\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.014702796936035 | KNN Loss: 5.912962436676025 | BCE Loss: 1.1017404794692993\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.985175132751465 | KNN Loss: 5.881042003631592 | BCE Loss: 1.104133129119873\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.982003211975098 | KNN Loss: 5.843730926513672 | BCE Loss: 1.1382720470428467\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.8553266525268555 | KNN Loss: 5.777772426605225 | BCE Loss: 1.0775541067123413\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.823657989501953 | KNN Loss: 5.716406345367432 | BCE Loss: 1.1072516441345215\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.77890157699585 | KNN Loss: 5.681288242340088 | BCE Loss: 1.0976133346557617\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.688963890075684 | KNN Loss: 5.58894157409668 | BCE Loss: 1.100022554397583\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 6.642072677612305 | KNN Loss: 5.541315078735352 | BCE Loss: 1.1007575988769531\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 6.553706169128418 | KNN Loss: 5.470162391662598 | BCE Loss: 1.0835440158843994\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 6.499044418334961 | KNN Loss: 5.419338703155518 | BCE Loss: 1.0797057151794434\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 6.424653053283691 | KNN Loss: 5.3342976570129395 | BCE Loss: 1.090355634689331\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 6.368924617767334 | KNN Loss: 5.295562267303467 | BCE Loss: 1.0733624696731567\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 6.266975402832031 | KNN Loss: 5.181401252746582 | BCE Loss: 1.0855739116668701\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 6.20408821105957 | KNN Loss: 5.112901210784912 | BCE Loss: 1.0911872386932373\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 6.144858360290527 | KNN Loss: 5.067317485809326 | BCE Loss: 1.0775408744812012\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 6.0709309577941895 | KNN Loss: 4.990567207336426 | BCE Loss: 1.0803637504577637\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 6.022798538208008 | KNN Loss: 4.926020622253418 | BCE Loss: 1.0967776775360107\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.980342864990234 | KNN Loss: 4.891716480255127 | BCE Loss: 1.088626503944397\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.93082857131958 | KNN Loss: 4.841821670532227 | BCE Loss: 1.0890069007873535\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.905767440795898 | KNN Loss: 4.809203147888184 | BCE Loss: 1.0965640544891357\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.819449424743652 | KNN Loss: 4.744608402252197 | BCE Loss: 1.074840784072876\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.824296951293945 | KNN Loss: 4.739656448364258 | BCE Loss: 1.084640622138977\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.781416893005371 | KNN Loss: 4.703592777252197 | BCE Loss: 1.0778238773345947\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.76313591003418 | KNN Loss: 4.6748833656311035 | BCE Loss: 1.0882524251937866\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.704407215118408 | KNN Loss: 4.637484073638916 | BCE Loss: 1.0669231414794922\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 5.697508811950684 | KNN Loss: 4.6103835105896 | BCE Loss: 1.087125539779663\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 5.697323322296143 | KNN Loss: 4.604415416717529 | BCE Loss: 1.0929079055786133\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.669968128204346 | KNN Loss: 4.583693027496338 | BCE Loss: 1.0862749814987183\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 5.649592399597168 | KNN Loss: 4.555371284484863 | BCE Loss: 1.0942211151123047\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 5.600571155548096 | KNN Loss: 4.537544250488281 | BCE Loss: 1.063027024269104\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 5.601070404052734 | KNN Loss: 4.5362396240234375 | BCE Loss: 1.0648307800292969\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 5.606904983520508 | KNN Loss: 4.535150051116943 | BCE Loss: 1.0717549324035645\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 5.594158172607422 | KNN Loss: 4.523777961730957 | BCE Loss: 1.0703799724578857\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 5.633767127990723 | KNN Loss: 4.526678085327148 | BCE Loss: 1.1070888042449951\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 5.57414436340332 | KNN Loss: 4.515594005584717 | BCE Loss: 1.0585505962371826\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 5.587507247924805 | KNN Loss: 4.507305145263672 | BCE Loss: 1.0802018642425537\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 5.557292461395264 | KNN Loss: 4.514627933502197 | BCE Loss: 1.0426645278930664\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 5.547096252441406 | KNN Loss: 4.489156723022461 | BCE Loss: 1.0579395294189453\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 5.527174949645996 | KNN Loss: 4.492211818695068 | BCE Loss: 1.0349630117416382\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 5.569415092468262 | KNN Loss: 4.4974212646484375 | BCE Loss: 1.0719935894012451\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 5.606377601623535 | KNN Loss: 4.517654895782471 | BCE Loss: 1.0887229442596436\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 5.571876049041748 | KNN Loss: 4.500491142272949 | BCE Loss: 1.0713849067687988\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 5.523006439208984 | KNN Loss: 4.481107234954834 | BCE Loss: 1.0418989658355713\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 5.524612903594971 | KNN Loss: 4.47483491897583 | BCE Loss: 1.0497779846191406\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 5.535293102264404 | KNN Loss: 4.482246398925781 | BCE Loss: 1.053046703338623\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 5.557420253753662 | KNN Loss: 4.466235160827637 | BCE Loss: 1.0911850929260254\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 5.522714614868164 | KNN Loss: 4.463199615478516 | BCE Loss: 1.0595149993896484\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 5.540731906890869 | KNN Loss: 4.475080966949463 | BCE Loss: 1.0656509399414062\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 5.549245357513428 | KNN Loss: 4.480120658874512 | BCE Loss: 1.069124698638916\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 5.531765937805176 | KNN Loss: 4.470680236816406 | BCE Loss: 1.0610857009887695\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 5.5229668617248535 | KNN Loss: 4.464019298553467 | BCE Loss: 1.0589475631713867\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 5.526214599609375 | KNN Loss: 4.474066257476807 | BCE Loss: 1.0521481037139893\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 5.534091472625732 | KNN Loss: 4.4733991622924805 | BCE Loss: 1.060692310333252\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 5.5051655769348145 | KNN Loss: 4.4534478187561035 | BCE Loss: 1.0517176389694214\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 5.522814750671387 | KNN Loss: 4.449338912963867 | BCE Loss: 1.073475956916809\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 5.557376861572266 | KNN Loss: 4.485620498657227 | BCE Loss: 1.07175612449646\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 5.522823333740234 | KNN Loss: 4.461062908172607 | BCE Loss: 1.061760425567627\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 5.516439437866211 | KNN Loss: 4.468045234680176 | BCE Loss: 1.0483942031860352\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 5.518825531005859 | KNN Loss: 4.469298839569092 | BCE Loss: 1.0495269298553467\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 5.499240875244141 | KNN Loss: 4.459029197692871 | BCE Loss: 1.0402119159698486\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 5.4894561767578125 | KNN Loss: 4.443867206573486 | BCE Loss: 1.045588731765747\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 5.529144763946533 | KNN Loss: 4.4609599113464355 | BCE Loss: 1.0681849718093872\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 5.5159711837768555 | KNN Loss: 4.475870132446289 | BCE Loss: 1.0401010513305664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 5.507378101348877 | KNN Loss: 4.423064231872559 | BCE Loss: 1.084313988685608\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 5.532404899597168 | KNN Loss: 4.467677116394043 | BCE Loss: 1.064727544784546\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 5.539498329162598 | KNN Loss: 4.463871002197266 | BCE Loss: 1.0756272077560425\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 5.464534759521484 | KNN Loss: 4.4394354820251465 | BCE Loss: 1.0250990390777588\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 5.4879913330078125 | KNN Loss: 4.418454170227051 | BCE Loss: 1.0695372819900513\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 5.512447357177734 | KNN Loss: 4.455063343048096 | BCE Loss: 1.0573842525482178\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 5.4826130867004395 | KNN Loss: 4.447882175445557 | BCE Loss: 1.0347310304641724\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 5.572408199310303 | KNN Loss: 4.486761569976807 | BCE Loss: 1.0856467485427856\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 5.50042724609375 | KNN Loss: 4.4441447257995605 | BCE Loss: 1.056282639503479\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 5.465780258178711 | KNN Loss: 4.43115234375 | BCE Loss: 1.0346280336380005\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 5.525461196899414 | KNN Loss: 4.447847366333008 | BCE Loss: 1.0776140689849854\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 5.500609397888184 | KNN Loss: 4.433899879455566 | BCE Loss: 1.0667093992233276\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 5.522757530212402 | KNN Loss: 4.437680244445801 | BCE Loss: 1.0850772857666016\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 5.452410697937012 | KNN Loss: 4.436208724975586 | BCE Loss: 1.0162022113800049\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 5.461235046386719 | KNN Loss: 4.425420761108398 | BCE Loss: 1.0358142852783203\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 5.47654390335083 | KNN Loss: 4.444199085235596 | BCE Loss: 1.0323448181152344\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 5.5163421630859375 | KNN Loss: 4.472015857696533 | BCE Loss: 1.0443265438079834\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 5.473045825958252 | KNN Loss: 4.432117462158203 | BCE Loss: 1.0409283638000488\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 5.48501443862915 | KNN Loss: 4.422359466552734 | BCE Loss: 1.0626548528671265\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 5.48193359375 | KNN Loss: 4.451792240142822 | BCE Loss: 1.0301411151885986\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 5.507370948791504 | KNN Loss: 4.442997932434082 | BCE Loss: 1.0643731355667114\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 5.495334148406982 | KNN Loss: 4.454250335693359 | BCE Loss: 1.0410839319229126\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 5.493625164031982 | KNN Loss: 4.4333319664001465 | BCE Loss: 1.060293197631836\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 5.509033203125 | KNN Loss: 4.450250625610352 | BCE Loss: 1.0587825775146484\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 5.484130382537842 | KNN Loss: 4.4260125160217285 | BCE Loss: 1.0581178665161133\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 5.525712966918945 | KNN Loss: 4.44918966293335 | BCE Loss: 1.0765233039855957\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 5.475094795227051 | KNN Loss: 4.408463001251221 | BCE Loss: 1.0666316747665405\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 5.482688903808594 | KNN Loss: 4.4307861328125 | BCE Loss: 1.0519025325775146\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 5.51147985458374 | KNN Loss: 4.444604873657227 | BCE Loss: 1.0668749809265137\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 5.484856605529785 | KNN Loss: 4.43594217300415 | BCE Loss: 1.0489144325256348\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 5.451030731201172 | KNN Loss: 4.417850017547607 | BCE Loss: 1.0331804752349854\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 5.509194374084473 | KNN Loss: 4.447810649871826 | BCE Loss: 1.0613839626312256\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 5.482823371887207 | KNN Loss: 4.427762985229492 | BCE Loss: 1.0550603866577148\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 5.485494136810303 | KNN Loss: 4.4457807540893555 | BCE Loss: 1.0397132635116577\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 5.513731002807617 | KNN Loss: 4.444121360778809 | BCE Loss: 1.0696096420288086\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 5.483459949493408 | KNN Loss: 4.44528865814209 | BCE Loss: 1.0381712913513184\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 5.474836349487305 | KNN Loss: 4.418797492980957 | BCE Loss: 1.0560389757156372\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 5.481930732727051 | KNN Loss: 4.413381099700928 | BCE Loss: 1.068549633026123\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 5.478402137756348 | KNN Loss: 4.421700954437256 | BCE Loss: 1.0567009449005127\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 5.512429714202881 | KNN Loss: 4.447422504425049 | BCE Loss: 1.0650073289871216\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 5.4593329429626465 | KNN Loss: 4.422907829284668 | BCE Loss: 1.036424994468689\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 5.472572326660156 | KNN Loss: 4.421321868896484 | BCE Loss: 1.0512503385543823\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 5.514416694641113 | KNN Loss: 4.453945159912109 | BCE Loss: 1.060471534729004\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 5.467055320739746 | KNN Loss: 4.40134334564209 | BCE Loss: 1.0657117366790771\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 5.481376647949219 | KNN Loss: 4.417720317840576 | BCE Loss: 1.0636563301086426\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 5.495537757873535 | KNN Loss: 4.413464546203613 | BCE Loss: 1.0820732116699219\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 5.499774932861328 | KNN Loss: 4.435885906219482 | BCE Loss: 1.0638887882232666\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 5.468114852905273 | KNN Loss: 4.429274559020996 | BCE Loss: 1.0388405323028564\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 5.475846290588379 | KNN Loss: 4.425395965576172 | BCE Loss: 1.050450325012207\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 5.475900650024414 | KNN Loss: 4.417624473571777 | BCE Loss: 1.0582764148712158\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 5.448786735534668 | KNN Loss: 4.419471740722656 | BCE Loss: 1.0293152332305908\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 5.484866142272949 | KNN Loss: 4.422190189361572 | BCE Loss: 1.0626757144927979\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 5.502574920654297 | KNN Loss: 4.4397172927856445 | BCE Loss: 1.0628575086593628\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 5.51845645904541 | KNN Loss: 4.458044052124023 | BCE Loss: 1.0604126453399658\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 5.475691318511963 | KNN Loss: 4.41798734664917 | BCE Loss: 1.057703971862793\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 5.446861267089844 | KNN Loss: 4.427771091461182 | BCE Loss: 1.0190904140472412\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 5.4570722579956055 | KNN Loss: 4.435197830200195 | BCE Loss: 1.0218746662139893\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 5.47381591796875 | KNN Loss: 4.409641742706299 | BCE Loss: 1.0641744136810303\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 5.491974353790283 | KNN Loss: 4.437350273132324 | BCE Loss: 1.054624080657959\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 5.47442102432251 | KNN Loss: 4.43102502822876 | BCE Loss: 1.0433958768844604\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 5.529812335968018 | KNN Loss: 4.44877815246582 | BCE Loss: 1.0810341835021973\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 5.450225830078125 | KNN Loss: 4.410366058349609 | BCE Loss: 1.0398597717285156\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 5.467296600341797 | KNN Loss: 4.396111965179443 | BCE Loss: 1.0711843967437744\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 5.528941631317139 | KNN Loss: 4.469511985778809 | BCE Loss: 1.0594297647476196\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 5.501497745513916 | KNN Loss: 4.436633110046387 | BCE Loss: 1.0648646354675293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 5.467789649963379 | KNN Loss: 4.427908420562744 | BCE Loss: 1.0398812294006348\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 5.515353679656982 | KNN Loss: 4.440191745758057 | BCE Loss: 1.0751619338989258\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 5.499302864074707 | KNN Loss: 4.43162727355957 | BCE Loss: 1.0676755905151367\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 5.4820556640625 | KNN Loss: 4.421136379241943 | BCE Loss: 1.060919165611267\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 5.483309268951416 | KNN Loss: 4.434974193572998 | BCE Loss: 1.048335075378418\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 5.489395618438721 | KNN Loss: 4.430936813354492 | BCE Loss: 1.058458924293518\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 5.49208402633667 | KNN Loss: 4.427916526794434 | BCE Loss: 1.0641676187515259\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 5.442554473876953 | KNN Loss: 4.402794361114502 | BCE Loss: 1.039759874343872\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 5.457050323486328 | KNN Loss: 4.397153854370117 | BCE Loss: 1.059896469116211\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 5.493463516235352 | KNN Loss: 4.453384876251221 | BCE Loss: 1.04007887840271\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 5.4465155601501465 | KNN Loss: 4.406793594360352 | BCE Loss: 1.039721965789795\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 5.463801860809326 | KNN Loss: 4.405598163604736 | BCE Loss: 1.0582036972045898\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 5.450245380401611 | KNN Loss: 4.413994789123535 | BCE Loss: 1.0362507104873657\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 5.476984024047852 | KNN Loss: 4.44531774520874 | BCE Loss: 1.0316660404205322\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 5.4618024826049805 | KNN Loss: 4.432126998901367 | BCE Loss: 1.0296754837036133\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 5.4616498947143555 | KNN Loss: 4.427192211151123 | BCE Loss: 1.0344576835632324\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 5.42123556137085 | KNN Loss: 4.403151988983154 | BCE Loss: 1.0180835723876953\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 5.466619968414307 | KNN Loss: 4.401581287384033 | BCE Loss: 1.0650385618209839\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 5.482345104217529 | KNN Loss: 4.434961318969727 | BCE Loss: 1.0473836660385132\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 5.456824779510498 | KNN Loss: 4.4135003089904785 | BCE Loss: 1.043324589729309\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 5.499303340911865 | KNN Loss: 4.4525580406188965 | BCE Loss: 1.0467454195022583\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 5.443122863769531 | KNN Loss: 4.417054176330566 | BCE Loss: 1.0260685682296753\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 5.477566719055176 | KNN Loss: 4.417997360229492 | BCE Loss: 1.0595691204071045\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 5.473811149597168 | KNN Loss: 4.413677215576172 | BCE Loss: 1.0601341724395752\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 5.419973373413086 | KNN Loss: 4.40785026550293 | BCE Loss: 1.0121233463287354\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 5.507781982421875 | KNN Loss: 4.46396541595459 | BCE Loss: 1.0438166856765747\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 5.495121955871582 | KNN Loss: 4.418656349182129 | BCE Loss: 1.0764657258987427\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 5.478653907775879 | KNN Loss: 4.439359664916992 | BCE Loss: 1.0392943620681763\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 5.489203453063965 | KNN Loss: 4.425329685211182 | BCE Loss: 1.0638736486434937\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 5.457779884338379 | KNN Loss: 4.41363000869751 | BCE Loss: 1.0441501140594482\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 5.477361679077148 | KNN Loss: 4.4039530754089355 | BCE Loss: 1.0734087228775024\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 5.520297050476074 | KNN Loss: 4.434019088745117 | BCE Loss: 1.0862778425216675\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 5.497981071472168 | KNN Loss: 4.424717903137207 | BCE Loss: 1.073263168334961\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 5.493671417236328 | KNN Loss: 4.425340175628662 | BCE Loss: 1.0683314800262451\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 5.463129997253418 | KNN Loss: 4.413155555725098 | BCE Loss: 1.0499744415283203\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 5.462902545928955 | KNN Loss: 4.438736915588379 | BCE Loss: 1.0241656303405762\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 5.4196295738220215 | KNN Loss: 4.3892669677734375 | BCE Loss: 1.0303627252578735\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 5.505612850189209 | KNN Loss: 4.451907634735107 | BCE Loss: 1.0537052154541016\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 5.49222469329834 | KNN Loss: 4.410205364227295 | BCE Loss: 1.0820192098617554\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 5.497561931610107 | KNN Loss: 4.4343581199646 | BCE Loss: 1.0632038116455078\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 5.447862148284912 | KNN Loss: 4.420926570892334 | BCE Loss: 1.0269355773925781\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 5.465865612030029 | KNN Loss: 4.424614906311035 | BCE Loss: 1.0412507057189941\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 5.443358421325684 | KNN Loss: 4.399666786193848 | BCE Loss: 1.043691635131836\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 5.467668533325195 | KNN Loss: 4.421018123626709 | BCE Loss: 1.0466506481170654\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 5.454560279846191 | KNN Loss: 4.419888973236084 | BCE Loss: 1.0346713066101074\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 5.479342460632324 | KNN Loss: 4.444251537322998 | BCE Loss: 1.0350908041000366\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 5.45009183883667 | KNN Loss: 4.394751071929932 | BCE Loss: 1.0553406476974487\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 5.4446563720703125 | KNN Loss: 4.399212837219238 | BCE Loss: 1.0454435348510742\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 5.420962333679199 | KNN Loss: 4.389524936676025 | BCE Loss: 1.0314373970031738\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 5.434357643127441 | KNN Loss: 4.407047748565674 | BCE Loss: 1.0273101329803467\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 5.433144569396973 | KNN Loss: 4.419796943664551 | BCE Loss: 1.0133473873138428\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 5.4942498207092285 | KNN Loss: 4.429394721984863 | BCE Loss: 1.0648550987243652\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 5.443395137786865 | KNN Loss: 4.414299964904785 | BCE Loss: 1.02909517288208\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 5.452115058898926 | KNN Loss: 4.418038845062256 | BCE Loss: 1.03407621383667\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 5.462357044219971 | KNN Loss: 4.406843185424805 | BCE Loss: 1.055513858795166\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 5.413425445556641 | KNN Loss: 4.389313220977783 | BCE Loss: 1.0241122245788574\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 5.519284248352051 | KNN Loss: 4.435389995574951 | BCE Loss: 1.0838944911956787\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 5.40805721282959 | KNN Loss: 4.378603935241699 | BCE Loss: 1.0294535160064697\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 5.486617565155029 | KNN Loss: 4.411577224731445 | BCE Loss: 1.075040340423584\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 5.481655597686768 | KNN Loss: 4.408319473266602 | BCE Loss: 1.073336124420166\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 5.463638782501221 | KNN Loss: 4.407777786254883 | BCE Loss: 1.055860996246338\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 5.46693754196167 | KNN Loss: 4.418241024017334 | BCE Loss: 1.048696517944336\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 5.4488606452941895 | KNN Loss: 4.418953895568848 | BCE Loss: 1.0299067497253418\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 5.435227394104004 | KNN Loss: 4.382869720458984 | BCE Loss: 1.0523579120635986\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 5.42198371887207 | KNN Loss: 4.402755260467529 | BCE Loss: 1.019228219985962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 5.453668594360352 | KNN Loss: 4.428226947784424 | BCE Loss: 1.0254415273666382\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 5.423710823059082 | KNN Loss: 4.404366493225098 | BCE Loss: 1.0193440914154053\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 5.475948333740234 | KNN Loss: 4.4180426597595215 | BCE Loss: 1.057905673980713\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 5.454903602600098 | KNN Loss: 4.396880626678467 | BCE Loss: 1.0580229759216309\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 5.440617561340332 | KNN Loss: 4.386813640594482 | BCE Loss: 1.05380380153656\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 5.4464497566223145 | KNN Loss: 4.408674716949463 | BCE Loss: 1.037774920463562\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 5.449211120605469 | KNN Loss: 4.405559062957764 | BCE Loss: 1.043652057647705\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 5.44399356842041 | KNN Loss: 4.415781497955322 | BCE Loss: 1.028212070465088\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 5.49859619140625 | KNN Loss: 4.436796188354492 | BCE Loss: 1.061800241470337\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 5.450854301452637 | KNN Loss: 4.4013447761535645 | BCE Loss: 1.0495097637176514\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 5.451375484466553 | KNN Loss: 4.404071807861328 | BCE Loss: 1.0473036766052246\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 5.426054954528809 | KNN Loss: 4.415048122406006 | BCE Loss: 1.0110067129135132\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 5.432041645050049 | KNN Loss: 4.401387691497803 | BCE Loss: 1.030653953552246\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 5.4884490966796875 | KNN Loss: 4.4345879554748535 | BCE Loss: 1.0538609027862549\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 5.438074111938477 | KNN Loss: 4.400546073913574 | BCE Loss: 1.0375280380249023\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 5.432878494262695 | KNN Loss: 4.378277778625488 | BCE Loss: 1.054600477218628\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 5.470710277557373 | KNN Loss: 4.414028167724609 | BCE Loss: 1.0566819906234741\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 5.415417671203613 | KNN Loss: 4.3815107345581055 | BCE Loss: 1.0339069366455078\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 5.480396270751953 | KNN Loss: 4.414288520812988 | BCE Loss: 1.0661075115203857\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 5.433111190795898 | KNN Loss: 4.378732204437256 | BCE Loss: 1.0543787479400635\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 5.463362693786621 | KNN Loss: 4.402532577514648 | BCE Loss: 1.0608301162719727\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 5.504546165466309 | KNN Loss: 4.424232006072998 | BCE Loss: 1.080314040184021\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 5.414764881134033 | KNN Loss: 4.387035369873047 | BCE Loss: 1.0277295112609863\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 5.446942329406738 | KNN Loss: 4.406679630279541 | BCE Loss: 1.0402629375457764\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 5.434902191162109 | KNN Loss: 4.3923444747924805 | BCE Loss: 1.042557716369629\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 5.471654891967773 | KNN Loss: 4.432251930236816 | BCE Loss: 1.0394032001495361\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 5.452592372894287 | KNN Loss: 4.4049391746521 | BCE Loss: 1.047653079032898\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 5.4157843589782715 | KNN Loss: 4.3727593421936035 | BCE Loss: 1.0430248975753784\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 5.440422534942627 | KNN Loss: 4.420473098754883 | BCE Loss: 1.0199493169784546\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 5.4315972328186035 | KNN Loss: 4.39706563949585 | BCE Loss: 1.034531593322754\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 5.481675148010254 | KNN Loss: 4.432714462280273 | BCE Loss: 1.0489604473114014\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 5.46012020111084 | KNN Loss: 4.412116050720215 | BCE Loss: 1.048004388809204\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 5.429512977600098 | KNN Loss: 4.410886764526367 | BCE Loss: 1.0186259746551514\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 5.428672790527344 | KNN Loss: 4.384674549102783 | BCE Loss: 1.0439984798431396\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 5.452417373657227 | KNN Loss: 4.424330234527588 | BCE Loss: 1.0280871391296387\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 5.435826778411865 | KNN Loss: 4.376924991607666 | BCE Loss: 1.0589019060134888\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 5.469706058502197 | KNN Loss: 4.420031547546387 | BCE Loss: 1.0496745109558105\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 5.417193412780762 | KNN Loss: 4.385652542114258 | BCE Loss: 1.031541109085083\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 5.486201763153076 | KNN Loss: 4.414856910705566 | BCE Loss: 1.0713448524475098\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 5.4348273277282715 | KNN Loss: 4.405813217163086 | BCE Loss: 1.0290141105651855\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 5.410027980804443 | KNN Loss: 4.3817667961120605 | BCE Loss: 1.0282613039016724\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 5.433859825134277 | KNN Loss: 4.391068458557129 | BCE Loss: 1.0427913665771484\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 5.403697967529297 | KNN Loss: 4.374950408935547 | BCE Loss: 1.028747320175171\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 5.417995452880859 | KNN Loss: 4.390749931335449 | BCE Loss: 1.0272455215454102\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 5.42943000793457 | KNN Loss: 4.393357753753662 | BCE Loss: 1.0360724925994873\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 5.46047306060791 | KNN Loss: 4.396335601806641 | BCE Loss: 1.0641374588012695\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 5.436169147491455 | KNN Loss: 4.39015531539917 | BCE Loss: 1.0460139513015747\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 5.460610389709473 | KNN Loss: 4.415046691894531 | BCE Loss: 1.0455636978149414\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 5.42369270324707 | KNN Loss: 4.400759696960449 | BCE Loss: 1.022933006286621\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 5.462289810180664 | KNN Loss: 4.402784824371338 | BCE Loss: 1.059504747390747\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 5.492166042327881 | KNN Loss: 4.447051048278809 | BCE Loss: 1.0451149940490723\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 5.398780822753906 | KNN Loss: 4.355320453643799 | BCE Loss: 1.0434603691101074\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 5.41335391998291 | KNN Loss: 4.382816314697266 | BCE Loss: 1.0305373668670654\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 5.4707207679748535 | KNN Loss: 4.435209274291992 | BCE Loss: 1.0355114936828613\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 5.420544147491455 | KNN Loss: 4.426719665527344 | BCE Loss: 0.9938243627548218\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 5.437863349914551 | KNN Loss: 4.369341850280762 | BCE Loss: 1.0685216188430786\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 5.423709392547607 | KNN Loss: 4.398725509643555 | BCE Loss: 1.0249838829040527\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 5.407042026519775 | KNN Loss: 4.403189182281494 | BCE Loss: 1.0038529634475708\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 5.431221008300781 | KNN Loss: 4.390604496002197 | BCE Loss: 1.040616750717163\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 5.4182233810424805 | KNN Loss: 4.399209022521973 | BCE Loss: 1.0190143585205078\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 5.4389872550964355 | KNN Loss: 4.408168315887451 | BCE Loss: 1.030819058418274\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 5.435871124267578 | KNN Loss: 4.373834609985352 | BCE Loss: 1.0620367527008057\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 5.454524993896484 | KNN Loss: 4.408453941345215 | BCE Loss: 1.0460710525512695\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 5.406766891479492 | KNN Loss: 4.393301010131836 | BCE Loss: 1.0134656429290771\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 5.382391452789307 | KNN Loss: 4.370204925537109 | BCE Loss: 1.0121865272521973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 5.434545516967773 | KNN Loss: 4.3978753089904785 | BCE Loss: 1.036670207977295\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 5.446662902832031 | KNN Loss: 4.389941692352295 | BCE Loss: 1.0567212104797363\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 5.4409284591674805 | KNN Loss: 4.405837059020996 | BCE Loss: 1.0350916385650635\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 5.483055591583252 | KNN Loss: 4.4302496910095215 | BCE Loss: 1.05280601978302\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 5.443774223327637 | KNN Loss: 4.407904624938965 | BCE Loss: 1.0358693599700928\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 5.478604793548584 | KNN Loss: 4.426198959350586 | BCE Loss: 1.0524059534072876\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 5.390327453613281 | KNN Loss: 4.3837103843688965 | BCE Loss: 1.0066173076629639\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 5.435505390167236 | KNN Loss: 4.40519905090332 | BCE Loss: 1.0303064584732056\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 5.403539180755615 | KNN Loss: 4.368476867675781 | BCE Loss: 1.0350621938705444\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 5.393362045288086 | KNN Loss: 4.371075630187988 | BCE Loss: 1.0222864151000977\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 5.395422458648682 | KNN Loss: 4.371728897094727 | BCE Loss: 1.023693561553955\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 5.430959701538086 | KNN Loss: 4.387829303741455 | BCE Loss: 1.0431301593780518\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 5.405011177062988 | KNN Loss: 4.403622627258301 | BCE Loss: 1.0013887882232666\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 5.422900676727295 | KNN Loss: 4.379324436187744 | BCE Loss: 1.0435761213302612\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 5.441094398498535 | KNN Loss: 4.415433883666992 | BCE Loss: 1.025660753250122\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 5.445282936096191 | KNN Loss: 4.387818336486816 | BCE Loss: 1.057464838027954\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 5.437815189361572 | KNN Loss: 4.378910541534424 | BCE Loss: 1.058904767036438\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 5.423161506652832 | KNN Loss: 4.380948066711426 | BCE Loss: 1.0422134399414062\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 5.436860084533691 | KNN Loss: 4.3836164474487305 | BCE Loss: 1.053243637084961\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 5.432389259338379 | KNN Loss: 4.426557540893555 | BCE Loss: 1.0058317184448242\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 5.414929389953613 | KNN Loss: 4.405322074890137 | BCE Loss: 1.0096070766448975\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 5.422773361206055 | KNN Loss: 4.3968825340271 | BCE Loss: 1.0258910655975342\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 5.444767475128174 | KNN Loss: 4.390870094299316 | BCE Loss: 1.0538972616195679\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 5.418972969055176 | KNN Loss: 4.400600433349609 | BCE Loss: 1.0183725357055664\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 5.457006931304932 | KNN Loss: 4.412337303161621 | BCE Loss: 1.0446696281433105\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 5.434134483337402 | KNN Loss: 4.375082492828369 | BCE Loss: 1.0590519905090332\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 5.417885780334473 | KNN Loss: 4.38470983505249 | BCE Loss: 1.0331757068634033\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 5.377290725708008 | KNN Loss: 4.3476457595825195 | BCE Loss: 1.0296449661254883\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 5.43540620803833 | KNN Loss: 4.411625862121582 | BCE Loss: 1.023780345916748\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 5.467578411102295 | KNN Loss: 4.41748046875 | BCE Loss: 1.0500980615615845\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 5.435534477233887 | KNN Loss: 4.39844274520874 | BCE Loss: 1.0370919704437256\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 5.422428131103516 | KNN Loss: 4.396152496337891 | BCE Loss: 1.026275634765625\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 5.421031475067139 | KNN Loss: 4.385540962219238 | BCE Loss: 1.0354903936386108\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 5.43485164642334 | KNN Loss: 4.422367095947266 | BCE Loss: 1.0124843120574951\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 5.417897701263428 | KNN Loss: 4.367912292480469 | BCE Loss: 1.0499855279922485\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 5.4333648681640625 | KNN Loss: 4.408695220947266 | BCE Loss: 1.0246694087982178\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 5.451075553894043 | KNN Loss: 4.421236038208008 | BCE Loss: 1.0298397541046143\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 5.450921535491943 | KNN Loss: 4.406599998474121 | BCE Loss: 1.0443214178085327\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 5.4039177894592285 | KNN Loss: 4.395211219787598 | BCE Loss: 1.0087066888809204\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 5.442482948303223 | KNN Loss: 4.416147232055664 | BCE Loss: 1.0263359546661377\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 5.447319030761719 | KNN Loss: 4.400068283081055 | BCE Loss: 1.0472508668899536\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 5.441379547119141 | KNN Loss: 4.3957438468933105 | BCE Loss: 1.0456359386444092\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 5.3856353759765625 | KNN Loss: 4.38733434677124 | BCE Loss: 0.9983009696006775\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 5.414301872253418 | KNN Loss: 4.418506622314453 | BCE Loss: 0.9957951903343201\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 5.472830772399902 | KNN Loss: 4.391789436340332 | BCE Loss: 1.0810414552688599\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 5.395208358764648 | KNN Loss: 4.37199592590332 | BCE Loss: 1.0232124328613281\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 5.490309715270996 | KNN Loss: 4.418188571929932 | BCE Loss: 1.0721213817596436\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 5.4669671058654785 | KNN Loss: 4.419143199920654 | BCE Loss: 1.0478239059448242\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 5.418513774871826 | KNN Loss: 4.377244472503662 | BCE Loss: 1.041269302368164\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 5.3892741203308105 | KNN Loss: 4.385829925537109 | BCE Loss: 1.0034441947937012\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 5.411076068878174 | KNN Loss: 4.398876667022705 | BCE Loss: 1.0121992826461792\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 5.462806701660156 | KNN Loss: 4.426013946533203 | BCE Loss: 1.036792516708374\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 5.440337181091309 | KNN Loss: 4.424778938293457 | BCE Loss: 1.0155580043792725\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 5.450835227966309 | KNN Loss: 4.41645622253418 | BCE Loss: 1.0343791246414185\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 5.399789333343506 | KNN Loss: 4.386370658874512 | BCE Loss: 1.0134187936782837\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 5.43447732925415 | KNN Loss: 4.390566825866699 | BCE Loss: 1.0439105033874512\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 5.402538299560547 | KNN Loss: 4.380439281463623 | BCE Loss: 1.0220988988876343\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 5.444320201873779 | KNN Loss: 4.407044410705566 | BCE Loss: 1.0372759103775024\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 5.427384376525879 | KNN Loss: 4.366123199462891 | BCE Loss: 1.0612611770629883\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 5.39236307144165 | KNN Loss: 4.372001647949219 | BCE Loss: 1.0203614234924316\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 5.427819728851318 | KNN Loss: 4.401857852935791 | BCE Loss: 1.0259617567062378\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 5.451093673706055 | KNN Loss: 4.412635803222656 | BCE Loss: 1.0384576320648193\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 5.447279930114746 | KNN Loss: 4.389007091522217 | BCE Loss: 1.0582727193832397\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 5.392374515533447 | KNN Loss: 4.371283531188965 | BCE Loss: 1.021091103553772\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 5.419380187988281 | KNN Loss: 4.375027179718018 | BCE Loss: 1.0443527698516846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 5.482130527496338 | KNN Loss: 4.4495720863342285 | BCE Loss: 1.0325583219528198\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 5.416349411010742 | KNN Loss: 4.391154766082764 | BCE Loss: 1.0251948833465576\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 5.40132474899292 | KNN Loss: 4.3675537109375 | BCE Loss: 1.0337709188461304\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 5.4426188468933105 | KNN Loss: 4.402673244476318 | BCE Loss: 1.0399457216262817\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 5.425076961517334 | KNN Loss: 4.366964340209961 | BCE Loss: 1.0581125020980835\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 5.445033073425293 | KNN Loss: 4.4020562171936035 | BCE Loss: 1.0429770946502686\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 5.413837432861328 | KNN Loss: 4.368912696838379 | BCE Loss: 1.0449244976043701\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 5.414743423461914 | KNN Loss: 4.385481834411621 | BCE Loss: 1.0292614698410034\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 5.390563011169434 | KNN Loss: 4.388589382171631 | BCE Loss: 1.0019738674163818\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 5.3945722579956055 | KNN Loss: 4.374748706817627 | BCE Loss: 1.0198233127593994\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 5.392688751220703 | KNN Loss: 4.377896308898926 | BCE Loss: 1.0147922039031982\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 5.394636154174805 | KNN Loss: 4.378599643707275 | BCE Loss: 1.0160363912582397\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 5.373312950134277 | KNN Loss: 4.363597393035889 | BCE Loss: 1.0097157955169678\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 5.4143877029418945 | KNN Loss: 4.394699573516846 | BCE Loss: 1.0196878910064697\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 5.4384379386901855 | KNN Loss: 4.38857889175415 | BCE Loss: 1.0498591661453247\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 5.378848075866699 | KNN Loss: 4.374880790710449 | BCE Loss: 1.0039674043655396\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 5.446116924285889 | KNN Loss: 4.386848449707031 | BCE Loss: 1.0592684745788574\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 5.41823673248291 | KNN Loss: 4.4053120613098145 | BCE Loss: 1.0129245519638062\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 5.427247047424316 | KNN Loss: 4.373958110809326 | BCE Loss: 1.0532889366149902\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 5.444316864013672 | KNN Loss: 4.4238762855529785 | BCE Loss: 1.0204408168792725\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 5.420755863189697 | KNN Loss: 4.416810989379883 | BCE Loss: 1.003944993019104\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 5.397078514099121 | KNN Loss: 4.372246742248535 | BCE Loss: 1.0248315334320068\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 5.418079853057861 | KNN Loss: 4.400141716003418 | BCE Loss: 1.017938256263733\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 5.426908493041992 | KNN Loss: 4.407068729400635 | BCE Loss: 1.019839882850647\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 5.416799068450928 | KNN Loss: 4.387230396270752 | BCE Loss: 1.0295686721801758\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 5.366858959197998 | KNN Loss: 4.361423015594482 | BCE Loss: 1.005435824394226\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 5.437886714935303 | KNN Loss: 4.401484966278076 | BCE Loss: 1.0364018678665161\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 5.383868217468262 | KNN Loss: 4.377203941345215 | BCE Loss: 1.0066643953323364\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 5.443002700805664 | KNN Loss: 4.3818864822387695 | BCE Loss: 1.0611159801483154\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 5.378550052642822 | KNN Loss: 4.376219272613525 | BCE Loss: 1.0023306608200073\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 5.398832321166992 | KNN Loss: 4.3514628410339355 | BCE Loss: 1.0473697185516357\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 5.42055606842041 | KNN Loss: 4.377251148223877 | BCE Loss: 1.0433051586151123\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 5.401498794555664 | KNN Loss: 4.4119343757629395 | BCE Loss: 0.9895642995834351\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 5.437849044799805 | KNN Loss: 4.419205665588379 | BCE Loss: 1.0186433792114258\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 5.415555953979492 | KNN Loss: 4.400128364562988 | BCE Loss: 1.0154273509979248\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 5.385341167449951 | KNN Loss: 4.371817111968994 | BCE Loss: 1.013524055480957\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 5.44505500793457 | KNN Loss: 4.411309242248535 | BCE Loss: 1.0337457656860352\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 5.406773567199707 | KNN Loss: 4.395844459533691 | BCE Loss: 1.0109292268753052\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 5.410783767700195 | KNN Loss: 4.378615379333496 | BCE Loss: 1.0321681499481201\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 5.421842575073242 | KNN Loss: 4.380419731140137 | BCE Loss: 1.0414228439331055\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 5.422256946563721 | KNN Loss: 4.377237796783447 | BCE Loss: 1.0450190305709839\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 5.430320739746094 | KNN Loss: 4.409036159515381 | BCE Loss: 1.021284818649292\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 5.4666547775268555 | KNN Loss: 4.416626930236816 | BCE Loss: 1.0500279664993286\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 5.404745578765869 | KNN Loss: 4.375545024871826 | BCE Loss: 1.029200553894043\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 5.419983386993408 | KNN Loss: 4.38043737411499 | BCE Loss: 1.0395458936691284\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 5.391324996948242 | KNN Loss: 4.377220630645752 | BCE Loss: 1.0141042470932007\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 5.414241313934326 | KNN Loss: 4.380435943603516 | BCE Loss: 1.0338053703308105\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 5.4102325439453125 | KNN Loss: 4.40443229675293 | BCE Loss: 1.0058001279830933\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 5.429116249084473 | KNN Loss: 4.414564609527588 | BCE Loss: 1.0145516395568848\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 5.432494640350342 | KNN Loss: 4.403378486633301 | BCE Loss: 1.029116153717041\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 5.423715591430664 | KNN Loss: 4.3834428787231445 | BCE Loss: 1.0402729511260986\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 5.37031364440918 | KNN Loss: 4.361037731170654 | BCE Loss: 1.0092757940292358\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 5.428086280822754 | KNN Loss: 4.399930000305176 | BCE Loss: 1.0281561613082886\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 5.403000831604004 | KNN Loss: 4.382854461669922 | BCE Loss: 1.020146131515503\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 5.4029083251953125 | KNN Loss: 4.366415500640869 | BCE Loss: 1.0364928245544434\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 5.461095809936523 | KNN Loss: 4.410228729248047 | BCE Loss: 1.0508673191070557\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 5.422532558441162 | KNN Loss: 4.397611618041992 | BCE Loss: 1.02492094039917\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 5.371613025665283 | KNN Loss: 4.362313270568848 | BCE Loss: 1.0092997550964355\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 5.405917644500732 | KNN Loss: 4.367300033569336 | BCE Loss: 1.038617730140686\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 5.438208103179932 | KNN Loss: 4.392361640930176 | BCE Loss: 1.0458464622497559\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 5.42564058303833 | KNN Loss: 4.396439075469971 | BCE Loss: 1.0292015075683594\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 5.404363632202148 | KNN Loss: 4.378187656402588 | BCE Loss: 1.02617609500885\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 5.452517509460449 | KNN Loss: 4.415711879730225 | BCE Loss: 1.0368053913116455\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 5.43586540222168 | KNN Loss: 4.398733615875244 | BCE Loss: 1.0371315479278564\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 5.392479419708252 | KNN Loss: 4.371038436889648 | BCE Loss: 1.021440863609314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 5.430509567260742 | KNN Loss: 4.407435894012451 | BCE Loss: 1.023073673248291\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 5.410862445831299 | KNN Loss: 4.379970550537109 | BCE Loss: 1.0308917760849\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 5.372937202453613 | KNN Loss: 4.369564056396484 | BCE Loss: 1.0033729076385498\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 5.407941818237305 | KNN Loss: 4.3928070068359375 | BCE Loss: 1.015134572982788\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 5.412952899932861 | KNN Loss: 4.3678388595581055 | BCE Loss: 1.0451140403747559\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 5.430639266967773 | KNN Loss: 4.390250205993652 | BCE Loss: 1.0403889417648315\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 5.407138824462891 | KNN Loss: 4.386468887329102 | BCE Loss: 1.0206701755523682\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 5.411380767822266 | KNN Loss: 4.355388641357422 | BCE Loss: 1.0559921264648438\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 5.385422229766846 | KNN Loss: 4.380801677703857 | BCE Loss: 1.0046206712722778\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 5.42003059387207 | KNN Loss: 4.380758762359619 | BCE Loss: 1.0392720699310303\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 5.40517520904541 | KNN Loss: 4.373293876647949 | BCE Loss: 1.031881332397461\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 5.41352653503418 | KNN Loss: 4.36696195602417 | BCE Loss: 1.0465645790100098\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 5.3844757080078125 | KNN Loss: 4.361170768737793 | BCE Loss: 1.0233051776885986\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 5.407530307769775 | KNN Loss: 4.35751485824585 | BCE Loss: 1.0500154495239258\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 5.399849891662598 | KNN Loss: 4.371140003204346 | BCE Loss: 1.0287100076675415\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 5.385306358337402 | KNN Loss: 4.368804931640625 | BCE Loss: 1.0165013074874878\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 5.384654998779297 | KNN Loss: 4.35784387588501 | BCE Loss: 1.0268113613128662\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 5.451160907745361 | KNN Loss: 4.403656959533691 | BCE Loss: 1.04750394821167\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 5.387025833129883 | KNN Loss: 4.3686089515686035 | BCE Loss: 1.0184168815612793\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 5.420738220214844 | KNN Loss: 4.382491588592529 | BCE Loss: 1.0382468700408936\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 5.39621114730835 | KNN Loss: 4.3712592124938965 | BCE Loss: 1.0249519348144531\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 5.396481513977051 | KNN Loss: 4.370197772979736 | BCE Loss: 1.0262839794158936\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 5.426237106323242 | KNN Loss: 4.386247634887695 | BCE Loss: 1.0399894714355469\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 5.398006439208984 | KNN Loss: 4.364704132080078 | BCE Loss: 1.0333023071289062\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 5.389259338378906 | KNN Loss: 4.344644069671631 | BCE Loss: 1.0446155071258545\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 5.432528972625732 | KNN Loss: 4.406186103820801 | BCE Loss: 1.0263429880142212\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 5.397322654724121 | KNN Loss: 4.401960849761963 | BCE Loss: 0.9953619837760925\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 5.474702835083008 | KNN Loss: 4.408365726470947 | BCE Loss: 1.0663371086120605\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 5.377472877502441 | KNN Loss: 4.361136436462402 | BCE Loss: 1.016336441040039\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 5.419395446777344 | KNN Loss: 4.393406867980957 | BCE Loss: 1.0259883403778076\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 5.359624862670898 | KNN Loss: 4.358865261077881 | BCE Loss: 1.0007598400115967\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 5.382353782653809 | KNN Loss: 4.33548641204834 | BCE Loss: 1.0468673706054688\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 5.448719024658203 | KNN Loss: 4.414816379547119 | BCE Loss: 1.033902883529663\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 5.429335594177246 | KNN Loss: 4.433009147644043 | BCE Loss: 0.9963264465332031\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 5.3709306716918945 | KNN Loss: 4.365622043609619 | BCE Loss: 1.0053086280822754\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 5.417743682861328 | KNN Loss: 4.385220050811768 | BCE Loss: 1.032523512840271\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 5.402100563049316 | KNN Loss: 4.363544464111328 | BCE Loss: 1.0385563373565674\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 5.412197113037109 | KNN Loss: 4.381130695343018 | BCE Loss: 1.0310664176940918\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 5.397555351257324 | KNN Loss: 4.395386219024658 | BCE Loss: 1.002168893814087\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 5.42171573638916 | KNN Loss: 4.3835296630859375 | BCE Loss: 1.0381860733032227\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 5.403748989105225 | KNN Loss: 4.361660003662109 | BCE Loss: 1.0420888662338257\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 5.41912841796875 | KNN Loss: 4.355712890625 | BCE Loss: 1.063415288925171\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 5.402585029602051 | KNN Loss: 4.359442234039307 | BCE Loss: 1.0431427955627441\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 5.401137351989746 | KNN Loss: 4.351077079772949 | BCE Loss: 1.0500602722167969\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 5.411070346832275 | KNN Loss: 4.367486476898193 | BCE Loss: 1.043583869934082\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 5.4040937423706055 | KNN Loss: 4.369579792022705 | BCE Loss: 1.0345139503479004\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 5.352697372436523 | KNN Loss: 4.356564521789551 | BCE Loss: 0.9961329102516174\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 5.399921894073486 | KNN Loss: 4.372124671936035 | BCE Loss: 1.0277973413467407\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 5.389200210571289 | KNN Loss: 4.377827167510986 | BCE Loss: 1.0113732814788818\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 5.363495349884033 | KNN Loss: 4.342298984527588 | BCE Loss: 1.0211964845657349\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 5.414651393890381 | KNN Loss: 4.366270065307617 | BCE Loss: 1.0483813285827637\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 5.389001846313477 | KNN Loss: 4.355289459228516 | BCE Loss: 1.0337121486663818\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 5.3939595222473145 | KNN Loss: 4.369266510009766 | BCE Loss: 1.0246930122375488\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 5.391822814941406 | KNN Loss: 4.365565776824951 | BCE Loss: 1.0262572765350342\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 5.461832523345947 | KNN Loss: 4.437546730041504 | BCE Loss: 1.0242857933044434\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 5.371608734130859 | KNN Loss: 4.366523742675781 | BCE Loss: 1.005084753036499\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 5.349741458892822 | KNN Loss: 4.361936569213867 | BCE Loss: 0.9878048896789551\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 5.409297466278076 | KNN Loss: 4.3770599365234375 | BCE Loss: 1.0322374105453491\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 5.412793159484863 | KNN Loss: 4.371819496154785 | BCE Loss: 1.040973424911499\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 5.413173675537109 | KNN Loss: 4.391499996185303 | BCE Loss: 1.0216736793518066\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 5.433906555175781 | KNN Loss: 4.404404163360596 | BCE Loss: 1.0295023918151855\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 5.42244815826416 | KNN Loss: 4.388693809509277 | BCE Loss: 1.0337543487548828\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 5.4574689865112305 | KNN Loss: 4.439914226531982 | BCE Loss: 1.0175549983978271\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 5.414525985717773 | KNN Loss: 4.367627143859863 | BCE Loss: 1.0468987226486206\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 5.442781448364258 | KNN Loss: 4.406982898712158 | BCE Loss: 1.0357986688613892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 5.400032997131348 | KNN Loss: 4.377598285675049 | BCE Loss: 1.0224347114562988\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 5.382449626922607 | KNN Loss: 4.363124847412109 | BCE Loss: 1.019324779510498\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 5.414988040924072 | KNN Loss: 4.399794101715088 | BCE Loss: 1.015194058418274\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 5.438265800476074 | KNN Loss: 4.3769989013671875 | BCE Loss: 1.0612666606903076\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 5.432356834411621 | KNN Loss: 4.434698104858398 | BCE Loss: 0.9976584911346436\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 5.386996269226074 | KNN Loss: 4.361823558807373 | BCE Loss: 1.0251728296279907\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 5.396116256713867 | KNN Loss: 4.374709129333496 | BCE Loss: 1.0214072465896606\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 5.388461589813232 | KNN Loss: 4.357341766357422 | BCE Loss: 1.031119704246521\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 5.3616509437561035 | KNN Loss: 4.352533340454102 | BCE Loss: 1.009117603302002\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 5.428648948669434 | KNN Loss: 4.398135662078857 | BCE Loss: 1.0305132865905762\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 5.4018635749816895 | KNN Loss: 4.352987766265869 | BCE Loss: 1.0488758087158203\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 5.400476455688477 | KNN Loss: 4.361745357513428 | BCE Loss: 1.0387310981750488\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 5.383567810058594 | KNN Loss: 4.373068332672119 | BCE Loss: 1.0104992389678955\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 5.418514728546143 | KNN Loss: 4.366179943084717 | BCE Loss: 1.0523349046707153\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 5.420520305633545 | KNN Loss: 4.359325408935547 | BCE Loss: 1.0611950159072876\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 5.419196605682373 | KNN Loss: 4.37952184677124 | BCE Loss: 1.0396748781204224\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 5.4366536140441895 | KNN Loss: 4.364933013916016 | BCE Loss: 1.0717206001281738\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 5.357796669006348 | KNN Loss: 4.349684715270996 | BCE Loss: 1.0081119537353516\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 5.367758274078369 | KNN Loss: 4.352202892303467 | BCE Loss: 1.0155552625656128\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 5.412859916687012 | KNN Loss: 4.383510589599609 | BCE Loss: 1.029349446296692\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 5.384387016296387 | KNN Loss: 4.373425006866455 | BCE Loss: 1.0109621286392212\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 5.3882293701171875 | KNN Loss: 4.392444133758545 | BCE Loss: 0.995785117149353\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 5.4151811599731445 | KNN Loss: 4.396448135375977 | BCE Loss: 1.018733263015747\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 5.434906005859375 | KNN Loss: 4.400750637054443 | BCE Loss: 1.0341556072235107\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 5.401304244995117 | KNN Loss: 4.361091136932373 | BCE Loss: 1.040212869644165\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 5.3829755783081055 | KNN Loss: 4.3673176765441895 | BCE Loss: 1.0156577825546265\n",
      "Epoch    91: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 5.380448341369629 | KNN Loss: 4.356808185577393 | BCE Loss: 1.0236399173736572\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 5.437044143676758 | KNN Loss: 4.409864902496338 | BCE Loss: 1.027179479598999\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 5.4276299476623535 | KNN Loss: 4.366847515106201 | BCE Loss: 1.060782551765442\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 5.3811798095703125 | KNN Loss: 4.337527751922607 | BCE Loss: 1.0436522960662842\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 5.37214469909668 | KNN Loss: 4.350597858428955 | BCE Loss: 1.0215470790863037\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 5.366508483886719 | KNN Loss: 4.339229106903076 | BCE Loss: 1.0272791385650635\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 5.378325939178467 | KNN Loss: 4.356283664703369 | BCE Loss: 1.0220422744750977\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 5.404151916503906 | KNN Loss: 4.377260684967041 | BCE Loss: 1.0268909931182861\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 5.4461989402771 | KNN Loss: 4.413480758666992 | BCE Loss: 1.0327180624008179\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 5.370226860046387 | KNN Loss: 4.353203773498535 | BCE Loss: 1.0170233249664307\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 5.3862223625183105 | KNN Loss: 4.346739768981934 | BCE Loss: 1.039482593536377\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 5.365785121917725 | KNN Loss: 4.35672664642334 | BCE Loss: 1.0090585947036743\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 5.415310859680176 | KNN Loss: 4.387610912322998 | BCE Loss: 1.0277001857757568\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 5.378507614135742 | KNN Loss: 4.354139804840088 | BCE Loss: 1.0243678092956543\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 5.409285545349121 | KNN Loss: 4.3741302490234375 | BCE Loss: 1.035155177116394\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 5.375470161437988 | KNN Loss: 4.349061012268066 | BCE Loss: 1.026409387588501\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 5.41456413269043 | KNN Loss: 4.383923053741455 | BCE Loss: 1.0306413173675537\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 5.366031646728516 | KNN Loss: 4.345939636230469 | BCE Loss: 1.0200918912887573\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 5.385191917419434 | KNN Loss: 4.3721699714660645 | BCE Loss: 1.0130219459533691\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 5.445242881774902 | KNN Loss: 4.403104782104492 | BCE Loss: 1.0421380996704102\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 5.3842573165893555 | KNN Loss: 4.363174915313721 | BCE Loss: 1.0210821628570557\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 5.395259857177734 | KNN Loss: 4.381743431091309 | BCE Loss: 1.0135164260864258\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 5.386409759521484 | KNN Loss: 4.380726337432861 | BCE Loss: 1.0056836605072021\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 5.3853654861450195 | KNN Loss: 4.35655403137207 | BCE Loss: 1.0288114547729492\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 5.396202087402344 | KNN Loss: 4.356683731079102 | BCE Loss: 1.0395184755325317\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 5.402776718139648 | KNN Loss: 4.378923416137695 | BCE Loss: 1.0238533020019531\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 5.401617527008057 | KNN Loss: 4.390544414520264 | BCE Loss: 1.0110732316970825\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 5.407349586486816 | KNN Loss: 4.390656471252441 | BCE Loss: 1.016693353652954\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 5.39467716217041 | KNN Loss: 4.379218101501465 | BCE Loss: 1.0154590606689453\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 5.416331768035889 | KNN Loss: 4.383567810058594 | BCE Loss: 1.0327640771865845\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 5.372725486755371 | KNN Loss: 4.331874370574951 | BCE Loss: 1.040851354598999\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 5.429737567901611 | KNN Loss: 4.372952938079834 | BCE Loss: 1.0567846298217773\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 5.394767761230469 | KNN Loss: 4.392390727996826 | BCE Loss: 1.0023772716522217\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 5.414651870727539 | KNN Loss: 4.373364448547363 | BCE Loss: 1.0412876605987549\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 5.436038017272949 | KNN Loss: 4.41194486618042 | BCE Loss: 1.0240929126739502\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 5.417063236236572 | KNN Loss: 4.406318664550781 | BCE Loss: 1.010744571685791\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 5.41166353225708 | KNN Loss: 4.3661789894104 | BCE Loss: 1.0454845428466797\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 5.358957290649414 | KNN Loss: 4.356961250305176 | BCE Loss: 1.0019961595535278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 5.3568806648254395 | KNN Loss: 4.362505912780762 | BCE Loss: 0.9943749308586121\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 5.423254013061523 | KNN Loss: 4.384307384490967 | BCE Loss: 1.0389467477798462\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 5.373359203338623 | KNN Loss: 4.3711724281311035 | BCE Loss: 1.002186894416809\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 5.384055137634277 | KNN Loss: 4.355935096740723 | BCE Loss: 1.0281202793121338\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 5.454859733581543 | KNN Loss: 4.401942253112793 | BCE Loss: 1.05291748046875\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 5.398942947387695 | KNN Loss: 4.381374359130859 | BCE Loss: 1.017568826675415\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 5.4126410484313965 | KNN Loss: 4.365908145904541 | BCE Loss: 1.046732783317566\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 5.38692569732666 | KNN Loss: 4.355402946472168 | BCE Loss: 1.031522512435913\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 5.3976545333862305 | KNN Loss: 4.364941596984863 | BCE Loss: 1.0327131748199463\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 5.461543083190918 | KNN Loss: 4.408356666564941 | BCE Loss: 1.0531866550445557\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 5.370673179626465 | KNN Loss: 4.352018356323242 | BCE Loss: 1.0186548233032227\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 5.404207229614258 | KNN Loss: 4.36050271987915 | BCE Loss: 1.0437043905258179\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 5.387176513671875 | KNN Loss: 4.376559257507324 | BCE Loss: 1.0106170177459717\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 5.3457722663879395 | KNN Loss: 4.334720134735107 | BCE Loss: 1.0110520124435425\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 5.4094343185424805 | KNN Loss: 4.371187210083008 | BCE Loss: 1.0382468700408936\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 5.422905921936035 | KNN Loss: 4.393375873565674 | BCE Loss: 1.0295299291610718\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 5.35056734085083 | KNN Loss: 4.355776786804199 | BCE Loss: 0.9947903752326965\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 5.353023052215576 | KNN Loss: 4.361344814300537 | BCE Loss: 0.9916781187057495\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 5.4148101806640625 | KNN Loss: 4.372581958770752 | BCE Loss: 1.0422279834747314\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 5.404300212860107 | KNN Loss: 4.367551803588867 | BCE Loss: 1.0367482900619507\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 5.394047737121582 | KNN Loss: 4.37472677230835 | BCE Loss: 1.0193207263946533\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 5.403236389160156 | KNN Loss: 4.368535995483398 | BCE Loss: 1.034700632095337\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 5.42973518371582 | KNN Loss: 4.363070011138916 | BCE Loss: 1.0666654109954834\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 5.388808250427246 | KNN Loss: 4.369593143463135 | BCE Loss: 1.0192153453826904\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 5.391985893249512 | KNN Loss: 4.347550392150879 | BCE Loss: 1.0444352626800537\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 5.378006935119629 | KNN Loss: 4.362578392028809 | BCE Loss: 1.0154285430908203\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 5.34682035446167 | KNN Loss: 4.341402053833008 | BCE Loss: 1.0054184198379517\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 5.425673007965088 | KNN Loss: 4.3883795738220215 | BCE Loss: 1.0372934341430664\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 5.458925247192383 | KNN Loss: 4.4354047775268555 | BCE Loss: 1.0235204696655273\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 5.4056620597839355 | KNN Loss: 4.3726630210876465 | BCE Loss: 1.032999038696289\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 5.377388000488281 | KNN Loss: 4.380105495452881 | BCE Loss: 0.9972825050354004\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 5.429977893829346 | KNN Loss: 4.400720596313477 | BCE Loss: 1.0292571783065796\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 5.390064239501953 | KNN Loss: 4.380612373352051 | BCE Loss: 1.0094516277313232\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 5.365021705627441 | KNN Loss: 4.349391937255859 | BCE Loss: 1.015629768371582\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 5.411770820617676 | KNN Loss: 4.3732829093933105 | BCE Loss: 1.0384876728057861\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 5.390138149261475 | KNN Loss: 4.361487865447998 | BCE Loss: 1.0286502838134766\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 5.396994590759277 | KNN Loss: 4.386667728424072 | BCE Loss: 1.010326623916626\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 5.415519714355469 | KNN Loss: 4.3871750831604 | BCE Loss: 1.0283445119857788\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 5.423984527587891 | KNN Loss: 4.382762432098389 | BCE Loss: 1.0412222146987915\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 5.372226238250732 | KNN Loss: 4.340194225311279 | BCE Loss: 1.0320320129394531\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 5.367850303649902 | KNN Loss: 4.348016262054443 | BCE Loss: 1.0198338031768799\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 5.3863325119018555 | KNN Loss: 4.360198020935059 | BCE Loss: 1.0261344909667969\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 5.351682186126709 | KNN Loss: 4.3352952003479 | BCE Loss: 1.0163871049880981\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 5.381223678588867 | KNN Loss: 4.354973316192627 | BCE Loss: 1.0262504816055298\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 5.397500038146973 | KNN Loss: 4.387018203735352 | BCE Loss: 1.010481834411621\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 5.3745808601379395 | KNN Loss: 4.356411457061768 | BCE Loss: 1.0181694030761719\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 5.389120101928711 | KNN Loss: 4.356966018676758 | BCE Loss: 1.0321540832519531\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 5.386540412902832 | KNN Loss: 4.3535003662109375 | BCE Loss: 1.0330400466918945\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 5.384803295135498 | KNN Loss: 4.380270481109619 | BCE Loss: 1.0045326948165894\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 5.4510650634765625 | KNN Loss: 4.375396251678467 | BCE Loss: 1.0756689310073853\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 5.39124870300293 | KNN Loss: 4.389570236206055 | BCE Loss: 1.0016783475875854\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 5.366625785827637 | KNN Loss: 4.3670430183410645 | BCE Loss: 0.9995830059051514\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 5.35968542098999 | KNN Loss: 4.369071960449219 | BCE Loss: 0.990613579750061\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 5.37071418762207 | KNN Loss: 4.355442523956299 | BCE Loss: 1.0152714252471924\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 5.363701820373535 | KNN Loss: 4.371501445770264 | BCE Loss: 0.9922002553939819\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 5.395947456359863 | KNN Loss: 4.351800441741943 | BCE Loss: 1.044147253036499\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 5.353858947753906 | KNN Loss: 4.338228702545166 | BCE Loss: 1.0156304836273193\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 5.402333736419678 | KNN Loss: 4.361221790313721 | BCE Loss: 1.041111946105957\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 5.397946357727051 | KNN Loss: 4.357003211975098 | BCE Loss: 1.0409431457519531\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 5.375608921051025 | KNN Loss: 4.347978115081787 | BCE Loss: 1.0276306867599487\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 5.401592254638672 | KNN Loss: 4.387576580047607 | BCE Loss: 1.014015793800354\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 5.353671550750732 | KNN Loss: 4.354133605957031 | BCE Loss: 0.9995378851890564\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 5.381567001342773 | KNN Loss: 4.344655990600586 | BCE Loss: 1.0369112491607666\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 5.358301639556885 | KNN Loss: 4.336723804473877 | BCE Loss: 1.0215777158737183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 5.371675491333008 | KNN Loss: 4.346179485321045 | BCE Loss: 1.0254961252212524\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 5.365689754486084 | KNN Loss: 4.352165699005127 | BCE Loss: 1.0135241746902466\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 5.394217491149902 | KNN Loss: 4.344084739685059 | BCE Loss: 1.0501327514648438\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 5.407317161560059 | KNN Loss: 4.36619234085083 | BCE Loss: 1.041124939918518\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 5.41560173034668 | KNN Loss: 4.379758834838867 | BCE Loss: 1.0358431339263916\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 5.345952987670898 | KNN Loss: 4.336705684661865 | BCE Loss: 1.0092475414276123\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 5.355257034301758 | KNN Loss: 4.335052967071533 | BCE Loss: 1.0202040672302246\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 5.379927635192871 | KNN Loss: 4.373350620269775 | BCE Loss: 1.0065767765045166\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 5.420610427856445 | KNN Loss: 4.393221378326416 | BCE Loss: 1.0273888111114502\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 5.427475929260254 | KNN Loss: 4.38552188873291 | BCE Loss: 1.0419538021087646\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 5.3928022384643555 | KNN Loss: 4.373482704162598 | BCE Loss: 1.019319772720337\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 5.356923580169678 | KNN Loss: 4.3627824783325195 | BCE Loss: 0.9941412210464478\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 5.365866661071777 | KNN Loss: 4.343434810638428 | BCE Loss: 1.0224318504333496\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 5.410260200500488 | KNN Loss: 4.374539375305176 | BCE Loss: 1.0357208251953125\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 5.4178466796875 | KNN Loss: 4.3934454917907715 | BCE Loss: 1.024401307106018\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 5.378646373748779 | KNN Loss: 4.3460469245910645 | BCE Loss: 1.0325994491577148\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 5.404908180236816 | KNN Loss: 4.351498603820801 | BCE Loss: 1.0534093379974365\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 5.381544589996338 | KNN Loss: 4.3467698097229 | BCE Loss: 1.034774661064148\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 5.380734920501709 | KNN Loss: 4.355595111846924 | BCE Loss: 1.0251398086547852\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 5.350162506103516 | KNN Loss: 4.344170093536377 | BCE Loss: 1.0059925317764282\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 5.364957809448242 | KNN Loss: 4.360757827758789 | BCE Loss: 1.004199743270874\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 5.3997392654418945 | KNN Loss: 4.36526346206665 | BCE Loss: 1.0344760417938232\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 5.408412933349609 | KNN Loss: 4.386530876159668 | BCE Loss: 1.0218819379806519\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 5.429126739501953 | KNN Loss: 4.375251293182373 | BCE Loss: 1.053875207901001\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 5.372904300689697 | KNN Loss: 4.343257904052734 | BCE Loss: 1.0296462774276733\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 5.384191513061523 | KNN Loss: 4.347531318664551 | BCE Loss: 1.036660075187683\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 5.436213493347168 | KNN Loss: 4.396510124206543 | BCE Loss: 1.039703369140625\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 5.3620734214782715 | KNN Loss: 4.336214065551758 | BCE Loss: 1.0258593559265137\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 5.386231422424316 | KNN Loss: 4.358004570007324 | BCE Loss: 1.0282268524169922\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 5.408150672912598 | KNN Loss: 4.364694595336914 | BCE Loss: 1.043455958366394\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 5.37380313873291 | KNN Loss: 4.3311076164245605 | BCE Loss: 1.0426955223083496\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 5.354742050170898 | KNN Loss: 4.342964172363281 | BCE Loss: 1.0117778778076172\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 5.384217262268066 | KNN Loss: 4.349721908569336 | BCE Loss: 1.0344955921173096\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 5.36759090423584 | KNN Loss: 4.356640815734863 | BCE Loss: 1.0109502077102661\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 5.354725360870361 | KNN Loss: 4.336973667144775 | BCE Loss: 1.017751693725586\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 5.345876216888428 | KNN Loss: 4.3573808670043945 | BCE Loss: 0.988495409488678\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 5.361493110656738 | KNN Loss: 4.347543239593506 | BCE Loss: 1.0139498710632324\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 5.388078212738037 | KNN Loss: 4.367044448852539 | BCE Loss: 1.021033763885498\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 5.3629984855651855 | KNN Loss: 4.362828254699707 | BCE Loss: 1.0001702308654785\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 5.398332595825195 | KNN Loss: 4.372506141662598 | BCE Loss: 1.0258264541625977\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 5.429009914398193 | KNN Loss: 4.36367654800415 | BCE Loss: 1.065333366394043\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 5.379154205322266 | KNN Loss: 4.379206657409668 | BCE Loss: 0.9999476671218872\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 5.368078231811523 | KNN Loss: 4.345239639282227 | BCE Loss: 1.0228385925292969\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 5.372047424316406 | KNN Loss: 4.361153602600098 | BCE Loss: 1.0108939409255981\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 5.362133979797363 | KNN Loss: 4.353578090667725 | BCE Loss: 1.0085558891296387\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 5.407734394073486 | KNN Loss: 4.382785797119141 | BCE Loss: 1.0249485969543457\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 5.402739524841309 | KNN Loss: 4.363152980804443 | BCE Loss: 1.0395863056182861\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 5.366903781890869 | KNN Loss: 4.339478492736816 | BCE Loss: 1.0274251699447632\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 5.377371788024902 | KNN Loss: 4.342260360717773 | BCE Loss: 1.035111665725708\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 5.370174407958984 | KNN Loss: 4.350453853607178 | BCE Loss: 1.0197205543518066\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 5.361138343811035 | KNN Loss: 4.35728120803833 | BCE Loss: 1.003857135772705\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 5.351781845092773 | KNN Loss: 4.35359525680542 | BCE Loss: 0.9981863498687744\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 5.3750386238098145 | KNN Loss: 4.352227210998535 | BCE Loss: 1.0228114128112793\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 5.356995582580566 | KNN Loss: 4.333538055419922 | BCE Loss: 1.0234575271606445\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 5.3911542892456055 | KNN Loss: 4.3617472648620605 | BCE Loss: 1.0294067859649658\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 5.348102569580078 | KNN Loss: 4.341868877410889 | BCE Loss: 1.0062334537506104\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 5.362649917602539 | KNN Loss: 4.374157905578613 | BCE Loss: 0.9884921312332153\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 5.341744899749756 | KNN Loss: 4.341853141784668 | BCE Loss: 0.9998919367790222\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 5.346056938171387 | KNN Loss: 4.363804340362549 | BCE Loss: 0.9822525978088379\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 5.373880863189697 | KNN Loss: 4.354811668395996 | BCE Loss: 1.0190693140029907\n",
      "Epoch   118: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 5.445769786834717 | KNN Loss: 4.4168195724487305 | BCE Loss: 1.0289502143859863\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 5.343316078186035 | KNN Loss: 4.3271660804748535 | BCE Loss: 1.0161502361297607\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 5.3634467124938965 | KNN Loss: 4.32631254196167 | BCE Loss: 1.0371342897415161\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 5.353567600250244 | KNN Loss: 4.330493927001953 | BCE Loss: 1.0230737924575806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 5.3619585037231445 | KNN Loss: 4.334133625030518 | BCE Loss: 1.027824878692627\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 5.364171028137207 | KNN Loss: 4.351348876953125 | BCE Loss: 1.012822151184082\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 5.396387577056885 | KNN Loss: 4.346325397491455 | BCE Loss: 1.0500621795654297\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 5.381161212921143 | KNN Loss: 4.363773822784424 | BCE Loss: 1.0173872709274292\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 5.381629467010498 | KNN Loss: 4.378503322601318 | BCE Loss: 1.0031261444091797\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 5.409842014312744 | KNN Loss: 4.380771636962891 | BCE Loss: 1.029070496559143\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 5.420607566833496 | KNN Loss: 4.399882793426514 | BCE Loss: 1.0207245349884033\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 5.368844032287598 | KNN Loss: 4.324952125549316 | BCE Loss: 1.0438921451568604\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 5.352566719055176 | KNN Loss: 4.340043544769287 | BCE Loss: 1.0125234127044678\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 5.373735427856445 | KNN Loss: 4.343532085418701 | BCE Loss: 1.0302035808563232\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 5.343289375305176 | KNN Loss: 4.333815574645996 | BCE Loss: 1.0094740390777588\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 5.4353108406066895 | KNN Loss: 4.375436305999756 | BCE Loss: 1.0598745346069336\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 5.417487621307373 | KNN Loss: 4.376160621643066 | BCE Loss: 1.0413269996643066\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 5.386679172515869 | KNN Loss: 4.377020359039307 | BCE Loss: 1.0096588134765625\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 5.3470988273620605 | KNN Loss: 4.334776878356934 | BCE Loss: 1.0123218297958374\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 5.419523239135742 | KNN Loss: 4.361809730529785 | BCE Loss: 1.057713270187378\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 5.358728408813477 | KNN Loss: 4.352745056152344 | BCE Loss: 1.0059834718704224\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 5.369046211242676 | KNN Loss: 4.355181694030762 | BCE Loss: 1.0138647556304932\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 5.343382835388184 | KNN Loss: 4.360280990600586 | BCE Loss: 0.9831017255783081\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 5.322572708129883 | KNN Loss: 4.32789945602417 | BCE Loss: 0.9946734309196472\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 5.396785259246826 | KNN Loss: 4.3622212409973145 | BCE Loss: 1.0345638990402222\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 5.3613972663879395 | KNN Loss: 4.360924243927002 | BCE Loss: 1.0004730224609375\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 5.372581958770752 | KNN Loss: 4.344971179962158 | BCE Loss: 1.0276107788085938\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 5.393486976623535 | KNN Loss: 4.369988441467285 | BCE Loss: 1.023498773574829\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 5.378904342651367 | KNN Loss: 4.353780269622803 | BCE Loss: 1.0251238346099854\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 5.369545936584473 | KNN Loss: 4.331599235534668 | BCE Loss: 1.0379468202590942\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 5.387371063232422 | KNN Loss: 4.360379219055176 | BCE Loss: 1.026991844177246\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 5.364378452301025 | KNN Loss: 4.342883586883545 | BCE Loss: 1.02149498462677\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 5.413102149963379 | KNN Loss: 4.379921913146973 | BCE Loss: 1.0331799983978271\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 5.354607582092285 | KNN Loss: 4.331406116485596 | BCE Loss: 1.0232012271881104\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 5.391101837158203 | KNN Loss: 4.363462924957275 | BCE Loss: 1.0276389122009277\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 5.359804153442383 | KNN Loss: 4.322686672210693 | BCE Loss: 1.0371174812316895\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 5.369607448577881 | KNN Loss: 4.353996276855469 | BCE Loss: 1.0156112909317017\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 5.364291191101074 | KNN Loss: 4.330819606781006 | BCE Loss: 1.0334713459014893\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 5.385516166687012 | KNN Loss: 4.366028308868408 | BCE Loss: 1.0194876194000244\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 5.34397029876709 | KNN Loss: 4.3351664543151855 | BCE Loss: 1.0088039636611938\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 5.391819477081299 | KNN Loss: 4.367890357971191 | BCE Loss: 1.0239291191101074\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 5.362789154052734 | KNN Loss: 4.350231647491455 | BCE Loss: 1.0125575065612793\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 5.3824944496154785 | KNN Loss: 4.339872360229492 | BCE Loss: 1.0426219701766968\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 5.384305953979492 | KNN Loss: 4.362752914428711 | BCE Loss: 1.0215532779693604\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 5.364109992980957 | KNN Loss: 4.332179546356201 | BCE Loss: 1.0319303274154663\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 5.333742618560791 | KNN Loss: 4.3137946128845215 | BCE Loss: 1.01994788646698\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 5.4245710372924805 | KNN Loss: 4.390942573547363 | BCE Loss: 1.0336284637451172\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 5.402751445770264 | KNN Loss: 4.368756294250488 | BCE Loss: 1.033995270729065\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 5.328630447387695 | KNN Loss: 4.330330848693848 | BCE Loss: 0.9982998371124268\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 5.3632049560546875 | KNN Loss: 4.347391128540039 | BCE Loss: 1.0158135890960693\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 5.375812530517578 | KNN Loss: 4.3300676345825195 | BCE Loss: 1.045744776725769\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 5.40158748626709 | KNN Loss: 4.370870113372803 | BCE Loss: 1.030717134475708\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 5.371699333190918 | KNN Loss: 4.349432945251465 | BCE Loss: 1.0222663879394531\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 5.3576507568359375 | KNN Loss: 4.343600273132324 | BCE Loss: 1.0140506029129028\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 5.367841720581055 | KNN Loss: 4.338631629943848 | BCE Loss: 1.0292103290557861\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 5.392580509185791 | KNN Loss: 4.350142955780029 | BCE Loss: 1.0424375534057617\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 5.345892906188965 | KNN Loss: 4.322561264038086 | BCE Loss: 1.023331880569458\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 5.331643104553223 | KNN Loss: 4.3218231201171875 | BCE Loss: 1.0098198652267456\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 5.317970275878906 | KNN Loss: 4.318338394165039 | BCE Loss: 0.9996317625045776\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 5.36643123626709 | KNN Loss: 4.338163375854492 | BCE Loss: 1.0282680988311768\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 5.343640327453613 | KNN Loss: 4.333491802215576 | BCE Loss: 1.010148525238037\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 5.349216461181641 | KNN Loss: 4.3215813636779785 | BCE Loss: 1.027634859085083\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 5.367902755737305 | KNN Loss: 4.314226150512695 | BCE Loss: 1.0536768436431885\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 5.394560813903809 | KNN Loss: 4.351369857788086 | BCE Loss: 1.0431911945343018\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 5.386626720428467 | KNN Loss: 4.3537068367004395 | BCE Loss: 1.0329198837280273\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 5.405205726623535 | KNN Loss: 4.376241207122803 | BCE Loss: 1.0289647579193115\n",
      "Epoch   129: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 5.329294204711914 | KNN Loss: 4.330167770385742 | BCE Loss: 0.9991265535354614\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 5.327847957611084 | KNN Loss: 4.337884426116943 | BCE Loss: 0.9899635314941406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 5.387449264526367 | KNN Loss: 4.358529567718506 | BCE Loss: 1.0289195775985718\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 5.385261535644531 | KNN Loss: 4.359062671661377 | BCE Loss: 1.0261991024017334\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 5.379024028778076 | KNN Loss: 4.3583598136901855 | BCE Loss: 1.0206643342971802\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 5.367504119873047 | KNN Loss: 4.338450908660889 | BCE Loss: 1.0290533304214478\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 5.407863140106201 | KNN Loss: 4.360833644866943 | BCE Loss: 1.0470296144485474\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 5.4156365394592285 | KNN Loss: 4.346874713897705 | BCE Loss: 1.0687617063522339\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 5.3751139640808105 | KNN Loss: 4.358725547790527 | BCE Loss: 1.0163884162902832\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 5.425351142883301 | KNN Loss: 4.366213321685791 | BCE Loss: 1.0591380596160889\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 5.355302333831787 | KNN Loss: 4.351202964782715 | BCE Loss: 1.0040993690490723\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 5.3645243644714355 | KNN Loss: 4.344188690185547 | BCE Loss: 1.0203356742858887\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 5.427051544189453 | KNN Loss: 4.386429786682129 | BCE Loss: 1.0406217575073242\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 5.3899993896484375 | KNN Loss: 4.3766350746154785 | BCE Loss: 1.0133640766143799\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 5.414603233337402 | KNN Loss: 4.3704962730407715 | BCE Loss: 1.0441069602966309\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 5.398979663848877 | KNN Loss: 4.3832879066467285 | BCE Loss: 1.0156917572021484\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 5.345451831817627 | KNN Loss: 4.356888294219971 | BCE Loss: 0.9885635375976562\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 5.343809127807617 | KNN Loss: 4.3278985023498535 | BCE Loss: 1.0159106254577637\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 5.348923683166504 | KNN Loss: 4.341879844665527 | BCE Loss: 1.0070440769195557\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 5.359099864959717 | KNN Loss: 4.332559108734131 | BCE Loss: 1.0265406370162964\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 5.334194183349609 | KNN Loss: 4.322657585144043 | BCE Loss: 1.0115365982055664\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 5.356991767883301 | KNN Loss: 4.322593688964844 | BCE Loss: 1.034397840499878\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 5.358194351196289 | KNN Loss: 4.3299970626831055 | BCE Loss: 1.028197169303894\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 5.38637113571167 | KNN Loss: 4.3528618812561035 | BCE Loss: 1.0335092544555664\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 5.37628173828125 | KNN Loss: 4.345667362213135 | BCE Loss: 1.0306143760681152\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 5.364304065704346 | KNN Loss: 4.332801818847656 | BCE Loss: 1.0315022468566895\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 5.334906578063965 | KNN Loss: 4.336709022521973 | BCE Loss: 0.9981974363327026\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 5.368697166442871 | KNN Loss: 4.361019611358643 | BCE Loss: 1.007677435874939\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 5.334448337554932 | KNN Loss: 4.337238311767578 | BCE Loss: 0.9972100257873535\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 5.369117736816406 | KNN Loss: 4.35120964050293 | BCE Loss: 1.0179083347320557\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 5.350831031799316 | KNN Loss: 4.338207244873047 | BCE Loss: 1.0126240253448486\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 5.3986968994140625 | KNN Loss: 4.362732410430908 | BCE Loss: 1.0359644889831543\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 5.373947620391846 | KNN Loss: 4.371922492980957 | BCE Loss: 1.0020251274108887\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 5.385759353637695 | KNN Loss: 4.359460830688477 | BCE Loss: 1.0262985229492188\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 5.3756303787231445 | KNN Loss: 4.342498779296875 | BCE Loss: 1.0331313610076904\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 5.389456748962402 | KNN Loss: 4.370340347290039 | BCE Loss: 1.0191162824630737\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 5.371609687805176 | KNN Loss: 4.333189010620117 | BCE Loss: 1.0384209156036377\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 5.40036153793335 | KNN Loss: 4.374622821807861 | BCE Loss: 1.0257385969161987\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 5.37276554107666 | KNN Loss: 4.343721866607666 | BCE Loss: 1.0290435552597046\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 5.3732500076293945 | KNN Loss: 4.337096691131592 | BCE Loss: 1.0361530780792236\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 5.369160175323486 | KNN Loss: 4.345392227172852 | BCE Loss: 1.0237679481506348\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 5.40880012512207 | KNN Loss: 4.387619972229004 | BCE Loss: 1.021180272102356\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 5.374685764312744 | KNN Loss: 4.34204626083374 | BCE Loss: 1.032639503479004\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 5.334246635437012 | KNN Loss: 4.333889484405518 | BCE Loss: 1.0003573894500732\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 5.359687805175781 | KNN Loss: 4.353084564208984 | BCE Loss: 1.0066032409667969\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 5.31872034072876 | KNN Loss: 4.325674533843994 | BCE Loss: 0.9930456280708313\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 5.343471050262451 | KNN Loss: 4.312826156616211 | BCE Loss: 1.0306448936462402\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 5.376208305358887 | KNN Loss: 4.376532077789307 | BCE Loss: 0.9996763467788696\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 5.370318412780762 | KNN Loss: 4.338271617889404 | BCE Loss: 1.0320467948913574\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 5.397643089294434 | KNN Loss: 4.391474723815918 | BCE Loss: 1.0061686038970947\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 5.430637359619141 | KNN Loss: 4.348121643066406 | BCE Loss: 1.0825154781341553\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 5.339540958404541 | KNN Loss: 4.335193157196045 | BCE Loss: 1.004347801208496\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 5.434178829193115 | KNN Loss: 4.393786907196045 | BCE Loss: 1.0403919219970703\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 5.404973030090332 | KNN Loss: 4.376160621643066 | BCE Loss: 1.0288126468658447\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 5.394189357757568 | KNN Loss: 4.3642072677612305 | BCE Loss: 1.029982089996338\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 5.388416290283203 | KNN Loss: 4.360945701599121 | BCE Loss: 1.0274708271026611\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 5.351572036743164 | KNN Loss: 4.343649387359619 | BCE Loss: 1.0079224109649658\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 5.422372817993164 | KNN Loss: 4.374512195587158 | BCE Loss: 1.0478607416152954\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 5.427402973175049 | KNN Loss: 4.3651604652404785 | BCE Loss: 1.0622426271438599\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 5.374941825866699 | KNN Loss: 4.328120708465576 | BCE Loss: 1.046820878982544\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 5.346004962921143 | KNN Loss: 4.3350138664245605 | BCE Loss: 1.010991096496582\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 5.338583946228027 | KNN Loss: 4.311432361602783 | BCE Loss: 1.0271515846252441\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 5.3796586990356445 | KNN Loss: 4.356238842010498 | BCE Loss: 1.0234196186065674\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 5.363356590270996 | KNN Loss: 4.3580498695373535 | BCE Loss: 1.0053064823150635\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 5.4270501136779785 | KNN Loss: 4.371994495391846 | BCE Loss: 1.0550557374954224\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 5.354185581207275 | KNN Loss: 4.339096546173096 | BCE Loss: 1.0150889158248901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   140: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 5.3478217124938965 | KNN Loss: 4.334627628326416 | BCE Loss: 1.0131940841674805\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 5.357454299926758 | KNN Loss: 4.319551944732666 | BCE Loss: 1.0379023551940918\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 5.425537109375 | KNN Loss: 4.387564182281494 | BCE Loss: 1.0379729270935059\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 5.379631996154785 | KNN Loss: 4.346367835998535 | BCE Loss: 1.0332640409469604\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 5.392107009887695 | KNN Loss: 4.335211277008057 | BCE Loss: 1.0568957328796387\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 5.354142189025879 | KNN Loss: 4.360055446624756 | BCE Loss: 0.9940869212150574\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 5.344489097595215 | KNN Loss: 4.339416980743408 | BCE Loss: 1.0050718784332275\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 5.399883270263672 | KNN Loss: 4.359591960906982 | BCE Loss: 1.0402915477752686\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 5.385326385498047 | KNN Loss: 4.343927383422852 | BCE Loss: 1.0413987636566162\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 5.3401031494140625 | KNN Loss: 4.331070899963379 | BCE Loss: 1.0090323686599731\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 5.37932014465332 | KNN Loss: 4.363302707672119 | BCE Loss: 1.0160174369812012\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 5.423271179199219 | KNN Loss: 4.388374328613281 | BCE Loss: 1.0348966121673584\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 5.375620365142822 | KNN Loss: 4.3697638511657715 | BCE Loss: 1.0058563947677612\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 5.362586498260498 | KNN Loss: 4.3246169090271 | BCE Loss: 1.0379695892333984\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 5.364416122436523 | KNN Loss: 4.354666233062744 | BCE Loss: 1.0097498893737793\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 5.3542022705078125 | KNN Loss: 4.333894729614258 | BCE Loss: 1.0203075408935547\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 5.371708869934082 | KNN Loss: 4.352059841156006 | BCE Loss: 1.019648790359497\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 5.353370189666748 | KNN Loss: 4.321765422821045 | BCE Loss: 1.0316047668457031\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 5.445122718811035 | KNN Loss: 4.383725643157959 | BCE Loss: 1.061396837234497\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 5.407609939575195 | KNN Loss: 4.404564380645752 | BCE Loss: 1.0030453205108643\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 5.328828811645508 | KNN Loss: 4.321468353271484 | BCE Loss: 1.0073604583740234\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 5.343542575836182 | KNN Loss: 4.360860347747803 | BCE Loss: 0.9826820492744446\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 5.371969699859619 | KNN Loss: 4.359727382659912 | BCE Loss: 1.012242317199707\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 5.3556132316589355 | KNN Loss: 4.346996784210205 | BCE Loss: 1.008616328239441\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 5.391404151916504 | KNN Loss: 4.3478899002075195 | BCE Loss: 1.0435140132904053\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 5.3480024337768555 | KNN Loss: 4.3550519943237305 | BCE Loss: 0.9929503798484802\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 5.397883892059326 | KNN Loss: 4.3617262840271 | BCE Loss: 1.0361576080322266\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 5.370675086975098 | KNN Loss: 4.37505578994751 | BCE Loss: 0.995619535446167\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 5.364302635192871 | KNN Loss: 4.339613914489746 | BCE Loss: 1.024688959121704\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 5.365474224090576 | KNN Loss: 4.345770359039307 | BCE Loss: 1.019703984260559\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 5.386481285095215 | KNN Loss: 4.355421543121338 | BCE Loss: 1.0310595035552979\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 5.349770545959473 | KNN Loss: 4.341048717498779 | BCE Loss: 1.0087215900421143\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 5.366158962249756 | KNN Loss: 4.348830223083496 | BCE Loss: 1.0173286199569702\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 5.360291481018066 | KNN Loss: 4.354852199554443 | BCE Loss: 1.0054391622543335\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 5.331813812255859 | KNN Loss: 4.328948974609375 | BCE Loss: 1.0028645992279053\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 5.380980491638184 | KNN Loss: 4.338765621185303 | BCE Loss: 1.04221510887146\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 5.295770168304443 | KNN Loss: 4.313482761383057 | BCE Loss: 0.9822872877120972\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 5.387290000915527 | KNN Loss: 4.3845977783203125 | BCE Loss: 1.002692461013794\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 5.348962783813477 | KNN Loss: 4.339338779449463 | BCE Loss: 1.0096240043640137\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 5.428492546081543 | KNN Loss: 4.365860939025879 | BCE Loss: 1.0626317262649536\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 5.359344482421875 | KNN Loss: 4.341887474060059 | BCE Loss: 1.0174572467803955\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 5.354940891265869 | KNN Loss: 4.350009918212891 | BCE Loss: 1.0049309730529785\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 5.369256019592285 | KNN Loss: 4.364261150360107 | BCE Loss: 1.0049951076507568\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 5.337381362915039 | KNN Loss: 4.321848392486572 | BCE Loss: 1.0155329704284668\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 5.423055171966553 | KNN Loss: 4.364217758178711 | BCE Loss: 1.0588374137878418\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 5.381571292877197 | KNN Loss: 4.356231212615967 | BCE Loss: 1.0253400802612305\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 5.397090435028076 | KNN Loss: 4.348546981811523 | BCE Loss: 1.0485435724258423\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 5.351617813110352 | KNN Loss: 4.332391262054443 | BCE Loss: 1.0192265510559082\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 5.339331150054932 | KNN Loss: 4.3349928855896 | BCE Loss: 1.004338264465332\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 5.3652472496032715 | KNN Loss: 4.361215591430664 | BCE Loss: 1.004031777381897\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 5.484704494476318 | KNN Loss: 4.424267768859863 | BCE Loss: 1.0604366064071655\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 5.393353462219238 | KNN Loss: 4.361122131347656 | BCE Loss: 1.0322315692901611\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 5.363847732543945 | KNN Loss: 4.377680778503418 | BCE Loss: 0.9861670732498169\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 5.355935573577881 | KNN Loss: 4.3173723220825195 | BCE Loss: 1.0385632514953613\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 5.375113010406494 | KNN Loss: 4.378108501434326 | BCE Loss: 0.9970044493675232\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 5.383881568908691 | KNN Loss: 4.354063987731934 | BCE Loss: 1.0298173427581787\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 5.357390880584717 | KNN Loss: 4.3230438232421875 | BCE Loss: 1.0343471765518188\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 5.404719829559326 | KNN Loss: 4.389438152313232 | BCE Loss: 1.0152816772460938\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 5.357583999633789 | KNN Loss: 4.347472190856934 | BCE Loss: 1.0101118087768555\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 5.389026641845703 | KNN Loss: 4.3571906089782715 | BCE Loss: 1.0318360328674316\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 5.3775529861450195 | KNN Loss: 4.349244117736816 | BCE Loss: 1.028308629989624\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 5.388467788696289 | KNN Loss: 4.383615493774414 | BCE Loss: 1.0048521757125854\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 5.388204574584961 | KNN Loss: 4.366232395172119 | BCE Loss: 1.0219721794128418\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 5.36302375793457 | KNN Loss: 4.336461067199707 | BCE Loss: 1.0265626907348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 5.3416829109191895 | KNN Loss: 4.344029426574707 | BCE Loss: 0.9976534843444824\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 5.351125240325928 | KNN Loss: 4.360593795776367 | BCE Loss: 0.9905316233634949\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 5.385076522827148 | KNN Loss: 4.356515407562256 | BCE Loss: 1.028560996055603\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 5.3765106201171875 | KNN Loss: 4.341403484344482 | BCE Loss: 1.035107135772705\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 5.383762359619141 | KNN Loss: 4.349235534667969 | BCE Loss: 1.0345265865325928\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 5.393272399902344 | KNN Loss: 4.374727725982666 | BCE Loss: 1.0185449123382568\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 5.359528541564941 | KNN Loss: 4.335381507873535 | BCE Loss: 1.0241470336914062\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 5.388387680053711 | KNN Loss: 4.39791202545166 | BCE Loss: 0.9904758930206299\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 5.341222763061523 | KNN Loss: 4.319007396697998 | BCE Loss: 1.0222156047821045\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 5.385225296020508 | KNN Loss: 4.348637104034424 | BCE Loss: 1.0365879535675049\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 5.405488967895508 | KNN Loss: 4.403809547424316 | BCE Loss: 1.0016796588897705\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 5.372559547424316 | KNN Loss: 4.354689121246338 | BCE Loss: 1.0178701877593994\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 5.411009788513184 | KNN Loss: 4.388795852661133 | BCE Loss: 1.0222136974334717\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 5.387594223022461 | KNN Loss: 4.3542799949646 | BCE Loss: 1.0333143472671509\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 5.373573303222656 | KNN Loss: 4.359696388244629 | BCE Loss: 1.0138766765594482\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 5.3860321044921875 | KNN Loss: 4.373356342315674 | BCE Loss: 1.0126757621765137\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 5.370211601257324 | KNN Loss: 4.341210842132568 | BCE Loss: 1.029000997543335\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 5.362576961517334 | KNN Loss: 4.32658052444458 | BCE Loss: 1.035996437072754\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 5.373231887817383 | KNN Loss: 4.33756685256958 | BCE Loss: 1.0356652736663818\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 5.35415506362915 | KNN Loss: 4.35804557800293 | BCE Loss: 0.9961093664169312\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 5.375722885131836 | KNN Loss: 4.3254780769348145 | BCE Loss: 1.0502450466156006\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 5.3602399826049805 | KNN Loss: 4.338665008544922 | BCE Loss: 1.0215749740600586\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 5.3742852210998535 | KNN Loss: 4.32440710067749 | BCE Loss: 1.0498780012130737\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 5.394275188446045 | KNN Loss: 4.359116554260254 | BCE Loss: 1.035158634185791\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 5.418490886688232 | KNN Loss: 4.387257099151611 | BCE Loss: 1.031233787536621\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 5.369477272033691 | KNN Loss: 4.332911014556885 | BCE Loss: 1.0365660190582275\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 5.381098747253418 | KNN Loss: 4.336596488952637 | BCE Loss: 1.0445024967193604\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 5.3439717292785645 | KNN Loss: 4.32791805267334 | BCE Loss: 1.0160537958145142\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 5.3143415451049805 | KNN Loss: 4.31956672668457 | BCE Loss: 0.9947748184204102\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 5.389017105102539 | KNN Loss: 4.365673065185547 | BCE Loss: 1.0233441591262817\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 5.380237579345703 | KNN Loss: 4.37678337097168 | BCE Loss: 1.0034542083740234\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 5.3673601150512695 | KNN Loss: 4.358388423919678 | BCE Loss: 1.008971929550171\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 5.390143871307373 | KNN Loss: 4.339664936065674 | BCE Loss: 1.0504789352416992\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 5.381809234619141 | KNN Loss: 4.374074459075928 | BCE Loss: 1.0077348947525024\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 5.36761474609375 | KNN Loss: 4.348855018615723 | BCE Loss: 1.0187597274780273\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 5.35134220123291 | KNN Loss: 4.330783843994141 | BCE Loss: 1.020558476448059\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 5.389313697814941 | KNN Loss: 4.392533779144287 | BCE Loss: 0.9967797994613647\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 5.346938610076904 | KNN Loss: 4.344940185546875 | BCE Loss: 1.0019985437393188\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 5.3696393966674805 | KNN Loss: 4.328801155090332 | BCE Loss: 1.0408382415771484\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 5.3507537841796875 | KNN Loss: 4.34572172164917 | BCE Loss: 1.0050320625305176\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 5.4083099365234375 | KNN Loss: 4.378846645355225 | BCE Loss: 1.029463291168213\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 5.402798652648926 | KNN Loss: 4.361960411071777 | BCE Loss: 1.0408381223678589\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 5.328430652618408 | KNN Loss: 4.30506706237793 | BCE Loss: 1.0233635902404785\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 5.327475070953369 | KNN Loss: 4.3232245445251465 | BCE Loss: 1.0042505264282227\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 5.381914138793945 | KNN Loss: 4.325721263885498 | BCE Loss: 1.0561928749084473\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 5.385772705078125 | KNN Loss: 4.3751139640808105 | BCE Loss: 1.0106587409973145\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 5.3936967849731445 | KNN Loss: 4.333150863647461 | BCE Loss: 1.0605456829071045\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 5.366129398345947 | KNN Loss: 4.335893154144287 | BCE Loss: 1.0302362442016602\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 5.3379011154174805 | KNN Loss: 4.30826997756958 | BCE Loss: 1.0296310186386108\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 5.397176742553711 | KNN Loss: 4.365851402282715 | BCE Loss: 1.0313255786895752\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 5.379500389099121 | KNN Loss: 4.344749927520752 | BCE Loss: 1.0347506999969482\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 5.403346061706543 | KNN Loss: 4.354142665863037 | BCE Loss: 1.0492033958435059\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 5.393214225769043 | KNN Loss: 4.348588466644287 | BCE Loss: 1.0446257591247559\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 5.311915874481201 | KNN Loss: 4.3227314949035645 | BCE Loss: 0.9891843795776367\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 5.378159523010254 | KNN Loss: 4.366939067840576 | BCE Loss: 1.0112206935882568\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 5.346184730529785 | KNN Loss: 4.342036724090576 | BCE Loss: 1.0041477680206299\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 5.3617048263549805 | KNN Loss: 4.328947067260742 | BCE Loss: 1.0327577590942383\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 5.385368347167969 | KNN Loss: 4.345823287963867 | BCE Loss: 1.039544939994812\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 5.396401405334473 | KNN Loss: 4.368557453155518 | BCE Loss: 1.0278441905975342\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 5.3748860359191895 | KNN Loss: 4.351986408233643 | BCE Loss: 1.0228996276855469\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 5.338924884796143 | KNN Loss: 4.320323944091797 | BCE Loss: 1.0186010599136353\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 5.342557907104492 | KNN Loss: 4.322125434875488 | BCE Loss: 1.020432710647583\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 5.318639755249023 | KNN Loss: 4.326320171356201 | BCE Loss: 0.9923193454742432\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 5.403896331787109 | KNN Loss: 4.365891933441162 | BCE Loss: 1.0380046367645264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 5.399094104766846 | KNN Loss: 4.341405868530273 | BCE Loss: 1.0576882362365723\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 5.378274917602539 | KNN Loss: 4.356893062591553 | BCE Loss: 1.0213820934295654\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 5.378725051879883 | KNN Loss: 4.3676438331604 | BCE Loss: 1.0110814571380615\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 5.351779937744141 | KNN Loss: 4.35300874710083 | BCE Loss: 0.9987711906433105\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 5.337748050689697 | KNN Loss: 4.316977024078369 | BCE Loss: 1.0207711458206177\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 5.366402626037598 | KNN Loss: 4.3359150886535645 | BCE Loss: 1.0304875373840332\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 5.371758937835693 | KNN Loss: 4.321863651275635 | BCE Loss: 1.0498952865600586\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 5.368221759796143 | KNN Loss: 4.346579074859619 | BCE Loss: 1.0216425657272339\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 5.372682571411133 | KNN Loss: 4.353209972381592 | BCE Loss: 1.0194724798202515\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 5.370992660522461 | KNN Loss: 4.369107246398926 | BCE Loss: 1.0018852949142456\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 5.350784778594971 | KNN Loss: 4.327646732330322 | BCE Loss: 1.0231380462646484\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 5.335027694702148 | KNN Loss: 4.338770866394043 | BCE Loss: 0.9962565898895264\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 5.387931823730469 | KNN Loss: 4.348803997039795 | BCE Loss: 1.0391275882720947\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 5.364477157592773 | KNN Loss: 4.348656177520752 | BCE Loss: 1.015820860862732\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 5.354916572570801 | KNN Loss: 4.355599403381348 | BCE Loss: 0.9993170499801636\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 5.380983352661133 | KNN Loss: 4.382129669189453 | BCE Loss: 0.9988536834716797\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 5.317591667175293 | KNN Loss: 4.327232360839844 | BCE Loss: 0.9903590679168701\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 5.374628067016602 | KNN Loss: 4.337596416473389 | BCE Loss: 1.037031650543213\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 5.398932456970215 | KNN Loss: 4.385932922363281 | BCE Loss: 1.0129995346069336\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 5.354338645935059 | KNN Loss: 4.311995983123779 | BCE Loss: 1.0423429012298584\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 5.366916656494141 | KNN Loss: 4.358953475952148 | BCE Loss: 1.0079631805419922\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 5.368586540222168 | KNN Loss: 4.358142375946045 | BCE Loss: 1.0104444026947021\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 5.359662055969238 | KNN Loss: 4.333340644836426 | BCE Loss: 1.0263214111328125\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 5.331790924072266 | KNN Loss: 4.330251216888428 | BCE Loss: 1.0015398263931274\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 5.367957592010498 | KNN Loss: 4.345729827880859 | BCE Loss: 1.0222278833389282\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 5.396373748779297 | KNN Loss: 4.364594459533691 | BCE Loss: 1.0317790508270264\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 5.384943008422852 | KNN Loss: 4.380468368530273 | BCE Loss: 1.0044748783111572\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 5.350687503814697 | KNN Loss: 4.335504055023193 | BCE Loss: 1.015183448791504\n",
      "Epoch   166: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 5.393073081970215 | KNN Loss: 4.38042688369751 | BCE Loss: 1.0126460790634155\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 5.377410888671875 | KNN Loss: 4.353641033172607 | BCE Loss: 1.0237696170806885\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 5.356619834899902 | KNN Loss: 4.342358112335205 | BCE Loss: 1.0142618417739868\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 5.323590278625488 | KNN Loss: 4.328588962554932 | BCE Loss: 0.9950011968612671\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 5.370216369628906 | KNN Loss: 4.3587799072265625 | BCE Loss: 1.0114362239837646\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 5.362846374511719 | KNN Loss: 4.346831321716309 | BCE Loss: 1.0160149335861206\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 5.397424221038818 | KNN Loss: 4.362290859222412 | BCE Loss: 1.0351332426071167\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 5.423161029815674 | KNN Loss: 4.406958103179932 | BCE Loss: 1.0162029266357422\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 5.36513614654541 | KNN Loss: 4.3514275550842285 | BCE Loss: 1.0137085914611816\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 5.368671417236328 | KNN Loss: 4.345000267028809 | BCE Loss: 1.0236709117889404\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 5.3379225730896 | KNN Loss: 4.333557605743408 | BCE Loss: 1.0043649673461914\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 5.31971549987793 | KNN Loss: 4.326227188110352 | BCE Loss: 0.993488073348999\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 5.332204341888428 | KNN Loss: 4.324389934539795 | BCE Loss: 1.0078144073486328\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 5.398482322692871 | KNN Loss: 4.373987674713135 | BCE Loss: 1.0244948863983154\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 5.396243095397949 | KNN Loss: 4.3845977783203125 | BCE Loss: 1.0116451978683472\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 5.322070121765137 | KNN Loss: 4.327592372894287 | BCE Loss: 0.9944778084754944\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 5.4136061668396 | KNN Loss: 4.4012837409973145 | BCE Loss: 1.0123223066329956\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 5.388981819152832 | KNN Loss: 4.366979122161865 | BCE Loss: 1.0220026969909668\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 5.336682319641113 | KNN Loss: 4.320087909698486 | BCE Loss: 1.0165941715240479\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 5.371523857116699 | KNN Loss: 4.36288595199585 | BCE Loss: 1.0086381435394287\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 5.369058609008789 | KNN Loss: 4.342881679534912 | BCE Loss: 1.026176929473877\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 5.405008316040039 | KNN Loss: 4.375413417816162 | BCE Loss: 1.0295947790145874\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 5.360280990600586 | KNN Loss: 4.315190315246582 | BCE Loss: 1.045090675354004\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 5.3731279373168945 | KNN Loss: 4.358864784240723 | BCE Loss: 1.0142629146575928\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 5.404270172119141 | KNN Loss: 4.367103099822998 | BCE Loss: 1.037166953086853\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 5.371152400970459 | KNN Loss: 4.3642988204956055 | BCE Loss: 1.006853699684143\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 5.3666768074035645 | KNN Loss: 4.376340866088867 | BCE Loss: 0.9903359413146973\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 5.413527965545654 | KNN Loss: 4.350963115692139 | BCE Loss: 1.062564730644226\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 5.360556602478027 | KNN Loss: 4.343660354614258 | BCE Loss: 1.0168964862823486\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 5.37593936920166 | KNN Loss: 4.343776702880859 | BCE Loss: 1.0321624279022217\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 5.3369245529174805 | KNN Loss: 4.3210883140563965 | BCE Loss: 1.0158360004425049\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 5.4204607009887695 | KNN Loss: 4.3679609298706055 | BCE Loss: 1.052499771118164\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 5.416263580322266 | KNN Loss: 4.368088722229004 | BCE Loss: 1.0481750965118408\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 5.3336873054504395 | KNN Loss: 4.320137977600098 | BCE Loss: 1.0135492086410522\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 5.314973831176758 | KNN Loss: 4.324173927307129 | BCE Loss: 0.9908000230789185\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 5.366837024688721 | KNN Loss: 4.324817180633545 | BCE Loss: 1.0420199632644653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 5.388667583465576 | KNN Loss: 4.359400272369385 | BCE Loss: 1.0292671918869019\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 5.357452392578125 | KNN Loss: 4.33066463470459 | BCE Loss: 1.026787519454956\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 5.384127616882324 | KNN Loss: 4.349640369415283 | BCE Loss: 1.0344874858856201\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 5.387388706207275 | KNN Loss: 4.34055757522583 | BCE Loss: 1.0468310117721558\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 5.390714645385742 | KNN Loss: 4.356607437133789 | BCE Loss: 1.0341072082519531\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 5.344223976135254 | KNN Loss: 4.324267864227295 | BCE Loss: 1.0199558734893799\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 5.379513263702393 | KNN Loss: 4.343180179595947 | BCE Loss: 1.0363332033157349\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 5.391201972961426 | KNN Loss: 4.361751556396484 | BCE Loss: 1.0294506549835205\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 5.366886615753174 | KNN Loss: 4.331033229827881 | BCE Loss: 1.035853385925293\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 5.401422023773193 | KNN Loss: 4.386000633239746 | BCE Loss: 1.0154212713241577\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 5.386131763458252 | KNN Loss: 4.369141101837158 | BCE Loss: 1.0169906616210938\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 5.338621139526367 | KNN Loss: 4.307045936584473 | BCE Loss: 1.0315752029418945\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 5.381733417510986 | KNN Loss: 4.3693647384643555 | BCE Loss: 1.0123686790466309\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 5.32358980178833 | KNN Loss: 4.314108848571777 | BCE Loss: 1.0094809532165527\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 5.316905975341797 | KNN Loss: 4.313354015350342 | BCE Loss: 1.0035518407821655\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 5.351975440979004 | KNN Loss: 4.34216833114624 | BCE Loss: 1.0098071098327637\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 5.363168239593506 | KNN Loss: 4.348631858825684 | BCE Loss: 1.0145362615585327\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 5.367392539978027 | KNN Loss: 4.353151798248291 | BCE Loss: 1.0142408609390259\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 5.323660850524902 | KNN Loss: 4.32988166809082 | BCE Loss: 0.9937791228294373\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 5.379935264587402 | KNN Loss: 4.375547885894775 | BCE Loss: 1.004387378692627\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 5.361710548400879 | KNN Loss: 4.335355281829834 | BCE Loss: 1.0263551473617554\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 5.341054439544678 | KNN Loss: 4.363331317901611 | BCE Loss: 0.9777230024337769\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 5.362878322601318 | KNN Loss: 4.347094535827637 | BCE Loss: 1.0157837867736816\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 5.420984745025635 | KNN Loss: 4.3739519119262695 | BCE Loss: 1.0470327138900757\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 5.3210601806640625 | KNN Loss: 4.327634811401367 | BCE Loss: 0.9934251308441162\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 5.345504283905029 | KNN Loss: 4.320114612579346 | BCE Loss: 1.0253896713256836\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 5.404642105102539 | KNN Loss: 4.365606307983398 | BCE Loss: 1.0390355587005615\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 5.384781837463379 | KNN Loss: 4.349709510803223 | BCE Loss: 1.0350722074508667\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 5.386772155761719 | KNN Loss: 4.350940227508545 | BCE Loss: 1.0358319282531738\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 5.38281774520874 | KNN Loss: 4.356482028961182 | BCE Loss: 1.0263357162475586\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 5.348532199859619 | KNN Loss: 4.352372169494629 | BCE Loss: 0.9961598515510559\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 5.358619213104248 | KNN Loss: 4.368021488189697 | BCE Loss: 0.9905977249145508\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 5.385982513427734 | KNN Loss: 4.35685920715332 | BCE Loss: 1.029123306274414\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 5.382347106933594 | KNN Loss: 4.3511738777160645 | BCE Loss: 1.0311733484268188\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 5.355694770812988 | KNN Loss: 4.327906608581543 | BCE Loss: 1.0277879238128662\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 5.374543190002441 | KNN Loss: 4.3614091873168945 | BCE Loss: 1.0131340026855469\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 5.369258880615234 | KNN Loss: 4.345992088317871 | BCE Loss: 1.0232669115066528\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 5.382028102874756 | KNN Loss: 4.338586330413818 | BCE Loss: 1.043441653251648\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 5.336337089538574 | KNN Loss: 4.325577735900879 | BCE Loss: 1.0107592344284058\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 5.377321243286133 | KNN Loss: 4.3419952392578125 | BCE Loss: 1.0353262424468994\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 5.355671405792236 | KNN Loss: 4.3594970703125 | BCE Loss: 0.9961743354797363\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 5.3622212409973145 | KNN Loss: 4.340514183044434 | BCE Loss: 1.0217071771621704\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 5.3836565017700195 | KNN Loss: 4.36214017868042 | BCE Loss: 1.0215165615081787\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 5.339279651641846 | KNN Loss: 4.330729961395264 | BCE Loss: 1.008549690246582\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 5.377440452575684 | KNN Loss: 4.358113765716553 | BCE Loss: 1.01932692527771\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 5.4042253494262695 | KNN Loss: 4.354754447937012 | BCE Loss: 1.049471139907837\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 5.312189102172852 | KNN Loss: 4.306970596313477 | BCE Loss: 1.005218267440796\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 5.352887153625488 | KNN Loss: 4.366806983947754 | BCE Loss: 0.9860802292823792\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 5.364349365234375 | KNN Loss: 4.32103157043457 | BCE Loss: 1.0433175563812256\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 5.329843521118164 | KNN Loss: 4.311682224273682 | BCE Loss: 1.0181615352630615\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 5.341204643249512 | KNN Loss: 4.318370819091797 | BCE Loss: 1.0228335857391357\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 5.393816947937012 | KNN Loss: 4.355523109436035 | BCE Loss: 1.0382938385009766\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 5.355531692504883 | KNN Loss: 4.333184242248535 | BCE Loss: 1.0223474502563477\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 5.339552402496338 | KNN Loss: 4.312195301055908 | BCE Loss: 1.0273572206497192\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 5.3821001052856445 | KNN Loss: 4.35991907119751 | BCE Loss: 1.0221809148788452\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 5.3797430992126465 | KNN Loss: 4.356143474578857 | BCE Loss: 1.0235995054244995\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 5.372162342071533 | KNN Loss: 4.344049453735352 | BCE Loss: 1.0281130075454712\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 5.385028839111328 | KNN Loss: 4.340765476226807 | BCE Loss: 1.0442636013031006\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 5.349116802215576 | KNN Loss: 4.327053070068359 | BCE Loss: 1.0220636129379272\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 5.374664306640625 | KNN Loss: 4.341739177703857 | BCE Loss: 1.0329248905181885\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 5.388607025146484 | KNN Loss: 4.326610088348389 | BCE Loss: 1.0619970560073853\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 5.340517044067383 | KNN Loss: 4.335790157318115 | BCE Loss: 1.0047271251678467\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 5.330018997192383 | KNN Loss: 4.314502239227295 | BCE Loss: 1.015516996383667\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 5.363052845001221 | KNN Loss: 4.3251261711120605 | BCE Loss: 1.0379266738891602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 5.375922203063965 | KNN Loss: 4.3643927574157715 | BCE Loss: 1.0115294456481934\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 5.369171619415283 | KNN Loss: 4.338595867156982 | BCE Loss: 1.0305757522583008\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 5.336714267730713 | KNN Loss: 4.325250625610352 | BCE Loss: 1.0114637613296509\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 5.358863353729248 | KNN Loss: 4.368281364440918 | BCE Loss: 0.9905818104743958\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 5.396547317504883 | KNN Loss: 4.35877799987793 | BCE Loss: 1.0377695560455322\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 5.370776176452637 | KNN Loss: 4.341398239135742 | BCE Loss: 1.0293781757354736\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 5.366744518280029 | KNN Loss: 4.36600399017334 | BCE Loss: 1.000740647315979\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 5.329010486602783 | KNN Loss: 4.330703258514404 | BCE Loss: 0.9983072280883789\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 5.332177639007568 | KNN Loss: 4.3277997970581055 | BCE Loss: 1.0043777227401733\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 5.374660968780518 | KNN Loss: 4.361576080322266 | BCE Loss: 1.013084888458252\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 5.343860626220703 | KNN Loss: 4.338357925415039 | BCE Loss: 1.005502700805664\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 5.368213653564453 | KNN Loss: 4.348559856414795 | BCE Loss: 1.0196536779403687\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 5.349871635437012 | KNN Loss: 4.355610370635986 | BCE Loss: 0.9942614436149597\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 5.329017639160156 | KNN Loss: 4.321120262145996 | BCE Loss: 1.0078976154327393\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 5.375059127807617 | KNN Loss: 4.3399248123168945 | BCE Loss: 1.0351344347000122\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 5.382293701171875 | KNN Loss: 4.351109981536865 | BCE Loss: 1.0311837196350098\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.325936794281006 | KNN Loss: 4.330474376678467 | BCE Loss: 0.9954624176025391\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 5.461950302124023 | KNN Loss: 4.410591125488281 | BCE Loss: 1.0513591766357422\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 5.355822563171387 | KNN Loss: 4.3278279304504395 | BCE Loss: 1.0279946327209473\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 5.35912561416626 | KNN Loss: 4.319549083709717 | BCE Loss: 1.039576530456543\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 5.370691299438477 | KNN Loss: 4.339468955993652 | BCE Loss: 1.0312223434448242\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 5.337306022644043 | KNN Loss: 4.325133800506592 | BCE Loss: 1.0121723413467407\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 5.349233150482178 | KNN Loss: 4.337320327758789 | BCE Loss: 1.0119128227233887\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 5.359156608581543 | KNN Loss: 4.342765808105469 | BCE Loss: 1.0163908004760742\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 5.366286754608154 | KNN Loss: 4.345127582550049 | BCE Loss: 1.021159291267395\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 5.3256001472473145 | KNN Loss: 4.326626300811768 | BCE Loss: 0.9989737272262573\n",
      "Epoch   187: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 5.390496253967285 | KNN Loss: 4.3536057472229 | BCE Loss: 1.0368906259536743\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 5.365407466888428 | KNN Loss: 4.332910537719727 | BCE Loss: 1.0324969291687012\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 5.383905410766602 | KNN Loss: 4.368342876434326 | BCE Loss: 1.0155625343322754\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 5.351430892944336 | KNN Loss: 4.32097053527832 | BCE Loss: 1.030460238456726\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 5.384851932525635 | KNN Loss: 4.348843097686768 | BCE Loss: 1.0360088348388672\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 5.360687732696533 | KNN Loss: 4.326587677001953 | BCE Loss: 1.03410005569458\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 5.37472677230835 | KNN Loss: 4.333153247833252 | BCE Loss: 1.0415736436843872\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 5.347020149230957 | KNN Loss: 4.3647284507751465 | BCE Loss: 0.9822916984558105\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 5.349298477172852 | KNN Loss: 4.330406188964844 | BCE Loss: 1.0188924074172974\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 5.346313953399658 | KNN Loss: 4.331512928009033 | BCE Loss: 1.0148009061813354\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 5.401148796081543 | KNN Loss: 4.3725152015686035 | BCE Loss: 1.0286335945129395\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 5.354503154754639 | KNN Loss: 4.3175048828125 | BCE Loss: 1.0369982719421387\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 5.3932785987854 | KNN Loss: 4.338944435119629 | BCE Loss: 1.0543341636657715\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 5.353593826293945 | KNN Loss: 4.3234758377075195 | BCE Loss: 1.0301182270050049\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 5.3637776374816895 | KNN Loss: 4.349817276000977 | BCE Loss: 1.013960361480713\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 5.401050090789795 | KNN Loss: 4.355723857879639 | BCE Loss: 1.0453262329101562\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 5.3572845458984375 | KNN Loss: 4.328328609466553 | BCE Loss: 1.0289556980133057\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 5.360504150390625 | KNN Loss: 4.35040807723999 | BCE Loss: 1.0100959539413452\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 5.320775032043457 | KNN Loss: 4.3127288818359375 | BCE Loss: 1.0080459117889404\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 5.358938694000244 | KNN Loss: 4.353170394897461 | BCE Loss: 1.0057681798934937\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 5.376794815063477 | KNN Loss: 4.314605236053467 | BCE Loss: 1.0621895790100098\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 5.333683967590332 | KNN Loss: 4.333514213562012 | BCE Loss: 1.0001697540283203\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 5.367966651916504 | KNN Loss: 4.356142520904541 | BCE Loss: 1.0118242502212524\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 5.386504173278809 | KNN Loss: 4.357402801513672 | BCE Loss: 1.0291014909744263\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 5.382241249084473 | KNN Loss: 4.322439670562744 | BCE Loss: 1.059801697731018\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 5.36454963684082 | KNN Loss: 4.336547374725342 | BCE Loss: 1.0280020236968994\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 5.349785804748535 | KNN Loss: 4.337018966674805 | BCE Loss: 1.0127665996551514\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 5.337873935699463 | KNN Loss: 4.316595077514648 | BCE Loss: 1.0212788581848145\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 5.404548645019531 | KNN Loss: 4.37719202041626 | BCE Loss: 1.0273568630218506\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 5.339953422546387 | KNN Loss: 4.322023391723633 | BCE Loss: 1.0179299116134644\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 5.372522354125977 | KNN Loss: 4.337936878204346 | BCE Loss: 1.0345854759216309\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 5.335168838500977 | KNN Loss: 4.342468738555908 | BCE Loss: 0.9927000999450684\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 5.367026329040527 | KNN Loss: 4.332699298858643 | BCE Loss: 1.0343272686004639\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 5.402474403381348 | KNN Loss: 4.355986595153809 | BCE Loss: 1.0464880466461182\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 5.3718132972717285 | KNN Loss: 4.3433098793029785 | BCE Loss: 1.0285035371780396\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 5.367932319641113 | KNN Loss: 4.380107879638672 | BCE Loss: 0.9878242015838623\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 5.3815460205078125 | KNN Loss: 4.371600151062012 | BCE Loss: 1.0099456310272217\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 5.344147682189941 | KNN Loss: 4.329011917114258 | BCE Loss: 1.0151360034942627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 5.331056594848633 | KNN Loss: 4.323740005493164 | BCE Loss: 1.0073168277740479\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 5.373711585998535 | KNN Loss: 4.360569953918457 | BCE Loss: 1.0131416320800781\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 5.360617637634277 | KNN Loss: 4.320070743560791 | BCE Loss: 1.0405471324920654\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 5.397516250610352 | KNN Loss: 4.378911018371582 | BCE Loss: 1.0186049938201904\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 5.364614963531494 | KNN Loss: 4.372374057769775 | BCE Loss: 0.9922410249710083\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 5.375185012817383 | KNN Loss: 4.350447177886963 | BCE Loss: 1.024738073348999\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 5.368709564208984 | KNN Loss: 4.334160327911377 | BCE Loss: 1.034549355506897\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 5.361690998077393 | KNN Loss: 4.338366508483887 | BCE Loss: 1.0233244895935059\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 5.379204750061035 | KNN Loss: 4.3498454093933105 | BCE Loss: 1.0293593406677246\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 5.3648834228515625 | KNN Loss: 4.333311557769775 | BCE Loss: 1.031571626663208\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 5.376261234283447 | KNN Loss: 4.324880599975586 | BCE Loss: 1.0513806343078613\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 5.385308265686035 | KNN Loss: 4.378481388092041 | BCE Loss: 1.0068267583847046\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 5.343440532684326 | KNN Loss: 4.340418338775635 | BCE Loss: 1.0030220746994019\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 5.389892578125 | KNN Loss: 4.350453853607178 | BCE Loss: 1.0394388437271118\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 5.351878643035889 | KNN Loss: 4.329281806945801 | BCE Loss: 1.022596836090088\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 5.3463029861450195 | KNN Loss: 4.326638698577881 | BCE Loss: 1.0196640491485596\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 5.355556964874268 | KNN Loss: 4.33485221862793 | BCE Loss: 1.0207048654556274\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 5.375295639038086 | KNN Loss: 4.339047908782959 | BCE Loss: 1.0362476110458374\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 5.356973171234131 | KNN Loss: 4.357637405395508 | BCE Loss: 0.9993358254432678\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 5.399592399597168 | KNN Loss: 4.367722511291504 | BCE Loss: 1.031869649887085\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 5.395195960998535 | KNN Loss: 4.3587751388549805 | BCE Loss: 1.0364207029342651\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 5.371486186981201 | KNN Loss: 4.352799892425537 | BCE Loss: 1.018686294555664\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 5.35684061050415 | KNN Loss: 4.342379570007324 | BCE Loss: 1.0144611597061157\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 5.363758563995361 | KNN Loss: 4.342278480529785 | BCE Loss: 1.0214800834655762\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 5.355489730834961 | KNN Loss: 4.349446773529053 | BCE Loss: 1.0060430765151978\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 5.356754779815674 | KNN Loss: 4.342502117156982 | BCE Loss: 1.0142525434494019\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 5.417093753814697 | KNN Loss: 4.367289066314697 | BCE Loss: 1.0498046875\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 5.382333755493164 | KNN Loss: 4.368920803070068 | BCE Loss: 1.0134131908416748\n",
      "Epoch   198: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 5.3704423904418945 | KNN Loss: 4.34366512298584 | BCE Loss: 1.0267770290374756\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 5.370499610900879 | KNN Loss: 4.344121932983398 | BCE Loss: 1.0263779163360596\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 5.330000400543213 | KNN Loss: 4.31740665435791 | BCE Loss: 1.0125938653945923\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 5.323977947235107 | KNN Loss: 4.341108798980713 | BCE Loss: 0.9828692078590393\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 5.366536617279053 | KNN Loss: 4.345380783081055 | BCE Loss: 1.0211557149887085\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 5.365350246429443 | KNN Loss: 4.343507766723633 | BCE Loss: 1.0218424797058105\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 5.409976005554199 | KNN Loss: 4.390357494354248 | BCE Loss: 1.019618272781372\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 5.359053611755371 | KNN Loss: 4.306722640991211 | BCE Loss: 1.0523309707641602\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 5.351605415344238 | KNN Loss: 4.345438003540039 | BCE Loss: 1.0061675310134888\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 5.348513603210449 | KNN Loss: 4.320918560028076 | BCE Loss: 1.0275952816009521\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 5.351136207580566 | KNN Loss: 4.349864482879639 | BCE Loss: 1.0012714862823486\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 5.392101287841797 | KNN Loss: 4.347083568572998 | BCE Loss: 1.0450177192687988\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 5.329334735870361 | KNN Loss: 4.315157890319824 | BCE Loss: 1.014176845550537\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 5.35106086730957 | KNN Loss: 4.343231678009033 | BCE Loss: 1.0078294277191162\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 5.427260398864746 | KNN Loss: 4.391483306884766 | BCE Loss: 1.0357773303985596\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 5.400306701660156 | KNN Loss: 4.3546295166015625 | BCE Loss: 1.0456771850585938\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 5.3430304527282715 | KNN Loss: 4.309244155883789 | BCE Loss: 1.0337862968444824\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 5.3818559646606445 | KNN Loss: 4.363951206207275 | BCE Loss: 1.01790452003479\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 5.381434917449951 | KNN Loss: 4.348628997802734 | BCE Loss: 1.0328058004379272\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 5.374743461608887 | KNN Loss: 4.354925632476807 | BCE Loss: 1.01981782913208\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 5.350434303283691 | KNN Loss: 4.349510669708252 | BCE Loss: 1.0009233951568604\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 5.389480113983154 | KNN Loss: 4.370079040527344 | BCE Loss: 1.0194010734558105\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 5.358105659484863 | KNN Loss: 4.333372592926025 | BCE Loss: 1.024733066558838\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 5.424107551574707 | KNN Loss: 4.3909687995910645 | BCE Loss: 1.0331388711929321\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 5.393743991851807 | KNN Loss: 4.368335723876953 | BCE Loss: 1.0254082679748535\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 5.447265625 | KNN Loss: 4.397476673126221 | BCE Loss: 1.0497887134552002\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 5.361075401306152 | KNN Loss: 4.33209753036499 | BCE Loss: 1.028977870941162\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 5.346413612365723 | KNN Loss: 4.3271050453186035 | BCE Loss: 1.0193086862564087\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 5.383143901824951 | KNN Loss: 4.35984992980957 | BCE Loss: 1.0232939720153809\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 5.342512130737305 | KNN Loss: 4.352806091308594 | BCE Loss: 0.98970627784729\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 5.340066909790039 | KNN Loss: 4.328180313110352 | BCE Loss: 1.0118863582611084\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 5.358838081359863 | KNN Loss: 4.333571434020996 | BCE Loss: 1.0252668857574463\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 5.332305431365967 | KNN Loss: 4.341771125793457 | BCE Loss: 0.990534245967865\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 5.34397554397583 | KNN Loss: 4.340773582458496 | BCE Loss: 1.003201961517334\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 5.3815412521362305 | KNN Loss: 4.356531620025635 | BCE Loss: 1.0250093936920166\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 5.437884330749512 | KNN Loss: 4.3893585205078125 | BCE Loss: 1.0485255718231201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 5.345978736877441 | KNN Loss: 4.31674861907959 | BCE Loss: 1.0292302370071411\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 5.331292629241943 | KNN Loss: 4.316649913787842 | BCE Loss: 1.0146427154541016\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 5.340664863586426 | KNN Loss: 4.326846599578857 | BCE Loss: 1.0138185024261475\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 5.320669174194336 | KNN Loss: 4.309599876403809 | BCE Loss: 1.0110692977905273\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 5.397899627685547 | KNN Loss: 4.362926959991455 | BCE Loss: 1.0349727869033813\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 5.366705894470215 | KNN Loss: 4.357923984527588 | BCE Loss: 1.0087817907333374\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 5.354964733123779 | KNN Loss: 4.330198287963867 | BCE Loss: 1.0247663259506226\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 5.392163276672363 | KNN Loss: 4.342433452606201 | BCE Loss: 1.0497297048568726\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 5.346873760223389 | KNN Loss: 4.350389003753662 | BCE Loss: 0.9964845776557922\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 5.37058162689209 | KNN Loss: 4.344019412994385 | BCE Loss: 1.026562213897705\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 5.386770248413086 | KNN Loss: 4.364694118499756 | BCE Loss: 1.0220762491226196\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 5.390040397644043 | KNN Loss: 4.352047920227051 | BCE Loss: 1.0379925966262817\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 5.337804794311523 | KNN Loss: 4.3165364265441895 | BCE Loss: 1.021268367767334\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 5.404706954956055 | KNN Loss: 4.396683692932129 | BCE Loss: 1.0080232620239258\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 5.3425984382629395 | KNN Loss: 4.320550918579102 | BCE Loss: 1.022047519683838\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 5.368278980255127 | KNN Loss: 4.337115287780762 | BCE Loss: 1.0311638116836548\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 5.382397651672363 | KNN Loss: 4.35178804397583 | BCE Loss: 1.0306096076965332\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 5.319610118865967 | KNN Loss: 4.300448894500732 | BCE Loss: 1.0191611051559448\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 5.370789051055908 | KNN Loss: 4.33035945892334 | BCE Loss: 1.0404294729232788\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 5.367341041564941 | KNN Loss: 4.320725440979004 | BCE Loss: 1.046615481376648\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 5.377775192260742 | KNN Loss: 4.353052616119385 | BCE Loss: 1.0247228145599365\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 5.358377456665039 | KNN Loss: 4.333963394165039 | BCE Loss: 1.024413824081421\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 5.364396095275879 | KNN Loss: 4.333477973937988 | BCE Loss: 1.0309183597564697\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 5.332205772399902 | KNN Loss: 4.327666759490967 | BCE Loss: 1.0045390129089355\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 5.350031852722168 | KNN Loss: 4.312095642089844 | BCE Loss: 1.0379362106323242\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 5.351664066314697 | KNN Loss: 4.345200538635254 | BCE Loss: 1.0064635276794434\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 5.361578941345215 | KNN Loss: 4.335355281829834 | BCE Loss: 1.02622389793396\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 5.433791160583496 | KNN Loss: 4.396844387054443 | BCE Loss: 1.0369465351104736\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 5.394872665405273 | KNN Loss: 4.382684230804443 | BCE Loss: 1.0121886730194092\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 5.389595031738281 | KNN Loss: 4.349809169769287 | BCE Loss: 1.0397858619689941\n",
      "Epoch   209: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 5.336158752441406 | KNN Loss: 4.334182262420654 | BCE Loss: 1.001976490020752\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 5.432250022888184 | KNN Loss: 4.4083967208862305 | BCE Loss: 1.0238535404205322\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 5.355803966522217 | KNN Loss: 4.320835113525391 | BCE Loss: 1.0349687337875366\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 5.412806510925293 | KNN Loss: 4.387778282165527 | BCE Loss: 1.0250284671783447\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 5.391900062561035 | KNN Loss: 4.335973739624023 | BCE Loss: 1.0559262037277222\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 5.383275985717773 | KNN Loss: 4.349935054779053 | BCE Loss: 1.0333409309387207\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 5.38664436340332 | KNN Loss: 4.352014064788818 | BCE Loss: 1.0346300601959229\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 5.393496990203857 | KNN Loss: 4.36785364151001 | BCE Loss: 1.0256433486938477\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 5.3746137619018555 | KNN Loss: 4.338810443878174 | BCE Loss: 1.0358035564422607\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 5.354583263397217 | KNN Loss: 4.328256607055664 | BCE Loss: 1.0263266563415527\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 5.357650279998779 | KNN Loss: 4.304775238037109 | BCE Loss: 1.05287504196167\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 5.28864049911499 | KNN Loss: 4.306286334991455 | BCE Loss: 0.9823540449142456\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 5.365072250366211 | KNN Loss: 4.352129936218262 | BCE Loss: 1.0129421949386597\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 5.397909164428711 | KNN Loss: 4.339046955108643 | BCE Loss: 1.0588620901107788\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 5.405013084411621 | KNN Loss: 4.374906063079834 | BCE Loss: 1.030107021331787\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 5.390648365020752 | KNN Loss: 4.343032360076904 | BCE Loss: 1.0476160049438477\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 5.411889553070068 | KNN Loss: 4.378983974456787 | BCE Loss: 1.0329055786132812\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 5.3592634201049805 | KNN Loss: 4.329882621765137 | BCE Loss: 1.0293807983398438\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 5.354701995849609 | KNN Loss: 4.326784133911133 | BCE Loss: 1.0279181003570557\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 5.376281261444092 | KNN Loss: 4.3431267738342285 | BCE Loss: 1.0331544876098633\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 5.323493480682373 | KNN Loss: 4.313094615936279 | BCE Loss: 1.0103988647460938\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 5.376849174499512 | KNN Loss: 4.375779151916504 | BCE Loss: 1.0010697841644287\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 5.347224235534668 | KNN Loss: 4.33840799331665 | BCE Loss: 1.0088164806365967\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 5.3431830406188965 | KNN Loss: 4.316293716430664 | BCE Loss: 1.0268893241882324\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 5.411948204040527 | KNN Loss: 4.37418794631958 | BCE Loss: 1.0377602577209473\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 5.359963417053223 | KNN Loss: 4.333109378814697 | BCE Loss: 1.0268540382385254\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 5.323319435119629 | KNN Loss: 4.3017120361328125 | BCE Loss: 1.0216071605682373\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 5.346774101257324 | KNN Loss: 4.336184024810791 | BCE Loss: 1.0105900764465332\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 5.348166465759277 | KNN Loss: 4.328772068023682 | BCE Loss: 1.0193942785263062\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 5.328197479248047 | KNN Loss: 4.314395904541016 | BCE Loss: 1.0138013362884521\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 5.380651950836182 | KNN Loss: 4.380609512329102 | BCE Loss: 1.0000425577163696\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 5.340725898742676 | KNN Loss: 4.328403472900391 | BCE Loss: 1.0123224258422852\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 5.398890495300293 | KNN Loss: 4.377737998962402 | BCE Loss: 1.0211527347564697\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 5.371042251586914 | KNN Loss: 4.359502792358398 | BCE Loss: 1.0115392208099365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 5.368538856506348 | KNN Loss: 4.339588642120361 | BCE Loss: 1.0289503335952759\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 5.362987995147705 | KNN Loss: 4.3609418869018555 | BCE Loss: 1.0020461082458496\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 5.39769983291626 | KNN Loss: 4.364389896392822 | BCE Loss: 1.0333099365234375\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 5.345359802246094 | KNN Loss: 4.323641777038574 | BCE Loss: 1.0217177867889404\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 5.384208679199219 | KNN Loss: 4.337435245513916 | BCE Loss: 1.0467736721038818\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 5.350454330444336 | KNN Loss: 4.317714691162109 | BCE Loss: 1.0327394008636475\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 5.352890968322754 | KNN Loss: 4.341034412384033 | BCE Loss: 1.0118566751480103\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 5.415961265563965 | KNN Loss: 4.399179458618164 | BCE Loss: 1.0167819261550903\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 5.320744514465332 | KNN Loss: 4.318277359008789 | BCE Loss: 1.0024670362472534\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 5.322607040405273 | KNN Loss: 4.31312370300293 | BCE Loss: 1.0094834566116333\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 5.335480213165283 | KNN Loss: 4.313817977905273 | BCE Loss: 1.0216621160507202\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 5.395288944244385 | KNN Loss: 4.381494522094727 | BCE Loss: 1.0137944221496582\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 5.381081581115723 | KNN Loss: 4.380205154418945 | BCE Loss: 1.0008766651153564\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 5.382035255432129 | KNN Loss: 4.355193614959717 | BCE Loss: 1.0268418788909912\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 5.384120941162109 | KNN Loss: 4.369851589202881 | BCE Loss: 1.0142691135406494\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 5.376911163330078 | KNN Loss: 4.339704990386963 | BCE Loss: 1.0372059345245361\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 5.320610523223877 | KNN Loss: 4.310626983642578 | BCE Loss: 1.0099834203720093\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 5.354960918426514 | KNN Loss: 4.339545726776123 | BCE Loss: 1.0154151916503906\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 5.4399213790893555 | KNN Loss: 4.381619930267334 | BCE Loss: 1.0583016872406006\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 5.379140853881836 | KNN Loss: 4.353699207305908 | BCE Loss: 1.0254418849945068\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 5.333648204803467 | KNN Loss: 4.313050746917725 | BCE Loss: 1.0205973386764526\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 5.370302200317383 | KNN Loss: 4.336339950561523 | BCE Loss: 1.033962368965149\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 5.324436664581299 | KNN Loss: 4.309516906738281 | BCE Loss: 1.0149197578430176\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 5.313480854034424 | KNN Loss: 4.309014320373535 | BCE Loss: 1.0044664144515991\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 5.336503028869629 | KNN Loss: 4.327033519744873 | BCE Loss: 1.0094693899154663\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 5.391156196594238 | KNN Loss: 4.362802982330322 | BCE Loss: 1.028353214263916\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 5.375227928161621 | KNN Loss: 4.3456830978393555 | BCE Loss: 1.0295450687408447\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 5.371826171875 | KNN Loss: 4.351339340209961 | BCE Loss: 1.0204870700836182\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 5.411600589752197 | KNN Loss: 4.384403705596924 | BCE Loss: 1.0271968841552734\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 5.364020347595215 | KNN Loss: 4.35020112991333 | BCE Loss: 1.0138193368911743\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 5.308883190155029 | KNN Loss: 4.303580284118652 | BCE Loss: 1.0053027868270874\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 5.324739456176758 | KNN Loss: 4.326610088348389 | BCE Loss: 0.9981293678283691\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 5.3116254806518555 | KNN Loss: 4.319719314575195 | BCE Loss: 0.9919060468673706\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 5.354358196258545 | KNN Loss: 4.313440799713135 | BCE Loss: 1.0409173965454102\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 5.3652448654174805 | KNN Loss: 4.353204727172852 | BCE Loss: 1.0120398998260498\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 5.356271266937256 | KNN Loss: 4.342044353485107 | BCE Loss: 1.0142269134521484\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 5.36376953125 | KNN Loss: 4.34046745300293 | BCE Loss: 1.0233018398284912\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 5.368277549743652 | KNN Loss: 4.337226867675781 | BCE Loss: 1.0310508012771606\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 5.353862285614014 | KNN Loss: 4.362386226654053 | BCE Loss: 0.9914759397506714\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 5.353762626647949 | KNN Loss: 4.347624778747559 | BCE Loss: 1.006137728691101\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 5.349409580230713 | KNN Loss: 4.337542533874512 | BCE Loss: 1.0118670463562012\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 5.389648914337158 | KNN Loss: 4.358834266662598 | BCE Loss: 1.03081476688385\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 5.3517866134643555 | KNN Loss: 4.325641632080078 | BCE Loss: 1.0261449813842773\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 5.406388282775879 | KNN Loss: 4.371582984924316 | BCE Loss: 1.0348055362701416\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 5.38416862487793 | KNN Loss: 4.365652561187744 | BCE Loss: 1.018515944480896\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 5.3714213371276855 | KNN Loss: 4.341255187988281 | BCE Loss: 1.0301661491394043\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 5.402308940887451 | KNN Loss: 4.358708381652832 | BCE Loss: 1.0436005592346191\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 5.34488582611084 | KNN Loss: 4.313005447387695 | BCE Loss: 1.0318803787231445\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 5.349402904510498 | KNN Loss: 4.361363887786865 | BCE Loss: 0.9880391359329224\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 5.338260650634766 | KNN Loss: 4.326716423034668 | BCE Loss: 1.0115439891815186\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 5.331525802612305 | KNN Loss: 4.327867031097412 | BCE Loss: 1.0036585330963135\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 5.40105676651001 | KNN Loss: 4.387421607971191 | BCE Loss: 1.013635277748108\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 5.377933502197266 | KNN Loss: 4.338714599609375 | BCE Loss: 1.039218783378601\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 5.42010498046875 | KNN Loss: 4.374889373779297 | BCE Loss: 1.045215368270874\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 5.334342956542969 | KNN Loss: 4.321596145629883 | BCE Loss: 1.0127465724945068\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 5.389382362365723 | KNN Loss: 4.357416152954102 | BCE Loss: 1.031965970993042\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 5.393332481384277 | KNN Loss: 4.357083320617676 | BCE Loss: 1.0362489223480225\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 5.366016864776611 | KNN Loss: 4.331348896026611 | BCE Loss: 1.0346680879592896\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 5.427124977111816 | KNN Loss: 4.389532089233398 | BCE Loss: 1.0375926494598389\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 5.340639114379883 | KNN Loss: 4.31427001953125 | BCE Loss: 1.026369333267212\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 5.417867660522461 | KNN Loss: 4.368770122528076 | BCE Loss: 1.0490976572036743\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 5.370824813842773 | KNN Loss: 4.3333210945129395 | BCE Loss: 1.0375038385391235\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 5.33644962310791 | KNN Loss: 4.319164752960205 | BCE Loss: 1.017284631729126\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 5.3222455978393555 | KNN Loss: 4.32844352722168 | BCE Loss: 0.9938019514083862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 5.344145774841309 | KNN Loss: 4.307365894317627 | BCE Loss: 1.0367796421051025\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 5.384427070617676 | KNN Loss: 4.348245143890381 | BCE Loss: 1.036182165145874\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 5.352842807769775 | KNN Loss: 4.366208076477051 | BCE Loss: 0.9866347312927246\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 5.316404342651367 | KNN Loss: 4.313209533691406 | BCE Loss: 1.0031949281692505\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 5.3978729248046875 | KNN Loss: 4.384249687194824 | BCE Loss: 1.0136229991912842\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 5.375531196594238 | KNN Loss: 4.314569473266602 | BCE Loss: 1.0609617233276367\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 5.361615180969238 | KNN Loss: 4.3304948806762695 | BCE Loss: 1.0311203002929688\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 5.334537982940674 | KNN Loss: 4.32710075378418 | BCE Loss: 1.0074371099472046\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 5.348217010498047 | KNN Loss: 4.3346381187438965 | BCE Loss: 1.0135786533355713\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 5.343937873840332 | KNN Loss: 4.340226173400879 | BCE Loss: 1.0037117004394531\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 5.334262847900391 | KNN Loss: 4.3248162269592285 | BCE Loss: 1.0094468593597412\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 5.431154251098633 | KNN Loss: 4.41800594329834 | BCE Loss: 1.0131481885910034\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 5.371291637420654 | KNN Loss: 4.335441589355469 | BCE Loss: 1.035849928855896\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 5.357471466064453 | KNN Loss: 4.354642391204834 | BCE Loss: 1.0028293132781982\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 5.311248779296875 | KNN Loss: 4.312908172607422 | BCE Loss: 0.9983408451080322\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 5.3878912925720215 | KNN Loss: 4.373336315155029 | BCE Loss: 1.0145549774169922\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 5.378247261047363 | KNN Loss: 4.3476080894470215 | BCE Loss: 1.0306389331817627\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 5.404685020446777 | KNN Loss: 4.355420112609863 | BCE Loss: 1.0492647886276245\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 5.372114658355713 | KNN Loss: 4.323615550994873 | BCE Loss: 1.0484992265701294\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 5.331182956695557 | KNN Loss: 4.313477993011475 | BCE Loss: 1.0177050828933716\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 5.393032073974609 | KNN Loss: 4.361701965332031 | BCE Loss: 1.0313299894332886\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 5.32271671295166 | KNN Loss: 4.321436405181885 | BCE Loss: 1.001280426979065\n",
      "Epoch   229: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 5.3421454429626465 | KNN Loss: 4.3304443359375 | BCE Loss: 1.0117011070251465\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 5.372298240661621 | KNN Loss: 4.327584743499756 | BCE Loss: 1.0447132587432861\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 5.389608383178711 | KNN Loss: 4.390157222747803 | BCE Loss: 0.9994511008262634\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 5.354436874389648 | KNN Loss: 4.335552215576172 | BCE Loss: 1.0188848972320557\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 5.346906661987305 | KNN Loss: 4.320415019989014 | BCE Loss: 1.0264918804168701\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 5.351089000701904 | KNN Loss: 4.326223850250244 | BCE Loss: 1.0248652696609497\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 5.314119338989258 | KNN Loss: 4.334413051605225 | BCE Loss: 0.9797061681747437\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 5.390233039855957 | KNN Loss: 4.362065315246582 | BCE Loss: 1.028167486190796\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 5.350377082824707 | KNN Loss: 4.315840244293213 | BCE Loss: 1.0345370769500732\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 5.360932350158691 | KNN Loss: 4.35011625289917 | BCE Loss: 1.010815978050232\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 5.333717346191406 | KNN Loss: 4.33767557144165 | BCE Loss: 0.996042013168335\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 5.365614414215088 | KNN Loss: 4.36674165725708 | BCE Loss: 0.9988728761672974\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 5.332956314086914 | KNN Loss: 4.310772895812988 | BCE Loss: 1.0221836566925049\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 5.349314212799072 | KNN Loss: 4.350749969482422 | BCE Loss: 0.9985643625259399\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 5.380434036254883 | KNN Loss: 4.339348316192627 | BCE Loss: 1.041085958480835\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 5.372003555297852 | KNN Loss: 4.351571559906006 | BCE Loss: 1.0204322338104248\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 5.345913410186768 | KNN Loss: 4.3166351318359375 | BCE Loss: 1.0292783975601196\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 5.3910017013549805 | KNN Loss: 4.362752914428711 | BCE Loss: 1.0282490253448486\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 5.318148136138916 | KNN Loss: 4.297115325927734 | BCE Loss: 1.0210328102111816\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 5.42811918258667 | KNN Loss: 4.390675067901611 | BCE Loss: 1.0374441146850586\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 5.359821319580078 | KNN Loss: 4.349513053894043 | BCE Loss: 1.0103081464767456\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 5.401572227478027 | KNN Loss: 4.361590385437012 | BCE Loss: 1.0399816036224365\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 5.402003765106201 | KNN Loss: 4.376482963562012 | BCE Loss: 1.0255208015441895\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 5.312347412109375 | KNN Loss: 4.31820011138916 | BCE Loss: 0.9941474795341492\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 5.373320579528809 | KNN Loss: 4.351796627044678 | BCE Loss: 1.0215240716934204\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 5.340539455413818 | KNN Loss: 4.339047431945801 | BCE Loss: 1.0014920234680176\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 5.364564418792725 | KNN Loss: 4.3509111404418945 | BCE Loss: 1.01365327835083\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 5.368859767913818 | KNN Loss: 4.362018585205078 | BCE Loss: 1.0068411827087402\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 5.407851696014404 | KNN Loss: 4.3733344078063965 | BCE Loss: 1.0345172882080078\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 5.384960651397705 | KNN Loss: 4.346590042114258 | BCE Loss: 1.0383706092834473\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 5.345909118652344 | KNN Loss: 4.309161186218262 | BCE Loss: 1.036747932434082\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 5.35115909576416 | KNN Loss: 4.341115951538086 | BCE Loss: 1.0100430250167847\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 5.405160427093506 | KNN Loss: 4.358621120452881 | BCE Loss: 1.0465394258499146\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 5.334443092346191 | KNN Loss: 4.320913791656494 | BCE Loss: 1.0135295391082764\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 5.3451128005981445 | KNN Loss: 4.322796821594238 | BCE Loss: 1.0223158597946167\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 5.333208084106445 | KNN Loss: 4.32411527633667 | BCE Loss: 1.009092926979065\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 5.459061145782471 | KNN Loss: 4.408298492431641 | BCE Loss: 1.05076265335083\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 5.358344078063965 | KNN Loss: 4.322498798370361 | BCE Loss: 1.035845160484314\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 5.3931884765625 | KNN Loss: 4.379584789276123 | BCE Loss: 1.0136038064956665\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 5.356502532958984 | KNN Loss: 4.3076677322387695 | BCE Loss: 1.048835039138794\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 5.3564558029174805 | KNN Loss: 4.347448348999023 | BCE Loss: 1.009007453918457\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 5.3629655838012695 | KNN Loss: 4.361633777618408 | BCE Loss: 1.0013320446014404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 5.340029716491699 | KNN Loss: 4.324741840362549 | BCE Loss: 1.0152881145477295\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 5.36462926864624 | KNN Loss: 4.33961820602417 | BCE Loss: 1.0250111818313599\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 5.339353561401367 | KNN Loss: 4.335816860198975 | BCE Loss: 1.0035369396209717\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 5.364731311798096 | KNN Loss: 4.314316749572754 | BCE Loss: 1.0504145622253418\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 5.332450866699219 | KNN Loss: 4.319447040557861 | BCE Loss: 1.0130040645599365\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 5.339555740356445 | KNN Loss: 4.333128452301025 | BCE Loss: 1.0064270496368408\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 5.356820583343506 | KNN Loss: 4.352006435394287 | BCE Loss: 1.0048142671585083\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 5.403218746185303 | KNN Loss: 4.413683891296387 | BCE Loss: 0.9895347952842712\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 5.392106533050537 | KNN Loss: 4.3720197677612305 | BCE Loss: 1.020086646080017\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 5.327679634094238 | KNN Loss: 4.335599422454834 | BCE Loss: 0.9920804500579834\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 5.418219566345215 | KNN Loss: 4.354555606842041 | BCE Loss: 1.063664197921753\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 5.374545574188232 | KNN Loss: 4.358353137969971 | BCE Loss: 1.0161924362182617\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 5.357047080993652 | KNN Loss: 4.3424973487854 | BCE Loss: 1.014549732208252\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 5.322775840759277 | KNN Loss: 4.307669639587402 | BCE Loss: 1.015106439590454\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 5.3210296630859375 | KNN Loss: 4.308804512023926 | BCE Loss: 1.0122252702713013\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 5.400637626647949 | KNN Loss: 4.351212024688721 | BCE Loss: 1.0494258403778076\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 5.380892276763916 | KNN Loss: 4.369987487792969 | BCE Loss: 1.0109047889709473\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 5.317052841186523 | KNN Loss: 4.322056770324707 | BCE Loss: 0.9949959516525269\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 5.350332260131836 | KNN Loss: 4.32493257522583 | BCE Loss: 1.0253994464874268\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 5.4045090675354 | KNN Loss: 4.370974540710449 | BCE Loss: 1.0335344076156616\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 5.35302734375 | KNN Loss: 4.332951068878174 | BCE Loss: 1.0200765132904053\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 5.396607875823975 | KNN Loss: 4.388492107391357 | BCE Loss: 1.0081158876419067\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 5.379643440246582 | KNN Loss: 4.349531650543213 | BCE Loss: 1.0301119089126587\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 5.3349609375 | KNN Loss: 4.32961893081665 | BCE Loss: 1.0053417682647705\n",
      "Epoch   240: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 5.398588180541992 | KNN Loss: 4.377732276916504 | BCE Loss: 1.0208561420440674\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 5.358466625213623 | KNN Loss: 4.350879192352295 | BCE Loss: 1.0075875520706177\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 5.381697177886963 | KNN Loss: 4.356561660766602 | BCE Loss: 1.0251353979110718\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 5.377541542053223 | KNN Loss: 4.337163925170898 | BCE Loss: 1.0403776168823242\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 5.395341396331787 | KNN Loss: 4.374791622161865 | BCE Loss: 1.0205497741699219\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 5.4214043617248535 | KNN Loss: 4.3563971519470215 | BCE Loss: 1.0650070905685425\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 5.36865234375 | KNN Loss: 4.365142345428467 | BCE Loss: 1.0035099983215332\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 5.366555213928223 | KNN Loss: 4.331397533416748 | BCE Loss: 1.0351574420928955\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 5.3436174392700195 | KNN Loss: 4.32418155670166 | BCE Loss: 1.0194358825683594\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 5.343729019165039 | KNN Loss: 4.3317108154296875 | BCE Loss: 1.0120184421539307\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 5.395360946655273 | KNN Loss: 4.329996585845947 | BCE Loss: 1.0653644800186157\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 5.394552230834961 | KNN Loss: 4.345956802368164 | BCE Loss: 1.0485954284667969\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 5.382820129394531 | KNN Loss: 4.358129978179932 | BCE Loss: 1.0246899127960205\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 5.341850757598877 | KNN Loss: 4.31479549407959 | BCE Loss: 1.027055263519287\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 5.307141304016113 | KNN Loss: 4.29500150680542 | BCE Loss: 1.0121395587921143\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 5.352921009063721 | KNN Loss: 4.3445844650268555 | BCE Loss: 1.0083364248275757\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 5.373543739318848 | KNN Loss: 4.338271617889404 | BCE Loss: 1.0352721214294434\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 5.395582675933838 | KNN Loss: 4.376303195953369 | BCE Loss: 1.0192793607711792\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 5.300238132476807 | KNN Loss: 4.297767162322998 | BCE Loss: 1.0024710893630981\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 5.403988361358643 | KNN Loss: 4.372758388519287 | BCE Loss: 1.0312299728393555\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 5.352839946746826 | KNN Loss: 4.358017444610596 | BCE Loss: 0.9948226809501648\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 5.371764183044434 | KNN Loss: 4.344457149505615 | BCE Loss: 1.0273067951202393\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 5.363280296325684 | KNN Loss: 4.338165760040283 | BCE Loss: 1.0251147747039795\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 5.386634826660156 | KNN Loss: 4.343576908111572 | BCE Loss: 1.0430576801300049\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 5.356704235076904 | KNN Loss: 4.333222389221191 | BCE Loss: 1.0234817266464233\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 5.308279991149902 | KNN Loss: 4.306337833404541 | BCE Loss: 1.0019423961639404\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 5.3174729347229 | KNN Loss: 4.3260321617126465 | BCE Loss: 0.9914407730102539\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 5.32991361618042 | KNN Loss: 4.306593418121338 | BCE Loss: 1.0233203172683716\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 5.391108989715576 | KNN Loss: 4.347650527954102 | BCE Loss: 1.043458342552185\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 5.363264083862305 | KNN Loss: 4.331546783447266 | BCE Loss: 1.0317171812057495\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 5.424450397491455 | KNN Loss: 4.396028995513916 | BCE Loss: 1.0284215211868286\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 5.352001667022705 | KNN Loss: 4.335750579833984 | BCE Loss: 1.0162510871887207\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 5.312536239624023 | KNN Loss: 4.310593128204346 | BCE Loss: 1.0019428730010986\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 5.338855266571045 | KNN Loss: 4.316965103149414 | BCE Loss: 1.0218900442123413\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 5.3886237144470215 | KNN Loss: 4.341116428375244 | BCE Loss: 1.0475071668624878\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 5.397566795349121 | KNN Loss: 4.366261005401611 | BCE Loss: 1.0313060283660889\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 5.364396095275879 | KNN Loss: 4.3475494384765625 | BCE Loss: 1.0168466567993164\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 5.413318157196045 | KNN Loss: 4.372182369232178 | BCE Loss: 1.0411359071731567\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 5.377124786376953 | KNN Loss: 4.331597328186035 | BCE Loss: 1.045527696609497\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 5.337302207946777 | KNN Loss: 4.324230670928955 | BCE Loss: 1.0130712985992432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 5.411556243896484 | KNN Loss: 4.341714859008789 | BCE Loss: 1.0698415040969849\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 5.374431610107422 | KNN Loss: 4.358969211578369 | BCE Loss: 1.0154621601104736\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 5.373733997344971 | KNN Loss: 4.3409576416015625 | BCE Loss: 1.0327763557434082\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 5.409998416900635 | KNN Loss: 4.397111415863037 | BCE Loss: 1.0128871202468872\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 5.320089340209961 | KNN Loss: 4.322471618652344 | BCE Loss: 0.9976178407669067\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 5.359219551086426 | KNN Loss: 4.320870399475098 | BCE Loss: 1.038348913192749\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 5.379520416259766 | KNN Loss: 4.336143970489502 | BCE Loss: 1.0433766841888428\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 5.332324981689453 | KNN Loss: 4.322441577911377 | BCE Loss: 1.0098836421966553\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 5.331323623657227 | KNN Loss: 4.316038131713867 | BCE Loss: 1.0152857303619385\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 5.343127250671387 | KNN Loss: 4.334127426147461 | BCE Loss: 1.0089998245239258\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 5.383365631103516 | KNN Loss: 4.3866705894470215 | BCE Loss: 0.9966948628425598\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 5.310199737548828 | KNN Loss: 4.334263801574707 | BCE Loss: 0.9759358167648315\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 5.431089401245117 | KNN Loss: 4.40537691116333 | BCE Loss: 1.0257126092910767\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 5.3835601806640625 | KNN Loss: 4.34395694732666 | BCE Loss: 1.0396034717559814\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 5.370767116546631 | KNN Loss: 4.3648681640625 | BCE Loss: 1.0058989524841309\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 5.3589630126953125 | KNN Loss: 4.364458084106445 | BCE Loss: 0.9945051670074463\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 5.3833465576171875 | KNN Loss: 4.355985164642334 | BCE Loss: 1.0273616313934326\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 5.337247371673584 | KNN Loss: 4.322988510131836 | BCE Loss: 1.014258861541748\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 5.349171161651611 | KNN Loss: 4.326419353485107 | BCE Loss: 1.0227516889572144\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 5.347614288330078 | KNN Loss: 4.3196282386779785 | BCE Loss: 1.0279861688613892\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 5.318639278411865 | KNN Loss: 4.3156256675720215 | BCE Loss: 1.0030136108398438\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 5.3654279708862305 | KNN Loss: 4.31681489944458 | BCE Loss: 1.0486128330230713\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 5.357746124267578 | KNN Loss: 4.357019424438477 | BCE Loss: 1.0007269382476807\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 5.363558292388916 | KNN Loss: 4.333710193634033 | BCE Loss: 1.0298480987548828\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 5.368711948394775 | KNN Loss: 4.342693328857422 | BCE Loss: 1.0260186195373535\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 5.405344486236572 | KNN Loss: 4.353804588317871 | BCE Loss: 1.0515398979187012\n",
      "Epoch   251: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 5.295873641967773 | KNN Loss: 4.315762996673584 | BCE Loss: 0.9801106452941895\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 5.300156116485596 | KNN Loss: 4.3106689453125 | BCE Loss: 0.9894871711730957\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 5.362969875335693 | KNN Loss: 4.349024772644043 | BCE Loss: 1.0139449834823608\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 5.36269474029541 | KNN Loss: 4.326568603515625 | BCE Loss: 1.0361263751983643\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 5.402259826660156 | KNN Loss: 4.3541388511657715 | BCE Loss: 1.0481209754943848\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 5.392021179199219 | KNN Loss: 4.344408988952637 | BCE Loss: 1.047611951828003\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 5.365115642547607 | KNN Loss: 4.33563756942749 | BCE Loss: 1.0294780731201172\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 5.390138626098633 | KNN Loss: 4.362744331359863 | BCE Loss: 1.027394413948059\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 5.389252662658691 | KNN Loss: 4.364176273345947 | BCE Loss: 1.0250766277313232\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 5.374143123626709 | KNN Loss: 4.352919578552246 | BCE Loss: 1.0212236642837524\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 5.407462120056152 | KNN Loss: 4.360334396362305 | BCE Loss: 1.0471277236938477\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 5.361444473266602 | KNN Loss: 4.320257663726807 | BCE Loss: 1.041186809539795\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 5.364424705505371 | KNN Loss: 4.334921360015869 | BCE Loss: 1.029503583908081\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 5.378954887390137 | KNN Loss: 4.3526530265808105 | BCE Loss: 1.026301622390747\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 5.338239669799805 | KNN Loss: 4.345310211181641 | BCE Loss: 0.9929294586181641\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 5.343885898590088 | KNN Loss: 4.336460590362549 | BCE Loss: 1.007425308227539\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 5.353621482849121 | KNN Loss: 4.320777416229248 | BCE Loss: 1.0328443050384521\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 5.352604866027832 | KNN Loss: 4.343527317047119 | BCE Loss: 1.009077787399292\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 5.326202392578125 | KNN Loss: 4.318502426147461 | BCE Loss: 1.007699966430664\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 5.40332555770874 | KNN Loss: 4.410000324249268 | BCE Loss: 0.9933253526687622\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 5.373070240020752 | KNN Loss: 4.3520402908325195 | BCE Loss: 1.0210299491882324\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 5.397505283355713 | KNN Loss: 4.348273754119873 | BCE Loss: 1.0492315292358398\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 5.384881019592285 | KNN Loss: 4.363605976104736 | BCE Loss: 1.021275281906128\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 5.356147766113281 | KNN Loss: 4.3413920402526855 | BCE Loss: 1.0147559642791748\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 5.357356548309326 | KNN Loss: 4.328629493713379 | BCE Loss: 1.0287270545959473\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 5.41092586517334 | KNN Loss: 4.3601202964782715 | BCE Loss: 1.0508053302764893\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 5.327148914337158 | KNN Loss: 4.32327127456665 | BCE Loss: 1.0038776397705078\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 5.340224266052246 | KNN Loss: 4.337205410003662 | BCE Loss: 1.003018856048584\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 5.381087779998779 | KNN Loss: 4.34487771987915 | BCE Loss: 1.0362099409103394\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 5.383421897888184 | KNN Loss: 4.347564220428467 | BCE Loss: 1.0358577966690063\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 5.351912498474121 | KNN Loss: 4.34282922744751 | BCE Loss: 1.0090830326080322\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 5.4100236892700195 | KNN Loss: 4.373353481292725 | BCE Loss: 1.036670446395874\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 5.3556952476501465 | KNN Loss: 4.337860584259033 | BCE Loss: 1.0178347826004028\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 5.370825290679932 | KNN Loss: 4.3456315994262695 | BCE Loss: 1.025193691253662\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 5.31010627746582 | KNN Loss: 4.301712512969971 | BCE Loss: 1.0083937644958496\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 5.34527063369751 | KNN Loss: 4.320766925811768 | BCE Loss: 1.0245035886764526\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 5.315887451171875 | KNN Loss: 4.318165302276611 | BCE Loss: 0.9977219700813293\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 5.4348297119140625 | KNN Loss: 4.402104377746582 | BCE Loss: 1.0327250957489014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 5.407764911651611 | KNN Loss: 4.372744083404541 | BCE Loss: 1.0350208282470703\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 5.310039520263672 | KNN Loss: 4.310190200805664 | BCE Loss: 0.9998493194580078\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 5.365618705749512 | KNN Loss: 4.333090782165527 | BCE Loss: 1.0325276851654053\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 5.347814083099365 | KNN Loss: 4.32740592956543 | BCE Loss: 1.0204081535339355\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 5.323830604553223 | KNN Loss: 4.3236541748046875 | BCE Loss: 1.0001763105392456\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 5.332890033721924 | KNN Loss: 4.319753646850586 | BCE Loss: 1.013136386871338\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 5.363752365112305 | KNN Loss: 4.340979099273682 | BCE Loss: 1.022773265838623\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 5.34808349609375 | KNN Loss: 4.326876163482666 | BCE Loss: 1.0212074518203735\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 5.364189147949219 | KNN Loss: 4.368435382843018 | BCE Loss: 0.9957540035247803\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 5.338861465454102 | KNN Loss: 4.304681301116943 | BCE Loss: 1.034179925918579\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 5.365080833435059 | KNN Loss: 4.357640743255615 | BCE Loss: 1.0074400901794434\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 5.342501640319824 | KNN Loss: 4.33422327041626 | BCE Loss: 1.0082786083221436\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 5.420156478881836 | KNN Loss: 4.40427827835083 | BCE Loss: 1.0158779621124268\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 5.3721771240234375 | KNN Loss: 4.34735631942749 | BCE Loss: 1.0248205661773682\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 5.401029109954834 | KNN Loss: 4.360559463500977 | BCE Loss: 1.0404696464538574\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 5.377448081970215 | KNN Loss: 4.356348514556885 | BCE Loss: 1.021099328994751\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 5.371066093444824 | KNN Loss: 4.327892303466797 | BCE Loss: 1.0431740283966064\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 5.35588264465332 | KNN Loss: 4.346827030181885 | BCE Loss: 1.009055495262146\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 5.37407112121582 | KNN Loss: 4.352826118469238 | BCE Loss: 1.021245002746582\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 5.382403373718262 | KNN Loss: 4.354710102081299 | BCE Loss: 1.0276931524276733\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 5.364339351654053 | KNN Loss: 4.3261919021606445 | BCE Loss: 1.0381474494934082\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 5.367436408996582 | KNN Loss: 4.354844570159912 | BCE Loss: 1.012592077255249\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 5.394961357116699 | KNN Loss: 4.346776008605957 | BCE Loss: 1.0481855869293213\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 5.348530292510986 | KNN Loss: 4.344089984893799 | BCE Loss: 1.0044403076171875\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 5.3396382331848145 | KNN Loss: 4.34345817565918 | BCE Loss: 0.9961801767349243\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 5.321386337280273 | KNN Loss: 4.319309711456299 | BCE Loss: 1.0020763874053955\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 5.395329475402832 | KNN Loss: 4.356907367706299 | BCE Loss: 1.0384223461151123\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 5.389533519744873 | KNN Loss: 4.371114253997803 | BCE Loss: 1.0184191465377808\n",
      "Epoch   262: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 5.336467742919922 | KNN Loss: 4.318682670593262 | BCE Loss: 1.0177853107452393\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 5.323840618133545 | KNN Loss: 4.323121547698975 | BCE Loss: 1.0007190704345703\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 5.350803375244141 | KNN Loss: 4.316723346710205 | BCE Loss: 1.0340797901153564\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 5.378751754760742 | KNN Loss: 4.371176242828369 | BCE Loss: 1.007575511932373\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 5.35304069519043 | KNN Loss: 4.335993766784668 | BCE Loss: 1.0170466899871826\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 5.352971076965332 | KNN Loss: 4.305660247802734 | BCE Loss: 1.0473108291625977\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 5.380705833435059 | KNN Loss: 4.333686828613281 | BCE Loss: 1.0470187664031982\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 5.372159957885742 | KNN Loss: 4.3607306480407715 | BCE Loss: 1.0114295482635498\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 5.3841423988342285 | KNN Loss: 4.337303161621094 | BCE Loss: 1.0468393564224243\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 5.346645355224609 | KNN Loss: 4.325285911560059 | BCE Loss: 1.0213596820831299\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 5.387558937072754 | KNN Loss: 4.357353687286377 | BCE Loss: 1.030205488204956\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 5.314095497131348 | KNN Loss: 4.3219194412231445 | BCE Loss: 0.9921760559082031\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 5.357644557952881 | KNN Loss: 4.349472522735596 | BCE Loss: 1.0081720352172852\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 5.339770317077637 | KNN Loss: 4.345137119293213 | BCE Loss: 0.9946334362030029\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 5.348653793334961 | KNN Loss: 4.327999591827393 | BCE Loss: 1.0206544399261475\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 5.3396148681640625 | KNN Loss: 4.322702884674072 | BCE Loss: 1.0169119834899902\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 5.350439071655273 | KNN Loss: 4.335966110229492 | BCE Loss: 1.0144727230072021\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 5.340529441833496 | KNN Loss: 4.330519676208496 | BCE Loss: 1.0100098848342896\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 5.352688789367676 | KNN Loss: 4.341275691986084 | BCE Loss: 1.0114130973815918\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 5.369569301605225 | KNN Loss: 4.329596042633057 | BCE Loss: 1.039973258972168\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 5.369653224945068 | KNN Loss: 4.325536727905273 | BCE Loss: 1.044116497039795\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 5.370204448699951 | KNN Loss: 4.329817771911621 | BCE Loss: 1.04038667678833\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 5.404315948486328 | KNN Loss: 4.371639251708984 | BCE Loss: 1.0326764583587646\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 5.344731330871582 | KNN Loss: 4.340821266174316 | BCE Loss: 1.0039098262786865\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 5.373503684997559 | KNN Loss: 4.3624444007873535 | BCE Loss: 1.011059045791626\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 5.375509738922119 | KNN Loss: 4.363821983337402 | BCE Loss: 1.0116876363754272\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 5.334968566894531 | KNN Loss: 4.327482223510742 | BCE Loss: 1.00748610496521\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 5.37498140335083 | KNN Loss: 4.348213195800781 | BCE Loss: 1.0267682075500488\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 5.357879161834717 | KNN Loss: 4.327678203582764 | BCE Loss: 1.0302009582519531\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 5.333171367645264 | KNN Loss: 4.318843841552734 | BCE Loss: 1.0143275260925293\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 5.3946638107299805 | KNN Loss: 4.374803066253662 | BCE Loss: 1.0198609828948975\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 5.363984107971191 | KNN Loss: 4.359606742858887 | BCE Loss: 1.0043773651123047\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 5.3531270027160645 | KNN Loss: 4.313517093658447 | BCE Loss: 1.0396100282669067\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 5.360572814941406 | KNN Loss: 4.346795558929443 | BCE Loss: 1.013777256011963\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 5.367194175720215 | KNN Loss: 4.335061550140381 | BCE Loss: 1.0321323871612549\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 5.3426008224487305 | KNN Loss: 4.367156505584717 | BCE Loss: 0.9754441976547241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 5.329870700836182 | KNN Loss: 4.333670616149902 | BCE Loss: 0.996199905872345\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 5.408700466156006 | KNN Loss: 4.367873668670654 | BCE Loss: 1.0408267974853516\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 5.318916320800781 | KNN Loss: 4.297286510467529 | BCE Loss: 1.0216299295425415\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 5.383553504943848 | KNN Loss: 4.333485126495361 | BCE Loss: 1.0500683784484863\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 5.342248916625977 | KNN Loss: 4.355373382568359 | BCE Loss: 0.9868755340576172\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 5.36327600479126 | KNN Loss: 4.367438793182373 | BCE Loss: 0.9958371520042419\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 5.384873390197754 | KNN Loss: 4.375438213348389 | BCE Loss: 1.0094350576400757\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 5.381702899932861 | KNN Loss: 4.366947174072266 | BCE Loss: 1.0147557258605957\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 5.416259765625 | KNN Loss: 4.376039505004883 | BCE Loss: 1.040220022201538\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 5.349479675292969 | KNN Loss: 4.342423915863037 | BCE Loss: 1.0070558786392212\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 5.388010501861572 | KNN Loss: 4.353869915008545 | BCE Loss: 1.0341405868530273\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 5.337058067321777 | KNN Loss: 4.326772212982178 | BCE Loss: 1.0102858543395996\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 5.372246742248535 | KNN Loss: 4.342048168182373 | BCE Loss: 1.030198574066162\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 5.369990348815918 | KNN Loss: 4.352290630340576 | BCE Loss: 1.017699956893921\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 5.351507186889648 | KNN Loss: 4.347269535064697 | BCE Loss: 1.0042376518249512\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 5.320181369781494 | KNN Loss: 4.341407775878906 | BCE Loss: 0.9787734150886536\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 5.354770183563232 | KNN Loss: 4.317131042480469 | BCE Loss: 1.0376391410827637\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 5.359753131866455 | KNN Loss: 4.338717460632324 | BCE Loss: 1.0210356712341309\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 5.335533618927002 | KNN Loss: 4.328391075134277 | BCE Loss: 1.007142424583435\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 5.404962539672852 | KNN Loss: 4.372780799865723 | BCE Loss: 1.032181978225708\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 5.364418029785156 | KNN Loss: 4.3629374504089355 | BCE Loss: 1.0014805793762207\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 5.342497825622559 | KNN Loss: 4.331626892089844 | BCE Loss: 1.010871171951294\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 5.38432502746582 | KNN Loss: 4.379779815673828 | BCE Loss: 1.0045452117919922\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 5.386722564697266 | KNN Loss: 4.377176761627197 | BCE Loss: 1.009545922279358\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 5.373260498046875 | KNN Loss: 4.337779998779297 | BCE Loss: 1.035480260848999\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 5.316855430603027 | KNN Loss: 4.3151373863220215 | BCE Loss: 1.0017180442810059\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 5.333238124847412 | KNN Loss: 4.320405960083008 | BCE Loss: 1.0128321647644043\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 5.357933044433594 | KNN Loss: 4.343233585357666 | BCE Loss: 1.0146996974945068\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 5.3514018058776855 | KNN Loss: 4.322381019592285 | BCE Loss: 1.0290206670761108\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 5.439350128173828 | KNN Loss: 4.388695240020752 | BCE Loss: 1.0506548881530762\n",
      "Epoch   273: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 5.341770172119141 | KNN Loss: 4.357333660125732 | BCE Loss: 0.984436571598053\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 5.34248161315918 | KNN Loss: 4.326868534088135 | BCE Loss: 1.0156128406524658\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 5.345925331115723 | KNN Loss: 4.3248796463012695 | BCE Loss: 1.0210455656051636\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 5.430964469909668 | KNN Loss: 4.364351272583008 | BCE Loss: 1.0666133165359497\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 5.371182918548584 | KNN Loss: 4.339818954467773 | BCE Loss: 1.0313639640808105\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 5.341573715209961 | KNN Loss: 4.312586307525635 | BCE Loss: 1.028987169265747\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 5.35158634185791 | KNN Loss: 4.34528112411499 | BCE Loss: 1.00630521774292\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 5.3432841300964355 | KNN Loss: 4.336716175079346 | BCE Loss: 1.0065679550170898\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 5.3768086433410645 | KNN Loss: 4.346490859985352 | BCE Loss: 1.030317783355713\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 5.385762691497803 | KNN Loss: 4.37654972076416 | BCE Loss: 1.0092129707336426\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 5.381856918334961 | KNN Loss: 4.351157188415527 | BCE Loss: 1.0306997299194336\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 5.361175537109375 | KNN Loss: 4.324888229370117 | BCE Loss: 1.0362873077392578\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 5.328522682189941 | KNN Loss: 4.321084976196289 | BCE Loss: 1.0074374675750732\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 5.3570733070373535 | KNN Loss: 4.346337795257568 | BCE Loss: 1.0107356309890747\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 5.410464286804199 | KNN Loss: 4.382874965667725 | BCE Loss: 1.0275895595550537\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 5.303025245666504 | KNN Loss: 4.307155132293701 | BCE Loss: 0.9958701133728027\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 5.3544769287109375 | KNN Loss: 4.356266021728516 | BCE Loss: 0.9982110261917114\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 5.408045291900635 | KNN Loss: 4.37835693359375 | BCE Loss: 1.0296882390975952\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 5.371966361999512 | KNN Loss: 4.344717025756836 | BCE Loss: 1.0272490978240967\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 5.30232572555542 | KNN Loss: 4.318539619445801 | BCE Loss: 0.9837859869003296\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 5.333463668823242 | KNN Loss: 4.337497234344482 | BCE Loss: 0.9959663152694702\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 5.413473606109619 | KNN Loss: 4.391990661621094 | BCE Loss: 1.0214829444885254\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 5.365304946899414 | KNN Loss: 4.3414764404296875 | BCE Loss: 1.0238285064697266\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 5.346186637878418 | KNN Loss: 4.311915397644043 | BCE Loss: 1.034271240234375\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 5.3866353034973145 | KNN Loss: 4.348788261413574 | BCE Loss: 1.0378471612930298\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 5.352277755737305 | KNN Loss: 4.346946716308594 | BCE Loss: 1.005331039428711\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 5.330849647521973 | KNN Loss: 4.335664749145508 | BCE Loss: 0.9951846599578857\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 5.330291271209717 | KNN Loss: 4.322869777679443 | BCE Loss: 1.007421612739563\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 5.37246561050415 | KNN Loss: 4.352161407470703 | BCE Loss: 1.0203042030334473\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 5.403573989868164 | KNN Loss: 4.355233669281006 | BCE Loss: 1.0483403205871582\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 5.378268241882324 | KNN Loss: 4.344728469848633 | BCE Loss: 1.0335395336151123\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 5.4285569190979 | KNN Loss: 4.376856327056885 | BCE Loss: 1.0517007112503052\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 5.381730556488037 | KNN Loss: 4.362826824188232 | BCE Loss: 1.0189037322998047\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 5.368539810180664 | KNN Loss: 4.354564189910889 | BCE Loss: 1.0139758586883545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 5.398136615753174 | KNN Loss: 4.381958484649658 | BCE Loss: 1.0161781311035156\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 5.3371429443359375 | KNN Loss: 4.339204788208008 | BCE Loss: 0.9979379773139954\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 5.379116535186768 | KNN Loss: 4.3719635009765625 | BCE Loss: 1.0071529150009155\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 5.39136266708374 | KNN Loss: 4.3366498947143555 | BCE Loss: 1.0547127723693848\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 5.360705852508545 | KNN Loss: 4.357628345489502 | BCE Loss: 1.003077507019043\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 5.312936782836914 | KNN Loss: 4.312992572784424 | BCE Loss: 0.9999443292617798\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 5.370644569396973 | KNN Loss: 4.340776443481445 | BCE Loss: 1.0298678874969482\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 5.346396446228027 | KNN Loss: 4.316761493682861 | BCE Loss: 1.0296348333358765\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 5.375784873962402 | KNN Loss: 4.358590126037598 | BCE Loss: 1.0171945095062256\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 5.377013683319092 | KNN Loss: 4.372898578643799 | BCE Loss: 1.004115104675293\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 5.348193645477295 | KNN Loss: 4.347228050231934 | BCE Loss: 1.0009654760360718\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 5.392246246337891 | KNN Loss: 4.348665237426758 | BCE Loss: 1.0435808897018433\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 5.374486923217773 | KNN Loss: 4.360215663909912 | BCE Loss: 1.0142712593078613\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 5.342865467071533 | KNN Loss: 4.347843647003174 | BCE Loss: 0.9950217604637146\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 5.40603494644165 | KNN Loss: 4.367943286895752 | BCE Loss: 1.038091778755188\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 5.379358291625977 | KNN Loss: 4.365610599517822 | BCE Loss: 1.0137476921081543\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 5.374124526977539 | KNN Loss: 4.364031791687012 | BCE Loss: 1.0100929737091064\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 5.406880855560303 | KNN Loss: 4.375457286834717 | BCE Loss: 1.0314236879348755\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 5.339287757873535 | KNN Loss: 4.320651531219482 | BCE Loss: 1.0186364650726318\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 5.373815059661865 | KNN Loss: 4.357962131500244 | BCE Loss: 1.0158530473709106\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 5.369411468505859 | KNN Loss: 4.365240573883057 | BCE Loss: 1.0041708946228027\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 5.351396560668945 | KNN Loss: 4.317873001098633 | BCE Loss: 1.0335237979888916\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 5.36652946472168 | KNN Loss: 4.333489894866943 | BCE Loss: 1.0330395698547363\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 5.372901439666748 | KNN Loss: 4.349567890167236 | BCE Loss: 1.0233334302902222\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 5.323402404785156 | KNN Loss: 4.30931282043457 | BCE Loss: 1.014089822769165\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 5.419764995574951 | KNN Loss: 4.399489402770996 | BCE Loss: 1.020275592803955\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 5.353137016296387 | KNN Loss: 4.330441474914551 | BCE Loss: 1.0226954221725464\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 5.367099761962891 | KNN Loss: 4.362576484680176 | BCE Loss: 1.0045230388641357\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 5.385537624359131 | KNN Loss: 4.363158226013184 | BCE Loss: 1.0223792791366577\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 5.376733303070068 | KNN Loss: 4.331641674041748 | BCE Loss: 1.0450916290283203\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 5.367086887359619 | KNN Loss: 4.332920551300049 | BCE Loss: 1.0341663360595703\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 5.41389274597168 | KNN Loss: 4.377211093902588 | BCE Loss: 1.036681890487671\n",
      "Epoch   284: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 5.39145565032959 | KNN Loss: 4.3469462394714355 | BCE Loss: 1.0445091724395752\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 5.356283187866211 | KNN Loss: 4.35543966293335 | BCE Loss: 1.0008436441421509\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 5.38495397567749 | KNN Loss: 4.366293430328369 | BCE Loss: 1.018660545349121\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 5.387416839599609 | KNN Loss: 4.327538967132568 | BCE Loss: 1.0598781108856201\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 5.3934245109558105 | KNN Loss: 4.348995208740234 | BCE Loss: 1.0444291830062866\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 5.348318099975586 | KNN Loss: 4.318767070770264 | BCE Loss: 1.0295511484146118\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 5.386813163757324 | KNN Loss: 4.367663860321045 | BCE Loss: 1.0191495418548584\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 5.348100662231445 | KNN Loss: 4.335834980010986 | BCE Loss: 1.012265682220459\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 5.389242172241211 | KNN Loss: 4.363539695739746 | BCE Loss: 1.025702714920044\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 5.366502285003662 | KNN Loss: 4.330006122589111 | BCE Loss: 1.0364961624145508\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 5.340524673461914 | KNN Loss: 4.33272647857666 | BCE Loss: 1.0077983140945435\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 5.348751544952393 | KNN Loss: 4.341491222381592 | BCE Loss: 1.0072604417800903\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 5.329192638397217 | KNN Loss: 4.318760871887207 | BCE Loss: 1.0104317665100098\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 5.397607326507568 | KNN Loss: 4.350565433502197 | BCE Loss: 1.047041893005371\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 5.334650039672852 | KNN Loss: 4.345866680145264 | BCE Loss: 0.9887831211090088\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 5.3549580574035645 | KNN Loss: 4.366523265838623 | BCE Loss: 0.9884347319602966\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 5.352701663970947 | KNN Loss: 4.325489044189453 | BCE Loss: 1.0272127389907837\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 5.387392997741699 | KNN Loss: 4.362942695617676 | BCE Loss: 1.0244505405426025\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 5.350549221038818 | KNN Loss: 4.323602199554443 | BCE Loss: 1.026947021484375\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 5.288498401641846 | KNN Loss: 4.302340030670166 | BCE Loss: 0.986158549785614\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 5.383557319641113 | KNN Loss: 4.345099449157715 | BCE Loss: 1.0384581089019775\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 5.358112335205078 | KNN Loss: 4.329920768737793 | BCE Loss: 1.0281914472579956\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 5.423032283782959 | KNN Loss: 4.393197059631348 | BCE Loss: 1.0298351049423218\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 5.35972785949707 | KNN Loss: 4.3046793937683105 | BCE Loss: 1.0550482273101807\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.303094387054443 | KNN Loss: 4.312719821929932 | BCE Loss: 0.9903746247291565\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 5.376323223114014 | KNN Loss: 4.364943981170654 | BCE Loss: 1.0113792419433594\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 5.342074871063232 | KNN Loss: 4.322627544403076 | BCE Loss: 1.0194473266601562\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 5.376528739929199 | KNN Loss: 4.3671159744262695 | BCE Loss: 1.0094127655029297\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 5.388627052307129 | KNN Loss: 4.368311405181885 | BCE Loss: 1.0203158855438232\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 5.355126857757568 | KNN Loss: 4.3479390144348145 | BCE Loss: 1.007187843322754\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 5.291018486022949 | KNN Loss: 4.308526515960693 | BCE Loss: 0.9824919700622559\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 5.406026363372803 | KNN Loss: 4.373688220977783 | BCE Loss: 1.0323381423950195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 5.425412654876709 | KNN Loss: 4.3963847160339355 | BCE Loss: 1.0290278196334839\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 5.341582298278809 | KNN Loss: 4.326824188232422 | BCE Loss: 1.0147583484649658\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 5.381550312042236 | KNN Loss: 4.370083808898926 | BCE Loss: 1.0114665031433105\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 5.351917266845703 | KNN Loss: 4.332967758178711 | BCE Loss: 1.0189496278762817\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 5.354959487915039 | KNN Loss: 4.3364033699035645 | BCE Loss: 1.0185561180114746\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 5.401604652404785 | KNN Loss: 4.352441310882568 | BCE Loss: 1.0491632223129272\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 5.330008029937744 | KNN Loss: 4.3344573974609375 | BCE Loss: 0.995550811290741\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 5.339901924133301 | KNN Loss: 4.32356595993042 | BCE Loss: 1.0163357257843018\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 5.315470218658447 | KNN Loss: 4.31953763961792 | BCE Loss: 0.9959325790405273\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 5.362871170043945 | KNN Loss: 4.3399457931518555 | BCE Loss: 1.0229253768920898\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 5.3807597160339355 | KNN Loss: 4.369185447692871 | BCE Loss: 1.0115742683410645\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 5.377937316894531 | KNN Loss: 4.340867519378662 | BCE Loss: 1.03706955909729\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 5.344462871551514 | KNN Loss: 4.33775520324707 | BCE Loss: 1.006707787513733\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 5.351017951965332 | KNN Loss: 4.330513954162598 | BCE Loss: 1.0205039978027344\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 5.397084712982178 | KNN Loss: 4.360682964324951 | BCE Loss: 1.0364017486572266\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 5.409633636474609 | KNN Loss: 4.371770858764648 | BCE Loss: 1.0378625392913818\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 5.366580486297607 | KNN Loss: 4.3423638343811035 | BCE Loss: 1.024216651916504\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 5.34585428237915 | KNN Loss: 4.361410617828369 | BCE Loss: 0.9844438433647156\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 5.39408540725708 | KNN Loss: 4.340507984161377 | BCE Loss: 1.0535774230957031\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 5.349621772766113 | KNN Loss: 4.3373847007751465 | BCE Loss: 1.0122369527816772\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 5.345185279846191 | KNN Loss: 4.312841415405273 | BCE Loss: 1.032343864440918\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 5.399384498596191 | KNN Loss: 4.364270210266113 | BCE Loss: 1.0351145267486572\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 5.342612266540527 | KNN Loss: 4.316685199737549 | BCE Loss: 1.0259273052215576\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 5.389001846313477 | KNN Loss: 4.3500189781188965 | BCE Loss: 1.03898286819458\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 5.365677833557129 | KNN Loss: 4.329990386962891 | BCE Loss: 1.0356874465942383\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 5.4166765213012695 | KNN Loss: 4.368243217468262 | BCE Loss: 1.0484333038330078\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 5.3944854736328125 | KNN Loss: 4.383891582489014 | BCE Loss: 1.0105936527252197\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 5.397965908050537 | KNN Loss: 4.35569429397583 | BCE Loss: 1.042271614074707\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 5.37856388092041 | KNN Loss: 4.368930816650391 | BCE Loss: 1.0096333026885986\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 5.357224941253662 | KNN Loss: 4.320356369018555 | BCE Loss: 1.0368685722351074\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 5.356741905212402 | KNN Loss: 4.332028388977051 | BCE Loss: 1.0247137546539307\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 5.380856513977051 | KNN Loss: 4.358412742614746 | BCE Loss: 1.0224435329437256\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 5.361069679260254 | KNN Loss: 4.372878551483154 | BCE Loss: 0.9881910681724548\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 5.385851860046387 | KNN Loss: 4.368603706359863 | BCE Loss: 1.0172483921051025\n",
      "Epoch   295: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 5.385699272155762 | KNN Loss: 4.365869998931885 | BCE Loss: 1.019829273223877\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 5.418001651763916 | KNN Loss: 4.404946804046631 | BCE Loss: 1.0130548477172852\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 5.373878002166748 | KNN Loss: 4.336967945098877 | BCE Loss: 1.036910057067871\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 5.36226749420166 | KNN Loss: 4.330064296722412 | BCE Loss: 1.032202959060669\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 5.345702171325684 | KNN Loss: 4.314235687255859 | BCE Loss: 1.0314666032791138\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 5.371758460998535 | KNN Loss: 4.35307502746582 | BCE Loss: 1.0186833143234253\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 5.369117259979248 | KNN Loss: 4.362764835357666 | BCE Loss: 1.006352424621582\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 5.364673137664795 | KNN Loss: 4.357941150665283 | BCE Loss: 1.0067319869995117\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 5.3435587882995605 | KNN Loss: 4.322875499725342 | BCE Loss: 1.0206831693649292\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 5.361005783081055 | KNN Loss: 4.354235649108887 | BCE Loss: 1.006770133972168\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 5.352231979370117 | KNN Loss: 4.329946517944336 | BCE Loss: 1.0222856998443604\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 5.352806091308594 | KNN Loss: 4.327265739440918 | BCE Loss: 1.0255403518676758\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 5.37704610824585 | KNN Loss: 4.332910060882568 | BCE Loss: 1.0441360473632812\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 5.338556289672852 | KNN Loss: 4.341070652008057 | BCE Loss: 0.9974856376647949\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 5.340704917907715 | KNN Loss: 4.306146621704102 | BCE Loss: 1.0345581769943237\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 5.314457416534424 | KNN Loss: 4.328669548034668 | BCE Loss: 0.9857878684997559\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 5.306850433349609 | KNN Loss: 4.323294639587402 | BCE Loss: 0.9835559129714966\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 5.368049144744873 | KNN Loss: 4.346256256103516 | BCE Loss: 1.0217928886413574\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 5.332089900970459 | KNN Loss: 4.322467803955078 | BCE Loss: 1.0096222162246704\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 5.396022796630859 | KNN Loss: 4.378718376159668 | BCE Loss: 1.017304539680481\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 5.339638710021973 | KNN Loss: 4.3187689781188965 | BCE Loss: 1.0208699703216553\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 5.41107177734375 | KNN Loss: 4.354567527770996 | BCE Loss: 1.0565043687820435\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 5.328925609588623 | KNN Loss: 4.31874418258667 | BCE Loss: 1.0101814270019531\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 5.328182220458984 | KNN Loss: 4.310309886932373 | BCE Loss: 1.0178723335266113\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 5.378369331359863 | KNN Loss: 4.380568027496338 | BCE Loss: 0.9978011250495911\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 5.333377838134766 | KNN Loss: 4.309597492218018 | BCE Loss: 1.023780107498169\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 5.371383190155029 | KNN Loss: 4.360297679901123 | BCE Loss: 1.0110855102539062\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 5.384302139282227 | KNN Loss: 4.389735698699951 | BCE Loss: 0.9945662021636963\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 5.371606349945068 | KNN Loss: 4.334535598754883 | BCE Loss: 1.0370707511901855\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 5.3837738037109375 | KNN Loss: 4.356138706207275 | BCE Loss: 1.027635097503662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 5.400449752807617 | KNN Loss: 4.355912685394287 | BCE Loss: 1.0445373058319092\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 5.391111373901367 | KNN Loss: 4.359960079193115 | BCE Loss: 1.031151533126831\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 5.391209125518799 | KNN Loss: 4.353403091430664 | BCE Loss: 1.0378060340881348\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 5.347984313964844 | KNN Loss: 4.3153886795043945 | BCE Loss: 1.0325953960418701\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 5.336020469665527 | KNN Loss: 4.323631286621094 | BCE Loss: 1.0123893022537231\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 5.398921489715576 | KNN Loss: 4.345242977142334 | BCE Loss: 1.0536786317825317\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 5.340672492980957 | KNN Loss: 4.352476119995117 | BCE Loss: 0.9881964921951294\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 5.395585060119629 | KNN Loss: 4.346435070037842 | BCE Loss: 1.049149990081787\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 5.368350982666016 | KNN Loss: 4.356741905212402 | BCE Loss: 1.0116088390350342\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 5.438541412353516 | KNN Loss: 4.416937351226807 | BCE Loss: 1.0216038227081299\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 5.313834190368652 | KNN Loss: 4.3200154304504395 | BCE Loss: 0.993818998336792\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 5.364287376403809 | KNN Loss: 4.3441548347473145 | BCE Loss: 1.0201324224472046\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 5.379124641418457 | KNN Loss: 4.354523181915283 | BCE Loss: 1.024601697921753\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 5.327714443206787 | KNN Loss: 4.333909034729004 | BCE Loss: 0.9938053488731384\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 5.397744178771973 | KNN Loss: 4.367862701416016 | BCE Loss: 1.029881477355957\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 5.391695976257324 | KNN Loss: 4.353236198425293 | BCE Loss: 1.0384597778320312\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 5.3967084884643555 | KNN Loss: 4.365350246429443 | BCE Loss: 1.031358003616333\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 5.374847412109375 | KNN Loss: 4.347314834594727 | BCE Loss: 1.0275325775146484\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 5.374605655670166 | KNN Loss: 4.341106414794922 | BCE Loss: 1.0334992408752441\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 5.409261226654053 | KNN Loss: 4.364175796508789 | BCE Loss: 1.0450854301452637\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 5.38658332824707 | KNN Loss: 4.3490400314331055 | BCE Loss: 1.0375430583953857\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 5.320484638214111 | KNN Loss: 4.313704967498779 | BCE Loss: 1.006779670715332\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 5.360419273376465 | KNN Loss: 4.355279445648193 | BCE Loss: 1.005139946937561\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 5.341053009033203 | KNN Loss: 4.325890064239502 | BCE Loss: 1.0151630640029907\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 5.41578483581543 | KNN Loss: 4.378501892089844 | BCE Loss: 1.0372830629348755\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 5.341602325439453 | KNN Loss: 4.342960357666016 | BCE Loss: 0.9986421465873718\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 5.35738468170166 | KNN Loss: 4.338165760040283 | BCE Loss: 1.019219160079956\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 5.379035472869873 | KNN Loss: 4.350639820098877 | BCE Loss: 1.0283955335617065\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 5.372613906860352 | KNN Loss: 4.3432135581970215 | BCE Loss: 1.02940034866333\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 5.362997531890869 | KNN Loss: 4.333108425140381 | BCE Loss: 1.0298891067504883\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 5.34837532043457 | KNN Loss: 4.329372406005859 | BCE Loss: 1.0190027952194214\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 5.370414733886719 | KNN Loss: 4.349630832672119 | BCE Loss: 1.0207836627960205\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 5.362996578216553 | KNN Loss: 4.334940433502197 | BCE Loss: 1.028056263923645\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 5.35702657699585 | KNN Loss: 4.362198352813721 | BCE Loss: 0.9948282837867737\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 5.382649898529053 | KNN Loss: 4.3665313720703125 | BCE Loss: 1.0161185264587402\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 5.357896327972412 | KNN Loss: 4.342813014984131 | BCE Loss: 1.0150834321975708\n",
      "Epoch   306: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 5.358784198760986 | KNN Loss: 4.330905437469482 | BCE Loss: 1.0278788805007935\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 5.398937702178955 | KNN Loss: 4.360843658447266 | BCE Loss: 1.0380939245224\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 5.350990295410156 | KNN Loss: 4.37554407119751 | BCE Loss: 0.9754464626312256\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 5.392563343048096 | KNN Loss: 4.387014389038086 | BCE Loss: 1.0055489540100098\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 5.324213981628418 | KNN Loss: 4.309750080108643 | BCE Loss: 1.014464020729065\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 5.329561233520508 | KNN Loss: 4.322382926940918 | BCE Loss: 1.0071780681610107\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 5.338433265686035 | KNN Loss: 4.3384199142456055 | BCE Loss: 1.0000132322311401\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 5.3542938232421875 | KNN Loss: 4.334034442901611 | BCE Loss: 1.0202593803405762\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 5.397233009338379 | KNN Loss: 4.360800266265869 | BCE Loss: 1.0364327430725098\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 5.337047576904297 | KNN Loss: 4.308676719665527 | BCE Loss: 1.0283706188201904\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 5.334151268005371 | KNN Loss: 4.327723979949951 | BCE Loss: 1.0064274072647095\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 5.307582855224609 | KNN Loss: 4.299195766448975 | BCE Loss: 1.0083873271942139\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 5.4530487060546875 | KNN Loss: 4.445185661315918 | BCE Loss: 1.0078632831573486\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 5.378995895385742 | KNN Loss: 4.353039741516113 | BCE Loss: 1.0259559154510498\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 5.369312763214111 | KNN Loss: 4.331432819366455 | BCE Loss: 1.0378799438476562\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 5.376406669616699 | KNN Loss: 4.339318752288818 | BCE Loss: 1.0370880365371704\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 5.32647705078125 | KNN Loss: 4.312456130981445 | BCE Loss: 1.0140209197998047\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 5.363254547119141 | KNN Loss: 4.361638069152832 | BCE Loss: 1.0016165971755981\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 5.31341552734375 | KNN Loss: 4.309396743774414 | BCE Loss: 1.004018783569336\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 5.365732669830322 | KNN Loss: 4.33939266204834 | BCE Loss: 1.0263400077819824\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 5.347955703735352 | KNN Loss: 4.3227386474609375 | BCE Loss: 1.0252171754837036\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 5.366443634033203 | KNN Loss: 4.3457489013671875 | BCE Loss: 1.0206944942474365\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 5.38718318939209 | KNN Loss: 4.337848663330078 | BCE Loss: 1.0493347644805908\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 5.336486339569092 | KNN Loss: 4.314869403839111 | BCE Loss: 1.0216169357299805\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 5.3747029304504395 | KNN Loss: 4.345954895019531 | BCE Loss: 1.0287479162216187\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 5.374787330627441 | KNN Loss: 4.3579254150390625 | BCE Loss: 1.016862154006958\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 5.373809814453125 | KNN Loss: 4.356206893920898 | BCE Loss: 1.0176026821136475\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 5.332805633544922 | KNN Loss: 4.319741725921631 | BCE Loss: 1.0130641460418701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 5.38198184967041 | KNN Loss: 4.354580402374268 | BCE Loss: 1.0274014472961426\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 5.357471466064453 | KNN Loss: 4.357285499572754 | BCE Loss: 1.0001859664916992\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 5.411555290222168 | KNN Loss: 4.376363277435303 | BCE Loss: 1.0351921319961548\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 5.3743133544921875 | KNN Loss: 4.352755546569824 | BCE Loss: 1.0215578079223633\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 5.37565279006958 | KNN Loss: 4.344982147216797 | BCE Loss: 1.0306705236434937\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 5.418898105621338 | KNN Loss: 4.391064167022705 | BCE Loss: 1.0278340578079224\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 5.36530876159668 | KNN Loss: 4.346837043762207 | BCE Loss: 1.018471598625183\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 5.347202777862549 | KNN Loss: 4.320523262023926 | BCE Loss: 1.0266796350479126\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 5.389509201049805 | KNN Loss: 4.348126411437988 | BCE Loss: 1.0413825511932373\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 5.340933799743652 | KNN Loss: 4.317978858947754 | BCE Loss: 1.0229551792144775\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 5.391530513763428 | KNN Loss: 4.346341609954834 | BCE Loss: 1.0451890230178833\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 5.350846290588379 | KNN Loss: 4.348926544189453 | BCE Loss: 1.0019197463989258\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 5.394352912902832 | KNN Loss: 4.369403839111328 | BCE Loss: 1.0249488353729248\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 5.376670837402344 | KNN Loss: 4.3346052169799805 | BCE Loss: 1.0420656204223633\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 5.348329544067383 | KNN Loss: 4.338137149810791 | BCE Loss: 1.0101921558380127\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 5.381628513336182 | KNN Loss: 4.3519158363342285 | BCE Loss: 1.0297125577926636\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 5.3489274978637695 | KNN Loss: 4.324971675872803 | BCE Loss: 1.023956060409546\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 5.3922624588012695 | KNN Loss: 4.360507011413574 | BCE Loss: 1.0317556858062744\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 5.358895301818848 | KNN Loss: 4.337499618530273 | BCE Loss: 1.0213954448699951\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 5.372309684753418 | KNN Loss: 4.347184658050537 | BCE Loss: 1.0251250267028809\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 5.381977081298828 | KNN Loss: 4.356306552886963 | BCE Loss: 1.0256707668304443\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 5.340673446655273 | KNN Loss: 4.3279337882995605 | BCE Loss: 1.012739896774292\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 5.427099227905273 | KNN Loss: 4.368797779083252 | BCE Loss: 1.0583012104034424\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 5.371046543121338 | KNN Loss: 4.377076148986816 | BCE Loss: 0.9939703941345215\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 5.375912666320801 | KNN Loss: 4.341697692871094 | BCE Loss: 1.034214735031128\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 5.3538312911987305 | KNN Loss: 4.323081016540527 | BCE Loss: 1.030750036239624\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 5.355424880981445 | KNN Loss: 4.351785182952881 | BCE Loss: 1.0036394596099854\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 5.385631561279297 | KNN Loss: 4.344537258148193 | BCE Loss: 1.0410945415496826\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 5.335339546203613 | KNN Loss: 4.316039562225342 | BCE Loss: 1.0192999839782715\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 5.344780445098877 | KNN Loss: 4.317022323608398 | BCE Loss: 1.027758240699768\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 5.328078269958496 | KNN Loss: 4.304450511932373 | BCE Loss: 1.023627519607544\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 5.331927299499512 | KNN Loss: 4.321685791015625 | BCE Loss: 1.0102416276931763\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 5.374774932861328 | KNN Loss: 4.382852554321289 | BCE Loss: 0.9919224977493286\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 5.4349164962768555 | KNN Loss: 4.381586074829102 | BCE Loss: 1.053330659866333\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 5.385468482971191 | KNN Loss: 4.359786510467529 | BCE Loss: 1.0256820917129517\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 5.378115653991699 | KNN Loss: 4.367356300354004 | BCE Loss: 1.0107594728469849\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 5.354744911193848 | KNN Loss: 4.325324058532715 | BCE Loss: 1.0294206142425537\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 5.312746524810791 | KNN Loss: 4.321086883544922 | BCE Loss: 0.9916597604751587\n",
      "Epoch   317: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 5.359748840332031 | KNN Loss: 4.347757816314697 | BCE Loss: 1.011991024017334\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 5.35155725479126 | KNN Loss: 4.340098857879639 | BCE Loss: 1.0114582777023315\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 5.37030029296875 | KNN Loss: 4.346033096313477 | BCE Loss: 1.0242670774459839\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 5.313725471496582 | KNN Loss: 4.318638801574707 | BCE Loss: 0.9950864315032959\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 5.359506130218506 | KNN Loss: 4.322038173675537 | BCE Loss: 1.0374678373336792\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 5.383394241333008 | KNN Loss: 4.34596586227417 | BCE Loss: 1.037428617477417\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 5.342508316040039 | KNN Loss: 4.325047969818115 | BCE Loss: 1.0174604654312134\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 5.3653669357299805 | KNN Loss: 4.330486297607422 | BCE Loss: 1.0348806381225586\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 5.39140510559082 | KNN Loss: 4.37095832824707 | BCE Loss: 1.0204468965530396\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 5.343495845794678 | KNN Loss: 4.331629753112793 | BCE Loss: 1.0118660926818848\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 5.3778276443481445 | KNN Loss: 4.337904453277588 | BCE Loss: 1.0399234294891357\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 5.329287052154541 | KNN Loss: 4.325371742248535 | BCE Loss: 1.0039153099060059\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 5.382052898406982 | KNN Loss: 4.3210248947143555 | BCE Loss: 1.0610281229019165\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 5.335755348205566 | KNN Loss: 4.331826210021973 | BCE Loss: 1.0039288997650146\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 5.334194183349609 | KNN Loss: 4.321135520935059 | BCE Loss: 1.0130586624145508\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 5.33363151550293 | KNN Loss: 4.3225417137146 | BCE Loss: 1.0110900402069092\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 5.34570837020874 | KNN Loss: 4.353930950164795 | BCE Loss: 0.9917775392532349\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 5.353389739990234 | KNN Loss: 4.349743843078613 | BCE Loss: 1.0036457777023315\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 5.350964546203613 | KNN Loss: 4.357166767120361 | BCE Loss: 0.993798017501831\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 5.372471809387207 | KNN Loss: 4.3422651290893555 | BCE Loss: 1.0302066802978516\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 5.316380500793457 | KNN Loss: 4.318374156951904 | BCE Loss: 0.9980065822601318\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 5.387320518493652 | KNN Loss: 4.3601250648498535 | BCE Loss: 1.0271954536437988\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 5.385251522064209 | KNN Loss: 4.3910136222839355 | BCE Loss: 0.9942379593849182\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 5.433104991912842 | KNN Loss: 4.382207870483398 | BCE Loss: 1.0508971214294434\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 5.368980407714844 | KNN Loss: 4.344435691833496 | BCE Loss: 1.0245447158813477\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 5.3712873458862305 | KNN Loss: 4.361572742462158 | BCE Loss: 1.0097146034240723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 5.354340553283691 | KNN Loss: 4.343430519104004 | BCE Loss: 1.0109102725982666\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 5.340439796447754 | KNN Loss: 4.328736305236816 | BCE Loss: 1.0117034912109375\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 5.387660026550293 | KNN Loss: 4.368063449859619 | BCE Loss: 1.019596815109253\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 5.352064609527588 | KNN Loss: 4.363457679748535 | BCE Loss: 0.9886068105697632\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 5.3659138679504395 | KNN Loss: 4.350650787353516 | BCE Loss: 1.0152629613876343\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 5.382969379425049 | KNN Loss: 4.358417987823486 | BCE Loss: 1.0245513916015625\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 5.37991189956665 | KNN Loss: 4.347390651702881 | BCE Loss: 1.0325212478637695\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 5.352431774139404 | KNN Loss: 4.341634750366211 | BCE Loss: 1.0107970237731934\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.379621505737305 | KNN Loss: 4.347778797149658 | BCE Loss: 1.031842589378357\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 5.403003692626953 | KNN Loss: 4.368718147277832 | BCE Loss: 1.0342856645584106\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 5.342350006103516 | KNN Loss: 4.323117256164551 | BCE Loss: 1.0192327499389648\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 5.351506233215332 | KNN Loss: 4.32423210144043 | BCE Loss: 1.027274250984192\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 5.352325439453125 | KNN Loss: 4.343095302581787 | BCE Loss: 1.009230136871338\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 5.392362594604492 | KNN Loss: 4.360424518585205 | BCE Loss: 1.031937837600708\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 5.340351104736328 | KNN Loss: 4.319468975067139 | BCE Loss: 1.0208820104599\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 5.358823776245117 | KNN Loss: 4.327693462371826 | BCE Loss: 1.031130313873291\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 5.392499923706055 | KNN Loss: 4.354781627655029 | BCE Loss: 1.0377182960510254\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 5.342058181762695 | KNN Loss: 4.323245048522949 | BCE Loss: 1.0188132524490356\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 5.3933820724487305 | KNN Loss: 4.354389667510986 | BCE Loss: 1.038992166519165\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 5.331199645996094 | KNN Loss: 4.324985504150391 | BCE Loss: 1.0062143802642822\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 5.3838043212890625 | KNN Loss: 4.349940299987793 | BCE Loss: 1.0338642597198486\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 5.413708686828613 | KNN Loss: 4.369857311248779 | BCE Loss: 1.043851613998413\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 5.378366470336914 | KNN Loss: 4.343826770782471 | BCE Loss: 1.0345396995544434\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 5.39459228515625 | KNN Loss: 4.349954605102539 | BCE Loss: 1.044637680053711\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 5.326872825622559 | KNN Loss: 4.344676494598389 | BCE Loss: 0.9821964502334595\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 5.368498802185059 | KNN Loss: 4.363882064819336 | BCE Loss: 1.0046164989471436\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 5.351633071899414 | KNN Loss: 4.362165451049805 | BCE Loss: 0.9894676208496094\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 5.328941822052002 | KNN Loss: 4.330592155456543 | BCE Loss: 0.9983498454093933\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 5.376330852508545 | KNN Loss: 4.358664512634277 | BCE Loss: 1.017666220664978\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 5.376001834869385 | KNN Loss: 4.379238128662109 | BCE Loss: 0.9967638850212097\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 5.359248161315918 | KNN Loss: 4.333689212799072 | BCE Loss: 1.0255591869354248\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 5.362939834594727 | KNN Loss: 4.337687969207764 | BCE Loss: 1.025251865386963\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 5.348283290863037 | KNN Loss: 4.308716297149658 | BCE Loss: 1.0395671129226685\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 5.337805271148682 | KNN Loss: 4.341185569763184 | BCE Loss: 0.9966197609901428\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 5.352315425872803 | KNN Loss: 4.327788352966309 | BCE Loss: 1.0245271921157837\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 5.347696304321289 | KNN Loss: 4.33809232711792 | BCE Loss: 1.0096040964126587\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 5.299066066741943 | KNN Loss: 4.308610439300537 | BCE Loss: 0.990455687046051\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 5.36075496673584 | KNN Loss: 4.378425121307373 | BCE Loss: 0.9823297262191772\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 5.39857292175293 | KNN Loss: 4.379870891571045 | BCE Loss: 1.0187020301818848\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 5.336942195892334 | KNN Loss: 4.312167644500732 | BCE Loss: 1.024774432182312\n",
      "Epoch   328: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 5.426708221435547 | KNN Loss: 4.385254859924316 | BCE Loss: 1.0414535999298096\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 5.384097099304199 | KNN Loss: 4.353060722351074 | BCE Loss: 1.031036138534546\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 5.369496822357178 | KNN Loss: 4.330221176147461 | BCE Loss: 1.0392755270004272\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 5.367053508758545 | KNN Loss: 4.345575332641602 | BCE Loss: 1.0214781761169434\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 5.3923492431640625 | KNN Loss: 4.366460800170898 | BCE Loss: 1.0258885622024536\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 5.377656936645508 | KNN Loss: 4.343658924102783 | BCE Loss: 1.0339980125427246\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 5.388532638549805 | KNN Loss: 4.35255241394043 | BCE Loss: 1.035979986190796\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 5.371771335601807 | KNN Loss: 4.349670886993408 | BCE Loss: 1.0221004486083984\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 5.374283790588379 | KNN Loss: 4.372677326202393 | BCE Loss: 1.0016065835952759\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 5.35350227355957 | KNN Loss: 4.322633743286133 | BCE Loss: 1.0308687686920166\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 5.343147277832031 | KNN Loss: 4.327451229095459 | BCE Loss: 1.0156960487365723\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 5.360438346862793 | KNN Loss: 4.334986686706543 | BCE Loss: 1.02545166015625\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 5.375008583068848 | KNN Loss: 4.34948205947876 | BCE Loss: 1.025526523590088\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 5.420276641845703 | KNN Loss: 4.364180088043213 | BCE Loss: 1.0560963153839111\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 5.359157562255859 | KNN Loss: 4.342970371246338 | BCE Loss: 1.0161874294281006\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 5.354347229003906 | KNN Loss: 4.328667163848877 | BCE Loss: 1.0256801843643188\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 5.363837242126465 | KNN Loss: 4.342146396636963 | BCE Loss: 1.021690845489502\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 5.368594169616699 | KNN Loss: 4.327427387237549 | BCE Loss: 1.0411665439605713\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 5.348341941833496 | KNN Loss: 4.328080177307129 | BCE Loss: 1.020261526107788\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 5.39682674407959 | KNN Loss: 4.343018531799316 | BCE Loss: 1.053808331489563\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 5.345327854156494 | KNN Loss: 4.325059413909912 | BCE Loss: 1.020268440246582\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 5.350978374481201 | KNN Loss: 4.309039115905762 | BCE Loss: 1.04193913936615\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 5.351831436157227 | KNN Loss: 4.350200653076172 | BCE Loss: 1.0016309022903442\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 5.334537982940674 | KNN Loss: 4.327966690063477 | BCE Loss: 1.0065711736679077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 5.367119789123535 | KNN Loss: 4.361668586730957 | BCE Loss: 1.0054514408111572\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 5.31496000289917 | KNN Loss: 4.325533390045166 | BCE Loss: 0.9894266128540039\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 5.4323625564575195 | KNN Loss: 4.3792853355407715 | BCE Loss: 1.0530774593353271\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 5.357402324676514 | KNN Loss: 4.332403659820557 | BCE Loss: 1.0249987840652466\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 5.369745254516602 | KNN Loss: 4.320011138916016 | BCE Loss: 1.049734115600586\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 5.366730213165283 | KNN Loss: 4.37130880355835 | BCE Loss: 0.9954215288162231\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 5.349822521209717 | KNN Loss: 4.35531759262085 | BCE Loss: 0.9945051074028015\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 5.394263744354248 | KNN Loss: 4.346890926361084 | BCE Loss: 1.0473729372024536\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 5.365816116333008 | KNN Loss: 4.330740451812744 | BCE Loss: 1.0350759029388428\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 5.409100532531738 | KNN Loss: 4.3518548011779785 | BCE Loss: 1.0572456121444702\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 5.341032981872559 | KNN Loss: 4.320096492767334 | BCE Loss: 1.0209362506866455\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 5.409834384918213 | KNN Loss: 4.390511989593506 | BCE Loss: 1.0193225145339966\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 5.409811019897461 | KNN Loss: 4.370894908905029 | BCE Loss: 1.0389158725738525\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 5.361743927001953 | KNN Loss: 4.327243804931641 | BCE Loss: 1.0345003604888916\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 5.365348815917969 | KNN Loss: 4.344090461730957 | BCE Loss: 1.0212581157684326\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 5.426201343536377 | KNN Loss: 4.38615083694458 | BCE Loss: 1.0400503873825073\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 5.348504066467285 | KNN Loss: 4.328070640563965 | BCE Loss: 1.0204331874847412\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 5.3429155349731445 | KNN Loss: 4.33937931060791 | BCE Loss: 1.0035361051559448\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 5.358628273010254 | KNN Loss: 4.308595657348633 | BCE Loss: 1.050032377243042\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 5.348837375640869 | KNN Loss: 4.337697505950928 | BCE Loss: 1.0111398696899414\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 5.368592262268066 | KNN Loss: 4.370556831359863 | BCE Loss: 0.9980356693267822\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 5.393909931182861 | KNN Loss: 4.383324146270752 | BCE Loss: 1.0105857849121094\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 5.3519792556762695 | KNN Loss: 4.334024429321289 | BCE Loss: 1.0179550647735596\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 5.3873772621154785 | KNN Loss: 4.37652587890625 | BCE Loss: 1.0108513832092285\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 5.373948097229004 | KNN Loss: 4.361760139465332 | BCE Loss: 1.012188196182251\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 5.392014026641846 | KNN Loss: 4.370270252227783 | BCE Loss: 1.0217437744140625\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 5.361318588256836 | KNN Loss: 4.32974100112915 | BCE Loss: 1.031577706336975\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 5.3377180099487305 | KNN Loss: 4.314602375030518 | BCE Loss: 1.023115873336792\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 5.342665672302246 | KNN Loss: 4.303983211517334 | BCE Loss: 1.0386826992034912\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 5.346303462982178 | KNN Loss: 4.3399834632873535 | BCE Loss: 1.0063199996948242\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 5.321863174438477 | KNN Loss: 4.335659503936768 | BCE Loss: 0.9862034320831299\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 5.365039348602295 | KNN Loss: 4.329629421234131 | BCE Loss: 1.035409927368164\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 5.365725517272949 | KNN Loss: 4.348820209503174 | BCE Loss: 1.0169055461883545\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 5.373897075653076 | KNN Loss: 4.333849906921387 | BCE Loss: 1.0400470495224\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 5.353583812713623 | KNN Loss: 4.3313727378845215 | BCE Loss: 1.0222110748291016\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 5.368758201599121 | KNN Loss: 4.342117786407471 | BCE Loss: 1.0266401767730713\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 5.384517669677734 | KNN Loss: 4.355091094970703 | BCE Loss: 1.0294264554977417\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 5.364582061767578 | KNN Loss: 4.351932048797607 | BCE Loss: 1.0126502513885498\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 5.392795085906982 | KNN Loss: 4.370956897735596 | BCE Loss: 1.0218381881713867\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 5.313529968261719 | KNN Loss: 4.300495147705078 | BCE Loss: 1.0130345821380615\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 5.311582565307617 | KNN Loss: 4.315732955932617 | BCE Loss: 0.9958494901657104\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 5.339206218719482 | KNN Loss: 4.331287860870361 | BCE Loss: 1.007918357849121\n",
      "Epoch   339: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 5.328774929046631 | KNN Loss: 4.332498073577881 | BCE Loss: 0.9962767362594604\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 5.35382080078125 | KNN Loss: 4.340090751647949 | BCE Loss: 1.0137302875518799\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 5.394335746765137 | KNN Loss: 4.361629962921143 | BCE Loss: 1.0327060222625732\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 5.386636734008789 | KNN Loss: 4.352917194366455 | BCE Loss: 1.0337193012237549\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 5.347010612487793 | KNN Loss: 4.328680992126465 | BCE Loss: 1.0183295011520386\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 5.399517059326172 | KNN Loss: 4.349701404571533 | BCE Loss: 1.0498156547546387\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 5.384851932525635 | KNN Loss: 4.349682331085205 | BCE Loss: 1.0351696014404297\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 5.272708415985107 | KNN Loss: 4.307012557983398 | BCE Loss: 0.965695858001709\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 5.405278205871582 | KNN Loss: 4.350911617279053 | BCE Loss: 1.0543663501739502\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 5.405285835266113 | KNN Loss: 4.377047061920166 | BCE Loss: 1.0282387733459473\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 5.400848388671875 | KNN Loss: 4.386360168457031 | BCE Loss: 1.0144879817962646\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 5.342241287231445 | KNN Loss: 4.334269046783447 | BCE Loss: 1.0079723596572876\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 5.3327226638793945 | KNN Loss: 4.318999290466309 | BCE Loss: 1.013723611831665\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 5.363588809967041 | KNN Loss: 4.345389366149902 | BCE Loss: 1.0181994438171387\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 5.321207046508789 | KNN Loss: 4.3046417236328125 | BCE Loss: 1.0165655612945557\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 5.378164291381836 | KNN Loss: 4.336768627166748 | BCE Loss: 1.041395902633667\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 5.357563018798828 | KNN Loss: 4.356443405151367 | BCE Loss: 1.001119613647461\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 5.338900089263916 | KNN Loss: 4.308939456939697 | BCE Loss: 1.0299606323242188\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 5.374115943908691 | KNN Loss: 4.360579490661621 | BCE Loss: 1.0135364532470703\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 5.369158744812012 | KNN Loss: 4.319908618927002 | BCE Loss: 1.0492501258850098\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 5.406158447265625 | KNN Loss: 4.3731160163879395 | BCE Loss: 1.0330421924591064\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 5.390361785888672 | KNN Loss: 4.361536979675293 | BCE Loss: 1.0288249254226685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 5.32889461517334 | KNN Loss: 4.3204498291015625 | BCE Loss: 1.0084447860717773\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 5.385364532470703 | KNN Loss: 4.346867561340332 | BCE Loss: 1.0384972095489502\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 5.412097930908203 | KNN Loss: 4.381918430328369 | BCE Loss: 1.0301792621612549\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 5.305268287658691 | KNN Loss: 4.30812931060791 | BCE Loss: 0.997139036655426\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 5.381422519683838 | KNN Loss: 4.316338539123535 | BCE Loss: 1.0650840997695923\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 5.331233024597168 | KNN Loss: 4.320980072021484 | BCE Loss: 1.0102529525756836\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 5.374724388122559 | KNN Loss: 4.335403919219971 | BCE Loss: 1.039320707321167\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 5.380877494812012 | KNN Loss: 4.374922752380371 | BCE Loss: 1.0059545040130615\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 5.3988847732543945 | KNN Loss: 4.391112804412842 | BCE Loss: 1.0077717304229736\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 5.35331916809082 | KNN Loss: 4.340549945831299 | BCE Loss: 1.0127689838409424\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 5.324024200439453 | KNN Loss: 4.312773704528809 | BCE Loss: 1.011250615119934\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 5.386798858642578 | KNN Loss: 4.366984844207764 | BCE Loss: 1.0198140144348145\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 5.372304916381836 | KNN Loss: 4.358288288116455 | BCE Loss: 1.0140166282653809\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 5.369660377502441 | KNN Loss: 4.345198631286621 | BCE Loss: 1.0244617462158203\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 5.333407878875732 | KNN Loss: 4.32173490524292 | BCE Loss: 1.011672854423523\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 5.338642120361328 | KNN Loss: 4.32281494140625 | BCE Loss: 1.0158272981643677\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 5.375569820404053 | KNN Loss: 4.313380241394043 | BCE Loss: 1.0621896982192993\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 5.342869758605957 | KNN Loss: 4.313465118408203 | BCE Loss: 1.0294045209884644\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 5.379673957824707 | KNN Loss: 4.344699382781982 | BCE Loss: 1.0349743366241455\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 5.35581111907959 | KNN Loss: 4.338898181915283 | BCE Loss: 1.0169129371643066\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 5.353083610534668 | KNN Loss: 4.328665733337402 | BCE Loss: 1.0244178771972656\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 5.383499622344971 | KNN Loss: 4.370461940765381 | BCE Loss: 1.0130376815795898\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 5.336436748504639 | KNN Loss: 4.328875541687012 | BCE Loss: 1.007561206817627\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 5.388426780700684 | KNN Loss: 4.3524651527404785 | BCE Loss: 1.035961389541626\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 5.371111869812012 | KNN Loss: 4.361256122589111 | BCE Loss: 1.0098557472229004\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 5.364412307739258 | KNN Loss: 4.337299823760986 | BCE Loss: 1.027112603187561\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 5.364448070526123 | KNN Loss: 4.349874973297119 | BCE Loss: 1.014573097229004\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 5.336136817932129 | KNN Loss: 4.317465305328369 | BCE Loss: 1.0186713933944702\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 5.400323867797852 | KNN Loss: 4.359233856201172 | BCE Loss: 1.0410900115966797\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 5.375354290008545 | KNN Loss: 4.3522515296936035 | BCE Loss: 1.0231026411056519\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 5.338939189910889 | KNN Loss: 4.344393253326416 | BCE Loss: 0.9945458173751831\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 5.3846259117126465 | KNN Loss: 4.360946178436279 | BCE Loss: 1.0236797332763672\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 5.364330291748047 | KNN Loss: 4.351459503173828 | BCE Loss: 1.0128707885742188\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 5.321028232574463 | KNN Loss: 4.3334527015686035 | BCE Loss: 0.9875754714012146\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 5.376576900482178 | KNN Loss: 4.3199052810668945 | BCE Loss: 1.0566715002059937\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 5.3899078369140625 | KNN Loss: 4.349448204040527 | BCE Loss: 1.040459394454956\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 5.365479469299316 | KNN Loss: 4.339918613433838 | BCE Loss: 1.0255606174468994\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 5.33222770690918 | KNN Loss: 4.311947345733643 | BCE Loss: 1.020280361175537\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 5.363001823425293 | KNN Loss: 4.341287612915039 | BCE Loss: 1.0217139720916748\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 5.397106170654297 | KNN Loss: 4.385190486907959 | BCE Loss: 1.0119158029556274\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 5.3341217041015625 | KNN Loss: 4.3236403465271 | BCE Loss: 1.0104814767837524\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 5.322835922241211 | KNN Loss: 4.313087463378906 | BCE Loss: 1.0097482204437256\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 5.373906135559082 | KNN Loss: 4.331758975982666 | BCE Loss: 1.042146921157837\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 5.326135158538818 | KNN Loss: 4.326264381408691 | BCE Loss: 0.9998708367347717\n",
      "Epoch   350: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 5.373722553253174 | KNN Loss: 4.33363151550293 | BCE Loss: 1.0400910377502441\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 5.367283821105957 | KNN Loss: 4.346842288970947 | BCE Loss: 1.0204412937164307\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 5.351463317871094 | KNN Loss: 4.35093879699707 | BCE Loss: 1.0005245208740234\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 5.38996696472168 | KNN Loss: 4.353271961212158 | BCE Loss: 1.0366950035095215\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 5.365232467651367 | KNN Loss: 4.363595008850098 | BCE Loss: 1.001637578010559\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 5.391003131866455 | KNN Loss: 4.357823371887207 | BCE Loss: 1.033179759979248\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 5.371540069580078 | KNN Loss: 4.351157188415527 | BCE Loss: 1.0203827619552612\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 5.346879005432129 | KNN Loss: 4.333165168762207 | BCE Loss: 1.013714075088501\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 5.395707130432129 | KNN Loss: 4.351282596588135 | BCE Loss: 1.044424295425415\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 5.331884860992432 | KNN Loss: 4.345577239990234 | BCE Loss: 0.9863077402114868\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 5.385929584503174 | KNN Loss: 4.354115962982178 | BCE Loss: 1.031813621520996\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 5.384312629699707 | KNN Loss: 4.378999710083008 | BCE Loss: 1.0053129196166992\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 5.417980194091797 | KNN Loss: 4.371053218841553 | BCE Loss: 1.0469268560409546\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 5.393553256988525 | KNN Loss: 4.351578235626221 | BCE Loss: 1.0419750213623047\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 5.396892070770264 | KNN Loss: 4.326005935668945 | BCE Loss: 1.0708861351013184\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 5.307428359985352 | KNN Loss: 4.325075149536133 | BCE Loss: 0.9823529720306396\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 5.334779739379883 | KNN Loss: 4.344420909881592 | BCE Loss: 0.9903590679168701\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 5.389000415802002 | KNN Loss: 4.373833179473877 | BCE Loss: 1.015167236328125\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 5.340897560119629 | KNN Loss: 4.329066753387451 | BCE Loss: 1.0118305683135986\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 5.375773906707764 | KNN Loss: 4.325683116912842 | BCE Loss: 1.0500907897949219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 5.3176727294921875 | KNN Loss: 4.314280986785889 | BCE Loss: 1.0033915042877197\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 5.365478992462158 | KNN Loss: 4.341991901397705 | BCE Loss: 1.0234870910644531\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 5.3853349685668945 | KNN Loss: 4.367983341217041 | BCE Loss: 1.0173516273498535\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 5.372584342956543 | KNN Loss: 4.357744216918945 | BCE Loss: 1.0148403644561768\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 5.365780353546143 | KNN Loss: 4.33652400970459 | BCE Loss: 1.0292563438415527\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 5.332111358642578 | KNN Loss: 4.326883792877197 | BCE Loss: 1.0052275657653809\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 5.378783226013184 | KNN Loss: 4.367127418518066 | BCE Loss: 1.0116560459136963\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 5.355651378631592 | KNN Loss: 4.324416637420654 | BCE Loss: 1.031234860420227\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 5.321792125701904 | KNN Loss: 4.306210994720459 | BCE Loss: 1.0155811309814453\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 5.318700313568115 | KNN Loss: 4.322340965270996 | BCE Loss: 0.9963593482971191\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 5.383640289306641 | KNN Loss: 4.366765022277832 | BCE Loss: 1.0168755054473877\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 5.334294319152832 | KNN Loss: 4.3258585929870605 | BCE Loss: 1.0084354877471924\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 5.37745475769043 | KNN Loss: 4.353851318359375 | BCE Loss: 1.0236034393310547\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 5.376286029815674 | KNN Loss: 4.362335681915283 | BCE Loss: 1.0139503479003906\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 5.390424728393555 | KNN Loss: 4.3499755859375 | BCE Loss: 1.0404491424560547\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 5.344658374786377 | KNN Loss: 4.3216657638549805 | BCE Loss: 1.022992730140686\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 5.364038467407227 | KNN Loss: 4.3510966300964355 | BCE Loss: 1.012941598892212\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 5.339158535003662 | KNN Loss: 4.308903217315674 | BCE Loss: 1.0302553176879883\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 5.368528842926025 | KNN Loss: 4.331017017364502 | BCE Loss: 1.0375118255615234\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 5.350568771362305 | KNN Loss: 4.330963611602783 | BCE Loss: 1.019605278968811\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 5.341489791870117 | KNN Loss: 4.313766002655029 | BCE Loss: 1.027723789215088\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 5.399555683135986 | KNN Loss: 4.358616828918457 | BCE Loss: 1.0409388542175293\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 5.35135555267334 | KNN Loss: 4.331896781921387 | BCE Loss: 1.0194586515426636\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 5.375687122344971 | KNN Loss: 4.3407793045043945 | BCE Loss: 1.0349078178405762\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 5.348028659820557 | KNN Loss: 4.324059009552002 | BCE Loss: 1.0239697694778442\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 5.404792308807373 | KNN Loss: 4.3585333824157715 | BCE Loss: 1.0462589263916016\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 5.364053726196289 | KNN Loss: 4.3534135818481445 | BCE Loss: 1.0106403827667236\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 5.37770414352417 | KNN Loss: 4.371446132659912 | BCE Loss: 1.0062581300735474\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 5.365614891052246 | KNN Loss: 4.3239312171936035 | BCE Loss: 1.0416839122772217\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 5.323606491088867 | KNN Loss: 4.322729110717773 | BCE Loss: 1.0008773803710938\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 5.291911602020264 | KNN Loss: 4.321771144866943 | BCE Loss: 0.9701404571533203\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 5.372939109802246 | KNN Loss: 4.343814373016357 | BCE Loss: 1.0291244983673096\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 5.422077655792236 | KNN Loss: 4.3739190101623535 | BCE Loss: 1.0481586456298828\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 5.359093189239502 | KNN Loss: 4.327563762664795 | BCE Loss: 1.031529426574707\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 5.358417510986328 | KNN Loss: 4.335749626159668 | BCE Loss: 1.022667646408081\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 5.348115921020508 | KNN Loss: 4.333458423614502 | BCE Loss: 1.0146572589874268\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 5.367152214050293 | KNN Loss: 4.342267990112305 | BCE Loss: 1.0248843431472778\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 5.402856826782227 | KNN Loss: 4.371892929077148 | BCE Loss: 1.0309638977050781\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 5.385131359100342 | KNN Loss: 4.362908840179443 | BCE Loss: 1.0222223997116089\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 5.374050140380859 | KNN Loss: 4.345583915710449 | BCE Loss: 1.0284662246704102\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 5.3034210205078125 | KNN Loss: 4.300917148590088 | BCE Loss: 1.0025041103363037\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 5.355132579803467 | KNN Loss: 4.312335014343262 | BCE Loss: 1.0427974462509155\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 5.360175609588623 | KNN Loss: 4.337098121643066 | BCE Loss: 1.0230774879455566\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 5.3220133781433105 | KNN Loss: 4.327840328216553 | BCE Loss: 0.994172990322113\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 5.362858772277832 | KNN Loss: 4.3306803703308105 | BCE Loss: 1.0321781635284424\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 5.429970741271973 | KNN Loss: 4.371323108673096 | BCE Loss: 1.058647871017456\n",
      "Epoch   361: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 5.329148292541504 | KNN Loss: 4.321033477783203 | BCE Loss: 1.0081148147583008\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 5.379645824432373 | KNN Loss: 4.346528053283691 | BCE Loss: 1.0331178903579712\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 5.344992637634277 | KNN Loss: 4.344818115234375 | BCE Loss: 1.0001745223999023\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 5.362089157104492 | KNN Loss: 4.348650932312012 | BCE Loss: 1.01343834400177\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 5.373955249786377 | KNN Loss: 4.381206512451172 | BCE Loss: 0.9927488565444946\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 5.40456485748291 | KNN Loss: 4.3613104820251465 | BCE Loss: 1.0432543754577637\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 5.37351131439209 | KNN Loss: 4.340287685394287 | BCE Loss: 1.0332236289978027\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 5.403139114379883 | KNN Loss: 4.3742852210998535 | BCE Loss: 1.0288536548614502\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 5.366927146911621 | KNN Loss: 4.339351177215576 | BCE Loss: 1.027575969696045\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 5.351093292236328 | KNN Loss: 4.322157859802246 | BCE Loss: 1.028935432434082\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 5.361824035644531 | KNN Loss: 4.3228230476379395 | BCE Loss: 1.039001226425171\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 5.396606922149658 | KNN Loss: 4.3637871742248535 | BCE Loss: 1.0328196287155151\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 5.333520889282227 | KNN Loss: 4.309782981872559 | BCE Loss: 1.023738145828247\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 5.338930606842041 | KNN Loss: 4.329724311828613 | BCE Loss: 1.0092061758041382\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 5.336150646209717 | KNN Loss: 4.345787048339844 | BCE Loss: 0.9903636574745178\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 5.368168830871582 | KNN Loss: 4.349939823150635 | BCE Loss: 1.0182291269302368\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 5.402409553527832 | KNN Loss: 4.353561878204346 | BCE Loss: 1.0488474369049072\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 5.432170867919922 | KNN Loss: 4.38930082321167 | BCE Loss: 1.042870283126831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 5.350191593170166 | KNN Loss: 4.338006019592285 | BCE Loss: 1.0121856927871704\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 5.357417106628418 | KNN Loss: 4.339258193969727 | BCE Loss: 1.0181591510772705\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 5.360299110412598 | KNN Loss: 4.342634677886963 | BCE Loss: 1.0176646709442139\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 5.33585262298584 | KNN Loss: 4.3140692710876465 | BCE Loss: 1.0217833518981934\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 5.338958740234375 | KNN Loss: 4.320520877838135 | BCE Loss: 1.0184376239776611\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 5.383331775665283 | KNN Loss: 4.370683193206787 | BCE Loss: 1.0126484632492065\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 5.3489227294921875 | KNN Loss: 4.354379177093506 | BCE Loss: 0.9945437908172607\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 5.3695969581604 | KNN Loss: 4.355698108673096 | BCE Loss: 1.0138988494873047\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 5.328360080718994 | KNN Loss: 4.332241058349609 | BCE Loss: 0.9961191415786743\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 5.3770952224731445 | KNN Loss: 4.337319850921631 | BCE Loss: 1.0397752523422241\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 5.347216606140137 | KNN Loss: 4.3451104164123535 | BCE Loss: 1.0021061897277832\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 5.356359004974365 | KNN Loss: 4.319463729858398 | BCE Loss: 1.0368953943252563\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 5.357574462890625 | KNN Loss: 4.351566791534424 | BCE Loss: 1.006007432937622\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 5.3725690841674805 | KNN Loss: 4.337108612060547 | BCE Loss: 1.0354604721069336\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 5.3748908042907715 | KNN Loss: 4.344907760620117 | BCE Loss: 1.0299830436706543\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 5.36684513092041 | KNN Loss: 4.354776859283447 | BCE Loss: 1.012068271636963\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 5.347683429718018 | KNN Loss: 4.319271087646484 | BCE Loss: 1.0284123420715332\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 5.3621673583984375 | KNN Loss: 4.344286918640137 | BCE Loss: 1.0178802013397217\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 5.375987529754639 | KNN Loss: 4.354928016662598 | BCE Loss: 1.021059513092041\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 5.362849235534668 | KNN Loss: 4.351228713989258 | BCE Loss: 1.0116206407546997\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 5.420683860778809 | KNN Loss: 4.3622260093688965 | BCE Loss: 1.0584580898284912\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 5.362586498260498 | KNN Loss: 4.334356307983398 | BCE Loss: 1.0282301902770996\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 5.366849899291992 | KNN Loss: 4.3398332595825195 | BCE Loss: 1.0270168781280518\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 5.394260883331299 | KNN Loss: 4.349730014801025 | BCE Loss: 1.0445308685302734\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 5.369755744934082 | KNN Loss: 4.348690509796143 | BCE Loss: 1.0210654735565186\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 5.384278297424316 | KNN Loss: 4.329685688018799 | BCE Loss: 1.0545926094055176\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 5.3925909996032715 | KNN Loss: 4.386211395263672 | BCE Loss: 1.0063796043395996\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 5.393317222595215 | KNN Loss: 4.361353874206543 | BCE Loss: 1.031963586807251\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 5.3438496589660645 | KNN Loss: 4.333310604095459 | BCE Loss: 1.0105390548706055\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 5.369443893432617 | KNN Loss: 4.32429313659668 | BCE Loss: 1.0451509952545166\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 5.336792945861816 | KNN Loss: 4.332671642303467 | BCE Loss: 1.0041215419769287\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 5.378765106201172 | KNN Loss: 4.345230579376221 | BCE Loss: 1.033534288406372\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 5.371105194091797 | KNN Loss: 4.34506893157959 | BCE Loss: 1.0260365009307861\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 5.354306221008301 | KNN Loss: 4.331978797912598 | BCE Loss: 1.0223274230957031\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 5.335079193115234 | KNN Loss: 4.322024345397949 | BCE Loss: 1.013054609298706\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 5.395861625671387 | KNN Loss: 4.365172386169434 | BCE Loss: 1.030689001083374\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 5.348820686340332 | KNN Loss: 4.350231170654297 | BCE Loss: 0.998589277267456\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 5.364738464355469 | KNN Loss: 4.3445820808410645 | BCE Loss: 1.0201565027236938\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 5.402082920074463 | KNN Loss: 4.373762130737305 | BCE Loss: 1.0283207893371582\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 5.306676864624023 | KNN Loss: 4.30574369430542 | BCE Loss: 1.0009334087371826\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 5.399548530578613 | KNN Loss: 4.365534782409668 | BCE Loss: 1.0340137481689453\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 5.397564888000488 | KNN Loss: 4.358187198638916 | BCE Loss: 1.0393775701522827\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 5.3845624923706055 | KNN Loss: 4.3452019691467285 | BCE Loss: 1.039360523223877\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 5.365747451782227 | KNN Loss: 4.35385274887085 | BCE Loss: 1.011894941329956\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 5.38932991027832 | KNN Loss: 4.351747035980225 | BCE Loss: 1.0375829935073853\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 5.38933801651001 | KNN Loss: 4.353195667266846 | BCE Loss: 1.0361424684524536\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 5.376298904418945 | KNN Loss: 4.354926586151123 | BCE Loss: 1.0213725566864014\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 5.299867630004883 | KNN Loss: 4.300021171569824 | BCE Loss: 0.9998466968536377\n",
      "Epoch   372: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 5.341436386108398 | KNN Loss: 4.332904815673828 | BCE Loss: 1.0085315704345703\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 5.361514568328857 | KNN Loss: 4.330896377563477 | BCE Loss: 1.0306180715560913\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 5.315133094787598 | KNN Loss: 4.3006086349487305 | BCE Loss: 1.0145244598388672\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 5.396634101867676 | KNN Loss: 4.37692928314209 | BCE Loss: 1.019705057144165\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 5.367984771728516 | KNN Loss: 4.305866718292236 | BCE Loss: 1.0621182918548584\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 5.349516868591309 | KNN Loss: 4.331189155578613 | BCE Loss: 1.0183278322219849\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 5.3705339431762695 | KNN Loss: 4.3279643058776855 | BCE Loss: 1.0425693988800049\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 5.329153537750244 | KNN Loss: 4.319120407104492 | BCE Loss: 1.010033130645752\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 5.4002685546875 | KNN Loss: 4.357539653778076 | BCE Loss: 1.042729139328003\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 5.373592376708984 | KNN Loss: 4.333926677703857 | BCE Loss: 1.0396654605865479\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 5.371328353881836 | KNN Loss: 4.337691307067871 | BCE Loss: 1.0336370468139648\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 5.352573871612549 | KNN Loss: 4.34598970413208 | BCE Loss: 1.0065842866897583\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 5.382720947265625 | KNN Loss: 4.3534698486328125 | BCE Loss: 1.0292508602142334\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 5.382723808288574 | KNN Loss: 4.335019588470459 | BCE Loss: 1.0477044582366943\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 5.327478408813477 | KNN Loss: 4.349455833435059 | BCE Loss: 0.9780225157737732\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 5.378663063049316 | KNN Loss: 4.340949058532715 | BCE Loss: 1.0377140045166016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 5.342226028442383 | KNN Loss: 4.327207088470459 | BCE Loss: 1.015019178390503\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 5.369772434234619 | KNN Loss: 4.334789752960205 | BCE Loss: 1.0349825620651245\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 5.391184329986572 | KNN Loss: 4.3752923011779785 | BCE Loss: 1.0158920288085938\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 5.383306503295898 | KNN Loss: 4.355643272399902 | BCE Loss: 1.0276634693145752\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 5.365352630615234 | KNN Loss: 4.337962627410889 | BCE Loss: 1.0273897647857666\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 5.375711441040039 | KNN Loss: 4.356298923492432 | BCE Loss: 1.0194122791290283\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 5.374667167663574 | KNN Loss: 4.351133823394775 | BCE Loss: 1.0235333442687988\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 5.355513095855713 | KNN Loss: 4.34341287612915 | BCE Loss: 1.012100338935852\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 5.3754987716674805 | KNN Loss: 4.362283706665039 | BCE Loss: 1.0132148265838623\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 5.40306282043457 | KNN Loss: 4.37468147277832 | BCE Loss: 1.0283814668655396\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 5.365347862243652 | KNN Loss: 4.343906879425049 | BCE Loss: 1.0214412212371826\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 5.354687213897705 | KNN Loss: 4.349167823791504 | BCE Loss: 1.0055195093154907\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 5.346249580383301 | KNN Loss: 4.310547351837158 | BCE Loss: 1.0357024669647217\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 5.399949073791504 | KNN Loss: 4.378144264221191 | BCE Loss: 1.0218045711517334\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 5.348308086395264 | KNN Loss: 4.355722904205322 | BCE Loss: 0.992585301399231\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 5.3794636726379395 | KNN Loss: 4.372733116149902 | BCE Loss: 1.006730556488037\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 5.356491565704346 | KNN Loss: 4.32689905166626 | BCE Loss: 1.0295926332473755\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 5.332758903503418 | KNN Loss: 4.330036163330078 | BCE Loss: 1.002722978591919\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 5.353573799133301 | KNN Loss: 4.310042858123779 | BCE Loss: 1.043530821800232\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 5.3811564445495605 | KNN Loss: 4.3642258644104 | BCE Loss: 1.0169304609298706\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 5.313655376434326 | KNN Loss: 4.303064346313477 | BCE Loss: 1.0105911493301392\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 5.34591007232666 | KNN Loss: 4.331210136413574 | BCE Loss: 1.0146996974945068\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 5.329024314880371 | KNN Loss: 4.3278656005859375 | BCE Loss: 1.0011589527130127\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 5.37684965133667 | KNN Loss: 4.343261241912842 | BCE Loss: 1.0335884094238281\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 5.368651390075684 | KNN Loss: 4.34964656829834 | BCE Loss: 1.0190050601959229\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 5.3493428230285645 | KNN Loss: 4.3301777839660645 | BCE Loss: 1.0191650390625\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 5.343522548675537 | KNN Loss: 4.3460798263549805 | BCE Loss: 0.9974425435066223\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 5.350075721740723 | KNN Loss: 4.340853214263916 | BCE Loss: 1.0092222690582275\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 5.390660762786865 | KNN Loss: 4.339052200317383 | BCE Loss: 1.0516085624694824\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 5.416165828704834 | KNN Loss: 4.380596160888672 | BCE Loss: 1.035569667816162\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 5.370638847351074 | KNN Loss: 4.362717628479004 | BCE Loss: 1.0079212188720703\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 5.369342803955078 | KNN Loss: 4.340303897857666 | BCE Loss: 1.0290391445159912\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 5.377650737762451 | KNN Loss: 4.38355016708374 | BCE Loss: 0.9941005706787109\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 5.397347927093506 | KNN Loss: 4.361144542694092 | BCE Loss: 1.036203384399414\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 5.347033977508545 | KNN Loss: 4.349648952484131 | BCE Loss: 0.9973851442337036\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 5.341137886047363 | KNN Loss: 4.330747604370117 | BCE Loss: 1.0103905200958252\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 5.420230865478516 | KNN Loss: 4.377510070800781 | BCE Loss: 1.0427207946777344\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 5.374050617218018 | KNN Loss: 4.331600666046143 | BCE Loss: 1.042449951171875\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 5.371940612792969 | KNN Loss: 4.354026794433594 | BCE Loss: 1.017914056777954\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 5.398168563842773 | KNN Loss: 4.3518781661987305 | BCE Loss: 1.046290397644043\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 5.374461650848389 | KNN Loss: 4.369349479675293 | BCE Loss: 1.0051121711730957\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 5.337329864501953 | KNN Loss: 4.314273357391357 | BCE Loss: 1.0230562686920166\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 5.32120943069458 | KNN Loss: 4.305985927581787 | BCE Loss: 1.015223503112793\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 5.371535301208496 | KNN Loss: 4.375670433044434 | BCE Loss: 0.9958646893501282\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 5.398529052734375 | KNN Loss: 4.354310512542725 | BCE Loss: 1.0442185401916504\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 5.349349021911621 | KNN Loss: 4.335531711578369 | BCE Loss: 1.0138170719146729\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 5.355785369873047 | KNN Loss: 4.3279194831848145 | BCE Loss: 1.027866005897522\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 5.372300624847412 | KNN Loss: 4.3835554122924805 | BCE Loss: 0.9887452125549316\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 5.394756317138672 | KNN Loss: 4.354300498962402 | BCE Loss: 1.0404555797576904\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 5.382781982421875 | KNN Loss: 4.342607021331787 | BCE Loss: 1.040175199508667\n",
      "Epoch   383: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 5.319677829742432 | KNN Loss: 4.334162712097168 | BCE Loss: 0.9855152368545532\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 5.327489852905273 | KNN Loss: 4.329931735992432 | BCE Loss: 0.9975583553314209\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 5.356142997741699 | KNN Loss: 4.309343338012695 | BCE Loss: 1.0467997789382935\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 5.397187232971191 | KNN Loss: 4.361164569854736 | BCE Loss: 1.0360229015350342\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 5.353944778442383 | KNN Loss: 4.323736190795898 | BCE Loss: 1.030208706855774\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 5.325160503387451 | KNN Loss: 4.315674304962158 | BCE Loss: 1.0094863176345825\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 5.360276222229004 | KNN Loss: 4.321103572845459 | BCE Loss: 1.0391727685928345\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 5.424248218536377 | KNN Loss: 4.390853404998779 | BCE Loss: 1.0333948135375977\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 5.3092041015625 | KNN Loss: 4.315883159637451 | BCE Loss: 0.9933211803436279\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 5.401546478271484 | KNN Loss: 4.36826753616333 | BCE Loss: 1.0332791805267334\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 5.381289005279541 | KNN Loss: 4.345792293548584 | BCE Loss: 1.035496711730957\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 5.359044075012207 | KNN Loss: 4.320239543914795 | BCE Loss: 1.038804292678833\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 5.369297981262207 | KNN Loss: 4.358351230621338 | BCE Loss: 1.0109468698501587\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 5.344986915588379 | KNN Loss: 4.340091228485107 | BCE Loss: 1.0048956871032715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 5.318363189697266 | KNN Loss: 4.314396381378174 | BCE Loss: 1.0039669275283813\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 5.428670883178711 | KNN Loss: 4.358550548553467 | BCE Loss: 1.0701205730438232\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 5.3431172370910645 | KNN Loss: 4.32890510559082 | BCE Loss: 1.0142121315002441\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 5.321158409118652 | KNN Loss: 4.315371990203857 | BCE Loss: 1.0057862997055054\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 5.419076919555664 | KNN Loss: 4.350430011749268 | BCE Loss: 1.0686466693878174\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 5.373912334442139 | KNN Loss: 4.353179454803467 | BCE Loss: 1.0207327604293823\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 5.3345627784729 | KNN Loss: 4.318312168121338 | BCE Loss: 1.016250729560852\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 5.378401756286621 | KNN Loss: 4.370051860809326 | BCE Loss: 1.008349895477295\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 5.361909866333008 | KNN Loss: 4.329385280609131 | BCE Loss: 1.032524585723877\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 5.366912841796875 | KNN Loss: 4.346303939819336 | BCE Loss: 1.02060866355896\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 5.31948184967041 | KNN Loss: 4.319882392883301 | BCE Loss: 0.9995996952056885\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 5.373497009277344 | KNN Loss: 4.336636543273926 | BCE Loss: 1.036860466003418\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 5.356090545654297 | KNN Loss: 4.329814910888672 | BCE Loss: 1.0262757539749146\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 5.365575790405273 | KNN Loss: 4.331823825836182 | BCE Loss: 1.033752202987671\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 5.404609203338623 | KNN Loss: 4.368198394775391 | BCE Loss: 1.0364108085632324\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 5.3874711990356445 | KNN Loss: 4.34564733505249 | BCE Loss: 1.0418236255645752\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 5.329457759857178 | KNN Loss: 4.326748371124268 | BCE Loss: 1.0027095079421997\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 5.3379645347595215 | KNN Loss: 4.328127384185791 | BCE Loss: 1.00983726978302\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 5.325686454772949 | KNN Loss: 4.323951244354248 | BCE Loss: 1.001734972000122\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 5.417238235473633 | KNN Loss: 4.3805365562438965 | BCE Loss: 1.0367015600204468\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 5.393405914306641 | KNN Loss: 4.348742485046387 | BCE Loss: 1.0446635484695435\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 5.326458930969238 | KNN Loss: 4.334773063659668 | BCE Loss: 0.9916861057281494\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 5.396864891052246 | KNN Loss: 4.367598533630371 | BCE Loss: 1.0292664766311646\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 5.327649116516113 | KNN Loss: 4.313006401062012 | BCE Loss: 1.0146424770355225\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 5.415077209472656 | KNN Loss: 4.3684210777282715 | BCE Loss: 1.0466563701629639\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 5.386685371398926 | KNN Loss: 4.367495059967041 | BCE Loss: 1.0191903114318848\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 5.349059581756592 | KNN Loss: 4.325734615325928 | BCE Loss: 1.023324966430664\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 5.373198986053467 | KNN Loss: 4.3427300453186035 | BCE Loss: 1.0304689407348633\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 5.366155624389648 | KNN Loss: 4.322324752807617 | BCE Loss: 1.0438311100006104\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 5.447222709655762 | KNN Loss: 4.4045820236206055 | BCE Loss: 1.0426405668258667\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 5.333905220031738 | KNN Loss: 4.327764987945557 | BCE Loss: 1.0061404705047607\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 5.324152946472168 | KNN Loss: 4.325001239776611 | BCE Loss: 0.9991514682769775\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 5.338641166687012 | KNN Loss: 4.309738636016846 | BCE Loss: 1.0289027690887451\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 5.363166332244873 | KNN Loss: 4.345642566680908 | BCE Loss: 1.0175238847732544\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 5.392399787902832 | KNN Loss: 4.354706287384033 | BCE Loss: 1.0376932621002197\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 5.330913543701172 | KNN Loss: 4.328132152557373 | BCE Loss: 1.002781629562378\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 5.326537132263184 | KNN Loss: 4.319428443908691 | BCE Loss: 1.007108449935913\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 5.397467136383057 | KNN Loss: 4.354257106781006 | BCE Loss: 1.0432101488113403\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 5.384833812713623 | KNN Loss: 4.336277484893799 | BCE Loss: 1.0485562086105347\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 5.350715637207031 | KNN Loss: 4.320883750915527 | BCE Loss: 1.0298317670822144\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 5.352007865905762 | KNN Loss: 4.384437084197998 | BCE Loss: 0.9675706624984741\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 5.360589504241943 | KNN Loss: 4.330204963684082 | BCE Loss: 1.0303845405578613\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 5.389345169067383 | KNN Loss: 4.356260776519775 | BCE Loss: 1.0330846309661865\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 5.326863765716553 | KNN Loss: 4.318510055541992 | BCE Loss: 1.00835382938385\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 5.33588171005249 | KNN Loss: 4.326040267944336 | BCE Loss: 1.0098413228988647\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 5.362262725830078 | KNN Loss: 4.35102653503418 | BCE Loss: 1.0112361907958984\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 5.344661235809326 | KNN Loss: 4.331773281097412 | BCE Loss: 1.0128878355026245\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 5.33458948135376 | KNN Loss: 4.352165699005127 | BCE Loss: 0.9824236631393433\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 5.387372016906738 | KNN Loss: 4.347960472106934 | BCE Loss: 1.0394114255905151\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 5.380306243896484 | KNN Loss: 4.350116729736328 | BCE Loss: 1.0301897525787354\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 5.360972881317139 | KNN Loss: 4.3210930824279785 | BCE Loss: 1.0398799180984497\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 5.3426594734191895 | KNN Loss: 4.347172737121582 | BCE Loss: 0.9954865574836731\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 5.330367565155029 | KNN Loss: 4.324538707733154 | BCE Loss: 1.005828857421875\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 5.386811256408691 | KNN Loss: 4.338024139404297 | BCE Loss: 1.0487873554229736\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 5.387536525726318 | KNN Loss: 4.346428871154785 | BCE Loss: 1.0411075353622437\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 5.426573753356934 | KNN Loss: 4.372596740722656 | BCE Loss: 1.0539768934249878\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 5.357522010803223 | KNN Loss: 4.364261627197266 | BCE Loss: 0.9932604432106018\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 5.384524345397949 | KNN Loss: 4.343596458435059 | BCE Loss: 1.0409281253814697\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 5.355602264404297 | KNN Loss: 4.34236478805542 | BCE Loss: 1.0132375955581665\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 5.344095230102539 | KNN Loss: 4.331354141235352 | BCE Loss: 1.0127413272857666\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 5.346637725830078 | KNN Loss: 4.335613250732422 | BCE Loss: 1.0110247135162354\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 5.372901916503906 | KNN Loss: 4.351711750030518 | BCE Loss: 1.0211899280548096\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 5.3221330642700195 | KNN Loss: 4.319736957550049 | BCE Loss: 1.0023959875106812\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 5.376581192016602 | KNN Loss: 4.342769145965576 | BCE Loss: 1.0338122844696045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 5.384727478027344 | KNN Loss: 4.367968559265137 | BCE Loss: 1.016758680343628\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 5.4140143394470215 | KNN Loss: 4.384173393249512 | BCE Loss: 1.0298408269882202\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 5.387234687805176 | KNN Loss: 4.365289688110352 | BCE Loss: 1.0219447612762451\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 5.367539405822754 | KNN Loss: 4.3419060707092285 | BCE Loss: 1.0256335735321045\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 5.342684745788574 | KNN Loss: 4.307793140411377 | BCE Loss: 1.0348918437957764\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 5.325706481933594 | KNN Loss: 4.3463897705078125 | BCE Loss: 0.9793168306350708\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 5.373893737792969 | KNN Loss: 4.355412006378174 | BCE Loss: 1.0184818506240845\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 5.363255023956299 | KNN Loss: 4.342340469360352 | BCE Loss: 1.0209144353866577\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 5.365383625030518 | KNN Loss: 4.339574337005615 | BCE Loss: 1.025809407234192\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 5.313134670257568 | KNN Loss: 4.319724082946777 | BCE Loss: 0.9934104681015015\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 5.38943338394165 | KNN Loss: 4.368570327758789 | BCE Loss: 1.0208630561828613\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 5.3400373458862305 | KNN Loss: 4.336119651794434 | BCE Loss: 1.0039176940917969\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 5.380326271057129 | KNN Loss: 4.357112407684326 | BCE Loss: 1.0232141017913818\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 5.363048553466797 | KNN Loss: 4.34207820892334 | BCE Loss: 1.0209704637527466\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 5.339056968688965 | KNN Loss: 4.344196796417236 | BCE Loss: 0.9948599338531494\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 5.396821022033691 | KNN Loss: 4.356899261474609 | BCE Loss: 1.039921760559082\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 5.361562252044678 | KNN Loss: 4.36287784576416 | BCE Loss: 0.9986844658851624\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 5.350051403045654 | KNN Loss: 4.319296836853027 | BCE Loss: 1.030754566192627\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 5.382699012756348 | KNN Loss: 4.3349738121032715 | BCE Loss: 1.0477254390716553\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 5.369874954223633 | KNN Loss: 4.339879989624023 | BCE Loss: 1.0299948453903198\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 5.3191752433776855 | KNN Loss: 4.329927444458008 | BCE Loss: 0.9892477989196777\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 5.3250346183776855 | KNN Loss: 4.319911956787109 | BCE Loss: 1.0051225423812866\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 5.395168304443359 | KNN Loss: 4.351032733917236 | BCE Loss: 1.0441358089447021\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 5.361332893371582 | KNN Loss: 4.343139171600342 | BCE Loss: 1.0181937217712402\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 5.3837456703186035 | KNN Loss: 4.364204406738281 | BCE Loss: 1.0195412635803223\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 5.359347820281982 | KNN Loss: 4.354393005371094 | BCE Loss: 1.0049548149108887\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 5.364969730377197 | KNN Loss: 4.343453407287598 | BCE Loss: 1.0215164422988892\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 5.387540340423584 | KNN Loss: 4.35188102722168 | BCE Loss: 1.0356593132019043\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 5.394242286682129 | KNN Loss: 4.357752323150635 | BCE Loss: 1.0364902019500732\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 5.335618019104004 | KNN Loss: 4.307314395904541 | BCE Loss: 1.028303861618042\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 5.332132339477539 | KNN Loss: 4.327080249786377 | BCE Loss: 1.005051851272583\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 5.349154472351074 | KNN Loss: 4.324697017669678 | BCE Loss: 1.0244572162628174\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 5.354866981506348 | KNN Loss: 4.320642471313477 | BCE Loss: 1.034224271774292\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 5.378698825836182 | KNN Loss: 4.364541053771973 | BCE Loss: 1.014157772064209\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 5.361420631408691 | KNN Loss: 4.353866100311279 | BCE Loss: 1.0075546503067017\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 5.356670379638672 | KNN Loss: 4.3200788497924805 | BCE Loss: 1.0365915298461914\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 5.366548538208008 | KNN Loss: 4.342820644378662 | BCE Loss: 1.0237281322479248\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 5.3852338790893555 | KNN Loss: 4.359960079193115 | BCE Loss: 1.0252735614776611\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 5.364578723907471 | KNN Loss: 4.31917142868042 | BCE Loss: 1.0454072952270508\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 5.401347637176514 | KNN Loss: 4.345214366912842 | BCE Loss: 1.0561332702636719\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 5.34432315826416 | KNN Loss: 4.31620979309082 | BCE Loss: 1.0281134843826294\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 5.338557243347168 | KNN Loss: 4.351149082183838 | BCE Loss: 0.9874082803726196\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 5.371811866760254 | KNN Loss: 4.346986770629883 | BCE Loss: 1.0248249769210815\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 5.356045722961426 | KNN Loss: 4.316976070404053 | BCE Loss: 1.039069414138794\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 5.388043403625488 | KNN Loss: 4.37617301940918 | BCE Loss: 1.0118706226348877\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 5.386495590209961 | KNN Loss: 4.372391700744629 | BCE Loss: 1.014103651046753\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 5.357833385467529 | KNN Loss: 4.332866668701172 | BCE Loss: 1.0249667167663574\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 5.392633438110352 | KNN Loss: 4.356705188751221 | BCE Loss: 1.0359280109405518\n",
      "Epoch   404: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 5.378541946411133 | KNN Loss: 4.345083713531494 | BCE Loss: 1.0334582328796387\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 5.336892127990723 | KNN Loss: 4.322452068328857 | BCE Loss: 1.0144400596618652\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 5.380544662475586 | KNN Loss: 4.369441986083984 | BCE Loss: 1.0111027956008911\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 5.345849514007568 | KNN Loss: 4.3477888107299805 | BCE Loss: 0.9980607032775879\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 5.3763885498046875 | KNN Loss: 4.337158679962158 | BCE Loss: 1.0392298698425293\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 5.332832336425781 | KNN Loss: 4.308370113372803 | BCE Loss: 1.0244622230529785\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 5.431724548339844 | KNN Loss: 4.405476093292236 | BCE Loss: 1.0262483358383179\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 5.327974796295166 | KNN Loss: 4.314676284790039 | BCE Loss: 1.0132983922958374\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 5.3589558601379395 | KNN Loss: 4.340109825134277 | BCE Loss: 1.018846035003662\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 5.329489707946777 | KNN Loss: 4.331509113311768 | BCE Loss: 0.9979808330535889\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 5.399909019470215 | KNN Loss: 4.3742756843566895 | BCE Loss: 1.0256335735321045\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 5.351306915283203 | KNN Loss: 4.351229667663574 | BCE Loss: 1.000077247619629\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 5.341631889343262 | KNN Loss: 4.333202838897705 | BCE Loss: 1.0084291696548462\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 5.3393635749816895 | KNN Loss: 4.322582244873047 | BCE Loss: 1.016781210899353\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 5.353707313537598 | KNN Loss: 4.34271240234375 | BCE Loss: 1.0109951496124268\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 5.360359191894531 | KNN Loss: 4.326099395751953 | BCE Loss: 1.034259557723999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 5.393985748291016 | KNN Loss: 4.3762078285217285 | BCE Loss: 1.0177780389785767\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 5.421331882476807 | KNN Loss: 4.413797855377197 | BCE Loss: 1.007534146308899\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 5.325910568237305 | KNN Loss: 4.335816383361816 | BCE Loss: 0.9900941848754883\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 5.333857536315918 | KNN Loss: 4.328846454620361 | BCE Loss: 1.0050112009048462\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 5.360915184020996 | KNN Loss: 4.356260776519775 | BCE Loss: 1.0046545267105103\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 5.367401599884033 | KNN Loss: 4.3526997566223145 | BCE Loss: 1.0147018432617188\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 5.391834259033203 | KNN Loss: 4.334609031677246 | BCE Loss: 1.0572254657745361\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 5.389742374420166 | KNN Loss: 4.356434345245361 | BCE Loss: 1.0333081483840942\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 5.390117168426514 | KNN Loss: 4.346796035766602 | BCE Loss: 1.043321132659912\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 5.367793560028076 | KNN Loss: 4.3567891120910645 | BCE Loss: 1.0110043287277222\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 5.349944591522217 | KNN Loss: 4.3555006980896 | BCE Loss: 0.9944438934326172\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 5.419125556945801 | KNN Loss: 4.3567609786987305 | BCE Loss: 1.0623646974563599\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 5.34402322769165 | KNN Loss: 4.329288482666016 | BCE Loss: 1.0147348642349243\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 5.394234657287598 | KNN Loss: 4.369896411895752 | BCE Loss: 1.0243380069732666\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 5.378294944763184 | KNN Loss: 4.371840000152588 | BCE Loss: 1.0064550638198853\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 5.379539489746094 | KNN Loss: 4.3766303062438965 | BCE Loss: 1.0029089450836182\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 5.332221984863281 | KNN Loss: 4.319279193878174 | BCE Loss: 1.0129426717758179\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 5.401904106140137 | KNN Loss: 4.383125305175781 | BCE Loss: 1.0187788009643555\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 5.35208797454834 | KNN Loss: 4.3625640869140625 | BCE Loss: 0.9895241260528564\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 5.388612747192383 | KNN Loss: 4.350547790527344 | BCE Loss: 1.0380651950836182\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 5.399364471435547 | KNN Loss: 4.363011837005615 | BCE Loss: 1.0363526344299316\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 5.331437110900879 | KNN Loss: 4.323379039764404 | BCE Loss: 1.0080580711364746\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 5.311654090881348 | KNN Loss: 4.30883264541626 | BCE Loss: 1.0028213262557983\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 5.331326007843018 | KNN Loss: 4.322273254394531 | BCE Loss: 1.0090527534484863\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 5.351825714111328 | KNN Loss: 4.363309860229492 | BCE Loss: 0.9885159730911255\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 5.392843246459961 | KNN Loss: 4.373507022857666 | BCE Loss: 1.019336462020874\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 5.33908224105835 | KNN Loss: 4.339780330657959 | BCE Loss: 0.9993018507957458\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 5.452592372894287 | KNN Loss: 4.402334690093994 | BCE Loss: 1.0502575635910034\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 5.383445739746094 | KNN Loss: 4.371629238128662 | BCE Loss: 1.011816382408142\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 5.369284629821777 | KNN Loss: 4.3662824630737305 | BCE Loss: 1.0030021667480469\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 5.388180732727051 | KNN Loss: 4.365933418273926 | BCE Loss: 1.022247552871704\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 5.375552177429199 | KNN Loss: 4.362313747406006 | BCE Loss: 1.0132381916046143\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 5.378915786743164 | KNN Loss: 4.366137981414795 | BCE Loss: 1.01277756690979\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 5.3901824951171875 | KNN Loss: 4.36991548538208 | BCE Loss: 1.0202672481536865\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 5.359525203704834 | KNN Loss: 4.339758396148682 | BCE Loss: 1.0197668075561523\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 5.396531105041504 | KNN Loss: 4.353756427764893 | BCE Loss: 1.0427746772766113\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 5.323945045471191 | KNN Loss: 4.323887348175049 | BCE Loss: 1.0000574588775635\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 5.366386413574219 | KNN Loss: 4.330596923828125 | BCE Loss: 1.0357896089553833\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 5.336780548095703 | KNN Loss: 4.310617446899414 | BCE Loss: 1.02616286277771\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 5.341939449310303 | KNN Loss: 4.338912010192871 | BCE Loss: 1.0030274391174316\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 5.405600070953369 | KNN Loss: 4.353243350982666 | BCE Loss: 1.0523566007614136\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 5.375365257263184 | KNN Loss: 4.348326206207275 | BCE Loss: 1.0270390510559082\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 5.327112197875977 | KNN Loss: 4.335687160491943 | BCE Loss: 0.9914252161979675\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 5.379319190979004 | KNN Loss: 4.355202674865723 | BCE Loss: 1.0241165161132812\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 5.3892927169799805 | KNN Loss: 4.345874309539795 | BCE Loss: 1.043418526649475\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 5.385495662689209 | KNN Loss: 4.359740734100342 | BCE Loss: 1.0257550477981567\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 5.373288154602051 | KNN Loss: 4.332147598266602 | BCE Loss: 1.0411405563354492\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 5.418278217315674 | KNN Loss: 4.39115047454834 | BCE Loss: 1.027127742767334\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 5.338932991027832 | KNN Loss: 4.34023904800415 | BCE Loss: 0.9986937046051025\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 5.399389266967773 | KNN Loss: 4.343842506408691 | BCE Loss: 1.0555468797683716\n",
      "Epoch   415: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 5.404814720153809 | KNN Loss: 4.373666286468506 | BCE Loss: 1.0311486721038818\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 5.424469470977783 | KNN Loss: 4.376458644866943 | BCE Loss: 1.0480107069015503\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 5.438644886016846 | KNN Loss: 4.400367259979248 | BCE Loss: 1.038277506828308\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 5.368805885314941 | KNN Loss: 4.348405361175537 | BCE Loss: 1.0204007625579834\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 5.372779369354248 | KNN Loss: 4.374439239501953 | BCE Loss: 0.9983401298522949\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 5.363726615905762 | KNN Loss: 4.321127891540527 | BCE Loss: 1.0425984859466553\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 5.356045246124268 | KNN Loss: 4.356626987457275 | BCE Loss: 0.9994183778762817\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 5.3587846755981445 | KNN Loss: 4.325938701629639 | BCE Loss: 1.032846212387085\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 5.361162185668945 | KNN Loss: 4.348204135894775 | BCE Loss: 1.01295804977417\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 5.365649700164795 | KNN Loss: 4.3524298667907715 | BCE Loss: 1.013219952583313\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 5.420494079589844 | KNN Loss: 4.395945072174072 | BCE Loss: 1.0245490074157715\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 5.376687049865723 | KNN Loss: 4.347888469696045 | BCE Loss: 1.0287983417510986\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 5.405381679534912 | KNN Loss: 4.374011039733887 | BCE Loss: 1.0313706398010254\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 5.357686996459961 | KNN Loss: 4.323696136474609 | BCE Loss: 1.0339906215667725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 5.391670227050781 | KNN Loss: 4.370935916900635 | BCE Loss: 1.0207343101501465\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 5.349358081817627 | KNN Loss: 4.352179527282715 | BCE Loss: 0.9971783757209778\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 5.352061748504639 | KNN Loss: 4.324198246002197 | BCE Loss: 1.027863621711731\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 5.3263959884643555 | KNN Loss: 4.31308650970459 | BCE Loss: 1.0133097171783447\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 5.412875652313232 | KNN Loss: 4.392711162567139 | BCE Loss: 1.0201644897460938\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 5.37197732925415 | KNN Loss: 4.3461384773254395 | BCE Loss: 1.025838851928711\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 5.365981578826904 | KNN Loss: 4.325695037841797 | BCE Loss: 1.040286660194397\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 5.353357315063477 | KNN Loss: 4.332359790802002 | BCE Loss: 1.0209972858428955\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 5.382058620452881 | KNN Loss: 4.366494178771973 | BCE Loss: 1.0155645608901978\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 5.375730514526367 | KNN Loss: 4.361492156982422 | BCE Loss: 1.0142381191253662\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 5.400457859039307 | KNN Loss: 4.338733673095703 | BCE Loss: 1.061724066734314\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 5.396864891052246 | KNN Loss: 4.370945930480957 | BCE Loss: 1.0259191989898682\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 5.2870306968688965 | KNN Loss: 4.301589012145996 | BCE Loss: 0.9854415059089661\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 5.3120646476745605 | KNN Loss: 4.3072357177734375 | BCE Loss: 1.0048290491104126\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 5.338007926940918 | KNN Loss: 4.328530311584473 | BCE Loss: 1.0094773769378662\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 5.394283771514893 | KNN Loss: 4.364124774932861 | BCE Loss: 1.0301588773727417\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 5.329467296600342 | KNN Loss: 4.339918613433838 | BCE Loss: 0.9895486831665039\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 5.391368865966797 | KNN Loss: 4.346853733062744 | BCE Loss: 1.0445150136947632\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 5.41282320022583 | KNN Loss: 4.377746105194092 | BCE Loss: 1.0350770950317383\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 5.370767116546631 | KNN Loss: 4.330068111419678 | BCE Loss: 1.0406991243362427\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 5.394031524658203 | KNN Loss: 4.343072891235352 | BCE Loss: 1.0509583950042725\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 5.347407817840576 | KNN Loss: 4.3396687507629395 | BCE Loss: 1.0077391862869263\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 5.437160015106201 | KNN Loss: 4.3796186447143555 | BCE Loss: 1.0575413703918457\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 5.32131814956665 | KNN Loss: 4.3491530418396 | BCE Loss: 0.9721649885177612\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 5.35997200012207 | KNN Loss: 4.337610244750977 | BCE Loss: 1.0223617553710938\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 5.348514080047607 | KNN Loss: 4.3324198722839355 | BCE Loss: 1.0160942077636719\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 5.328387260437012 | KNN Loss: 4.341063976287842 | BCE Loss: 0.987323522567749\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 5.368473529815674 | KNN Loss: 4.3358869552612305 | BCE Loss: 1.0325865745544434\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 5.442439079284668 | KNN Loss: 4.417557716369629 | BCE Loss: 1.0248812437057495\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 5.339479446411133 | KNN Loss: 4.330403804779053 | BCE Loss: 1.0090758800506592\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 5.340429306030273 | KNN Loss: 4.334203720092773 | BCE Loss: 1.0062257051467896\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 5.393396854400635 | KNN Loss: 4.350583553314209 | BCE Loss: 1.0428131818771362\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 5.362337589263916 | KNN Loss: 4.328086853027344 | BCE Loss: 1.0342506170272827\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 5.405538558959961 | KNN Loss: 4.379157543182373 | BCE Loss: 1.0263807773590088\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 5.337851047515869 | KNN Loss: 4.316234588623047 | BCE Loss: 1.0216163396835327\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 5.399154186248779 | KNN Loss: 4.346365928649902 | BCE Loss: 1.0527881383895874\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 5.422112464904785 | KNN Loss: 4.3897528648376465 | BCE Loss: 1.0323593616485596\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 5.34536075592041 | KNN Loss: 4.32381534576416 | BCE Loss: 1.021545648574829\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 5.359190940856934 | KNN Loss: 4.3336076736450195 | BCE Loss: 1.025583028793335\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 5.392071723937988 | KNN Loss: 4.361793518066406 | BCE Loss: 1.0302784442901611\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 5.36068058013916 | KNN Loss: 4.344965934753418 | BCE Loss: 1.015714406967163\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 5.382067680358887 | KNN Loss: 4.347402095794678 | BCE Loss: 1.034665822982788\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 5.313867568969727 | KNN Loss: 4.321872711181641 | BCE Loss: 0.991995096206665\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 5.3878278732299805 | KNN Loss: 4.364081859588623 | BCE Loss: 1.0237457752227783\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 5.377100467681885 | KNN Loss: 4.361758232116699 | BCE Loss: 1.015342354774475\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 5.398420333862305 | KNN Loss: 4.359021186828613 | BCE Loss: 1.0393991470336914\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 5.357806205749512 | KNN Loss: 4.342174530029297 | BCE Loss: 1.0156314373016357\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 5.380480766296387 | KNN Loss: 4.359092712402344 | BCE Loss: 1.021388292312622\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 5.369251251220703 | KNN Loss: 4.344156742095947 | BCE Loss: 1.025094747543335\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 5.413910865783691 | KNN Loss: 4.370580196380615 | BCE Loss: 1.043330430984497\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 5.373772621154785 | KNN Loss: 4.354669094085693 | BCE Loss: 1.0191035270690918\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 5.355670928955078 | KNN Loss: 4.317839622497559 | BCE Loss: 1.0378315448760986\n",
      "Epoch   426: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 5.369091987609863 | KNN Loss: 4.350742816925049 | BCE Loss: 1.0183494091033936\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 5.330009937286377 | KNN Loss: 4.309457778930664 | BCE Loss: 1.020552158355713\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 5.325631141662598 | KNN Loss: 4.315485000610352 | BCE Loss: 1.0101463794708252\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 5.350982666015625 | KNN Loss: 4.347726821899414 | BCE Loss: 1.0032559633255005\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 5.332135200500488 | KNN Loss: 4.3168559074401855 | BCE Loss: 1.0152795314788818\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 5.345321178436279 | KNN Loss: 4.324735641479492 | BCE Loss: 1.020585536956787\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 5.3850908279418945 | KNN Loss: 4.347304344177246 | BCE Loss: 1.0377867221832275\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 5.373865604400635 | KNN Loss: 4.357546329498291 | BCE Loss: 1.0163193941116333\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 5.411309242248535 | KNN Loss: 4.3536529541015625 | BCE Loss: 1.0576562881469727\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 5.38144063949585 | KNN Loss: 4.353669166564941 | BCE Loss: 1.0277714729309082\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 5.413045406341553 | KNN Loss: 4.3929338455200195 | BCE Loss: 1.0201114416122437\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 5.34617805480957 | KNN Loss: 4.345828056335449 | BCE Loss: 1.0003501176834106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 5.38817834854126 | KNN Loss: 4.370124340057373 | BCE Loss: 1.0180541276931763\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 5.366745471954346 | KNN Loss: 4.344224452972412 | BCE Loss: 1.0225211381912231\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 5.362945079803467 | KNN Loss: 4.344245910644531 | BCE Loss: 1.0186991691589355\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 5.349668502807617 | KNN Loss: 4.318215847015381 | BCE Loss: 1.0314526557922363\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 5.344742298126221 | KNN Loss: 4.332143306732178 | BCE Loss: 1.0125988721847534\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 5.41508674621582 | KNN Loss: 4.385757923126221 | BCE Loss: 1.0293288230895996\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 5.345803260803223 | KNN Loss: 4.332432270050049 | BCE Loss: 1.0133711099624634\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 5.35964298248291 | KNN Loss: 4.3458733558654785 | BCE Loss: 1.0137698650360107\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 5.365449905395508 | KNN Loss: 4.348247528076172 | BCE Loss: 1.0172024965286255\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 5.328707695007324 | KNN Loss: 4.309873104095459 | BCE Loss: 1.0188345909118652\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 5.38465690612793 | KNN Loss: 4.367301940917969 | BCE Loss: 1.01735520362854\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 5.321805953979492 | KNN Loss: 4.314520359039307 | BCE Loss: 1.0072858333587646\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 5.370543956756592 | KNN Loss: 4.326483726501465 | BCE Loss: 1.0440601110458374\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 5.372304439544678 | KNN Loss: 4.3148722648620605 | BCE Loss: 1.0574322938919067\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 5.4326276779174805 | KNN Loss: 4.38324499130249 | BCE Loss: 1.0493825674057007\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 5.400148391723633 | KNN Loss: 4.369992733001709 | BCE Loss: 1.0301554203033447\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 5.408803939819336 | KNN Loss: 4.392678260803223 | BCE Loss: 1.0161256790161133\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 5.367060661315918 | KNN Loss: 4.378334999084473 | BCE Loss: 0.9887259006500244\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 5.34895658493042 | KNN Loss: 4.331594944000244 | BCE Loss: 1.0173616409301758\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 5.341766834259033 | KNN Loss: 4.325150489807129 | BCE Loss: 1.0166162252426147\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 5.35976505279541 | KNN Loss: 4.349410533905029 | BCE Loss: 1.0103545188903809\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 5.36646842956543 | KNN Loss: 4.348942756652832 | BCE Loss: 1.017525553703308\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 5.448522567749023 | KNN Loss: 4.431072235107422 | BCE Loss: 1.0174504518508911\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 5.318070888519287 | KNN Loss: 4.313929557800293 | BCE Loss: 1.0041413307189941\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 5.390047073364258 | KNN Loss: 4.339146137237549 | BCE Loss: 1.0509008169174194\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 5.373691558837891 | KNN Loss: 4.353328227996826 | BCE Loss: 1.0203633308410645\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 5.317314624786377 | KNN Loss: 4.307098865509033 | BCE Loss: 1.0102157592773438\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 5.3212785720825195 | KNN Loss: 4.322930335998535 | BCE Loss: 0.9983482360839844\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 5.391120910644531 | KNN Loss: 4.379364490509033 | BCE Loss: 1.0117566585540771\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 5.313159465789795 | KNN Loss: 4.3029680252075195 | BCE Loss: 1.010191559791565\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 5.355875015258789 | KNN Loss: 4.317258358001709 | BCE Loss: 1.038616418838501\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 5.327524185180664 | KNN Loss: 4.335330009460449 | BCE Loss: 0.992194414138794\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 5.372518062591553 | KNN Loss: 4.3550872802734375 | BCE Loss: 1.0174307823181152\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 5.328316688537598 | KNN Loss: 4.320995330810547 | BCE Loss: 1.0073211193084717\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 5.350966453552246 | KNN Loss: 4.341095447540283 | BCE Loss: 1.009871006011963\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 5.371100902557373 | KNN Loss: 4.363203525543213 | BCE Loss: 1.0078973770141602\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 5.391679763793945 | KNN Loss: 4.330192565917969 | BCE Loss: 1.0614869594573975\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 5.358394145965576 | KNN Loss: 4.328138828277588 | BCE Loss: 1.0302553176879883\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 5.409595489501953 | KNN Loss: 4.383220195770264 | BCE Loss: 1.0263752937316895\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 5.341490745544434 | KNN Loss: 4.323155403137207 | BCE Loss: 1.0183355808258057\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 5.371784210205078 | KNN Loss: 4.344961166381836 | BCE Loss: 1.0268230438232422\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 5.375831604003906 | KNN Loss: 4.330050468444824 | BCE Loss: 1.0457812547683716\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 5.332060813903809 | KNN Loss: 4.328202724456787 | BCE Loss: 1.0038583278656006\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 5.349677085876465 | KNN Loss: 4.343297481536865 | BCE Loss: 1.0063796043395996\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 5.409729957580566 | KNN Loss: 4.360506534576416 | BCE Loss: 1.0492231845855713\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 5.39302921295166 | KNN Loss: 4.352762222290039 | BCE Loss: 1.040266990661621\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 5.340631484985352 | KNN Loss: 4.323039531707764 | BCE Loss: 1.0175917148590088\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 5.368315696716309 | KNN Loss: 4.3308868408203125 | BCE Loss: 1.037428855895996\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 5.318955898284912 | KNN Loss: 4.301133632659912 | BCE Loss: 1.0178223848342896\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 5.387875556945801 | KNN Loss: 4.3684492111206055 | BCE Loss: 1.0194262266159058\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 5.391483306884766 | KNN Loss: 4.360297679901123 | BCE Loss: 1.0311857461929321\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 5.314976215362549 | KNN Loss: 4.314643383026123 | BCE Loss: 1.0003327131271362\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 5.283623695373535 | KNN Loss: 4.320610523223877 | BCE Loss: 0.9630134105682373\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 5.366410255432129 | KNN Loss: 4.368248462677002 | BCE Loss: 0.9981615543365479\n",
      "Epoch   437: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 5.336577415466309 | KNN Loss: 4.323526382446289 | BCE Loss: 1.013051152229309\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 5.395279884338379 | KNN Loss: 4.378271102905273 | BCE Loss: 1.017008900642395\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 5.372471332550049 | KNN Loss: 4.365583419799805 | BCE Loss: 1.0068880319595337\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 5.366621017456055 | KNN Loss: 4.346240520477295 | BCE Loss: 1.0203806161880493\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 5.410236358642578 | KNN Loss: 4.3757429122924805 | BCE Loss: 1.0344936847686768\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 5.376186370849609 | KNN Loss: 4.357659339904785 | BCE Loss: 1.0185272693634033\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 5.456302165985107 | KNN Loss: 4.4305901527404785 | BCE Loss: 1.025712013244629\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 5.366961479187012 | KNN Loss: 4.329488754272461 | BCE Loss: 1.0374727249145508\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 5.403871536254883 | KNN Loss: 4.355162620544434 | BCE Loss: 1.0487087965011597\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 5.349029541015625 | KNN Loss: 4.3328351974487305 | BCE Loss: 1.0161943435668945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 5.323318958282471 | KNN Loss: 4.311581611633301 | BCE Loss: 1.0117374658584595\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 5.362170696258545 | KNN Loss: 4.335082054138184 | BCE Loss: 1.0270886421203613\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 5.435814380645752 | KNN Loss: 4.394586563110352 | BCE Loss: 1.0412278175354004\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 5.361844539642334 | KNN Loss: 4.361156940460205 | BCE Loss: 1.0006874799728394\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 5.325392246246338 | KNN Loss: 4.312139987945557 | BCE Loss: 1.0132522583007812\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 5.370399475097656 | KNN Loss: 4.345261573791504 | BCE Loss: 1.0251379013061523\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 5.35184383392334 | KNN Loss: 4.333664894104004 | BCE Loss: 1.0181788206100464\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 5.35351037979126 | KNN Loss: 4.330833911895752 | BCE Loss: 1.0226763486862183\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 5.3770341873168945 | KNN Loss: 4.353868007659912 | BCE Loss: 1.0231661796569824\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 5.337684631347656 | KNN Loss: 4.343018054962158 | BCE Loss: 0.9946666359901428\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 5.347980499267578 | KNN Loss: 4.316727161407471 | BCE Loss: 1.0312533378601074\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 5.328960418701172 | KNN Loss: 4.325671195983887 | BCE Loss: 1.0032892227172852\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 5.4489312171936035 | KNN Loss: 4.409168720245361 | BCE Loss: 1.0397624969482422\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 5.3647332191467285 | KNN Loss: 4.33299446105957 | BCE Loss: 1.0317388772964478\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 5.37947416305542 | KNN Loss: 4.3440423011779785 | BCE Loss: 1.0354318618774414\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 5.392683029174805 | KNN Loss: 4.357497215270996 | BCE Loss: 1.0351860523223877\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 5.369771957397461 | KNN Loss: 4.351024627685547 | BCE Loss: 1.0187474489212036\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 5.364912033081055 | KNN Loss: 4.348459243774414 | BCE Loss: 1.016452670097351\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 5.3910722732543945 | KNN Loss: 4.389004230499268 | BCE Loss: 1.002068042755127\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 5.341978549957275 | KNN Loss: 4.331337928771973 | BCE Loss: 1.0106406211853027\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 5.323726177215576 | KNN Loss: 4.3083271980285645 | BCE Loss: 1.0153988599777222\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 5.294146537780762 | KNN Loss: 4.301454067230225 | BCE Loss: 0.9926923513412476\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 5.390445709228516 | KNN Loss: 4.331206798553467 | BCE Loss: 1.0592389106750488\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 5.349119186401367 | KNN Loss: 4.334541320800781 | BCE Loss: 1.0145776271820068\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 5.3545684814453125 | KNN Loss: 4.327253341674805 | BCE Loss: 1.0273151397705078\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 5.316612243652344 | KNN Loss: 4.314764976501465 | BCE Loss: 1.0018470287322998\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 5.374555587768555 | KNN Loss: 4.3519206047058105 | BCE Loss: 1.0226349830627441\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 5.342779636383057 | KNN Loss: 4.341413497924805 | BCE Loss: 1.001366138458252\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 5.369120121002197 | KNN Loss: 4.334556579589844 | BCE Loss: 1.0345635414123535\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 5.335760593414307 | KNN Loss: 4.345813274383545 | BCE Loss: 0.9899473190307617\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 5.3329925537109375 | KNN Loss: 4.339743614196777 | BCE Loss: 0.9932489395141602\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 5.374914169311523 | KNN Loss: 4.356218338012695 | BCE Loss: 1.0186957120895386\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 5.349003791809082 | KNN Loss: 4.324628829956055 | BCE Loss: 1.0243749618530273\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 5.320940971374512 | KNN Loss: 4.295568466186523 | BCE Loss: 1.0253722667694092\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 5.3801422119140625 | KNN Loss: 4.356409549713135 | BCE Loss: 1.0237326622009277\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 5.329133033752441 | KNN Loss: 4.3194661140441895 | BCE Loss: 1.009666919708252\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 5.359539031982422 | KNN Loss: 4.334414482116699 | BCE Loss: 1.0251243114471436\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 5.355696678161621 | KNN Loss: 4.317931175231934 | BCE Loss: 1.0377655029296875\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 5.370046138763428 | KNN Loss: 4.3509931564331055 | BCE Loss: 1.0190529823303223\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 5.3665924072265625 | KNN Loss: 4.346987724304199 | BCE Loss: 1.0196044445037842\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 5.361437797546387 | KNN Loss: 4.329268932342529 | BCE Loss: 1.0321686267852783\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 5.364653587341309 | KNN Loss: 4.34841251373291 | BCE Loss: 1.0162410736083984\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 5.389654159545898 | KNN Loss: 4.335738658905029 | BCE Loss: 1.0539157390594482\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 5.360432147979736 | KNN Loss: 4.351039886474609 | BCE Loss: 1.009392261505127\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 5.374960899353027 | KNN Loss: 4.362988471984863 | BCE Loss: 1.011972188949585\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 5.360405921936035 | KNN Loss: 4.343070030212402 | BCE Loss: 1.0173360109329224\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 5.345519542694092 | KNN Loss: 4.325695991516113 | BCE Loss: 1.0198235511779785\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 5.402978420257568 | KNN Loss: 4.377834320068359 | BCE Loss: 1.0251442193984985\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 5.380825042724609 | KNN Loss: 4.356252670288086 | BCE Loss: 1.0245721340179443\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 5.371211528778076 | KNN Loss: 4.353463172912598 | BCE Loss: 1.017748236656189\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 5.359416484832764 | KNN Loss: 4.362617492675781 | BCE Loss: 0.9967989921569824\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 5.363309383392334 | KNN Loss: 4.341616153717041 | BCE Loss: 1.021693229675293\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 5.338618755340576 | KNN Loss: 4.320229530334473 | BCE Loss: 1.0183892250061035\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 5.36821174621582 | KNN Loss: 4.336576461791992 | BCE Loss: 1.0316351652145386\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 5.368729591369629 | KNN Loss: 4.356657028198242 | BCE Loss: 1.0120725631713867\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 5.370263576507568 | KNN Loss: 4.329634189605713 | BCE Loss: 1.040629506111145\n",
      "Epoch   448: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 5.391820907592773 | KNN Loss: 4.356589317321777 | BCE Loss: 1.0352318286895752\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 5.360119819641113 | KNN Loss: 4.327392101287842 | BCE Loss: 1.032727599143982\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 5.347968101501465 | KNN Loss: 4.3543572425842285 | BCE Loss: 0.9936107397079468\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 5.375636577606201 | KNN Loss: 4.336983680725098 | BCE Loss: 1.0386528968811035\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 5.409194469451904 | KNN Loss: 4.375543117523193 | BCE Loss: 1.033651351928711\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 5.393739223480225 | KNN Loss: 4.351355075836182 | BCE Loss: 1.0423840284347534\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 5.346652984619141 | KNN Loss: 4.306296348571777 | BCE Loss: 1.0403566360473633\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 5.365215301513672 | KNN Loss: 4.331324577331543 | BCE Loss: 1.0338904857635498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 5.347927093505859 | KNN Loss: 4.343445301055908 | BCE Loss: 1.004481554031372\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 5.371289253234863 | KNN Loss: 4.334761142730713 | BCE Loss: 1.03652822971344\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 5.332276344299316 | KNN Loss: 4.34283447265625 | BCE Loss: 0.9894418120384216\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 5.342615127563477 | KNN Loss: 4.345034599304199 | BCE Loss: 0.9975806474685669\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 5.359007835388184 | KNN Loss: 4.367350101470947 | BCE Loss: 0.9916574954986572\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 5.344439506530762 | KNN Loss: 4.330578327178955 | BCE Loss: 1.0138612985610962\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 5.396506309509277 | KNN Loss: 4.3855133056640625 | BCE Loss: 1.0109928846359253\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 5.382510662078857 | KNN Loss: 4.359109878540039 | BCE Loss: 1.0234006643295288\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 5.363676071166992 | KNN Loss: 4.320862293243408 | BCE Loss: 1.0428136587142944\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 5.390929222106934 | KNN Loss: 4.3702006340026855 | BCE Loss: 1.020728588104248\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 5.376385688781738 | KNN Loss: 4.346236228942871 | BCE Loss: 1.0301496982574463\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 5.33932638168335 | KNN Loss: 4.3560333251953125 | BCE Loss: 0.9832932353019714\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 5.3439812660217285 | KNN Loss: 4.30859375 | BCE Loss: 1.0353875160217285\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 5.325308799743652 | KNN Loss: 4.328250885009766 | BCE Loss: 0.9970577359199524\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 5.397470474243164 | KNN Loss: 4.364163875579834 | BCE Loss: 1.0333068370819092\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 5.381827354431152 | KNN Loss: 4.355709552764893 | BCE Loss: 1.0261175632476807\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 5.384920120239258 | KNN Loss: 4.345444679260254 | BCE Loss: 1.039475440979004\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 5.401455879211426 | KNN Loss: 4.366877555847168 | BCE Loss: 1.0345780849456787\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 5.378786563873291 | KNN Loss: 4.380302429199219 | BCE Loss: 0.9984841346740723\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 5.324297904968262 | KNN Loss: 4.308159351348877 | BCE Loss: 1.0161385536193848\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 5.364605903625488 | KNN Loss: 4.3217597007751465 | BCE Loss: 1.0428460836410522\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 5.341378211975098 | KNN Loss: 4.321590423583984 | BCE Loss: 1.0197875499725342\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 5.326533794403076 | KNN Loss: 4.318951606750488 | BCE Loss: 1.0075820684432983\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 5.361440658569336 | KNN Loss: 4.347532272338867 | BCE Loss: 1.0139085054397583\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 5.357062816619873 | KNN Loss: 4.343202590942383 | BCE Loss: 1.0138602256774902\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 5.371979713439941 | KNN Loss: 4.369606971740723 | BCE Loss: 1.0023725032806396\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 5.41973352432251 | KNN Loss: 4.378598690032959 | BCE Loss: 1.0411347150802612\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 5.356097221374512 | KNN Loss: 4.340066432952881 | BCE Loss: 1.01603102684021\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 5.360726356506348 | KNN Loss: 4.334069728851318 | BCE Loss: 1.0266563892364502\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 5.364340782165527 | KNN Loss: 4.322768211364746 | BCE Loss: 1.0415728092193604\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 5.372845649719238 | KNN Loss: 4.38027811050415 | BCE Loss: 0.9925674200057983\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 5.364975929260254 | KNN Loss: 4.37218713760376 | BCE Loss: 0.9927889108657837\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 5.387807369232178 | KNN Loss: 4.3593902587890625 | BCE Loss: 1.0284172296524048\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 5.388091087341309 | KNN Loss: 4.355169296264648 | BCE Loss: 1.0329220294952393\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 5.377325057983398 | KNN Loss: 4.368414878845215 | BCE Loss: 1.008910059928894\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 5.3652777671813965 | KNN Loss: 4.33924674987793 | BCE Loss: 1.0260308980941772\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 5.3232598304748535 | KNN Loss: 4.336275100708008 | BCE Loss: 0.9869846701622009\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 5.338030815124512 | KNN Loss: 4.318241119384766 | BCE Loss: 1.0197899341583252\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 5.384910583496094 | KNN Loss: 4.359103202819824 | BCE Loss: 1.0258073806762695\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 5.351327419281006 | KNN Loss: 4.347209453582764 | BCE Loss: 1.0041179656982422\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 5.376103401184082 | KNN Loss: 4.363004684448242 | BCE Loss: 1.0130985975265503\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 5.381654739379883 | KNN Loss: 4.3620991706848145 | BCE Loss: 1.0195553302764893\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 5.394252777099609 | KNN Loss: 4.374091625213623 | BCE Loss: 1.0201613903045654\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 5.361417770385742 | KNN Loss: 4.333462715148926 | BCE Loss: 1.0279552936553955\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 5.380810737609863 | KNN Loss: 4.346614837646484 | BCE Loss: 1.0341960191726685\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 5.376779079437256 | KNN Loss: 4.3374762535095215 | BCE Loss: 1.0393028259277344\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 5.421206474304199 | KNN Loss: 4.378581523895264 | BCE Loss: 1.042625069618225\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 5.317731857299805 | KNN Loss: 4.3144354820251465 | BCE Loss: 1.0032966136932373\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 5.379697799682617 | KNN Loss: 4.340640544891357 | BCE Loss: 1.0390574932098389\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 5.296747207641602 | KNN Loss: 4.312868118286133 | BCE Loss: 0.9838789701461792\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 5.358959197998047 | KNN Loss: 4.350022792816162 | BCE Loss: 1.0089361667633057\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 5.369566917419434 | KNN Loss: 4.367405414581299 | BCE Loss: 1.0021617412567139\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 5.430825233459473 | KNN Loss: 4.40906286239624 | BCE Loss: 1.0217623710632324\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 5.388725280761719 | KNN Loss: 4.336978912353516 | BCE Loss: 1.051746129989624\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 5.346391677856445 | KNN Loss: 4.318672180175781 | BCE Loss: 1.0277197360992432\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 5.376150131225586 | KNN Loss: 4.3692240715026855 | BCE Loss: 1.00692617893219\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 5.377695083618164 | KNN Loss: 4.356226921081543 | BCE Loss: 1.021467924118042\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 5.36876916885376 | KNN Loss: 4.374093532562256 | BCE Loss: 0.9946755170822144\n",
      "Epoch   459: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 5.3713531494140625 | KNN Loss: 4.338085651397705 | BCE Loss: 1.0332677364349365\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 5.380296230316162 | KNN Loss: 4.356801509857178 | BCE Loss: 1.0234947204589844\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 5.307773113250732 | KNN Loss: 4.301130771636963 | BCE Loss: 1.00664222240448\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 5.374912738800049 | KNN Loss: 4.335512161254883 | BCE Loss: 1.0394006967544556\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 5.363681793212891 | KNN Loss: 4.334259986877441 | BCE Loss: 1.0294218063354492\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 5.390701770782471 | KNN Loss: 4.338132858276367 | BCE Loss: 1.0525689125061035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 5.353022575378418 | KNN Loss: 4.3539886474609375 | BCE Loss: 0.9990341663360596\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 5.396484375 | KNN Loss: 4.3535637855529785 | BCE Loss: 1.0429205894470215\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 5.351693630218506 | KNN Loss: 4.327581405639648 | BCE Loss: 1.0241122245788574\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 5.358039379119873 | KNN Loss: 4.322794437408447 | BCE Loss: 1.0352450609207153\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 5.373109817504883 | KNN Loss: 4.332299709320068 | BCE Loss: 1.0408098697662354\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 5.381913185119629 | KNN Loss: 4.349060535430908 | BCE Loss: 1.0328526496887207\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 5.328824043273926 | KNN Loss: 4.314085960388184 | BCE Loss: 1.0147383213043213\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 5.407964706420898 | KNN Loss: 4.3662428855896 | BCE Loss: 1.0417218208312988\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 5.431783199310303 | KNN Loss: 4.425815582275391 | BCE Loss: 1.0059674978256226\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 5.392434597015381 | KNN Loss: 4.341890335083008 | BCE Loss: 1.0505443811416626\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 5.360872268676758 | KNN Loss: 4.358730316162109 | BCE Loss: 1.0021421909332275\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 5.410496711730957 | KNN Loss: 4.3775763511657715 | BCE Loss: 1.032920479774475\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 5.3417558670043945 | KNN Loss: 4.3316168785095215 | BCE Loss: 1.0101392269134521\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 5.3832268714904785 | KNN Loss: 4.363791465759277 | BCE Loss: 1.0194354057312012\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 5.333512306213379 | KNN Loss: 4.318672180175781 | BCE Loss: 1.0148403644561768\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 5.334319114685059 | KNN Loss: 4.328668594360352 | BCE Loss: 1.0056506395339966\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 5.388006687164307 | KNN Loss: 4.3632988929748535 | BCE Loss: 1.0247077941894531\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 5.360602378845215 | KNN Loss: 4.338612079620361 | BCE Loss: 1.021990418434143\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 5.362503528594971 | KNN Loss: 4.345210552215576 | BCE Loss: 1.017293095588684\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 5.372410774230957 | KNN Loss: 4.342883586883545 | BCE Loss: 1.0295270681381226\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 5.39484977722168 | KNN Loss: 4.362054824829102 | BCE Loss: 1.0327951908111572\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 5.3673272132873535 | KNN Loss: 4.344984531402588 | BCE Loss: 1.0223428010940552\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 5.348107814788818 | KNN Loss: 4.333487510681152 | BCE Loss: 1.0146204233169556\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 5.3421525955200195 | KNN Loss: 4.327915191650391 | BCE Loss: 1.014237403869629\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 5.389841079711914 | KNN Loss: 4.348336219787598 | BCE Loss: 1.0415050983428955\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 5.396492958068848 | KNN Loss: 4.375247955322266 | BCE Loss: 1.0212452411651611\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 5.342288494110107 | KNN Loss: 4.335613250732422 | BCE Loss: 1.006675362586975\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 5.3119072914123535 | KNN Loss: 4.3043389320373535 | BCE Loss: 1.0075682401657104\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 5.392159938812256 | KNN Loss: 4.345623016357422 | BCE Loss: 1.046536922454834\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 5.346643447875977 | KNN Loss: 4.331491947174072 | BCE Loss: 1.0151513814926147\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 5.323933124542236 | KNN Loss: 4.335317134857178 | BCE Loss: 0.9886159896850586\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 5.351474761962891 | KNN Loss: 4.324731349945068 | BCE Loss: 1.0267432928085327\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 5.378322601318359 | KNN Loss: 4.3670806884765625 | BCE Loss: 1.0112419128417969\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 5.402612686157227 | KNN Loss: 4.350347518920898 | BCE Loss: 1.052264928817749\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 5.445458889007568 | KNN Loss: 4.397336959838867 | BCE Loss: 1.0481219291687012\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 5.420582294464111 | KNN Loss: 4.369903564453125 | BCE Loss: 1.0506788492202759\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 5.355194091796875 | KNN Loss: 4.331829071044922 | BCE Loss: 1.0233650207519531\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 5.389960289001465 | KNN Loss: 4.3599677085876465 | BCE Loss: 1.0299925804138184\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 5.408340930938721 | KNN Loss: 4.3804426193237305 | BCE Loss: 1.0278981924057007\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 5.3559980392456055 | KNN Loss: 4.361722469329834 | BCE Loss: 0.994275689125061\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 5.345240592956543 | KNN Loss: 4.325447082519531 | BCE Loss: 1.0197937488555908\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 5.392066478729248 | KNN Loss: 4.364806652069092 | BCE Loss: 1.0272599458694458\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 5.356792449951172 | KNN Loss: 4.328596591949463 | BCE Loss: 1.0281959772109985\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 5.3277740478515625 | KNN Loss: 4.318571090698242 | BCE Loss: 1.0092028379440308\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 5.342555046081543 | KNN Loss: 4.318461894989014 | BCE Loss: 1.0240931510925293\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 5.359252452850342 | KNN Loss: 4.321659088134766 | BCE Loss: 1.0375934839248657\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 5.322934627532959 | KNN Loss: 4.304314613342285 | BCE Loss: 1.0186198949813843\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 5.375236988067627 | KNN Loss: 4.3507537841796875 | BCE Loss: 1.024483323097229\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 5.385812759399414 | KNN Loss: 4.358142852783203 | BCE Loss: 1.027669906616211\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 5.39948034286499 | KNN Loss: 4.350276947021484 | BCE Loss: 1.0492032766342163\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 5.35809326171875 | KNN Loss: 4.335891246795654 | BCE Loss: 1.0222020149230957\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 5.350370407104492 | KNN Loss: 4.332316875457764 | BCE Loss: 1.0180535316467285\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 5.4186625480651855 | KNN Loss: 4.366915225982666 | BCE Loss: 1.0517473220825195\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 5.349379062652588 | KNN Loss: 4.343212604522705 | BCE Loss: 1.0061664581298828\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 5.35469913482666 | KNN Loss: 4.311977863311768 | BCE Loss: 1.0427210330963135\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 5.335479259490967 | KNN Loss: 4.303797245025635 | BCE Loss: 1.031682014465332\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 5.40447998046875 | KNN Loss: 4.3754353523254395 | BCE Loss: 1.0290447473526\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 5.3529181480407715 | KNN Loss: 4.339841365814209 | BCE Loss: 1.0130767822265625\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 5.368585586547852 | KNN Loss: 4.350846767425537 | BCE Loss: 1.0177388191223145\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 5.31109619140625 | KNN Loss: 4.294373035430908 | BCE Loss: 1.0167231559753418\n",
      "Epoch   470: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 5.355121612548828 | KNN Loss: 4.329237461090088 | BCE Loss: 1.0258841514587402\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 5.360445022583008 | KNN Loss: 4.343910217285156 | BCE Loss: 1.016534686088562\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 5.32600212097168 | KNN Loss: 4.307861328125 | BCE Loss: 1.0181405544281006\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 5.310223579406738 | KNN Loss: 4.312890529632568 | BCE Loss: 0.9973328113555908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 5.346129894256592 | KNN Loss: 4.314916133880615 | BCE Loss: 1.0312138795852661\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 5.4247517585754395 | KNN Loss: 4.376527309417725 | BCE Loss: 1.0482245683670044\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 5.441575050354004 | KNN Loss: 4.41087007522583 | BCE Loss: 1.0307047367095947\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 5.354607582092285 | KNN Loss: 4.321402072906494 | BCE Loss: 1.0332057476043701\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 5.36297607421875 | KNN Loss: 4.337094783782959 | BCE Loss: 1.025881052017212\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 5.333868980407715 | KNN Loss: 4.327200889587402 | BCE Loss: 1.0066680908203125\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 5.383738040924072 | KNN Loss: 4.366393566131592 | BCE Loss: 1.017344355583191\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 5.364250659942627 | KNN Loss: 4.344139575958252 | BCE Loss: 1.0201112031936646\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 5.376491546630859 | KNN Loss: 4.348618984222412 | BCE Loss: 1.0278725624084473\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 5.372124195098877 | KNN Loss: 4.348013877868652 | BCE Loss: 1.0241104364395142\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 5.431676387786865 | KNN Loss: 4.3763885498046875 | BCE Loss: 1.0552879571914673\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 5.379570007324219 | KNN Loss: 4.3504533767700195 | BCE Loss: 1.0291163921356201\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 5.347840309143066 | KNN Loss: 4.332337379455566 | BCE Loss: 1.015503168106079\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 5.394342422485352 | KNN Loss: 4.3536224365234375 | BCE Loss: 1.0407202243804932\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 5.374391555786133 | KNN Loss: 4.352596759796143 | BCE Loss: 1.0217946767807007\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 5.30462646484375 | KNN Loss: 4.300144195556641 | BCE Loss: 1.0044822692871094\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 5.345239639282227 | KNN Loss: 4.3408894538879395 | BCE Loss: 1.004349946975708\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 5.392839431762695 | KNN Loss: 4.319705486297607 | BCE Loss: 1.073133945465088\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 5.33760404586792 | KNN Loss: 4.341341495513916 | BCE Loss: 0.9962626695632935\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 5.355395317077637 | KNN Loss: 4.335538864135742 | BCE Loss: 1.0198564529418945\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 5.39852237701416 | KNN Loss: 4.364753723144531 | BCE Loss: 1.033768653869629\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 5.330408096313477 | KNN Loss: 4.307624340057373 | BCE Loss: 1.0227839946746826\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 5.399145126342773 | KNN Loss: 4.366535186767578 | BCE Loss: 1.0326101779937744\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 5.35926628112793 | KNN Loss: 4.330782890319824 | BCE Loss: 1.028483271598816\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 5.323678016662598 | KNN Loss: 4.3480634689331055 | BCE Loss: 0.9756144285202026\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 5.324893474578857 | KNN Loss: 4.311169147491455 | BCE Loss: 1.0137243270874023\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 5.410370826721191 | KNN Loss: 4.38240909576416 | BCE Loss: 1.0279618501663208\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 5.401761054992676 | KNN Loss: 4.37369966506958 | BCE Loss: 1.0280613899230957\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 5.331203937530518 | KNN Loss: 4.327486515045166 | BCE Loss: 1.0037174224853516\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 5.401718616485596 | KNN Loss: 4.361259937286377 | BCE Loss: 1.0404586791992188\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 5.368485450744629 | KNN Loss: 4.310838222503662 | BCE Loss: 1.0576472282409668\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 5.386579513549805 | KNN Loss: 4.376094818115234 | BCE Loss: 1.0104849338531494\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 5.376067161560059 | KNN Loss: 4.3515424728393555 | BCE Loss: 1.0245249271392822\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 5.396669387817383 | KNN Loss: 4.369822978973389 | BCE Loss: 1.026846170425415\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 5.357993125915527 | KNN Loss: 4.322635173797607 | BCE Loss: 1.03535795211792\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 5.334707260131836 | KNN Loss: 4.3305439949035645 | BCE Loss: 1.0041635036468506\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 5.402670860290527 | KNN Loss: 4.341127395629883 | BCE Loss: 1.0615437030792236\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 5.376991271972656 | KNN Loss: 4.3546977043151855 | BCE Loss: 1.0222938060760498\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 5.34124755859375 | KNN Loss: 4.333004474639893 | BCE Loss: 1.0082433223724365\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 5.351102352142334 | KNN Loss: 4.334325790405273 | BCE Loss: 1.01677668094635\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 5.357344627380371 | KNN Loss: 4.330051422119141 | BCE Loss: 1.0272934436798096\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 5.322268486022949 | KNN Loss: 4.324993133544922 | BCE Loss: 0.9972753524780273\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 5.351452350616455 | KNN Loss: 4.3278069496154785 | BCE Loss: 1.023645281791687\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 5.364716529846191 | KNN Loss: 4.349562644958496 | BCE Loss: 1.0151541233062744\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 5.419892311096191 | KNN Loss: 4.363280773162842 | BCE Loss: 1.0566116571426392\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 5.435088157653809 | KNN Loss: 4.387976169586182 | BCE Loss: 1.047111988067627\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 5.350100517272949 | KNN Loss: 4.342779636383057 | BCE Loss: 1.0073210000991821\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 5.367460250854492 | KNN Loss: 4.342903137207031 | BCE Loss: 1.024557113647461\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 5.38455867767334 | KNN Loss: 4.347678184509277 | BCE Loss: 1.036880373954773\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 5.378920555114746 | KNN Loss: 4.344514846801758 | BCE Loss: 1.0344059467315674\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 5.38564395904541 | KNN Loss: 4.371278762817383 | BCE Loss: 1.0143654346466064\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 5.392675399780273 | KNN Loss: 4.3599371910095215 | BCE Loss: 1.032738208770752\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 5.322753429412842 | KNN Loss: 4.327136039733887 | BCE Loss: 0.9956173896789551\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 5.345232963562012 | KNN Loss: 4.338419437408447 | BCE Loss: 1.006813406944275\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 5.326781272888184 | KNN Loss: 4.325841426849365 | BCE Loss: 1.0009400844573975\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 5.387063980102539 | KNN Loss: 4.363967418670654 | BCE Loss: 1.0230966806411743\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 5.332672119140625 | KNN Loss: 4.330792427062988 | BCE Loss: 1.0018796920776367\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 5.383639812469482 | KNN Loss: 4.350864410400391 | BCE Loss: 1.0327755212783813\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 5.349477767944336 | KNN Loss: 4.334878444671631 | BCE Loss: 1.014599084854126\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 5.379034996032715 | KNN Loss: 4.353785037994385 | BCE Loss: 1.0252501964569092\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 5.364802360534668 | KNN Loss: 4.338752746582031 | BCE Loss: 1.0260493755340576\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 5.385323524475098 | KNN Loss: 4.366543292999268 | BCE Loss: 1.0187803506851196\n",
      "Epoch   481: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 5.385457515716553 | KNN Loss: 4.340404510498047 | BCE Loss: 1.0450531244277954\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 5.398715019226074 | KNN Loss: 4.360954284667969 | BCE Loss: 1.037760615348816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 5.380999565124512 | KNN Loss: 4.382349491119385 | BCE Loss: 0.9986500144004822\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 5.323890209197998 | KNN Loss: 4.32285737991333 | BCE Loss: 1.001032829284668\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 5.402249336242676 | KNN Loss: 4.356246471405029 | BCE Loss: 1.0460028648376465\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 5.36060905456543 | KNN Loss: 4.328298568725586 | BCE Loss: 1.0323104858398438\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 5.40244722366333 | KNN Loss: 4.394598484039307 | BCE Loss: 1.0078487396240234\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 5.389484405517578 | KNN Loss: 4.368040561676025 | BCE Loss: 1.0214439630508423\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 5.365406036376953 | KNN Loss: 4.366781711578369 | BCE Loss: 0.9986240863800049\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 5.40535306930542 | KNN Loss: 4.355584144592285 | BCE Loss: 1.0497688055038452\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 5.318089962005615 | KNN Loss: 4.314461708068848 | BCE Loss: 1.0036282539367676\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 5.3485026359558105 | KNN Loss: 4.316904544830322 | BCE Loss: 1.0315980911254883\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 5.344536781311035 | KNN Loss: 4.328054904937744 | BCE Loss: 1.016481637954712\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 5.343708038330078 | KNN Loss: 4.326493740081787 | BCE Loss: 1.0172144174575806\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 5.305399417877197 | KNN Loss: 4.295722484588623 | BCE Loss: 1.0096770524978638\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 5.287832260131836 | KNN Loss: 4.326537132263184 | BCE Loss: 0.9612953662872314\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 5.352059364318848 | KNN Loss: 4.3328328132629395 | BCE Loss: 1.0192264318466187\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 5.370100021362305 | KNN Loss: 4.321523189544678 | BCE Loss: 1.048576831817627\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 5.385987281799316 | KNN Loss: 4.3383965492248535 | BCE Loss: 1.0475908517837524\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 5.409841060638428 | KNN Loss: 4.384139060974121 | BCE Loss: 1.0257021188735962\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 5.344275951385498 | KNN Loss: 4.326251983642578 | BCE Loss: 1.0180238485336304\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 5.371767520904541 | KNN Loss: 4.342175006866455 | BCE Loss: 1.0295926332473755\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 5.321000099182129 | KNN Loss: 4.30827522277832 | BCE Loss: 1.0127248764038086\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 5.3605546951293945 | KNN Loss: 4.339852333068848 | BCE Loss: 1.020702600479126\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 5.3569865226745605 | KNN Loss: 4.33191442489624 | BCE Loss: 1.0250720977783203\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 5.403068542480469 | KNN Loss: 4.349247455596924 | BCE Loss: 1.0538208484649658\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 5.354009628295898 | KNN Loss: 4.3363189697265625 | BCE Loss: 1.017690896987915\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 5.3786234855651855 | KNN Loss: 4.353166103363037 | BCE Loss: 1.0254573822021484\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 5.337096214294434 | KNN Loss: 4.327651500701904 | BCE Loss: 1.0094444751739502\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 5.383138179779053 | KNN Loss: 4.361624240875244 | BCE Loss: 1.0215139389038086\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 5.381797790527344 | KNN Loss: 4.3658952713012695 | BCE Loss: 1.0159024000167847\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 5.3563923835754395 | KNN Loss: 4.3677568435668945 | BCE Loss: 0.9886355400085449\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 5.372927665710449 | KNN Loss: 4.350142955780029 | BCE Loss: 1.0227844715118408\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 5.36379861831665 | KNN Loss: 4.319472312927246 | BCE Loss: 1.0443263053894043\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 5.3604512214660645 | KNN Loss: 4.330101013183594 | BCE Loss: 1.0303502082824707\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 5.324555397033691 | KNN Loss: 4.311400890350342 | BCE Loss: 1.0131542682647705\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 5.370854377746582 | KNN Loss: 4.320908546447754 | BCE Loss: 1.0499458312988281\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 5.327731132507324 | KNN Loss: 4.338673114776611 | BCE Loss: 0.9890580177307129\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 5.340452194213867 | KNN Loss: 4.3270368576049805 | BCE Loss: 1.0134153366088867\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 5.34769344329834 | KNN Loss: 4.332799434661865 | BCE Loss: 1.0148942470550537\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 5.36981201171875 | KNN Loss: 4.338773727416992 | BCE Loss: 1.0310381650924683\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 5.3818817138671875 | KNN Loss: 4.363508224487305 | BCE Loss: 1.018373727798462\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 5.35969352722168 | KNN Loss: 4.349713325500488 | BCE Loss: 1.0099804401397705\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 5.343022346496582 | KNN Loss: 4.314932346343994 | BCE Loss: 1.028090238571167\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 5.398264408111572 | KNN Loss: 4.358214378356934 | BCE Loss: 1.0400500297546387\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 5.348780632019043 | KNN Loss: 4.342944145202637 | BCE Loss: 1.0058362483978271\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 5.356807231903076 | KNN Loss: 4.320896148681641 | BCE Loss: 1.035911202430725\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 5.33646297454834 | KNN Loss: 4.343982219696045 | BCE Loss: 0.992480993270874\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 5.3454108238220215 | KNN Loss: 4.356399059295654 | BCE Loss: 0.9890117645263672\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 5.401187419891357 | KNN Loss: 4.385538101196289 | BCE Loss: 1.015649437904358\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 5.383661270141602 | KNN Loss: 4.353832721710205 | BCE Loss: 1.0298285484313965\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 5.442812442779541 | KNN Loss: 4.3678669929504395 | BCE Loss: 1.0749454498291016\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 5.342127323150635 | KNN Loss: 4.325423240661621 | BCE Loss: 1.0167040824890137\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 5.338830947875977 | KNN Loss: 4.3154120445251465 | BCE Loss: 1.02341890335083\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 5.358990669250488 | KNN Loss: 4.320635795593262 | BCE Loss: 1.0383546352386475\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 5.405056953430176 | KNN Loss: 4.373534679412842 | BCE Loss: 1.0315223932266235\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 5.323827266693115 | KNN Loss: 4.318639278411865 | BCE Loss: 1.0051881074905396\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 5.354518413543701 | KNN Loss: 4.334649085998535 | BCE Loss: 1.019869327545166\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 5.358773708343506 | KNN Loss: 4.319828033447266 | BCE Loss: 1.0389455556869507\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 5.331362724304199 | KNN Loss: 4.339081764221191 | BCE Loss: 0.9922807216644287\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 5.324280261993408 | KNN Loss: 4.310611248016357 | BCE Loss: 1.0136691331863403\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 5.343919277191162 | KNN Loss: 4.337766170501709 | BCE Loss: 1.0061531066894531\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 5.368762969970703 | KNN Loss: 4.332010746002197 | BCE Loss: 1.0367522239685059\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 5.358858108520508 | KNN Loss: 4.337260723114014 | BCE Loss: 1.021597146987915\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 5.342234134674072 | KNN Loss: 4.3579630851745605 | BCE Loss: 0.9842709302902222\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 5.322701454162598 | KNN Loss: 4.296403408050537 | BCE Loss: 1.0262980461120605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   492: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 5.350766658782959 | KNN Loss: 4.337794780731201 | BCE Loss: 1.0129719972610474\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 5.3958539962768555 | KNN Loss: 4.357967376708984 | BCE Loss: 1.0378867387771606\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 5.367641448974609 | KNN Loss: 4.3413214683532715 | BCE Loss: 1.026320219039917\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 5.3972930908203125 | KNN Loss: 4.384162902832031 | BCE Loss: 1.0131301879882812\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 5.36600399017334 | KNN Loss: 4.376267910003662 | BCE Loss: 0.9897359609603882\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 5.402640342712402 | KNN Loss: 4.371490955352783 | BCE Loss: 1.03114914894104\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 5.321385383605957 | KNN Loss: 4.327150344848633 | BCE Loss: 0.9942350387573242\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 5.334458351135254 | KNN Loss: 4.325331687927246 | BCE Loss: 1.0091266632080078\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 5.321530818939209 | KNN Loss: 4.307707786560059 | BCE Loss: 1.0138230323791504\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 5.348188400268555 | KNN Loss: 4.337267875671387 | BCE Loss: 1.010920524597168\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 5.372868537902832 | KNN Loss: 4.348460674285889 | BCE Loss: 1.0244076251983643\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 5.331971168518066 | KNN Loss: 4.310779094696045 | BCE Loss: 1.0211923122406006\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 5.400603294372559 | KNN Loss: 4.369604587554932 | BCE Loss: 1.0309988260269165\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 5.365383148193359 | KNN Loss: 4.337472438812256 | BCE Loss: 1.0279104709625244\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 5.362233638763428 | KNN Loss: 4.3443684577941895 | BCE Loss: 1.0178650617599487\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 5.345653533935547 | KNN Loss: 4.353534698486328 | BCE Loss: 0.9921190738677979\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 5.419934272766113 | KNN Loss: 4.37420654296875 | BCE Loss: 1.0457277297973633\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 5.378140926361084 | KNN Loss: 4.343915939331055 | BCE Loss: 1.0342249870300293\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 5.3751935958862305 | KNN Loss: 4.345223903656006 | BCE Loss: 1.0299694538116455\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 5.332407474517822 | KNN Loss: 4.3165602684021 | BCE Loss: 1.0158472061157227\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 5.442325115203857 | KNN Loss: 4.392031192779541 | BCE Loss: 1.0502938032150269\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 5.353277206420898 | KNN Loss: 4.359368324279785 | BCE Loss: 0.9939089417457581\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 5.346190929412842 | KNN Loss: 4.349349498748779 | BCE Loss: 0.996841549873352\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 5.407086372375488 | KNN Loss: 4.365429878234863 | BCE Loss: 1.041656732559204\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 5.354282379150391 | KNN Loss: 4.362851142883301 | BCE Loss: 0.9914313554763794\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 5.397070407867432 | KNN Loss: 4.370316505432129 | BCE Loss: 1.0267537832260132\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 5.34812593460083 | KNN Loss: 4.314838886260986 | BCE Loss: 1.0332870483398438\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 5.359877586364746 | KNN Loss: 4.336668014526367 | BCE Loss: 1.0232093334197998\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 5.405893325805664 | KNN Loss: 4.342911720275879 | BCE Loss: 1.0629816055297852\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 5.379560470581055 | KNN Loss: 4.3798699378967285 | BCE Loss: 0.9996904730796814\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 5.33608341217041 | KNN Loss: 4.341864585876465 | BCE Loss: 0.9942189455032349\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 5.335831642150879 | KNN Loss: 4.3237714767456055 | BCE Loss: 1.0120604038238525\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 5.345458507537842 | KNN Loss: 4.332082748413086 | BCE Loss: 1.0133756399154663\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 5.350464820861816 | KNN Loss: 4.337594509124756 | BCE Loss: 1.0128700733184814\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 5.35847806930542 | KNN Loss: 4.3360443115234375 | BCE Loss: 1.0224337577819824\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 5.367028713226318 | KNN Loss: 4.340418338775635 | BCE Loss: 1.0266103744506836\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 5.3647003173828125 | KNN Loss: 4.33404016494751 | BCE Loss: 1.0306603908538818\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 5.371845722198486 | KNN Loss: 4.335065841674805 | BCE Loss: 1.0367798805236816\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 5.350679397583008 | KNN Loss: 4.348047256469727 | BCE Loss: 1.0026322603225708\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 5.381633758544922 | KNN Loss: 4.367032527923584 | BCE Loss: 1.014601469039917\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 5.355415344238281 | KNN Loss: 4.319683074951172 | BCE Loss: 1.0357321500778198\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 5.348328590393066 | KNN Loss: 4.321061134338379 | BCE Loss: 1.027267336845398\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 5.34697151184082 | KNN Loss: 4.336111545562744 | BCE Loss: 1.0108599662780762\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 5.374886989593506 | KNN Loss: 4.3626627922058105 | BCE Loss: 1.0122243165969849\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 5.403926849365234 | KNN Loss: 4.364216327667236 | BCE Loss: 1.0397107601165771\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 5.405725479125977 | KNN Loss: 4.353860855102539 | BCE Loss: 1.0518643856048584\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 5.376019477844238 | KNN Loss: 4.343590259552002 | BCE Loss: 1.0324294567108154\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 5.327408313751221 | KNN Loss: 4.332000255584717 | BCE Loss: 0.9954079389572144\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8325,  4.6160,  3.0677,  2.2880,  4.1206,  0.6368,  2.1909,  1.6315,\n",
      "          1.8769,  2.4564,  2.1214,  1.3905,  1.1680,  2.0682,  1.4719,  0.8914,\n",
      "          2.3867,  1.8554,  3.2410,  2.5316,  2.0258,  1.7814,  2.7396,  2.8656,\n",
      "          1.6242,  1.8446,  1.2237,  0.8096,  1.0281,  0.5324,  0.1516,  0.8719,\n",
      "         -0.3351,  0.9580,  2.0002,  0.9887,  0.4817,  3.2517,  0.8580,  0.8546,\n",
      "          0.7083, -1.2026, -0.3733,  2.2330,  2.6924,  0.8294,  0.1402, -0.0804,\n",
      "          1.7870,  1.6762,  2.2953,  0.1201,  0.7758,  0.4457, -0.4955,  1.0042,\n",
      "          1.3151,  1.6255,  1.2598,  1.7012,  0.4507,  0.7257,  0.1015,  1.6082,\n",
      "          1.2640,  1.6263, -1.6217,  0.1696,  2.7663,  1.7402,  2.9492,  0.2148,\n",
      "          1.1837,  2.8738,  1.5330,  1.7702,  0.6192,  0.5534, -0.3117,  1.0783,\n",
      "         -0.0685,  0.2990,  2.1036, -0.3987,  0.5665, -1.0399, -2.3851,  0.2871,\n",
      "          0.5592, -1.7341,  0.2284, -0.0333, -0.1515, -1.0996,  0.6627,  0.8139,\n",
      "         -0.4803, -0.3365,  0.3222,  0.9843,  0.6618, -1.2225,  0.8728,  1.0337,\n",
      "         -1.2922, -0.8089, -0.6464, -0.0913, -0.6106, -1.3219, -0.1269, -2.7358,\n",
      "         -0.1506,  1.2557,  1.4974, -0.2260, -0.1767,  0.3149,  1.8346, -3.0380,\n",
      "          0.3890, -0.4808,  0.7838, -0.8171, -0.0728, -0.6918, -1.1535,  1.1104,\n",
      "          0.1344, -0.6231, -0.1021, -0.7552, -1.3452, -0.7106, -0.3008,  0.8007,\n",
      "         -1.0042,  0.1679, -2.0371, -1.0904, -1.1694,  0.5490, -2.0660, -1.1389,\n",
      "         -0.7849, -0.1262, -1.5618, -0.9534, -2.2315, -0.9899, -0.9464, -0.3277,\n",
      "         -1.7525,  0.1533, -1.3427, -0.4094, -3.0780, -0.0770,  0.0546, -0.8040,\n",
      "         -2.1001, -1.9958, -1.4181, -1.2354, -2.2803, -2.1846, -3.4752]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.4752, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.6160, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69185766bebe443d92038becc795586c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 83.33it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af659ea49582406e8b26141095449f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ab2ec940224f5aa89a60b3a3f9b877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84232852b0c346b5bf5db620b50ea9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 028 | Total loss: 9.634 | Reg loss: 0.014 | Tree loss: 9.634 | Accuracy: 0.000000 | 3.626 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 028 | Total loss: 9.632 | Reg loss: 0.013 | Tree loss: 9.632 | Accuracy: 0.000000 | 3.556 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 028 | Total loss: 9.629 | Reg loss: 0.012 | Tree loss: 9.629 | Accuracy: 0.000000 | 3.564 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 028 | Total loss: 9.625 | Reg loss: 0.011 | Tree loss: 9.625 | Accuracy: 0.000000 | 4.062 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 028 | Total loss: 9.624 | Reg loss: 0.010 | Tree loss: 9.624 | Accuracy: 0.000000 | 4.377 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 028 | Total loss: 9.621 | Reg loss: 0.009 | Tree loss: 9.621 | Accuracy: 0.000000 | 4.601 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 028 | Total loss: 9.618 | Reg loss: 0.008 | Tree loss: 9.618 | Accuracy: 0.001953 | 4.749 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 028 | Total loss: 9.614 | Reg loss: 0.007 | Tree loss: 9.614 | Accuracy: 0.001953 | 5.092 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 028 | Total loss: 9.612 | Reg loss: 0.007 | Tree loss: 9.612 | Accuracy: 0.003906 | 5.352 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 028 | Total loss: 9.610 | Reg loss: 0.006 | Tree loss: 9.610 | Accuracy: 0.003906 | 5.553 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 028 | Total loss: 9.606 | Reg loss: 0.006 | Tree loss: 9.606 | Accuracy: 0.021484 | 5.722 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 028 | Total loss: 9.603 | Reg loss: 0.006 | Tree loss: 9.603 | Accuracy: 0.011719 | 5.859 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 028 | Total loss: 9.601 | Reg loss: 0.006 | Tree loss: 9.601 | Accuracy: 0.027344 | 5.977 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 028 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.070312 | 6.086 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 028 | Total loss: 9.598 | Reg loss: 0.006 | Tree loss: 9.598 | Accuracy: 0.060547 | 6.167 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 028 | Total loss: 9.594 | Reg loss: 0.006 | Tree loss: 9.594 | Accuracy: 0.097656 | 6.234 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 028 | Total loss: 9.592 | Reg loss: 0.006 | Tree loss: 9.592 | Accuracy: 0.117188 | 6.291 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 028 | Total loss: 9.589 | Reg loss: 0.006 | Tree loss: 9.589 | Accuracy: 0.136719 | 6.338 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 028 | Total loss: 9.587 | Reg loss: 0.006 | Tree loss: 9.587 | Accuracy: 0.183594 | 6.38 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 028 | Total loss: 9.584 | Reg loss: 0.007 | Tree loss: 9.584 | Accuracy: 0.173828 | 6.42 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 028 | Total loss: 9.582 | Reg loss: 0.007 | Tree loss: 9.582 | Accuracy: 0.191406 | 6.454 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 028 | Total loss: 9.582 | Reg loss: 0.007 | Tree loss: 9.582 | Accuracy: 0.167969 | 6.481 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 028 | Total loss: 9.578 | Reg loss: 0.007 | Tree loss: 9.578 | Accuracy: 0.179688 | 6.499 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 028 | Total loss: 9.576 | Reg loss: 0.007 | Tree loss: 9.576 | Accuracy: 0.197266 | 6.512 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 028 | Total loss: 9.576 | Reg loss: 0.007 | Tree loss: 9.576 | Accuracy: 0.195312 | 6.523 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 028 | Total loss: 9.573 | Reg loss: 0.008 | Tree loss: 9.573 | Accuracy: 0.214844 | 6.534 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 028 | Total loss: 9.570 | Reg loss: 0.008 | Tree loss: 9.570 | Accuracy: 0.207031 | 6.552 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 028 | Total loss: 9.577 | Reg loss: 0.008 | Tree loss: 9.577 | Accuracy: 0.125000 | 6.444 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 028 | Total loss: 9.587 | Reg loss: 0.004 | Tree loss: 9.587 | Accuracy: 0.216797 | 6.987 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 028 | Total loss: 9.586 | Reg loss: 0.004 | Tree loss: 9.586 | Accuracy: 0.169922 | 6.933 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 028 | Total loss: 9.584 | Reg loss: 0.004 | Tree loss: 9.584 | Accuracy: 0.189453 | 6.923 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 028 | Total loss: 9.582 | Reg loss: 0.004 | Tree loss: 9.582 | Accuracy: 0.230469 | 6.917 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 028 | Total loss: 9.580 | Reg loss: 0.005 | Tree loss: 9.580 | Accuracy: 0.212891 | 6.908 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 028 | Total loss: 9.576 | Reg loss: 0.005 | Tree loss: 9.576 | Accuracy: 0.216797 | 6.886 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 028 | Total loss: 9.576 | Reg loss: 0.005 | Tree loss: 9.576 | Accuracy: 0.210938 | 6.826 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 028 | Total loss: 9.573 | Reg loss: 0.005 | Tree loss: 9.573 | Accuracy: 0.224609 | 6.767 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 028 | Total loss: 9.572 | Reg loss: 0.005 | Tree loss: 9.572 | Accuracy: 0.197266 | 6.717 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 028 | Total loss: 9.569 | Reg loss: 0.006 | Tree loss: 9.569 | Accuracy: 0.232422 | 6.665 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 028 | Total loss: 9.569 | Reg loss: 0.006 | Tree loss: 9.569 | Accuracy: 0.169922 | 6.616 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 028 | Total loss: 9.566 | Reg loss: 0.006 | Tree loss: 9.566 | Accuracy: 0.205078 | 6.569 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 028 | Total loss: 9.564 | Reg loss: 0.006 | Tree loss: 9.564 | Accuracy: 0.199219 | 6.525 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 028 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.181641 | 6.484 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 028 | Total loss: 9.559 | Reg loss: 0.007 | Tree loss: 9.559 | Accuracy: 0.203125 | 6.446 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 028 | Total loss: 9.557 | Reg loss: 0.007 | Tree loss: 9.557 | Accuracy: 0.199219 | 6.464 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 028 | Total loss: 9.557 | Reg loss: 0.007 | Tree loss: 9.557 | Accuracy: 0.193359 | 6.482 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 028 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.203125 | 6.5 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 028 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.197266 | 6.512 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 028 | Total loss: 9.555 | Reg loss: 0.008 | Tree loss: 9.555 | Accuracy: 0.167969 | 6.527 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 028 | Total loss: 9.551 | Reg loss: 0.008 | Tree loss: 9.551 | Accuracy: 0.210938 | 6.541 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 028 | Total loss: 9.551 | Reg loss: 0.008 | Tree loss: 9.551 | Accuracy: 0.179688 | 6.551 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 028 | Total loss: 9.549 | Reg loss: 0.008 | Tree loss: 9.549 | Accuracy: 0.197266 | 6.564 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 028 | Total loss: 9.549 | Reg loss: 0.008 | Tree loss: 9.549 | Accuracy: 0.154297 | 6.577 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 028 | Total loss: 9.548 | Reg loss: 0.009 | Tree loss: 9.548 | Accuracy: 0.181641 | 6.586 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 028 | Total loss: 9.544 | Reg loss: 0.009 | Tree loss: 9.544 | Accuracy: 0.207031 | 6.594 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 028 | Total loss: 9.543 | Reg loss: 0.009 | Tree loss: 9.543 | Accuracy: 0.205078 | 6.602 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 028 | Total loss: 9.533 | Reg loss: 0.009 | Tree loss: 9.533 | Accuracy: 0.437500 | 6.541 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 028 | Total loss: 9.564 | Reg loss: 0.006 | Tree loss: 9.564 | Accuracy: 0.208984 | 6.856 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 028 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.179688 | 6.826 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 028 | Total loss: 9.560 | Reg loss: 0.006 | Tree loss: 9.560 | Accuracy: 0.214844 | 6.831 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 028 | Total loss: 9.558 | Reg loss: 0.007 | Tree loss: 9.558 | Accuracy: 0.207031 | 6.834 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 004 / 028 | Total loss: 9.558 | Reg loss: 0.007 | Tree loss: 9.558 | Accuracy: 0.171875 | 6.834 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 028 | Total loss: 9.555 | Reg loss: 0.007 | Tree loss: 9.555 | Accuracy: 0.199219 | 6.833 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 028 | Total loss: 9.556 | Reg loss: 0.007 | Tree loss: 9.556 | Accuracy: 0.169922 | 6.83 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 028 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.179688 | 6.828 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 028 | Total loss: 9.551 | Reg loss: 0.007 | Tree loss: 9.551 | Accuracy: 0.175781 | 6.828 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 028 | Total loss: 9.546 | Reg loss: 0.008 | Tree loss: 9.546 | Accuracy: 0.228516 | 6.826 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 028 | Total loss: 9.544 | Reg loss: 0.008 | Tree loss: 9.544 | Accuracy: 0.220703 | 6.827 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 028 | Total loss: 9.540 | Reg loss: 0.008 | Tree loss: 9.540 | Accuracy: 0.232422 | 6.825 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 028 | Total loss: 9.541 | Reg loss: 0.009 | Tree loss: 9.541 | Accuracy: 0.183594 | 6.824 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 028 | Total loss: 9.538 | Reg loss: 0.009 | Tree loss: 9.538 | Accuracy: 0.214844 | 6.801 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 028 | Total loss: 9.536 | Reg loss: 0.009 | Tree loss: 9.536 | Accuracy: 0.205078 | 6.774 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 028 | Total loss: 9.532 | Reg loss: 0.009 | Tree loss: 9.532 | Accuracy: 0.208984 | 6.746 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 028 | Total loss: 9.530 | Reg loss: 0.010 | Tree loss: 9.530 | Accuracy: 0.207031 | 6.719 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 028 | Total loss: 9.531 | Reg loss: 0.010 | Tree loss: 9.531 | Accuracy: 0.164062 | 6.692 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 028 | Total loss: 9.523 | Reg loss: 0.010 | Tree loss: 9.523 | Accuracy: 0.216797 | 6.668 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 028 | Total loss: 9.526 | Reg loss: 0.011 | Tree loss: 9.526 | Accuracy: 0.158203 | 6.642 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 028 | Total loss: 9.520 | Reg loss: 0.011 | Tree loss: 9.520 | Accuracy: 0.212891 | 6.62 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 028 | Total loss: 9.516 | Reg loss: 0.011 | Tree loss: 9.516 | Accuracy: 0.203125 | 6.596 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 028 | Total loss: 9.514 | Reg loss: 0.012 | Tree loss: 9.514 | Accuracy: 0.214844 | 6.599 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 028 | Total loss: 9.515 | Reg loss: 0.012 | Tree loss: 9.515 | Accuracy: 0.207031 | 6.603 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 028 | Total loss: 9.505 | Reg loss: 0.012 | Tree loss: 9.505 | Accuracy: 0.193359 | 6.607 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 028 | Total loss: 9.504 | Reg loss: 0.013 | Tree loss: 9.504 | Accuracy: 0.181641 | 6.612 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 028 | Total loss: 9.501 | Reg loss: 0.013 | Tree loss: 9.501 | Accuracy: 0.191406 | 6.615 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 028 | Total loss: 9.470 | Reg loss: 0.013 | Tree loss: 9.470 | Accuracy: 0.187500 | 6.574 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 028 | Total loss: 9.541 | Reg loss: 0.009 | Tree loss: 9.541 | Accuracy: 0.179688 | 6.809 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 028 | Total loss: 9.540 | Reg loss: 0.009 | Tree loss: 9.540 | Accuracy: 0.195312 | 6.788 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 028 | Total loss: 9.535 | Reg loss: 0.009 | Tree loss: 9.535 | Accuracy: 0.216797 | 6.788 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 028 | Total loss: 9.533 | Reg loss: 0.009 | Tree loss: 9.533 | Accuracy: 0.185547 | 6.786 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 028 | Total loss: 9.528 | Reg loss: 0.009 | Tree loss: 9.528 | Accuracy: 0.222656 | 6.784 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 028 | Total loss: 9.528 | Reg loss: 0.010 | Tree loss: 9.528 | Accuracy: 0.179688 | 6.783 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 028 | Total loss: 9.524 | Reg loss: 0.010 | Tree loss: 9.524 | Accuracy: 0.154297 | 6.781 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 028 | Total loss: 9.521 | Reg loss: 0.010 | Tree loss: 9.521 | Accuracy: 0.169922 | 6.781 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 028 | Total loss: 9.515 | Reg loss: 0.010 | Tree loss: 9.515 | Accuracy: 0.197266 | 6.78 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 028 | Total loss: 9.510 | Reg loss: 0.011 | Tree loss: 9.510 | Accuracy: 0.210938 | 6.781 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 028 | Total loss: 9.510 | Reg loss: 0.011 | Tree loss: 9.510 | Accuracy: 0.183594 | 6.783 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 028 | Total loss: 9.502 | Reg loss: 0.011 | Tree loss: 9.502 | Accuracy: 0.216797 | 6.786 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 028 | Total loss: 9.500 | Reg loss: 0.012 | Tree loss: 9.500 | Accuracy: 0.177734 | 6.79 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 028 | Total loss: 9.490 | Reg loss: 0.012 | Tree loss: 9.490 | Accuracy: 0.216797 | 6.792 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 028 | Total loss: 9.490 | Reg loss: 0.012 | Tree loss: 9.490 | Accuracy: 0.210938 | 6.796 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 028 | Total loss: 9.482 | Reg loss: 0.013 | Tree loss: 9.482 | Accuracy: 0.203125 | 6.799 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 028 | Total loss: 9.481 | Reg loss: 0.013 | Tree loss: 9.481 | Accuracy: 0.205078 | 6.802 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 028 | Total loss: 9.469 | Reg loss: 0.014 | Tree loss: 9.469 | Accuracy: 0.191406 | 6.806 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 028 | Total loss: 9.471 | Reg loss: 0.014 | Tree loss: 9.471 | Accuracy: 0.205078 | 6.808 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 028 | Total loss: 9.461 | Reg loss: 0.014 | Tree loss: 9.461 | Accuracy: 0.199219 | 6.81 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 028 | Total loss: 9.454 | Reg loss: 0.015 | Tree loss: 9.454 | Accuracy: 0.199219 | 6.791 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 028 | Total loss: 9.449 | Reg loss: 0.015 | Tree loss: 9.449 | Accuracy: 0.199219 | 6.772 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 028 | Total loss: 9.439 | Reg loss: 0.016 | Tree loss: 9.439 | Accuracy: 0.181641 | 6.755 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 028 | Total loss: 9.432 | Reg loss: 0.016 | Tree loss: 9.432 | Accuracy: 0.195312 | 6.738 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 028 | Total loss: 9.426 | Reg loss: 0.016 | Tree loss: 9.426 | Accuracy: 0.205078 | 6.72 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 028 | Total loss: 9.413 | Reg loss: 0.017 | Tree loss: 9.413 | Accuracy: 0.203125 | 6.702 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 028 | Total loss: 9.405 | Reg loss: 0.017 | Tree loss: 9.405 | Accuracy: 0.166016 | 6.685 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 028 | Total loss: 9.419 | Reg loss: 0.017 | Tree loss: 9.419 | Accuracy: 0.125000 | 6.653 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 028 | Total loss: 9.499 | Reg loss: 0.011 | Tree loss: 9.499 | Accuracy: 0.201172 | 6.789 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 028 | Total loss: 9.493 | Reg loss: 0.011 | Tree loss: 9.493 | Accuracy: 0.162109 | 6.775 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 028 | Total loss: 9.491 | Reg loss: 0.011 | Tree loss: 9.491 | Accuracy: 0.189453 | 6.76 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 028 | Total loss: 9.481 | Reg loss: 0.012 | Tree loss: 9.481 | Accuracy: 0.210938 | 6.746 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 028 | Total loss: 9.478 | Reg loss: 0.012 | Tree loss: 9.478 | Accuracy: 0.187500 | 6.751 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 028 | Total loss: 9.472 | Reg loss: 0.012 | Tree loss: 9.472 | Accuracy: 0.191406 | 6.756 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 028 | Total loss: 9.466 | Reg loss: 0.012 | Tree loss: 9.466 | Accuracy: 0.226562 | 6.76 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 028 | Total loss: 9.457 | Reg loss: 0.013 | Tree loss: 9.457 | Accuracy: 0.183594 | 6.765 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 028 | Total loss: 9.444 | Reg loss: 0.013 | Tree loss: 9.444 | Accuracy: 0.208984 | 6.769 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 009 / 028 | Total loss: 9.444 | Reg loss: 0.013 | Tree loss: 9.444 | Accuracy: 0.222656 | 6.774 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 028 | Total loss: 9.435 | Reg loss: 0.013 | Tree loss: 9.435 | Accuracy: 0.205078 | 6.777 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 028 | Total loss: 9.427 | Reg loss: 0.014 | Tree loss: 9.427 | Accuracy: 0.177734 | 6.78 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 028 | Total loss: 9.421 | Reg loss: 0.014 | Tree loss: 9.421 | Accuracy: 0.203125 | 6.783 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 028 | Total loss: 9.416 | Reg loss: 0.015 | Tree loss: 9.416 | Accuracy: 0.177734 | 6.786 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 028 | Total loss: 9.403 | Reg loss: 0.015 | Tree loss: 9.403 | Accuracy: 0.205078 | 6.789 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 028 | Total loss: 9.400 | Reg loss: 0.015 | Tree loss: 9.400 | Accuracy: 0.183594 | 6.791 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 028 | Total loss: 9.377 | Reg loss: 0.016 | Tree loss: 9.377 | Accuracy: 0.169922 | 6.795 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 028 | Total loss: 9.367 | Reg loss: 0.016 | Tree loss: 9.367 | Accuracy: 0.177734 | 6.798 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 028 | Total loss: 9.366 | Reg loss: 0.017 | Tree loss: 9.366 | Accuracy: 0.185547 | 6.8 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 028 | Total loss: 9.351 | Reg loss: 0.017 | Tree loss: 9.351 | Accuracy: 0.173828 | 6.803 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 028 | Total loss: 9.330 | Reg loss: 0.017 | Tree loss: 9.330 | Accuracy: 0.177734 | 6.805 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 028 | Total loss: 9.318 | Reg loss: 0.018 | Tree loss: 9.318 | Accuracy: 0.158203 | 6.807 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 028 | Total loss: 9.299 | Reg loss: 0.018 | Tree loss: 9.299 | Accuracy: 0.201172 | 6.807 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 028 | Total loss: 9.290 | Reg loss: 0.019 | Tree loss: 9.290 | Accuracy: 0.212891 | 6.807 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 028 | Total loss: 9.278 | Reg loss: 0.019 | Tree loss: 9.278 | Accuracy: 0.175781 | 6.807 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 028 | Total loss: 9.268 | Reg loss: 0.019 | Tree loss: 9.268 | Accuracy: 0.197266 | 6.803 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 028 | Total loss: 9.250 | Reg loss: 0.020 | Tree loss: 9.250 | Accuracy: 0.207031 | 6.79 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 028 | Total loss: 9.198 | Reg loss: 0.020 | Tree loss: 9.198 | Accuracy: 0.250000 | 6.764 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 028 | Total loss: 9.427 | Reg loss: 0.014 | Tree loss: 9.427 | Accuracy: 0.187500 | 6.858 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 028 | Total loss: 9.418 | Reg loss: 0.014 | Tree loss: 9.418 | Accuracy: 0.171875 | 6.861 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 028 | Total loss: 9.406 | Reg loss: 0.014 | Tree loss: 9.406 | Accuracy: 0.195312 | 6.849 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 028 | Total loss: 9.387 | Reg loss: 0.014 | Tree loss: 9.387 | Accuracy: 0.189453 | 6.837 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 028 | Total loss: 9.384 | Reg loss: 0.014 | Tree loss: 9.384 | Accuracy: 0.171875 | 6.824 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 028 | Total loss: 9.379 | Reg loss: 0.014 | Tree loss: 9.379 | Accuracy: 0.189453 | 6.81 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 028 | Total loss: 9.350 | Reg loss: 0.015 | Tree loss: 9.350 | Accuracy: 0.208984 | 6.797 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 028 | Total loss: 9.338 | Reg loss: 0.015 | Tree loss: 9.338 | Accuracy: 0.216797 | 6.787 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 028 | Total loss: 9.328 | Reg loss: 0.015 | Tree loss: 9.328 | Accuracy: 0.201172 | 6.786 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 028 | Total loss: 9.317 | Reg loss: 0.016 | Tree loss: 9.317 | Accuracy: 0.162109 | 6.788 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 028 | Total loss: 9.310 | Reg loss: 0.016 | Tree loss: 9.310 | Accuracy: 0.162109 | 6.79 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 028 | Total loss: 9.284 | Reg loss: 0.016 | Tree loss: 9.284 | Accuracy: 0.185547 | 6.792 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 028 | Total loss: 9.277 | Reg loss: 0.017 | Tree loss: 9.277 | Accuracy: 0.167969 | 6.795 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 028 | Total loss: 9.255 | Reg loss: 0.017 | Tree loss: 9.255 | Accuracy: 0.183594 | 6.798 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 028 | Total loss: 9.241 | Reg loss: 0.018 | Tree loss: 9.241 | Accuracy: 0.195312 | 6.801 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 028 | Total loss: 9.223 | Reg loss: 0.018 | Tree loss: 9.223 | Accuracy: 0.191406 | 6.804 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 028 | Total loss: 9.226 | Reg loss: 0.019 | Tree loss: 9.226 | Accuracy: 0.199219 | 6.807 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 028 | Total loss: 9.200 | Reg loss: 0.019 | Tree loss: 9.200 | Accuracy: 0.167969 | 6.809 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 028 | Total loss: 9.172 | Reg loss: 0.020 | Tree loss: 9.172 | Accuracy: 0.183594 | 6.812 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 028 | Total loss: 9.161 | Reg loss: 0.020 | Tree loss: 9.161 | Accuracy: 0.175781 | 6.814 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 028 | Total loss: 9.148 | Reg loss: 0.021 | Tree loss: 9.148 | Accuracy: 0.189453 | 6.817 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 028 | Total loss: 9.126 | Reg loss: 0.021 | Tree loss: 9.126 | Accuracy: 0.177734 | 6.82 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 028 | Total loss: 9.101 | Reg loss: 0.022 | Tree loss: 9.101 | Accuracy: 0.197266 | 6.822 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 028 | Total loss: 9.095 | Reg loss: 0.022 | Tree loss: 9.095 | Accuracy: 0.160156 | 6.825 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 028 | Total loss: 9.076 | Reg loss: 0.023 | Tree loss: 9.076 | Accuracy: 0.199219 | 6.827 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 028 | Total loss: 9.060 | Reg loss: 0.023 | Tree loss: 9.060 | Accuracy: 0.175781 | 6.828 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 028 | Total loss: 9.033 | Reg loss: 0.024 | Tree loss: 9.033 | Accuracy: 0.191406 | 6.829 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 028 | Total loss: 9.009 | Reg loss: 0.024 | Tree loss: 9.009 | Accuracy: 0.187500 | 6.807 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 028 | Total loss: 9.274 | Reg loss: 0.016 | Tree loss: 9.274 | Accuracy: 0.183594 | 6.866 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 028 | Total loss: 9.266 | Reg loss: 0.016 | Tree loss: 9.266 | Accuracy: 0.210938 | 6.867 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 028 | Total loss: 9.244 | Reg loss: 0.017 | Tree loss: 9.244 | Accuracy: 0.201172 | 6.867 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 028 | Total loss: 9.212 | Reg loss: 0.017 | Tree loss: 9.212 | Accuracy: 0.226562 | 6.868 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 028 | Total loss: 9.226 | Reg loss: 0.017 | Tree loss: 9.226 | Accuracy: 0.158203 | 6.868 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 028 | Total loss: 9.208 | Reg loss: 0.017 | Tree loss: 9.208 | Accuracy: 0.187500 | 6.87 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 028 | Total loss: 9.178 | Reg loss: 0.018 | Tree loss: 9.178 | Accuracy: 0.173828 | 6.868 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 028 | Total loss: 9.167 | Reg loss: 0.018 | Tree loss: 9.167 | Accuracy: 0.171875 | 6.857 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 028 | Total loss: 9.146 | Reg loss: 0.018 | Tree loss: 9.146 | Accuracy: 0.187500 | 6.845 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 028 | Total loss: 9.136 | Reg loss: 0.019 | Tree loss: 9.136 | Accuracy: 0.203125 | 6.834 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 028 | Total loss: 9.117 | Reg loss: 0.019 | Tree loss: 9.117 | Accuracy: 0.173828 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 028 | Total loss: 9.085 | Reg loss: 0.019 | Tree loss: 9.085 | Accuracy: 0.183594 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 028 | Total loss: 9.067 | Reg loss: 0.020 | Tree loss: 9.067 | Accuracy: 0.195312 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 028 | Total loss: 9.055 | Reg loss: 0.020 | Tree loss: 9.055 | Accuracy: 0.175781 | 6.824 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 014 / 028 | Total loss: 9.040 | Reg loss: 0.021 | Tree loss: 9.040 | Accuracy: 0.156250 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 028 | Total loss: 9.006 | Reg loss: 0.021 | Tree loss: 9.006 | Accuracy: 0.162109 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 028 | Total loss: 8.982 | Reg loss: 0.022 | Tree loss: 8.982 | Accuracy: 0.210938 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 028 | Total loss: 8.968 | Reg loss: 0.022 | Tree loss: 8.968 | Accuracy: 0.171875 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 028 | Total loss: 8.960 | Reg loss: 0.023 | Tree loss: 8.960 | Accuracy: 0.169922 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 028 | Total loss: 8.906 | Reg loss: 0.023 | Tree loss: 8.906 | Accuracy: 0.167969 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 028 | Total loss: 8.885 | Reg loss: 0.024 | Tree loss: 8.885 | Accuracy: 0.195312 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 028 | Total loss: 8.881 | Reg loss: 0.024 | Tree loss: 8.881 | Accuracy: 0.208984 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 028 | Total loss: 8.871 | Reg loss: 0.024 | Tree loss: 8.871 | Accuracy: 0.189453 | 6.823 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 028 | Total loss: 8.828 | Reg loss: 0.025 | Tree loss: 8.828 | Accuracy: 0.187500 | 6.824 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 028 | Total loss: 8.802 | Reg loss: 0.025 | Tree loss: 8.802 | Accuracy: 0.197266 | 6.825 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 028 | Total loss: 8.796 | Reg loss: 0.026 | Tree loss: 8.796 | Accuracy: 0.210938 | 6.827 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 028 | Total loss: 8.768 | Reg loss: 0.026 | Tree loss: 8.768 | Accuracy: 0.154297 | 6.829 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 028 | Total loss: 8.779 | Reg loss: 0.027 | Tree loss: 8.779 | Accuracy: 0.187500 | 6.81 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 028 | Total loss: 9.091 | Reg loss: 0.019 | Tree loss: 9.091 | Accuracy: 0.171875 | 6.829 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 028 | Total loss: 9.086 | Reg loss: 0.019 | Tree loss: 9.086 | Accuracy: 0.197266 | 6.833 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 028 | Total loss: 9.023 | Reg loss: 0.019 | Tree loss: 9.023 | Accuracy: 0.201172 | 6.835 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 028 | Total loss: 9.018 | Reg loss: 0.019 | Tree loss: 9.018 | Accuracy: 0.171875 | 6.838 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 028 | Total loss: 8.971 | Reg loss: 0.020 | Tree loss: 8.971 | Accuracy: 0.193359 | 6.84 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 028 | Total loss: 8.983 | Reg loss: 0.020 | Tree loss: 8.983 | Accuracy: 0.173828 | 6.842 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 028 | Total loss: 8.966 | Reg loss: 0.020 | Tree loss: 8.966 | Accuracy: 0.205078 | 6.844 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 028 | Total loss: 8.948 | Reg loss: 0.020 | Tree loss: 8.948 | Accuracy: 0.191406 | 6.846 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 028 | Total loss: 8.922 | Reg loss: 0.021 | Tree loss: 8.922 | Accuracy: 0.208984 | 6.848 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 028 | Total loss: 8.893 | Reg loss: 0.021 | Tree loss: 8.893 | Accuracy: 0.183594 | 6.85 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 028 | Total loss: 8.857 | Reg loss: 0.021 | Tree loss: 8.857 | Accuracy: 0.181641 | 6.852 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 028 | Total loss: 8.855 | Reg loss: 0.022 | Tree loss: 8.855 | Accuracy: 0.185547 | 6.844 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 028 | Total loss: 8.826 | Reg loss: 0.022 | Tree loss: 8.826 | Accuracy: 0.169922 | 6.836 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 028 | Total loss: 8.799 | Reg loss: 0.022 | Tree loss: 8.799 | Accuracy: 0.187500 | 6.827 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 028 | Total loss: 8.777 | Reg loss: 0.023 | Tree loss: 8.777 | Accuracy: 0.189453 | 6.817 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 028 | Total loss: 8.754 | Reg loss: 0.023 | Tree loss: 8.754 | Accuracy: 0.166016 | 6.808 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 028 | Total loss: 8.718 | Reg loss: 0.024 | Tree loss: 8.718 | Accuracy: 0.185547 | 6.806 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 028 | Total loss: 8.692 | Reg loss: 0.024 | Tree loss: 8.692 | Accuracy: 0.203125 | 6.807 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 028 | Total loss: 8.690 | Reg loss: 0.024 | Tree loss: 8.690 | Accuracy: 0.185547 | 6.808 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 028 | Total loss: 8.639 | Reg loss: 0.025 | Tree loss: 8.639 | Accuracy: 0.179688 | 6.809 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 028 | Total loss: 8.609 | Reg loss: 0.025 | Tree loss: 8.609 | Accuracy: 0.193359 | 6.81 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 028 | Total loss: 8.604 | Reg loss: 0.026 | Tree loss: 8.604 | Accuracy: 0.171875 | 6.811 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 028 | Total loss: 8.548 | Reg loss: 0.026 | Tree loss: 8.548 | Accuracy: 0.226562 | 6.812 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 028 | Total loss: 8.526 | Reg loss: 0.026 | Tree loss: 8.526 | Accuracy: 0.169922 | 6.814 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 028 | Total loss: 8.546 | Reg loss: 0.027 | Tree loss: 8.546 | Accuracy: 0.193359 | 6.815 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 028 | Total loss: 8.515 | Reg loss: 0.027 | Tree loss: 8.515 | Accuracy: 0.207031 | 6.817 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 028 | Total loss: 8.508 | Reg loss: 0.028 | Tree loss: 8.508 | Accuracy: 0.169922 | 6.818 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 028 | Total loss: 8.449 | Reg loss: 0.028 | Tree loss: 8.449 | Accuracy: 0.187500 | 6.801 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 028 | Total loss: 8.839 | Reg loss: 0.021 | Tree loss: 8.839 | Accuracy: 0.208984 | 6.855 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 028 | Total loss: 8.812 | Reg loss: 0.021 | Tree loss: 8.812 | Accuracy: 0.171875 | 6.857 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 028 | Total loss: 8.805 | Reg loss: 0.022 | Tree loss: 8.805 | Accuracy: 0.197266 | 6.859 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 028 | Total loss: 8.765 | Reg loss: 0.022 | Tree loss: 8.765 | Accuracy: 0.189453 | 6.861 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 028 | Total loss: 8.750 | Reg loss: 0.022 | Tree loss: 8.750 | Accuracy: 0.187500 | 6.863 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 028 | Total loss: 8.727 | Reg loss: 0.022 | Tree loss: 8.727 | Accuracy: 0.220703 | 6.864 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 028 | Total loss: 8.715 | Reg loss: 0.022 | Tree loss: 8.715 | Accuracy: 0.173828 | 6.867 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 028 | Total loss: 8.636 | Reg loss: 0.022 | Tree loss: 8.636 | Accuracy: 0.148438 | 6.869 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 028 | Total loss: 8.641 | Reg loss: 0.023 | Tree loss: 8.641 | Accuracy: 0.185547 | 6.872 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 028 | Total loss: 8.616 | Reg loss: 0.023 | Tree loss: 8.616 | Accuracy: 0.208984 | 6.873 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 028 | Total loss: 8.633 | Reg loss: 0.023 | Tree loss: 8.633 | Accuracy: 0.160156 | 6.875 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 028 | Total loss: 8.586 | Reg loss: 0.023 | Tree loss: 8.586 | Accuracy: 0.187500 | 6.876 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 028 | Total loss: 8.553 | Reg loss: 0.024 | Tree loss: 8.553 | Accuracy: 0.199219 | 6.875 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 028 | Total loss: 8.538 | Reg loss: 0.024 | Tree loss: 8.538 | Accuracy: 0.171875 | 6.874 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 028 | Total loss: 8.487 | Reg loss: 0.024 | Tree loss: 8.487 | Accuracy: 0.189453 | 6.869 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 028 | Total loss: 8.446 | Reg loss: 0.025 | Tree loss: 8.446 | Accuracy: 0.205078 | 6.86 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 028 | Total loss: 8.442 | Reg loss: 0.025 | Tree loss: 8.442 | Accuracy: 0.171875 | 6.852 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 028 | Total loss: 8.422 | Reg loss: 0.025 | Tree loss: 8.422 | Accuracy: 0.201172 | 6.844 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 028 | Total loss: 8.374 | Reg loss: 0.026 | Tree loss: 8.374 | Accuracy: 0.195312 | 6.835 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 019 / 028 | Total loss: 8.379 | Reg loss: 0.026 | Tree loss: 8.379 | Accuracy: 0.195312 | 6.832 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 028 | Total loss: 8.336 | Reg loss: 0.027 | Tree loss: 8.336 | Accuracy: 0.201172 | 6.831 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 028 | Total loss: 8.329 | Reg loss: 0.027 | Tree loss: 8.329 | Accuracy: 0.173828 | 6.83 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 028 | Total loss: 8.321 | Reg loss: 0.027 | Tree loss: 8.321 | Accuracy: 0.179688 | 6.83 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 028 | Total loss: 8.288 | Reg loss: 0.028 | Tree loss: 8.288 | Accuracy: 0.181641 | 6.829 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 028 | Total loss: 8.234 | Reg loss: 0.028 | Tree loss: 8.234 | Accuracy: 0.179688 | 6.829 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 028 | Total loss: 8.216 | Reg loss: 0.028 | Tree loss: 8.216 | Accuracy: 0.175781 | 6.83 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 028 | Total loss: 8.206 | Reg loss: 0.029 | Tree loss: 8.206 | Accuracy: 0.197266 | 6.831 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 028 | Total loss: 8.273 | Reg loss: 0.029 | Tree loss: 8.273 | Accuracy: 0.125000 | 6.817 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 028 | Total loss: 8.555 | Reg loss: 0.023 | Tree loss: 8.555 | Accuracy: 0.183594 | 6.866 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 028 | Total loss: 8.567 | Reg loss: 0.023 | Tree loss: 8.567 | Accuracy: 0.179688 | 6.861 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 028 | Total loss: 8.531 | Reg loss: 0.023 | Tree loss: 8.531 | Accuracy: 0.212891 | 6.855 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 028 | Total loss: 8.535 | Reg loss: 0.024 | Tree loss: 8.535 | Accuracy: 0.201172 | 6.849 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 028 | Total loss: 8.510 | Reg loss: 0.024 | Tree loss: 8.510 | Accuracy: 0.191406 | 6.851 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 028 | Total loss: 8.477 | Reg loss: 0.024 | Tree loss: 8.477 | Accuracy: 0.189453 | 6.852 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 028 | Total loss: 8.437 | Reg loss: 0.024 | Tree loss: 8.437 | Accuracy: 0.189453 | 6.854 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 028 | Total loss: 8.384 | Reg loss: 0.024 | Tree loss: 8.384 | Accuracy: 0.183594 | 6.856 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 028 | Total loss: 8.377 | Reg loss: 0.024 | Tree loss: 8.377 | Accuracy: 0.216797 | 6.857 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 028 | Total loss: 8.348 | Reg loss: 0.025 | Tree loss: 8.348 | Accuracy: 0.234375 | 6.858 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 028 | Total loss: 8.362 | Reg loss: 0.025 | Tree loss: 8.362 | Accuracy: 0.167969 | 6.86 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 028 | Total loss: 8.291 | Reg loss: 0.025 | Tree loss: 8.291 | Accuracy: 0.175781 | 6.861 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 028 | Total loss: 8.240 | Reg loss: 0.025 | Tree loss: 8.240 | Accuracy: 0.185547 | 6.861 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 028 | Total loss: 8.241 | Reg loss: 0.026 | Tree loss: 8.241 | Accuracy: 0.144531 | 6.862 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 028 | Total loss: 8.232 | Reg loss: 0.026 | Tree loss: 8.232 | Accuracy: 0.173828 | 6.863 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 028 | Total loss: 8.173 | Reg loss: 0.026 | Tree loss: 8.173 | Accuracy: 0.183594 | 6.864 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 028 | Total loss: 8.155 | Reg loss: 0.026 | Tree loss: 8.155 | Accuracy: 0.189453 | 6.864 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 028 | Total loss: 8.133 | Reg loss: 0.027 | Tree loss: 8.133 | Accuracy: 0.201172 | 6.865 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 028 | Total loss: 8.106 | Reg loss: 0.027 | Tree loss: 8.106 | Accuracy: 0.181641 | 6.86 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 028 | Total loss: 8.091 | Reg loss: 0.027 | Tree loss: 8.091 | Accuracy: 0.152344 | 6.854 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 028 | Total loss: 8.083 | Reg loss: 0.028 | Tree loss: 8.083 | Accuracy: 0.191406 | 6.848 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 028 | Total loss: 8.014 | Reg loss: 0.028 | Tree loss: 8.014 | Accuracy: 0.187500 | 6.844 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 028 | Total loss: 8.004 | Reg loss: 0.028 | Tree loss: 8.004 | Accuracy: 0.189453 | 6.844 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 028 | Total loss: 7.981 | Reg loss: 0.028 | Tree loss: 7.981 | Accuracy: 0.197266 | 6.844 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 028 | Total loss: 7.988 | Reg loss: 0.029 | Tree loss: 7.988 | Accuracy: 0.183594 | 6.845 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 028 | Total loss: 7.952 | Reg loss: 0.029 | Tree loss: 7.952 | Accuracy: 0.162109 | 6.845 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 028 | Total loss: 7.944 | Reg loss: 0.029 | Tree loss: 7.944 | Accuracy: 0.166016 | 6.846 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 028 | Total loss: 7.977 | Reg loss: 0.030 | Tree loss: 7.977 | Accuracy: 0.125000 | 6.833 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 028 | Total loss: 8.312 | Reg loss: 0.025 | Tree loss: 8.312 | Accuracy: 0.183594 | 6.877 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 028 | Total loss: 8.297 | Reg loss: 0.025 | Tree loss: 8.297 | Accuracy: 0.150391 | 6.877 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 028 | Total loss: 8.260 | Reg loss: 0.025 | Tree loss: 8.260 | Accuracy: 0.212891 | 6.878 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 028 | Total loss: 8.201 | Reg loss: 0.025 | Tree loss: 8.201 | Accuracy: 0.205078 | 6.874 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 028 | Total loss: 8.217 | Reg loss: 0.025 | Tree loss: 8.217 | Accuracy: 0.181641 | 6.867 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 028 | Total loss: 8.174 | Reg loss: 0.025 | Tree loss: 8.174 | Accuracy: 0.173828 | 6.86 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 028 | Total loss: 8.141 | Reg loss: 0.025 | Tree loss: 8.141 | Accuracy: 0.191406 | 6.853 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 028 | Total loss: 8.105 | Reg loss: 0.026 | Tree loss: 8.105 | Accuracy: 0.167969 | 6.846 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 028 | Total loss: 8.134 | Reg loss: 0.026 | Tree loss: 8.134 | Accuracy: 0.171875 | 6.839 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 028 | Total loss: 8.059 | Reg loss: 0.026 | Tree loss: 8.059 | Accuracy: 0.181641 | 6.832 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 028 | Total loss: 8.066 | Reg loss: 0.026 | Tree loss: 8.066 | Accuracy: 0.197266 | 6.824 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 028 | Total loss: 8.065 | Reg loss: 0.026 | Tree loss: 8.065 | Accuracy: 0.160156 | 6.818 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 028 | Total loss: 8.006 | Reg loss: 0.027 | Tree loss: 8.006 | Accuracy: 0.197266 | 6.818 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 028 | Total loss: 7.987 | Reg loss: 0.027 | Tree loss: 7.987 | Accuracy: 0.175781 | 6.819 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 028 | Total loss: 7.914 | Reg loss: 0.027 | Tree loss: 7.914 | Accuracy: 0.177734 | 6.82 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 028 | Total loss: 7.941 | Reg loss: 0.027 | Tree loss: 7.941 | Accuracy: 0.199219 | 6.821 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 028 | Total loss: 7.890 | Reg loss: 0.027 | Tree loss: 7.890 | Accuracy: 0.201172 | 6.822 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 028 | Total loss: 7.870 | Reg loss: 0.028 | Tree loss: 7.870 | Accuracy: 0.181641 | 6.824 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 028 | Total loss: 7.850 | Reg loss: 0.028 | Tree loss: 7.850 | Accuracy: 0.191406 | 6.825 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 028 | Total loss: 7.787 | Reg loss: 0.028 | Tree loss: 7.787 | Accuracy: 0.187500 | 6.826 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 028 | Total loss: 7.800 | Reg loss: 0.029 | Tree loss: 7.800 | Accuracy: 0.177734 | 6.823 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 028 | Total loss: 7.730 | Reg loss: 0.029 | Tree loss: 7.730 | Accuracy: 0.187500 | 6.818 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 028 | Total loss: 7.727 | Reg loss: 0.029 | Tree loss: 7.727 | Accuracy: 0.207031 | 6.812 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 028 | Total loss: 7.717 | Reg loss: 0.029 | Tree loss: 7.717 | Accuracy: 0.191406 | 6.805 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 024 / 028 | Total loss: 7.722 | Reg loss: 0.030 | Tree loss: 7.722 | Accuracy: 0.156250 | 6.799 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 028 | Total loss: 7.685 | Reg loss: 0.030 | Tree loss: 7.685 | Accuracy: 0.173828 | 6.801 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 028 | Total loss: 7.655 | Reg loss: 0.030 | Tree loss: 7.655 | Accuracy: 0.177734 | 6.802 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 028 | Total loss: 7.528 | Reg loss: 0.030 | Tree loss: 7.528 | Accuracy: 0.375000 | 6.791 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 028 | Total loss: 8.040 | Reg loss: 0.026 | Tree loss: 8.040 | Accuracy: 0.210938 | 6.95 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 028 | Total loss: 8.046 | Reg loss: 0.026 | Tree loss: 8.046 | Accuracy: 0.197266 | 6.95 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 028 | Total loss: 7.964 | Reg loss: 0.026 | Tree loss: 7.964 | Accuracy: 0.158203 | 6.95 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 028 | Total loss: 7.968 | Reg loss: 0.026 | Tree loss: 7.968 | Accuracy: 0.183594 | 6.944 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 028 | Total loss: 7.936 | Reg loss: 0.026 | Tree loss: 7.936 | Accuracy: 0.171875 | 6.937 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 028 | Total loss: 7.905 | Reg loss: 0.027 | Tree loss: 7.905 | Accuracy: 0.183594 | 6.931 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 028 | Total loss: 7.855 | Reg loss: 0.027 | Tree loss: 7.855 | Accuracy: 0.212891 | 6.932 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 028 | Total loss: 7.875 | Reg loss: 0.027 | Tree loss: 7.875 | Accuracy: 0.199219 | 6.934 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 028 | Total loss: 7.827 | Reg loss: 0.027 | Tree loss: 7.827 | Accuracy: 0.185547 | 6.935 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 028 | Total loss: 7.763 | Reg loss: 0.027 | Tree loss: 7.763 | Accuracy: 0.218750 | 6.936 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 028 | Total loss: 7.751 | Reg loss: 0.027 | Tree loss: 7.751 | Accuracy: 0.173828 | 6.937 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 028 | Total loss: 7.740 | Reg loss: 0.027 | Tree loss: 7.740 | Accuracy: 0.177734 | 6.938 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 028 | Total loss: 7.745 | Reg loss: 0.028 | Tree loss: 7.745 | Accuracy: 0.181641 | 6.939 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 028 | Total loss: 7.694 | Reg loss: 0.028 | Tree loss: 7.694 | Accuracy: 0.167969 | 6.939 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 028 | Total loss: 7.697 | Reg loss: 0.028 | Tree loss: 7.697 | Accuracy: 0.164062 | 6.939 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 028 | Total loss: 7.643 | Reg loss: 0.028 | Tree loss: 7.643 | Accuracy: 0.181641 | 6.94 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 028 | Total loss: 7.606 | Reg loss: 0.029 | Tree loss: 7.606 | Accuracy: 0.173828 | 6.933 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 028 | Total loss: 7.613 | Reg loss: 0.029 | Tree loss: 7.613 | Accuracy: 0.177734 | 6.927 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 028 | Total loss: 7.568 | Reg loss: 0.029 | Tree loss: 7.568 | Accuracy: 0.185547 | 6.92 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 028 | Total loss: 7.544 | Reg loss: 0.029 | Tree loss: 7.544 | Accuracy: 0.183594 | 6.914 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 028 | Total loss: 7.587 | Reg loss: 0.029 | Tree loss: 7.587 | Accuracy: 0.167969 | 6.908 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 028 | Total loss: 7.506 | Reg loss: 0.030 | Tree loss: 7.506 | Accuracy: 0.173828 | 6.902 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 028 | Total loss: 7.501 | Reg loss: 0.030 | Tree loss: 7.501 | Accuracy: 0.169922 | 6.895 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 028 | Total loss: 7.450 | Reg loss: 0.030 | Tree loss: 7.450 | Accuracy: 0.195312 | 6.89 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 028 | Total loss: 7.412 | Reg loss: 0.030 | Tree loss: 7.412 | Accuracy: 0.173828 | 6.89 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 028 | Total loss: 7.426 | Reg loss: 0.031 | Tree loss: 7.426 | Accuracy: 0.169922 | 6.892 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 028 | Total loss: 7.363 | Reg loss: 0.031 | Tree loss: 7.363 | Accuracy: 0.195312 | 6.893 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 028 | Total loss: 7.048 | Reg loss: 0.031 | Tree loss: 7.048 | Accuracy: 0.437500 | 6.882 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 028 | Total loss: 7.790 | Reg loss: 0.027 | Tree loss: 7.790 | Accuracy: 0.183594 | 6.977 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 028 | Total loss: 7.756 | Reg loss: 0.027 | Tree loss: 7.756 | Accuracy: 0.179688 | 6.971 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 028 | Total loss: 7.759 | Reg loss: 0.028 | Tree loss: 7.759 | Accuracy: 0.197266 | 6.965 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 028 | Total loss: 7.713 | Reg loss: 0.028 | Tree loss: 7.713 | Accuracy: 0.158203 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 028 | Total loss: 7.662 | Reg loss: 0.028 | Tree loss: 7.662 | Accuracy: 0.150391 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 028 | Total loss: 7.660 | Reg loss: 0.028 | Tree loss: 7.660 | Accuracy: 0.183594 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 028 | Total loss: 7.571 | Reg loss: 0.028 | Tree loss: 7.571 | Accuracy: 0.195312 | 6.957 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 028 | Total loss: 7.581 | Reg loss: 0.028 | Tree loss: 7.581 | Accuracy: 0.201172 | 6.957 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 028 | Total loss: 7.556 | Reg loss: 0.028 | Tree loss: 7.556 | Accuracy: 0.162109 | 6.957 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 028 | Total loss: 7.555 | Reg loss: 0.028 | Tree loss: 7.555 | Accuracy: 0.199219 | 6.957 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 028 | Total loss: 7.532 | Reg loss: 0.028 | Tree loss: 7.532 | Accuracy: 0.181641 | 6.956 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 028 | Total loss: 7.471 | Reg loss: 0.029 | Tree loss: 7.471 | Accuracy: 0.162109 | 6.956 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 028 | Total loss: 7.465 | Reg loss: 0.029 | Tree loss: 7.465 | Accuracy: 0.171875 | 6.956 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 028 | Total loss: 7.441 | Reg loss: 0.029 | Tree loss: 7.441 | Accuracy: 0.205078 | 6.957 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 028 | Total loss: 7.345 | Reg loss: 0.029 | Tree loss: 7.345 | Accuracy: 0.179688 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 028 | Total loss: 7.379 | Reg loss: 0.029 | Tree loss: 7.379 | Accuracy: 0.183594 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 028 | Total loss: 7.356 | Reg loss: 0.030 | Tree loss: 7.356 | Accuracy: 0.177734 | 6.958 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 028 | Total loss: 7.383 | Reg loss: 0.030 | Tree loss: 7.383 | Accuracy: 0.195312 | 6.959 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 028 | Total loss: 7.298 | Reg loss: 0.030 | Tree loss: 7.298 | Accuracy: 0.191406 | 6.956 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 028 | Total loss: 7.250 | Reg loss: 0.030 | Tree loss: 7.250 | Accuracy: 0.185547 | 6.952 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 028 | Total loss: 7.224 | Reg loss: 0.031 | Tree loss: 7.224 | Accuracy: 0.187500 | 6.946 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 028 | Total loss: 7.246 | Reg loss: 0.031 | Tree loss: 7.246 | Accuracy: 0.175781 | 6.947 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 028 | Total loss: 7.186 | Reg loss: 0.031 | Tree loss: 7.186 | Accuracy: 0.207031 | 6.947 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 028 | Total loss: 7.204 | Reg loss: 0.031 | Tree loss: 7.204 | Accuracy: 0.150391 | 6.946 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 028 | Total loss: 7.148 | Reg loss: 0.031 | Tree loss: 7.148 | Accuracy: 0.179688 | 6.945 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 028 | Total loss: 7.144 | Reg loss: 0.032 | Tree loss: 7.144 | Accuracy: 0.197266 | 6.945 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 028 | Total loss: 7.096 | Reg loss: 0.032 | Tree loss: 7.096 | Accuracy: 0.164062 | 6.944 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 028 | Total loss: 7.078 | Reg loss: 0.032 | Tree loss: 7.078 | Accuracy: 0.187500 | 6.933 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 000 / 028 | Total loss: 7.512 | Reg loss: 0.029 | Tree loss: 7.512 | Accuracy: 0.173828 | 6.964 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 028 | Total loss: 7.483 | Reg loss: 0.029 | Tree loss: 7.483 | Accuracy: 0.228516 | 6.964 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 028 | Total loss: 7.438 | Reg loss: 0.029 | Tree loss: 7.438 | Accuracy: 0.193359 | 6.964 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 028 | Total loss: 7.447 | Reg loss: 0.029 | Tree loss: 7.447 | Accuracy: 0.181641 | 6.962 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 028 | Total loss: 7.426 | Reg loss: 0.029 | Tree loss: 7.426 | Accuracy: 0.212891 | 6.96 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 028 | Total loss: 7.378 | Reg loss: 0.029 | Tree loss: 7.378 | Accuracy: 0.169922 | 6.954 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 028 | Total loss: 7.353 | Reg loss: 0.029 | Tree loss: 7.353 | Accuracy: 0.175781 | 6.949 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 028 | Total loss: 7.342 | Reg loss: 0.029 | Tree loss: 7.342 | Accuracy: 0.167969 | 6.943 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 028 | Total loss: 7.269 | Reg loss: 0.029 | Tree loss: 7.269 | Accuracy: 0.183594 | 6.937 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 028 | Total loss: 7.249 | Reg loss: 0.029 | Tree loss: 7.249 | Accuracy: 0.201172 | 6.932 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 028 | Total loss: 7.190 | Reg loss: 0.030 | Tree loss: 7.190 | Accuracy: 0.197266 | 6.933 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 028 | Total loss: 7.203 | Reg loss: 0.030 | Tree loss: 7.203 | Accuracy: 0.189453 | 6.934 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 028 | Total loss: 7.169 | Reg loss: 0.030 | Tree loss: 7.169 | Accuracy: 0.166016 | 6.935 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 028 | Total loss: 7.176 | Reg loss: 0.030 | Tree loss: 7.176 | Accuracy: 0.160156 | 6.936 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 028 | Total loss: 7.174 | Reg loss: 0.030 | Tree loss: 7.174 | Accuracy: 0.175781 | 6.937 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 028 | Total loss: 7.061 | Reg loss: 0.031 | Tree loss: 7.061 | Accuracy: 0.212891 | 6.938 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 028 | Total loss: 7.097 | Reg loss: 0.031 | Tree loss: 7.097 | Accuracy: 0.164062 | 6.938 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 028 | Total loss: 7.053 | Reg loss: 0.031 | Tree loss: 7.053 | Accuracy: 0.177734 | 6.939 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 028 | Total loss: 7.000 | Reg loss: 0.031 | Tree loss: 7.000 | Accuracy: 0.195312 | 6.939 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 028 | Total loss: 7.015 | Reg loss: 0.031 | Tree loss: 7.015 | Accuracy: 0.156250 | 6.94 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 028 | Total loss: 6.988 | Reg loss: 0.032 | Tree loss: 6.988 | Accuracy: 0.183594 | 6.94 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 028 | Total loss: 6.962 | Reg loss: 0.032 | Tree loss: 6.962 | Accuracy: 0.167969 | 6.935 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 028 | Total loss: 6.917 | Reg loss: 0.032 | Tree loss: 6.917 | Accuracy: 0.167969 | 6.93 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 028 | Total loss: 6.876 | Reg loss: 0.032 | Tree loss: 6.876 | Accuracy: 0.195312 | 6.924 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 028 | Total loss: 6.900 | Reg loss: 0.033 | Tree loss: 6.900 | Accuracy: 0.156250 | 6.919 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 028 | Total loss: 6.862 | Reg loss: 0.033 | Tree loss: 6.862 | Accuracy: 0.162109 | 6.914 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 028 | Total loss: 6.814 | Reg loss: 0.033 | Tree loss: 6.814 | Accuracy: 0.203125 | 6.908 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 028 | Total loss: 6.711 | Reg loss: 0.033 | Tree loss: 6.711 | Accuracy: 0.187500 | 6.899 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 028 | Total loss: 7.244 | Reg loss: 0.030 | Tree loss: 7.244 | Accuracy: 0.203125 | 6.926 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 028 | Total loss: 7.224 | Reg loss: 0.030 | Tree loss: 7.224 | Accuracy: 0.169922 | 6.927 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 028 | Total loss: 7.171 | Reg loss: 0.030 | Tree loss: 7.171 | Accuracy: 0.218750 | 6.928 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 028 | Total loss: 7.149 | Reg loss: 0.030 | Tree loss: 7.149 | Accuracy: 0.167969 | 6.929 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 028 | Total loss: 7.098 | Reg loss: 0.030 | Tree loss: 7.098 | Accuracy: 0.173828 | 6.93 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 028 | Total loss: 7.130 | Reg loss: 0.030 | Tree loss: 7.130 | Accuracy: 0.189453 | 6.931 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 028 | Total loss: 7.085 | Reg loss: 0.030 | Tree loss: 7.085 | Accuracy: 0.173828 | 6.932 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 028 | Total loss: 7.026 | Reg loss: 0.030 | Tree loss: 7.026 | Accuracy: 0.212891 | 6.933 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 028 | Total loss: 7.018 | Reg loss: 0.031 | Tree loss: 7.018 | Accuracy: 0.175781 | 6.934 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 028 | Total loss: 6.979 | Reg loss: 0.031 | Tree loss: 6.979 | Accuracy: 0.193359 | 6.931 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 028 | Total loss: 6.953 | Reg loss: 0.031 | Tree loss: 6.953 | Accuracy: 0.197266 | 6.927 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 028 | Total loss: 6.937 | Reg loss: 0.031 | Tree loss: 6.937 | Accuracy: 0.216797 | 6.928 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 028 | Total loss: 6.911 | Reg loss: 0.031 | Tree loss: 6.911 | Accuracy: 0.162109 | 6.927 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 028 | Total loss: 6.853 | Reg loss: 0.031 | Tree loss: 6.853 | Accuracy: 0.177734 | 6.927 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 028 | Total loss: 6.836 | Reg loss: 0.032 | Tree loss: 6.836 | Accuracy: 0.162109 | 6.926 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 028 | Total loss: 6.864 | Reg loss: 0.032 | Tree loss: 6.864 | Accuracy: 0.154297 | 6.926 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 028 | Total loss: 6.737 | Reg loss: 0.032 | Tree loss: 6.737 | Accuracy: 0.162109 | 6.925 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 028 | Total loss: 6.768 | Reg loss: 0.032 | Tree loss: 6.768 | Accuracy: 0.173828 | 6.924 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 028 | Total loss: 6.736 | Reg loss: 0.033 | Tree loss: 6.736 | Accuracy: 0.183594 | 6.924 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 028 | Total loss: 6.760 | Reg loss: 0.033 | Tree loss: 6.760 | Accuracy: 0.162109 | 6.923 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 028 | Total loss: 6.659 | Reg loss: 0.033 | Tree loss: 6.659 | Accuracy: 0.203125 | 6.923 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 028 | Total loss: 6.676 | Reg loss: 0.033 | Tree loss: 6.676 | Accuracy: 0.162109 | 6.924 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 028 | Total loss: 6.627 | Reg loss: 0.033 | Tree loss: 6.627 | Accuracy: 0.179688 | 6.924 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 028 | Total loss: 6.683 | Reg loss: 0.034 | Tree loss: 6.683 | Accuracy: 0.164062 | 6.925 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 028 | Total loss: 6.569 | Reg loss: 0.034 | Tree loss: 6.569 | Accuracy: 0.189453 | 6.926 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 028 | Total loss: 6.566 | Reg loss: 0.034 | Tree loss: 6.566 | Accuracy: 0.195312 | 6.926 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 028 | Total loss: 6.514 | Reg loss: 0.034 | Tree loss: 6.514 | Accuracy: 0.179688 | 6.927 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 028 | Total loss: 6.409 | Reg loss: 0.035 | Tree loss: 6.409 | Accuracy: 0.375000 | 6.917 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 028 | Total loss: 6.946 | Reg loss: 0.031 | Tree loss: 6.946 | Accuracy: 0.191406 | 6.938 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 028 | Total loss: 6.883 | Reg loss: 0.031 | Tree loss: 6.883 | Accuracy: 0.158203 | 6.938 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 028 | Total loss: 6.895 | Reg loss: 0.031 | Tree loss: 6.895 | Accuracy: 0.205078 | 6.939 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 028 | Total loss: 6.880 | Reg loss: 0.031 | Tree loss: 6.880 | Accuracy: 0.173828 | 6.94 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 028 | Total loss: 6.851 | Reg loss: 0.031 | Tree loss: 6.851 | Accuracy: 0.160156 | 6.941 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 005 / 028 | Total loss: 6.848 | Reg loss: 0.031 | Tree loss: 6.848 | Accuracy: 0.173828 | 6.943 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 028 | Total loss: 6.783 | Reg loss: 0.032 | Tree loss: 6.783 | Accuracy: 0.154297 | 6.944 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 028 | Total loss: 6.775 | Reg loss: 0.032 | Tree loss: 6.775 | Accuracy: 0.181641 | 6.946 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 028 | Total loss: 6.701 | Reg loss: 0.032 | Tree loss: 6.701 | Accuracy: 0.171875 | 6.946 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 028 | Total loss: 6.687 | Reg loss: 0.032 | Tree loss: 6.687 | Accuracy: 0.181641 | 6.947 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 028 | Total loss: 6.737 | Reg loss: 0.032 | Tree loss: 6.737 | Accuracy: 0.207031 | 6.948 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 028 | Total loss: 6.632 | Reg loss: 0.032 | Tree loss: 6.632 | Accuracy: 0.189453 | 6.944 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 028 | Total loss: 6.594 | Reg loss: 0.033 | Tree loss: 6.594 | Accuracy: 0.169922 | 6.94 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 028 | Total loss: 6.589 | Reg loss: 0.033 | Tree loss: 6.589 | Accuracy: 0.177734 | 6.942 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 028 | Total loss: 6.547 | Reg loss: 0.033 | Tree loss: 6.547 | Accuracy: 0.185547 | 6.943 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 028 | Total loss: 6.535 | Reg loss: 0.033 | Tree loss: 6.535 | Accuracy: 0.210938 | 6.944 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 028 | Total loss: 6.482 | Reg loss: 0.033 | Tree loss: 6.482 | Accuracy: 0.195312 | 6.945 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 028 | Total loss: 6.484 | Reg loss: 0.034 | Tree loss: 6.484 | Accuracy: 0.193359 | 6.945 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 028 | Total loss: 6.420 | Reg loss: 0.034 | Tree loss: 6.420 | Accuracy: 0.205078 | 6.946 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 028 | Total loss: 6.423 | Reg loss: 0.034 | Tree loss: 6.423 | Accuracy: 0.205078 | 6.947 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 028 | Total loss: 6.415 | Reg loss: 0.034 | Tree loss: 6.415 | Accuracy: 0.156250 | 6.947 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 028 | Total loss: 6.381 | Reg loss: 0.034 | Tree loss: 6.381 | Accuracy: 0.175781 | 6.947 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 028 | Total loss: 6.377 | Reg loss: 0.035 | Tree loss: 6.377 | Accuracy: 0.169922 | 6.946 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 028 | Total loss: 6.337 | Reg loss: 0.035 | Tree loss: 6.337 | Accuracy: 0.167969 | 6.946 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 028 | Total loss: 6.289 | Reg loss: 0.035 | Tree loss: 6.289 | Accuracy: 0.173828 | 6.945 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 028 | Total loss: 6.282 | Reg loss: 0.035 | Tree loss: 6.282 | Accuracy: 0.212891 | 6.944 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 028 | Total loss: 6.300 | Reg loss: 0.036 | Tree loss: 6.300 | Accuracy: 0.154297 | 6.943 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 028 | Total loss: 6.183 | Reg loss: 0.036 | Tree loss: 6.183 | Accuracy: 0.062500 | 6.935 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 028 | Total loss: 6.674 | Reg loss: 0.032 | Tree loss: 6.674 | Accuracy: 0.189453 | 6.946 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 028 | Total loss: 6.645 | Reg loss: 0.032 | Tree loss: 6.645 | Accuracy: 0.154297 | 6.946 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 028 | Total loss: 6.559 | Reg loss: 0.032 | Tree loss: 6.559 | Accuracy: 0.208984 | 6.946 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 028 | Total loss: 6.586 | Reg loss: 0.032 | Tree loss: 6.586 | Accuracy: 0.193359 | 6.944 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 028 | Total loss: 6.565 | Reg loss: 0.033 | Tree loss: 6.565 | Accuracy: 0.162109 | 6.944 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 028 | Total loss: 6.535 | Reg loss: 0.033 | Tree loss: 6.535 | Accuracy: 0.175781 | 6.943 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 028 | Total loss: 6.502 | Reg loss: 0.033 | Tree loss: 6.502 | Accuracy: 0.199219 | 6.943 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 028 | Total loss: 6.444 | Reg loss: 0.033 | Tree loss: 6.444 | Accuracy: 0.183594 | 6.942 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 028 | Total loss: 6.453 | Reg loss: 0.033 | Tree loss: 6.453 | Accuracy: 0.173828 | 6.942 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 028 | Total loss: 6.414 | Reg loss: 0.033 | Tree loss: 6.414 | Accuracy: 0.171875 | 6.941 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 028 | Total loss: 6.432 | Reg loss: 0.033 | Tree loss: 6.432 | Accuracy: 0.175781 | 6.941 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 028 | Total loss: 6.391 | Reg loss: 0.033 | Tree loss: 6.391 | Accuracy: 0.175781 | 6.941 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 028 | Total loss: 6.338 | Reg loss: 0.034 | Tree loss: 6.338 | Accuracy: 0.197266 | 6.942 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 028 | Total loss: 6.322 | Reg loss: 0.034 | Tree loss: 6.322 | Accuracy: 0.191406 | 6.942 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 028 | Total loss: 6.286 | Reg loss: 0.034 | Tree loss: 6.286 | Accuracy: 0.158203 | 6.939 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 028 | Total loss: 6.221 | Reg loss: 0.034 | Tree loss: 6.221 | Accuracy: 0.207031 | 6.934 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 028 | Total loss: 6.224 | Reg loss: 0.034 | Tree loss: 6.224 | Accuracy: 0.185547 | 6.929 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 028 | Total loss: 6.201 | Reg loss: 0.035 | Tree loss: 6.201 | Accuracy: 0.189453 | 6.924 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 028 | Total loss: 6.201 | Reg loss: 0.035 | Tree loss: 6.201 | Accuracy: 0.142578 | 6.92 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 028 | Total loss: 6.139 | Reg loss: 0.035 | Tree loss: 6.139 | Accuracy: 0.197266 | 6.916 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 028 | Total loss: 6.165 | Reg loss: 0.035 | Tree loss: 6.165 | Accuracy: 0.167969 | 6.911 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 028 | Total loss: 6.078 | Reg loss: 0.035 | Tree loss: 6.078 | Accuracy: 0.185547 | 6.906 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 028 | Total loss: 6.040 | Reg loss: 0.036 | Tree loss: 6.040 | Accuracy: 0.167969 | 6.902 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 028 | Total loss: 6.046 | Reg loss: 0.036 | Tree loss: 6.046 | Accuracy: 0.171875 | 6.898 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 028 | Total loss: 5.992 | Reg loss: 0.036 | Tree loss: 5.992 | Accuracy: 0.199219 | 6.899 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 028 | Total loss: 5.992 | Reg loss: 0.036 | Tree loss: 5.992 | Accuracy: 0.183594 | 6.899 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 028 | Total loss: 6.004 | Reg loss: 0.036 | Tree loss: 6.004 | Accuracy: 0.191406 | 6.9 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 028 | Total loss: 6.002 | Reg loss: 0.037 | Tree loss: 6.002 | Accuracy: 0.125000 | 6.892 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 028 | Total loss: 6.408 | Reg loss: 0.033 | Tree loss: 6.408 | Accuracy: 0.167969 | 6.913 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 028 | Total loss: 6.369 | Reg loss: 0.033 | Tree loss: 6.369 | Accuracy: 0.167969 | 6.914 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 028 | Total loss: 6.331 | Reg loss: 0.034 | Tree loss: 6.331 | Accuracy: 0.181641 | 6.915 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 028 | Total loss: 6.321 | Reg loss: 0.034 | Tree loss: 6.321 | Accuracy: 0.201172 | 6.916 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 028 | Total loss: 6.270 | Reg loss: 0.034 | Tree loss: 6.270 | Accuracy: 0.199219 | 6.917 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 028 | Total loss: 6.276 | Reg loss: 0.034 | Tree loss: 6.276 | Accuracy: 0.179688 | 6.917 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 028 | Total loss: 6.200 | Reg loss: 0.034 | Tree loss: 6.200 | Accuracy: 0.150391 | 6.917 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 028 | Total loss: 6.186 | Reg loss: 0.034 | Tree loss: 6.186 | Accuracy: 0.187500 | 6.916 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 028 | Total loss: 6.191 | Reg loss: 0.034 | Tree loss: 6.191 | Accuracy: 0.187500 | 6.915 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 028 | Total loss: 6.115 | Reg loss: 0.034 | Tree loss: 6.115 | Accuracy: 0.197266 | 6.914 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 010 / 028 | Total loss: 6.129 | Reg loss: 0.034 | Tree loss: 6.129 | Accuracy: 0.187500 | 6.913 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 028 | Total loss: 6.058 | Reg loss: 0.034 | Tree loss: 6.058 | Accuracy: 0.160156 | 6.913 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 028 | Total loss: 6.042 | Reg loss: 0.035 | Tree loss: 6.042 | Accuracy: 0.187500 | 6.913 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 028 | Total loss: 6.000 | Reg loss: 0.035 | Tree loss: 6.000 | Accuracy: 0.193359 | 6.914 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 028 | Total loss: 5.970 | Reg loss: 0.035 | Tree loss: 5.970 | Accuracy: 0.207031 | 6.914 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 028 | Total loss: 5.990 | Reg loss: 0.035 | Tree loss: 5.990 | Accuracy: 0.183594 | 6.915 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 028 | Total loss: 5.939 | Reg loss: 0.035 | Tree loss: 5.939 | Accuracy: 0.201172 | 6.916 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 028 | Total loss: 5.921 | Reg loss: 0.035 | Tree loss: 5.921 | Accuracy: 0.164062 | 6.917 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 028 | Total loss: 5.908 | Reg loss: 0.036 | Tree loss: 5.908 | Accuracy: 0.207031 | 6.918 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 028 | Total loss: 5.887 | Reg loss: 0.036 | Tree loss: 5.887 | Accuracy: 0.152344 | 6.918 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 028 | Total loss: 5.886 | Reg loss: 0.036 | Tree loss: 5.886 | Accuracy: 0.181641 | 6.919 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 028 | Total loss: 5.835 | Reg loss: 0.036 | Tree loss: 5.835 | Accuracy: 0.201172 | 6.92 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 028 | Total loss: 5.812 | Reg loss: 0.036 | Tree loss: 5.812 | Accuracy: 0.199219 | 6.92 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 028 | Total loss: 5.799 | Reg loss: 0.037 | Tree loss: 5.799 | Accuracy: 0.128906 | 6.921 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 028 | Total loss: 5.774 | Reg loss: 0.037 | Tree loss: 5.774 | Accuracy: 0.177734 | 6.917 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 028 | Total loss: 5.748 | Reg loss: 0.037 | Tree loss: 5.748 | Accuracy: 0.171875 | 6.913 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 028 | Total loss: 5.735 | Reg loss: 0.037 | Tree loss: 5.735 | Accuracy: 0.169922 | 6.909 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 028 | Total loss: 5.597 | Reg loss: 0.037 | Tree loss: 5.597 | Accuracy: 0.312500 | 6.901 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 028 | Total loss: 6.120 | Reg loss: 0.035 | Tree loss: 6.120 | Accuracy: 0.177734 | 6.925 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 028 | Total loss: 6.068 | Reg loss: 0.035 | Tree loss: 6.068 | Accuracy: 0.173828 | 6.92 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 028 | Total loss: 6.011 | Reg loss: 0.035 | Tree loss: 6.011 | Accuracy: 0.205078 | 6.916 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 028 | Total loss: 6.047 | Reg loss: 0.035 | Tree loss: 6.047 | Accuracy: 0.164062 | 6.912 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 028 | Total loss: 5.970 | Reg loss: 0.035 | Tree loss: 5.970 | Accuracy: 0.187500 | 6.908 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 028 | Total loss: 5.971 | Reg loss: 0.035 | Tree loss: 5.971 | Accuracy: 0.173828 | 6.904 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 028 | Total loss: 5.893 | Reg loss: 0.035 | Tree loss: 5.893 | Accuracy: 0.197266 | 6.9 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 028 | Total loss: 5.939 | Reg loss: 0.035 | Tree loss: 5.939 | Accuracy: 0.179688 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 028 | Total loss: 5.871 | Reg loss: 0.035 | Tree loss: 5.871 | Accuracy: 0.193359 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 028 | Total loss: 5.866 | Reg loss: 0.035 | Tree loss: 5.866 | Accuracy: 0.187500 | 6.896 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 028 | Total loss: 5.807 | Reg loss: 0.035 | Tree loss: 5.807 | Accuracy: 0.175781 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 028 | Total loss: 5.779 | Reg loss: 0.035 | Tree loss: 5.779 | Accuracy: 0.181641 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 028 | Total loss: 5.837 | Reg loss: 0.036 | Tree loss: 5.837 | Accuracy: 0.207031 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 028 | Total loss: 5.781 | Reg loss: 0.036 | Tree loss: 5.781 | Accuracy: 0.183594 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 028 | Total loss: 5.759 | Reg loss: 0.036 | Tree loss: 5.759 | Accuracy: 0.195312 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 028 | Total loss: 5.733 | Reg loss: 0.036 | Tree loss: 5.733 | Accuracy: 0.214844 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 028 | Total loss: 5.695 | Reg loss: 0.036 | Tree loss: 5.695 | Accuracy: 0.175781 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 028 | Total loss: 5.709 | Reg loss: 0.036 | Tree loss: 5.709 | Accuracy: 0.130859 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 028 | Total loss: 5.659 | Reg loss: 0.036 | Tree loss: 5.659 | Accuracy: 0.148438 | 6.894 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 028 | Total loss: 5.673 | Reg loss: 0.037 | Tree loss: 5.673 | Accuracy: 0.181641 | 6.894 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 028 | Total loss: 5.603 | Reg loss: 0.037 | Tree loss: 5.603 | Accuracy: 0.177734 | 6.894 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 028 | Total loss: 5.574 | Reg loss: 0.037 | Tree loss: 5.574 | Accuracy: 0.169922 | 6.894 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 028 | Total loss: 5.592 | Reg loss: 0.037 | Tree loss: 5.592 | Accuracy: 0.152344 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 028 | Total loss: 5.542 | Reg loss: 0.037 | Tree loss: 5.542 | Accuracy: 0.210938 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 028 | Total loss: 5.523 | Reg loss: 0.037 | Tree loss: 5.523 | Accuracy: 0.183594 | 6.895 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 028 | Total loss: 5.524 | Reg loss: 0.038 | Tree loss: 5.524 | Accuracy: 0.197266 | 6.896 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 028 | Total loss: 5.507 | Reg loss: 0.038 | Tree loss: 5.507 | Accuracy: 0.171875 | 6.896 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 028 | Total loss: 5.486 | Reg loss: 0.038 | Tree loss: 5.486 | Accuracy: 0.187500 | 6.889 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 028 | Total loss: 5.841 | Reg loss: 0.035 | Tree loss: 5.841 | Accuracy: 0.162109 | 6.906 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 028 | Total loss: 5.818 | Reg loss: 0.036 | Tree loss: 5.818 | Accuracy: 0.195312 | 6.907 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 028 | Total loss: 5.755 | Reg loss: 0.036 | Tree loss: 5.755 | Accuracy: 0.179688 | 6.907 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 028 | Total loss: 5.831 | Reg loss: 0.036 | Tree loss: 5.831 | Accuracy: 0.160156 | 6.908 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 028 | Total loss: 5.736 | Reg loss: 0.036 | Tree loss: 5.736 | Accuracy: 0.208984 | 6.909 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 028 | Total loss: 5.746 | Reg loss: 0.036 | Tree loss: 5.746 | Accuracy: 0.179688 | 6.909 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 028 | Total loss: 5.702 | Reg loss: 0.036 | Tree loss: 5.702 | Accuracy: 0.179688 | 6.91 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 028 | Total loss: 5.709 | Reg loss: 0.036 | Tree loss: 5.709 | Accuracy: 0.134766 | 6.911 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 028 | Total loss: 5.607 | Reg loss: 0.036 | Tree loss: 5.607 | Accuracy: 0.203125 | 6.907 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 028 | Total loss: 5.568 | Reg loss: 0.036 | Tree loss: 5.568 | Accuracy: 0.183594 | 6.903 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 028 | Total loss: 5.644 | Reg loss: 0.036 | Tree loss: 5.644 | Accuracy: 0.195312 | 6.899 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 028 | Total loss: 5.638 | Reg loss: 0.036 | Tree loss: 5.638 | Accuracy: 0.173828 | 6.895 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 028 | Total loss: 5.507 | Reg loss: 0.036 | Tree loss: 5.507 | Accuracy: 0.207031 | 6.891 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 028 | Total loss: 5.517 | Reg loss: 0.037 | Tree loss: 5.517 | Accuracy: 0.150391 | 6.887 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 028 | Total loss: 5.479 | Reg loss: 0.037 | Tree loss: 5.479 | Accuracy: 0.181641 | 6.884 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 015 / 028 | Total loss: 5.409 | Reg loss: 0.037 | Tree loss: 5.409 | Accuracy: 0.191406 | 6.88 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 028 | Total loss: 5.458 | Reg loss: 0.037 | Tree loss: 5.458 | Accuracy: 0.195312 | 6.878 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 028 | Total loss: 5.419 | Reg loss: 0.037 | Tree loss: 5.419 | Accuracy: 0.185547 | 6.878 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 028 | Total loss: 5.426 | Reg loss: 0.037 | Tree loss: 5.426 | Accuracy: 0.195312 | 6.877 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 028 | Total loss: 5.383 | Reg loss: 0.037 | Tree loss: 5.383 | Accuracy: 0.144531 | 6.877 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 028 | Total loss: 5.331 | Reg loss: 0.038 | Tree loss: 5.331 | Accuracy: 0.193359 | 6.877 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 028 | Total loss: 5.311 | Reg loss: 0.038 | Tree loss: 5.311 | Accuracy: 0.167969 | 6.877 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 028 | Total loss: 5.294 | Reg loss: 0.038 | Tree loss: 5.294 | Accuracy: 0.185547 | 6.877 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 028 | Total loss: 5.319 | Reg loss: 0.038 | Tree loss: 5.319 | Accuracy: 0.193359 | 6.878 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 028 | Total loss: 5.288 | Reg loss: 0.038 | Tree loss: 5.288 | Accuracy: 0.208984 | 6.878 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 028 | Total loss: 5.319 | Reg loss: 0.038 | Tree loss: 5.319 | Accuracy: 0.162109 | 6.879 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 028 | Total loss: 5.228 | Reg loss: 0.038 | Tree loss: 5.228 | Accuracy: 0.179688 | 6.88 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 028 | Total loss: 5.196 | Reg loss: 0.039 | Tree loss: 5.196 | Accuracy: 0.250000 | 6.873 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 028 | Total loss: 5.552 | Reg loss: 0.036 | Tree loss: 5.552 | Accuracy: 0.175781 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 028 | Total loss: 5.583 | Reg loss: 0.036 | Tree loss: 5.583 | Accuracy: 0.181641 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 028 | Total loss: 5.590 | Reg loss: 0.036 | Tree loss: 5.590 | Accuracy: 0.177734 | 6.881 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 028 | Total loss: 5.542 | Reg loss: 0.036 | Tree loss: 5.542 | Accuracy: 0.185547 | 6.881 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 028 | Total loss: 5.498 | Reg loss: 0.037 | Tree loss: 5.498 | Accuracy: 0.185547 | 6.881 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 028 | Total loss: 5.474 | Reg loss: 0.037 | Tree loss: 5.474 | Accuracy: 0.181641 | 6.881 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 028 | Total loss: 5.428 | Reg loss: 0.037 | Tree loss: 5.428 | Accuracy: 0.197266 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 028 | Total loss: 5.448 | Reg loss: 0.037 | Tree loss: 5.448 | Accuracy: 0.175781 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 028 | Total loss: 5.429 | Reg loss: 0.037 | Tree loss: 5.429 | Accuracy: 0.158203 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 028 | Total loss: 5.320 | Reg loss: 0.037 | Tree loss: 5.320 | Accuracy: 0.214844 | 6.879 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 028 | Total loss: 5.289 | Reg loss: 0.037 | Tree loss: 5.289 | Accuracy: 0.173828 | 6.879 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 028 | Total loss: 5.318 | Reg loss: 0.037 | Tree loss: 5.318 | Accuracy: 0.183594 | 6.879 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 028 | Total loss: 5.393 | Reg loss: 0.037 | Tree loss: 5.393 | Accuracy: 0.183594 | 6.879 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 028 | Total loss: 5.253 | Reg loss: 0.037 | Tree loss: 5.253 | Accuracy: 0.207031 | 6.879 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 028 | Total loss: 5.255 | Reg loss: 0.037 | Tree loss: 5.255 | Accuracy: 0.175781 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 028 | Total loss: 5.241 | Reg loss: 0.038 | Tree loss: 5.241 | Accuracy: 0.199219 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 028 | Total loss: 5.262 | Reg loss: 0.038 | Tree loss: 5.262 | Accuracy: 0.162109 | 6.881 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 028 | Total loss: 5.212 | Reg loss: 0.038 | Tree loss: 5.212 | Accuracy: 0.177734 | 6.88 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 028 | Total loss: 5.190 | Reg loss: 0.038 | Tree loss: 5.190 | Accuracy: 0.201172 | 6.877 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 028 | Total loss: 5.143 | Reg loss: 0.038 | Tree loss: 5.143 | Accuracy: 0.150391 | 6.874 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 028 | Total loss: 5.117 | Reg loss: 0.038 | Tree loss: 5.117 | Accuracy: 0.183594 | 6.87 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 028 | Total loss: 5.116 | Reg loss: 0.038 | Tree loss: 5.116 | Accuracy: 0.173828 | 6.866 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 028 | Total loss: 5.081 | Reg loss: 0.039 | Tree loss: 5.081 | Accuracy: 0.185547 | 6.863 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 028 | Total loss: 5.075 | Reg loss: 0.039 | Tree loss: 5.075 | Accuracy: 0.181641 | 6.859 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 028 | Total loss: 5.065 | Reg loss: 0.039 | Tree loss: 5.065 | Accuracy: 0.183594 | 6.86 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 028 | Total loss: 5.043 | Reg loss: 0.039 | Tree loss: 5.043 | Accuracy: 0.179688 | 6.86 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 028 | Total loss: 5.011 | Reg loss: 0.039 | Tree loss: 5.011 | Accuracy: 0.156250 | 6.86 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 028 | Total loss: 4.908 | Reg loss: 0.039 | Tree loss: 4.908 | Accuracy: 0.250000 | 6.853 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 028 | Total loss: 5.369 | Reg loss: 0.037 | Tree loss: 5.369 | Accuracy: 0.179688 | 6.877 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 028 | Total loss: 5.329 | Reg loss: 0.037 | Tree loss: 5.329 | Accuracy: 0.156250 | 6.875 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 028 | Total loss: 5.358 | Reg loss: 0.037 | Tree loss: 5.358 | Accuracy: 0.199219 | 6.874 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 028 | Total loss: 5.267 | Reg loss: 0.037 | Tree loss: 5.267 | Accuracy: 0.189453 | 6.875 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 028 | Total loss: 5.289 | Reg loss: 0.037 | Tree loss: 5.289 | Accuracy: 0.195312 | 6.876 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 028 | Total loss: 5.272 | Reg loss: 0.037 | Tree loss: 5.272 | Accuracy: 0.175781 | 6.877 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 028 | Total loss: 5.239 | Reg loss: 0.037 | Tree loss: 5.239 | Accuracy: 0.189453 | 6.878 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 028 | Total loss: 5.207 | Reg loss: 0.038 | Tree loss: 5.207 | Accuracy: 0.177734 | 6.878 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 028 | Total loss: 5.209 | Reg loss: 0.038 | Tree loss: 5.209 | Accuracy: 0.191406 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 028 | Total loss: 5.124 | Reg loss: 0.038 | Tree loss: 5.124 | Accuracy: 0.175781 | 6.88 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 028 | Total loss: 5.121 | Reg loss: 0.038 | Tree loss: 5.121 | Accuracy: 0.207031 | 6.88 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 028 | Total loss: 5.093 | Reg loss: 0.038 | Tree loss: 5.093 | Accuracy: 0.171875 | 6.88 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 028 | Total loss: 5.104 | Reg loss: 0.038 | Tree loss: 5.104 | Accuracy: 0.167969 | 6.88 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 028 | Total loss: 4.994 | Reg loss: 0.038 | Tree loss: 4.994 | Accuracy: 0.179688 | 6.88 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 028 | Total loss: 5.035 | Reg loss: 0.038 | Tree loss: 5.035 | Accuracy: 0.208984 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 028 | Total loss: 4.972 | Reg loss: 0.038 | Tree loss: 4.972 | Accuracy: 0.193359 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 028 | Total loss: 4.999 | Reg loss: 0.038 | Tree loss: 4.999 | Accuracy: 0.203125 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 028 | Total loss: 4.983 | Reg loss: 0.039 | Tree loss: 4.983 | Accuracy: 0.156250 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 028 | Total loss: 4.977 | Reg loss: 0.039 | Tree loss: 4.977 | Accuracy: 0.195312 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 028 | Total loss: 4.953 | Reg loss: 0.039 | Tree loss: 4.953 | Accuracy: 0.187500 | 6.879 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 020 / 028 | Total loss: 4.886 | Reg loss: 0.039 | Tree loss: 4.886 | Accuracy: 0.183594 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 028 | Total loss: 4.897 | Reg loss: 0.039 | Tree loss: 4.897 | Accuracy: 0.164062 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 028 | Total loss: 4.863 | Reg loss: 0.039 | Tree loss: 4.863 | Accuracy: 0.189453 | 6.879 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 028 | Total loss: 4.886 | Reg loss: 0.039 | Tree loss: 4.886 | Accuracy: 0.166016 | 6.876 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 028 | Total loss: 4.826 | Reg loss: 0.039 | Tree loss: 4.826 | Accuracy: 0.152344 | 6.872 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 028 | Total loss: 4.865 | Reg loss: 0.040 | Tree loss: 4.865 | Accuracy: 0.156250 | 6.869 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 028 | Total loss: 4.847 | Reg loss: 0.040 | Tree loss: 4.847 | Accuracy: 0.181641 | 6.866 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 028 | Total loss: 4.943 | Reg loss: 0.040 | Tree loss: 4.943 | Accuracy: 0.187500 | 6.86 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 028 | Total loss: 5.151 | Reg loss: 0.038 | Tree loss: 5.151 | Accuracy: 0.181641 | 6.885 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 028 | Total loss: 5.120 | Reg loss: 0.038 | Tree loss: 5.120 | Accuracy: 0.181641 | 6.883 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 028 | Total loss: 5.091 | Reg loss: 0.038 | Tree loss: 5.091 | Accuracy: 0.205078 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 028 | Total loss: 5.089 | Reg loss: 0.038 | Tree loss: 5.089 | Accuracy: 0.156250 | 6.881 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 028 | Total loss: 5.086 | Reg loss: 0.038 | Tree loss: 5.086 | Accuracy: 0.166016 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 028 | Total loss: 4.952 | Reg loss: 0.038 | Tree loss: 4.952 | Accuracy: 0.177734 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 028 | Total loss: 5.027 | Reg loss: 0.038 | Tree loss: 5.027 | Accuracy: 0.158203 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 028 | Total loss: 4.961 | Reg loss: 0.038 | Tree loss: 4.961 | Accuracy: 0.164062 | 6.879 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 028 | Total loss: 4.982 | Reg loss: 0.038 | Tree loss: 4.982 | Accuracy: 0.195312 | 6.879 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 028 | Total loss: 4.960 | Reg loss: 0.038 | Tree loss: 4.960 | Accuracy: 0.197266 | 6.878 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 028 | Total loss: 4.903 | Reg loss: 0.038 | Tree loss: 4.903 | Accuracy: 0.193359 | 6.878 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 028 | Total loss: 4.908 | Reg loss: 0.039 | Tree loss: 4.908 | Accuracy: 0.183594 | 6.878 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 028 | Total loss: 4.933 | Reg loss: 0.039 | Tree loss: 4.933 | Accuracy: 0.150391 | 6.878 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 028 | Total loss: 4.830 | Reg loss: 0.039 | Tree loss: 4.830 | Accuracy: 0.189453 | 6.879 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 028 | Total loss: 4.809 | Reg loss: 0.039 | Tree loss: 4.809 | Accuracy: 0.203125 | 6.879 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 028 | Total loss: 4.800 | Reg loss: 0.039 | Tree loss: 4.800 | Accuracy: 0.193359 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 028 | Total loss: 4.828 | Reg loss: 0.039 | Tree loss: 4.828 | Accuracy: 0.171875 | 6.88 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 028 | Total loss: 4.809 | Reg loss: 0.039 | Tree loss: 4.809 | Accuracy: 0.193359 | 6.881 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 028 | Total loss: 4.778 | Reg loss: 0.039 | Tree loss: 4.778 | Accuracy: 0.183594 | 6.881 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 028 | Total loss: 4.716 | Reg loss: 0.039 | Tree loss: 4.716 | Accuracy: 0.164062 | 6.882 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 028 | Total loss: 4.728 | Reg loss: 0.040 | Tree loss: 4.728 | Accuracy: 0.185547 | 6.882 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 028 | Total loss: 4.691 | Reg loss: 0.040 | Tree loss: 4.691 | Accuracy: 0.191406 | 6.882 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 028 | Total loss: 4.644 | Reg loss: 0.040 | Tree loss: 4.644 | Accuracy: 0.197266 | 6.883 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 028 | Total loss: 4.672 | Reg loss: 0.040 | Tree loss: 4.672 | Accuracy: 0.164062 | 6.883 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 028 | Total loss: 4.614 | Reg loss: 0.040 | Tree loss: 4.614 | Accuracy: 0.189453 | 6.884 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 028 | Total loss: 4.628 | Reg loss: 0.040 | Tree loss: 4.628 | Accuracy: 0.189453 | 6.884 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 028 | Total loss: 4.617 | Reg loss: 0.040 | Tree loss: 4.617 | Accuracy: 0.162109 | 6.883 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 028 | Total loss: 4.632 | Reg loss: 0.040 | Tree loss: 4.632 | Accuracy: 0.312500 | 6.878 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 028 | Total loss: 4.964 | Reg loss: 0.039 | Tree loss: 4.964 | Accuracy: 0.191406 | 6.89 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 028 | Total loss: 4.958 | Reg loss: 0.039 | Tree loss: 4.958 | Accuracy: 0.164062 | 6.891 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 028 | Total loss: 4.922 | Reg loss: 0.039 | Tree loss: 4.922 | Accuracy: 0.185547 | 6.891 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 028 | Total loss: 4.890 | Reg loss: 0.039 | Tree loss: 4.890 | Accuracy: 0.175781 | 6.889 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 028 | Total loss: 4.813 | Reg loss: 0.039 | Tree loss: 4.813 | Accuracy: 0.173828 | 6.886 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 028 | Total loss: 4.824 | Reg loss: 0.039 | Tree loss: 4.824 | Accuracy: 0.166016 | 6.882 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 028 | Total loss: 4.777 | Reg loss: 0.039 | Tree loss: 4.777 | Accuracy: 0.210938 | 6.879 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 028 | Total loss: 4.730 | Reg loss: 0.039 | Tree loss: 4.730 | Accuracy: 0.183594 | 6.876 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 028 | Total loss: 4.702 | Reg loss: 0.039 | Tree loss: 4.702 | Accuracy: 0.193359 | 6.873 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 028 | Total loss: 4.739 | Reg loss: 0.039 | Tree loss: 4.739 | Accuracy: 0.179688 | 6.87 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 028 | Total loss: 4.783 | Reg loss: 0.039 | Tree loss: 4.783 | Accuracy: 0.181641 | 6.867 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 028 | Total loss: 4.695 | Reg loss: 0.039 | Tree loss: 4.695 | Accuracy: 0.152344 | 6.864 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 028 | Total loss: 4.622 | Reg loss: 0.039 | Tree loss: 4.622 | Accuracy: 0.185547 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 028 | Total loss: 4.661 | Reg loss: 0.039 | Tree loss: 4.661 | Accuracy: 0.171875 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 028 | Total loss: 4.599 | Reg loss: 0.040 | Tree loss: 4.599 | Accuracy: 0.193359 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 028 | Total loss: 4.606 | Reg loss: 0.040 | Tree loss: 4.606 | Accuracy: 0.152344 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 028 | Total loss: 4.553 | Reg loss: 0.040 | Tree loss: 4.553 | Accuracy: 0.183594 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 028 | Total loss: 4.606 | Reg loss: 0.040 | Tree loss: 4.606 | Accuracy: 0.166016 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 028 | Total loss: 4.503 | Reg loss: 0.040 | Tree loss: 4.503 | Accuracy: 0.183594 | 6.862 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 028 | Total loss: 4.518 | Reg loss: 0.040 | Tree loss: 4.518 | Accuracy: 0.208984 | 6.863 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 028 | Total loss: 4.437 | Reg loss: 0.040 | Tree loss: 4.437 | Accuracy: 0.177734 | 6.863 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 028 | Total loss: 4.497 | Reg loss: 0.040 | Tree loss: 4.497 | Accuracy: 0.169922 | 6.864 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 028 | Total loss: 4.472 | Reg loss: 0.040 | Tree loss: 4.472 | Accuracy: 0.193359 | 6.864 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 028 | Total loss: 4.413 | Reg loss: 0.041 | Tree loss: 4.413 | Accuracy: 0.171875 | 6.864 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 028 | Total loss: 4.416 | Reg loss: 0.041 | Tree loss: 4.416 | Accuracy: 0.189453 | 6.865 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 025 / 028 | Total loss: 4.365 | Reg loss: 0.041 | Tree loss: 4.365 | Accuracy: 0.201172 | 6.865 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 028 | Total loss: 4.406 | Reg loss: 0.041 | Tree loss: 4.406 | Accuracy: 0.181641 | 6.866 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 028 | Total loss: 4.233 | Reg loss: 0.041 | Tree loss: 4.233 | Accuracy: 0.312500 | 6.86 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 028 | Total loss: 4.728 | Reg loss: 0.039 | Tree loss: 4.728 | Accuracy: 0.205078 | 6.871 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 028 | Total loss: 4.726 | Reg loss: 0.039 | Tree loss: 4.726 | Accuracy: 0.177734 | 6.871 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 028 | Total loss: 4.686 | Reg loss: 0.039 | Tree loss: 4.686 | Accuracy: 0.179688 | 6.872 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 028 | Total loss: 4.666 | Reg loss: 0.039 | Tree loss: 4.666 | Accuracy: 0.167969 | 6.873 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 028 | Total loss: 4.599 | Reg loss: 0.039 | Tree loss: 4.599 | Accuracy: 0.167969 | 6.873 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 028 | Total loss: 4.550 | Reg loss: 0.039 | Tree loss: 4.550 | Accuracy: 0.191406 | 6.874 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 028 | Total loss: 4.557 | Reg loss: 0.039 | Tree loss: 4.557 | Accuracy: 0.234375 | 6.875 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 028 | Total loss: 4.534 | Reg loss: 0.040 | Tree loss: 4.534 | Accuracy: 0.212891 | 6.875 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 028 | Total loss: 4.531 | Reg loss: 0.040 | Tree loss: 4.531 | Accuracy: 0.146484 | 6.876 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 028 | Total loss: 4.573 | Reg loss: 0.040 | Tree loss: 4.573 | Accuracy: 0.164062 | 6.876 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 028 | Total loss: 4.519 | Reg loss: 0.040 | Tree loss: 4.519 | Accuracy: 0.173828 | 6.876 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 028 | Total loss: 4.460 | Reg loss: 0.040 | Tree loss: 4.460 | Accuracy: 0.169922 | 6.876 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 028 | Total loss: 4.442 | Reg loss: 0.040 | Tree loss: 4.442 | Accuracy: 0.208984 | 6.875 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 028 | Total loss: 4.506 | Reg loss: 0.040 | Tree loss: 4.506 | Accuracy: 0.201172 | 6.872 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 028 | Total loss: 4.408 | Reg loss: 0.040 | Tree loss: 4.408 | Accuracy: 0.175781 | 6.868 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 028 | Total loss: 4.370 | Reg loss: 0.040 | Tree loss: 4.370 | Accuracy: 0.201172 | 6.865 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 028 | Total loss: 4.366 | Reg loss: 0.040 | Tree loss: 4.366 | Accuracy: 0.175781 | 6.862 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 028 | Total loss: 4.316 | Reg loss: 0.040 | Tree loss: 4.316 | Accuracy: 0.171875 | 6.859 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 028 | Total loss: 4.289 | Reg loss: 0.041 | Tree loss: 4.289 | Accuracy: 0.201172 | 6.856 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 028 | Total loss: 4.326 | Reg loss: 0.041 | Tree loss: 4.326 | Accuracy: 0.166016 | 6.853 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 028 | Total loss: 4.320 | Reg loss: 0.041 | Tree loss: 4.320 | Accuracy: 0.169922 | 6.85 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 028 | Total loss: 4.274 | Reg loss: 0.041 | Tree loss: 4.274 | Accuracy: 0.167969 | 6.849 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 028 | Total loss: 4.222 | Reg loss: 0.041 | Tree loss: 4.222 | Accuracy: 0.185547 | 6.848 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 028 | Total loss: 4.231 | Reg loss: 0.041 | Tree loss: 4.231 | Accuracy: 0.169922 | 6.848 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 028 | Total loss: 4.174 | Reg loss: 0.041 | Tree loss: 4.174 | Accuracy: 0.195312 | 6.848 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 028 | Total loss: 4.199 | Reg loss: 0.041 | Tree loss: 4.199 | Accuracy: 0.160156 | 6.848 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 028 | Total loss: 4.193 | Reg loss: 0.041 | Tree loss: 4.193 | Accuracy: 0.148438 | 6.848 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 028 | Total loss: 4.064 | Reg loss: 0.042 | Tree loss: 4.064 | Accuracy: 0.250000 | 6.843 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 028 | Total loss: 4.510 | Reg loss: 0.040 | Tree loss: 4.510 | Accuracy: 0.166016 | 6.854 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 028 | Total loss: 4.445 | Reg loss: 0.040 | Tree loss: 4.445 | Accuracy: 0.181641 | 6.851 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 028 | Total loss: 4.455 | Reg loss: 0.040 | Tree loss: 4.455 | Accuracy: 0.201172 | 6.851 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 028 | Total loss: 4.427 | Reg loss: 0.040 | Tree loss: 4.427 | Accuracy: 0.181641 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 028 | Total loss: 4.425 | Reg loss: 0.040 | Tree loss: 4.425 | Accuracy: 0.187500 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 028 | Total loss: 4.398 | Reg loss: 0.040 | Tree loss: 4.398 | Accuracy: 0.183594 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 028 | Total loss: 4.349 | Reg loss: 0.040 | Tree loss: 4.349 | Accuracy: 0.197266 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 028 | Total loss: 4.353 | Reg loss: 0.040 | Tree loss: 4.353 | Accuracy: 0.185547 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 028 | Total loss: 4.311 | Reg loss: 0.040 | Tree loss: 4.311 | Accuracy: 0.171875 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 028 | Total loss: 4.307 | Reg loss: 0.040 | Tree loss: 4.307 | Accuracy: 0.167969 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 028 | Total loss: 4.335 | Reg loss: 0.040 | Tree loss: 4.335 | Accuracy: 0.164062 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 028 | Total loss: 4.249 | Reg loss: 0.040 | Tree loss: 4.249 | Accuracy: 0.183594 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 028 | Total loss: 4.241 | Reg loss: 0.041 | Tree loss: 4.241 | Accuracy: 0.187500 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 028 | Total loss: 4.207 | Reg loss: 0.041 | Tree loss: 4.207 | Accuracy: 0.193359 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 028 | Total loss: 4.187 | Reg loss: 0.041 | Tree loss: 4.187 | Accuracy: 0.167969 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 028 | Total loss: 4.206 | Reg loss: 0.041 | Tree loss: 4.206 | Accuracy: 0.166016 | 6.851 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 028 | Total loss: 4.142 | Reg loss: 0.041 | Tree loss: 4.142 | Accuracy: 0.195312 | 6.851 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 028 | Total loss: 4.173 | Reg loss: 0.041 | Tree loss: 4.173 | Accuracy: 0.191406 | 6.851 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 028 | Total loss: 4.123 | Reg loss: 0.041 | Tree loss: 4.123 | Accuracy: 0.179688 | 6.852 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 028 | Total loss: 4.130 | Reg loss: 0.041 | Tree loss: 4.130 | Accuracy: 0.140625 | 6.852 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 028 | Total loss: 4.070 | Reg loss: 0.041 | Tree loss: 4.070 | Accuracy: 0.181641 | 6.853 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 028 | Total loss: 4.019 | Reg loss: 0.041 | Tree loss: 4.019 | Accuracy: 0.214844 | 6.853 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 028 | Total loss: 4.025 | Reg loss: 0.042 | Tree loss: 4.025 | Accuracy: 0.195312 | 6.85 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 028 | Total loss: 4.090 | Reg loss: 0.042 | Tree loss: 4.090 | Accuracy: 0.154297 | 6.847 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 028 | Total loss: 4.030 | Reg loss: 0.042 | Tree loss: 4.030 | Accuracy: 0.189453 | 6.844 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 028 | Total loss: 3.990 | Reg loss: 0.042 | Tree loss: 3.990 | Accuracy: 0.171875 | 6.841 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 028 | Total loss: 4.014 | Reg loss: 0.042 | Tree loss: 4.014 | Accuracy: 0.187500 | 6.839 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 028 | Total loss: 3.907 | Reg loss: 0.042 | Tree loss: 3.907 | Accuracy: 0.312500 | 6.833 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 028 | Total loss: 4.299 | Reg loss: 0.040 | Tree loss: 4.299 | Accuracy: 0.175781 | 6.853 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 028 | Total loss: 4.287 | Reg loss: 0.040 | Tree loss: 4.287 | Accuracy: 0.197266 | 6.85 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 028 | Total loss: 4.307 | Reg loss: 0.040 | Tree loss: 4.307 | Accuracy: 0.164062 | 6.848 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 028 | Total loss: 4.279 | Reg loss: 0.040 | Tree loss: 4.279 | Accuracy: 0.169922 | 6.845 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 028 | Total loss: 4.218 | Reg loss: 0.041 | Tree loss: 4.218 | Accuracy: 0.191406 | 6.842 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 028 | Total loss: 4.229 | Reg loss: 0.041 | Tree loss: 4.229 | Accuracy: 0.181641 | 6.839 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 028 | Total loss: 4.152 | Reg loss: 0.041 | Tree loss: 4.152 | Accuracy: 0.203125 | 6.837 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 028 | Total loss: 4.085 | Reg loss: 0.041 | Tree loss: 4.085 | Accuracy: 0.218750 | 6.834 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 028 | Total loss: 4.139 | Reg loss: 0.041 | Tree loss: 4.139 | Accuracy: 0.197266 | 6.835 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 028 | Total loss: 4.160 | Reg loss: 0.041 | Tree loss: 4.160 | Accuracy: 0.154297 | 6.835 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 028 | Total loss: 4.099 | Reg loss: 0.041 | Tree loss: 4.099 | Accuracy: 0.201172 | 6.835 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 028 | Total loss: 4.070 | Reg loss: 0.041 | Tree loss: 4.070 | Accuracy: 0.177734 | 6.835 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 028 | Total loss: 4.046 | Reg loss: 0.041 | Tree loss: 4.046 | Accuracy: 0.185547 | 6.835 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 028 | Total loss: 4.056 | Reg loss: 0.041 | Tree loss: 4.056 | Accuracy: 0.167969 | 6.836 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 028 | Total loss: 4.016 | Reg loss: 0.041 | Tree loss: 4.016 | Accuracy: 0.189453 | 6.836 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 028 | Total loss: 3.965 | Reg loss: 0.041 | Tree loss: 3.965 | Accuracy: 0.166016 | 6.836 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 028 | Total loss: 3.942 | Reg loss: 0.041 | Tree loss: 3.942 | Accuracy: 0.189453 | 6.837 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 028 | Total loss: 3.970 | Reg loss: 0.041 | Tree loss: 3.970 | Accuracy: 0.173828 | 6.837 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 028 | Total loss: 3.909 | Reg loss: 0.042 | Tree loss: 3.909 | Accuracy: 0.173828 | 6.837 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 028 | Total loss: 3.896 | Reg loss: 0.042 | Tree loss: 3.896 | Accuracy: 0.177734 | 6.838 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 028 | Total loss: 3.900 | Reg loss: 0.042 | Tree loss: 3.900 | Accuracy: 0.181641 | 6.838 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 028 | Total loss: 3.839 | Reg loss: 0.042 | Tree loss: 3.839 | Accuracy: 0.179688 | 6.838 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 028 | Total loss: 3.888 | Reg loss: 0.042 | Tree loss: 3.888 | Accuracy: 0.173828 | 6.839 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 028 | Total loss: 3.833 | Reg loss: 0.042 | Tree loss: 3.833 | Accuracy: 0.181641 | 6.839 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 028 | Total loss: 3.804 | Reg loss: 0.042 | Tree loss: 3.804 | Accuracy: 0.185547 | 6.839 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 028 | Total loss: 3.807 | Reg loss: 0.042 | Tree loss: 3.807 | Accuracy: 0.183594 | 6.84 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 028 | Total loss: 3.803 | Reg loss: 0.042 | Tree loss: 3.803 | Accuracy: 0.164062 | 6.839 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 028 | Total loss: 3.746 | Reg loss: 0.042 | Tree loss: 3.746 | Accuracy: 0.125000 | 6.834 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 028 | Total loss: 4.189 | Reg loss: 0.041 | Tree loss: 4.189 | Accuracy: 0.166016 | 6.846 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 028 | Total loss: 4.094 | Reg loss: 0.041 | Tree loss: 4.094 | Accuracy: 0.187500 | 6.847 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 028 | Total loss: 4.094 | Reg loss: 0.041 | Tree loss: 4.094 | Accuracy: 0.205078 | 6.847 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 028 | Total loss: 4.055 | Reg loss: 0.041 | Tree loss: 4.055 | Accuracy: 0.160156 | 6.847 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 028 | Total loss: 4.058 | Reg loss: 0.041 | Tree loss: 4.058 | Accuracy: 0.169922 | 6.848 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 028 | Total loss: 4.040 | Reg loss: 0.041 | Tree loss: 4.040 | Accuracy: 0.156250 | 6.848 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 028 | Total loss: 3.973 | Reg loss: 0.041 | Tree loss: 3.973 | Accuracy: 0.183594 | 6.849 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 028 | Total loss: 3.981 | Reg loss: 0.041 | Tree loss: 3.981 | Accuracy: 0.175781 | 6.846 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 028 | Total loss: 3.985 | Reg loss: 0.041 | Tree loss: 3.985 | Accuracy: 0.179688 | 6.843 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 028 | Total loss: 3.850 | Reg loss: 0.041 | Tree loss: 3.850 | Accuracy: 0.187500 | 6.841 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 028 | Total loss: 3.894 | Reg loss: 0.041 | Tree loss: 3.894 | Accuracy: 0.205078 | 6.838 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 028 | Total loss: 3.910 | Reg loss: 0.041 | Tree loss: 3.910 | Accuracy: 0.189453 | 6.835 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 028 | Total loss: 3.890 | Reg loss: 0.041 | Tree loss: 3.890 | Accuracy: 0.164062 | 6.833 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 028 | Total loss: 3.842 | Reg loss: 0.042 | Tree loss: 3.842 | Accuracy: 0.203125 | 6.83 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 028 | Total loss: 3.808 | Reg loss: 0.042 | Tree loss: 3.808 | Accuracy: 0.189453 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 028 | Total loss: 3.815 | Reg loss: 0.042 | Tree loss: 3.815 | Accuracy: 0.199219 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 028 | Total loss: 3.773 | Reg loss: 0.042 | Tree loss: 3.773 | Accuracy: 0.181641 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 028 | Total loss: 3.765 | Reg loss: 0.042 | Tree loss: 3.765 | Accuracy: 0.173828 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 028 | Total loss: 3.770 | Reg loss: 0.042 | Tree loss: 3.770 | Accuracy: 0.191406 | 6.828 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 028 | Total loss: 3.728 | Reg loss: 0.042 | Tree loss: 3.728 | Accuracy: 0.181641 | 6.828 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 028 | Total loss: 3.699 | Reg loss: 0.042 | Tree loss: 3.699 | Accuracy: 0.191406 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 028 | Total loss: 3.682 | Reg loss: 0.042 | Tree loss: 3.682 | Accuracy: 0.169922 | 6.829 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 028 | Total loss: 3.654 | Reg loss: 0.042 | Tree loss: 3.654 | Accuracy: 0.181641 | 6.83 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 028 | Total loss: 3.682 | Reg loss: 0.042 | Tree loss: 3.682 | Accuracy: 0.169922 | 6.83 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 028 | Total loss: 3.658 | Reg loss: 0.043 | Tree loss: 3.658 | Accuracy: 0.201172 | 6.831 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 028 | Total loss: 3.671 | Reg loss: 0.043 | Tree loss: 3.671 | Accuracy: 0.140625 | 6.831 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 028 | Total loss: 3.618 | Reg loss: 0.043 | Tree loss: 3.618 | Accuracy: 0.167969 | 6.832 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 028 | Total loss: 3.465 | Reg loss: 0.043 | Tree loss: 3.465 | Accuracy: 0.312500 | 6.827 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 028 | Total loss: 3.904 | Reg loss: 0.041 | Tree loss: 3.904 | Accuracy: 0.191406 | 6.833 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 028 | Total loss: 3.939 | Reg loss: 0.041 | Tree loss: 3.939 | Accuracy: 0.191406 | 6.833 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 028 | Total loss: 3.925 | Reg loss: 0.041 | Tree loss: 3.925 | Accuracy: 0.162109 | 6.832 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 028 | Total loss: 3.897 | Reg loss: 0.041 | Tree loss: 3.897 | Accuracy: 0.197266 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 028 | Total loss: 3.869 | Reg loss: 0.041 | Tree loss: 3.869 | Accuracy: 0.185547 | 6.831 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 005 / 028 | Total loss: 3.791 | Reg loss: 0.041 | Tree loss: 3.791 | Accuracy: 0.179688 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 028 | Total loss: 3.799 | Reg loss: 0.041 | Tree loss: 3.799 | Accuracy: 0.167969 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 028 | Total loss: 3.789 | Reg loss: 0.042 | Tree loss: 3.789 | Accuracy: 0.167969 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 028 | Total loss: 3.798 | Reg loss: 0.042 | Tree loss: 3.798 | Accuracy: 0.148438 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 028 | Total loss: 3.786 | Reg loss: 0.042 | Tree loss: 3.786 | Accuracy: 0.185547 | 6.832 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 028 | Total loss: 3.792 | Reg loss: 0.042 | Tree loss: 3.792 | Accuracy: 0.193359 | 6.833 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 028 | Total loss: 3.694 | Reg loss: 0.042 | Tree loss: 3.694 | Accuracy: 0.197266 | 6.833 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 028 | Total loss: 3.706 | Reg loss: 0.042 | Tree loss: 3.706 | Accuracy: 0.185547 | 6.834 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 028 | Total loss: 3.654 | Reg loss: 0.042 | Tree loss: 3.654 | Accuracy: 0.179688 | 6.834 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 028 | Total loss: 3.691 | Reg loss: 0.042 | Tree loss: 3.691 | Accuracy: 0.187500 | 6.834 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 028 | Total loss: 3.664 | Reg loss: 0.042 | Tree loss: 3.664 | Accuracy: 0.205078 | 6.835 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 028 | Total loss: 3.646 | Reg loss: 0.042 | Tree loss: 3.646 | Accuracy: 0.173828 | 6.833 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 028 | Total loss: 3.651 | Reg loss: 0.042 | Tree loss: 3.651 | Accuracy: 0.179688 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 028 | Total loss: 3.612 | Reg loss: 0.042 | Tree loss: 3.612 | Accuracy: 0.199219 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 028 | Total loss: 3.544 | Reg loss: 0.042 | Tree loss: 3.544 | Accuracy: 0.187500 | 6.831 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 028 | Total loss: 3.571 | Reg loss: 0.042 | Tree loss: 3.571 | Accuracy: 0.169922 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 028 | Total loss: 3.519 | Reg loss: 0.043 | Tree loss: 3.519 | Accuracy: 0.166016 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 028 | Total loss: 3.552 | Reg loss: 0.043 | Tree loss: 3.552 | Accuracy: 0.183594 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 028 | Total loss: 3.520 | Reg loss: 0.043 | Tree loss: 3.520 | Accuracy: 0.156250 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 028 | Total loss: 3.495 | Reg loss: 0.043 | Tree loss: 3.495 | Accuracy: 0.173828 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 028 | Total loss: 3.520 | Reg loss: 0.043 | Tree loss: 3.520 | Accuracy: 0.187500 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 028 | Total loss: 3.462 | Reg loss: 0.043 | Tree loss: 3.462 | Accuracy: 0.171875 | 6.83 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 028 | Total loss: 3.404 | Reg loss: 0.043 | Tree loss: 3.404 | Accuracy: 0.250000 | 6.825 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 028 | Total loss: 3.830 | Reg loss: 0.042 | Tree loss: 3.830 | Accuracy: 0.160156 | 6.835 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 028 | Total loss: 3.788 | Reg loss: 0.042 | Tree loss: 3.788 | Accuracy: 0.181641 | 6.833 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 028 | Total loss: 3.702 | Reg loss: 0.042 | Tree loss: 3.702 | Accuracy: 0.183594 | 6.831 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 028 | Total loss: 3.710 | Reg loss: 0.042 | Tree loss: 3.710 | Accuracy: 0.208984 | 6.83 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 028 | Total loss: 3.691 | Reg loss: 0.042 | Tree loss: 3.691 | Accuracy: 0.187500 | 6.83 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 028 | Total loss: 3.700 | Reg loss: 0.042 | Tree loss: 3.700 | Accuracy: 0.177734 | 6.831 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 028 | Total loss: 3.684 | Reg loss: 0.042 | Tree loss: 3.684 | Accuracy: 0.208984 | 6.831 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 028 | Total loss: 3.634 | Reg loss: 0.042 | Tree loss: 3.634 | Accuracy: 0.181641 | 6.832 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 028 | Total loss: 3.632 | Reg loss: 0.042 | Tree loss: 3.632 | Accuracy: 0.185547 | 6.832 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 028 | Total loss: 3.651 | Reg loss: 0.042 | Tree loss: 3.651 | Accuracy: 0.160156 | 6.833 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 028 | Total loss: 3.632 | Reg loss: 0.042 | Tree loss: 3.632 | Accuracy: 0.171875 | 6.833 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 028 | Total loss: 3.574 | Reg loss: 0.042 | Tree loss: 3.574 | Accuracy: 0.160156 | 6.834 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 028 | Total loss: 3.585 | Reg loss: 0.042 | Tree loss: 3.585 | Accuracy: 0.164062 | 6.834 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 028 | Total loss: 3.545 | Reg loss: 0.042 | Tree loss: 3.545 | Accuracy: 0.199219 | 6.834 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 028 | Total loss: 3.470 | Reg loss: 0.042 | Tree loss: 3.470 | Accuracy: 0.181641 | 6.835 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 028 | Total loss: 3.553 | Reg loss: 0.042 | Tree loss: 3.553 | Accuracy: 0.179688 | 6.835 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 028 | Total loss: 3.448 | Reg loss: 0.042 | Tree loss: 3.448 | Accuracy: 0.201172 | 6.836 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 028 | Total loss: 3.500 | Reg loss: 0.043 | Tree loss: 3.500 | Accuracy: 0.183594 | 6.836 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 028 | Total loss: 3.496 | Reg loss: 0.043 | Tree loss: 3.496 | Accuracy: 0.167969 | 6.834 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 028 | Total loss: 3.453 | Reg loss: 0.043 | Tree loss: 3.453 | Accuracy: 0.152344 | 6.832 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 028 | Total loss: 3.458 | Reg loss: 0.043 | Tree loss: 3.458 | Accuracy: 0.193359 | 6.829 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 028 | Total loss: 3.424 | Reg loss: 0.043 | Tree loss: 3.424 | Accuracy: 0.156250 | 6.827 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 028 | Total loss: 3.345 | Reg loss: 0.043 | Tree loss: 3.345 | Accuracy: 0.212891 | 6.824 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 028 | Total loss: 3.353 | Reg loss: 0.043 | Tree loss: 3.353 | Accuracy: 0.183594 | 6.822 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 028 | Total loss: 3.400 | Reg loss: 0.043 | Tree loss: 3.400 | Accuracy: 0.193359 | 6.82 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 028 | Total loss: 3.310 | Reg loss: 0.043 | Tree loss: 3.310 | Accuracy: 0.183594 | 6.817 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 028 | Total loss: 3.325 | Reg loss: 0.043 | Tree loss: 3.325 | Accuracy: 0.189453 | 6.815 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 028 | Total loss: 3.462 | Reg loss: 0.043 | Tree loss: 3.462 | Accuracy: 0.187500 | 6.811 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 028 | Total loss: 3.697 | Reg loss: 0.042 | Tree loss: 3.697 | Accuracy: 0.185547 | 6.84 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 028 | Total loss: 3.617 | Reg loss: 0.042 | Tree loss: 3.617 | Accuracy: 0.181641 | 6.839 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 028 | Total loss: 3.608 | Reg loss: 0.042 | Tree loss: 3.608 | Accuracy: 0.185547 | 6.839 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 028 | Total loss: 3.576 | Reg loss: 0.042 | Tree loss: 3.576 | Accuracy: 0.169922 | 6.84 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 028 | Total loss: 3.565 | Reg loss: 0.042 | Tree loss: 3.565 | Accuracy: 0.199219 | 6.84 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 028 | Total loss: 3.559 | Reg loss: 0.042 | Tree loss: 3.559 | Accuracy: 0.177734 | 6.841 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 028 | Total loss: 3.566 | Reg loss: 0.042 | Tree loss: 3.566 | Accuracy: 0.166016 | 6.841 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 028 | Total loss: 3.540 | Reg loss: 0.042 | Tree loss: 3.540 | Accuracy: 0.169922 | 6.842 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 028 | Total loss: 3.506 | Reg loss: 0.042 | Tree loss: 3.506 | Accuracy: 0.164062 | 6.843 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 028 | Total loss: 3.440 | Reg loss: 0.042 | Tree loss: 3.440 | Accuracy: 0.173828 | 6.843 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 010 / 028 | Total loss: 3.426 | Reg loss: 0.042 | Tree loss: 3.426 | Accuracy: 0.167969 | 6.843 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 028 | Total loss: 3.438 | Reg loss: 0.042 | Tree loss: 3.438 | Accuracy: 0.199219 | 6.844 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 028 | Total loss: 3.406 | Reg loss: 0.042 | Tree loss: 3.406 | Accuracy: 0.187500 | 6.844 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 028 | Total loss: 3.463 | Reg loss: 0.042 | Tree loss: 3.463 | Accuracy: 0.160156 | 6.845 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 028 | Total loss: 3.379 | Reg loss: 0.043 | Tree loss: 3.379 | Accuracy: 0.187500 | 6.845 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 028 | Total loss: 3.380 | Reg loss: 0.043 | Tree loss: 3.380 | Accuracy: 0.191406 | 6.845 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 028 | Total loss: 3.353 | Reg loss: 0.043 | Tree loss: 3.353 | Accuracy: 0.201172 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 028 | Total loss: 3.340 | Reg loss: 0.043 | Tree loss: 3.340 | Accuracy: 0.197266 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 028 | Total loss: 3.338 | Reg loss: 0.043 | Tree loss: 3.338 | Accuracy: 0.167969 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 028 | Total loss: 3.318 | Reg loss: 0.043 | Tree loss: 3.318 | Accuracy: 0.169922 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 028 | Total loss: 3.324 | Reg loss: 0.043 | Tree loss: 3.324 | Accuracy: 0.162109 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 028 | Total loss: 3.297 | Reg loss: 0.043 | Tree loss: 3.297 | Accuracy: 0.173828 | 6.846 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 028 | Total loss: 3.273 | Reg loss: 0.043 | Tree loss: 3.273 | Accuracy: 0.205078 | 6.844 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 028 | Total loss: 3.259 | Reg loss: 0.043 | Tree loss: 3.259 | Accuracy: 0.167969 | 6.842 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 028 | Total loss: 3.255 | Reg loss: 0.043 | Tree loss: 3.255 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 028 | Total loss: 3.219 | Reg loss: 0.043 | Tree loss: 3.219 | Accuracy: 0.189453 | 6.837 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 028 | Total loss: 3.225 | Reg loss: 0.043 | Tree loss: 3.225 | Accuracy: 0.183594 | 6.835 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 028 | Total loss: 3.117 | Reg loss: 0.044 | Tree loss: 3.117 | Accuracy: 0.312500 | 6.831 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 028 | Total loss: 3.526 | Reg loss: 0.042 | Tree loss: 3.526 | Accuracy: 0.187500 | 6.847 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 028 | Total loss: 3.486 | Reg loss: 0.042 | Tree loss: 3.486 | Accuracy: 0.185547 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 028 | Total loss: 3.489 | Reg loss: 0.042 | Tree loss: 3.489 | Accuracy: 0.175781 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 028 | Total loss: 3.510 | Reg loss: 0.042 | Tree loss: 3.510 | Accuracy: 0.167969 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 028 | Total loss: 3.426 | Reg loss: 0.042 | Tree loss: 3.426 | Accuracy: 0.187500 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 028 | Total loss: 3.402 | Reg loss: 0.042 | Tree loss: 3.402 | Accuracy: 0.171875 | 6.844 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 028 | Total loss: 3.461 | Reg loss: 0.042 | Tree loss: 3.461 | Accuracy: 0.187500 | 6.844 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 028 | Total loss: 3.355 | Reg loss: 0.042 | Tree loss: 3.355 | Accuracy: 0.169922 | 6.843 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 028 | Total loss: 3.340 | Reg loss: 0.042 | Tree loss: 3.340 | Accuracy: 0.167969 | 6.843 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 028 | Total loss: 3.369 | Reg loss: 0.042 | Tree loss: 3.369 | Accuracy: 0.187500 | 6.844 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 028 | Total loss: 3.396 | Reg loss: 0.043 | Tree loss: 3.396 | Accuracy: 0.156250 | 6.844 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 028 | Total loss: 3.348 | Reg loss: 0.043 | Tree loss: 3.348 | Accuracy: 0.199219 | 6.844 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 028 | Total loss: 3.325 | Reg loss: 0.043 | Tree loss: 3.325 | Accuracy: 0.175781 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 028 | Total loss: 3.299 | Reg loss: 0.043 | Tree loss: 3.299 | Accuracy: 0.187500 | 6.845 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 028 | Total loss: 3.273 | Reg loss: 0.043 | Tree loss: 3.273 | Accuracy: 0.183594 | 6.846 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 028 | Total loss: 3.275 | Reg loss: 0.043 | Tree loss: 3.275 | Accuracy: 0.158203 | 6.846 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 028 | Total loss: 3.261 | Reg loss: 0.043 | Tree loss: 3.261 | Accuracy: 0.185547 | 6.847 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 028 | Total loss: 3.226 | Reg loss: 0.043 | Tree loss: 3.226 | Accuracy: 0.191406 | 6.847 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 028 | Total loss: 3.184 | Reg loss: 0.043 | Tree loss: 3.184 | Accuracy: 0.177734 | 6.847 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 028 | Total loss: 3.225 | Reg loss: 0.043 | Tree loss: 3.225 | Accuracy: 0.167969 | 6.847 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 028 | Total loss: 3.176 | Reg loss: 0.043 | Tree loss: 3.176 | Accuracy: 0.183594 | 6.848 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 028 | Total loss: 3.165 | Reg loss: 0.043 | Tree loss: 3.165 | Accuracy: 0.185547 | 6.848 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 028 | Total loss: 3.214 | Reg loss: 0.043 | Tree loss: 3.214 | Accuracy: 0.177734 | 6.848 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 028 | Total loss: 3.153 | Reg loss: 0.043 | Tree loss: 3.153 | Accuracy: 0.166016 | 6.849 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 028 | Total loss: 3.149 | Reg loss: 0.043 | Tree loss: 3.149 | Accuracy: 0.167969 | 6.849 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 028 | Total loss: 3.098 | Reg loss: 0.043 | Tree loss: 3.098 | Accuracy: 0.193359 | 6.849 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 028 | Total loss: 3.142 | Reg loss: 0.044 | Tree loss: 3.142 | Accuracy: 0.199219 | 6.849 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 028 | Total loss: 3.031 | Reg loss: 0.044 | Tree loss: 3.031 | Accuracy: 0.062500 | 6.845 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 028 | Total loss: 3.402 | Reg loss: 0.042 | Tree loss: 3.402 | Accuracy: 0.181641 | 6.854 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 028 | Total loss: 3.362 | Reg loss: 0.042 | Tree loss: 3.362 | Accuracy: 0.181641 | 6.854 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 028 | Total loss: 3.415 | Reg loss: 0.042 | Tree loss: 3.415 | Accuracy: 0.152344 | 6.853 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 028 | Total loss: 3.383 | Reg loss: 0.042 | Tree loss: 3.383 | Accuracy: 0.181641 | 6.851 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 028 | Total loss: 3.315 | Reg loss: 0.043 | Tree loss: 3.315 | Accuracy: 0.187500 | 6.849 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 028 | Total loss: 3.377 | Reg loss: 0.043 | Tree loss: 3.377 | Accuracy: 0.173828 | 6.846 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 028 | Total loss: 3.341 | Reg loss: 0.043 | Tree loss: 3.341 | Accuracy: 0.185547 | 6.844 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 028 | Total loss: 3.273 | Reg loss: 0.043 | Tree loss: 3.273 | Accuracy: 0.187500 | 6.842 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 028 | Total loss: 3.314 | Reg loss: 0.043 | Tree loss: 3.314 | Accuracy: 0.158203 | 6.84 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 028 | Total loss: 3.262 | Reg loss: 0.043 | Tree loss: 3.262 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 028 | Total loss: 3.198 | Reg loss: 0.043 | Tree loss: 3.198 | Accuracy: 0.203125 | 6.841 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 028 | Total loss: 3.223 | Reg loss: 0.043 | Tree loss: 3.223 | Accuracy: 0.175781 | 6.842 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 028 | Total loss: 3.222 | Reg loss: 0.043 | Tree loss: 3.222 | Accuracy: 0.183594 | 6.842 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 028 | Total loss: 3.193 | Reg loss: 0.043 | Tree loss: 3.193 | Accuracy: 0.183594 | 6.843 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 028 | Total loss: 3.168 | Reg loss: 0.043 | Tree loss: 3.168 | Accuracy: 0.158203 | 6.843 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 015 / 028 | Total loss: 3.160 | Reg loss: 0.043 | Tree loss: 3.160 | Accuracy: 0.162109 | 6.844 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 028 | Total loss: 3.178 | Reg loss: 0.043 | Tree loss: 3.178 | Accuracy: 0.183594 | 6.844 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 028 | Total loss: 3.142 | Reg loss: 0.043 | Tree loss: 3.142 | Accuracy: 0.183594 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 028 | Total loss: 3.110 | Reg loss: 0.043 | Tree loss: 3.110 | Accuracy: 0.199219 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 028 | Total loss: 3.121 | Reg loss: 0.043 | Tree loss: 3.121 | Accuracy: 0.199219 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 028 | Total loss: 3.056 | Reg loss: 0.043 | Tree loss: 3.056 | Accuracy: 0.156250 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 028 | Total loss: 3.062 | Reg loss: 0.043 | Tree loss: 3.062 | Accuracy: 0.201172 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 028 | Total loss: 3.087 | Reg loss: 0.043 | Tree loss: 3.087 | Accuracy: 0.152344 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 028 | Total loss: 3.033 | Reg loss: 0.043 | Tree loss: 3.033 | Accuracy: 0.183594 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 028 | Total loss: 3.103 | Reg loss: 0.044 | Tree loss: 3.103 | Accuracy: 0.185547 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 028 | Total loss: 2.980 | Reg loss: 0.044 | Tree loss: 2.980 | Accuracy: 0.187500 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 028 | Total loss: 3.022 | Reg loss: 0.044 | Tree loss: 3.022 | Accuracy: 0.183594 | 6.845 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 028 | Total loss: 3.342 | Reg loss: 0.044 | Tree loss: 3.342 | Accuracy: 0.125000 | 6.841 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 028 | Total loss: 3.336 | Reg loss: 0.043 | Tree loss: 3.336 | Accuracy: 0.162109 | 6.846 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 028 | Total loss: 3.270 | Reg loss: 0.043 | Tree loss: 3.270 | Accuracy: 0.205078 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 028 | Total loss: 3.278 | Reg loss: 0.043 | Tree loss: 3.278 | Accuracy: 0.167969 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 028 | Total loss: 3.297 | Reg loss: 0.043 | Tree loss: 3.297 | Accuracy: 0.162109 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 028 | Total loss: 3.207 | Reg loss: 0.043 | Tree loss: 3.207 | Accuracy: 0.201172 | 6.849 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 028 | Total loss: 3.176 | Reg loss: 0.043 | Tree loss: 3.176 | Accuracy: 0.167969 | 6.849 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 028 | Total loss: 3.172 | Reg loss: 0.043 | Tree loss: 3.172 | Accuracy: 0.189453 | 6.85 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 028 | Total loss: 3.193 | Reg loss: 0.043 | Tree loss: 3.193 | Accuracy: 0.156250 | 6.85 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 028 | Total loss: 3.190 | Reg loss: 0.043 | Tree loss: 3.190 | Accuracy: 0.166016 | 6.85 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 028 | Total loss: 3.154 | Reg loss: 0.043 | Tree loss: 3.154 | Accuracy: 0.179688 | 6.849 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 028 | Total loss: 3.182 | Reg loss: 0.043 | Tree loss: 3.182 | Accuracy: 0.177734 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 028 | Total loss: 3.088 | Reg loss: 0.043 | Tree loss: 3.088 | Accuracy: 0.199219 | 6.845 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 028 | Total loss: 3.084 | Reg loss: 0.043 | Tree loss: 3.084 | Accuracy: 0.169922 | 6.843 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 028 | Total loss: 3.101 | Reg loss: 0.043 | Tree loss: 3.101 | Accuracy: 0.167969 | 6.844 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 028 | Total loss: 3.128 | Reg loss: 0.043 | Tree loss: 3.128 | Accuracy: 0.160156 | 6.845 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 028 | Total loss: 3.110 | Reg loss: 0.043 | Tree loss: 3.110 | Accuracy: 0.240234 | 6.845 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 028 | Total loss: 3.054 | Reg loss: 0.043 | Tree loss: 3.054 | Accuracy: 0.201172 | 6.846 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 028 | Total loss: 3.020 | Reg loss: 0.043 | Tree loss: 3.020 | Accuracy: 0.146484 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 028 | Total loss: 3.087 | Reg loss: 0.043 | Tree loss: 3.087 | Accuracy: 0.169922 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 028 | Total loss: 3.016 | Reg loss: 0.043 | Tree loss: 3.016 | Accuracy: 0.177734 | 6.847 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 028 | Total loss: 3.046 | Reg loss: 0.043 | Tree loss: 3.046 | Accuracy: 0.189453 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 028 | Total loss: 3.031 | Reg loss: 0.043 | Tree loss: 3.031 | Accuracy: 0.169922 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 028 | Total loss: 2.998 | Reg loss: 0.043 | Tree loss: 2.998 | Accuracy: 0.162109 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 028 | Total loss: 2.993 | Reg loss: 0.043 | Tree loss: 2.993 | Accuracy: 0.185547 | 6.849 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 028 | Total loss: 2.950 | Reg loss: 0.044 | Tree loss: 2.950 | Accuracy: 0.181641 | 6.849 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 028 | Total loss: 2.963 | Reg loss: 0.044 | Tree loss: 2.963 | Accuracy: 0.179688 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 028 | Total loss: 2.930 | Reg loss: 0.044 | Tree loss: 2.930 | Accuracy: 0.203125 | 6.848 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 028 | Total loss: 3.249 | Reg loss: 0.044 | Tree loss: 3.249 | Accuracy: 0.125000 | 6.844 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 028 | Total loss: 3.198 | Reg loss: 0.043 | Tree loss: 3.198 | Accuracy: 0.181641 | 6.849 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 028 | Total loss: 3.153 | Reg loss: 0.043 | Tree loss: 3.153 | Accuracy: 0.166016 | 6.85 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 028 | Total loss: 3.249 | Reg loss: 0.043 | Tree loss: 3.249 | Accuracy: 0.162109 | 6.85 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 028 | Total loss: 3.172 | Reg loss: 0.043 | Tree loss: 3.172 | Accuracy: 0.148438 | 6.85 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 028 | Total loss: 3.159 | Reg loss: 0.043 | Tree loss: 3.159 | Accuracy: 0.179688 | 6.851 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 028 | Total loss: 3.130 | Reg loss: 0.043 | Tree loss: 3.130 | Accuracy: 0.224609 | 6.851 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 028 | Total loss: 3.116 | Reg loss: 0.043 | Tree loss: 3.116 | Accuracy: 0.191406 | 6.852 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 028 | Total loss: 3.153 | Reg loss: 0.043 | Tree loss: 3.153 | Accuracy: 0.179688 | 6.853 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 028 | Total loss: 3.080 | Reg loss: 0.043 | Tree loss: 3.080 | Accuracy: 0.185547 | 6.853 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 028 | Total loss: 3.074 | Reg loss: 0.043 | Tree loss: 3.074 | Accuracy: 0.181641 | 6.853 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 028 | Total loss: 3.074 | Reg loss: 0.043 | Tree loss: 3.074 | Accuracy: 0.175781 | 6.854 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 028 | Total loss: 3.115 | Reg loss: 0.043 | Tree loss: 3.115 | Accuracy: 0.166016 | 6.854 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 028 | Total loss: 3.044 | Reg loss: 0.043 | Tree loss: 3.044 | Accuracy: 0.177734 | 6.853 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 028 | Total loss: 3.016 | Reg loss: 0.043 | Tree loss: 3.016 | Accuracy: 0.183594 | 6.851 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 028 | Total loss: 2.987 | Reg loss: 0.043 | Tree loss: 2.987 | Accuracy: 0.201172 | 6.849 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 028 | Total loss: 3.036 | Reg loss: 0.043 | Tree loss: 3.036 | Accuracy: 0.164062 | 6.847 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 028 | Total loss: 2.960 | Reg loss: 0.043 | Tree loss: 2.960 | Accuracy: 0.195312 | 6.845 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 028 | Total loss: 2.948 | Reg loss: 0.043 | Tree loss: 2.948 | Accuracy: 0.171875 | 6.843 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 028 | Total loss: 2.960 | Reg loss: 0.043 | Tree loss: 2.960 | Accuracy: 0.166016 | 6.841 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 028 | Total loss: 2.928 | Reg loss: 0.043 | Tree loss: 2.928 | Accuracy: 0.189453 | 6.838 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 020 / 028 | Total loss: 2.940 | Reg loss: 0.043 | Tree loss: 2.940 | Accuracy: 0.181641 | 6.838 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 028 | Total loss: 2.913 | Reg loss: 0.043 | Tree loss: 2.913 | Accuracy: 0.169922 | 6.838 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 028 | Total loss: 2.907 | Reg loss: 0.043 | Tree loss: 2.907 | Accuracy: 0.181641 | 6.839 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 028 | Total loss: 2.857 | Reg loss: 0.044 | Tree loss: 2.857 | Accuracy: 0.193359 | 6.839 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 028 | Total loss: 2.887 | Reg loss: 0.044 | Tree loss: 2.887 | Accuracy: 0.175781 | 6.839 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 028 | Total loss: 2.879 | Reg loss: 0.044 | Tree loss: 2.879 | Accuracy: 0.164062 | 6.839 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 028 | Total loss: 2.914 | Reg loss: 0.044 | Tree loss: 2.914 | Accuracy: 0.177734 | 6.839 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 028 | Total loss: 2.591 | Reg loss: 0.044 | Tree loss: 2.591 | Accuracy: 0.250000 | 6.836 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 028 | Total loss: 3.149 | Reg loss: 0.043 | Tree loss: 3.149 | Accuracy: 0.193359 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 028 | Total loss: 3.096 | Reg loss: 0.043 | Tree loss: 3.096 | Accuracy: 0.205078 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 028 | Total loss: 3.152 | Reg loss: 0.043 | Tree loss: 3.152 | Accuracy: 0.189453 | 6.848 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 028 | Total loss: 3.115 | Reg loss: 0.043 | Tree loss: 3.115 | Accuracy: 0.185547 | 6.848 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 028 | Total loss: 3.090 | Reg loss: 0.043 | Tree loss: 3.090 | Accuracy: 0.181641 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 028 | Total loss: 3.089 | Reg loss: 0.043 | Tree loss: 3.089 | Accuracy: 0.171875 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 028 | Total loss: 3.020 | Reg loss: 0.043 | Tree loss: 3.020 | Accuracy: 0.167969 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 028 | Total loss: 3.043 | Reg loss: 0.043 | Tree loss: 3.043 | Accuracy: 0.162109 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 028 | Total loss: 3.018 | Reg loss: 0.043 | Tree loss: 3.018 | Accuracy: 0.195312 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 028 | Total loss: 2.998 | Reg loss: 0.043 | Tree loss: 2.998 | Accuracy: 0.187500 | 6.846 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 028 | Total loss: 2.983 | Reg loss: 0.043 | Tree loss: 2.983 | Accuracy: 0.177734 | 6.846 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 028 | Total loss: 2.995 | Reg loss: 0.043 | Tree loss: 2.995 | Accuracy: 0.191406 | 6.846 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 028 | Total loss: 2.963 | Reg loss: 0.043 | Tree loss: 2.963 | Accuracy: 0.179688 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 028 | Total loss: 2.937 | Reg loss: 0.043 | Tree loss: 2.937 | Accuracy: 0.183594 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 028 | Total loss: 2.922 | Reg loss: 0.043 | Tree loss: 2.922 | Accuracy: 0.185547 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 028 | Total loss: 2.903 | Reg loss: 0.043 | Tree loss: 2.903 | Accuracy: 0.187500 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 028 | Total loss: 2.898 | Reg loss: 0.043 | Tree loss: 2.898 | Accuracy: 0.189453 | 6.848 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 028 | Total loss: 2.907 | Reg loss: 0.043 | Tree loss: 2.907 | Accuracy: 0.193359 | 6.848 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 028 | Total loss: 2.898 | Reg loss: 0.043 | Tree loss: 2.898 | Accuracy: 0.164062 | 6.848 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 028 | Total loss: 2.865 | Reg loss: 0.043 | Tree loss: 2.865 | Accuracy: 0.175781 | 6.849 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 028 | Total loss: 2.897 | Reg loss: 0.043 | Tree loss: 2.897 | Accuracy: 0.171875 | 6.847 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 028 | Total loss: 2.858 | Reg loss: 0.043 | Tree loss: 2.858 | Accuracy: 0.152344 | 6.845 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 028 | Total loss: 2.839 | Reg loss: 0.043 | Tree loss: 2.839 | Accuracy: 0.166016 | 6.843 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 028 | Total loss: 2.852 | Reg loss: 0.044 | Tree loss: 2.852 | Accuracy: 0.183594 | 6.841 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 028 | Total loss: 2.863 | Reg loss: 0.044 | Tree loss: 2.863 | Accuracy: 0.171875 | 6.839 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 028 | Total loss: 2.818 | Reg loss: 0.044 | Tree loss: 2.818 | Accuracy: 0.166016 | 6.837 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 028 | Total loss: 2.795 | Reg loss: 0.044 | Tree loss: 2.795 | Accuracy: 0.160156 | 6.835 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 028 | Total loss: 2.628 | Reg loss: 0.044 | Tree loss: 2.628 | Accuracy: 0.312500 | 6.831 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 028 | Total loss: 3.088 | Reg loss: 0.043 | Tree loss: 3.088 | Accuracy: 0.195312 | 6.843 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 028 | Total loss: 3.064 | Reg loss: 0.043 | Tree loss: 3.064 | Accuracy: 0.187500 | 6.842 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 028 | Total loss: 2.988 | Reg loss: 0.043 | Tree loss: 2.988 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 028 | Total loss: 3.040 | Reg loss: 0.043 | Tree loss: 3.040 | Accuracy: 0.189453 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 028 | Total loss: 2.972 | Reg loss: 0.043 | Tree loss: 2.972 | Accuracy: 0.160156 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 028 | Total loss: 2.964 | Reg loss: 0.043 | Tree loss: 2.964 | Accuracy: 0.185547 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 028 | Total loss: 3.005 | Reg loss: 0.043 | Tree loss: 3.005 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 028 | Total loss: 2.939 | Reg loss: 0.043 | Tree loss: 2.939 | Accuracy: 0.203125 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 028 | Total loss: 2.916 | Reg loss: 0.043 | Tree loss: 2.916 | Accuracy: 0.207031 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 028 | Total loss: 2.961 | Reg loss: 0.043 | Tree loss: 2.961 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 028 | Total loss: 2.915 | Reg loss: 0.043 | Tree loss: 2.915 | Accuracy: 0.181641 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 028 | Total loss: 2.925 | Reg loss: 0.043 | Tree loss: 2.925 | Accuracy: 0.181641 | 6.839 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 028 | Total loss: 2.901 | Reg loss: 0.043 | Tree loss: 2.901 | Accuracy: 0.167969 | 6.839 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 028 | Total loss: 2.881 | Reg loss: 0.043 | Tree loss: 2.881 | Accuracy: 0.187500 | 6.839 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 028 | Total loss: 2.893 | Reg loss: 0.043 | Tree loss: 2.893 | Accuracy: 0.162109 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 028 | Total loss: 2.870 | Reg loss: 0.043 | Tree loss: 2.870 | Accuracy: 0.160156 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 028 | Total loss: 2.879 | Reg loss: 0.043 | Tree loss: 2.879 | Accuracy: 0.162109 | 6.84 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 028 | Total loss: 2.878 | Reg loss: 0.043 | Tree loss: 2.878 | Accuracy: 0.191406 | 6.841 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 028 | Total loss: 2.783 | Reg loss: 0.043 | Tree loss: 2.783 | Accuracy: 0.187500 | 6.841 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 028 | Total loss: 2.822 | Reg loss: 0.043 | Tree loss: 2.822 | Accuracy: 0.191406 | 6.841 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 028 | Total loss: 2.800 | Reg loss: 0.043 | Tree loss: 2.800 | Accuracy: 0.177734 | 6.841 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 028 | Total loss: 2.832 | Reg loss: 0.043 | Tree loss: 2.832 | Accuracy: 0.166016 | 6.842 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 028 | Total loss: 2.756 | Reg loss: 0.043 | Tree loss: 2.756 | Accuracy: 0.175781 | 6.842 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 028 | Total loss: 2.843 | Reg loss: 0.043 | Tree loss: 2.843 | Accuracy: 0.193359 | 6.842 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 028 | Total loss: 2.736 | Reg loss: 0.044 | Tree loss: 2.736 | Accuracy: 0.195312 | 6.842 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 025 / 028 | Total loss: 2.781 | Reg loss: 0.044 | Tree loss: 2.781 | Accuracy: 0.156250 | 6.843 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 028 | Total loss: 2.775 | Reg loss: 0.044 | Tree loss: 2.775 | Accuracy: 0.162109 | 6.843 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 028 | Total loss: 2.795 | Reg loss: 0.044 | Tree loss: 2.795 | Accuracy: 0.187500 | 6.839 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 028 | Total loss: 2.948 | Reg loss: 0.043 | Tree loss: 2.948 | Accuracy: 0.171875 | 6.848 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 028 | Total loss: 2.981 | Reg loss: 0.043 | Tree loss: 2.981 | Accuracy: 0.193359 | 6.848 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 028 | Total loss: 2.958 | Reg loss: 0.043 | Tree loss: 2.958 | Accuracy: 0.199219 | 6.849 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 028 | Total loss: 2.963 | Reg loss: 0.043 | Tree loss: 2.963 | Accuracy: 0.187500 | 6.847 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 028 | Total loss: 2.940 | Reg loss: 0.043 | Tree loss: 2.940 | Accuracy: 0.167969 | 6.845 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 028 | Total loss: 2.964 | Reg loss: 0.043 | Tree loss: 2.964 | Accuracy: 0.160156 | 6.843 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 028 | Total loss: 2.921 | Reg loss: 0.043 | Tree loss: 2.921 | Accuracy: 0.154297 | 6.841 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 028 | Total loss: 2.905 | Reg loss: 0.043 | Tree loss: 2.905 | Accuracy: 0.185547 | 6.839 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 028 | Total loss: 2.885 | Reg loss: 0.043 | Tree loss: 2.885 | Accuracy: 0.199219 | 6.837 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 028 | Total loss: 2.845 | Reg loss: 0.043 | Tree loss: 2.845 | Accuracy: 0.226562 | 6.835 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 028 | Total loss: 2.839 | Reg loss: 0.043 | Tree loss: 2.839 | Accuracy: 0.173828 | 6.833 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 028 | Total loss: 2.853 | Reg loss: 0.043 | Tree loss: 2.853 | Accuracy: 0.156250 | 6.831 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 028 | Total loss: 2.830 | Reg loss: 0.043 | Tree loss: 2.830 | Accuracy: 0.193359 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 028 | Total loss: 2.835 | Reg loss: 0.043 | Tree loss: 2.835 | Accuracy: 0.173828 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 028 | Total loss: 2.796 | Reg loss: 0.043 | Tree loss: 2.796 | Accuracy: 0.189453 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 028 | Total loss: 2.833 | Reg loss: 0.043 | Tree loss: 2.833 | Accuracy: 0.158203 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 028 | Total loss: 2.788 | Reg loss: 0.043 | Tree loss: 2.788 | Accuracy: 0.158203 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 028 | Total loss: 2.810 | Reg loss: 0.043 | Tree loss: 2.810 | Accuracy: 0.140625 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 028 | Total loss: 2.768 | Reg loss: 0.043 | Tree loss: 2.768 | Accuracy: 0.193359 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 028 | Total loss: 2.803 | Reg loss: 0.043 | Tree loss: 2.803 | Accuracy: 0.177734 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 028 | Total loss: 2.785 | Reg loss: 0.043 | Tree loss: 2.785 | Accuracy: 0.154297 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 028 | Total loss: 2.747 | Reg loss: 0.043 | Tree loss: 2.747 | Accuracy: 0.179688 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 028 | Total loss: 2.747 | Reg loss: 0.043 | Tree loss: 2.747 | Accuracy: 0.181641 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 028 | Total loss: 2.734 | Reg loss: 0.043 | Tree loss: 2.734 | Accuracy: 0.166016 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 028 | Total loss: 2.724 | Reg loss: 0.043 | Tree loss: 2.724 | Accuracy: 0.210938 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 028 | Total loss: 2.681 | Reg loss: 0.044 | Tree loss: 2.681 | Accuracy: 0.208984 | 6.829 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 028 | Total loss: 2.720 | Reg loss: 0.044 | Tree loss: 2.720 | Accuracy: 0.181641 | 6.83 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 028 | Total loss: 2.614 | Reg loss: 0.044 | Tree loss: 2.614 | Accuracy: 0.187500 | 6.826 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 028 | Total loss: 2.980 | Reg loss: 0.043 | Tree loss: 2.980 | Accuracy: 0.195312 | 6.83 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 028 | Total loss: 2.934 | Reg loss: 0.043 | Tree loss: 2.934 | Accuracy: 0.164062 | 6.831 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 028 | Total loss: 2.870 | Reg loss: 0.043 | Tree loss: 2.870 | Accuracy: 0.197266 | 6.831 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 028 | Total loss: 2.960 | Reg loss: 0.043 | Tree loss: 2.960 | Accuracy: 0.160156 | 6.832 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 028 | Total loss: 2.889 | Reg loss: 0.043 | Tree loss: 2.889 | Accuracy: 0.195312 | 6.832 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 028 | Total loss: 2.850 | Reg loss: 0.043 | Tree loss: 2.850 | Accuracy: 0.162109 | 6.833 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 028 | Total loss: 2.845 | Reg loss: 0.043 | Tree loss: 2.845 | Accuracy: 0.166016 | 6.833 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 028 | Total loss: 2.838 | Reg loss: 0.043 | Tree loss: 2.838 | Accuracy: 0.175781 | 6.834 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 028 | Total loss: 2.847 | Reg loss: 0.043 | Tree loss: 2.847 | Accuracy: 0.191406 | 6.834 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 028 | Total loss: 2.813 | Reg loss: 0.043 | Tree loss: 2.813 | Accuracy: 0.181641 | 6.835 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 028 | Total loss: 2.863 | Reg loss: 0.043 | Tree loss: 2.863 | Accuracy: 0.160156 | 6.835 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 028 | Total loss: 2.795 | Reg loss: 0.043 | Tree loss: 2.795 | Accuracy: 0.191406 | 6.835 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 028 | Total loss: 2.783 | Reg loss: 0.043 | Tree loss: 2.783 | Accuracy: 0.183594 | 6.836 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 028 | Total loss: 2.815 | Reg loss: 0.043 | Tree loss: 2.815 | Accuracy: 0.169922 | 6.834 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 028 | Total loss: 2.793 | Reg loss: 0.043 | Tree loss: 2.793 | Accuracy: 0.185547 | 6.832 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 028 | Total loss: 2.773 | Reg loss: 0.043 | Tree loss: 2.773 | Accuracy: 0.150391 | 6.83 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 028 | Total loss: 2.725 | Reg loss: 0.043 | Tree loss: 2.725 | Accuracy: 0.160156 | 6.828 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 028 | Total loss: 2.714 | Reg loss: 0.043 | Tree loss: 2.714 | Accuracy: 0.191406 | 6.826 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 028 | Total loss: 2.713 | Reg loss: 0.043 | Tree loss: 2.713 | Accuracy: 0.193359 | 6.825 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 028 | Total loss: 2.692 | Reg loss: 0.043 | Tree loss: 2.692 | Accuracy: 0.173828 | 6.823 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 028 | Total loss: 2.677 | Reg loss: 0.043 | Tree loss: 2.677 | Accuracy: 0.191406 | 6.821 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 028 | Total loss: 2.697 | Reg loss: 0.043 | Tree loss: 2.697 | Accuracy: 0.191406 | 6.819 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 028 | Total loss: 2.675 | Reg loss: 0.043 | Tree loss: 2.675 | Accuracy: 0.166016 | 6.818 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 028 | Total loss: 2.651 | Reg loss: 0.043 | Tree loss: 2.651 | Accuracy: 0.175781 | 6.818 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 028 | Total loss: 2.678 | Reg loss: 0.043 | Tree loss: 2.678 | Accuracy: 0.199219 | 6.818 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 028 | Total loss: 2.658 | Reg loss: 0.043 | Tree loss: 2.658 | Accuracy: 0.164062 | 6.818 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 028 | Total loss: 2.668 | Reg loss: 0.043 | Tree loss: 2.668 | Accuracy: 0.199219 | 6.818 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 028 | Total loss: 2.690 | Reg loss: 0.043 | Tree loss: 2.690 | Accuracy: 0.062500 | 6.815 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 028 | Total loss: 2.863 | Reg loss: 0.043 | Tree loss: 2.863 | Accuracy: 0.189453 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 028 | Total loss: 2.888 | Reg loss: 0.043 | Tree loss: 2.888 | Accuracy: 0.175781 | 6.821 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 028 | Total loss: 2.872 | Reg loss: 0.043 | Tree loss: 2.872 | Accuracy: 0.148438 | 6.82 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 028 | Total loss: 2.903 | Reg loss: 0.043 | Tree loss: 2.903 | Accuracy: 0.173828 | 6.818 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 028 | Total loss: 2.822 | Reg loss: 0.043 | Tree loss: 2.822 | Accuracy: 0.187500 | 6.819 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 028 | Total loss: 2.831 | Reg loss: 0.043 | Tree loss: 2.831 | Accuracy: 0.166016 | 6.819 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 028 | Total loss: 2.800 | Reg loss: 0.043 | Tree loss: 2.800 | Accuracy: 0.185547 | 6.82 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 028 | Total loss: 2.843 | Reg loss: 0.043 | Tree loss: 2.843 | Accuracy: 0.193359 | 6.82 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 028 | Total loss: 2.800 | Reg loss: 0.043 | Tree loss: 2.800 | Accuracy: 0.175781 | 6.821 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 028 | Total loss: 2.729 | Reg loss: 0.043 | Tree loss: 2.729 | Accuracy: 0.173828 | 6.821 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 028 | Total loss: 2.783 | Reg loss: 0.043 | Tree loss: 2.783 | Accuracy: 0.181641 | 6.821 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 028 | Total loss: 2.734 | Reg loss: 0.043 | Tree loss: 2.734 | Accuracy: 0.193359 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 028 | Total loss: 2.679 | Reg loss: 0.043 | Tree loss: 2.679 | Accuracy: 0.199219 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 028 | Total loss: 2.733 | Reg loss: 0.043 | Tree loss: 2.733 | Accuracy: 0.177734 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 028 | Total loss: 2.719 | Reg loss: 0.043 | Tree loss: 2.719 | Accuracy: 0.187500 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 028 | Total loss: 2.672 | Reg loss: 0.043 | Tree loss: 2.672 | Accuracy: 0.179688 | 6.823 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 028 | Total loss: 2.703 | Reg loss: 0.043 | Tree loss: 2.703 | Accuracy: 0.164062 | 6.823 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 028 | Total loss: 2.709 | Reg loss: 0.043 | Tree loss: 2.709 | Accuracy: 0.189453 | 6.823 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 028 | Total loss: 2.668 | Reg loss: 0.043 | Tree loss: 2.668 | Accuracy: 0.175781 | 6.823 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 028 | Total loss: 2.683 | Reg loss: 0.043 | Tree loss: 2.683 | Accuracy: 0.160156 | 6.823 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 028 | Total loss: 2.674 | Reg loss: 0.043 | Tree loss: 2.674 | Accuracy: 0.199219 | 6.824 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 028 | Total loss: 2.657 | Reg loss: 0.043 | Tree loss: 2.657 | Accuracy: 0.169922 | 6.824 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 028 | Total loss: 2.675 | Reg loss: 0.043 | Tree loss: 2.675 | Accuracy: 0.203125 | 6.822 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 028 | Total loss: 2.653 | Reg loss: 0.043 | Tree loss: 2.653 | Accuracy: 0.183594 | 6.821 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 028 | Total loss: 2.631 | Reg loss: 0.043 | Tree loss: 2.631 | Accuracy: 0.177734 | 6.819 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 028 | Total loss: 2.606 | Reg loss: 0.043 | Tree loss: 2.606 | Accuracy: 0.214844 | 6.818 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 028 | Total loss: 2.617 | Reg loss: 0.043 | Tree loss: 2.617 | Accuracy: 0.154297 | 6.816 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 028 | Total loss: 2.759 | Reg loss: 0.043 | Tree loss: 2.759 | Accuracy: 0.250000 | 6.813 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 028 | Total loss: 2.813 | Reg loss: 0.043 | Tree loss: 2.813 | Accuracy: 0.193359 | 6.827 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 028 | Total loss: 2.824 | Reg loss: 0.043 | Tree loss: 2.824 | Accuracy: 0.179688 | 6.827 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 028 | Total loss: 2.814 | Reg loss: 0.043 | Tree loss: 2.814 | Accuracy: 0.177734 | 6.826 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 028 | Total loss: 2.835 | Reg loss: 0.043 | Tree loss: 2.835 | Accuracy: 0.167969 | 6.824 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 028 | Total loss: 2.789 | Reg loss: 0.043 | Tree loss: 2.789 | Accuracy: 0.197266 | 6.823 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 028 | Total loss: 2.756 | Reg loss: 0.043 | Tree loss: 2.756 | Accuracy: 0.167969 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 028 | Total loss: 2.755 | Reg loss: 0.043 | Tree loss: 2.755 | Accuracy: 0.199219 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 028 | Total loss: 2.770 | Reg loss: 0.043 | Tree loss: 2.770 | Accuracy: 0.160156 | 6.817 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 028 | Total loss: 2.759 | Reg loss: 0.043 | Tree loss: 2.759 | Accuracy: 0.203125 | 6.818 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 028 | Total loss: 2.746 | Reg loss: 0.043 | Tree loss: 2.746 | Accuracy: 0.162109 | 6.818 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 028 | Total loss: 2.686 | Reg loss: 0.043 | Tree loss: 2.686 | Accuracy: 0.201172 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 028 | Total loss: 2.729 | Reg loss: 0.043 | Tree loss: 2.729 | Accuracy: 0.191406 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 028 | Total loss: 2.675 | Reg loss: 0.043 | Tree loss: 2.675 | Accuracy: 0.195312 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 028 | Total loss: 2.733 | Reg loss: 0.043 | Tree loss: 2.733 | Accuracy: 0.191406 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 028 | Total loss: 2.665 | Reg loss: 0.043 | Tree loss: 2.665 | Accuracy: 0.189453 | 6.82 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 028 | Total loss: 2.666 | Reg loss: 0.043 | Tree loss: 2.666 | Accuracy: 0.201172 | 6.82 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 028 | Total loss: 2.622 | Reg loss: 0.043 | Tree loss: 2.622 | Accuracy: 0.185547 | 6.82 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 028 | Total loss: 2.688 | Reg loss: 0.043 | Tree loss: 2.688 | Accuracy: 0.185547 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 028 | Total loss: 2.639 | Reg loss: 0.043 | Tree loss: 2.639 | Accuracy: 0.144531 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 028 | Total loss: 2.612 | Reg loss: 0.043 | Tree loss: 2.612 | Accuracy: 0.183594 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 028 | Total loss: 2.675 | Reg loss: 0.043 | Tree loss: 2.675 | Accuracy: 0.162109 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 028 | Total loss: 2.635 | Reg loss: 0.043 | Tree loss: 2.635 | Accuracy: 0.191406 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 028 | Total loss: 2.640 | Reg loss: 0.043 | Tree loss: 2.640 | Accuracy: 0.160156 | 6.822 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 028 | Total loss: 2.572 | Reg loss: 0.043 | Tree loss: 2.572 | Accuracy: 0.191406 | 6.822 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 028 | Total loss: 2.600 | Reg loss: 0.043 | Tree loss: 2.600 | Accuracy: 0.199219 | 6.822 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 028 | Total loss: 2.555 | Reg loss: 0.043 | Tree loss: 2.555 | Accuracy: 0.179688 | 6.821 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 028 | Total loss: 2.593 | Reg loss: 0.043 | Tree loss: 2.593 | Accuracy: 0.158203 | 6.819 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 028 | Total loss: 2.539 | Reg loss: 0.043 | Tree loss: 2.539 | Accuracy: 0.062500 | 6.816 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 028 | Total loss: 2.740 | Reg loss: 0.043 | Tree loss: 2.740 | Accuracy: 0.179688 | 6.826 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 028 | Total loss: 2.758 | Reg loss: 0.043 | Tree loss: 2.758 | Accuracy: 0.203125 | 6.826 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 028 | Total loss: 2.743 | Reg loss: 0.043 | Tree loss: 2.743 | Accuracy: 0.197266 | 6.826 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 028 | Total loss: 2.766 | Reg loss: 0.043 | Tree loss: 2.766 | Accuracy: 0.197266 | 6.827 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 028 | Total loss: 2.755 | Reg loss: 0.043 | Tree loss: 2.755 | Accuracy: 0.173828 | 6.827 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 005 / 028 | Total loss: 2.722 | Reg loss: 0.043 | Tree loss: 2.722 | Accuracy: 0.193359 | 6.828 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 028 | Total loss: 2.713 | Reg loss: 0.043 | Tree loss: 2.713 | Accuracy: 0.193359 | 6.828 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 028 | Total loss: 2.742 | Reg loss: 0.043 | Tree loss: 2.742 | Accuracy: 0.181641 | 6.826 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 028 | Total loss: 2.731 | Reg loss: 0.043 | Tree loss: 2.731 | Accuracy: 0.166016 | 6.827 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 028 | Total loss: 2.721 | Reg loss: 0.043 | Tree loss: 2.721 | Accuracy: 0.142578 | 6.828 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 028 | Total loss: 2.665 | Reg loss: 0.043 | Tree loss: 2.665 | Accuracy: 0.193359 | 6.828 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 028 | Total loss: 2.674 | Reg loss: 0.043 | Tree loss: 2.674 | Accuracy: 0.164062 | 6.828 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 028 | Total loss: 2.636 | Reg loss: 0.043 | Tree loss: 2.636 | Accuracy: 0.193359 | 6.829 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 028 | Total loss: 2.686 | Reg loss: 0.043 | Tree loss: 2.686 | Accuracy: 0.193359 | 6.829 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 028 | Total loss: 2.631 | Reg loss: 0.043 | Tree loss: 2.631 | Accuracy: 0.158203 | 6.83 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 028 | Total loss: 2.656 | Reg loss: 0.043 | Tree loss: 2.656 | Accuracy: 0.181641 | 6.83 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 028 | Total loss: 2.661 | Reg loss: 0.043 | Tree loss: 2.661 | Accuracy: 0.169922 | 6.831 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 028 | Total loss: 2.613 | Reg loss: 0.043 | Tree loss: 2.613 | Accuracy: 0.181641 | 6.831 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 028 | Total loss: 2.611 | Reg loss: 0.043 | Tree loss: 2.611 | Accuracy: 0.181641 | 6.831 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 028 | Total loss: 2.580 | Reg loss: 0.043 | Tree loss: 2.580 | Accuracy: 0.201172 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 028 | Total loss: 2.605 | Reg loss: 0.043 | Tree loss: 2.605 | Accuracy: 0.169922 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 028 | Total loss: 2.602 | Reg loss: 0.043 | Tree loss: 2.602 | Accuracy: 0.156250 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 028 | Total loss: 2.612 | Reg loss: 0.043 | Tree loss: 2.612 | Accuracy: 0.197266 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 028 | Total loss: 2.573 | Reg loss: 0.043 | Tree loss: 2.573 | Accuracy: 0.179688 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 028 | Total loss: 2.563 | Reg loss: 0.043 | Tree loss: 2.563 | Accuracy: 0.169922 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 028 | Total loss: 2.534 | Reg loss: 0.043 | Tree loss: 2.534 | Accuracy: 0.193359 | 6.832 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 028 | Total loss: 2.529 | Reg loss: 0.043 | Tree loss: 2.529 | Accuracy: 0.181641 | 6.831 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 028 | Total loss: 2.647 | Reg loss: 0.043 | Tree loss: 2.647 | Accuracy: 0.312500 | 6.828 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 028 | Total loss: 2.797 | Reg loss: 0.042 | Tree loss: 2.797 | Accuracy: 0.158203 | 6.835 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 028 | Total loss: 2.705 | Reg loss: 0.042 | Tree loss: 2.705 | Accuracy: 0.199219 | 6.836 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 028 | Total loss: 2.741 | Reg loss: 0.042 | Tree loss: 2.741 | Accuracy: 0.220703 | 6.836 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 028 | Total loss: 2.736 | Reg loss: 0.042 | Tree loss: 2.736 | Accuracy: 0.164062 | 6.836 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 028 | Total loss: 2.699 | Reg loss: 0.042 | Tree loss: 2.699 | Accuracy: 0.183594 | 6.837 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 028 | Total loss: 2.706 | Reg loss: 0.042 | Tree loss: 2.706 | Accuracy: 0.144531 | 6.837 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 028 | Total loss: 2.672 | Reg loss: 0.042 | Tree loss: 2.672 | Accuracy: 0.197266 | 6.837 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 028 | Total loss: 2.722 | Reg loss: 0.042 | Tree loss: 2.722 | Accuracy: 0.144531 | 6.836 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 028 | Total loss: 2.646 | Reg loss: 0.042 | Tree loss: 2.646 | Accuracy: 0.179688 | 6.835 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 028 | Total loss: 2.638 | Reg loss: 0.042 | Tree loss: 2.638 | Accuracy: 0.193359 | 6.833 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 028 | Total loss: 2.628 | Reg loss: 0.042 | Tree loss: 2.628 | Accuracy: 0.197266 | 6.831 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 028 | Total loss: 2.665 | Reg loss: 0.042 | Tree loss: 2.665 | Accuracy: 0.183594 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 028 | Total loss: 2.641 | Reg loss: 0.042 | Tree loss: 2.641 | Accuracy: 0.169922 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 028 | Total loss: 2.624 | Reg loss: 0.043 | Tree loss: 2.624 | Accuracy: 0.183594 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 028 | Total loss: 2.621 | Reg loss: 0.043 | Tree loss: 2.621 | Accuracy: 0.166016 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 028 | Total loss: 2.599 | Reg loss: 0.043 | Tree loss: 2.599 | Accuracy: 0.173828 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 028 | Total loss: 2.584 | Reg loss: 0.043 | Tree loss: 2.584 | Accuracy: 0.208984 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 028 | Total loss: 2.610 | Reg loss: 0.043 | Tree loss: 2.610 | Accuracy: 0.185547 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 028 | Total loss: 2.573 | Reg loss: 0.043 | Tree loss: 2.573 | Accuracy: 0.208984 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 028 | Total loss: 2.509 | Reg loss: 0.043 | Tree loss: 2.509 | Accuracy: 0.210938 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 028 | Total loss: 2.561 | Reg loss: 0.043 | Tree loss: 2.561 | Accuracy: 0.187500 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 028 | Total loss: 2.531 | Reg loss: 0.043 | Tree loss: 2.531 | Accuracy: 0.201172 | 6.83 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 028 | Total loss: 2.529 | Reg loss: 0.043 | Tree loss: 2.529 | Accuracy: 0.162109 | 6.831 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 028 | Total loss: 2.554 | Reg loss: 0.043 | Tree loss: 2.554 | Accuracy: 0.179688 | 6.831 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 028 | Total loss: 2.557 | Reg loss: 0.043 | Tree loss: 2.557 | Accuracy: 0.189453 | 6.831 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 028 | Total loss: 2.533 | Reg loss: 0.043 | Tree loss: 2.533 | Accuracy: 0.171875 | 6.831 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 028 | Total loss: 2.520 | Reg loss: 0.043 | Tree loss: 2.520 | Accuracy: 0.130859 | 6.832 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 028 | Total loss: 2.414 | Reg loss: 0.043 | Tree loss: 2.414 | Accuracy: 0.375000 | 6.829 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 028 | Total loss: 2.678 | Reg loss: 0.042 | Tree loss: 2.678 | Accuracy: 0.191406 | 6.834 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 028 | Total loss: 2.701 | Reg loss: 0.042 | Tree loss: 2.701 | Accuracy: 0.179688 | 6.834 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 028 | Total loss: 2.730 | Reg loss: 0.042 | Tree loss: 2.730 | Accuracy: 0.181641 | 6.835 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 028 | Total loss: 2.685 | Reg loss: 0.042 | Tree loss: 2.685 | Accuracy: 0.169922 | 6.835 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 028 | Total loss: 2.660 | Reg loss: 0.042 | Tree loss: 2.660 | Accuracy: 0.197266 | 6.835 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 028 | Total loss: 2.616 | Reg loss: 0.042 | Tree loss: 2.616 | Accuracy: 0.187500 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 028 | Total loss: 2.672 | Reg loss: 0.042 | Tree loss: 2.672 | Accuracy: 0.189453 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 028 | Total loss: 2.677 | Reg loss: 0.042 | Tree loss: 2.677 | Accuracy: 0.169922 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 028 | Total loss: 2.647 | Reg loss: 0.042 | Tree loss: 2.647 | Accuracy: 0.173828 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 028 | Total loss: 2.606 | Reg loss: 0.042 | Tree loss: 2.606 | Accuracy: 0.189453 | 6.836 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 028 | Total loss: 2.631 | Reg loss: 0.042 | Tree loss: 2.631 | Accuracy: 0.152344 | 6.837 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 028 | Total loss: 2.653 | Reg loss: 0.042 | Tree loss: 2.653 | Accuracy: 0.175781 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 028 | Total loss: 2.587 | Reg loss: 0.042 | Tree loss: 2.587 | Accuracy: 0.164062 | 6.836 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 028 | Total loss: 2.610 | Reg loss: 0.042 | Tree loss: 2.610 | Accuracy: 0.177734 | 6.834 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 028 | Total loss: 2.555 | Reg loss: 0.042 | Tree loss: 2.555 | Accuracy: 0.193359 | 6.832 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 028 | Total loss: 2.646 | Reg loss: 0.042 | Tree loss: 2.646 | Accuracy: 0.162109 | 6.831 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 028 | Total loss: 2.558 | Reg loss: 0.042 | Tree loss: 2.558 | Accuracy: 0.179688 | 6.829 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 028 | Total loss: 2.552 | Reg loss: 0.042 | Tree loss: 2.552 | Accuracy: 0.181641 | 6.827 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 028 | Total loss: 2.535 | Reg loss: 0.042 | Tree loss: 2.535 | Accuracy: 0.169922 | 6.826 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 028 | Total loss: 2.506 | Reg loss: 0.043 | Tree loss: 2.506 | Accuracy: 0.181641 | 6.824 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 028 | Total loss: 2.502 | Reg loss: 0.043 | Tree loss: 2.502 | Accuracy: 0.195312 | 6.822 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 028 | Total loss: 2.496 | Reg loss: 0.043 | Tree loss: 2.496 | Accuracy: 0.201172 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 028 | Total loss: 2.539 | Reg loss: 0.043 | Tree loss: 2.539 | Accuracy: 0.181641 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 028 | Total loss: 2.520 | Reg loss: 0.043 | Tree loss: 2.520 | Accuracy: 0.179688 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 028 | Total loss: 2.504 | Reg loss: 0.043 | Tree loss: 2.504 | Accuracy: 0.183594 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 028 | Total loss: 2.518 | Reg loss: 0.043 | Tree loss: 2.518 | Accuracy: 0.195312 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 028 | Total loss: 2.499 | Reg loss: 0.043 | Tree loss: 2.499 | Accuracy: 0.191406 | 6.821 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 028 | Total loss: 2.344 | Reg loss: 0.043 | Tree loss: 2.344 | Accuracy: 0.250000 | 6.818 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 028 | Total loss: 2.705 | Reg loss: 0.042 | Tree loss: 2.705 | Accuracy: 0.183594 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 028 | Total loss: 2.614 | Reg loss: 0.042 | Tree loss: 2.614 | Accuracy: 0.201172 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 028 | Total loss: 2.690 | Reg loss: 0.042 | Tree loss: 2.690 | Accuracy: 0.164062 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 028 | Total loss: 2.655 | Reg loss: 0.042 | Tree loss: 2.655 | Accuracy: 0.193359 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 028 | Total loss: 2.667 | Reg loss: 0.042 | Tree loss: 2.667 | Accuracy: 0.177734 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 028 | Total loss: 2.661 | Reg loss: 0.042 | Tree loss: 2.661 | Accuracy: 0.177734 | 6.826 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 028 | Total loss: 2.624 | Reg loss: 0.042 | Tree loss: 2.624 | Accuracy: 0.201172 | 6.826 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 028 | Total loss: 2.663 | Reg loss: 0.042 | Tree loss: 2.663 | Accuracy: 0.156250 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 028 | Total loss: 2.621 | Reg loss: 0.042 | Tree loss: 2.621 | Accuracy: 0.160156 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 028 | Total loss: 2.611 | Reg loss: 0.042 | Tree loss: 2.611 | Accuracy: 0.167969 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 028 | Total loss: 2.582 | Reg loss: 0.042 | Tree loss: 2.582 | Accuracy: 0.160156 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 028 | Total loss: 2.602 | Reg loss: 0.042 | Tree loss: 2.602 | Accuracy: 0.185547 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 028 | Total loss: 2.555 | Reg loss: 0.042 | Tree loss: 2.555 | Accuracy: 0.171875 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 028 | Total loss: 2.553 | Reg loss: 0.042 | Tree loss: 2.553 | Accuracy: 0.171875 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 028 | Total loss: 2.557 | Reg loss: 0.042 | Tree loss: 2.557 | Accuracy: 0.205078 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 028 | Total loss: 2.552 | Reg loss: 0.042 | Tree loss: 2.552 | Accuracy: 0.162109 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 028 | Total loss: 2.476 | Reg loss: 0.042 | Tree loss: 2.476 | Accuracy: 0.205078 | 6.826 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 028 | Total loss: 2.497 | Reg loss: 0.042 | Tree loss: 2.497 | Accuracy: 0.177734 | 6.826 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 028 | Total loss: 2.514 | Reg loss: 0.042 | Tree loss: 2.514 | Accuracy: 0.167969 | 6.826 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 028 | Total loss: 2.547 | Reg loss: 0.042 | Tree loss: 2.547 | Accuracy: 0.164062 | 6.827 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 028 | Total loss: 2.473 | Reg loss: 0.042 | Tree loss: 2.473 | Accuracy: 0.193359 | 6.827 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 028 | Total loss: 2.463 | Reg loss: 0.042 | Tree loss: 2.463 | Accuracy: 0.197266 | 6.827 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 028 | Total loss: 2.517 | Reg loss: 0.042 | Tree loss: 2.517 | Accuracy: 0.224609 | 6.827 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 028 | Total loss: 2.475 | Reg loss: 0.042 | Tree loss: 2.475 | Accuracy: 0.189453 | 6.825 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 028 | Total loss: 2.511 | Reg loss: 0.043 | Tree loss: 2.511 | Accuracy: 0.171875 | 6.824 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 028 | Total loss: 2.455 | Reg loss: 0.043 | Tree loss: 2.455 | Accuracy: 0.169922 | 6.823 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 028 | Total loss: 2.489 | Reg loss: 0.043 | Tree loss: 2.489 | Accuracy: 0.195312 | 6.823 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 028 | Total loss: 2.584 | Reg loss: 0.043 | Tree loss: 2.584 | Accuracy: 0.125000 | 6.82 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 028 | Total loss: 2.668 | Reg loss: 0.042 | Tree loss: 2.668 | Accuracy: 0.179688 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 028 | Total loss: 2.656 | Reg loss: 0.042 | Tree loss: 2.656 | Accuracy: 0.171875 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 028 | Total loss: 2.673 | Reg loss: 0.042 | Tree loss: 2.673 | Accuracy: 0.191406 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 028 | Total loss: 2.615 | Reg loss: 0.042 | Tree loss: 2.615 | Accuracy: 0.177734 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 028 | Total loss: 2.631 | Reg loss: 0.042 | Tree loss: 2.631 | Accuracy: 0.195312 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 028 | Total loss: 2.574 | Reg loss: 0.042 | Tree loss: 2.574 | Accuracy: 0.166016 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 028 | Total loss: 2.585 | Reg loss: 0.042 | Tree loss: 2.585 | Accuracy: 0.220703 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 028 | Total loss: 2.572 | Reg loss: 0.042 | Tree loss: 2.572 | Accuracy: 0.195312 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 028 | Total loss: 2.557 | Reg loss: 0.042 | Tree loss: 2.557 | Accuracy: 0.195312 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 028 | Total loss: 2.543 | Reg loss: 0.042 | Tree loss: 2.543 | Accuracy: 0.164062 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 028 | Total loss: 2.551 | Reg loss: 0.042 | Tree loss: 2.551 | Accuracy: 0.169922 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 028 | Total loss: 2.565 | Reg loss: 0.042 | Tree loss: 2.565 | Accuracy: 0.166016 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 028 | Total loss: 2.519 | Reg loss: 0.042 | Tree loss: 2.519 | Accuracy: 0.154297 | 6.828 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 028 | Total loss: 2.514 | Reg loss: 0.042 | Tree loss: 2.514 | Accuracy: 0.171875 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 028 | Total loss: 2.565 | Reg loss: 0.042 | Tree loss: 2.565 | Accuracy: 0.175781 | 6.829 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Batch: 015 / 028 | Total loss: 2.479 | Reg loss: 0.042 | Tree loss: 2.479 | Accuracy: 0.199219 | 6.829 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 028 | Total loss: 2.525 | Reg loss: 0.042 | Tree loss: 2.525 | Accuracy: 0.193359 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 028 | Total loss: 2.515 | Reg loss: 0.042 | Tree loss: 2.515 | Accuracy: 0.201172 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 028 | Total loss: 2.475 | Reg loss: 0.042 | Tree loss: 2.475 | Accuracy: 0.169922 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 028 | Total loss: 2.529 | Reg loss: 0.042 | Tree loss: 2.529 | Accuracy: 0.183594 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 028 | Total loss: 2.470 | Reg loss: 0.042 | Tree loss: 2.470 | Accuracy: 0.205078 | 6.831 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 028 | Total loss: 2.497 | Reg loss: 0.042 | Tree loss: 2.497 | Accuracy: 0.148438 | 6.831 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 028 | Total loss: 2.483 | Reg loss: 0.042 | Tree loss: 2.483 | Accuracy: 0.181641 | 6.831 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 028 | Total loss: 2.492 | Reg loss: 0.042 | Tree loss: 2.492 | Accuracy: 0.201172 | 6.831 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 028 | Total loss: 2.435 | Reg loss: 0.042 | Tree loss: 2.435 | Accuracy: 0.169922 | 6.832 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 028 | Total loss: 2.506 | Reg loss: 0.042 | Tree loss: 2.506 | Accuracy: 0.169922 | 6.831 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 028 | Total loss: 2.455 | Reg loss: 0.042 | Tree loss: 2.455 | Accuracy: 0.173828 | 6.83 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 028 | Total loss: 2.476 | Reg loss: 0.042 | Tree loss: 2.476 | Accuracy: 0.125000 | 6.827 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 028 | Total loss: 2.666 | Reg loss: 0.042 | Tree loss: 2.666 | Accuracy: 0.171875 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 028 | Total loss: 2.620 | Reg loss: 0.042 | Tree loss: 2.620 | Accuracy: 0.185547 | 6.838 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 028 | Total loss: 2.602 | Reg loss: 0.042 | Tree loss: 2.602 | Accuracy: 0.199219 | 6.838 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 028 | Total loss: 2.615 | Reg loss: 0.042 | Tree loss: 2.615 | Accuracy: 0.171875 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 028 | Total loss: 2.548 | Reg loss: 0.042 | Tree loss: 2.548 | Accuracy: 0.173828 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 028 | Total loss: 2.562 | Reg loss: 0.042 | Tree loss: 2.562 | Accuracy: 0.189453 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 028 | Total loss: 2.596 | Reg loss: 0.042 | Tree loss: 2.596 | Accuracy: 0.169922 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 028 | Total loss: 2.569 | Reg loss: 0.042 | Tree loss: 2.569 | Accuracy: 0.171875 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 028 | Total loss: 2.563 | Reg loss: 0.042 | Tree loss: 2.563 | Accuracy: 0.179688 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 028 | Total loss: 2.568 | Reg loss: 0.042 | Tree loss: 2.568 | Accuracy: 0.189453 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 028 | Total loss: 2.561 | Reg loss: 0.042 | Tree loss: 2.561 | Accuracy: 0.189453 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 028 | Total loss: 2.467 | Reg loss: 0.042 | Tree loss: 2.467 | Accuracy: 0.197266 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 028 | Total loss: 2.580 | Reg loss: 0.042 | Tree loss: 2.580 | Accuracy: 0.156250 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 028 | Total loss: 2.516 | Reg loss: 0.042 | Tree loss: 2.516 | Accuracy: 0.179688 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 028 | Total loss: 2.502 | Reg loss: 0.042 | Tree loss: 2.502 | Accuracy: 0.167969 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 028 | Total loss: 2.454 | Reg loss: 0.042 | Tree loss: 2.454 | Accuracy: 0.205078 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 028 | Total loss: 2.496 | Reg loss: 0.042 | Tree loss: 2.496 | Accuracy: 0.210938 | 6.839 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 028 | Total loss: 2.476 | Reg loss: 0.042 | Tree loss: 2.476 | Accuracy: 0.167969 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 028 | Total loss: 2.470 | Reg loss: 0.042 | Tree loss: 2.470 | Accuracy: 0.185547 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 028 | Total loss: 2.459 | Reg loss: 0.042 | Tree loss: 2.459 | Accuracy: 0.162109 | 6.84 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 028 | Total loss: 2.476 | Reg loss: 0.042 | Tree loss: 2.476 | Accuracy: 0.193359 | 6.841 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 028 | Total loss: 2.474 | Reg loss: 0.042 | Tree loss: 2.474 | Accuracy: 0.169922 | 6.841 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 028 | Total loss: 2.436 | Reg loss: 0.042 | Tree loss: 2.436 | Accuracy: 0.197266 | 6.841 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 028 | Total loss: 2.449 | Reg loss: 0.042 | Tree loss: 2.449 | Accuracy: 0.191406 | 6.841 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 028 | Total loss: 2.438 | Reg loss: 0.042 | Tree loss: 2.438 | Accuracy: 0.171875 | 6.842 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 028 | Total loss: 2.412 | Reg loss: 0.042 | Tree loss: 2.412 | Accuracy: 0.187500 | 6.842 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 028 | Total loss: 2.413 | Reg loss: 0.042 | Tree loss: 2.413 | Accuracy: 0.158203 | 6.842 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 028 | Total loss: 2.391 | Reg loss: 0.042 | Tree loss: 2.391 | Accuracy: 0.312500 | 6.839 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 028 | Total loss: 2.601 | Reg loss: 0.042 | Tree loss: 2.601 | Accuracy: 0.181641 | 6.846 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 028 | Total loss: 2.603 | Reg loss: 0.042 | Tree loss: 2.603 | Accuracy: 0.216797 | 6.846 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 028 | Total loss: 2.527 | Reg loss: 0.042 | Tree loss: 2.527 | Accuracy: 0.195312 | 6.845 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 028 | Total loss: 2.553 | Reg loss: 0.042 | Tree loss: 2.553 | Accuracy: 0.187500 | 6.846 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 028 | Total loss: 2.584 | Reg loss: 0.042 | Tree loss: 2.584 | Accuracy: 0.156250 | 6.846 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 028 | Total loss: 2.557 | Reg loss: 0.042 | Tree loss: 2.557 | Accuracy: 0.197266 | 6.847 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 028 | Total loss: 2.597 | Reg loss: 0.042 | Tree loss: 2.597 | Accuracy: 0.169922 | 6.847 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 028 | Total loss: 2.526 | Reg loss: 0.042 | Tree loss: 2.526 | Accuracy: 0.164062 | 6.848 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 028 | Total loss: 2.509 | Reg loss: 0.042 | Tree loss: 2.509 | Accuracy: 0.179688 | 6.848 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 028 | Total loss: 2.558 | Reg loss: 0.042 | Tree loss: 2.558 | Accuracy: 0.167969 | 6.848 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 028 | Total loss: 2.557 | Reg loss: 0.042 | Tree loss: 2.557 | Accuracy: 0.162109 | 6.849 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 028 | Total loss: 2.495 | Reg loss: 0.042 | Tree loss: 2.495 | Accuracy: 0.177734 | 6.849 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 028 | Total loss: 2.458 | Reg loss: 0.042 | Tree loss: 2.458 | Accuracy: 0.193359 | 6.85 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 028 | Total loss: 2.471 | Reg loss: 0.042 | Tree loss: 2.471 | Accuracy: 0.169922 | 6.85 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 028 | Total loss: 2.448 | Reg loss: 0.042 | Tree loss: 2.448 | Accuracy: 0.189453 | 6.85 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 028 | Total loss: 2.521 | Reg loss: 0.042 | Tree loss: 2.521 | Accuracy: 0.179688 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 028 | Total loss: 2.492 | Reg loss: 0.042 | Tree loss: 2.492 | Accuracy: 0.173828 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 028 | Total loss: 2.467 | Reg loss: 0.042 | Tree loss: 2.467 | Accuracy: 0.154297 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 028 | Total loss: 2.474 | Reg loss: 0.042 | Tree loss: 2.474 | Accuracy: 0.166016 | 6.852 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 028 | Total loss: 2.468 | Reg loss: 0.042 | Tree loss: 2.468 | Accuracy: 0.201172 | 6.852 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 020 / 028 | Total loss: 2.432 | Reg loss: 0.042 | Tree loss: 2.432 | Accuracy: 0.175781 | 6.852 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 028 | Total loss: 2.497 | Reg loss: 0.042 | Tree loss: 2.497 | Accuracy: 0.187500 | 6.852 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 028 | Total loss: 2.431 | Reg loss: 0.042 | Tree loss: 2.431 | Accuracy: 0.238281 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 028 | Total loss: 2.402 | Reg loss: 0.042 | Tree loss: 2.402 | Accuracy: 0.166016 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 028 | Total loss: 2.405 | Reg loss: 0.042 | Tree loss: 2.405 | Accuracy: 0.208984 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 028 | Total loss: 2.420 | Reg loss: 0.042 | Tree loss: 2.420 | Accuracy: 0.160156 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 028 | Total loss: 2.414 | Reg loss: 0.042 | Tree loss: 2.414 | Accuracy: 0.173828 | 6.851 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 028 | Total loss: 2.690 | Reg loss: 0.042 | Tree loss: 2.690 | Accuracy: 0.187500 | 6.848 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 028 | Total loss: 2.555 | Reg loss: 0.041 | Tree loss: 2.555 | Accuracy: 0.183594 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 028 | Total loss: 2.547 | Reg loss: 0.041 | Tree loss: 2.547 | Accuracy: 0.205078 | 6.852 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 028 | Total loss: 2.547 | Reg loss: 0.041 | Tree loss: 2.547 | Accuracy: 0.179688 | 6.852 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 028 | Total loss: 2.586 | Reg loss: 0.041 | Tree loss: 2.586 | Accuracy: 0.179688 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 028 | Total loss: 2.519 | Reg loss: 0.041 | Tree loss: 2.519 | Accuracy: 0.167969 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 028 | Total loss: 2.526 | Reg loss: 0.041 | Tree loss: 2.526 | Accuracy: 0.158203 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 028 | Total loss: 2.536 | Reg loss: 0.041 | Tree loss: 2.536 | Accuracy: 0.183594 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 028 | Total loss: 2.527 | Reg loss: 0.041 | Tree loss: 2.527 | Accuracy: 0.183594 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 028 | Total loss: 2.506 | Reg loss: 0.041 | Tree loss: 2.506 | Accuracy: 0.152344 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 028 | Total loss: 2.500 | Reg loss: 0.041 | Tree loss: 2.500 | Accuracy: 0.208984 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 028 | Total loss: 2.509 | Reg loss: 0.041 | Tree loss: 2.509 | Accuracy: 0.154297 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 028 | Total loss: 2.432 | Reg loss: 0.041 | Tree loss: 2.432 | Accuracy: 0.162109 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 028 | Total loss: 2.488 | Reg loss: 0.041 | Tree loss: 2.488 | Accuracy: 0.173828 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 028 | Total loss: 2.481 | Reg loss: 0.042 | Tree loss: 2.481 | Accuracy: 0.210938 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 028 | Total loss: 2.557 | Reg loss: 0.042 | Tree loss: 2.557 | Accuracy: 0.150391 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 028 | Total loss: 2.463 | Reg loss: 0.042 | Tree loss: 2.463 | Accuracy: 0.208984 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 028 | Total loss: 2.457 | Reg loss: 0.042 | Tree loss: 2.457 | Accuracy: 0.167969 | 6.849 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 028 | Total loss: 2.445 | Reg loss: 0.042 | Tree loss: 2.445 | Accuracy: 0.185547 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 028 | Total loss: 2.399 | Reg loss: 0.042 | Tree loss: 2.399 | Accuracy: 0.181641 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 028 | Total loss: 2.448 | Reg loss: 0.042 | Tree loss: 2.448 | Accuracy: 0.173828 | 6.85 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 028 | Total loss: 2.434 | Reg loss: 0.042 | Tree loss: 2.434 | Accuracy: 0.181641 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 028 | Total loss: 2.412 | Reg loss: 0.042 | Tree loss: 2.412 | Accuracy: 0.189453 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 028 | Total loss: 2.439 | Reg loss: 0.042 | Tree loss: 2.439 | Accuracy: 0.193359 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 028 | Total loss: 2.422 | Reg loss: 0.042 | Tree loss: 2.422 | Accuracy: 0.177734 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 028 | Total loss: 2.387 | Reg loss: 0.042 | Tree loss: 2.387 | Accuracy: 0.203125 | 6.851 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 028 | Total loss: 2.389 | Reg loss: 0.042 | Tree loss: 2.389 | Accuracy: 0.191406 | 6.852 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 028 | Total loss: 2.398 | Reg loss: 0.042 | Tree loss: 2.398 | Accuracy: 0.195312 | 6.852 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 028 | Total loss: 2.398 | Reg loss: 0.042 | Tree loss: 2.398 | Accuracy: 0.062500 | 6.849 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 028 | Total loss: 2.569 | Reg loss: 0.041 | Tree loss: 2.569 | Accuracy: 0.185547 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 028 | Total loss: 2.572 | Reg loss: 0.041 | Tree loss: 2.572 | Accuracy: 0.177734 | 6.86 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 028 | Total loss: 2.540 | Reg loss: 0.041 | Tree loss: 2.540 | Accuracy: 0.210938 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 028 | Total loss: 2.480 | Reg loss: 0.041 | Tree loss: 2.480 | Accuracy: 0.220703 | 6.86 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 028 | Total loss: 2.525 | Reg loss: 0.041 | Tree loss: 2.525 | Accuracy: 0.181641 | 6.858 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 028 | Total loss: 2.508 | Reg loss: 0.041 | Tree loss: 2.508 | Accuracy: 0.185547 | 6.859 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 028 | Total loss: 2.510 | Reg loss: 0.041 | Tree loss: 2.510 | Accuracy: 0.181641 | 6.859 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 028 | Total loss: 2.522 | Reg loss: 0.041 | Tree loss: 2.522 | Accuracy: 0.156250 | 6.86 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 028 | Total loss: 2.505 | Reg loss: 0.041 | Tree loss: 2.505 | Accuracy: 0.203125 | 6.86 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 028 | Total loss: 2.476 | Reg loss: 0.041 | Tree loss: 2.476 | Accuracy: 0.183594 | 6.86 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 028 | Total loss: 2.476 | Reg loss: 0.041 | Tree loss: 2.476 | Accuracy: 0.175781 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 028 | Total loss: 2.480 | Reg loss: 0.041 | Tree loss: 2.480 | Accuracy: 0.154297 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 028 | Total loss: 2.431 | Reg loss: 0.041 | Tree loss: 2.431 | Accuracy: 0.189453 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 028 | Total loss: 2.435 | Reg loss: 0.041 | Tree loss: 2.435 | Accuracy: 0.183594 | 6.861 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 028 | Total loss: 2.484 | Reg loss: 0.041 | Tree loss: 2.484 | Accuracy: 0.171875 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 028 | Total loss: 2.408 | Reg loss: 0.041 | Tree loss: 2.408 | Accuracy: 0.208984 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 028 | Total loss: 2.416 | Reg loss: 0.041 | Tree loss: 2.416 | Accuracy: 0.162109 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 028 | Total loss: 2.397 | Reg loss: 0.041 | Tree loss: 2.397 | Accuracy: 0.150391 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 028 | Total loss: 2.427 | Reg loss: 0.041 | Tree loss: 2.427 | Accuracy: 0.199219 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 028 | Total loss: 2.437 | Reg loss: 0.041 | Tree loss: 2.437 | Accuracy: 0.201172 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 028 | Total loss: 2.391 | Reg loss: 0.041 | Tree loss: 2.391 | Accuracy: 0.173828 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 028 | Total loss: 2.397 | Reg loss: 0.041 | Tree loss: 2.397 | Accuracy: 0.197266 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 028 | Total loss: 2.449 | Reg loss: 0.042 | Tree loss: 2.449 | Accuracy: 0.142578 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 028 | Total loss: 2.397 | Reg loss: 0.042 | Tree loss: 2.397 | Accuracy: 0.171875 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 028 | Total loss: 2.386 | Reg loss: 0.042 | Tree loss: 2.386 | Accuracy: 0.187500 | 6.862 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 025 / 028 | Total loss: 2.397 | Reg loss: 0.042 | Tree loss: 2.397 | Accuracy: 0.169922 | 6.862 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 028 | Total loss: 2.425 | Reg loss: 0.042 | Tree loss: 2.425 | Accuracy: 0.181641 | 6.863 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 028 | Total loss: 2.393 | Reg loss: 0.042 | Tree loss: 2.393 | Accuracy: 0.062500 | 6.86 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 028 | Total loss: 2.530 | Reg loss: 0.041 | Tree loss: 2.530 | Accuracy: 0.195312 | 6.87 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 028 | Total loss: 2.501 | Reg loss: 0.041 | Tree loss: 2.501 | Accuracy: 0.181641 | 6.869 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 028 | Total loss: 2.493 | Reg loss: 0.041 | Tree loss: 2.493 | Accuracy: 0.183594 | 6.87 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 028 | Total loss: 2.490 | Reg loss: 0.041 | Tree loss: 2.490 | Accuracy: 0.183594 | 6.869 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 028 | Total loss: 2.517 | Reg loss: 0.041 | Tree loss: 2.517 | Accuracy: 0.162109 | 6.869 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 028 | Total loss: 2.502 | Reg loss: 0.041 | Tree loss: 2.502 | Accuracy: 0.199219 | 6.87 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 028 | Total loss: 2.536 | Reg loss: 0.041 | Tree loss: 2.536 | Accuracy: 0.181641 | 6.871 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 028 | Total loss: 2.454 | Reg loss: 0.041 | Tree loss: 2.454 | Accuracy: 0.195312 | 6.871 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 028 | Total loss: 2.467 | Reg loss: 0.041 | Tree loss: 2.467 | Accuracy: 0.166016 | 6.871 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 028 | Total loss: 2.415 | Reg loss: 0.041 | Tree loss: 2.415 | Accuracy: 0.207031 | 6.872 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 028 | Total loss: 2.449 | Reg loss: 0.041 | Tree loss: 2.449 | Accuracy: 0.189453 | 6.872 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 028 | Total loss: 2.442 | Reg loss: 0.041 | Tree loss: 2.442 | Accuracy: 0.193359 | 6.872 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 028 | Total loss: 2.483 | Reg loss: 0.041 | Tree loss: 2.483 | Accuracy: 0.171875 | 6.873 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 028 | Total loss: 2.462 | Reg loss: 0.041 | Tree loss: 2.462 | Accuracy: 0.175781 | 6.873 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 028 | Total loss: 2.415 | Reg loss: 0.041 | Tree loss: 2.415 | Accuracy: 0.185547 | 6.873 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 028 | Total loss: 2.399 | Reg loss: 0.041 | Tree loss: 2.399 | Accuracy: 0.187500 | 6.873 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 028 | Total loss: 2.444 | Reg loss: 0.041 | Tree loss: 2.444 | Accuracy: 0.169922 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 028 | Total loss: 2.465 | Reg loss: 0.041 | Tree loss: 2.465 | Accuracy: 0.183594 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 028 | Total loss: 2.384 | Reg loss: 0.041 | Tree loss: 2.384 | Accuracy: 0.195312 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 028 | Total loss: 2.407 | Reg loss: 0.041 | Tree loss: 2.407 | Accuracy: 0.199219 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 028 | Total loss: 2.384 | Reg loss: 0.041 | Tree loss: 2.384 | Accuracy: 0.175781 | 6.875 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 028 | Total loss: 2.368 | Reg loss: 0.041 | Tree loss: 2.368 | Accuracy: 0.195312 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 028 | Total loss: 2.387 | Reg loss: 0.041 | Tree loss: 2.387 | Accuracy: 0.166016 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 028 | Total loss: 2.364 | Reg loss: 0.041 | Tree loss: 2.364 | Accuracy: 0.195312 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 028 | Total loss: 2.412 | Reg loss: 0.041 | Tree loss: 2.412 | Accuracy: 0.171875 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 028 | Total loss: 2.418 | Reg loss: 0.041 | Tree loss: 2.418 | Accuracy: 0.167969 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 028 | Total loss: 2.409 | Reg loss: 0.041 | Tree loss: 2.409 | Accuracy: 0.152344 | 6.874 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 028 | Total loss: 2.357 | Reg loss: 0.041 | Tree loss: 2.357 | Accuracy: 0.062500 | 6.872 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 028 | Total loss: 2.514 | Reg loss: 0.041 | Tree loss: 2.514 | Accuracy: 0.167969 | 6.88 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 028 | Total loss: 2.518 | Reg loss: 0.041 | Tree loss: 2.518 | Accuracy: 0.169922 | 6.879 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 028 | Total loss: 2.472 | Reg loss: 0.041 | Tree loss: 2.472 | Accuracy: 0.203125 | 6.878 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 028 | Total loss: 2.524 | Reg loss: 0.041 | Tree loss: 2.524 | Accuracy: 0.164062 | 6.876 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 028 | Total loss: 2.480 | Reg loss: 0.041 | Tree loss: 2.480 | Accuracy: 0.177734 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 028 | Total loss: 2.442 | Reg loss: 0.041 | Tree loss: 2.442 | Accuracy: 0.208984 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 028 | Total loss: 2.466 | Reg loss: 0.041 | Tree loss: 2.466 | Accuracy: 0.177734 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 028 | Total loss: 2.515 | Reg loss: 0.041 | Tree loss: 2.515 | Accuracy: 0.156250 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 028 | Total loss: 2.442 | Reg loss: 0.041 | Tree loss: 2.442 | Accuracy: 0.203125 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 028 | Total loss: 2.470 | Reg loss: 0.041 | Tree loss: 2.470 | Accuracy: 0.175781 | 6.874 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 028 | Total loss: 2.420 | Reg loss: 0.041 | Tree loss: 2.420 | Accuracy: 0.185547 | 6.874 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 028 | Total loss: 2.412 | Reg loss: 0.041 | Tree loss: 2.412 | Accuracy: 0.187500 | 6.874 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 028 | Total loss: 2.452 | Reg loss: 0.041 | Tree loss: 2.452 | Accuracy: 0.162109 | 6.874 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 028 | Total loss: 2.428 | Reg loss: 0.041 | Tree loss: 2.428 | Accuracy: 0.179688 | 6.874 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 028 | Total loss: 2.359 | Reg loss: 0.041 | Tree loss: 2.359 | Accuracy: 0.199219 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 028 | Total loss: 2.421 | Reg loss: 0.041 | Tree loss: 2.421 | Accuracy: 0.179688 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 028 | Total loss: 2.411 | Reg loss: 0.041 | Tree loss: 2.411 | Accuracy: 0.214844 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 028 | Total loss: 2.448 | Reg loss: 0.041 | Tree loss: 2.448 | Accuracy: 0.183594 | 6.875 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 028 | Total loss: 2.407 | Reg loss: 0.041 | Tree loss: 2.407 | Accuracy: 0.183594 | 6.876 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 028 | Total loss: 2.391 | Reg loss: 0.041 | Tree loss: 2.391 | Accuracy: 0.187500 | 6.876 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 028 | Total loss: 2.373 | Reg loss: 0.041 | Tree loss: 2.373 | Accuracy: 0.203125 | 6.876 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 028 | Total loss: 2.385 | Reg loss: 0.041 | Tree loss: 2.385 | Accuracy: 0.193359 | 6.876 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 028 | Total loss: 2.358 | Reg loss: 0.041 | Tree loss: 2.358 | Accuracy: 0.167969 | 6.877 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 028 | Total loss: 2.409 | Reg loss: 0.041 | Tree loss: 2.409 | Accuracy: 0.177734 | 6.877 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 028 | Total loss: 2.343 | Reg loss: 0.041 | Tree loss: 2.343 | Accuracy: 0.181641 | 6.877 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 028 | Total loss: 2.342 | Reg loss: 0.041 | Tree loss: 2.342 | Accuracy: 0.181641 | 6.877 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 028 | Total loss: 2.363 | Reg loss: 0.041 | Tree loss: 2.363 | Accuracy: 0.177734 | 6.877 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 028 | Total loss: 2.181 | Reg loss: 0.041 | Tree loss: 2.181 | Accuracy: 0.375000 | 6.875 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 028 | Total loss: 2.467 | Reg loss: 0.041 | Tree loss: 2.467 | Accuracy: 0.207031 | 6.892 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 028 | Total loss: 2.553 | Reg loss: 0.041 | Tree loss: 2.553 | Accuracy: 0.185547 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 028 | Total loss: 2.491 | Reg loss: 0.041 | Tree loss: 2.491 | Accuracy: 0.175781 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 028 | Total loss: 2.477 | Reg loss: 0.041 | Tree loss: 2.477 | Accuracy: 0.187500 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 028 | Total loss: 2.513 | Reg loss: 0.041 | Tree loss: 2.513 | Accuracy: 0.173828 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 028 | Total loss: 2.492 | Reg loss: 0.041 | Tree loss: 2.492 | Accuracy: 0.148438 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 028 | Total loss: 2.522 | Reg loss: 0.041 | Tree loss: 2.522 | Accuracy: 0.154297 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 028 | Total loss: 2.433 | Reg loss: 0.041 | Tree loss: 2.433 | Accuracy: 0.158203 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 028 | Total loss: 2.490 | Reg loss: 0.041 | Tree loss: 2.490 | Accuracy: 0.150391 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 028 | Total loss: 2.416 | Reg loss: 0.041 | Tree loss: 2.416 | Accuracy: 0.181641 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 028 | Total loss: 2.436 | Reg loss: 0.041 | Tree loss: 2.436 | Accuracy: 0.154297 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 028 | Total loss: 2.437 | Reg loss: 0.041 | Tree loss: 2.437 | Accuracy: 0.169922 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 028 | Total loss: 2.383 | Reg loss: 0.041 | Tree loss: 2.383 | Accuracy: 0.199219 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 028 | Total loss: 2.466 | Reg loss: 0.041 | Tree loss: 2.466 | Accuracy: 0.185547 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 028 | Total loss: 2.429 | Reg loss: 0.041 | Tree loss: 2.429 | Accuracy: 0.181641 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 028 | Total loss: 2.367 | Reg loss: 0.041 | Tree loss: 2.367 | Accuracy: 0.203125 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 028 | Total loss: 2.356 | Reg loss: 0.041 | Tree loss: 2.356 | Accuracy: 0.214844 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 028 | Total loss: 2.409 | Reg loss: 0.041 | Tree loss: 2.409 | Accuracy: 0.193359 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 028 | Total loss: 2.339 | Reg loss: 0.041 | Tree loss: 2.339 | Accuracy: 0.191406 | 6.89 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 028 | Total loss: 2.369 | Reg loss: 0.041 | Tree loss: 2.369 | Accuracy: 0.179688 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 028 | Total loss: 2.372 | Reg loss: 0.041 | Tree loss: 2.372 | Accuracy: 0.197266 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 028 | Total loss: 2.362 | Reg loss: 0.041 | Tree loss: 2.362 | Accuracy: 0.158203 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 028 | Total loss: 2.318 | Reg loss: 0.041 | Tree loss: 2.318 | Accuracy: 0.207031 | 6.891 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 028 | Total loss: 2.343 | Reg loss: 0.041 | Tree loss: 2.343 | Accuracy: 0.199219 | 6.892 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 028 | Total loss: 2.308 | Reg loss: 0.041 | Tree loss: 2.308 | Accuracy: 0.207031 | 6.892 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 028 | Total loss: 2.323 | Reg loss: 0.041 | Tree loss: 2.323 | Accuracy: 0.187500 | 6.892 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 028 | Total loss: 2.353 | Reg loss: 0.041 | Tree loss: 2.353 | Accuracy: 0.199219 | 6.892 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 028 | Total loss: 2.282 | Reg loss: 0.041 | Tree loss: 2.282 | Accuracy: 0.125000 | 6.889 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 028 | Total loss: 2.463 | Reg loss: 0.040 | Tree loss: 2.463 | Accuracy: 0.171875 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 028 | Total loss: 2.511 | Reg loss: 0.040 | Tree loss: 2.511 | Accuracy: 0.162109 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 028 | Total loss: 2.428 | Reg loss: 0.040 | Tree loss: 2.428 | Accuracy: 0.208984 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 028 | Total loss: 2.460 | Reg loss: 0.040 | Tree loss: 2.460 | Accuracy: 0.187500 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 028 | Total loss: 2.468 | Reg loss: 0.040 | Tree loss: 2.468 | Accuracy: 0.193359 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 028 | Total loss: 2.422 | Reg loss: 0.040 | Tree loss: 2.422 | Accuracy: 0.212891 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 028 | Total loss: 2.451 | Reg loss: 0.040 | Tree loss: 2.451 | Accuracy: 0.197266 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 028 | Total loss: 2.432 | Reg loss: 0.040 | Tree loss: 2.432 | Accuracy: 0.185547 | 6.898 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 028 | Total loss: 2.452 | Reg loss: 0.040 | Tree loss: 2.452 | Accuracy: 0.162109 | 6.898 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 028 | Total loss: 2.446 | Reg loss: 0.040 | Tree loss: 2.446 | Accuracy: 0.175781 | 6.898 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 028 | Total loss: 2.418 | Reg loss: 0.041 | Tree loss: 2.418 | Accuracy: 0.185547 | 6.898 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 028 | Total loss: 2.365 | Reg loss: 0.041 | Tree loss: 2.365 | Accuracy: 0.191406 | 6.898 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 028 | Total loss: 2.399 | Reg loss: 0.041 | Tree loss: 2.399 | Accuracy: 0.177734 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 028 | Total loss: 2.427 | Reg loss: 0.041 | Tree loss: 2.427 | Accuracy: 0.193359 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 028 | Total loss: 2.357 | Reg loss: 0.041 | Tree loss: 2.357 | Accuracy: 0.203125 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 028 | Total loss: 2.416 | Reg loss: 0.041 | Tree loss: 2.416 | Accuracy: 0.150391 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 028 | Total loss: 2.414 | Reg loss: 0.041 | Tree loss: 2.414 | Accuracy: 0.177734 | 6.899 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 028 | Total loss: 2.344 | Reg loss: 0.041 | Tree loss: 2.344 | Accuracy: 0.191406 | 6.9 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 028 | Total loss: 2.355 | Reg loss: 0.041 | Tree loss: 2.355 | Accuracy: 0.167969 | 6.9 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 028 | Total loss: 2.314 | Reg loss: 0.041 | Tree loss: 2.314 | Accuracy: 0.205078 | 6.9 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 028 | Total loss: 2.357 | Reg loss: 0.041 | Tree loss: 2.357 | Accuracy: 0.169922 | 6.9 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 028 | Total loss: 2.337 | Reg loss: 0.041 | Tree loss: 2.337 | Accuracy: 0.197266 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 028 | Total loss: 2.395 | Reg loss: 0.041 | Tree loss: 2.395 | Accuracy: 0.171875 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 028 | Total loss: 2.334 | Reg loss: 0.041 | Tree loss: 2.334 | Accuracy: 0.191406 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 028 | Total loss: 2.374 | Reg loss: 0.041 | Tree loss: 2.374 | Accuracy: 0.181641 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 028 | Total loss: 2.379 | Reg loss: 0.041 | Tree loss: 2.379 | Accuracy: 0.173828 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 028 | Total loss: 2.364 | Reg loss: 0.041 | Tree loss: 2.364 | Accuracy: 0.191406 | 6.901 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 028 | Total loss: 2.127 | Reg loss: 0.041 | Tree loss: 2.127 | Accuracy: 0.312500 | 6.899 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 028 | Total loss: 2.530 | Reg loss: 0.040 | Tree loss: 2.530 | Accuracy: 0.179688 | 6.908 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 028 | Total loss: 2.528 | Reg loss: 0.040 | Tree loss: 2.528 | Accuracy: 0.138672 | 6.906 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 028 | Total loss: 2.406 | Reg loss: 0.040 | Tree loss: 2.406 | Accuracy: 0.199219 | 6.906 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 028 | Total loss: 2.449 | Reg loss: 0.040 | Tree loss: 2.449 | Accuracy: 0.195312 | 6.907 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 028 | Total loss: 2.462 | Reg loss: 0.040 | Tree loss: 2.462 | Accuracy: 0.162109 | 6.907 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 005 / 028 | Total loss: 2.473 | Reg loss: 0.040 | Tree loss: 2.473 | Accuracy: 0.171875 | 6.907 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 028 | Total loss: 2.384 | Reg loss: 0.040 | Tree loss: 2.384 | Accuracy: 0.164062 | 6.908 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 028 | Total loss: 2.462 | Reg loss: 0.040 | Tree loss: 2.462 | Accuracy: 0.193359 | 6.908 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 028 | Total loss: 2.426 | Reg loss: 0.040 | Tree loss: 2.426 | Accuracy: 0.185547 | 6.908 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 028 | Total loss: 2.353 | Reg loss: 0.040 | Tree loss: 2.353 | Accuracy: 0.216797 | 6.908 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 028 | Total loss: 2.382 | Reg loss: 0.040 | Tree loss: 2.382 | Accuracy: 0.181641 | 6.909 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 028 | Total loss: 2.381 | Reg loss: 0.040 | Tree loss: 2.381 | Accuracy: 0.179688 | 6.909 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 028 | Total loss: 2.390 | Reg loss: 0.040 | Tree loss: 2.390 | Accuracy: 0.197266 | 6.909 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 028 | Total loss: 2.395 | Reg loss: 0.040 | Tree loss: 2.395 | Accuracy: 0.183594 | 6.909 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 028 | Total loss: 2.354 | Reg loss: 0.040 | Tree loss: 2.354 | Accuracy: 0.208984 | 6.909 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 028 | Total loss: 2.410 | Reg loss: 0.040 | Tree loss: 2.410 | Accuracy: 0.173828 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 028 | Total loss: 2.352 | Reg loss: 0.040 | Tree loss: 2.352 | Accuracy: 0.199219 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 028 | Total loss: 2.311 | Reg loss: 0.040 | Tree loss: 2.311 | Accuracy: 0.210938 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 028 | Total loss: 2.406 | Reg loss: 0.040 | Tree loss: 2.406 | Accuracy: 0.181641 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 028 | Total loss: 2.357 | Reg loss: 0.040 | Tree loss: 2.357 | Accuracy: 0.169922 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 028 | Total loss: 2.344 | Reg loss: 0.040 | Tree loss: 2.344 | Accuracy: 0.205078 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 028 | Total loss: 2.311 | Reg loss: 0.041 | Tree loss: 2.311 | Accuracy: 0.173828 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 028 | Total loss: 2.360 | Reg loss: 0.041 | Tree loss: 2.360 | Accuracy: 0.195312 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 028 | Total loss: 2.329 | Reg loss: 0.041 | Tree loss: 2.329 | Accuracy: 0.148438 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 028 | Total loss: 2.362 | Reg loss: 0.041 | Tree loss: 2.362 | Accuracy: 0.171875 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 028 | Total loss: 2.290 | Reg loss: 0.041 | Tree loss: 2.290 | Accuracy: 0.203125 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 028 | Total loss: 2.360 | Reg loss: 0.041 | Tree loss: 2.360 | Accuracy: 0.193359 | 6.91 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 028 | Total loss: 2.318 | Reg loss: 0.041 | Tree loss: 2.318 | Accuracy: 0.125000 | 6.907 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 028 | Total loss: 2.454 | Reg loss: 0.040 | Tree loss: 2.454 | Accuracy: 0.197266 | 6.915 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 028 | Total loss: 2.448 | Reg loss: 0.040 | Tree loss: 2.448 | Accuracy: 0.183594 | 6.913 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 028 | Total loss: 2.471 | Reg loss: 0.040 | Tree loss: 2.471 | Accuracy: 0.156250 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 028 | Total loss: 2.427 | Reg loss: 0.040 | Tree loss: 2.427 | Accuracy: 0.181641 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 028 | Total loss: 2.464 | Reg loss: 0.040 | Tree loss: 2.464 | Accuracy: 0.166016 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 028 | Total loss: 2.454 | Reg loss: 0.040 | Tree loss: 2.454 | Accuracy: 0.171875 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 028 | Total loss: 2.420 | Reg loss: 0.040 | Tree loss: 2.420 | Accuracy: 0.175781 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 028 | Total loss: 2.417 | Reg loss: 0.040 | Tree loss: 2.417 | Accuracy: 0.193359 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 028 | Total loss: 2.361 | Reg loss: 0.040 | Tree loss: 2.361 | Accuracy: 0.191406 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 028 | Total loss: 2.396 | Reg loss: 0.040 | Tree loss: 2.396 | Accuracy: 0.201172 | 6.91 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 028 | Total loss: 2.379 | Reg loss: 0.040 | Tree loss: 2.379 | Accuracy: 0.164062 | 6.91 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 028 | Total loss: 2.403 | Reg loss: 0.040 | Tree loss: 2.403 | Accuracy: 0.185547 | 6.91 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 028 | Total loss: 2.366 | Reg loss: 0.040 | Tree loss: 2.366 | Accuracy: 0.173828 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 028 | Total loss: 2.371 | Reg loss: 0.040 | Tree loss: 2.371 | Accuracy: 0.195312 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 028 | Total loss: 2.325 | Reg loss: 0.040 | Tree loss: 2.325 | Accuracy: 0.207031 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 028 | Total loss: 2.311 | Reg loss: 0.040 | Tree loss: 2.311 | Accuracy: 0.208984 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 028 | Total loss: 2.348 | Reg loss: 0.040 | Tree loss: 2.348 | Accuracy: 0.179688 | 6.911 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 028 | Total loss: 2.332 | Reg loss: 0.040 | Tree loss: 2.332 | Accuracy: 0.169922 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 028 | Total loss: 2.364 | Reg loss: 0.040 | Tree loss: 2.364 | Accuracy: 0.187500 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 028 | Total loss: 2.335 | Reg loss: 0.040 | Tree loss: 2.335 | Accuracy: 0.171875 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 028 | Total loss: 2.334 | Reg loss: 0.040 | Tree loss: 2.334 | Accuracy: 0.189453 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 028 | Total loss: 2.376 | Reg loss: 0.040 | Tree loss: 2.376 | Accuracy: 0.214844 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 028 | Total loss: 2.397 | Reg loss: 0.040 | Tree loss: 2.397 | Accuracy: 0.156250 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 028 | Total loss: 2.317 | Reg loss: 0.040 | Tree loss: 2.317 | Accuracy: 0.164062 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 028 | Total loss: 2.294 | Reg loss: 0.040 | Tree loss: 2.294 | Accuracy: 0.195312 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 028 | Total loss: 2.387 | Reg loss: 0.040 | Tree loss: 2.387 | Accuracy: 0.166016 | 6.912 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 028 | Total loss: 2.292 | Reg loss: 0.040 | Tree loss: 2.292 | Accuracy: 0.228516 | 6.913 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 028 | Total loss: 2.677 | Reg loss: 0.040 | Tree loss: 2.677 | Accuracy: 0.312500 | 6.91 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 028 | Total loss: 2.462 | Reg loss: 0.040 | Tree loss: 2.462 | Accuracy: 0.193359 | 6.923 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 028 | Total loss: 2.424 | Reg loss: 0.040 | Tree loss: 2.424 | Accuracy: 0.207031 | 6.922 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 028 | Total loss: 2.434 | Reg loss: 0.040 | Tree loss: 2.434 | Accuracy: 0.193359 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 028 | Total loss: 2.395 | Reg loss: 0.040 | Tree loss: 2.395 | Accuracy: 0.199219 | 6.919 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 028 | Total loss: 2.431 | Reg loss: 0.040 | Tree loss: 2.431 | Accuracy: 0.189453 | 6.919 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 028 | Total loss: 2.447 | Reg loss: 0.040 | Tree loss: 2.447 | Accuracy: 0.169922 | 6.919 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 028 | Total loss: 2.433 | Reg loss: 0.040 | Tree loss: 2.433 | Accuracy: 0.179688 | 6.919 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 028 | Total loss: 2.344 | Reg loss: 0.040 | Tree loss: 2.344 | Accuracy: 0.189453 | 6.919 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 028 | Total loss: 2.332 | Reg loss: 0.040 | Tree loss: 2.332 | Accuracy: 0.218750 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 028 | Total loss: 2.371 | Reg loss: 0.040 | Tree loss: 2.371 | Accuracy: 0.199219 | 6.92 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 010 / 028 | Total loss: 2.376 | Reg loss: 0.040 | Tree loss: 2.376 | Accuracy: 0.183594 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 028 | Total loss: 2.396 | Reg loss: 0.040 | Tree loss: 2.396 | Accuracy: 0.152344 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 028 | Total loss: 2.410 | Reg loss: 0.040 | Tree loss: 2.410 | Accuracy: 0.179688 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 028 | Total loss: 2.358 | Reg loss: 0.040 | Tree loss: 2.358 | Accuracy: 0.189453 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 028 | Total loss: 2.358 | Reg loss: 0.040 | Tree loss: 2.358 | Accuracy: 0.177734 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 028 | Total loss: 2.365 | Reg loss: 0.040 | Tree loss: 2.365 | Accuracy: 0.152344 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 028 | Total loss: 2.347 | Reg loss: 0.040 | Tree loss: 2.347 | Accuracy: 0.214844 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 028 | Total loss: 2.365 | Reg loss: 0.040 | Tree loss: 2.365 | Accuracy: 0.181641 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 028 | Total loss: 2.376 | Reg loss: 0.040 | Tree loss: 2.376 | Accuracy: 0.162109 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 028 | Total loss: 2.323 | Reg loss: 0.040 | Tree loss: 2.323 | Accuracy: 0.162109 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 028 | Total loss: 2.335 | Reg loss: 0.040 | Tree loss: 2.335 | Accuracy: 0.156250 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 028 | Total loss: 2.337 | Reg loss: 0.040 | Tree loss: 2.337 | Accuracy: 0.173828 | 6.92 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 028 | Total loss: 2.297 | Reg loss: 0.040 | Tree loss: 2.297 | Accuracy: 0.179688 | 6.921 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 028 | Total loss: 2.279 | Reg loss: 0.040 | Tree loss: 2.279 | Accuracy: 0.203125 | 6.921 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 028 | Total loss: 2.307 | Reg loss: 0.040 | Tree loss: 2.307 | Accuracy: 0.185547 | 6.921 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 028 | Total loss: 2.327 | Reg loss: 0.040 | Tree loss: 2.327 | Accuracy: 0.199219 | 6.921 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 028 | Total loss: 2.353 | Reg loss: 0.040 | Tree loss: 2.353 | Accuracy: 0.197266 | 6.921 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 028 | Total loss: 2.333 | Reg loss: 0.040 | Tree loss: 2.333 | Accuracy: 0.312500 | 6.918 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 028 | Total loss: 2.442 | Reg loss: 0.040 | Tree loss: 2.442 | Accuracy: 0.177734 | 6.925 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 028 | Total loss: 2.429 | Reg loss: 0.040 | Tree loss: 2.429 | Accuracy: 0.181641 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 028 | Total loss: 2.418 | Reg loss: 0.040 | Tree loss: 2.418 | Accuracy: 0.175781 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 028 | Total loss: 2.444 | Reg loss: 0.040 | Tree loss: 2.444 | Accuracy: 0.191406 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 028 | Total loss: 2.448 | Reg loss: 0.040 | Tree loss: 2.448 | Accuracy: 0.183594 | 6.925 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 028 | Total loss: 2.435 | Reg loss: 0.040 | Tree loss: 2.435 | Accuracy: 0.185547 | 6.925 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 028 | Total loss: 2.351 | Reg loss: 0.040 | Tree loss: 2.351 | Accuracy: 0.203125 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 028 | Total loss: 2.384 | Reg loss: 0.040 | Tree loss: 2.384 | Accuracy: 0.197266 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 028 | Total loss: 2.387 | Reg loss: 0.040 | Tree loss: 2.387 | Accuracy: 0.175781 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 028 | Total loss: 2.353 | Reg loss: 0.040 | Tree loss: 2.353 | Accuracy: 0.158203 | 6.926 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 028 | Total loss: 2.415 | Reg loss: 0.040 | Tree loss: 2.415 | Accuracy: 0.195312 | 6.927 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 028 | Total loss: 2.401 | Reg loss: 0.040 | Tree loss: 2.401 | Accuracy: 0.156250 | 6.927 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 028 | Total loss: 2.341 | Reg loss: 0.040 | Tree loss: 2.341 | Accuracy: 0.205078 | 6.927 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 028 | Total loss: 2.354 | Reg loss: 0.040 | Tree loss: 2.354 | Accuracy: 0.181641 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 028 | Total loss: 2.339 | Reg loss: 0.040 | Tree loss: 2.339 | Accuracy: 0.193359 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 028 | Total loss: 2.358 | Reg loss: 0.040 | Tree loss: 2.358 | Accuracy: 0.167969 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 028 | Total loss: 2.332 | Reg loss: 0.040 | Tree loss: 2.332 | Accuracy: 0.191406 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 028 | Total loss: 2.347 | Reg loss: 0.040 | Tree loss: 2.347 | Accuracy: 0.189453 | 6.929 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 028 | Total loss: 2.291 | Reg loss: 0.040 | Tree loss: 2.291 | Accuracy: 0.193359 | 6.929 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 028 | Total loss: 2.342 | Reg loss: 0.040 | Tree loss: 2.342 | Accuracy: 0.160156 | 6.929 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 028 | Total loss: 2.333 | Reg loss: 0.040 | Tree loss: 2.333 | Accuracy: 0.169922 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 028 | Total loss: 2.307 | Reg loss: 0.040 | Tree loss: 2.307 | Accuracy: 0.193359 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 028 | Total loss: 2.295 | Reg loss: 0.040 | Tree loss: 2.295 | Accuracy: 0.189453 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 028 | Total loss: 2.302 | Reg loss: 0.040 | Tree loss: 2.302 | Accuracy: 0.189453 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 028 | Total loss: 2.275 | Reg loss: 0.040 | Tree loss: 2.275 | Accuracy: 0.181641 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 028 | Total loss: 2.306 | Reg loss: 0.040 | Tree loss: 2.306 | Accuracy: 0.228516 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 028 | Total loss: 2.298 | Reg loss: 0.040 | Tree loss: 2.298 | Accuracy: 0.173828 | 6.928 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 028 | Total loss: 2.291 | Reg loss: 0.040 | Tree loss: 2.291 | Accuracy: 0.250000 | 6.925 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 028 | Total loss: 2.407 | Reg loss: 0.040 | Tree loss: 2.407 | Accuracy: 0.181641 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 028 | Total loss: 2.428 | Reg loss: 0.040 | Tree loss: 2.428 | Accuracy: 0.203125 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 028 | Total loss: 2.431 | Reg loss: 0.040 | Tree loss: 2.431 | Accuracy: 0.158203 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 028 | Total loss: 2.431 | Reg loss: 0.040 | Tree loss: 2.431 | Accuracy: 0.179688 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 028 | Total loss: 2.387 | Reg loss: 0.040 | Tree loss: 2.387 | Accuracy: 0.208984 | 6.93 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 028 | Total loss: 2.398 | Reg loss: 0.040 | Tree loss: 2.398 | Accuracy: 0.199219 | 6.93 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 028 | Total loss: 2.391 | Reg loss: 0.040 | Tree loss: 2.391 | Accuracy: 0.205078 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 028 | Total loss: 2.431 | Reg loss: 0.040 | Tree loss: 2.431 | Accuracy: 0.175781 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 028 | Total loss: 2.410 | Reg loss: 0.040 | Tree loss: 2.410 | Accuracy: 0.158203 | 6.931 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 028 | Total loss: 2.337 | Reg loss: 0.040 | Tree loss: 2.337 | Accuracy: 0.195312 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 028 | Total loss: 2.360 | Reg loss: 0.040 | Tree loss: 2.360 | Accuracy: 0.179688 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 028 | Total loss: 2.382 | Reg loss: 0.040 | Tree loss: 2.382 | Accuracy: 0.175781 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 028 | Total loss: 2.354 | Reg loss: 0.040 | Tree loss: 2.354 | Accuracy: 0.199219 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 028 | Total loss: 2.332 | Reg loss: 0.040 | Tree loss: 2.332 | Accuracy: 0.181641 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 028 | Total loss: 2.356 | Reg loss: 0.040 | Tree loss: 2.356 | Accuracy: 0.177734 | 6.933 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 015 / 028 | Total loss: 2.352 | Reg loss: 0.040 | Tree loss: 2.352 | Accuracy: 0.160156 | 6.933 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 028 | Total loss: 2.362 | Reg loss: 0.040 | Tree loss: 2.362 | Accuracy: 0.183594 | 6.933 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 028 | Total loss: 2.330 | Reg loss: 0.040 | Tree loss: 2.330 | Accuracy: 0.183594 | 6.933 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 028 | Total loss: 2.334 | Reg loss: 0.040 | Tree loss: 2.334 | Accuracy: 0.187500 | 6.933 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 028 | Total loss: 2.273 | Reg loss: 0.040 | Tree loss: 2.273 | Accuracy: 0.179688 | 6.933 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 028 | Total loss: 2.294 | Reg loss: 0.040 | Tree loss: 2.294 | Accuracy: 0.191406 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 028 | Total loss: 2.277 | Reg loss: 0.040 | Tree loss: 2.277 | Accuracy: 0.210938 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 028 | Total loss: 2.290 | Reg loss: 0.040 | Tree loss: 2.290 | Accuracy: 0.193359 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 028 | Total loss: 2.261 | Reg loss: 0.040 | Tree loss: 2.261 | Accuracy: 0.187500 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 028 | Total loss: 2.316 | Reg loss: 0.040 | Tree loss: 2.316 | Accuracy: 0.181641 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 028 | Total loss: 2.281 | Reg loss: 0.040 | Tree loss: 2.281 | Accuracy: 0.183594 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 028 | Total loss: 2.295 | Reg loss: 0.040 | Tree loss: 2.295 | Accuracy: 0.195312 | 6.932 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 028 | Total loss: 2.425 | Reg loss: 0.040 | Tree loss: 2.425 | Accuracy: 0.187500 | 6.93 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 028 | Total loss: 2.414 | Reg loss: 0.039 | Tree loss: 2.414 | Accuracy: 0.208984 | 6.94 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 028 | Total loss: 2.442 | Reg loss: 0.039 | Tree loss: 2.442 | Accuracy: 0.175781 | 6.939 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 028 | Total loss: 2.424 | Reg loss: 0.039 | Tree loss: 2.424 | Accuracy: 0.177734 | 6.939 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 028 | Total loss: 2.395 | Reg loss: 0.039 | Tree loss: 2.395 | Accuracy: 0.183594 | 6.938 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 028 | Total loss: 2.389 | Reg loss: 0.039 | Tree loss: 2.389 | Accuracy: 0.195312 | 6.937 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 028 | Total loss: 2.393 | Reg loss: 0.039 | Tree loss: 2.393 | Accuracy: 0.185547 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 028 | Total loss: 2.378 | Reg loss: 0.039 | Tree loss: 2.378 | Accuracy: 0.185547 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 028 | Total loss: 2.323 | Reg loss: 0.039 | Tree loss: 2.323 | Accuracy: 0.210938 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 028 | Total loss: 2.344 | Reg loss: 0.039 | Tree loss: 2.344 | Accuracy: 0.193359 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 028 | Total loss: 2.358 | Reg loss: 0.039 | Tree loss: 2.358 | Accuracy: 0.173828 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 028 | Total loss: 2.317 | Reg loss: 0.039 | Tree loss: 2.317 | Accuracy: 0.218750 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 028 | Total loss: 2.313 | Reg loss: 0.039 | Tree loss: 2.313 | Accuracy: 0.189453 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 028 | Total loss: 2.339 | Reg loss: 0.039 | Tree loss: 2.339 | Accuracy: 0.181641 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 028 | Total loss: 2.344 | Reg loss: 0.039 | Tree loss: 2.344 | Accuracy: 0.187500 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 028 | Total loss: 2.377 | Reg loss: 0.039 | Tree loss: 2.377 | Accuracy: 0.212891 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 028 | Total loss: 2.322 | Reg loss: 0.039 | Tree loss: 2.322 | Accuracy: 0.189453 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 028 | Total loss: 2.298 | Reg loss: 0.039 | Tree loss: 2.298 | Accuracy: 0.173828 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 028 | Total loss: 2.311 | Reg loss: 0.040 | Tree loss: 2.311 | Accuracy: 0.212891 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 028 | Total loss: 2.338 | Reg loss: 0.040 | Tree loss: 2.338 | Accuracy: 0.169922 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 028 | Total loss: 2.367 | Reg loss: 0.040 | Tree loss: 2.367 | Accuracy: 0.166016 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 028 | Total loss: 2.312 | Reg loss: 0.040 | Tree loss: 2.312 | Accuracy: 0.173828 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 028 | Total loss: 2.302 | Reg loss: 0.040 | Tree loss: 2.302 | Accuracy: 0.179688 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 028 | Total loss: 2.271 | Reg loss: 0.040 | Tree loss: 2.271 | Accuracy: 0.175781 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 028 | Total loss: 2.267 | Reg loss: 0.040 | Tree loss: 2.267 | Accuracy: 0.195312 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 028 | Total loss: 2.347 | Reg loss: 0.040 | Tree loss: 2.347 | Accuracy: 0.148438 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 028 | Total loss: 2.274 | Reg loss: 0.040 | Tree loss: 2.274 | Accuracy: 0.193359 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 028 | Total loss: 2.289 | Reg loss: 0.040 | Tree loss: 2.289 | Accuracy: 0.134766 | 6.936 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 028 | Total loss: 2.198 | Reg loss: 0.040 | Tree loss: 2.198 | Accuracy: 0.312500 | 6.934 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 028 | Total loss: 2.377 | Reg loss: 0.039 | Tree loss: 2.377 | Accuracy: 0.201172 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 028 | Total loss: 2.427 | Reg loss: 0.039 | Tree loss: 2.427 | Accuracy: 0.171875 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 028 | Total loss: 2.393 | Reg loss: 0.039 | Tree loss: 2.393 | Accuracy: 0.208984 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 028 | Total loss: 2.381 | Reg loss: 0.039 | Tree loss: 2.381 | Accuracy: 0.175781 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 028 | Total loss: 2.381 | Reg loss: 0.039 | Tree loss: 2.381 | Accuracy: 0.175781 | 6.946 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 028 | Total loss: 2.373 | Reg loss: 0.039 | Tree loss: 2.373 | Accuracy: 0.193359 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 028 | Total loss: 2.378 | Reg loss: 0.039 | Tree loss: 2.378 | Accuracy: 0.197266 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 028 | Total loss: 2.340 | Reg loss: 0.039 | Tree loss: 2.340 | Accuracy: 0.230469 | 6.947 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 028 | Total loss: 2.343 | Reg loss: 0.039 | Tree loss: 2.343 | Accuracy: 0.195312 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 028 | Total loss: 2.371 | Reg loss: 0.039 | Tree loss: 2.371 | Accuracy: 0.205078 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 028 | Total loss: 2.333 | Reg loss: 0.039 | Tree loss: 2.333 | Accuracy: 0.179688 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 028 | Total loss: 2.349 | Reg loss: 0.039 | Tree loss: 2.349 | Accuracy: 0.152344 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 028 | Total loss: 2.370 | Reg loss: 0.039 | Tree loss: 2.370 | Accuracy: 0.181641 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 028 | Total loss: 2.305 | Reg loss: 0.039 | Tree loss: 2.305 | Accuracy: 0.179688 | 6.948 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 028 | Total loss: 2.321 | Reg loss: 0.039 | Tree loss: 2.321 | Accuracy: 0.183594 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 028 | Total loss: 2.295 | Reg loss: 0.039 | Tree loss: 2.295 | Accuracy: 0.179688 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 028 | Total loss: 2.324 | Reg loss: 0.039 | Tree loss: 2.324 | Accuracy: 0.183594 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 028 | Total loss: 2.312 | Reg loss: 0.039 | Tree loss: 2.312 | Accuracy: 0.207031 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 028 | Total loss: 2.317 | Reg loss: 0.039 | Tree loss: 2.317 | Accuracy: 0.187500 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 028 | Total loss: 2.343 | Reg loss: 0.039 | Tree loss: 2.343 | Accuracy: 0.154297 | 6.949 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 020 / 028 | Total loss: 2.299 | Reg loss: 0.039 | Tree loss: 2.299 | Accuracy: 0.193359 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 028 | Total loss: 2.315 | Reg loss: 0.039 | Tree loss: 2.315 | Accuracy: 0.167969 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 028 | Total loss: 2.239 | Reg loss: 0.039 | Tree loss: 2.239 | Accuracy: 0.201172 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 028 | Total loss: 2.288 | Reg loss: 0.039 | Tree loss: 2.288 | Accuracy: 0.183594 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 028 | Total loss: 2.274 | Reg loss: 0.039 | Tree loss: 2.274 | Accuracy: 0.222656 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 028 | Total loss: 2.310 | Reg loss: 0.039 | Tree loss: 2.310 | Accuracy: 0.185547 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 028 | Total loss: 2.274 | Reg loss: 0.039 | Tree loss: 2.274 | Accuracy: 0.207031 | 6.949 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 028 | Total loss: 2.320 | Reg loss: 0.039 | Tree loss: 2.320 | Accuracy: 0.125000 | 6.947 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 028 | Total loss: 2.360 | Reg loss: 0.039 | Tree loss: 2.360 | Accuracy: 0.205078 | 6.952 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 028 | Total loss: 2.395 | Reg loss: 0.039 | Tree loss: 2.395 | Accuracy: 0.203125 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 028 | Total loss: 2.377 | Reg loss: 0.039 | Tree loss: 2.377 | Accuracy: 0.164062 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 028 | Total loss: 2.398 | Reg loss: 0.039 | Tree loss: 2.398 | Accuracy: 0.197266 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 028 | Total loss: 2.369 | Reg loss: 0.039 | Tree loss: 2.369 | Accuracy: 0.171875 | 6.952 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 028 | Total loss: 2.347 | Reg loss: 0.039 | Tree loss: 2.347 | Accuracy: 0.179688 | 6.952 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 028 | Total loss: 2.372 | Reg loss: 0.039 | Tree loss: 2.372 | Accuracy: 0.205078 | 6.952 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 028 | Total loss: 2.360 | Reg loss: 0.039 | Tree loss: 2.360 | Accuracy: 0.154297 | 6.952 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 028 | Total loss: 2.424 | Reg loss: 0.039 | Tree loss: 2.424 | Accuracy: 0.191406 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 028 | Total loss: 2.336 | Reg loss: 0.039 | Tree loss: 2.336 | Accuracy: 0.208984 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 028 | Total loss: 2.331 | Reg loss: 0.039 | Tree loss: 2.331 | Accuracy: 0.177734 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 028 | Total loss: 2.326 | Reg loss: 0.039 | Tree loss: 2.326 | Accuracy: 0.216797 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 028 | Total loss: 2.343 | Reg loss: 0.039 | Tree loss: 2.343 | Accuracy: 0.193359 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 028 | Total loss: 2.299 | Reg loss: 0.039 | Tree loss: 2.299 | Accuracy: 0.181641 | 6.953 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 028 | Total loss: 2.362 | Reg loss: 0.039 | Tree loss: 2.362 | Accuracy: 0.197266 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 028 | Total loss: 2.308 | Reg loss: 0.039 | Tree loss: 2.308 | Accuracy: 0.189453 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 028 | Total loss: 2.305 | Reg loss: 0.039 | Tree loss: 2.305 | Accuracy: 0.179688 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 028 | Total loss: 2.282 | Reg loss: 0.039 | Tree loss: 2.282 | Accuracy: 0.214844 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 028 | Total loss: 2.309 | Reg loss: 0.039 | Tree loss: 2.309 | Accuracy: 0.193359 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 028 | Total loss: 2.257 | Reg loss: 0.039 | Tree loss: 2.257 | Accuracy: 0.185547 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 028 | Total loss: 2.300 | Reg loss: 0.039 | Tree loss: 2.300 | Accuracy: 0.187500 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 028 | Total loss: 2.323 | Reg loss: 0.039 | Tree loss: 2.323 | Accuracy: 0.175781 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 028 | Total loss: 2.298 | Reg loss: 0.039 | Tree loss: 2.298 | Accuracy: 0.185547 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 028 | Total loss: 2.224 | Reg loss: 0.039 | Tree loss: 2.224 | Accuracy: 0.238281 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 028 | Total loss: 2.269 | Reg loss: 0.039 | Tree loss: 2.269 | Accuracy: 0.208984 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 028 | Total loss: 2.320 | Reg loss: 0.039 | Tree loss: 2.320 | Accuracy: 0.177734 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 028 | Total loss: 2.258 | Reg loss: 0.039 | Tree loss: 2.258 | Accuracy: 0.191406 | 6.954 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 028 | Total loss: 2.098 | Reg loss: 0.039 | Tree loss: 2.098 | Accuracy: 0.375000 | 6.952 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 028 | Total loss: 2.407 | Reg loss: 0.039 | Tree loss: 2.407 | Accuracy: 0.189453 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 028 | Total loss: 2.430 | Reg loss: 0.039 | Tree loss: 2.430 | Accuracy: 0.185547 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 028 | Total loss: 2.374 | Reg loss: 0.039 | Tree loss: 2.374 | Accuracy: 0.191406 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 028 | Total loss: 2.388 | Reg loss: 0.039 | Tree loss: 2.388 | Accuracy: 0.212891 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 028 | Total loss: 2.368 | Reg loss: 0.039 | Tree loss: 2.368 | Accuracy: 0.195312 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 028 | Total loss: 2.352 | Reg loss: 0.039 | Tree loss: 2.352 | Accuracy: 0.216797 | 6.957 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 028 | Total loss: 2.364 | Reg loss: 0.039 | Tree loss: 2.364 | Accuracy: 0.195312 | 6.955 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 028 | Total loss: 2.364 | Reg loss: 0.039 | Tree loss: 2.364 | Accuracy: 0.146484 | 6.954 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 028 | Total loss: 2.292 | Reg loss: 0.039 | Tree loss: 2.292 | Accuracy: 0.210938 | 6.953 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 028 | Total loss: 2.269 | Reg loss: 0.039 | Tree loss: 2.269 | Accuracy: 0.207031 | 6.952 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 028 | Total loss: 2.286 | Reg loss: 0.039 | Tree loss: 2.286 | Accuracy: 0.212891 | 6.95 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 028 | Total loss: 2.310 | Reg loss: 0.039 | Tree loss: 2.310 | Accuracy: 0.195312 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 028 | Total loss: 2.294 | Reg loss: 0.039 | Tree loss: 2.294 | Accuracy: 0.220703 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 028 | Total loss: 2.290 | Reg loss: 0.039 | Tree loss: 2.290 | Accuracy: 0.222656 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 028 | Total loss: 2.290 | Reg loss: 0.039 | Tree loss: 2.290 | Accuracy: 0.203125 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 028 | Total loss: 2.302 | Reg loss: 0.039 | Tree loss: 2.302 | Accuracy: 0.205078 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 028 | Total loss: 2.358 | Reg loss: 0.039 | Tree loss: 2.358 | Accuracy: 0.183594 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 028 | Total loss: 2.317 | Reg loss: 0.039 | Tree loss: 2.317 | Accuracy: 0.187500 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 028 | Total loss: 2.270 | Reg loss: 0.039 | Tree loss: 2.270 | Accuracy: 0.201172 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 028 | Total loss: 2.278 | Reg loss: 0.039 | Tree loss: 2.278 | Accuracy: 0.199219 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 028 | Total loss: 2.316 | Reg loss: 0.039 | Tree loss: 2.316 | Accuracy: 0.164062 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 028 | Total loss: 2.304 | Reg loss: 0.039 | Tree loss: 2.304 | Accuracy: 0.187500 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 028 | Total loss: 2.295 | Reg loss: 0.039 | Tree loss: 2.295 | Accuracy: 0.201172 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 028 | Total loss: 2.305 | Reg loss: 0.039 | Tree loss: 2.305 | Accuracy: 0.199219 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 028 | Total loss: 2.287 | Reg loss: 0.039 | Tree loss: 2.287 | Accuracy: 0.207031 | 6.949 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 025 / 028 | Total loss: 2.273 | Reg loss: 0.039 | Tree loss: 2.273 | Accuracy: 0.171875 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 028 | Total loss: 2.274 | Reg loss: 0.039 | Tree loss: 2.274 | Accuracy: 0.189453 | 6.949 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 028 | Total loss: 2.320 | Reg loss: 0.039 | Tree loss: 2.320 | Accuracy: 0.312500 | 6.947 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 028 | Total loss: 2.381 | Reg loss: 0.039 | Tree loss: 2.381 | Accuracy: 0.218750 | 6.956 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 028 | Total loss: 2.381 | Reg loss: 0.039 | Tree loss: 2.381 | Accuracy: 0.220703 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 028 | Total loss: 2.409 | Reg loss: 0.039 | Tree loss: 2.409 | Accuracy: 0.193359 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 028 | Total loss: 2.346 | Reg loss: 0.039 | Tree loss: 2.346 | Accuracy: 0.201172 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 028 | Total loss: 2.322 | Reg loss: 0.039 | Tree loss: 2.322 | Accuracy: 0.216797 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 028 | Total loss: 2.354 | Reg loss: 0.039 | Tree loss: 2.354 | Accuracy: 0.214844 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 028 | Total loss: 2.376 | Reg loss: 0.039 | Tree loss: 2.376 | Accuracy: 0.199219 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 028 | Total loss: 2.327 | Reg loss: 0.039 | Tree loss: 2.327 | Accuracy: 0.191406 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 028 | Total loss: 2.348 | Reg loss: 0.039 | Tree loss: 2.348 | Accuracy: 0.173828 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 028 | Total loss: 2.337 | Reg loss: 0.039 | Tree loss: 2.337 | Accuracy: 0.148438 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 028 | Total loss: 2.327 | Reg loss: 0.039 | Tree loss: 2.327 | Accuracy: 0.191406 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 028 | Total loss: 2.357 | Reg loss: 0.039 | Tree loss: 2.357 | Accuracy: 0.214844 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 028 | Total loss: 2.306 | Reg loss: 0.039 | Tree loss: 2.306 | Accuracy: 0.185547 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 028 | Total loss: 2.285 | Reg loss: 0.039 | Tree loss: 2.285 | Accuracy: 0.216797 | 6.957 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 028 | Total loss: 2.322 | Reg loss: 0.039 | Tree loss: 2.322 | Accuracy: 0.224609 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 028 | Total loss: 2.309 | Reg loss: 0.039 | Tree loss: 2.309 | Accuracy: 0.197266 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 028 | Total loss: 2.293 | Reg loss: 0.039 | Tree loss: 2.293 | Accuracy: 0.212891 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 028 | Total loss: 2.292 | Reg loss: 0.039 | Tree loss: 2.292 | Accuracy: 0.193359 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 028 | Total loss: 2.287 | Reg loss: 0.039 | Tree loss: 2.287 | Accuracy: 0.185547 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 028 | Total loss: 2.272 | Reg loss: 0.039 | Tree loss: 2.272 | Accuracy: 0.187500 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 028 | Total loss: 2.241 | Reg loss: 0.039 | Tree loss: 2.241 | Accuracy: 0.207031 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 028 | Total loss: 2.289 | Reg loss: 0.039 | Tree loss: 2.289 | Accuracy: 0.181641 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 028 | Total loss: 2.287 | Reg loss: 0.039 | Tree loss: 2.287 | Accuracy: 0.214844 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 028 | Total loss: 2.270 | Reg loss: 0.039 | Tree loss: 2.270 | Accuracy: 0.185547 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 028 | Total loss: 2.294 | Reg loss: 0.039 | Tree loss: 2.294 | Accuracy: 0.187500 | 6.958 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 028 | Total loss: 2.243 | Reg loss: 0.039 | Tree loss: 2.243 | Accuracy: 0.195312 | 6.959 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 028 | Total loss: 2.229 | Reg loss: 0.039 | Tree loss: 2.229 | Accuracy: 0.203125 | 6.959 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 028 | Total loss: 2.293 | Reg loss: 0.039 | Tree loss: 2.293 | Accuracy: 0.125000 | 6.957 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 028 | Total loss: 2.378 | Reg loss: 0.038 | Tree loss: 2.378 | Accuracy: 0.199219 | 6.963 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 028 | Total loss: 2.364 | Reg loss: 0.038 | Tree loss: 2.364 | Accuracy: 0.191406 | 6.963 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 028 | Total loss: 2.359 | Reg loss: 0.038 | Tree loss: 2.359 | Accuracy: 0.218750 | 6.963 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 028 | Total loss: 2.336 | Reg loss: 0.038 | Tree loss: 2.336 | Accuracy: 0.193359 | 6.963 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 028 | Total loss: 2.377 | Reg loss: 0.038 | Tree loss: 2.377 | Accuracy: 0.167969 | 6.963 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 028 | Total loss: 2.391 | Reg loss: 0.038 | Tree loss: 2.391 | Accuracy: 0.208984 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 028 | Total loss: 2.383 | Reg loss: 0.038 | Tree loss: 2.383 | Accuracy: 0.179688 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 028 | Total loss: 2.412 | Reg loss: 0.038 | Tree loss: 2.412 | Accuracy: 0.193359 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 028 | Total loss: 2.295 | Reg loss: 0.038 | Tree loss: 2.295 | Accuracy: 0.236328 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 028 | Total loss: 2.351 | Reg loss: 0.038 | Tree loss: 2.351 | Accuracy: 0.175781 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 028 | Total loss: 2.285 | Reg loss: 0.038 | Tree loss: 2.285 | Accuracy: 0.207031 | 6.962 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 028 | Total loss: 2.326 | Reg loss: 0.039 | Tree loss: 2.326 | Accuracy: 0.216797 | 6.961 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 028 | Total loss: 2.295 | Reg loss: 0.039 | Tree loss: 2.295 | Accuracy: 0.179688 | 6.959 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 028 | Total loss: 2.263 | Reg loss: 0.039 | Tree loss: 2.263 | Accuracy: 0.226562 | 6.958 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 028 | Total loss: 2.278 | Reg loss: 0.039 | Tree loss: 2.278 | Accuracy: 0.201172 | 6.957 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 028 | Total loss: 2.270 | Reg loss: 0.039 | Tree loss: 2.270 | Accuracy: 0.197266 | 6.956 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 028 | Total loss: 2.300 | Reg loss: 0.039 | Tree loss: 2.300 | Accuracy: 0.175781 | 6.955 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 028 | Total loss: 2.294 | Reg loss: 0.039 | Tree loss: 2.294 | Accuracy: 0.203125 | 6.953 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 028 | Total loss: 2.271 | Reg loss: 0.039 | Tree loss: 2.271 | Accuracy: 0.201172 | 6.952 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 028 | Total loss: 2.282 | Reg loss: 0.039 | Tree loss: 2.282 | Accuracy: 0.187500 | 6.951 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 028 | Total loss: 2.247 | Reg loss: 0.039 | Tree loss: 2.247 | Accuracy: 0.187500 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 028 | Total loss: 2.254 | Reg loss: 0.039 | Tree loss: 2.254 | Accuracy: 0.205078 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 028 | Total loss: 2.243 | Reg loss: 0.039 | Tree loss: 2.243 | Accuracy: 0.203125 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 028 | Total loss: 2.305 | Reg loss: 0.039 | Tree loss: 2.305 | Accuracy: 0.193359 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 028 | Total loss: 2.255 | Reg loss: 0.039 | Tree loss: 2.255 | Accuracy: 0.205078 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 028 | Total loss: 2.256 | Reg loss: 0.039 | Tree loss: 2.256 | Accuracy: 0.212891 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 028 | Total loss: 2.237 | Reg loss: 0.039 | Tree loss: 2.237 | Accuracy: 0.195312 | 6.95 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 028 | Total loss: 2.161 | Reg loss: 0.039 | Tree loss: 2.161 | Accuracy: 0.250000 | 6.948 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 028 | Total loss: 2.362 | Reg loss: 0.038 | Tree loss: 2.362 | Accuracy: 0.203125 | 6.955 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 028 | Total loss: 2.363 | Reg loss: 0.038 | Tree loss: 2.363 | Accuracy: 0.187500 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 028 | Total loss: 2.399 | Reg loss: 0.038 | Tree loss: 2.399 | Accuracy: 0.171875 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 028 | Total loss: 2.356 | Reg loss: 0.038 | Tree loss: 2.356 | Accuracy: 0.181641 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 028 | Total loss: 2.351 | Reg loss: 0.038 | Tree loss: 2.351 | Accuracy: 0.207031 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 028 | Total loss: 2.334 | Reg loss: 0.038 | Tree loss: 2.334 | Accuracy: 0.195312 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 028 | Total loss: 2.310 | Reg loss: 0.038 | Tree loss: 2.310 | Accuracy: 0.191406 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 028 | Total loss: 2.357 | Reg loss: 0.038 | Tree loss: 2.357 | Accuracy: 0.208984 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 028 | Total loss: 2.342 | Reg loss: 0.038 | Tree loss: 2.342 | Accuracy: 0.199219 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 028 | Total loss: 2.311 | Reg loss: 0.038 | Tree loss: 2.311 | Accuracy: 0.218750 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 028 | Total loss: 2.330 | Reg loss: 0.038 | Tree loss: 2.330 | Accuracy: 0.158203 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 028 | Total loss: 2.328 | Reg loss: 0.038 | Tree loss: 2.328 | Accuracy: 0.199219 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 028 | Total loss: 2.294 | Reg loss: 0.038 | Tree loss: 2.294 | Accuracy: 0.185547 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 028 | Total loss: 2.287 | Reg loss: 0.038 | Tree loss: 2.287 | Accuracy: 0.185547 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 028 | Total loss: 2.270 | Reg loss: 0.038 | Tree loss: 2.270 | Accuracy: 0.201172 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 028 | Total loss: 2.317 | Reg loss: 0.038 | Tree loss: 2.317 | Accuracy: 0.203125 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 028 | Total loss: 2.305 | Reg loss: 0.038 | Tree loss: 2.305 | Accuracy: 0.218750 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 028 | Total loss: 2.259 | Reg loss: 0.038 | Tree loss: 2.259 | Accuracy: 0.226562 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 028 | Total loss: 2.292 | Reg loss: 0.038 | Tree loss: 2.292 | Accuracy: 0.203125 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 028 | Total loss: 2.311 | Reg loss: 0.038 | Tree loss: 2.311 | Accuracy: 0.214844 | 6.954 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 028 | Total loss: 2.281 | Reg loss: 0.038 | Tree loss: 2.281 | Accuracy: 0.175781 | 6.953 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 028 | Total loss: 2.218 | Reg loss: 0.039 | Tree loss: 2.218 | Accuracy: 0.220703 | 6.952 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 028 | Total loss: 2.285 | Reg loss: 0.039 | Tree loss: 2.285 | Accuracy: 0.185547 | 6.951 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 028 | Total loss: 2.213 | Reg loss: 0.039 | Tree loss: 2.213 | Accuracy: 0.208984 | 6.95 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 028 | Total loss: 2.296 | Reg loss: 0.039 | Tree loss: 2.296 | Accuracy: 0.191406 | 6.951 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 028 | Total loss: 2.202 | Reg loss: 0.039 | Tree loss: 2.202 | Accuracy: 0.199219 | 6.951 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 028 | Total loss: 2.210 | Reg loss: 0.039 | Tree loss: 2.210 | Accuracy: 0.220703 | 6.951 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 028 | Total loss: 2.172 | Reg loss: 0.039 | Tree loss: 2.172 | Accuracy: 0.250000 | 6.949 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 028 | Total loss: 2.397 | Reg loss: 0.038 | Tree loss: 2.397 | Accuracy: 0.207031 | 6.959 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 028 | Total loss: 2.348 | Reg loss: 0.038 | Tree loss: 2.348 | Accuracy: 0.191406 | 6.96 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 028 | Total loss: 2.405 | Reg loss: 0.038 | Tree loss: 2.405 | Accuracy: 0.185547 | 6.96 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 028 | Total loss: 2.351 | Reg loss: 0.038 | Tree loss: 2.351 | Accuracy: 0.195312 | 6.96 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 028 | Total loss: 2.357 | Reg loss: 0.038 | Tree loss: 2.357 | Accuracy: 0.187500 | 6.96 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 028 | Total loss: 2.350 | Reg loss: 0.038 | Tree loss: 2.350 | Accuracy: 0.171875 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 028 | Total loss: 2.364 | Reg loss: 0.038 | Tree loss: 2.364 | Accuracy: 0.175781 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 028 | Total loss: 2.299 | Reg loss: 0.038 | Tree loss: 2.299 | Accuracy: 0.220703 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 028 | Total loss: 2.355 | Reg loss: 0.038 | Tree loss: 2.355 | Accuracy: 0.175781 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 028 | Total loss: 2.307 | Reg loss: 0.038 | Tree loss: 2.307 | Accuracy: 0.171875 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 028 | Total loss: 2.321 | Reg loss: 0.038 | Tree loss: 2.321 | Accuracy: 0.189453 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 028 | Total loss: 2.261 | Reg loss: 0.038 | Tree loss: 2.261 | Accuracy: 0.208984 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 028 | Total loss: 2.296 | Reg loss: 0.038 | Tree loss: 2.296 | Accuracy: 0.193359 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 028 | Total loss: 2.304 | Reg loss: 0.038 | Tree loss: 2.304 | Accuracy: 0.232422 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 028 | Total loss: 2.270 | Reg loss: 0.038 | Tree loss: 2.270 | Accuracy: 0.195312 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 028 | Total loss: 2.281 | Reg loss: 0.038 | Tree loss: 2.281 | Accuracy: 0.208984 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 028 | Total loss: 2.276 | Reg loss: 0.038 | Tree loss: 2.276 | Accuracy: 0.230469 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 028 | Total loss: 2.238 | Reg loss: 0.038 | Tree loss: 2.238 | Accuracy: 0.205078 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 028 | Total loss: 2.250 | Reg loss: 0.038 | Tree loss: 2.250 | Accuracy: 0.195312 | 6.963 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 028 | Total loss: 2.266 | Reg loss: 0.038 | Tree loss: 2.266 | Accuracy: 0.197266 | 6.963 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 028 | Total loss: 2.248 | Reg loss: 0.038 | Tree loss: 2.248 | Accuracy: 0.250000 | 6.962 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 028 | Total loss: 2.262 | Reg loss: 0.038 | Tree loss: 2.262 | Accuracy: 0.191406 | 6.961 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 028 | Total loss: 2.264 | Reg loss: 0.038 | Tree loss: 2.264 | Accuracy: 0.201172 | 6.96 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 028 | Total loss: 2.236 | Reg loss: 0.038 | Tree loss: 2.236 | Accuracy: 0.195312 | 6.958 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 028 | Total loss: 2.256 | Reg loss: 0.038 | Tree loss: 2.256 | Accuracy: 0.173828 | 6.957 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 028 | Total loss: 2.250 | Reg loss: 0.038 | Tree loss: 2.250 | Accuracy: 0.199219 | 6.956 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 028 | Total loss: 2.212 | Reg loss: 0.038 | Tree loss: 2.212 | Accuracy: 0.214844 | 6.955 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 028 | Total loss: 2.132 | Reg loss: 0.038 | Tree loss: 2.132 | Accuracy: 0.187500 | 6.953 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 028 | Total loss: 2.394 | Reg loss: 0.038 | Tree loss: 2.394 | Accuracy: 0.167969 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 028 | Total loss: 2.352 | Reg loss: 0.038 | Tree loss: 2.352 | Accuracy: 0.208984 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 028 | Total loss: 2.350 | Reg loss: 0.038 | Tree loss: 2.350 | Accuracy: 0.185547 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 028 | Total loss: 2.396 | Reg loss: 0.038 | Tree loss: 2.396 | Accuracy: 0.177734 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 028 | Total loss: 2.394 | Reg loss: 0.038 | Tree loss: 2.394 | Accuracy: 0.187500 | 6.958 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 005 / 028 | Total loss: 2.345 | Reg loss: 0.038 | Tree loss: 2.345 | Accuracy: 0.197266 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 028 | Total loss: 2.281 | Reg loss: 0.038 | Tree loss: 2.281 | Accuracy: 0.185547 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 028 | Total loss: 2.318 | Reg loss: 0.038 | Tree loss: 2.318 | Accuracy: 0.240234 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 028 | Total loss: 2.311 | Reg loss: 0.038 | Tree loss: 2.311 | Accuracy: 0.210938 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 028 | Total loss: 2.262 | Reg loss: 0.038 | Tree loss: 2.262 | Accuracy: 0.210938 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 028 | Total loss: 2.289 | Reg loss: 0.038 | Tree loss: 2.289 | Accuracy: 0.197266 | 6.957 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 028 | Total loss: 2.317 | Reg loss: 0.038 | Tree loss: 2.317 | Accuracy: 0.181641 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 028 | Total loss: 2.337 | Reg loss: 0.038 | Tree loss: 2.337 | Accuracy: 0.183594 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 028 | Total loss: 2.242 | Reg loss: 0.038 | Tree loss: 2.242 | Accuracy: 0.199219 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 028 | Total loss: 2.329 | Reg loss: 0.038 | Tree loss: 2.329 | Accuracy: 0.201172 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 028 | Total loss: 2.255 | Reg loss: 0.038 | Tree loss: 2.255 | Accuracy: 0.181641 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 028 | Total loss: 2.279 | Reg loss: 0.038 | Tree loss: 2.279 | Accuracy: 0.199219 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 028 | Total loss: 2.227 | Reg loss: 0.038 | Tree loss: 2.227 | Accuracy: 0.201172 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 028 | Total loss: 2.268 | Reg loss: 0.038 | Tree loss: 2.268 | Accuracy: 0.216797 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 028 | Total loss: 2.235 | Reg loss: 0.038 | Tree loss: 2.235 | Accuracy: 0.216797 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 028 | Total loss: 2.245 | Reg loss: 0.038 | Tree loss: 2.245 | Accuracy: 0.193359 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 028 | Total loss: 2.273 | Reg loss: 0.038 | Tree loss: 2.273 | Accuracy: 0.207031 | 6.958 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 028 | Total loss: 2.242 | Reg loss: 0.038 | Tree loss: 2.242 | Accuracy: 0.189453 | 6.959 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 028 | Total loss: 2.254 | Reg loss: 0.038 | Tree loss: 2.254 | Accuracy: 0.197266 | 6.959 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 028 | Total loss: 2.217 | Reg loss: 0.038 | Tree loss: 2.217 | Accuracy: 0.224609 | 6.959 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 028 | Total loss: 2.265 | Reg loss: 0.038 | Tree loss: 2.265 | Accuracy: 0.205078 | 6.959 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 028 | Total loss: 2.220 | Reg loss: 0.038 | Tree loss: 2.220 | Accuracy: 0.199219 | 6.959 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 028 | Total loss: 2.159 | Reg loss: 0.038 | Tree loss: 2.159 | Accuracy: 0.125000 | 6.957 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 028 | Total loss: 2.413 | Reg loss: 0.038 | Tree loss: 2.413 | Accuracy: 0.228516 | 6.961 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 028 | Total loss: 2.343 | Reg loss: 0.038 | Tree loss: 2.343 | Accuracy: 0.210938 | 6.96 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 028 | Total loss: 2.350 | Reg loss: 0.038 | Tree loss: 2.350 | Accuracy: 0.205078 | 6.961 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 028 | Total loss: 2.331 | Reg loss: 0.038 | Tree loss: 2.331 | Accuracy: 0.197266 | 6.961 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 028 | Total loss: 2.322 | Reg loss: 0.038 | Tree loss: 2.322 | Accuracy: 0.199219 | 6.962 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 028 | Total loss: 2.333 | Reg loss: 0.038 | Tree loss: 2.333 | Accuracy: 0.181641 | 6.962 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 028 | Total loss: 2.285 | Reg loss: 0.038 | Tree loss: 2.285 | Accuracy: 0.207031 | 6.962 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 028 | Total loss: 2.310 | Reg loss: 0.038 | Tree loss: 2.310 | Accuracy: 0.187500 | 6.962 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 028 | Total loss: 2.330 | Reg loss: 0.038 | Tree loss: 2.330 | Accuracy: 0.220703 | 6.962 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 028 | Total loss: 2.295 | Reg loss: 0.038 | Tree loss: 2.295 | Accuracy: 0.193359 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 028 | Total loss: 2.297 | Reg loss: 0.038 | Tree loss: 2.297 | Accuracy: 0.195312 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 028 | Total loss: 2.288 | Reg loss: 0.038 | Tree loss: 2.288 | Accuracy: 0.228516 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 028 | Total loss: 2.282 | Reg loss: 0.038 | Tree loss: 2.282 | Accuracy: 0.226562 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 028 | Total loss: 2.282 | Reg loss: 0.038 | Tree loss: 2.282 | Accuracy: 0.187500 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 028 | Total loss: 2.247 | Reg loss: 0.038 | Tree loss: 2.247 | Accuracy: 0.177734 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 028 | Total loss: 2.259 | Reg loss: 0.038 | Tree loss: 2.259 | Accuracy: 0.220703 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 028 | Total loss: 2.319 | Reg loss: 0.038 | Tree loss: 2.319 | Accuracy: 0.179688 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 028 | Total loss: 2.247 | Reg loss: 0.038 | Tree loss: 2.247 | Accuracy: 0.189453 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 028 | Total loss: 2.268 | Reg loss: 0.038 | Tree loss: 2.268 | Accuracy: 0.185547 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 028 | Total loss: 2.222 | Reg loss: 0.038 | Tree loss: 2.222 | Accuracy: 0.208984 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 028 | Total loss: 2.259 | Reg loss: 0.038 | Tree loss: 2.259 | Accuracy: 0.173828 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 028 | Total loss: 2.276 | Reg loss: 0.038 | Tree loss: 2.276 | Accuracy: 0.193359 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 028 | Total loss: 2.230 | Reg loss: 0.038 | Tree loss: 2.230 | Accuracy: 0.203125 | 6.964 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 028 | Total loss: 2.208 | Reg loss: 0.038 | Tree loss: 2.208 | Accuracy: 0.228516 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 028 | Total loss: 2.234 | Reg loss: 0.038 | Tree loss: 2.234 | Accuracy: 0.199219 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 028 | Total loss: 2.246 | Reg loss: 0.038 | Tree loss: 2.246 | Accuracy: 0.173828 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 028 | Total loss: 2.284 | Reg loss: 0.038 | Tree loss: 2.284 | Accuracy: 0.166016 | 6.963 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 028 | Total loss: 2.246 | Reg loss: 0.038 | Tree loss: 2.246 | Accuracy: 0.062500 | 6.961 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 028 | Total loss: 2.304 | Reg loss: 0.038 | Tree loss: 2.304 | Accuracy: 0.253906 | 6.963 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 028 | Total loss: 2.333 | Reg loss: 0.038 | Tree loss: 2.333 | Accuracy: 0.175781 | 6.963 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 028 | Total loss: 2.364 | Reg loss: 0.038 | Tree loss: 2.364 | Accuracy: 0.199219 | 6.963 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 028 | Total loss: 2.335 | Reg loss: 0.038 | Tree loss: 2.335 | Accuracy: 0.181641 | 6.963 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 028 | Total loss: 2.349 | Reg loss: 0.038 | Tree loss: 2.349 | Accuracy: 0.171875 | 6.963 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 028 | Total loss: 2.325 | Reg loss: 0.038 | Tree loss: 2.325 | Accuracy: 0.179688 | 6.964 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 028 | Total loss: 2.335 | Reg loss: 0.038 | Tree loss: 2.335 | Accuracy: 0.179688 | 6.964 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 028 | Total loss: 2.297 | Reg loss: 0.038 | Tree loss: 2.297 | Accuracy: 0.232422 | 6.964 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 028 | Total loss: 2.294 | Reg loss: 0.038 | Tree loss: 2.294 | Accuracy: 0.218750 | 6.965 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 028 | Total loss: 2.275 | Reg loss: 0.038 | Tree loss: 2.275 | Accuracy: 0.197266 | 6.965 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 010 / 028 | Total loss: 2.288 | Reg loss: 0.038 | Tree loss: 2.288 | Accuracy: 0.183594 | 6.965 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 028 | Total loss: 2.293 | Reg loss: 0.038 | Tree loss: 2.293 | Accuracy: 0.222656 | 6.965 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 028 | Total loss: 2.328 | Reg loss: 0.038 | Tree loss: 2.328 | Accuracy: 0.195312 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 028 | Total loss: 2.286 | Reg loss: 0.038 | Tree loss: 2.286 | Accuracy: 0.175781 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 028 | Total loss: 2.226 | Reg loss: 0.038 | Tree loss: 2.226 | Accuracy: 0.222656 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 028 | Total loss: 2.293 | Reg loss: 0.038 | Tree loss: 2.293 | Accuracy: 0.195312 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 028 | Total loss: 2.283 | Reg loss: 0.038 | Tree loss: 2.283 | Accuracy: 0.199219 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 028 | Total loss: 2.262 | Reg loss: 0.038 | Tree loss: 2.262 | Accuracy: 0.193359 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 028 | Total loss: 2.303 | Reg loss: 0.038 | Tree loss: 2.303 | Accuracy: 0.179688 | 6.967 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 028 | Total loss: 2.228 | Reg loss: 0.038 | Tree loss: 2.228 | Accuracy: 0.179688 | 6.967 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 028 | Total loss: 2.253 | Reg loss: 0.038 | Tree loss: 2.253 | Accuracy: 0.197266 | 6.967 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 028 | Total loss: 2.233 | Reg loss: 0.038 | Tree loss: 2.233 | Accuracy: 0.205078 | 6.967 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 028 | Total loss: 2.222 | Reg loss: 0.038 | Tree loss: 2.222 | Accuracy: 0.185547 | 6.967 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 028 | Total loss: 2.211 | Reg loss: 0.038 | Tree loss: 2.211 | Accuracy: 0.228516 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 028 | Total loss: 2.234 | Reg loss: 0.038 | Tree loss: 2.234 | Accuracy: 0.193359 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 028 | Total loss: 2.254 | Reg loss: 0.038 | Tree loss: 2.254 | Accuracy: 0.207031 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 028 | Total loss: 2.218 | Reg loss: 0.038 | Tree loss: 2.218 | Accuracy: 0.205078 | 6.966 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 028 | Total loss: 2.455 | Reg loss: 0.038 | Tree loss: 2.455 | Accuracy: 0.375000 | 6.964 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 028 | Total loss: 2.387 | Reg loss: 0.037 | Tree loss: 2.387 | Accuracy: 0.216797 | 6.969 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 028 | Total loss: 2.335 | Reg loss: 0.037 | Tree loss: 2.335 | Accuracy: 0.167969 | 6.968 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 028 | Total loss: 2.310 | Reg loss: 0.037 | Tree loss: 2.310 | Accuracy: 0.218750 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 028 | Total loss: 2.353 | Reg loss: 0.037 | Tree loss: 2.353 | Accuracy: 0.179688 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 028 | Total loss: 2.296 | Reg loss: 0.037 | Tree loss: 2.296 | Accuracy: 0.208984 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 028 | Total loss: 2.342 | Reg loss: 0.037 | Tree loss: 2.342 | Accuracy: 0.205078 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 028 | Total loss: 2.361 | Reg loss: 0.037 | Tree loss: 2.361 | Accuracy: 0.164062 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 028 | Total loss: 2.346 | Reg loss: 0.037 | Tree loss: 2.346 | Accuracy: 0.179688 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 028 | Total loss: 2.279 | Reg loss: 0.037 | Tree loss: 2.279 | Accuracy: 0.205078 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 028 | Total loss: 2.282 | Reg loss: 0.037 | Tree loss: 2.282 | Accuracy: 0.193359 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 028 | Total loss: 2.299 | Reg loss: 0.037 | Tree loss: 2.299 | Accuracy: 0.207031 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 028 | Total loss: 2.303 | Reg loss: 0.038 | Tree loss: 2.303 | Accuracy: 0.199219 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 028 | Total loss: 2.250 | Reg loss: 0.038 | Tree loss: 2.250 | Accuracy: 0.183594 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 028 | Total loss: 2.271 | Reg loss: 0.038 | Tree loss: 2.271 | Accuracy: 0.195312 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 028 | Total loss: 2.240 | Reg loss: 0.038 | Tree loss: 2.240 | Accuracy: 0.220703 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 028 | Total loss: 2.267 | Reg loss: 0.038 | Tree loss: 2.267 | Accuracy: 0.181641 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 028 | Total loss: 2.247 | Reg loss: 0.038 | Tree loss: 2.247 | Accuracy: 0.218750 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 028 | Total loss: 2.267 | Reg loss: 0.038 | Tree loss: 2.267 | Accuracy: 0.205078 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 028 | Total loss: 2.251 | Reg loss: 0.038 | Tree loss: 2.251 | Accuracy: 0.193359 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 028 | Total loss: 2.252 | Reg loss: 0.038 | Tree loss: 2.252 | Accuracy: 0.267578 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 028 | Total loss: 2.233 | Reg loss: 0.038 | Tree loss: 2.233 | Accuracy: 0.197266 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 028 | Total loss: 2.233 | Reg loss: 0.038 | Tree loss: 2.233 | Accuracy: 0.173828 | 6.966 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 028 | Total loss: 2.224 | Reg loss: 0.038 | Tree loss: 2.224 | Accuracy: 0.210938 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 028 | Total loss: 2.215 | Reg loss: 0.038 | Tree loss: 2.215 | Accuracy: 0.193359 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 028 | Total loss: 2.251 | Reg loss: 0.038 | Tree loss: 2.251 | Accuracy: 0.183594 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 028 | Total loss: 2.163 | Reg loss: 0.038 | Tree loss: 2.163 | Accuracy: 0.226562 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 028 | Total loss: 2.241 | Reg loss: 0.038 | Tree loss: 2.241 | Accuracy: 0.171875 | 6.967 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 028 | Total loss: 2.544 | Reg loss: 0.038 | Tree loss: 2.544 | Accuracy: 0.062500 | 6.965 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 028 | Total loss: 2.354 | Reg loss: 0.037 | Tree loss: 2.354 | Accuracy: 0.189453 | 6.972 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 028 | Total loss: 2.292 | Reg loss: 0.037 | Tree loss: 2.292 | Accuracy: 0.195312 | 6.972 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 028 | Total loss: 2.324 | Reg loss: 0.037 | Tree loss: 2.324 | Accuracy: 0.193359 | 6.971 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 028 | Total loss: 2.328 | Reg loss: 0.037 | Tree loss: 2.328 | Accuracy: 0.207031 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 028 | Total loss: 2.301 | Reg loss: 0.037 | Tree loss: 2.301 | Accuracy: 0.226562 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 028 | Total loss: 2.311 | Reg loss: 0.037 | Tree loss: 2.311 | Accuracy: 0.189453 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 028 | Total loss: 2.324 | Reg loss: 0.037 | Tree loss: 2.324 | Accuracy: 0.199219 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 028 | Total loss: 2.313 | Reg loss: 0.037 | Tree loss: 2.313 | Accuracy: 0.179688 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 028 | Total loss: 2.282 | Reg loss: 0.037 | Tree loss: 2.282 | Accuracy: 0.201172 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 028 | Total loss: 2.254 | Reg loss: 0.037 | Tree loss: 2.254 | Accuracy: 0.222656 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 028 | Total loss: 2.249 | Reg loss: 0.037 | Tree loss: 2.249 | Accuracy: 0.238281 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 028 | Total loss: 2.267 | Reg loss: 0.037 | Tree loss: 2.267 | Accuracy: 0.222656 | 6.969 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 028 | Total loss: 2.312 | Reg loss: 0.037 | Tree loss: 2.312 | Accuracy: 0.193359 | 6.969 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 028 | Total loss: 2.302 | Reg loss: 0.037 | Tree loss: 2.302 | Accuracy: 0.187500 | 6.969 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 028 | Total loss: 2.244 | Reg loss: 0.037 | Tree loss: 2.244 | Accuracy: 0.224609 | 6.969 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 015 / 028 | Total loss: 2.247 | Reg loss: 0.037 | Tree loss: 2.247 | Accuracy: 0.193359 | 6.969 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 028 | Total loss: 2.249 | Reg loss: 0.037 | Tree loss: 2.249 | Accuracy: 0.220703 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 028 | Total loss: 2.232 | Reg loss: 0.037 | Tree loss: 2.232 | Accuracy: 0.218750 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 028 | Total loss: 2.268 | Reg loss: 0.037 | Tree loss: 2.268 | Accuracy: 0.185547 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 028 | Total loss: 2.292 | Reg loss: 0.037 | Tree loss: 2.292 | Accuracy: 0.169922 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 028 | Total loss: 2.245 | Reg loss: 0.038 | Tree loss: 2.245 | Accuracy: 0.175781 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 028 | Total loss: 2.217 | Reg loss: 0.038 | Tree loss: 2.217 | Accuracy: 0.193359 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 028 | Total loss: 2.292 | Reg loss: 0.038 | Tree loss: 2.292 | Accuracy: 0.150391 | 6.97 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 028 | Total loss: 2.222 | Reg loss: 0.038 | Tree loss: 2.222 | Accuracy: 0.197266 | 6.971 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 028 | Total loss: 2.212 | Reg loss: 0.038 | Tree loss: 2.212 | Accuracy: 0.203125 | 6.971 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 028 | Total loss: 2.286 | Reg loss: 0.038 | Tree loss: 2.286 | Accuracy: 0.169922 | 6.971 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 028 | Total loss: 2.210 | Reg loss: 0.038 | Tree loss: 2.210 | Accuracy: 0.216797 | 6.971 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 028 | Total loss: 2.075 | Reg loss: 0.038 | Tree loss: 2.075 | Accuracy: 0.187500 | 6.969 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 028 | Total loss: 2.325 | Reg loss: 0.037 | Tree loss: 2.325 | Accuracy: 0.222656 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 028 | Total loss: 2.313 | Reg loss: 0.037 | Tree loss: 2.313 | Accuracy: 0.175781 | 6.979 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 028 | Total loss: 2.340 | Reg loss: 0.037 | Tree loss: 2.340 | Accuracy: 0.185547 | 6.979 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 028 | Total loss: 2.378 | Reg loss: 0.037 | Tree loss: 2.378 | Accuracy: 0.183594 | 6.979 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 028 | Total loss: 2.285 | Reg loss: 0.037 | Tree loss: 2.285 | Accuracy: 0.193359 | 6.979 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 028 | Total loss: 2.324 | Reg loss: 0.037 | Tree loss: 2.324 | Accuracy: 0.187500 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 028 | Total loss: 2.314 | Reg loss: 0.037 | Tree loss: 2.314 | Accuracy: 0.207031 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 028 | Total loss: 2.255 | Reg loss: 0.037 | Tree loss: 2.255 | Accuracy: 0.201172 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 028 | Total loss: 2.289 | Reg loss: 0.037 | Tree loss: 2.289 | Accuracy: 0.201172 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 028 | Total loss: 2.300 | Reg loss: 0.037 | Tree loss: 2.300 | Accuracy: 0.208984 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 028 | Total loss: 2.247 | Reg loss: 0.037 | Tree loss: 2.247 | Accuracy: 0.210938 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 028 | Total loss: 2.295 | Reg loss: 0.037 | Tree loss: 2.295 | Accuracy: 0.185547 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 028 | Total loss: 2.280 | Reg loss: 0.037 | Tree loss: 2.280 | Accuracy: 0.179688 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 028 | Total loss: 2.288 | Reg loss: 0.037 | Tree loss: 2.288 | Accuracy: 0.179688 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 028 | Total loss: 2.258 | Reg loss: 0.037 | Tree loss: 2.258 | Accuracy: 0.207031 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 028 | Total loss: 2.220 | Reg loss: 0.037 | Tree loss: 2.220 | Accuracy: 0.199219 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 028 | Total loss: 2.254 | Reg loss: 0.037 | Tree loss: 2.254 | Accuracy: 0.212891 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 028 | Total loss: 2.250 | Reg loss: 0.037 | Tree loss: 2.250 | Accuracy: 0.197266 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 028 | Total loss: 2.257 | Reg loss: 0.037 | Tree loss: 2.257 | Accuracy: 0.218750 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 028 | Total loss: 2.200 | Reg loss: 0.037 | Tree loss: 2.200 | Accuracy: 0.228516 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 028 | Total loss: 2.250 | Reg loss: 0.037 | Tree loss: 2.250 | Accuracy: 0.185547 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 028 | Total loss: 2.229 | Reg loss: 0.037 | Tree loss: 2.229 | Accuracy: 0.175781 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 028 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.193359 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 028 | Total loss: 2.182 | Reg loss: 0.037 | Tree loss: 2.182 | Accuracy: 0.230469 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 028 | Total loss: 2.227 | Reg loss: 0.037 | Tree loss: 2.227 | Accuracy: 0.218750 | 6.981 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 028 | Total loss: 2.293 | Reg loss: 0.037 | Tree loss: 2.293 | Accuracy: 0.156250 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 028 | Total loss: 2.248 | Reg loss: 0.037 | Tree loss: 2.248 | Accuracy: 0.210938 | 6.98 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 028 | Total loss: 1.972 | Reg loss: 0.038 | Tree loss: 1.972 | Accuracy: 0.437500 | 6.979 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 028 | Total loss: 2.282 | Reg loss: 0.037 | Tree loss: 2.282 | Accuracy: 0.191406 | 6.984 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 028 | Total loss: 2.360 | Reg loss: 0.037 | Tree loss: 2.360 | Accuracy: 0.199219 | 6.983 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 028 | Total loss: 2.307 | Reg loss: 0.037 | Tree loss: 2.307 | Accuracy: 0.181641 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 028 | Total loss: 2.305 | Reg loss: 0.037 | Tree loss: 2.305 | Accuracy: 0.201172 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 028 | Total loss: 2.334 | Reg loss: 0.037 | Tree loss: 2.334 | Accuracy: 0.183594 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 028 | Total loss: 2.355 | Reg loss: 0.037 | Tree loss: 2.355 | Accuracy: 0.234375 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 028 | Total loss: 2.337 | Reg loss: 0.037 | Tree loss: 2.337 | Accuracy: 0.212891 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 028 | Total loss: 2.277 | Reg loss: 0.037 | Tree loss: 2.277 | Accuracy: 0.222656 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 028 | Total loss: 2.305 | Reg loss: 0.037 | Tree loss: 2.305 | Accuracy: 0.167969 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 028 | Total loss: 2.265 | Reg loss: 0.037 | Tree loss: 2.265 | Accuracy: 0.203125 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 028 | Total loss: 2.255 | Reg loss: 0.037 | Tree loss: 2.255 | Accuracy: 0.222656 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 028 | Total loss: 2.288 | Reg loss: 0.037 | Tree loss: 2.288 | Accuracy: 0.197266 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 028 | Total loss: 2.270 | Reg loss: 0.037 | Tree loss: 2.270 | Accuracy: 0.162109 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 028 | Total loss: 2.239 | Reg loss: 0.037 | Tree loss: 2.239 | Accuracy: 0.212891 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 028 | Total loss: 2.253 | Reg loss: 0.037 | Tree loss: 2.253 | Accuracy: 0.181641 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 028 | Total loss: 2.267 | Reg loss: 0.037 | Tree loss: 2.267 | Accuracy: 0.208984 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 028 | Total loss: 2.211 | Reg loss: 0.037 | Tree loss: 2.211 | Accuracy: 0.195312 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 028 | Total loss: 2.270 | Reg loss: 0.037 | Tree loss: 2.270 | Accuracy: 0.183594 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 028 | Total loss: 2.213 | Reg loss: 0.037 | Tree loss: 2.213 | Accuracy: 0.191406 | 6.981 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 028 | Total loss: 2.235 | Reg loss: 0.037 | Tree loss: 2.235 | Accuracy: 0.220703 | 6.982 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 020 / 028 | Total loss: 2.216 | Reg loss: 0.037 | Tree loss: 2.216 | Accuracy: 0.230469 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 028 | Total loss: 2.269 | Reg loss: 0.037 | Tree loss: 2.269 | Accuracy: 0.205078 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 028 | Total loss: 2.212 | Reg loss: 0.037 | Tree loss: 2.212 | Accuracy: 0.183594 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 028 | Total loss: 2.305 | Reg loss: 0.037 | Tree loss: 2.305 | Accuracy: 0.158203 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 028 | Total loss: 2.221 | Reg loss: 0.037 | Tree loss: 2.221 | Accuracy: 0.197266 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 028 | Total loss: 2.198 | Reg loss: 0.037 | Tree loss: 2.198 | Accuracy: 0.191406 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 028 | Total loss: 2.184 | Reg loss: 0.037 | Tree loss: 2.184 | Accuracy: 0.218750 | 6.982 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 028 | Total loss: 2.051 | Reg loss: 0.037 | Tree loss: 2.051 | Accuracy: 0.375000 | 6.98 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 028 | Total loss: 2.365 | Reg loss: 0.037 | Tree loss: 2.365 | Accuracy: 0.175781 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 028 | Total loss: 2.318 | Reg loss: 0.037 | Tree loss: 2.318 | Accuracy: 0.216797 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 028 | Total loss: 2.307 | Reg loss: 0.037 | Tree loss: 2.307 | Accuracy: 0.207031 | 6.988 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 028 | Total loss: 2.347 | Reg loss: 0.037 | Tree loss: 2.347 | Accuracy: 0.199219 | 6.988 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 028 | Total loss: 2.265 | Reg loss: 0.037 | Tree loss: 2.265 | Accuracy: 0.195312 | 6.988 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 028 | Total loss: 2.276 | Reg loss: 0.037 | Tree loss: 2.276 | Accuracy: 0.212891 | 6.988 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 028 | Total loss: 2.280 | Reg loss: 0.037 | Tree loss: 2.280 | Accuracy: 0.232422 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 028 | Total loss: 2.332 | Reg loss: 0.037 | Tree loss: 2.332 | Accuracy: 0.173828 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 028 | Total loss: 2.300 | Reg loss: 0.037 | Tree loss: 2.300 | Accuracy: 0.183594 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 028 | Total loss: 2.234 | Reg loss: 0.037 | Tree loss: 2.234 | Accuracy: 0.228516 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 028 | Total loss: 2.305 | Reg loss: 0.037 | Tree loss: 2.305 | Accuracy: 0.175781 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 028 | Total loss: 2.316 | Reg loss: 0.037 | Tree loss: 2.316 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 028 | Total loss: 2.284 | Reg loss: 0.037 | Tree loss: 2.284 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 028 | Total loss: 2.257 | Reg loss: 0.037 | Tree loss: 2.257 | Accuracy: 0.203125 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 028 | Total loss: 2.271 | Reg loss: 0.037 | Tree loss: 2.271 | Accuracy: 0.201172 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 028 | Total loss: 2.209 | Reg loss: 0.037 | Tree loss: 2.209 | Accuracy: 0.195312 | 6.99 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 028 | Total loss: 2.226 | Reg loss: 0.037 | Tree loss: 2.226 | Accuracy: 0.208984 | 6.99 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 028 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.226562 | 6.99 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 028 | Total loss: 2.255 | Reg loss: 0.037 | Tree loss: 2.255 | Accuracy: 0.199219 | 6.99 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 028 | Total loss: 2.196 | Reg loss: 0.037 | Tree loss: 2.196 | Accuracy: 0.226562 | 6.99 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 028 | Total loss: 2.206 | Reg loss: 0.037 | Tree loss: 2.206 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 028 | Total loss: 2.258 | Reg loss: 0.037 | Tree loss: 2.258 | Accuracy: 0.173828 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 028 | Total loss: 2.207 | Reg loss: 0.037 | Tree loss: 2.207 | Accuracy: 0.201172 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 028 | Total loss: 2.264 | Reg loss: 0.037 | Tree loss: 2.264 | Accuracy: 0.183594 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 028 | Total loss: 2.205 | Reg loss: 0.037 | Tree loss: 2.205 | Accuracy: 0.191406 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 028 | Total loss: 2.174 | Reg loss: 0.037 | Tree loss: 2.174 | Accuracy: 0.201172 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 028 | Total loss: 2.244 | Reg loss: 0.037 | Tree loss: 2.244 | Accuracy: 0.197266 | 6.989 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 028 | Total loss: 2.203 | Reg loss: 0.037 | Tree loss: 2.203 | Accuracy: 0.125000 | 6.987 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 028 | Total loss: 2.330 | Reg loss: 0.037 | Tree loss: 2.330 | Accuracy: 0.218750 | 6.992 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 028 | Total loss: 2.370 | Reg loss: 0.037 | Tree loss: 2.370 | Accuracy: 0.166016 | 6.992 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 028 | Total loss: 2.326 | Reg loss: 0.037 | Tree loss: 2.326 | Accuracy: 0.187500 | 6.991 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 028 | Total loss: 2.320 | Reg loss: 0.037 | Tree loss: 2.320 | Accuracy: 0.197266 | 6.99 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 028 | Total loss: 2.269 | Reg loss: 0.037 | Tree loss: 2.269 | Accuracy: 0.197266 | 6.99 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 028 | Total loss: 2.318 | Reg loss: 0.037 | Tree loss: 2.318 | Accuracy: 0.214844 | 6.99 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 028 | Total loss: 2.329 | Reg loss: 0.037 | Tree loss: 2.329 | Accuracy: 0.191406 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 028 | Total loss: 2.250 | Reg loss: 0.037 | Tree loss: 2.250 | Accuracy: 0.210938 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 028 | Total loss: 2.269 | Reg loss: 0.037 | Tree loss: 2.269 | Accuracy: 0.193359 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 028 | Total loss: 2.268 | Reg loss: 0.037 | Tree loss: 2.268 | Accuracy: 0.224609 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 028 | Total loss: 2.252 | Reg loss: 0.037 | Tree loss: 2.252 | Accuracy: 0.216797 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 028 | Total loss: 2.269 | Reg loss: 0.037 | Tree loss: 2.269 | Accuracy: 0.193359 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 028 | Total loss: 2.260 | Reg loss: 0.037 | Tree loss: 2.260 | Accuracy: 0.177734 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 028 | Total loss: 2.241 | Reg loss: 0.037 | Tree loss: 2.241 | Accuracy: 0.167969 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 028 | Total loss: 2.237 | Reg loss: 0.037 | Tree loss: 2.237 | Accuracy: 0.187500 | 6.988 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 028 | Total loss: 2.285 | Reg loss: 0.037 | Tree loss: 2.285 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 028 | Total loss: 2.197 | Reg loss: 0.037 | Tree loss: 2.197 | Accuracy: 0.199219 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 028 | Total loss: 2.216 | Reg loss: 0.037 | Tree loss: 2.216 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 028 | Total loss: 2.238 | Reg loss: 0.037 | Tree loss: 2.238 | Accuracy: 0.185547 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 028 | Total loss: 2.266 | Reg loss: 0.037 | Tree loss: 2.266 | Accuracy: 0.224609 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 028 | Total loss: 2.226 | Reg loss: 0.037 | Tree loss: 2.226 | Accuracy: 0.218750 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 028 | Total loss: 2.245 | Reg loss: 0.037 | Tree loss: 2.245 | Accuracy: 0.226562 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 028 | Total loss: 2.187 | Reg loss: 0.037 | Tree loss: 2.187 | Accuracy: 0.210938 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 028 | Total loss: 2.228 | Reg loss: 0.037 | Tree loss: 2.228 | Accuracy: 0.179688 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 028 | Total loss: 2.188 | Reg loss: 0.037 | Tree loss: 2.188 | Accuracy: 0.203125 | 6.989 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 025 / 028 | Total loss: 2.253 | Reg loss: 0.037 | Tree loss: 2.253 | Accuracy: 0.189453 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 028 | Total loss: 2.207 | Reg loss: 0.037 | Tree loss: 2.207 | Accuracy: 0.210938 | 6.989 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 028 | Total loss: 2.166 | Reg loss: 0.037 | Tree loss: 2.166 | Accuracy: 0.187500 | 6.987 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 028 | Total loss: 2.312 | Reg loss: 0.037 | Tree loss: 2.312 | Accuracy: 0.205078 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 028 | Total loss: 2.302 | Reg loss: 0.037 | Tree loss: 2.302 | Accuracy: 0.205078 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 028 | Total loss: 2.304 | Reg loss: 0.037 | Tree loss: 2.304 | Accuracy: 0.179688 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 028 | Total loss: 2.326 | Reg loss: 0.037 | Tree loss: 2.326 | Accuracy: 0.193359 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 028 | Total loss: 2.342 | Reg loss: 0.037 | Tree loss: 2.342 | Accuracy: 0.162109 | 6.994 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 028 | Total loss: 2.304 | Reg loss: 0.037 | Tree loss: 2.304 | Accuracy: 0.187500 | 6.994 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 028 | Total loss: 2.235 | Reg loss: 0.037 | Tree loss: 2.235 | Accuracy: 0.226562 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 028 | Total loss: 2.249 | Reg loss: 0.037 | Tree loss: 2.249 | Accuracy: 0.207031 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 028 | Total loss: 2.316 | Reg loss: 0.037 | Tree loss: 2.316 | Accuracy: 0.195312 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 028 | Total loss: 2.275 | Reg loss: 0.037 | Tree loss: 2.275 | Accuracy: 0.179688 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 028 | Total loss: 2.231 | Reg loss: 0.037 | Tree loss: 2.231 | Accuracy: 0.208984 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 028 | Total loss: 2.258 | Reg loss: 0.037 | Tree loss: 2.258 | Accuracy: 0.203125 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 028 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.205078 | 6.995 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 028 | Total loss: 2.256 | Reg loss: 0.037 | Tree loss: 2.256 | Accuracy: 0.207031 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 028 | Total loss: 2.300 | Reg loss: 0.037 | Tree loss: 2.300 | Accuracy: 0.183594 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 028 | Total loss: 2.225 | Reg loss: 0.037 | Tree loss: 2.225 | Accuracy: 0.183594 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 028 | Total loss: 2.281 | Reg loss: 0.037 | Tree loss: 2.281 | Accuracy: 0.220703 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 028 | Total loss: 2.242 | Reg loss: 0.037 | Tree loss: 2.242 | Accuracy: 0.201172 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 028 | Total loss: 2.234 | Reg loss: 0.037 | Tree loss: 2.234 | Accuracy: 0.193359 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 028 | Total loss: 2.237 | Reg loss: 0.037 | Tree loss: 2.237 | Accuracy: 0.187500 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 028 | Total loss: 2.240 | Reg loss: 0.037 | Tree loss: 2.240 | Accuracy: 0.197266 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 028 | Total loss: 2.215 | Reg loss: 0.037 | Tree loss: 2.215 | Accuracy: 0.205078 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 028 | Total loss: 2.233 | Reg loss: 0.037 | Tree loss: 2.233 | Accuracy: 0.224609 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 028 | Total loss: 2.203 | Reg loss: 0.037 | Tree loss: 2.203 | Accuracy: 0.203125 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 028 | Total loss: 2.202 | Reg loss: 0.037 | Tree loss: 2.202 | Accuracy: 0.193359 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 028 | Total loss: 2.202 | Reg loss: 0.037 | Tree loss: 2.202 | Accuracy: 0.189453 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 028 | Total loss: 2.184 | Reg loss: 0.037 | Tree loss: 2.184 | Accuracy: 0.218750 | 6.996 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 028 | Total loss: 2.447 | Reg loss: 0.037 | Tree loss: 2.447 | Accuracy: 0.125000 | 6.994 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 028 | Total loss: 2.310 | Reg loss: 0.036 | Tree loss: 2.310 | Accuracy: 0.207031 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 028 | Total loss: 2.286 | Reg loss: 0.036 | Tree loss: 2.286 | Accuracy: 0.210938 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 028 | Total loss: 2.278 | Reg loss: 0.036 | Tree loss: 2.278 | Accuracy: 0.218750 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 028 | Total loss: 2.283 | Reg loss: 0.036 | Tree loss: 2.283 | Accuracy: 0.208984 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 028 | Total loss: 2.270 | Reg loss: 0.036 | Tree loss: 2.270 | Accuracy: 0.199219 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 028 | Total loss: 2.280 | Reg loss: 0.036 | Tree loss: 2.280 | Accuracy: 0.199219 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 028 | Total loss: 2.335 | Reg loss: 0.036 | Tree loss: 2.335 | Accuracy: 0.205078 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 028 | Total loss: 2.258 | Reg loss: 0.036 | Tree loss: 2.258 | Accuracy: 0.220703 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 028 | Total loss: 2.214 | Reg loss: 0.036 | Tree loss: 2.214 | Accuracy: 0.226562 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 028 | Total loss: 2.233 | Reg loss: 0.036 | Tree loss: 2.233 | Accuracy: 0.185547 | 6.998 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 028 | Total loss: 2.309 | Reg loss: 0.036 | Tree loss: 2.309 | Accuracy: 0.167969 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.173828 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 028 | Total loss: 2.240 | Reg loss: 0.037 | Tree loss: 2.240 | Accuracy: 0.199219 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 028 | Total loss: 2.274 | Reg loss: 0.037 | Tree loss: 2.274 | Accuracy: 0.210938 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 028 | Total loss: 2.267 | Reg loss: 0.037 | Tree loss: 2.267 | Accuracy: 0.210938 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 028 | Total loss: 2.227 | Reg loss: 0.037 | Tree loss: 2.227 | Accuracy: 0.199219 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 028 | Total loss: 2.248 | Reg loss: 0.037 | Tree loss: 2.248 | Accuracy: 0.203125 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 028 | Total loss: 2.260 | Reg loss: 0.037 | Tree loss: 2.260 | Accuracy: 0.179688 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 028 | Total loss: 2.276 | Reg loss: 0.037 | Tree loss: 2.276 | Accuracy: 0.175781 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 028 | Total loss: 2.267 | Reg loss: 0.037 | Tree loss: 2.267 | Accuracy: 0.167969 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 028 | Total loss: 2.247 | Reg loss: 0.037 | Tree loss: 2.247 | Accuracy: 0.171875 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 028 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.189453 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 028 | Total loss: 2.191 | Reg loss: 0.037 | Tree loss: 2.191 | Accuracy: 0.246094 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 028 | Total loss: 2.242 | Reg loss: 0.037 | Tree loss: 2.242 | Accuracy: 0.181641 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 028 | Total loss: 2.242 | Reg loss: 0.037 | Tree loss: 2.242 | Accuracy: 0.164062 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 028 | Total loss: 2.178 | Reg loss: 0.037 | Tree loss: 2.178 | Accuracy: 0.216797 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 028 | Total loss: 2.181 | Reg loss: 0.037 | Tree loss: 2.181 | Accuracy: 0.224609 | 6.999 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 028 | Total loss: 2.185 | Reg loss: 0.037 | Tree loss: 2.185 | Accuracy: 0.187500 | 6.998 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 028 | Total loss: 2.299 | Reg loss: 0.036 | Tree loss: 2.299 | Accuracy: 0.191406 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 028 | Total loss: 2.308 | Reg loss: 0.036 | Tree loss: 2.308 | Accuracy: 0.214844 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 028 | Total loss: 2.371 | Reg loss: 0.036 | Tree loss: 2.371 | Accuracy: 0.191406 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 028 | Total loss: 2.297 | Reg loss: 0.036 | Tree loss: 2.297 | Accuracy: 0.185547 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 028 | Total loss: 2.334 | Reg loss: 0.036 | Tree loss: 2.334 | Accuracy: 0.175781 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 028 | Total loss: 2.284 | Reg loss: 0.036 | Tree loss: 2.284 | Accuracy: 0.187500 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 028 | Total loss: 2.283 | Reg loss: 0.036 | Tree loss: 2.283 | Accuracy: 0.236328 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 028 | Total loss: 2.240 | Reg loss: 0.036 | Tree loss: 2.240 | Accuracy: 0.201172 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 028 | Total loss: 2.290 | Reg loss: 0.036 | Tree loss: 2.290 | Accuracy: 0.164062 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 028 | Total loss: 2.246 | Reg loss: 0.036 | Tree loss: 2.246 | Accuracy: 0.179688 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 028 | Total loss: 2.245 | Reg loss: 0.036 | Tree loss: 2.245 | Accuracy: 0.226562 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 028 | Total loss: 2.260 | Reg loss: 0.036 | Tree loss: 2.260 | Accuracy: 0.214844 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 028 | Total loss: 2.241 | Reg loss: 0.036 | Tree loss: 2.241 | Accuracy: 0.210938 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 028 | Total loss: 2.272 | Reg loss: 0.036 | Tree loss: 2.272 | Accuracy: 0.187500 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 028 | Total loss: 2.236 | Reg loss: 0.036 | Tree loss: 2.236 | Accuracy: 0.240234 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 028 | Total loss: 2.274 | Reg loss: 0.036 | Tree loss: 2.274 | Accuracy: 0.187500 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 028 | Total loss: 2.238 | Reg loss: 0.036 | Tree loss: 2.238 | Accuracy: 0.173828 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 028 | Total loss: 2.201 | Reg loss: 0.036 | Tree loss: 2.201 | Accuracy: 0.195312 | 7.003 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 028 | Total loss: 2.227 | Reg loss: 0.036 | Tree loss: 2.227 | Accuracy: 0.199219 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 028 | Total loss: 2.235 | Reg loss: 0.037 | Tree loss: 2.235 | Accuracy: 0.177734 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 028 | Total loss: 2.217 | Reg loss: 0.037 | Tree loss: 2.217 | Accuracy: 0.199219 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 028 | Total loss: 2.193 | Reg loss: 0.037 | Tree loss: 2.193 | Accuracy: 0.205078 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 028 | Total loss: 2.209 | Reg loss: 0.037 | Tree loss: 2.209 | Accuracy: 0.189453 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 028 | Total loss: 2.194 | Reg loss: 0.037 | Tree loss: 2.194 | Accuracy: 0.181641 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 028 | Total loss: 2.206 | Reg loss: 0.037 | Tree loss: 2.206 | Accuracy: 0.207031 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 028 | Total loss: 2.188 | Reg loss: 0.037 | Tree loss: 2.188 | Accuracy: 0.210938 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 028 | Total loss: 2.181 | Reg loss: 0.037 | Tree loss: 2.181 | Accuracy: 0.230469 | 7.002 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 028 | Total loss: 2.136 | Reg loss: 0.037 | Tree loss: 2.136 | Accuracy: 0.187500 | 7.0 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 028 | Total loss: 2.329 | Reg loss: 0.036 | Tree loss: 2.329 | Accuracy: 0.162109 | 7.005 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 028 | Total loss: 2.317 | Reg loss: 0.036 | Tree loss: 2.317 | Accuracy: 0.191406 | 7.004 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 028 | Total loss: 2.309 | Reg loss: 0.036 | Tree loss: 2.309 | Accuracy: 0.210938 | 7.003 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 028 | Total loss: 2.238 | Reg loss: 0.036 | Tree loss: 2.238 | Accuracy: 0.232422 | 7.002 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 028 | Total loss: 2.324 | Reg loss: 0.036 | Tree loss: 2.324 | Accuracy: 0.191406 | 7.001 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 028 | Total loss: 2.299 | Reg loss: 0.036 | Tree loss: 2.299 | Accuracy: 0.189453 | 7.0 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 028 | Total loss: 2.312 | Reg loss: 0.036 | Tree loss: 2.312 | Accuracy: 0.191406 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 028 | Total loss: 2.237 | Reg loss: 0.036 | Tree loss: 2.237 | Accuracy: 0.191406 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 028 | Total loss: 2.254 | Reg loss: 0.036 | Tree loss: 2.254 | Accuracy: 0.195312 | 6.997 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 028 | Total loss: 2.295 | Reg loss: 0.036 | Tree loss: 2.295 | Accuracy: 0.193359 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 028 | Total loss: 2.254 | Reg loss: 0.036 | Tree loss: 2.254 | Accuracy: 0.220703 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.179688 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 028 | Total loss: 2.232 | Reg loss: 0.036 | Tree loss: 2.232 | Accuracy: 0.208984 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 028 | Total loss: 2.262 | Reg loss: 0.036 | Tree loss: 2.262 | Accuracy: 0.187500 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.208984 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 028 | Total loss: 2.208 | Reg loss: 0.036 | Tree loss: 2.208 | Accuracy: 0.220703 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 028 | Total loss: 2.199 | Reg loss: 0.036 | Tree loss: 2.199 | Accuracy: 0.181641 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 028 | Total loss: 2.224 | Reg loss: 0.036 | Tree loss: 2.224 | Accuracy: 0.177734 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 028 | Total loss: 2.173 | Reg loss: 0.036 | Tree loss: 2.173 | Accuracy: 0.216797 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 028 | Total loss: 2.234 | Reg loss: 0.036 | Tree loss: 2.234 | Accuracy: 0.199219 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 028 | Total loss: 2.266 | Reg loss: 0.036 | Tree loss: 2.266 | Accuracy: 0.179688 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 028 | Total loss: 2.202 | Reg loss: 0.036 | Tree loss: 2.202 | Accuracy: 0.201172 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 028 | Total loss: 2.230 | Reg loss: 0.036 | Tree loss: 2.230 | Accuracy: 0.220703 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 028 | Total loss: 2.218 | Reg loss: 0.036 | Tree loss: 2.218 | Accuracy: 0.195312 | 6.999 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 028 | Total loss: 2.189 | Reg loss: 0.036 | Tree loss: 2.189 | Accuracy: 0.203125 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 028 | Total loss: 2.210 | Reg loss: 0.036 | Tree loss: 2.210 | Accuracy: 0.222656 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 028 | Total loss: 2.204 | Reg loss: 0.037 | Tree loss: 2.204 | Accuracy: 0.183594 | 6.998 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 028 | Total loss: 2.004 | Reg loss: 0.037 | Tree loss: 2.004 | Accuracy: 0.437500 | 6.997 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 028 | Total loss: 2.357 | Reg loss: 0.036 | Tree loss: 2.357 | Accuracy: 0.185547 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 028 | Total loss: 2.302 | Reg loss: 0.036 | Tree loss: 2.302 | Accuracy: 0.220703 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 028 | Total loss: 2.324 | Reg loss: 0.036 | Tree loss: 2.324 | Accuracy: 0.214844 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 028 | Total loss: 2.336 | Reg loss: 0.036 | Tree loss: 2.336 | Accuracy: 0.207031 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 028 | Total loss: 2.313 | Reg loss: 0.036 | Tree loss: 2.313 | Accuracy: 0.199219 | 7.0 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 005 / 028 | Total loss: 2.261 | Reg loss: 0.036 | Tree loss: 2.261 | Accuracy: 0.195312 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 028 | Total loss: 2.287 | Reg loss: 0.036 | Tree loss: 2.287 | Accuracy: 0.189453 | 7.0 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 028 | Total loss: 2.225 | Reg loss: 0.036 | Tree loss: 2.225 | Accuracy: 0.208984 | 6.999 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 028 | Total loss: 2.272 | Reg loss: 0.036 | Tree loss: 2.272 | Accuracy: 0.177734 | 6.999 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 028 | Total loss: 2.259 | Reg loss: 0.036 | Tree loss: 2.259 | Accuracy: 0.207031 | 6.999 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.199219 | 6.998 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 028 | Total loss: 2.257 | Reg loss: 0.036 | Tree loss: 2.257 | Accuracy: 0.175781 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 028 | Total loss: 2.283 | Reg loss: 0.036 | Tree loss: 2.283 | Accuracy: 0.191406 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.193359 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 028 | Total loss: 2.245 | Reg loss: 0.036 | Tree loss: 2.245 | Accuracy: 0.152344 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 028 | Total loss: 2.262 | Reg loss: 0.036 | Tree loss: 2.262 | Accuracy: 0.183594 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 028 | Total loss: 2.182 | Reg loss: 0.036 | Tree loss: 2.182 | Accuracy: 0.203125 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 028 | Total loss: 2.210 | Reg loss: 0.036 | Tree loss: 2.210 | Accuracy: 0.224609 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 028 | Total loss: 2.169 | Reg loss: 0.036 | Tree loss: 2.169 | Accuracy: 0.199219 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.210938 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.214844 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 028 | Total loss: 2.183 | Reg loss: 0.036 | Tree loss: 2.183 | Accuracy: 0.230469 | 6.996 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 028 | Total loss: 2.213 | Reg loss: 0.036 | Tree loss: 2.213 | Accuracy: 0.201172 | 6.996 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 028 | Total loss: 2.165 | Reg loss: 0.036 | Tree loss: 2.165 | Accuracy: 0.191406 | 6.996 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 028 | Total loss: 2.192 | Reg loss: 0.036 | Tree loss: 2.192 | Accuracy: 0.199219 | 6.996 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 028 | Total loss: 2.160 | Reg loss: 0.036 | Tree loss: 2.160 | Accuracy: 0.224609 | 6.996 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 028 | Total loss: 2.209 | Reg loss: 0.036 | Tree loss: 2.209 | Accuracy: 0.166016 | 6.997 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 028 | Total loss: 2.133 | Reg loss: 0.036 | Tree loss: 2.133 | Accuracy: 0.125000 | 6.995 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 028 | Total loss: 2.328 | Reg loss: 0.036 | Tree loss: 2.328 | Accuracy: 0.203125 | 7.009 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 028 | Total loss: 2.342 | Reg loss: 0.036 | Tree loss: 2.342 | Accuracy: 0.162109 | 7.009 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 028 | Total loss: 2.310 | Reg loss: 0.036 | Tree loss: 2.310 | Accuracy: 0.230469 | 7.009 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 028 | Total loss: 2.309 | Reg loss: 0.036 | Tree loss: 2.309 | Accuracy: 0.201172 | 7.009 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 028 | Total loss: 2.307 | Reg loss: 0.036 | Tree loss: 2.307 | Accuracy: 0.207031 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 028 | Total loss: 2.271 | Reg loss: 0.036 | Tree loss: 2.271 | Accuracy: 0.214844 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 028 | Total loss: 2.226 | Reg loss: 0.036 | Tree loss: 2.226 | Accuracy: 0.201172 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 028 | Total loss: 2.276 | Reg loss: 0.036 | Tree loss: 2.276 | Accuracy: 0.242188 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 028 | Total loss: 2.270 | Reg loss: 0.036 | Tree loss: 2.270 | Accuracy: 0.148438 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 028 | Total loss: 2.276 | Reg loss: 0.036 | Tree loss: 2.276 | Accuracy: 0.169922 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 028 | Total loss: 2.236 | Reg loss: 0.036 | Tree loss: 2.236 | Accuracy: 0.187500 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 028 | Total loss: 2.268 | Reg loss: 0.036 | Tree loss: 2.268 | Accuracy: 0.189453 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 028 | Total loss: 2.281 | Reg loss: 0.036 | Tree loss: 2.281 | Accuracy: 0.160156 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.197266 | 7.006 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 028 | Total loss: 2.265 | Reg loss: 0.036 | Tree loss: 2.265 | Accuracy: 0.171875 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.197266 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 028 | Total loss: 2.245 | Reg loss: 0.036 | Tree loss: 2.245 | Accuracy: 0.193359 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 028 | Total loss: 2.200 | Reg loss: 0.036 | Tree loss: 2.200 | Accuracy: 0.197266 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 028 | Total loss: 2.196 | Reg loss: 0.036 | Tree loss: 2.196 | Accuracy: 0.214844 | 7.007 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 028 | Total loss: 2.208 | Reg loss: 0.036 | Tree loss: 2.208 | Accuracy: 0.193359 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 028 | Total loss: 2.198 | Reg loss: 0.036 | Tree loss: 2.198 | Accuracy: 0.203125 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 028 | Total loss: 2.182 | Reg loss: 0.036 | Tree loss: 2.182 | Accuracy: 0.216797 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 028 | Total loss: 2.213 | Reg loss: 0.036 | Tree loss: 2.213 | Accuracy: 0.201172 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 028 | Total loss: 2.145 | Reg loss: 0.036 | Tree loss: 2.145 | Accuracy: 0.222656 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 028 | Total loss: 2.182 | Reg loss: 0.036 | Tree loss: 2.182 | Accuracy: 0.201172 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 028 | Total loss: 2.198 | Reg loss: 0.036 | Tree loss: 2.198 | Accuracy: 0.203125 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 028 | Total loss: 2.175 | Reg loss: 0.036 | Tree loss: 2.175 | Accuracy: 0.236328 | 7.008 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 028 | Total loss: 2.199 | Reg loss: 0.036 | Tree loss: 2.199 | Accuracy: 0.125000 | 7.006 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 028 | Total loss: 2.336 | Reg loss: 0.036 | Tree loss: 2.336 | Accuracy: 0.193359 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 028 | Total loss: 2.327 | Reg loss: 0.036 | Tree loss: 2.327 | Accuracy: 0.183594 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 028 | Total loss: 2.307 | Reg loss: 0.036 | Tree loss: 2.307 | Accuracy: 0.224609 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 028 | Total loss: 2.262 | Reg loss: 0.036 | Tree loss: 2.262 | Accuracy: 0.210938 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 028 | Total loss: 2.275 | Reg loss: 0.036 | Tree loss: 2.275 | Accuracy: 0.210938 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 028 | Total loss: 2.280 | Reg loss: 0.036 | Tree loss: 2.280 | Accuracy: 0.218750 | 7.011 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 028 | Total loss: 2.293 | Reg loss: 0.036 | Tree loss: 2.293 | Accuracy: 0.189453 | 7.01 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 028 | Total loss: 2.239 | Reg loss: 0.036 | Tree loss: 2.239 | Accuracy: 0.203125 | 7.009 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 028 | Total loss: 2.266 | Reg loss: 0.036 | Tree loss: 2.266 | Accuracy: 0.175781 | 7.008 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 028 | Total loss: 2.204 | Reg loss: 0.036 | Tree loss: 2.204 | Accuracy: 0.222656 | 7.008 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 010 / 028 | Total loss: 2.225 | Reg loss: 0.036 | Tree loss: 2.225 | Accuracy: 0.212891 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 028 | Total loss: 2.243 | Reg loss: 0.036 | Tree loss: 2.243 | Accuracy: 0.197266 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 028 | Total loss: 2.231 | Reg loss: 0.036 | Tree loss: 2.231 | Accuracy: 0.197266 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 028 | Total loss: 2.328 | Reg loss: 0.036 | Tree loss: 2.328 | Accuracy: 0.146484 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 028 | Total loss: 2.229 | Reg loss: 0.036 | Tree loss: 2.229 | Accuracy: 0.201172 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 028 | Total loss: 2.259 | Reg loss: 0.036 | Tree loss: 2.259 | Accuracy: 0.205078 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 028 | Total loss: 2.196 | Reg loss: 0.036 | Tree loss: 2.196 | Accuracy: 0.187500 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 028 | Total loss: 2.168 | Reg loss: 0.036 | Tree loss: 2.168 | Accuracy: 0.228516 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 028 | Total loss: 2.252 | Reg loss: 0.036 | Tree loss: 2.252 | Accuracy: 0.177734 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 028 | Total loss: 2.200 | Reg loss: 0.036 | Tree loss: 2.200 | Accuracy: 0.210938 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 028 | Total loss: 2.167 | Reg loss: 0.036 | Tree loss: 2.167 | Accuracy: 0.220703 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 028 | Total loss: 2.211 | Reg loss: 0.036 | Tree loss: 2.211 | Accuracy: 0.169922 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 028 | Total loss: 2.188 | Reg loss: 0.036 | Tree loss: 2.188 | Accuracy: 0.187500 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 028 | Total loss: 2.219 | Reg loss: 0.036 | Tree loss: 2.219 | Accuracy: 0.220703 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 028 | Total loss: 2.241 | Reg loss: 0.036 | Tree loss: 2.241 | Accuracy: 0.160156 | 7.006 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 028 | Total loss: 2.179 | Reg loss: 0.036 | Tree loss: 2.179 | Accuracy: 0.224609 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 028 | Total loss: 2.184 | Reg loss: 0.036 | Tree loss: 2.184 | Accuracy: 0.185547 | 7.007 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 028 | Total loss: 2.165 | Reg loss: 0.036 | Tree loss: 2.165 | Accuracy: 0.125000 | 7.005 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 028 | Total loss: 2.302 | Reg loss: 0.036 | Tree loss: 2.302 | Accuracy: 0.199219 | 7.02 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 028 | Total loss: 2.290 | Reg loss: 0.036 | Tree loss: 2.290 | Accuracy: 0.214844 | 7.02 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 028 | Total loss: 2.264 | Reg loss: 0.036 | Tree loss: 2.264 | Accuracy: 0.193359 | 7.02 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 028 | Total loss: 2.263 | Reg loss: 0.036 | Tree loss: 2.263 | Accuracy: 0.218750 | 7.02 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 028 | Total loss: 2.270 | Reg loss: 0.036 | Tree loss: 2.270 | Accuracy: 0.207031 | 7.019 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 028 | Total loss: 2.279 | Reg loss: 0.036 | Tree loss: 2.279 | Accuracy: 0.189453 | 7.019 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 028 | Total loss: 2.227 | Reg loss: 0.036 | Tree loss: 2.227 | Accuracy: 0.210938 | 7.018 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 028 | Total loss: 2.259 | Reg loss: 0.036 | Tree loss: 2.259 | Accuracy: 0.208984 | 7.017 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 028 | Total loss: 2.293 | Reg loss: 0.036 | Tree loss: 2.293 | Accuracy: 0.189453 | 7.016 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 028 | Total loss: 2.253 | Reg loss: 0.036 | Tree loss: 2.253 | Accuracy: 0.187500 | 7.015 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 028 | Total loss: 2.259 | Reg loss: 0.036 | Tree loss: 2.259 | Accuracy: 0.173828 | 7.014 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 028 | Total loss: 2.215 | Reg loss: 0.036 | Tree loss: 2.215 | Accuracy: 0.214844 | 7.014 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 028 | Total loss: 2.249 | Reg loss: 0.036 | Tree loss: 2.249 | Accuracy: 0.201172 | 7.013 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 028 | Total loss: 2.210 | Reg loss: 0.036 | Tree loss: 2.210 | Accuracy: 0.195312 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 028 | Total loss: 2.210 | Reg loss: 0.036 | Tree loss: 2.210 | Accuracy: 0.207031 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 028 | Total loss: 2.253 | Reg loss: 0.036 | Tree loss: 2.253 | Accuracy: 0.205078 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 028 | Total loss: 2.224 | Reg loss: 0.036 | Tree loss: 2.224 | Accuracy: 0.207031 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 028 | Total loss: 2.254 | Reg loss: 0.036 | Tree loss: 2.254 | Accuracy: 0.156250 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 028 | Total loss: 2.185 | Reg loss: 0.036 | Tree loss: 2.185 | Accuracy: 0.222656 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 028 | Total loss: 2.203 | Reg loss: 0.036 | Tree loss: 2.203 | Accuracy: 0.177734 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 028 | Total loss: 2.175 | Reg loss: 0.036 | Tree loss: 2.175 | Accuracy: 0.212891 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 028 | Total loss: 2.191 | Reg loss: 0.036 | Tree loss: 2.191 | Accuracy: 0.203125 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.199219 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 028 | Total loss: 2.191 | Reg loss: 0.036 | Tree loss: 2.191 | Accuracy: 0.208984 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 028 | Total loss: 2.216 | Reg loss: 0.036 | Tree loss: 2.216 | Accuracy: 0.177734 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 028 | Total loss: 2.248 | Reg loss: 0.036 | Tree loss: 2.248 | Accuracy: 0.208984 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 028 | Total loss: 2.207 | Reg loss: 0.036 | Tree loss: 2.207 | Accuracy: 0.175781 | 7.012 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 028 | Total loss: 2.234 | Reg loss: 0.036 | Tree loss: 2.234 | Accuracy: 0.125000 | 7.01 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 028 | Total loss: 2.288 | Reg loss: 0.035 | Tree loss: 2.288 | Accuracy: 0.193359 | 7.024 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 028 | Total loss: 2.296 | Reg loss: 0.035 | Tree loss: 2.296 | Accuracy: 0.191406 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 028 | Total loss: 2.295 | Reg loss: 0.035 | Tree loss: 2.295 | Accuracy: 0.167969 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 028 | Total loss: 2.272 | Reg loss: 0.035 | Tree loss: 2.272 | Accuracy: 0.201172 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 028 | Total loss: 2.243 | Reg loss: 0.035 | Tree loss: 2.243 | Accuracy: 0.226562 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 028 | Total loss: 2.240 | Reg loss: 0.035 | Tree loss: 2.240 | Accuracy: 0.210938 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 028 | Total loss: 2.257 | Reg loss: 0.035 | Tree loss: 2.257 | Accuracy: 0.199219 | 7.026 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 028 | Total loss: 2.295 | Reg loss: 0.035 | Tree loss: 2.295 | Accuracy: 0.187500 | 7.026 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 028 | Total loss: 2.273 | Reg loss: 0.035 | Tree loss: 2.273 | Accuracy: 0.210938 | 7.025 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 028 | Total loss: 2.241 | Reg loss: 0.035 | Tree loss: 2.241 | Accuracy: 0.189453 | 7.024 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 028 | Total loss: 2.272 | Reg loss: 0.036 | Tree loss: 2.272 | Accuracy: 0.207031 | 7.023 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 028 | Total loss: 2.206 | Reg loss: 0.036 | Tree loss: 2.206 | Accuracy: 0.177734 | 7.022 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 028 | Total loss: 2.268 | Reg loss: 0.036 | Tree loss: 2.268 | Accuracy: 0.214844 | 7.021 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 028 | Total loss: 2.205 | Reg loss: 0.036 | Tree loss: 2.205 | Accuracy: 0.185547 | 7.02 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 028 | Total loss: 2.262 | Reg loss: 0.036 | Tree loss: 2.262 | Accuracy: 0.173828 | 7.019 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 015 / 028 | Total loss: 2.218 | Reg loss: 0.036 | Tree loss: 2.218 | Accuracy: 0.212891 | 7.019 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 028 | Total loss: 2.192 | Reg loss: 0.036 | Tree loss: 2.192 | Accuracy: 0.208984 | 7.018 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 028 | Total loss: 2.200 | Reg loss: 0.036 | Tree loss: 2.200 | Accuracy: 0.220703 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 028 | Total loss: 2.217 | Reg loss: 0.036 | Tree loss: 2.217 | Accuracy: 0.199219 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 028 | Total loss: 2.218 | Reg loss: 0.036 | Tree loss: 2.218 | Accuracy: 0.212891 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 028 | Total loss: 2.197 | Reg loss: 0.036 | Tree loss: 2.197 | Accuracy: 0.181641 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 028 | Total loss: 2.229 | Reg loss: 0.036 | Tree loss: 2.229 | Accuracy: 0.222656 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 028 | Total loss: 2.230 | Reg loss: 0.036 | Tree loss: 2.230 | Accuracy: 0.195312 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 028 | Total loss: 2.192 | Reg loss: 0.036 | Tree loss: 2.192 | Accuracy: 0.173828 | 7.017 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 028 | Total loss: 2.154 | Reg loss: 0.036 | Tree loss: 2.154 | Accuracy: 0.187500 | 7.018 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 028 | Total loss: 2.171 | Reg loss: 0.036 | Tree loss: 2.171 | Accuracy: 0.230469 | 7.018 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 028 | Total loss: 2.238 | Reg loss: 0.036 | Tree loss: 2.238 | Accuracy: 0.179688 | 7.018 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 028 | Total loss: 2.153 | Reg loss: 0.036 | Tree loss: 2.153 | Accuracy: 0.250000 | 7.016 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 028 | Total loss: 2.257 | Reg loss: 0.035 | Tree loss: 2.257 | Accuracy: 0.232422 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 028 | Total loss: 2.282 | Reg loss: 0.035 | Tree loss: 2.282 | Accuracy: 0.208984 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 028 | Total loss: 2.251 | Reg loss: 0.035 | Tree loss: 2.251 | Accuracy: 0.201172 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 028 | Total loss: 2.300 | Reg loss: 0.035 | Tree loss: 2.300 | Accuracy: 0.187500 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 028 | Total loss: 2.260 | Reg loss: 0.035 | Tree loss: 2.260 | Accuracy: 0.199219 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 028 | Total loss: 2.271 | Reg loss: 0.035 | Tree loss: 2.271 | Accuracy: 0.212891 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 028 | Total loss: 2.268 | Reg loss: 0.035 | Tree loss: 2.268 | Accuracy: 0.193359 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 028 | Total loss: 2.294 | Reg loss: 0.035 | Tree loss: 2.294 | Accuracy: 0.208984 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 028 | Total loss: 2.232 | Reg loss: 0.035 | Tree loss: 2.232 | Accuracy: 0.197266 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 028 | Total loss: 2.216 | Reg loss: 0.035 | Tree loss: 2.216 | Accuracy: 0.246094 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 028 | Total loss: 2.268 | Reg loss: 0.035 | Tree loss: 2.268 | Accuracy: 0.189453 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 028 | Total loss: 2.283 | Reg loss: 0.035 | Tree loss: 2.283 | Accuracy: 0.169922 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 028 | Total loss: 2.237 | Reg loss: 0.035 | Tree loss: 2.237 | Accuracy: 0.185547 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 028 | Total loss: 2.236 | Reg loss: 0.035 | Tree loss: 2.236 | Accuracy: 0.197266 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 028 | Total loss: 2.226 | Reg loss: 0.035 | Tree loss: 2.226 | Accuracy: 0.173828 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 028 | Total loss: 2.184 | Reg loss: 0.035 | Tree loss: 2.184 | Accuracy: 0.222656 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 028 | Total loss: 2.162 | Reg loss: 0.035 | Tree loss: 2.162 | Accuracy: 0.179688 | 7.021 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 028 | Total loss: 2.218 | Reg loss: 0.036 | Tree loss: 2.218 | Accuracy: 0.197266 | 7.02 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 028 | Total loss: 2.200 | Reg loss: 0.036 | Tree loss: 2.200 | Accuracy: 0.197266 | 7.019 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 028 | Total loss: 2.226 | Reg loss: 0.036 | Tree loss: 2.226 | Accuracy: 0.179688 | 7.018 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 028 | Total loss: 2.214 | Reg loss: 0.036 | Tree loss: 2.214 | Accuracy: 0.203125 | 7.017 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 028 | Total loss: 2.221 | Reg loss: 0.036 | Tree loss: 2.221 | Accuracy: 0.248047 | 7.016 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 028 | Total loss: 2.258 | Reg loss: 0.036 | Tree loss: 2.258 | Accuracy: 0.181641 | 7.015 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 028 | Total loss: 2.174 | Reg loss: 0.036 | Tree loss: 2.174 | Accuracy: 0.210938 | 7.014 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 028 | Total loss: 2.202 | Reg loss: 0.036 | Tree loss: 2.202 | Accuracy: 0.156250 | 7.014 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 028 | Total loss: 2.160 | Reg loss: 0.036 | Tree loss: 2.160 | Accuracy: 0.193359 | 7.014 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 028 | Total loss: 2.219 | Reg loss: 0.036 | Tree loss: 2.219 | Accuracy: 0.191406 | 7.014 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 028 | Total loss: 2.460 | Reg loss: 0.036 | Tree loss: 2.460 | Accuracy: 0.187500 | 7.012 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 028 | Total loss: 2.286 | Reg loss: 0.035 | Tree loss: 2.286 | Accuracy: 0.208984 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 028 | Total loss: 2.320 | Reg loss: 0.035 | Tree loss: 2.320 | Accuracy: 0.199219 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 028 | Total loss: 2.256 | Reg loss: 0.035 | Tree loss: 2.256 | Accuracy: 0.216797 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 028 | Total loss: 2.269 | Reg loss: 0.035 | Tree loss: 2.269 | Accuracy: 0.195312 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 028 | Total loss: 2.276 | Reg loss: 0.035 | Tree loss: 2.276 | Accuracy: 0.193359 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 028 | Total loss: 2.263 | Reg loss: 0.035 | Tree loss: 2.263 | Accuracy: 0.207031 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 028 | Total loss: 2.283 | Reg loss: 0.035 | Tree loss: 2.283 | Accuracy: 0.187500 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 028 | Total loss: 2.249 | Reg loss: 0.035 | Tree loss: 2.249 | Accuracy: 0.179688 | 7.017 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 028 | Total loss: 2.255 | Reg loss: 0.035 | Tree loss: 2.255 | Accuracy: 0.175781 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 028 | Total loss: 2.259 | Reg loss: 0.035 | Tree loss: 2.259 | Accuracy: 0.187500 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 028 | Total loss: 2.250 | Reg loss: 0.035 | Tree loss: 2.250 | Accuracy: 0.197266 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 028 | Total loss: 2.260 | Reg loss: 0.035 | Tree loss: 2.260 | Accuracy: 0.189453 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 028 | Total loss: 2.269 | Reg loss: 0.035 | Tree loss: 2.269 | Accuracy: 0.173828 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 028 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.214844 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 028 | Total loss: 2.219 | Reg loss: 0.035 | Tree loss: 2.219 | Accuracy: 0.201172 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 028 | Total loss: 2.234 | Reg loss: 0.035 | Tree loss: 2.234 | Accuracy: 0.169922 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 028 | Total loss: 2.218 | Reg loss: 0.035 | Tree loss: 2.218 | Accuracy: 0.177734 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 028 | Total loss: 2.241 | Reg loss: 0.035 | Tree loss: 2.241 | Accuracy: 0.189453 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 028 | Total loss: 2.242 | Reg loss: 0.035 | Tree loss: 2.242 | Accuracy: 0.205078 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 028 | Total loss: 2.169 | Reg loss: 0.035 | Tree loss: 2.169 | Accuracy: 0.228516 | 7.016 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 020 / 028 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.222656 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 028 | Total loss: 2.140 | Reg loss: 0.035 | Tree loss: 2.140 | Accuracy: 0.218750 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 028 | Total loss: 2.185 | Reg loss: 0.036 | Tree loss: 2.185 | Accuracy: 0.222656 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 028 | Total loss: 2.170 | Reg loss: 0.036 | Tree loss: 2.170 | Accuracy: 0.203125 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 028 | Total loss: 2.232 | Reg loss: 0.036 | Tree loss: 2.232 | Accuracy: 0.207031 | 7.016 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 028 | Total loss: 2.173 | Reg loss: 0.036 | Tree loss: 2.173 | Accuracy: 0.195312 | 7.015 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 028 | Total loss: 2.188 | Reg loss: 0.036 | Tree loss: 2.188 | Accuracy: 0.197266 | 7.014 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 028 | Total loss: 2.076 | Reg loss: 0.036 | Tree loss: 2.076 | Accuracy: 0.187500 | 7.012 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 028 | Total loss: 2.297 | Reg loss: 0.035 | Tree loss: 2.297 | Accuracy: 0.185547 | 7.018 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 028 | Total loss: 2.330 | Reg loss: 0.035 | Tree loss: 2.330 | Accuracy: 0.203125 | 7.018 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 028 | Total loss: 2.282 | Reg loss: 0.035 | Tree loss: 2.282 | Accuracy: 0.208984 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 028 | Total loss: 2.263 | Reg loss: 0.035 | Tree loss: 2.263 | Accuracy: 0.177734 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 028 | Total loss: 2.302 | Reg loss: 0.035 | Tree loss: 2.302 | Accuracy: 0.187500 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 028 | Total loss: 2.254 | Reg loss: 0.035 | Tree loss: 2.254 | Accuracy: 0.185547 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 028 | Total loss: 2.240 | Reg loss: 0.035 | Tree loss: 2.240 | Accuracy: 0.197266 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 028 | Total loss: 2.230 | Reg loss: 0.035 | Tree loss: 2.230 | Accuracy: 0.179688 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 028 | Total loss: 2.223 | Reg loss: 0.035 | Tree loss: 2.223 | Accuracy: 0.197266 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 028 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.212891 | 7.016 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 028 | Total loss: 2.264 | Reg loss: 0.035 | Tree loss: 2.264 | Accuracy: 0.160156 | 7.016 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 028 | Total loss: 2.267 | Reg loss: 0.035 | Tree loss: 2.267 | Accuracy: 0.210938 | 7.016 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 028 | Total loss: 2.251 | Reg loss: 0.035 | Tree loss: 2.251 | Accuracy: 0.195312 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 028 | Total loss: 2.236 | Reg loss: 0.035 | Tree loss: 2.236 | Accuracy: 0.218750 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 028 | Total loss: 2.244 | Reg loss: 0.035 | Tree loss: 2.244 | Accuracy: 0.230469 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 028 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.189453 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 028 | Total loss: 2.194 | Reg loss: 0.035 | Tree loss: 2.194 | Accuracy: 0.210938 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 028 | Total loss: 2.195 | Reg loss: 0.035 | Tree loss: 2.195 | Accuracy: 0.210938 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.203125 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 028 | Total loss: 2.204 | Reg loss: 0.035 | Tree loss: 2.204 | Accuracy: 0.222656 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 028 | Total loss: 2.184 | Reg loss: 0.035 | Tree loss: 2.184 | Accuracy: 0.193359 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 028 | Total loss: 2.172 | Reg loss: 0.035 | Tree loss: 2.172 | Accuracy: 0.208984 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 028 | Total loss: 2.200 | Reg loss: 0.035 | Tree loss: 2.200 | Accuracy: 0.187500 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 028 | Total loss: 2.177 | Reg loss: 0.035 | Tree loss: 2.177 | Accuracy: 0.236328 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.173828 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 028 | Total loss: 2.166 | Reg loss: 0.035 | Tree loss: 2.166 | Accuracy: 0.201172 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 028 | Total loss: 2.235 | Reg loss: 0.035 | Tree loss: 2.235 | Accuracy: 0.175781 | 7.017 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 028 | Total loss: 2.255 | Reg loss: 0.036 | Tree loss: 2.255 | Accuracy: 0.187500 | 7.016 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 028 | Total loss: 2.283 | Reg loss: 0.035 | Tree loss: 2.283 | Accuracy: 0.216797 | 7.019 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 028 | Total loss: 2.314 | Reg loss: 0.035 | Tree loss: 2.314 | Accuracy: 0.171875 | 7.018 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 028 | Total loss: 2.326 | Reg loss: 0.035 | Tree loss: 2.326 | Accuracy: 0.199219 | 7.017 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 028 | Total loss: 2.298 | Reg loss: 0.035 | Tree loss: 2.298 | Accuracy: 0.166016 | 7.017 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 028 | Total loss: 2.246 | Reg loss: 0.035 | Tree loss: 2.246 | Accuracy: 0.222656 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 028 | Total loss: 2.261 | Reg loss: 0.035 | Tree loss: 2.261 | Accuracy: 0.183594 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 028 | Total loss: 2.266 | Reg loss: 0.035 | Tree loss: 2.266 | Accuracy: 0.195312 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 028 | Total loss: 2.258 | Reg loss: 0.035 | Tree loss: 2.258 | Accuracy: 0.205078 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 028 | Total loss: 2.243 | Reg loss: 0.035 | Tree loss: 2.243 | Accuracy: 0.173828 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.207031 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 028 | Total loss: 2.237 | Reg loss: 0.035 | Tree loss: 2.237 | Accuracy: 0.238281 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.208984 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.187500 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 028 | Total loss: 2.237 | Reg loss: 0.035 | Tree loss: 2.237 | Accuracy: 0.181641 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 028 | Total loss: 2.227 | Reg loss: 0.035 | Tree loss: 2.227 | Accuracy: 0.212891 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 028 | Total loss: 2.203 | Reg loss: 0.035 | Tree loss: 2.203 | Accuracy: 0.197266 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 028 | Total loss: 2.228 | Reg loss: 0.035 | Tree loss: 2.228 | Accuracy: 0.218750 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 028 | Total loss: 2.175 | Reg loss: 0.035 | Tree loss: 2.175 | Accuracy: 0.228516 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 028 | Total loss: 2.215 | Reg loss: 0.035 | Tree loss: 2.215 | Accuracy: 0.171875 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 028 | Total loss: 2.251 | Reg loss: 0.035 | Tree loss: 2.251 | Accuracy: 0.179688 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 028 | Total loss: 2.164 | Reg loss: 0.035 | Tree loss: 2.164 | Accuracy: 0.214844 | 7.015 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 028 | Total loss: 2.196 | Reg loss: 0.035 | Tree loss: 2.196 | Accuracy: 0.203125 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 028 | Total loss: 2.238 | Reg loss: 0.035 | Tree loss: 2.238 | Accuracy: 0.162109 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 028 | Total loss: 2.161 | Reg loss: 0.035 | Tree loss: 2.161 | Accuracy: 0.236328 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 028 | Total loss: 2.135 | Reg loss: 0.035 | Tree loss: 2.135 | Accuracy: 0.199219 | 7.016 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 025 / 028 | Total loss: 2.202 | Reg loss: 0.035 | Tree loss: 2.202 | Accuracy: 0.191406 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 028 | Total loss: 2.153 | Reg loss: 0.035 | Tree loss: 2.153 | Accuracy: 0.193359 | 7.016 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 028 | Total loss: 2.107 | Reg loss: 0.035 | Tree loss: 2.107 | Accuracy: 0.125000 | 7.014 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 028 | Total loss: 2.274 | Reg loss: 0.035 | Tree loss: 2.274 | Accuracy: 0.214844 | 7.015 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 028 | Total loss: 2.294 | Reg loss: 0.035 | Tree loss: 2.294 | Accuracy: 0.166016 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 028 | Total loss: 2.335 | Reg loss: 0.035 | Tree loss: 2.335 | Accuracy: 0.167969 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 028 | Total loss: 2.258 | Reg loss: 0.035 | Tree loss: 2.258 | Accuracy: 0.199219 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 028 | Total loss: 2.269 | Reg loss: 0.035 | Tree loss: 2.269 | Accuracy: 0.218750 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 028 | Total loss: 2.275 | Reg loss: 0.035 | Tree loss: 2.275 | Accuracy: 0.189453 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 028 | Total loss: 2.221 | Reg loss: 0.035 | Tree loss: 2.221 | Accuracy: 0.203125 | 7.015 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 028 | Total loss: 2.260 | Reg loss: 0.035 | Tree loss: 2.260 | Accuracy: 0.185547 | 7.015 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 028 | Total loss: 2.245 | Reg loss: 0.035 | Tree loss: 2.245 | Accuracy: 0.191406 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 028 | Total loss: 2.267 | Reg loss: 0.035 | Tree loss: 2.267 | Accuracy: 0.164062 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 028 | Total loss: 2.216 | Reg loss: 0.035 | Tree loss: 2.216 | Accuracy: 0.203125 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 028 | Total loss: 2.243 | Reg loss: 0.035 | Tree loss: 2.243 | Accuracy: 0.183594 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 028 | Total loss: 2.231 | Reg loss: 0.035 | Tree loss: 2.231 | Accuracy: 0.207031 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 028 | Total loss: 2.262 | Reg loss: 0.035 | Tree loss: 2.262 | Accuracy: 0.208984 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 028 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.205078 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 028 | Total loss: 2.225 | Reg loss: 0.035 | Tree loss: 2.225 | Accuracy: 0.207031 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 028 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.224609 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 028 | Total loss: 2.219 | Reg loss: 0.035 | Tree loss: 2.219 | Accuracy: 0.201172 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 028 | Total loss: 2.186 | Reg loss: 0.035 | Tree loss: 2.186 | Accuracy: 0.205078 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 028 | Total loss: 2.212 | Reg loss: 0.035 | Tree loss: 2.212 | Accuracy: 0.216797 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 028 | Total loss: 2.180 | Reg loss: 0.035 | Tree loss: 2.180 | Accuracy: 0.187500 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 028 | Total loss: 2.195 | Reg loss: 0.035 | Tree loss: 2.195 | Accuracy: 0.210938 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 028 | Total loss: 2.165 | Reg loss: 0.035 | Tree loss: 2.165 | Accuracy: 0.226562 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 028 | Total loss: 2.199 | Reg loss: 0.035 | Tree loss: 2.199 | Accuracy: 0.187500 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 028 | Total loss: 2.156 | Reg loss: 0.035 | Tree loss: 2.156 | Accuracy: 0.207031 | 7.017 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 028 | Total loss: 2.188 | Reg loss: 0.035 | Tree loss: 2.188 | Accuracy: 0.191406 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 028 | Total loss: 2.171 | Reg loss: 0.035 | Tree loss: 2.171 | Accuracy: 0.195312 | 7.016 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 028 | Total loss: 2.378 | Reg loss: 0.035 | Tree loss: 2.378 | Accuracy: 0.062500 | 7.015 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 028 | Total loss: 2.323 | Reg loss: 0.035 | Tree loss: 2.323 | Accuracy: 0.173828 | 7.019 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 028 | Total loss: 2.321 | Reg loss: 0.035 | Tree loss: 2.321 | Accuracy: 0.212891 | 7.018 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 028 | Total loss: 2.299 | Reg loss: 0.035 | Tree loss: 2.299 | Accuracy: 0.197266 | 7.017 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 028 | Total loss: 2.253 | Reg loss: 0.035 | Tree loss: 2.253 | Accuracy: 0.207031 | 7.017 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 028 | Total loss: 2.241 | Reg loss: 0.035 | Tree loss: 2.241 | Accuracy: 0.193359 | 7.017 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 028 | Total loss: 2.270 | Reg loss: 0.035 | Tree loss: 2.270 | Accuracy: 0.214844 | 7.017 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 028 | Total loss: 2.273 | Reg loss: 0.035 | Tree loss: 2.273 | Accuracy: 0.205078 | 7.016 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 028 | Total loss: 2.252 | Reg loss: 0.035 | Tree loss: 2.252 | Accuracy: 0.199219 | 7.016 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 028 | Total loss: 2.207 | Reg loss: 0.035 | Tree loss: 2.207 | Accuracy: 0.224609 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 028 | Total loss: 2.239 | Reg loss: 0.035 | Tree loss: 2.239 | Accuracy: 0.171875 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 028 | Total loss: 2.277 | Reg loss: 0.035 | Tree loss: 2.277 | Accuracy: 0.189453 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 028 | Total loss: 2.256 | Reg loss: 0.035 | Tree loss: 2.256 | Accuracy: 0.175781 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 028 | Total loss: 2.196 | Reg loss: 0.035 | Tree loss: 2.196 | Accuracy: 0.183594 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 028 | Total loss: 2.208 | Reg loss: 0.035 | Tree loss: 2.208 | Accuracy: 0.208984 | 7.015 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 028 | Total loss: 2.224 | Reg loss: 0.035 | Tree loss: 2.224 | Accuracy: 0.189453 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 028 | Total loss: 2.240 | Reg loss: 0.035 | Tree loss: 2.240 | Accuracy: 0.177734 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 028 | Total loss: 2.157 | Reg loss: 0.035 | Tree loss: 2.157 | Accuracy: 0.203125 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 028 | Total loss: 2.242 | Reg loss: 0.035 | Tree loss: 2.242 | Accuracy: 0.181641 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 028 | Total loss: 2.205 | Reg loss: 0.035 | Tree loss: 2.205 | Accuracy: 0.189453 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 028 | Total loss: 2.176 | Reg loss: 0.035 | Tree loss: 2.176 | Accuracy: 0.203125 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 028 | Total loss: 2.146 | Reg loss: 0.035 | Tree loss: 2.146 | Accuracy: 0.230469 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 028 | Total loss: 2.193 | Reg loss: 0.035 | Tree loss: 2.193 | Accuracy: 0.210938 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 028 | Total loss: 2.190 | Reg loss: 0.035 | Tree loss: 2.190 | Accuracy: 0.189453 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 028 | Total loss: 2.178 | Reg loss: 0.035 | Tree loss: 2.178 | Accuracy: 0.205078 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 028 | Total loss: 2.179 | Reg loss: 0.035 | Tree loss: 2.179 | Accuracy: 0.207031 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 028 | Total loss: 2.153 | Reg loss: 0.035 | Tree loss: 2.153 | Accuracy: 0.208984 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 028 | Total loss: 2.171 | Reg loss: 0.035 | Tree loss: 2.171 | Accuracy: 0.210938 | 7.014 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 028 | Total loss: 2.061 | Reg loss: 0.035 | Tree loss: 2.061 | Accuracy: 0.187500 | 7.013 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 028 | Total loss: 2.279 | Reg loss: 0.035 | Tree loss: 2.279 | Accuracy: 0.199219 | 7.021 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 028 | Total loss: 2.288 | Reg loss: 0.035 | Tree loss: 2.288 | Accuracy: 0.197266 | 7.02 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 028 | Total loss: 2.263 | Reg loss: 0.035 | Tree loss: 2.263 | Accuracy: 0.197266 | 7.02 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 028 | Total loss: 2.358 | Reg loss: 0.035 | Tree loss: 2.358 | Accuracy: 0.197266 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 028 | Total loss: 2.224 | Reg loss: 0.035 | Tree loss: 2.224 | Accuracy: 0.230469 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 028 | Total loss: 2.245 | Reg loss: 0.035 | Tree loss: 2.245 | Accuracy: 0.201172 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 028 | Total loss: 2.258 | Reg loss: 0.035 | Tree loss: 2.258 | Accuracy: 0.203125 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 028 | Total loss: 2.201 | Reg loss: 0.035 | Tree loss: 2.201 | Accuracy: 0.208984 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 028 | Total loss: 2.239 | Reg loss: 0.035 | Tree loss: 2.239 | Accuracy: 0.179688 | 7.018 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 028 | Total loss: 2.210 | Reg loss: 0.035 | Tree loss: 2.210 | Accuracy: 0.210938 | 7.018 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 028 | Total loss: 2.279 | Reg loss: 0.035 | Tree loss: 2.279 | Accuracy: 0.218750 | 7.018 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 028 | Total loss: 2.214 | Reg loss: 0.035 | Tree loss: 2.214 | Accuracy: 0.193359 | 7.018 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 028 | Total loss: 2.239 | Reg loss: 0.035 | Tree loss: 2.239 | Accuracy: 0.183594 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 028 | Total loss: 2.217 | Reg loss: 0.035 | Tree loss: 2.217 | Accuracy: 0.195312 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 028 | Total loss: 2.200 | Reg loss: 0.035 | Tree loss: 2.200 | Accuracy: 0.234375 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 028 | Total loss: 2.200 | Reg loss: 0.035 | Tree loss: 2.200 | Accuracy: 0.189453 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 028 | Total loss: 2.224 | Reg loss: 0.035 | Tree loss: 2.224 | Accuracy: 0.210938 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 028 | Total loss: 2.244 | Reg loss: 0.035 | Tree loss: 2.244 | Accuracy: 0.158203 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 028 | Total loss: 2.198 | Reg loss: 0.035 | Tree loss: 2.198 | Accuracy: 0.189453 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 028 | Total loss: 2.174 | Reg loss: 0.035 | Tree loss: 2.174 | Accuracy: 0.197266 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 028 | Total loss: 2.204 | Reg loss: 0.035 | Tree loss: 2.204 | Accuracy: 0.181641 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 028 | Total loss: 2.159 | Reg loss: 0.035 | Tree loss: 2.159 | Accuracy: 0.216797 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 028 | Total loss: 2.205 | Reg loss: 0.035 | Tree loss: 2.205 | Accuracy: 0.166016 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 028 | Total loss: 2.153 | Reg loss: 0.035 | Tree loss: 2.153 | Accuracy: 0.197266 | 7.019 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 028 | Total loss: 2.165 | Reg loss: 0.035 | Tree loss: 2.165 | Accuracy: 0.177734 | 7.02 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 028 | Total loss: 2.208 | Reg loss: 0.035 | Tree loss: 2.208 | Accuracy: 0.207031 | 7.02 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 028 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.218750 | 7.02 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 028 | Total loss: 2.035 | Reg loss: 0.035 | Tree loss: 2.035 | Accuracy: 0.312500 | 7.018 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 028 | Total loss: 2.315 | Reg loss: 0.034 | Tree loss: 2.315 | Accuracy: 0.191406 | 7.027 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 028 | Total loss: 2.243 | Reg loss: 0.034 | Tree loss: 2.243 | Accuracy: 0.218750 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 028 | Total loss: 2.234 | Reg loss: 0.034 | Tree loss: 2.234 | Accuracy: 0.216797 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 028 | Total loss: 2.265 | Reg loss: 0.034 | Tree loss: 2.265 | Accuracy: 0.167969 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 028 | Total loss: 2.251 | Reg loss: 0.034 | Tree loss: 2.251 | Accuracy: 0.210938 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 028 | Total loss: 2.248 | Reg loss: 0.034 | Tree loss: 2.248 | Accuracy: 0.226562 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 028 | Total loss: 2.252 | Reg loss: 0.034 | Tree loss: 2.252 | Accuracy: 0.189453 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 028 | Total loss: 2.268 | Reg loss: 0.034 | Tree loss: 2.268 | Accuracy: 0.181641 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 028 | Total loss: 2.204 | Reg loss: 0.035 | Tree loss: 2.204 | Accuracy: 0.193359 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 028 | Total loss: 2.272 | Reg loss: 0.035 | Tree loss: 2.272 | Accuracy: 0.183594 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 028 | Total loss: 2.230 | Reg loss: 0.035 | Tree loss: 2.230 | Accuracy: 0.226562 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 028 | Total loss: 2.252 | Reg loss: 0.035 | Tree loss: 2.252 | Accuracy: 0.167969 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 028 | Total loss: 2.236 | Reg loss: 0.035 | Tree loss: 2.236 | Accuracy: 0.214844 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 028 | Total loss: 2.208 | Reg loss: 0.035 | Tree loss: 2.208 | Accuracy: 0.191406 | 7.025 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 028 | Total loss: 2.179 | Reg loss: 0.035 | Tree loss: 2.179 | Accuracy: 0.218750 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 028 | Total loss: 2.243 | Reg loss: 0.035 | Tree loss: 2.243 | Accuracy: 0.167969 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 028 | Total loss: 2.226 | Reg loss: 0.035 | Tree loss: 2.226 | Accuracy: 0.208984 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 028 | Total loss: 2.229 | Reg loss: 0.035 | Tree loss: 2.229 | Accuracy: 0.183594 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 028 | Total loss: 2.180 | Reg loss: 0.035 | Tree loss: 2.180 | Accuracy: 0.199219 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 028 | Total loss: 2.201 | Reg loss: 0.035 | Tree loss: 2.201 | Accuracy: 0.220703 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 028 | Total loss: 2.209 | Reg loss: 0.035 | Tree loss: 2.209 | Accuracy: 0.187500 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 028 | Total loss: 2.157 | Reg loss: 0.035 | Tree loss: 2.157 | Accuracy: 0.214844 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 028 | Total loss: 2.217 | Reg loss: 0.035 | Tree loss: 2.217 | Accuracy: 0.218750 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 028 | Total loss: 2.186 | Reg loss: 0.035 | Tree loss: 2.186 | Accuracy: 0.208984 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 028 | Total loss: 2.146 | Reg loss: 0.035 | Tree loss: 2.146 | Accuracy: 0.177734 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 028 | Total loss: 2.195 | Reg loss: 0.035 | Tree loss: 2.195 | Accuracy: 0.195312 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 028 | Total loss: 2.149 | Reg loss: 0.035 | Tree loss: 2.149 | Accuracy: 0.185547 | 7.026 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 028 | Total loss: 2.250 | Reg loss: 0.035 | Tree loss: 2.250 | Accuracy: 0.062500 | 7.024 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 028 | Total loss: 2.272 | Reg loss: 0.034 | Tree loss: 2.272 | Accuracy: 0.189453 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 028 | Total loss: 2.280 | Reg loss: 0.034 | Tree loss: 2.280 | Accuracy: 0.228516 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 028 | Total loss: 2.265 | Reg loss: 0.034 | Tree loss: 2.265 | Accuracy: 0.189453 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 028 | Total loss: 2.272 | Reg loss: 0.034 | Tree loss: 2.272 | Accuracy: 0.191406 | 7.03 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 004 / 028 | Total loss: 2.242 | Reg loss: 0.034 | Tree loss: 2.242 | Accuracy: 0.193359 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 028 | Total loss: 2.227 | Reg loss: 0.034 | Tree loss: 2.227 | Accuracy: 0.187500 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 028 | Total loss: 2.293 | Reg loss: 0.034 | Tree loss: 2.293 | Accuracy: 0.226562 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 028 | Total loss: 2.326 | Reg loss: 0.034 | Tree loss: 2.326 | Accuracy: 0.177734 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 028 | Total loss: 2.276 | Reg loss: 0.034 | Tree loss: 2.276 | Accuracy: 0.187500 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 028 | Total loss: 2.234 | Reg loss: 0.034 | Tree loss: 2.234 | Accuracy: 0.207031 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 028 | Total loss: 2.216 | Reg loss: 0.034 | Tree loss: 2.216 | Accuracy: 0.195312 | 7.03 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 028 | Total loss: 2.255 | Reg loss: 0.034 | Tree loss: 2.255 | Accuracy: 0.158203 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 028 | Total loss: 2.216 | Reg loss: 0.034 | Tree loss: 2.216 | Accuracy: 0.199219 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 028 | Total loss: 2.253 | Reg loss: 0.035 | Tree loss: 2.253 | Accuracy: 0.160156 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 028 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.201172 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 028 | Total loss: 2.177 | Reg loss: 0.035 | Tree loss: 2.177 | Accuracy: 0.187500 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 028 | Total loss: 2.215 | Reg loss: 0.035 | Tree loss: 2.215 | Accuracy: 0.193359 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.207031 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 028 | Total loss: 2.220 | Reg loss: 0.035 | Tree loss: 2.220 | Accuracy: 0.201172 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 028 | Total loss: 2.180 | Reg loss: 0.035 | Tree loss: 2.180 | Accuracy: 0.226562 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 028 | Total loss: 2.221 | Reg loss: 0.035 | Tree loss: 2.221 | Accuracy: 0.179688 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 028 | Total loss: 2.170 | Reg loss: 0.035 | Tree loss: 2.170 | Accuracy: 0.224609 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 028 | Total loss: 2.182 | Reg loss: 0.035 | Tree loss: 2.182 | Accuracy: 0.230469 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 028 | Total loss: 2.147 | Reg loss: 0.035 | Tree loss: 2.147 | Accuracy: 0.189453 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 028 | Total loss: 2.137 | Reg loss: 0.035 | Tree loss: 2.137 | Accuracy: 0.218750 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 028 | Total loss: 2.148 | Reg loss: 0.035 | Tree loss: 2.148 | Accuracy: 0.205078 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 028 | Total loss: 2.129 | Reg loss: 0.035 | Tree loss: 2.129 | Accuracy: 0.205078 | 7.031 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 028 | Total loss: 2.022 | Reg loss: 0.035 | Tree loss: 2.022 | Accuracy: 0.312500 | 7.029 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 028 | Total loss: 2.260 | Reg loss: 0.034 | Tree loss: 2.260 | Accuracy: 0.224609 | 7.033 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 028 | Total loss: 2.260 | Reg loss: 0.034 | Tree loss: 2.260 | Accuracy: 0.201172 | 7.033 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 028 | Total loss: 2.251 | Reg loss: 0.034 | Tree loss: 2.251 | Accuracy: 0.205078 | 7.033 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 028 | Total loss: 2.243 | Reg loss: 0.034 | Tree loss: 2.243 | Accuracy: 0.179688 | 7.032 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 028 | Total loss: 2.237 | Reg loss: 0.034 | Tree loss: 2.237 | Accuracy: 0.177734 | 7.032 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 028 | Total loss: 2.258 | Reg loss: 0.034 | Tree loss: 2.258 | Accuracy: 0.216797 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 028 | Total loss: 2.233 | Reg loss: 0.034 | Tree loss: 2.233 | Accuracy: 0.187500 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 028 | Total loss: 2.246 | Reg loss: 0.034 | Tree loss: 2.246 | Accuracy: 0.185547 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 028 | Total loss: 2.228 | Reg loss: 0.034 | Tree loss: 2.228 | Accuracy: 0.195312 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 028 | Total loss: 2.214 | Reg loss: 0.034 | Tree loss: 2.214 | Accuracy: 0.195312 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 028 | Total loss: 2.251 | Reg loss: 0.034 | Tree loss: 2.251 | Accuracy: 0.167969 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 028 | Total loss: 2.232 | Reg loss: 0.034 | Tree loss: 2.232 | Accuracy: 0.193359 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 028 | Total loss: 2.180 | Reg loss: 0.034 | Tree loss: 2.180 | Accuracy: 0.208984 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 028 | Total loss: 2.219 | Reg loss: 0.034 | Tree loss: 2.219 | Accuracy: 0.228516 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 028 | Total loss: 2.184 | Reg loss: 0.034 | Tree loss: 2.184 | Accuracy: 0.208984 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 028 | Total loss: 2.214 | Reg loss: 0.034 | Tree loss: 2.214 | Accuracy: 0.203125 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 028 | Total loss: 2.194 | Reg loss: 0.035 | Tree loss: 2.194 | Accuracy: 0.203125 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 028 | Total loss: 2.205 | Reg loss: 0.035 | Tree loss: 2.205 | Accuracy: 0.175781 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 028 | Total loss: 2.203 | Reg loss: 0.035 | Tree loss: 2.203 | Accuracy: 0.193359 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 028 | Total loss: 2.200 | Reg loss: 0.035 | Tree loss: 2.200 | Accuracy: 0.216797 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 028 | Total loss: 2.160 | Reg loss: 0.035 | Tree loss: 2.160 | Accuracy: 0.210938 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 028 | Total loss: 2.221 | Reg loss: 0.035 | Tree loss: 2.221 | Accuracy: 0.208984 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 028 | Total loss: 2.196 | Reg loss: 0.035 | Tree loss: 2.196 | Accuracy: 0.179688 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 028 | Total loss: 2.200 | Reg loss: 0.035 | Tree loss: 2.200 | Accuracy: 0.208984 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 028 | Total loss: 2.237 | Reg loss: 0.035 | Tree loss: 2.237 | Accuracy: 0.205078 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 028 | Total loss: 2.210 | Reg loss: 0.035 | Tree loss: 2.210 | Accuracy: 0.171875 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 028 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.210938 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 028 | Total loss: 2.150 | Reg loss: 0.035 | Tree loss: 2.150 | Accuracy: 0.187500 | 7.029 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 028 | Total loss: 2.243 | Reg loss: 0.034 | Tree loss: 2.243 | Accuracy: 0.242188 | 7.041 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 028 | Total loss: 2.271 | Reg loss: 0.034 | Tree loss: 2.271 | Accuracy: 0.199219 | 7.04 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 028 | Total loss: 2.282 | Reg loss: 0.034 | Tree loss: 2.282 | Accuracy: 0.203125 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 028 | Total loss: 2.280 | Reg loss: 0.034 | Tree loss: 2.280 | Accuracy: 0.212891 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 028 | Total loss: 2.291 | Reg loss: 0.034 | Tree loss: 2.291 | Accuracy: 0.193359 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 028 | Total loss: 2.263 | Reg loss: 0.034 | Tree loss: 2.263 | Accuracy: 0.185547 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 028 | Total loss: 2.233 | Reg loss: 0.034 | Tree loss: 2.233 | Accuracy: 0.218750 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 028 | Total loss: 2.239 | Reg loss: 0.034 | Tree loss: 2.239 | Accuracy: 0.205078 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 028 | Total loss: 2.235 | Reg loss: 0.034 | Tree loss: 2.235 | Accuracy: 0.220703 | 7.039 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 009 / 028 | Total loss: 2.289 | Reg loss: 0.034 | Tree loss: 2.289 | Accuracy: 0.167969 | 7.039 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 028 | Total loss: 2.211 | Reg loss: 0.034 | Tree loss: 2.211 | Accuracy: 0.210938 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 028 | Total loss: 2.209 | Reg loss: 0.034 | Tree loss: 2.209 | Accuracy: 0.199219 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 028 | Total loss: 2.208 | Reg loss: 0.034 | Tree loss: 2.208 | Accuracy: 0.179688 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 028 | Total loss: 2.185 | Reg loss: 0.034 | Tree loss: 2.185 | Accuracy: 0.191406 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 028 | Total loss: 2.174 | Reg loss: 0.034 | Tree loss: 2.174 | Accuracy: 0.214844 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 028 | Total loss: 2.199 | Reg loss: 0.034 | Tree loss: 2.199 | Accuracy: 0.183594 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 028 | Total loss: 2.225 | Reg loss: 0.034 | Tree loss: 2.225 | Accuracy: 0.183594 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 028 | Total loss: 2.152 | Reg loss: 0.034 | Tree loss: 2.152 | Accuracy: 0.197266 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 028 | Total loss: 2.192 | Reg loss: 0.034 | Tree loss: 2.192 | Accuracy: 0.208984 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 028 | Total loss: 2.230 | Reg loss: 0.034 | Tree loss: 2.230 | Accuracy: 0.189453 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 028 | Total loss: 2.171 | Reg loss: 0.034 | Tree loss: 2.171 | Accuracy: 0.201172 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 028 | Total loss: 2.249 | Reg loss: 0.035 | Tree loss: 2.249 | Accuracy: 0.195312 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 028 | Total loss: 2.212 | Reg loss: 0.035 | Tree loss: 2.212 | Accuracy: 0.169922 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 028 | Total loss: 2.136 | Reg loss: 0.035 | Tree loss: 2.136 | Accuracy: 0.234375 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 028 | Total loss: 2.170 | Reg loss: 0.035 | Tree loss: 2.170 | Accuracy: 0.199219 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 028 | Total loss: 2.159 | Reg loss: 0.035 | Tree loss: 2.159 | Accuracy: 0.177734 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 028 | Total loss: 2.148 | Reg loss: 0.035 | Tree loss: 2.148 | Accuracy: 0.177734 | 7.038 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 028 | Total loss: 2.233 | Reg loss: 0.035 | Tree loss: 2.233 | Accuracy: 0.250000 | 7.037 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 028 | Total loss: 2.298 | Reg loss: 0.034 | Tree loss: 2.298 | Accuracy: 0.216797 | 7.042 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 028 | Total loss: 2.289 | Reg loss: 0.034 | Tree loss: 2.289 | Accuracy: 0.232422 | 7.042 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 028 | Total loss: 2.324 | Reg loss: 0.034 | Tree loss: 2.324 | Accuracy: 0.201172 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 028 | Total loss: 2.255 | Reg loss: 0.034 | Tree loss: 2.255 | Accuracy: 0.201172 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 028 | Total loss: 2.287 | Reg loss: 0.034 | Tree loss: 2.287 | Accuracy: 0.191406 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 028 | Total loss: 2.251 | Reg loss: 0.034 | Tree loss: 2.251 | Accuracy: 0.193359 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 028 | Total loss: 2.269 | Reg loss: 0.034 | Tree loss: 2.269 | Accuracy: 0.173828 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 028 | Total loss: 2.277 | Reg loss: 0.034 | Tree loss: 2.277 | Accuracy: 0.169922 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 028 | Total loss: 2.247 | Reg loss: 0.034 | Tree loss: 2.247 | Accuracy: 0.230469 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 028 | Total loss: 2.261 | Reg loss: 0.034 | Tree loss: 2.261 | Accuracy: 0.181641 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 028 | Total loss: 2.186 | Reg loss: 0.034 | Tree loss: 2.186 | Accuracy: 0.224609 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 028 | Total loss: 2.166 | Reg loss: 0.034 | Tree loss: 2.166 | Accuracy: 0.214844 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 028 | Total loss: 2.203 | Reg loss: 0.034 | Tree loss: 2.203 | Accuracy: 0.208984 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 028 | Total loss: 2.199 | Reg loss: 0.034 | Tree loss: 2.199 | Accuracy: 0.208984 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 028 | Total loss: 2.179 | Reg loss: 0.034 | Tree loss: 2.179 | Accuracy: 0.191406 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 028 | Total loss: 2.171 | Reg loss: 0.034 | Tree loss: 2.171 | Accuracy: 0.214844 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 028 | Total loss: 2.150 | Reg loss: 0.034 | Tree loss: 2.150 | Accuracy: 0.162109 | 7.04 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 028 | Total loss: 2.220 | Reg loss: 0.034 | Tree loss: 2.220 | Accuracy: 0.146484 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 028 | Total loss: 2.165 | Reg loss: 0.034 | Tree loss: 2.165 | Accuracy: 0.212891 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 028 | Total loss: 2.205 | Reg loss: 0.034 | Tree loss: 2.205 | Accuracy: 0.187500 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 028 | Total loss: 2.233 | Reg loss: 0.034 | Tree loss: 2.233 | Accuracy: 0.199219 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 028 | Total loss: 2.152 | Reg loss: 0.034 | Tree loss: 2.152 | Accuracy: 0.216797 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 028 | Total loss: 2.189 | Reg loss: 0.034 | Tree loss: 2.189 | Accuracy: 0.193359 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 028 | Total loss: 2.184 | Reg loss: 0.034 | Tree loss: 2.184 | Accuracy: 0.173828 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 028 | Total loss: 2.176 | Reg loss: 0.034 | Tree loss: 2.176 | Accuracy: 0.203125 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 028 | Total loss: 2.156 | Reg loss: 0.034 | Tree loss: 2.156 | Accuracy: 0.212891 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 028 | Total loss: 2.143 | Reg loss: 0.034 | Tree loss: 2.143 | Accuracy: 0.199219 | 7.041 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 028 | Total loss: 2.104 | Reg loss: 0.035 | Tree loss: 2.104 | Accuracy: 0.250000 | 7.039 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 028 | Total loss: 2.282 | Reg loss: 0.034 | Tree loss: 2.282 | Accuracy: 0.218750 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 028 | Total loss: 2.273 | Reg loss: 0.034 | Tree loss: 2.273 | Accuracy: 0.191406 | 7.046 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 028 | Total loss: 2.259 | Reg loss: 0.034 | Tree loss: 2.259 | Accuracy: 0.208984 | 7.046 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 028 | Total loss: 2.270 | Reg loss: 0.034 | Tree loss: 2.270 | Accuracy: 0.173828 | 7.046 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 028 | Total loss: 2.236 | Reg loss: 0.034 | Tree loss: 2.236 | Accuracy: 0.195312 | 7.046 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 028 | Total loss: 2.251 | Reg loss: 0.034 | Tree loss: 2.251 | Accuracy: 0.177734 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 028 | Total loss: 2.259 | Reg loss: 0.034 | Tree loss: 2.259 | Accuracy: 0.212891 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 028 | Total loss: 2.263 | Reg loss: 0.034 | Tree loss: 2.263 | Accuracy: 0.181641 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 028 | Total loss: 2.296 | Reg loss: 0.034 | Tree loss: 2.296 | Accuracy: 0.166016 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 028 | Total loss: 2.184 | Reg loss: 0.034 | Tree loss: 2.184 | Accuracy: 0.212891 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 028 | Total loss: 2.223 | Reg loss: 0.034 | Tree loss: 2.223 | Accuracy: 0.201172 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 028 | Total loss: 2.239 | Reg loss: 0.034 | Tree loss: 2.239 | Accuracy: 0.197266 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 028 | Total loss: 2.208 | Reg loss: 0.034 | Tree loss: 2.208 | Accuracy: 0.218750 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 028 | Total loss: 2.228 | Reg loss: 0.034 | Tree loss: 2.228 | Accuracy: 0.193359 | 7.047 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 014 / 028 | Total loss: 2.157 | Reg loss: 0.034 | Tree loss: 2.157 | Accuracy: 0.197266 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 028 | Total loss: 2.205 | Reg loss: 0.034 | Tree loss: 2.205 | Accuracy: 0.199219 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 028 | Total loss: 2.207 | Reg loss: 0.034 | Tree loss: 2.207 | Accuracy: 0.201172 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 028 | Total loss: 2.198 | Reg loss: 0.034 | Tree loss: 2.198 | Accuracy: 0.181641 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 028 | Total loss: 2.214 | Reg loss: 0.034 | Tree loss: 2.214 | Accuracy: 0.212891 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 028 | Total loss: 2.157 | Reg loss: 0.034 | Tree loss: 2.157 | Accuracy: 0.207031 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 028 | Total loss: 2.169 | Reg loss: 0.034 | Tree loss: 2.169 | Accuracy: 0.199219 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 028 | Total loss: 2.174 | Reg loss: 0.034 | Tree loss: 2.174 | Accuracy: 0.203125 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 028 | Total loss: 2.146 | Reg loss: 0.034 | Tree loss: 2.146 | Accuracy: 0.203125 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 028 | Total loss: 2.152 | Reg loss: 0.034 | Tree loss: 2.152 | Accuracy: 0.193359 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 028 | Total loss: 2.162 | Reg loss: 0.034 | Tree loss: 2.162 | Accuracy: 0.222656 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 028 | Total loss: 2.177 | Reg loss: 0.034 | Tree loss: 2.177 | Accuracy: 0.191406 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 028 | Total loss: 2.189 | Reg loss: 0.034 | Tree loss: 2.189 | Accuracy: 0.207031 | 7.047 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 028 | Total loss: 2.678 | Reg loss: 0.034 | Tree loss: 2.678 | Accuracy: 0.062500 | 7.045 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 028 | Total loss: 2.300 | Reg loss: 0.034 | Tree loss: 2.300 | Accuracy: 0.224609 | 7.049 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 028 | Total loss: 2.277 | Reg loss: 0.034 | Tree loss: 2.277 | Accuracy: 0.181641 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 028 | Total loss: 2.273 | Reg loss: 0.034 | Tree loss: 2.273 | Accuracy: 0.195312 | 7.047 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 028 | Total loss: 2.243 | Reg loss: 0.034 | Tree loss: 2.243 | Accuracy: 0.189453 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 028 | Total loss: 2.252 | Reg loss: 0.034 | Tree loss: 2.252 | Accuracy: 0.212891 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 028 | Total loss: 2.269 | Reg loss: 0.034 | Tree loss: 2.269 | Accuracy: 0.181641 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 028 | Total loss: 2.237 | Reg loss: 0.034 | Tree loss: 2.237 | Accuracy: 0.197266 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 028 | Total loss: 2.271 | Reg loss: 0.034 | Tree loss: 2.271 | Accuracy: 0.167969 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 028 | Total loss: 2.211 | Reg loss: 0.034 | Tree loss: 2.211 | Accuracy: 0.203125 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 028 | Total loss: 2.189 | Reg loss: 0.034 | Tree loss: 2.189 | Accuracy: 0.216797 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 028 | Total loss: 2.208 | Reg loss: 0.034 | Tree loss: 2.208 | Accuracy: 0.183594 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 028 | Total loss: 2.200 | Reg loss: 0.034 | Tree loss: 2.200 | Accuracy: 0.195312 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 028 | Total loss: 2.258 | Reg loss: 0.034 | Tree loss: 2.258 | Accuracy: 0.183594 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 028 | Total loss: 2.186 | Reg loss: 0.034 | Tree loss: 2.186 | Accuracy: 0.175781 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 028 | Total loss: 2.221 | Reg loss: 0.034 | Tree loss: 2.221 | Accuracy: 0.212891 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 028 | Total loss: 2.159 | Reg loss: 0.034 | Tree loss: 2.159 | Accuracy: 0.199219 | 7.049 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 028 | Total loss: 2.212 | Reg loss: 0.034 | Tree loss: 2.212 | Accuracy: 0.203125 | 7.049 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 028 | Total loss: 2.206 | Reg loss: 0.034 | Tree loss: 2.206 | Accuracy: 0.179688 | 7.049 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 028 | Total loss: 2.162 | Reg loss: 0.034 | Tree loss: 2.162 | Accuracy: 0.224609 | 7.049 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 028 | Total loss: 2.170 | Reg loss: 0.034 | Tree loss: 2.170 | Accuracy: 0.193359 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 028 | Total loss: 2.174 | Reg loss: 0.034 | Tree loss: 2.174 | Accuracy: 0.199219 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 028 | Total loss: 2.203 | Reg loss: 0.034 | Tree loss: 2.203 | Accuracy: 0.199219 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 028 | Total loss: 2.159 | Reg loss: 0.034 | Tree loss: 2.159 | Accuracy: 0.216797 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 028 | Total loss: 2.204 | Reg loss: 0.034 | Tree loss: 2.204 | Accuracy: 0.201172 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 028 | Total loss: 2.150 | Reg loss: 0.034 | Tree loss: 2.150 | Accuracy: 0.216797 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 028 | Total loss: 2.177 | Reg loss: 0.034 | Tree loss: 2.177 | Accuracy: 0.210938 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 028 | Total loss: 2.182 | Reg loss: 0.034 | Tree loss: 2.182 | Accuracy: 0.201172 | 7.048 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 028 | Total loss: 2.120 | Reg loss: 0.034 | Tree loss: 2.120 | Accuracy: 0.125000 | 7.047 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 028 | Total loss: 2.290 | Reg loss: 0.034 | Tree loss: 2.290 | Accuracy: 0.201172 | 7.05 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 028 | Total loss: 2.304 | Reg loss: 0.034 | Tree loss: 2.304 | Accuracy: 0.185547 | 7.049 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 028 | Total loss: 2.259 | Reg loss: 0.034 | Tree loss: 2.259 | Accuracy: 0.210938 | 7.049 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 028 | Total loss: 2.255 | Reg loss: 0.034 | Tree loss: 2.255 | Accuracy: 0.224609 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 028 | Total loss: 2.265 | Reg loss: 0.034 | Tree loss: 2.265 | Accuracy: 0.193359 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 028 | Total loss: 2.265 | Reg loss: 0.034 | Tree loss: 2.265 | Accuracy: 0.169922 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 028 | Total loss: 2.205 | Reg loss: 0.034 | Tree loss: 2.205 | Accuracy: 0.208984 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 028 | Total loss: 2.248 | Reg loss: 0.034 | Tree loss: 2.248 | Accuracy: 0.189453 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 028 | Total loss: 2.259 | Reg loss: 0.034 | Tree loss: 2.259 | Accuracy: 0.191406 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 028 | Total loss: 2.226 | Reg loss: 0.034 | Tree loss: 2.226 | Accuracy: 0.201172 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 028 | Total loss: 2.226 | Reg loss: 0.034 | Tree loss: 2.226 | Accuracy: 0.212891 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 028 | Total loss: 2.201 | Reg loss: 0.034 | Tree loss: 2.201 | Accuracy: 0.207031 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 028 | Total loss: 2.225 | Reg loss: 0.034 | Tree loss: 2.225 | Accuracy: 0.226562 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 028 | Total loss: 2.245 | Reg loss: 0.034 | Tree loss: 2.245 | Accuracy: 0.197266 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 028 | Total loss: 2.189 | Reg loss: 0.034 | Tree loss: 2.189 | Accuracy: 0.193359 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 028 | Total loss: 2.164 | Reg loss: 0.034 | Tree loss: 2.164 | Accuracy: 0.207031 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 028 | Total loss: 2.179 | Reg loss: 0.034 | Tree loss: 2.179 | Accuracy: 0.193359 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 028 | Total loss: 2.206 | Reg loss: 0.034 | Tree loss: 2.206 | Accuracy: 0.183594 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 028 | Total loss: 2.178 | Reg loss: 0.034 | Tree loss: 2.178 | Accuracy: 0.187500 | 7.047 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 019 / 028 | Total loss: 2.194 | Reg loss: 0.034 | Tree loss: 2.194 | Accuracy: 0.214844 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 028 | Total loss: 2.189 | Reg loss: 0.034 | Tree loss: 2.189 | Accuracy: 0.193359 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 028 | Total loss: 2.190 | Reg loss: 0.034 | Tree loss: 2.190 | Accuracy: 0.197266 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 028 | Total loss: 2.144 | Reg loss: 0.034 | Tree loss: 2.144 | Accuracy: 0.203125 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 028 | Total loss: 2.177 | Reg loss: 0.034 | Tree loss: 2.177 | Accuracy: 0.203125 | 7.047 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 028 | Total loss: 2.152 | Reg loss: 0.034 | Tree loss: 2.152 | Accuracy: 0.169922 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 028 | Total loss: 2.144 | Reg loss: 0.034 | Tree loss: 2.144 | Accuracy: 0.187500 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 028 | Total loss: 2.131 | Reg loss: 0.034 | Tree loss: 2.131 | Accuracy: 0.210938 | 7.048 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 028 | Total loss: 2.066 | Reg loss: 0.034 | Tree loss: 2.066 | Accuracy: 0.187500 | 7.046 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf06840e09648a588457dd5876c4709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdb20ccfdbc42c0a42d23904fd8fc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7de92189254cbc86954564854e3309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c160b9441543cc97fce506f7d7c1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 11.93182416425375\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 3799\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "13840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n",
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n",
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n",
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n",
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n",
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n",
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n",
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n",
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n",
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n",
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n",
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n",
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n",
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n",
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "============== Pattern 2992 ==============\n",
      "============== Pattern 2993 ==============\n",
      "============== Pattern 2994 ==============\n",
      "============== Pattern 2995 ==============\n",
      "============== Pattern 2996 ==============\n",
      "============== Pattern 2997 ==============\n",
      "============== Pattern 2998 ==============\n",
      "============== Pattern 2999 ==============\n",
      "============== Pattern 3000 ==============\n",
      "============== Pattern 3001 ==============\n",
      "============== Pattern 3002 ==============\n",
      "============== Pattern 3003 ==============\n",
      "============== Pattern 3004 ==============\n",
      "============== Pattern 3005 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3006 ==============\n",
      "============== Pattern 3007 ==============\n",
      "============== Pattern 3008 ==============\n",
      "============== Pattern 3009 ==============\n",
      "============== Pattern 3010 ==============\n",
      "============== Pattern 3011 ==============\n",
      "============== Pattern 3012 ==============\n",
      "============== Pattern 3013 ==============\n",
      "============== Pattern 3014 ==============\n",
      "============== Pattern 3015 ==============\n",
      "============== Pattern 3016 ==============\n",
      "============== Pattern 3017 ==============\n",
      "============== Pattern 3018 ==============\n",
      "============== Pattern 3019 ==============\n",
      "============== Pattern 3020 ==============\n",
      "============== Pattern 3021 ==============\n",
      "============== Pattern 3022 ==============\n",
      "============== Pattern 3023 ==============\n",
      "============== Pattern 3024 ==============\n",
      "============== Pattern 3025 ==============\n",
      "============== Pattern 3026 ==============\n",
      "============== Pattern 3027 ==============\n",
      "============== Pattern 3028 ==============\n",
      "============== Pattern 3029 ==============\n",
      "============== Pattern 3030 ==============\n",
      "============== Pattern 3031 ==============\n",
      "============== Pattern 3032 ==============\n",
      "============== Pattern 3033 ==============\n",
      "============== Pattern 3034 ==============\n",
      "============== Pattern 3035 ==============\n",
      "============== Pattern 3036 ==============\n",
      "============== Pattern 3037 ==============\n",
      "============== Pattern 3038 ==============\n",
      "============== Pattern 3039 ==============\n",
      "============== Pattern 3040 ==============\n",
      "============== Pattern 3041 ==============\n",
      "============== Pattern 3042 ==============\n",
      "============== Pattern 3043 ==============\n",
      "============== Pattern 3044 ==============\n",
      "============== Pattern 3045 ==============\n",
      "============== Pattern 3046 ==============\n",
      "============== Pattern 3047 ==============\n",
      "============== Pattern 3048 ==============\n",
      "============== Pattern 3049 ==============\n",
      "============== Pattern 3050 ==============\n",
      "============== Pattern 3051 ==============\n",
      "============== Pattern 3052 ==============\n",
      "============== Pattern 3053 ==============\n",
      "============== Pattern 3054 ==============\n",
      "============== Pattern 3055 ==============\n",
      "============== Pattern 3056 ==============\n",
      "============== Pattern 3057 ==============\n",
      "============== Pattern 3058 ==============\n",
      "============== Pattern 3059 ==============\n",
      "============== Pattern 3060 ==============\n",
      "============== Pattern 3061 ==============\n",
      "============== Pattern 3062 ==============\n",
      "============== Pattern 3063 ==============\n",
      "============== Pattern 3064 ==============\n",
      "============== Pattern 3065 ==============\n",
      "============== Pattern 3066 ==============\n",
      "============== Pattern 3067 ==============\n",
      "============== Pattern 3068 ==============\n",
      "============== Pattern 3069 ==============\n",
      "============== Pattern 3070 ==============\n",
      "============== Pattern 3071 ==============\n",
      "============== Pattern 3072 ==============\n",
      "============== Pattern 3073 ==============\n",
      "============== Pattern 3074 ==============\n",
      "============== Pattern 3075 ==============\n",
      "============== Pattern 3076 ==============\n",
      "============== Pattern 3077 ==============\n",
      "============== Pattern 3078 ==============\n",
      "============== Pattern 3079 ==============\n",
      "============== Pattern 3080 ==============\n",
      "============== Pattern 3081 ==============\n",
      "============== Pattern 3082 ==============\n",
      "============== Pattern 3083 ==============\n",
      "============== Pattern 3084 ==============\n",
      "============== Pattern 3085 ==============\n",
      "============== Pattern 3086 ==============\n",
      "============== Pattern 3087 ==============\n",
      "============== Pattern 3088 ==============\n",
      "============== Pattern 3089 ==============\n",
      "============== Pattern 3090 ==============\n",
      "============== Pattern 3091 ==============\n",
      "============== Pattern 3092 ==============\n",
      "============== Pattern 3093 ==============\n",
      "============== Pattern 3094 ==============\n",
      "============== Pattern 3095 ==============\n",
      "============== Pattern 3096 ==============\n",
      "============== Pattern 3097 ==============\n",
      "============== Pattern 3098 ==============\n",
      "============== Pattern 3099 ==============\n",
      "============== Pattern 3100 ==============\n",
      "============== Pattern 3101 ==============\n",
      "============== Pattern 3102 ==============\n",
      "============== Pattern 3103 ==============\n",
      "============== Pattern 3104 ==============\n",
      "============== Pattern 3105 ==============\n",
      "============== Pattern 3106 ==============\n",
      "============== Pattern 3107 ==============\n",
      "============== Pattern 3108 ==============\n",
      "============== Pattern 3109 ==============\n",
      "============== Pattern 3110 ==============\n",
      "============== Pattern 3111 ==============\n",
      "============== Pattern 3112 ==============\n",
      "============== Pattern 3113 ==============\n",
      "============== Pattern 3114 ==============\n",
      "============== Pattern 3115 ==============\n",
      "============== Pattern 3116 ==============\n",
      "============== Pattern 3117 ==============\n",
      "============== Pattern 3118 ==============\n",
      "============== Pattern 3119 ==============\n",
      "============== Pattern 3120 ==============\n",
      "============== Pattern 3121 ==============\n",
      "============== Pattern 3122 ==============\n",
      "============== Pattern 3123 ==============\n",
      "============== Pattern 3124 ==============\n",
      "============== Pattern 3125 ==============\n",
      "============== Pattern 3126 ==============\n",
      "============== Pattern 3127 ==============\n",
      "============== Pattern 3128 ==============\n",
      "============== Pattern 3129 ==============\n",
      "============== Pattern 3130 ==============\n",
      "============== Pattern 3131 ==============\n",
      "============== Pattern 3132 ==============\n",
      "============== Pattern 3133 ==============\n",
      "============== Pattern 3134 ==============\n",
      "============== Pattern 3135 ==============\n",
      "============== Pattern 3136 ==============\n",
      "============== Pattern 3137 ==============\n",
      "============== Pattern 3138 ==============\n",
      "============== Pattern 3139 ==============\n",
      "============== Pattern 3140 ==============\n",
      "============== Pattern 3141 ==============\n",
      "============== Pattern 3142 ==============\n",
      "============== Pattern 3143 ==============\n",
      "============== Pattern 3144 ==============\n",
      "============== Pattern 3145 ==============\n",
      "============== Pattern 3146 ==============\n",
      "============== Pattern 3147 ==============\n",
      "============== Pattern 3148 ==============\n",
      "============== Pattern 3149 ==============\n",
      "============== Pattern 3150 ==============\n",
      "============== Pattern 3151 ==============\n",
      "============== Pattern 3152 ==============\n",
      "============== Pattern 3153 ==============\n",
      "============== Pattern 3154 ==============\n",
      "============== Pattern 3155 ==============\n",
      "============== Pattern 3156 ==============\n",
      "============== Pattern 3157 ==============\n",
      "============== Pattern 3158 ==============\n",
      "============== Pattern 3159 ==============\n",
      "============== Pattern 3160 ==============\n",
      "============== Pattern 3161 ==============\n",
      "============== Pattern 3162 ==============\n",
      "============== Pattern 3163 ==============\n",
      "============== Pattern 3164 ==============\n",
      "============== Pattern 3165 ==============\n",
      "============== Pattern 3166 ==============\n",
      "============== Pattern 3167 ==============\n",
      "============== Pattern 3168 ==============\n",
      "============== Pattern 3169 ==============\n",
      "============== Pattern 3170 ==============\n",
      "============== Pattern 3171 ==============\n",
      "============== Pattern 3172 ==============\n",
      "============== Pattern 3173 ==============\n",
      "============== Pattern 3174 ==============\n",
      "============== Pattern 3175 ==============\n",
      "============== Pattern 3176 ==============\n",
      "============== Pattern 3177 ==============\n",
      "============== Pattern 3178 ==============\n",
      "============== Pattern 3179 ==============\n",
      "============== Pattern 3180 ==============\n",
      "============== Pattern 3181 ==============\n",
      "============== Pattern 3182 ==============\n",
      "============== Pattern 3183 ==============\n",
      "============== Pattern 3184 ==============\n",
      "============== Pattern 3185 ==============\n",
      "============== Pattern 3186 ==============\n",
      "============== Pattern 3187 ==============\n",
      "============== Pattern 3188 ==============\n",
      "============== Pattern 3189 ==============\n",
      "============== Pattern 3190 ==============\n",
      "============== Pattern 3191 ==============\n",
      "============== Pattern 3192 ==============\n",
      "============== Pattern 3193 ==============\n",
      "============== Pattern 3194 ==============\n",
      "============== Pattern 3195 ==============\n",
      "============== Pattern 3196 ==============\n",
      "============== Pattern 3197 ==============\n",
      "============== Pattern 3198 ==============\n",
      "============== Pattern 3199 ==============\n",
      "============== Pattern 3200 ==============\n",
      "============== Pattern 3201 ==============\n",
      "============== Pattern 3202 ==============\n",
      "============== Pattern 3203 ==============\n",
      "============== Pattern 3204 ==============\n",
      "============== Pattern 3205 ==============\n",
      "============== Pattern 3206 ==============\n",
      "============== Pattern 3207 ==============\n",
      "============== Pattern 3208 ==============\n",
      "============== Pattern 3209 ==============\n",
      "============== Pattern 3210 ==============\n",
      "============== Pattern 3211 ==============\n",
      "============== Pattern 3212 ==============\n",
      "============== Pattern 3213 ==============\n",
      "============== Pattern 3214 ==============\n",
      "============== Pattern 3215 ==============\n",
      "============== Pattern 3216 ==============\n",
      "============== Pattern 3217 ==============\n",
      "============== Pattern 3218 ==============\n",
      "============== Pattern 3219 ==============\n",
      "============== Pattern 3220 ==============\n",
      "============== Pattern 3221 ==============\n",
      "============== Pattern 3222 ==============\n",
      "============== Pattern 3223 ==============\n",
      "============== Pattern 3224 ==============\n",
      "============== Pattern 3225 ==============\n",
      "============== Pattern 3226 ==============\n",
      "============== Pattern 3227 ==============\n",
      "============== Pattern 3228 ==============\n",
      "============== Pattern 3229 ==============\n",
      "============== Pattern 3230 ==============\n",
      "============== Pattern 3231 ==============\n",
      "============== Pattern 3232 ==============\n",
      "============== Pattern 3233 ==============\n",
      "============== Pattern 3234 ==============\n",
      "============== Pattern 3235 ==============\n",
      "============== Pattern 3236 ==============\n",
      "============== Pattern 3237 ==============\n",
      "============== Pattern 3238 ==============\n",
      "============== Pattern 3239 ==============\n",
      "============== Pattern 3240 ==============\n",
      "============== Pattern 3241 ==============\n",
      "============== Pattern 3242 ==============\n",
      "============== Pattern 3243 ==============\n",
      "============== Pattern 3244 ==============\n",
      "============== Pattern 3245 ==============\n",
      "============== Pattern 3246 ==============\n",
      "============== Pattern 3247 ==============\n",
      "============== Pattern 3248 ==============\n",
      "============== Pattern 3249 ==============\n",
      "============== Pattern 3250 ==============\n",
      "============== Pattern 3251 ==============\n",
      "============== Pattern 3252 ==============\n",
      "============== Pattern 3253 ==============\n",
      "============== Pattern 3254 ==============\n",
      "============== Pattern 3255 ==============\n",
      "============== Pattern 3256 ==============\n",
      "============== Pattern 3257 ==============\n",
      "============== Pattern 3258 ==============\n",
      "============== Pattern 3259 ==============\n",
      "============== Pattern 3260 ==============\n",
      "============== Pattern 3261 ==============\n",
      "============== Pattern 3262 ==============\n",
      "============== Pattern 3263 ==============\n",
      "============== Pattern 3264 ==============\n",
      "============== Pattern 3265 ==============\n",
      "============== Pattern 3266 ==============\n",
      "============== Pattern 3267 ==============\n",
      "============== Pattern 3268 ==============\n",
      "============== Pattern 3269 ==============\n",
      "============== Pattern 3270 ==============\n",
      "============== Pattern 3271 ==============\n",
      "============== Pattern 3272 ==============\n",
      "============== Pattern 3273 ==============\n",
      "============== Pattern 3274 ==============\n",
      "============== Pattern 3275 ==============\n",
      "============== Pattern 3276 ==============\n",
      "============== Pattern 3277 ==============\n",
      "============== Pattern 3278 ==============\n",
      "============== Pattern 3279 ==============\n",
      "============== Pattern 3280 ==============\n",
      "============== Pattern 3281 ==============\n",
      "============== Pattern 3282 ==============\n",
      "============== Pattern 3283 ==============\n",
      "============== Pattern 3284 ==============\n",
      "============== Pattern 3285 ==============\n",
      "============== Pattern 3286 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3287 ==============\n",
      "============== Pattern 3288 ==============\n",
      "============== Pattern 3289 ==============\n",
      "============== Pattern 3290 ==============\n",
      "============== Pattern 3291 ==============\n",
      "============== Pattern 3292 ==============\n",
      "============== Pattern 3293 ==============\n",
      "============== Pattern 3294 ==============\n",
      "============== Pattern 3295 ==============\n",
      "============== Pattern 3296 ==============\n",
      "============== Pattern 3297 ==============\n",
      "============== Pattern 3298 ==============\n",
      "============== Pattern 3299 ==============\n",
      "============== Pattern 3300 ==============\n",
      "============== Pattern 3301 ==============\n",
      "============== Pattern 3302 ==============\n",
      "============== Pattern 3303 ==============\n",
      "============== Pattern 3304 ==============\n",
      "============== Pattern 3305 ==============\n",
      "============== Pattern 3306 ==============\n",
      "============== Pattern 3307 ==============\n",
      "============== Pattern 3308 ==============\n",
      "============== Pattern 3309 ==============\n",
      "============== Pattern 3310 ==============\n",
      "============== Pattern 3311 ==============\n",
      "============== Pattern 3312 ==============\n",
      "============== Pattern 3313 ==============\n",
      "============== Pattern 3314 ==============\n",
      "============== Pattern 3315 ==============\n",
      "============== Pattern 3316 ==============\n",
      "============== Pattern 3317 ==============\n",
      "============== Pattern 3318 ==============\n",
      "============== Pattern 3319 ==============\n",
      "============== Pattern 3320 ==============\n",
      "============== Pattern 3321 ==============\n",
      "============== Pattern 3322 ==============\n",
      "============== Pattern 3323 ==============\n",
      "============== Pattern 3324 ==============\n",
      "============== Pattern 3325 ==============\n",
      "============== Pattern 3326 ==============\n",
      "============== Pattern 3327 ==============\n",
      "============== Pattern 3328 ==============\n",
      "============== Pattern 3329 ==============\n",
      "============== Pattern 3330 ==============\n",
      "============== Pattern 3331 ==============\n",
      "============== Pattern 3332 ==============\n",
      "============== Pattern 3333 ==============\n",
      "============== Pattern 3334 ==============\n",
      "============== Pattern 3335 ==============\n",
      "============== Pattern 3336 ==============\n",
      "============== Pattern 3337 ==============\n",
      "============== Pattern 3338 ==============\n",
      "============== Pattern 3339 ==============\n",
      "============== Pattern 3340 ==============\n",
      "============== Pattern 3341 ==============\n",
      "============== Pattern 3342 ==============\n",
      "============== Pattern 3343 ==============\n",
      "============== Pattern 3344 ==============\n",
      "============== Pattern 3345 ==============\n",
      "============== Pattern 3346 ==============\n",
      "============== Pattern 3347 ==============\n",
      "============== Pattern 3348 ==============\n",
      "============== Pattern 3349 ==============\n",
      "============== Pattern 3350 ==============\n",
      "============== Pattern 3351 ==============\n",
      "============== Pattern 3352 ==============\n",
      "============== Pattern 3353 ==============\n",
      "============== Pattern 3354 ==============\n",
      "============== Pattern 3355 ==============\n",
      "============== Pattern 3356 ==============\n",
      "============== Pattern 3357 ==============\n",
      "============== Pattern 3358 ==============\n",
      "============== Pattern 3359 ==============\n",
      "============== Pattern 3360 ==============\n",
      "============== Pattern 3361 ==============\n",
      "============== Pattern 3362 ==============\n",
      "============== Pattern 3363 ==============\n",
      "============== Pattern 3364 ==============\n",
      "============== Pattern 3365 ==============\n",
      "============== Pattern 3366 ==============\n",
      "============== Pattern 3367 ==============\n",
      "============== Pattern 3368 ==============\n",
      "============== Pattern 3369 ==============\n",
      "============== Pattern 3370 ==============\n",
      "============== Pattern 3371 ==============\n",
      "============== Pattern 3372 ==============\n",
      "============== Pattern 3373 ==============\n",
      "============== Pattern 3374 ==============\n",
      "============== Pattern 3375 ==============\n",
      "============== Pattern 3376 ==============\n",
      "============== Pattern 3377 ==============\n",
      "============== Pattern 3378 ==============\n",
      "============== Pattern 3379 ==============\n",
      "============== Pattern 3380 ==============\n",
      "============== Pattern 3381 ==============\n",
      "============== Pattern 3382 ==============\n",
      "============== Pattern 3383 ==============\n",
      "============== Pattern 3384 ==============\n",
      "============== Pattern 3385 ==============\n",
      "============== Pattern 3386 ==============\n",
      "============== Pattern 3387 ==============\n",
      "============== Pattern 3388 ==============\n",
      "============== Pattern 3389 ==============\n",
      "============== Pattern 3390 ==============\n",
      "============== Pattern 3391 ==============\n",
      "============== Pattern 3392 ==============\n",
      "============== Pattern 3393 ==============\n",
      "============== Pattern 3394 ==============\n",
      "============== Pattern 3395 ==============\n",
      "============== Pattern 3396 ==============\n",
      "============== Pattern 3397 ==============\n",
      "============== Pattern 3398 ==============\n",
      "============== Pattern 3399 ==============\n",
      "============== Pattern 3400 ==============\n",
      "============== Pattern 3401 ==============\n",
      "============== Pattern 3402 ==============\n",
      "============== Pattern 3403 ==============\n",
      "============== Pattern 3404 ==============\n",
      "============== Pattern 3405 ==============\n",
      "============== Pattern 3406 ==============\n",
      "============== Pattern 3407 ==============\n",
      "============== Pattern 3408 ==============\n",
      "============== Pattern 3409 ==============\n",
      "============== Pattern 3410 ==============\n",
      "============== Pattern 3411 ==============\n",
      "============== Pattern 3412 ==============\n",
      "============== Pattern 3413 ==============\n",
      "============== Pattern 3414 ==============\n",
      "============== Pattern 3415 ==============\n",
      "============== Pattern 3416 ==============\n",
      "============== Pattern 3417 ==============\n",
      "============== Pattern 3418 ==============\n",
      "============== Pattern 3419 ==============\n",
      "============== Pattern 3420 ==============\n",
      "============== Pattern 3421 ==============\n",
      "============== Pattern 3422 ==============\n",
      "============== Pattern 3423 ==============\n",
      "============== Pattern 3424 ==============\n",
      "============== Pattern 3425 ==============\n",
      "============== Pattern 3426 ==============\n",
      "============== Pattern 3427 ==============\n",
      "============== Pattern 3428 ==============\n",
      "============== Pattern 3429 ==============\n",
      "============== Pattern 3430 ==============\n",
      "============== Pattern 3431 ==============\n",
      "============== Pattern 3432 ==============\n",
      "============== Pattern 3433 ==============\n",
      "============== Pattern 3434 ==============\n",
      "============== Pattern 3435 ==============\n",
      "============== Pattern 3436 ==============\n",
      "============== Pattern 3437 ==============\n",
      "============== Pattern 3438 ==============\n",
      "============== Pattern 3439 ==============\n",
      "============== Pattern 3440 ==============\n",
      "============== Pattern 3441 ==============\n",
      "============== Pattern 3442 ==============\n",
      "============== Pattern 3443 ==============\n",
      "============== Pattern 3444 ==============\n",
      "============== Pattern 3445 ==============\n",
      "============== Pattern 3446 ==============\n",
      "============== Pattern 3447 ==============\n",
      "============== Pattern 3448 ==============\n",
      "============== Pattern 3449 ==============\n",
      "============== Pattern 3450 ==============\n",
      "============== Pattern 3451 ==============\n",
      "============== Pattern 3452 ==============\n",
      "============== Pattern 3453 ==============\n",
      "============== Pattern 3454 ==============\n",
      "============== Pattern 3455 ==============\n",
      "============== Pattern 3456 ==============\n",
      "============== Pattern 3457 ==============\n",
      "============== Pattern 3458 ==============\n",
      "============== Pattern 3459 ==============\n",
      "============== Pattern 3460 ==============\n",
      "============== Pattern 3461 ==============\n",
      "============== Pattern 3462 ==============\n",
      "============== Pattern 3463 ==============\n",
      "============== Pattern 3464 ==============\n",
      "============== Pattern 3465 ==============\n",
      "============== Pattern 3466 ==============\n",
      "============== Pattern 3467 ==============\n",
      "============== Pattern 3468 ==============\n",
      "============== Pattern 3469 ==============\n",
      "============== Pattern 3470 ==============\n",
      "============== Pattern 3471 ==============\n",
      "============== Pattern 3472 ==============\n",
      "============== Pattern 3473 ==============\n",
      "============== Pattern 3474 ==============\n",
      "============== Pattern 3475 ==============\n",
      "============== Pattern 3476 ==============\n",
      "============== Pattern 3477 ==============\n",
      "============== Pattern 3478 ==============\n",
      "============== Pattern 3479 ==============\n",
      "============== Pattern 3480 ==============\n",
      "============== Pattern 3481 ==============\n",
      "============== Pattern 3482 ==============\n",
      "============== Pattern 3483 ==============\n",
      "============== Pattern 3484 ==============\n",
      "============== Pattern 3485 ==============\n",
      "============== Pattern 3486 ==============\n",
      "============== Pattern 3487 ==============\n",
      "============== Pattern 3488 ==============\n",
      "============== Pattern 3489 ==============\n",
      "============== Pattern 3490 ==============\n",
      "============== Pattern 3491 ==============\n",
      "============== Pattern 3492 ==============\n",
      "============== Pattern 3493 ==============\n",
      "============== Pattern 3494 ==============\n",
      "============== Pattern 3495 ==============\n",
      "============== Pattern 3496 ==============\n",
      "============== Pattern 3497 ==============\n",
      "============== Pattern 3498 ==============\n",
      "============== Pattern 3499 ==============\n",
      "============== Pattern 3500 ==============\n",
      "============== Pattern 3501 ==============\n",
      "============== Pattern 3502 ==============\n",
      "============== Pattern 3503 ==============\n",
      "============== Pattern 3504 ==============\n",
      "============== Pattern 3505 ==============\n",
      "============== Pattern 3506 ==============\n",
      "============== Pattern 3507 ==============\n",
      "============== Pattern 3508 ==============\n",
      "============== Pattern 3509 ==============\n",
      "============== Pattern 3510 ==============\n",
      "============== Pattern 3511 ==============\n",
      "============== Pattern 3512 ==============\n",
      "============== Pattern 3513 ==============\n",
      "============== Pattern 3514 ==============\n",
      "============== Pattern 3515 ==============\n",
      "============== Pattern 3516 ==============\n",
      "============== Pattern 3517 ==============\n",
      "============== Pattern 3518 ==============\n",
      "============== Pattern 3519 ==============\n",
      "============== Pattern 3520 ==============\n",
      "============== Pattern 3521 ==============\n",
      "============== Pattern 3522 ==============\n",
      "============== Pattern 3523 ==============\n",
      "============== Pattern 3524 ==============\n",
      "============== Pattern 3525 ==============\n",
      "============== Pattern 3526 ==============\n",
      "============== Pattern 3527 ==============\n",
      "============== Pattern 3528 ==============\n",
      "============== Pattern 3529 ==============\n",
      "============== Pattern 3530 ==============\n",
      "============== Pattern 3531 ==============\n",
      "============== Pattern 3532 ==============\n",
      "============== Pattern 3533 ==============\n",
      "============== Pattern 3534 ==============\n",
      "============== Pattern 3535 ==============\n",
      "============== Pattern 3536 ==============\n",
      "============== Pattern 3537 ==============\n",
      "============== Pattern 3538 ==============\n",
      "============== Pattern 3539 ==============\n",
      "============== Pattern 3540 ==============\n",
      "============== Pattern 3541 ==============\n",
      "============== Pattern 3542 ==============\n",
      "============== Pattern 3543 ==============\n",
      "============== Pattern 3544 ==============\n",
      "============== Pattern 3545 ==============\n",
      "============== Pattern 3546 ==============\n",
      "============== Pattern 3547 ==============\n",
      "============== Pattern 3548 ==============\n",
      "============== Pattern 3549 ==============\n",
      "============== Pattern 3550 ==============\n",
      "============== Pattern 3551 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 3552 ==============\n",
      "============== Pattern 3553 ==============\n",
      "============== Pattern 3554 ==============\n",
      "============== Pattern 3555 ==============\n",
      "============== Pattern 3556 ==============\n",
      "============== Pattern 3557 ==============\n",
      "============== Pattern 3558 ==============\n",
      "============== Pattern 3559 ==============\n",
      "============== Pattern 3560 ==============\n",
      "============== Pattern 3561 ==============\n",
      "============== Pattern 3562 ==============\n",
      "============== Pattern 3563 ==============\n",
      "============== Pattern 3564 ==============\n",
      "============== Pattern 3565 ==============\n",
      "============== Pattern 3566 ==============\n",
      "============== Pattern 3567 ==============\n",
      "============== Pattern 3568 ==============\n",
      "============== Pattern 3569 ==============\n",
      "============== Pattern 3570 ==============\n",
      "============== Pattern 3571 ==============\n",
      "============== Pattern 3572 ==============\n",
      "============== Pattern 3573 ==============\n",
      "============== Pattern 3574 ==============\n",
      "============== Pattern 3575 ==============\n",
      "============== Pattern 3576 ==============\n",
      "============== Pattern 3577 ==============\n",
      "============== Pattern 3578 ==============\n",
      "============== Pattern 3579 ==============\n",
      "============== Pattern 3580 ==============\n",
      "============== Pattern 3581 ==============\n",
      "============== Pattern 3582 ==============\n",
      "============== Pattern 3583 ==============\n",
      "============== Pattern 3584 ==============\n",
      "============== Pattern 3585 ==============\n",
      "============== Pattern 3586 ==============\n",
      "============== Pattern 3587 ==============\n",
      "============== Pattern 3588 ==============\n",
      "============== Pattern 3589 ==============\n",
      "============== Pattern 3590 ==============\n",
      "============== Pattern 3591 ==============\n",
      "============== Pattern 3592 ==============\n",
      "============== Pattern 3593 ==============\n",
      "============== Pattern 3594 ==============\n",
      "============== Pattern 3595 ==============\n",
      "============== Pattern 3596 ==============\n",
      "============== Pattern 3597 ==============\n",
      "============== Pattern 3598 ==============\n",
      "============== Pattern 3599 ==============\n",
      "============== Pattern 3600 ==============\n",
      "============== Pattern 3601 ==============\n",
      "============== Pattern 3602 ==============\n",
      "============== Pattern 3603 ==============\n",
      "============== Pattern 3604 ==============\n",
      "============== Pattern 3605 ==============\n",
      "============== Pattern 3606 ==============\n",
      "============== Pattern 3607 ==============\n",
      "============== Pattern 3608 ==============\n",
      "============== Pattern 3609 ==============\n",
      "============== Pattern 3610 ==============\n",
      "============== Pattern 3611 ==============\n",
      "============== Pattern 3612 ==============\n",
      "============== Pattern 3613 ==============\n",
      "============== Pattern 3614 ==============\n",
      "============== Pattern 3615 ==============\n",
      "============== Pattern 3616 ==============\n",
      "============== Pattern 3617 ==============\n",
      "============== Pattern 3618 ==============\n",
      "============== Pattern 3619 ==============\n",
      "============== Pattern 3620 ==============\n",
      "============== Pattern 3621 ==============\n",
      "============== Pattern 3622 ==============\n",
      "============== Pattern 3623 ==============\n",
      "============== Pattern 3624 ==============\n",
      "============== Pattern 3625 ==============\n",
      "============== Pattern 3626 ==============\n",
      "============== Pattern 3627 ==============\n",
      "============== Pattern 3628 ==============\n",
      "============== Pattern 3629 ==============\n",
      "============== Pattern 3630 ==============\n",
      "============== Pattern 3631 ==============\n",
      "============== Pattern 3632 ==============\n",
      "============== Pattern 3633 ==============\n",
      "============== Pattern 3634 ==============\n",
      "============== Pattern 3635 ==============\n",
      "============== Pattern 3636 ==============\n",
      "============== Pattern 3637 ==============\n",
      "============== Pattern 3638 ==============\n",
      "============== Pattern 3639 ==============\n",
      "============== Pattern 3640 ==============\n",
      "============== Pattern 3641 ==============\n",
      "============== Pattern 3642 ==============\n",
      "============== Pattern 3643 ==============\n",
      "============== Pattern 3644 ==============\n",
      "============== Pattern 3645 ==============\n",
      "============== Pattern 3646 ==============\n",
      "============== Pattern 3647 ==============\n",
      "============== Pattern 3648 ==============\n",
      "============== Pattern 3649 ==============\n",
      "============== Pattern 3650 ==============\n",
      "============== Pattern 3651 ==============\n",
      "============== Pattern 3652 ==============\n",
      "============== Pattern 3653 ==============\n",
      "============== Pattern 3654 ==============\n",
      "============== Pattern 3655 ==============\n",
      "============== Pattern 3656 ==============\n",
      "============== Pattern 3657 ==============\n",
      "============== Pattern 3658 ==============\n",
      "============== Pattern 3659 ==============\n",
      "============== Pattern 3660 ==============\n",
      "============== Pattern 3661 ==============\n",
      "============== Pattern 3662 ==============\n",
      "============== Pattern 3663 ==============\n",
      "============== Pattern 3664 ==============\n",
      "============== Pattern 3665 ==============\n",
      "============== Pattern 3666 ==============\n",
      "============== Pattern 3667 ==============\n",
      "============== Pattern 3668 ==============\n",
      "============== Pattern 3669 ==============\n",
      "============== Pattern 3670 ==============\n",
      "============== Pattern 3671 ==============\n",
      "============== Pattern 3672 ==============\n",
      "============== Pattern 3673 ==============\n",
      "============== Pattern 3674 ==============\n",
      "============== Pattern 3675 ==============\n",
      "============== Pattern 3676 ==============\n",
      "============== Pattern 3677 ==============\n",
      "============== Pattern 3678 ==============\n",
      "============== Pattern 3679 ==============\n",
      "============== Pattern 3680 ==============\n",
      "============== Pattern 3681 ==============\n",
      "============== Pattern 3682 ==============\n",
      "============== Pattern 3683 ==============\n",
      "============== Pattern 3684 ==============\n",
      "============== Pattern 3685 ==============\n",
      "============== Pattern 3686 ==============\n",
      "============== Pattern 3687 ==============\n",
      "============== Pattern 3688 ==============\n",
      "============== Pattern 3689 ==============\n",
      "============== Pattern 3690 ==============\n",
      "============== Pattern 3691 ==============\n",
      "============== Pattern 3692 ==============\n",
      "============== Pattern 3693 ==============\n",
      "============== Pattern 3694 ==============\n",
      "============== Pattern 3695 ==============\n",
      "============== Pattern 3696 ==============\n",
      "============== Pattern 3697 ==============\n",
      "============== Pattern 3698 ==============\n",
      "============== Pattern 3699 ==============\n",
      "============== Pattern 3700 ==============\n",
      "============== Pattern 3701 ==============\n",
      "============== Pattern 3702 ==============\n",
      "============== Pattern 3703 ==============\n",
      "============== Pattern 3704 ==============\n",
      "============== Pattern 3705 ==============\n",
      "============== Pattern 3706 ==============\n",
      "============== Pattern 3707 ==============\n",
      "============== Pattern 3708 ==============\n",
      "============== Pattern 3709 ==============\n",
      "============== Pattern 3710 ==============\n",
      "============== Pattern 3711 ==============\n",
      "============== Pattern 3712 ==============\n",
      "============== Pattern 3713 ==============\n",
      "============== Pattern 3714 ==============\n",
      "============== Pattern 3715 ==============\n",
      "============== Pattern 3716 ==============\n",
      "============== Pattern 3717 ==============\n",
      "============== Pattern 3718 ==============\n",
      "============== Pattern 3719 ==============\n",
      "============== Pattern 3720 ==============\n",
      "============== Pattern 3721 ==============\n",
      "============== Pattern 3722 ==============\n",
      "============== Pattern 3723 ==============\n",
      "============== Pattern 3724 ==============\n",
      "============== Pattern 3725 ==============\n",
      "============== Pattern 3726 ==============\n",
      "============== Pattern 3727 ==============\n",
      "============== Pattern 3728 ==============\n",
      "============== Pattern 3729 ==============\n",
      "============== Pattern 3730 ==============\n",
      "============== Pattern 3731 ==============\n",
      "============== Pattern 3732 ==============\n",
      "============== Pattern 3733 ==============\n",
      "============== Pattern 3734 ==============\n",
      "============== Pattern 3735 ==============\n",
      "============== Pattern 3736 ==============\n",
      "============== Pattern 3737 ==============\n",
      "============== Pattern 3738 ==============\n",
      "============== Pattern 3739 ==============\n",
      "============== Pattern 3740 ==============\n",
      "============== Pattern 3741 ==============\n",
      "============== Pattern 3742 ==============\n",
      "============== Pattern 3743 ==============\n",
      "============== Pattern 3744 ==============\n",
      "============== Pattern 3745 ==============\n",
      "============== Pattern 3746 ==============\n",
      "============== Pattern 3747 ==============\n",
      "============== Pattern 3748 ==============\n",
      "============== Pattern 3749 ==============\n",
      "============== Pattern 3750 ==============\n",
      "============== Pattern 3751 ==============\n",
      "============== Pattern 3752 ==============\n",
      "============== Pattern 3753 ==============\n",
      "============== Pattern 3754 ==============\n",
      "============== Pattern 3755 ==============\n",
      "============== Pattern 3756 ==============\n",
      "============== Pattern 3757 ==============\n",
      "============== Pattern 3758 ==============\n",
      "============== Pattern 3759 ==============\n",
      "============== Pattern 3760 ==============\n",
      "============== Pattern 3761 ==============\n",
      "============== Pattern 3762 ==============\n",
      "============== Pattern 3763 ==============\n",
      "============== Pattern 3764 ==============\n",
      "============== Pattern 3765 ==============\n",
      "============== Pattern 3766 ==============\n",
      "============== Pattern 3767 ==============\n",
      "============== Pattern 3768 ==============\n",
      "============== Pattern 3769 ==============\n",
      "============== Pattern 3770 ==============\n",
      "============== Pattern 3771 ==============\n",
      "============== Pattern 3772 ==============\n",
      "============== Pattern 3773 ==============\n",
      "============== Pattern 3774 ==============\n",
      "============== Pattern 3775 ==============\n",
      "============== Pattern 3776 ==============\n",
      "============== Pattern 3777 ==============\n",
      "============== Pattern 3778 ==============\n",
      "============== Pattern 3779 ==============\n",
      "============== Pattern 3780 ==============\n",
      "============== Pattern 3781 ==============\n",
      "============== Pattern 3782 ==============\n",
      "============== Pattern 3783 ==============\n",
      "============== Pattern 3784 ==============\n",
      "============== Pattern 3785 ==============\n",
      "============== Pattern 3786 ==============\n",
      "============== Pattern 3787 ==============\n",
      "============== Pattern 3788 ==============\n",
      "============== Pattern 3789 ==============\n",
      "============== Pattern 3790 ==============\n",
      "============== Pattern 3791 ==============\n",
      "============== Pattern 3792 ==============\n",
      "============== Pattern 3793 ==============\n",
      "============== Pattern 3794 ==============\n",
      "============== Pattern 3795 ==============\n",
      "============== Pattern 3796 ==============\n",
      "============== Pattern 3797 ==============\n",
      "============== Pattern 3798 ==============\n",
      "============== Pattern 3799 ==============\n",
      "Average comprehensibility: 64.17425638325875\n",
      "std comprehensibility: 4.402990269709591\n",
      "var comprehensibility: 19.38632331515733\n",
      "minimum comprehensibility: 40\n",
      "maximum comprehensibility: 72\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
