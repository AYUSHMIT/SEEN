{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 8\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.131396293640137 | KNN Loss: 6.227344036102295 | BCE Loss: 1.9040518999099731\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.219286918640137 | KNN Loss: 6.227286338806152 | BCE Loss: 1.9920002222061157\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.189247131347656 | KNN Loss: 6.226543426513672 | BCE Loss: 1.9627039432525635\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.169183731079102 | KNN Loss: 6.226537704467773 | BCE Loss: 1.9426462650299072\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.15272331237793 | KNN Loss: 6.2260355949401855 | BCE Loss: 1.926687479019165\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.198753356933594 | KNN Loss: 6.225461959838867 | BCE Loss: 1.973291039466858\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.116584777832031 | KNN Loss: 6.225305557250977 | BCE Loss: 1.8912787437438965\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.173934936523438 | KNN Loss: 6.224647521972656 | BCE Loss: 1.9492878913879395\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.133499145507812 | KNN Loss: 6.2245774269104 | BCE Loss: 1.9089219570159912\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.14252758026123 | KNN Loss: 6.224137306213379 | BCE Loss: 1.9183903932571411\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.118864059448242 | KNN Loss: 6.223903179168701 | BCE Loss: 1.8949607610702515\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.164261817932129 | KNN Loss: 6.222790241241455 | BCE Loss: 1.941471815109253\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.062267303466797 | KNN Loss: 6.2227630615234375 | BCE Loss: 1.8395040035247803\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.076176643371582 | KNN Loss: 6.222609996795654 | BCE Loss: 1.8535666465759277\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.052228927612305 | KNN Loss: 6.222347259521484 | BCE Loss: 1.8298819065093994\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.125906944274902 | KNN Loss: 6.22182559967041 | BCE Loss: 1.9040815830230713\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.121522903442383 | KNN Loss: 6.2208356857299805 | BCE Loss: 1.9006867408752441\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.068533897399902 | KNN Loss: 6.220104694366455 | BCE Loss: 1.8484292030334473\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.016153335571289 | KNN Loss: 6.219286918640137 | BCE Loss: 1.7968659400939941\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.016769409179688 | KNN Loss: 6.219320297241211 | BCE Loss: 1.797448992729187\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.056300163269043 | KNN Loss: 6.218317985534668 | BCE Loss: 1.8379820585250854\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.027800559997559 | KNN Loss: 6.216907501220703 | BCE Loss: 1.8108932971954346\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.011094093322754 | KNN Loss: 6.216343402862549 | BCE Loss: 1.7947509288787842\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.018869400024414 | KNN Loss: 6.21601676940918 | BCE Loss: 1.802852749824524\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.00739860534668 | KNN Loss: 6.21458625793457 | BCE Loss: 1.7928123474121094\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.989277362823486 | KNN Loss: 6.213442325592041 | BCE Loss: 1.7758349180221558\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.941864967346191 | KNN Loss: 6.211440086364746 | BCE Loss: 1.7304251194000244\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.944711685180664 | KNN Loss: 6.210745334625244 | BCE Loss: 1.7339661121368408\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.984351634979248 | KNN Loss: 6.208042144775391 | BCE Loss: 1.776309609413147\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.922089576721191 | KNN Loss: 6.208771228790283 | BCE Loss: 1.713318109512329\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.938891410827637 | KNN Loss: 6.204841613769531 | BCE Loss: 1.7340500354766846\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.924600124359131 | KNN Loss: 6.2046098709106445 | BCE Loss: 1.7199902534484863\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.879550457000732 | KNN Loss: 6.199975967407227 | BCE Loss: 1.6795743703842163\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.867622375488281 | KNN Loss: 6.198607921600342 | BCE Loss: 1.669014573097229\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.862906455993652 | KNN Loss: 6.197588920593262 | BCE Loss: 1.6653177738189697\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.874168395996094 | KNN Loss: 6.1913251876831055 | BCE Loss: 1.6828430891036987\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.850286483764648 | KNN Loss: 6.189162254333496 | BCE Loss: 1.6611244678497314\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.845605850219727 | KNN Loss: 6.18058967590332 | BCE Loss: 1.6650164127349854\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.814574241638184 | KNN Loss: 6.178523063659668 | BCE Loss: 1.6360509395599365\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.7817535400390625 | KNN Loss: 6.168537616729736 | BCE Loss: 1.6132159233093262\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.7531232833862305 | KNN Loss: 6.155057430267334 | BCE Loss: 1.598065733909607\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.733285427093506 | KNN Loss: 6.147588729858398 | BCE Loss: 1.5856965780258179\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.730435371398926 | KNN Loss: 6.142013072967529 | BCE Loss: 1.588422179222107\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.6604509353637695 | KNN Loss: 6.130553245544434 | BCE Loss: 1.5298974514007568\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.670519828796387 | KNN Loss: 6.118217945098877 | BCE Loss: 1.5523016452789307\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.584078788757324 | KNN Loss: 6.102785110473633 | BCE Loss: 1.4812934398651123\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.599847793579102 | KNN Loss: 6.079061031341553 | BCE Loss: 1.520787000656128\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.516204357147217 | KNN Loss: 6.050693988800049 | BCE Loss: 1.465510368347168\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.499855041503906 | KNN Loss: 6.048608303070068 | BCE Loss: 1.451246738433838\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.436926364898682 | KNN Loss: 6.018132209777832 | BCE Loss: 1.4187941551208496\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.417667388916016 | KNN Loss: 5.977173328399658 | BCE Loss: 1.4404938220977783\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.3443756103515625 | KNN Loss: 5.944165229797363 | BCE Loss: 1.4002103805541992\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.289612770080566 | KNN Loss: 5.9079742431640625 | BCE Loss: 1.3816382884979248\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.2089152336120605 | KNN Loss: 5.846636772155762 | BCE Loss: 1.3622783422470093\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.135528087615967 | KNN Loss: 5.784082889556885 | BCE Loss: 1.351445198059082\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.06419563293457 | KNN Loss: 5.738746166229248 | BCE Loss: 1.3254493474960327\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.970467567443848 | KNN Loss: 5.670966148376465 | BCE Loss: 1.299501657485962\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.841743469238281 | KNN Loss: 5.554909706115723 | BCE Loss: 1.286833643913269\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.723897457122803 | KNN Loss: 5.493316650390625 | BCE Loss: 1.2305808067321777\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.607323169708252 | KNN Loss: 5.379526615142822 | BCE Loss: 1.2277966737747192\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.488699436187744 | KNN Loss: 5.252879619598389 | BCE Loss: 1.2358198165893555\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.357574462890625 | KNN Loss: 5.158622741699219 | BCE Loss: 1.1989514827728271\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.227585315704346 | KNN Loss: 5.057672023773193 | BCE Loss: 1.1699132919311523\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.014003276824951 | KNN Loss: 4.875308990478516 | BCE Loss: 1.1386942863464355\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 5.913588523864746 | KNN Loss: 4.777832984924316 | BCE Loss: 1.1357557773590088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.735421180725098 | KNN Loss: 4.626675605773926 | BCE Loss: 1.1087453365325928\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.6172637939453125 | KNN Loss: 4.494812965393066 | BCE Loss: 1.122450590133667\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.558211326599121 | KNN Loss: 4.4204607009887695 | BCE Loss: 1.1377503871917725\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.433544158935547 | KNN Loss: 4.292762279510498 | BCE Loss: 1.140782117843628\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.3397369384765625 | KNN Loss: 4.232474327087402 | BCE Loss: 1.1072626113891602\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.2252984046936035 | KNN Loss: 4.119917392730713 | BCE Loss: 1.1053811311721802\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.177340507507324 | KNN Loss: 4.050661563873291 | BCE Loss: 1.126678705215454\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.075552940368652 | KNN Loss: 3.9711804389953613 | BCE Loss: 1.104372501373291\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 4.999900817871094 | KNN Loss: 3.8714439868927 | BCE Loss: 1.1284570693969727\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 4.929929256439209 | KNN Loss: 3.816544532775879 | BCE Loss: 1.1133848428726196\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 4.870271682739258 | KNN Loss: 3.7739181518554688 | BCE Loss: 1.0963536500930786\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 4.782839775085449 | KNN Loss: 3.6815526485443115 | BCE Loss: 1.1012868881225586\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 4.751770973205566 | KNN Loss: 3.6620750427246094 | BCE Loss: 1.089695692062378\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 4.727425575256348 | KNN Loss: 3.623802900314331 | BCE Loss: 1.1036229133605957\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 4.660619735717773 | KNN Loss: 3.580169916152954 | BCE Loss: 1.0804495811462402\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 4.6360931396484375 | KNN Loss: 3.54231858253479 | BCE Loss: 1.0937745571136475\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 4.5600905418396 | KNN Loss: 3.4889094829559326 | BCE Loss: 1.071181058883667\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 4.544855117797852 | KNN Loss: 3.477066993713379 | BCE Loss: 1.0677882432937622\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.5803632736206055 | KNN Loss: 3.485858678817749 | BCE Loss: 1.0945045948028564\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.539196014404297 | KNN Loss: 3.4439494609832764 | BCE Loss: 1.09524667263031\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.479753017425537 | KNN Loss: 3.395434617996216 | BCE Loss: 1.0843185186386108\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.4947614669799805 | KNN Loss: 3.421332836151123 | BCE Loss: 1.0734288692474365\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.441792011260986 | KNN Loss: 3.39052152633667 | BCE Loss: 1.0512704849243164\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.5015668869018555 | KNN Loss: 3.432159900665283 | BCE Loss: 1.0694067478179932\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.433469772338867 | KNN Loss: 3.3742454051971436 | BCE Loss: 1.0592246055603027\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.410974979400635 | KNN Loss: 3.3069207668304443 | BCE Loss: 1.1040542125701904\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.427283763885498 | KNN Loss: 3.3700170516967773 | BCE Loss: 1.0572667121887207\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.414827346801758 | KNN Loss: 3.3190290927886963 | BCE Loss: 1.095798373222351\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.39992094039917 | KNN Loss: 3.3341991901397705 | BCE Loss: 1.0657216310501099\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.380707740783691 | KNN Loss: 3.337254285812378 | BCE Loss: 1.0434532165527344\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.458868980407715 | KNN Loss: 3.3710615634918213 | BCE Loss: 1.0878076553344727\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.421004772186279 | KNN Loss: 3.370436429977417 | BCE Loss: 1.0505682229995728\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.4031453132629395 | KNN Loss: 3.3149256706237793 | BCE Loss: 1.0882196426391602\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.420620441436768 | KNN Loss: 3.351163864135742 | BCE Loss: 1.0694565773010254\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.4065141677856445 | KNN Loss: 3.328150749206543 | BCE Loss: 1.0783631801605225\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.353744029998779 | KNN Loss: 3.292891502380371 | BCE Loss: 1.0608525276184082\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.368934631347656 | KNN Loss: 3.2888379096984863 | BCE Loss: 1.0800964832305908\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.362097263336182 | KNN Loss: 3.290414571762085 | BCE Loss: 1.0716828107833862\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.300698280334473 | KNN Loss: 3.248037576675415 | BCE Loss: 1.0526609420776367\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.319510459899902 | KNN Loss: 3.2674217224121094 | BCE Loss: 1.052088737487793\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.291889667510986 | KNN Loss: 3.221210479736328 | BCE Loss: 1.0706793069839478\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.3639020919799805 | KNN Loss: 3.299960136413574 | BCE Loss: 1.0639419555664062\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.28033447265625 | KNN Loss: 3.240421772003174 | BCE Loss: 1.0399129390716553\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.310236930847168 | KNN Loss: 3.2446930408477783 | BCE Loss: 1.0655441284179688\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.293252944946289 | KNN Loss: 3.2323389053344727 | BCE Loss: 1.0609138011932373\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.333366870880127 | KNN Loss: 3.2603909969329834 | BCE Loss: 1.072975993156433\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.314081192016602 | KNN Loss: 3.2401537895202637 | BCE Loss: 1.0739272832870483\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.348728179931641 | KNN Loss: 3.271484613418579 | BCE Loss: 1.0772433280944824\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.280937194824219 | KNN Loss: 3.220184803009033 | BCE Loss: 1.0607523918151855\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.294065475463867 | KNN Loss: 3.236039161682129 | BCE Loss: 1.0580260753631592\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.325290679931641 | KNN Loss: 3.2981650829315186 | BCE Loss: 1.027125358581543\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.303144454956055 | KNN Loss: 3.2525525093078613 | BCE Loss: 1.0505918264389038\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.270448684692383 | KNN Loss: 3.2206943035125732 | BCE Loss: 1.0497545003890991\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.275684356689453 | KNN Loss: 3.2116575241088867 | BCE Loss: 1.064026951789856\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.285688400268555 | KNN Loss: 3.2332100868225098 | BCE Loss: 1.052478551864624\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.242886066436768 | KNN Loss: 3.1980786323547363 | BCE Loss: 1.0448074340820312\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.3164591789245605 | KNN Loss: 3.2661678791046143 | BCE Loss: 1.0502912998199463\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.361149787902832 | KNN Loss: 3.288287401199341 | BCE Loss: 1.072862148284912\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.311713218688965 | KNN Loss: 3.252835512161255 | BCE Loss: 1.05887770652771\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.26365852355957 | KNN Loss: 3.212679386138916 | BCE Loss: 1.0509792566299438\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.274439811706543 | KNN Loss: 3.21437668800354 | BCE Loss: 1.0600628852844238\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.237247467041016 | KNN Loss: 3.1918537616729736 | BCE Loss: 1.045393943786621\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.292148590087891 | KNN Loss: 3.231940269470215 | BCE Loss: 1.0602080821990967\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.310086727142334 | KNN Loss: 3.2537410259246826 | BCE Loss: 1.0563455820083618\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.241975784301758 | KNN Loss: 3.2270185947418213 | BCE Loss: 1.0149571895599365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.31995964050293 | KNN Loss: 3.2258055210113525 | BCE Loss: 1.094153881072998\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.285663604736328 | KNN Loss: 3.239567518234253 | BCE Loss: 1.0460963249206543\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.283781051635742 | KNN Loss: 3.248910903930664 | BCE Loss: 1.034869909286499\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.315661907196045 | KNN Loss: 3.242034912109375 | BCE Loss: 1.0736271142959595\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.284772872924805 | KNN Loss: 3.226449728012085 | BCE Loss: 1.0583233833312988\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.303432464599609 | KNN Loss: 3.245673418045044 | BCE Loss: 1.0577588081359863\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.270933628082275 | KNN Loss: 3.180241346359253 | BCE Loss: 1.0906922817230225\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.238199234008789 | KNN Loss: 3.1943888664245605 | BCE Loss: 1.0438101291656494\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.236080169677734 | KNN Loss: 3.1972532272338867 | BCE Loss: 1.0388270616531372\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.254725456237793 | KNN Loss: 3.2150280475616455 | BCE Loss: 1.039697527885437\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.263209342956543 | KNN Loss: 3.2204372882843018 | BCE Loss: 1.0427720546722412\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.2154316902160645 | KNN Loss: 3.1770243644714355 | BCE Loss: 1.038407325744629\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.196951866149902 | KNN Loss: 3.1717469692230225 | BCE Loss: 1.025205135345459\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.215401649475098 | KNN Loss: 3.168125867843628 | BCE Loss: 1.0472759008407593\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.2402801513671875 | KNN Loss: 3.1914114952087402 | BCE Loss: 1.0488686561584473\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.270059585571289 | KNN Loss: 3.207874298095703 | BCE Loss: 1.0621850490570068\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.276688575744629 | KNN Loss: 3.2179176807403564 | BCE Loss: 1.058770775794983\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.199867248535156 | KNN Loss: 3.186129093170166 | BCE Loss: 1.0137379169464111\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.191973686218262 | KNN Loss: 3.1477465629577637 | BCE Loss: 1.0442270040512085\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.238898754119873 | KNN Loss: 3.156200885772705 | BCE Loss: 1.0826977491378784\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.182596206665039 | KNN Loss: 3.1556851863861084 | BCE Loss: 1.0269109010696411\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.250646114349365 | KNN Loss: 3.1972568035125732 | BCE Loss: 1.0533894300460815\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.190358638763428 | KNN Loss: 3.156001091003418 | BCE Loss: 1.0343575477600098\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.2175822257995605 | KNN Loss: 3.1757380962371826 | BCE Loss: 1.0418440103530884\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.207906723022461 | KNN Loss: 3.1612491607666016 | BCE Loss: 1.046657681465149\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.1982622146606445 | KNN Loss: 3.1558620929718018 | BCE Loss: 1.0423998832702637\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.227619171142578 | KNN Loss: 3.1637868881225586 | BCE Loss: 1.0638322830200195\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.275930404663086 | KNN Loss: 3.216120481491089 | BCE Loss: 1.059809923171997\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.223964691162109 | KNN Loss: 3.160142183303833 | BCE Loss: 1.0638225078582764\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.241523265838623 | KNN Loss: 3.1918041706085205 | BCE Loss: 1.0497190952301025\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.216525077819824 | KNN Loss: 3.175997257232666 | BCE Loss: 1.040527582168579\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.203012466430664 | KNN Loss: 3.1585545539855957 | BCE Loss: 1.0444576740264893\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.214311599731445 | KNN Loss: 3.151496410369873 | BCE Loss: 1.0628149509429932\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.235462665557861 | KNN Loss: 3.209000587463379 | BCE Loss: 1.026462197303772\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.2255964279174805 | KNN Loss: 3.179959297180176 | BCE Loss: 1.0456373691558838\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.199252128601074 | KNN Loss: 3.151569366455078 | BCE Loss: 1.0476828813552856\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.210369110107422 | KNN Loss: 3.1853067874908447 | BCE Loss: 1.0250622034072876\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.173543930053711 | KNN Loss: 3.154711961746216 | BCE Loss: 1.018831729888916\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.226199150085449 | KNN Loss: 3.168761730194092 | BCE Loss: 1.0574374198913574\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.226461410522461 | KNN Loss: 3.178046226501465 | BCE Loss: 1.048415184020996\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.205435752868652 | KNN Loss: 3.181439161300659 | BCE Loss: 1.0239964723587036\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.2060041427612305 | KNN Loss: 3.1542365550994873 | BCE Loss: 1.051767349243164\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.164248466491699 | KNN Loss: 3.1395950317382812 | BCE Loss: 1.0246535539627075\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.168019771575928 | KNN Loss: 3.1512932777404785 | BCE Loss: 1.0167266130447388\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.216031551361084 | KNN Loss: 3.166186809539795 | BCE Loss: 1.049844741821289\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.209429740905762 | KNN Loss: 3.1723294258117676 | BCE Loss: 1.0371005535125732\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.204034805297852 | KNN Loss: 3.1879215240478516 | BCE Loss: 1.01611328125\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.174731731414795 | KNN Loss: 3.1581645011901855 | BCE Loss: 1.016567349433899\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.195272445678711 | KNN Loss: 3.1607611179351807 | BCE Loss: 1.0345110893249512\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.194523334503174 | KNN Loss: 3.125560998916626 | BCE Loss: 1.0689622163772583\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.1938982009887695 | KNN Loss: 3.15000057220459 | BCE Loss: 1.0438978672027588\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.154813289642334 | KNN Loss: 3.147122859954834 | BCE Loss: 1.0076903104782104\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.188809394836426 | KNN Loss: 3.143925905227661 | BCE Loss: 1.0448832511901855\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.229056358337402 | KNN Loss: 3.18369197845459 | BCE Loss: 1.045364499092102\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.215646743774414 | KNN Loss: 3.152202606201172 | BCE Loss: 1.0634441375732422\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.173319339752197 | KNN Loss: 3.1642675399780273 | BCE Loss: 1.00905179977417\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.203705310821533 | KNN Loss: 3.1477737426757812 | BCE Loss: 1.055931568145752\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.196032524108887 | KNN Loss: 3.166445255279541 | BCE Loss: 1.0295875072479248\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.218882083892822 | KNN Loss: 3.17807936668396 | BCE Loss: 1.0408027172088623\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.181055068969727 | KNN Loss: 3.133291482925415 | BCE Loss: 1.0477638244628906\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.2284393310546875 | KNN Loss: 3.1688859462738037 | BCE Loss: 1.059553623199463\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.147632598876953 | KNN Loss: 3.1372296810150146 | BCE Loss: 1.0104031562805176\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.190829753875732 | KNN Loss: 3.157654285430908 | BCE Loss: 1.0331754684448242\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.26226806640625 | KNN Loss: 3.208526372909546 | BCE Loss: 1.053741455078125\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.183207035064697 | KNN Loss: 3.166205883026123 | BCE Loss: 1.0170011520385742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.13721227645874 | KNN Loss: 3.1251296997070312 | BCE Loss: 1.012082576751709\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.149568557739258 | KNN Loss: 3.1208372116088867 | BCE Loss: 1.0287314653396606\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.170516014099121 | KNN Loss: 3.142583131790161 | BCE Loss: 1.0279330015182495\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.163435935974121 | KNN Loss: 3.113342761993408 | BCE Loss: 1.0500929355621338\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.1312336921691895 | KNN Loss: 3.119642496109009 | BCE Loss: 1.0115911960601807\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.195626258850098 | KNN Loss: 3.148836612701416 | BCE Loss: 1.0467896461486816\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.19155216217041 | KNN Loss: 3.149794816970825 | BCE Loss: 1.041757583618164\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.164190292358398 | KNN Loss: 3.1522083282470703 | BCE Loss: 1.011981725692749\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.20261287689209 | KNN Loss: 3.164604902267456 | BCE Loss: 1.0380079746246338\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.221957206726074 | KNN Loss: 3.17142653465271 | BCE Loss: 1.0505304336547852\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.194150924682617 | KNN Loss: 3.158517360687256 | BCE Loss: 1.0356336832046509\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.247710227966309 | KNN Loss: 3.2048420906066895 | BCE Loss: 1.04286789894104\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.1989617347717285 | KNN Loss: 3.1468024253845215 | BCE Loss: 1.0521594285964966\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.137213230133057 | KNN Loss: 3.10705304145813 | BCE Loss: 1.0301601886749268\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.183040618896484 | KNN Loss: 3.1505937576293945 | BCE Loss: 1.0324466228485107\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.175400733947754 | KNN Loss: 3.1416261196136475 | BCE Loss: 1.0337743759155273\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.243335723876953 | KNN Loss: 3.18829607963562 | BCE Loss: 1.055039882659912\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.185103893280029 | KNN Loss: 3.146475315093994 | BCE Loss: 1.0386286973953247\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.179981708526611 | KNN Loss: 3.163172483444214 | BCE Loss: 1.0168092250823975\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.199604511260986 | KNN Loss: 3.1442928314208984 | BCE Loss: 1.055311679840088\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.138523101806641 | KNN Loss: 3.1072256565093994 | BCE Loss: 1.0312975645065308\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.201769828796387 | KNN Loss: 3.1629395484924316 | BCE Loss: 1.038830041885376\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.204693794250488 | KNN Loss: 3.1660845279693604 | BCE Loss: 1.038609266281128\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.1775712966918945 | KNN Loss: 3.144456148147583 | BCE Loss: 1.033115029335022\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.158329963684082 | KNN Loss: 3.123262643814087 | BCE Loss: 1.035067081451416\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.190876007080078 | KNN Loss: 3.1399242877960205 | BCE Loss: 1.0509518384933472\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.190556526184082 | KNN Loss: 3.1427719593048096 | BCE Loss: 1.0477845668792725\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.145506858825684 | KNN Loss: 3.1138994693756104 | BCE Loss: 1.0316076278686523\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.148839950561523 | KNN Loss: 3.1259677410125732 | BCE Loss: 1.0228724479675293\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.176401138305664 | KNN Loss: 3.1322689056396484 | BCE Loss: 1.0441319942474365\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.175576686859131 | KNN Loss: 3.1462831497192383 | BCE Loss: 1.029293417930603\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.158664703369141 | KNN Loss: 3.1280791759490967 | BCE Loss: 1.030585527420044\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.1578898429870605 | KNN Loss: 3.139333724975586 | BCE Loss: 1.0185561180114746\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.1531877517700195 | KNN Loss: 3.1279289722442627 | BCE Loss: 1.025259017944336\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.206541538238525 | KNN Loss: 3.1722609996795654 | BCE Loss: 1.0342806577682495\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.209219455718994 | KNN Loss: 3.164726495742798 | BCE Loss: 1.0444929599761963\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.139486312866211 | KNN Loss: 3.1290605068206787 | BCE Loss: 1.0104256868362427\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.157001495361328 | KNN Loss: 3.1159579753875732 | BCE Loss: 1.0410435199737549\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.123933792114258 | KNN Loss: 3.094425678253174 | BCE Loss: 1.029508113861084\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.194943428039551 | KNN Loss: 3.1436915397644043 | BCE Loss: 1.0512516498565674\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.183884620666504 | KNN Loss: 3.1436100006103516 | BCE Loss: 1.0402748584747314\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.123827934265137 | KNN Loss: 3.104574203491211 | BCE Loss: 1.0192537307739258\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.164800643920898 | KNN Loss: 3.1209733486175537 | BCE Loss: 1.0438270568847656\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.186236381530762 | KNN Loss: 3.134011745452881 | BCE Loss: 1.0522243976593018\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.15402889251709 | KNN Loss: 3.1014626026153564 | BCE Loss: 1.052566409111023\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.177159309387207 | KNN Loss: 3.1218161582946777 | BCE Loss: 1.0553431510925293\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.19167423248291 | KNN Loss: 3.1634812355041504 | BCE Loss: 1.0281927585601807\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.19012975692749 | KNN Loss: 3.1639108657836914 | BCE Loss: 1.0262188911437988\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.176502227783203 | KNN Loss: 3.141263961791992 | BCE Loss: 1.035238265991211\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.2197771072387695 | KNN Loss: 3.1667251586914062 | BCE Loss: 1.0530518293380737\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.146923065185547 | KNN Loss: 3.108412027359009 | BCE Loss: 1.038510799407959\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.173198699951172 | KNN Loss: 3.1098783016204834 | BCE Loss: 1.0633201599121094\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.204329013824463 | KNN Loss: 3.164945602416992 | BCE Loss: 1.0393834114074707\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.190493583679199 | KNN Loss: 3.1603806018829346 | BCE Loss: 1.0301127433776855\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.150026321411133 | KNN Loss: 3.125624656677246 | BCE Loss: 1.0244019031524658\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.182000160217285 | KNN Loss: 3.1546804904937744 | BCE Loss: 1.0273199081420898\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.150852203369141 | KNN Loss: 3.111025333404541 | BCE Loss: 1.0398268699645996\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.11350679397583 | KNN Loss: 3.09818959236145 | BCE Loss: 1.0153172016143799\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.129833698272705 | KNN Loss: 3.1249547004699707 | BCE Loss: 1.0048789978027344\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.179494857788086 | KNN Loss: 3.13999605178833 | BCE Loss: 1.0394985675811768\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.202077388763428 | KNN Loss: 3.162048101425171 | BCE Loss: 1.0400294065475464\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.165925979614258 | KNN Loss: 3.1509299278259277 | BCE Loss: 1.014995813369751\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.181524276733398 | KNN Loss: 3.140824794769287 | BCE Loss: 1.0406994819641113\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.142848491668701 | KNN Loss: 3.1135060787200928 | BCE Loss: 1.0293424129486084\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.137979030609131 | KNN Loss: 3.114407539367676 | BCE Loss: 1.023571491241455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.186305999755859 | KNN Loss: 3.139052391052246 | BCE Loss: 1.0472533702850342\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.175289154052734 | KNN Loss: 3.1487531661987305 | BCE Loss: 1.0265361070632935\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.121947288513184 | KNN Loss: 3.1192805767059326 | BCE Loss: 1.00266695022583\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.216286659240723 | KNN Loss: 3.1491591930389404 | BCE Loss: 1.0671277046203613\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.175373077392578 | KNN Loss: 3.1491129398345947 | BCE Loss: 1.0262600183486938\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.200582981109619 | KNN Loss: 3.1648108959198 | BCE Loss: 1.0357720851898193\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.13418436050415 | KNN Loss: 3.1326236724853516 | BCE Loss: 1.0015605688095093\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.181057929992676 | KNN Loss: 3.1439993381500244 | BCE Loss: 1.0370588302612305\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.135180473327637 | KNN Loss: 3.1183269023895264 | BCE Loss: 1.0168534517288208\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.187615871429443 | KNN Loss: 3.144101858139038 | BCE Loss: 1.0435140132904053\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.126002311706543 | KNN Loss: 3.107069253921509 | BCE Loss: 1.0189332962036133\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.18552303314209 | KNN Loss: 3.1543381214141846 | BCE Loss: 1.0311846733093262\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.144169330596924 | KNN Loss: 3.1448872089385986 | BCE Loss: 0.9992820024490356\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.1611456871032715 | KNN Loss: 3.1215057373046875 | BCE Loss: 1.0396400690078735\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.140533447265625 | KNN Loss: 3.1302433013916016 | BCE Loss: 1.0102901458740234\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.1313323974609375 | KNN Loss: 3.104658603668213 | BCE Loss: 1.0266737937927246\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.173209190368652 | KNN Loss: 3.144075393676758 | BCE Loss: 1.0291337966918945\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.165820121765137 | KNN Loss: 3.153146505355835 | BCE Loss: 1.0126737356185913\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.1617326736450195 | KNN Loss: 3.1378307342529297 | BCE Loss: 1.0239017009735107\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.131668567657471 | KNN Loss: 3.1115760803222656 | BCE Loss: 1.020092487335205\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.164979457855225 | KNN Loss: 3.1249325275421143 | BCE Loss: 1.0400470495224\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.1461944580078125 | KNN Loss: 3.114093780517578 | BCE Loss: 1.0321006774902344\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.127007007598877 | KNN Loss: 3.1150336265563965 | BCE Loss: 1.011973261833191\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.197903156280518 | KNN Loss: 3.165356397628784 | BCE Loss: 1.0325467586517334\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.175543785095215 | KNN Loss: 3.1524038314819336 | BCE Loss: 1.0231398344039917\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.177581310272217 | KNN Loss: 3.1446268558502197 | BCE Loss: 1.0329543352127075\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.129818916320801 | KNN Loss: 3.103245973587036 | BCE Loss: 1.026572823524475\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.117456436157227 | KNN Loss: 3.109251022338867 | BCE Loss: 1.0082054138183594\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.161420822143555 | KNN Loss: 3.143583059310913 | BCE Loss: 1.0178377628326416\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.154199600219727 | KNN Loss: 3.1203644275665283 | BCE Loss: 1.0338349342346191\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.179875373840332 | KNN Loss: 3.1466963291168213 | BCE Loss: 1.0331792831420898\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.194649696350098 | KNN Loss: 3.1373021602630615 | BCE Loss: 1.0573475360870361\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.162497043609619 | KNN Loss: 3.12945556640625 | BCE Loss: 1.0330414772033691\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.189238548278809 | KNN Loss: 3.143911123275757 | BCE Loss: 1.0453274250030518\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.148932933807373 | KNN Loss: 3.12872314453125 | BCE Loss: 1.0202096700668335\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.1193952560424805 | KNN Loss: 3.099358081817627 | BCE Loss: 1.020037055015564\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.151117324829102 | KNN Loss: 3.1213676929473877 | BCE Loss: 1.0297495126724243\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.153681755065918 | KNN Loss: 3.120100736618042 | BCE Loss: 1.0335808992385864\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.167672157287598 | KNN Loss: 3.1286306381225586 | BCE Loss: 1.039041519165039\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.145127296447754 | KNN Loss: 3.1164610385894775 | BCE Loss: 1.0286664962768555\n",
      "Epoch    50: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.1464338302612305 | KNN Loss: 3.1120080947875977 | BCE Loss: 1.0344256162643433\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.112035751342773 | KNN Loss: 3.1242458820343018 | BCE Loss: 0.9877899885177612\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.156120300292969 | KNN Loss: 3.114995241165161 | BCE Loss: 1.0411248207092285\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.142928123474121 | KNN Loss: 3.1163010597229004 | BCE Loss: 1.0266273021697998\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.139703750610352 | KNN Loss: 3.1194748878479004 | BCE Loss: 1.0202291011810303\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.179195404052734 | KNN Loss: 3.1451778411865234 | BCE Loss: 1.0340173244476318\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.141345500946045 | KNN Loss: 3.1372764110565186 | BCE Loss: 1.0040690898895264\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.1692399978637695 | KNN Loss: 3.123166799545288 | BCE Loss: 1.0460731983184814\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.152669429779053 | KNN Loss: 3.122506618499756 | BCE Loss: 1.0301628112792969\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.14576530456543 | KNN Loss: 3.1203818321228027 | BCE Loss: 1.0253833532333374\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.076380729675293 | KNN Loss: 3.076549530029297 | BCE Loss: 0.999830961227417\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.142824649810791 | KNN Loss: 3.1212451457977295 | BCE Loss: 1.021579623222351\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.136700630187988 | KNN Loss: 3.116337776184082 | BCE Loss: 1.0203628540039062\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.109818935394287 | KNN Loss: 3.0934603214263916 | BCE Loss: 1.0163586139678955\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.124332427978516 | KNN Loss: 3.116619110107422 | BCE Loss: 1.0077135562896729\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.157456398010254 | KNN Loss: 3.1287262439727783 | BCE Loss: 1.0287302732467651\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.127065658569336 | KNN Loss: 3.120598077774048 | BCE Loss: 1.006467342376709\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.133094787597656 | KNN Loss: 3.1192872524261475 | BCE Loss: 1.0138072967529297\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.1528215408325195 | KNN Loss: 3.126345634460449 | BCE Loss: 1.0264756679534912\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.158656120300293 | KNN Loss: 3.1090314388275146 | BCE Loss: 1.0496249198913574\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.157145977020264 | KNN Loss: 3.136727809906006 | BCE Loss: 1.0204182863235474\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.134947776794434 | KNN Loss: 3.1079599857330322 | BCE Loss: 1.026987910270691\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.1066694259643555 | KNN Loss: 3.0929605960845947 | BCE Loss: 1.0137085914611816\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.1297383308410645 | KNN Loss: 3.1302380561828613 | BCE Loss: 0.9995003938674927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.112297058105469 | KNN Loss: 3.101496934890747 | BCE Loss: 1.0108003616333008\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.187385082244873 | KNN Loss: 3.1383895874023438 | BCE Loss: 1.0489956140518188\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.11583948135376 | KNN Loss: 3.110107421875 | BCE Loss: 1.0057320594787598\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.141858100891113 | KNN Loss: 3.113483190536499 | BCE Loss: 1.0283749103546143\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.1421918869018555 | KNN Loss: 3.1321442127227783 | BCE Loss: 1.0100479125976562\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.166543006896973 | KNN Loss: 3.104938507080078 | BCE Loss: 1.0616044998168945\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.148497581481934 | KNN Loss: 3.126340627670288 | BCE Loss: 1.0221569538116455\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.132956027984619 | KNN Loss: 3.110224962234497 | BCE Loss: 1.0227311849594116\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.157405853271484 | KNN Loss: 3.1160974502563477 | BCE Loss: 1.0413081645965576\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.146901607513428 | KNN Loss: 3.1129212379455566 | BCE Loss: 1.0339802503585815\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.144957065582275 | KNN Loss: 3.120715379714966 | BCE Loss: 1.0242418050765991\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.137362480163574 | KNN Loss: 3.107322931289673 | BCE Loss: 1.030039668083191\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.131994247436523 | KNN Loss: 3.099816083908081 | BCE Loss: 1.0321784019470215\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.148634910583496 | KNN Loss: 3.1135003566741943 | BCE Loss: 1.0351346731185913\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.195155143737793 | KNN Loss: 3.154025077819824 | BCE Loss: 1.0411303043365479\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.119049072265625 | KNN Loss: 3.0980162620544434 | BCE Loss: 1.0210330486297607\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.143043041229248 | KNN Loss: 3.105879545211792 | BCE Loss: 1.0371633768081665\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.073680400848389 | KNN Loss: 3.0756027698516846 | BCE Loss: 0.9980774521827698\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.080066680908203 | KNN Loss: 3.070650339126587 | BCE Loss: 1.0094165802001953\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.192843914031982 | KNN Loss: 3.1005971431732178 | BCE Loss: 1.0922467708587646\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.138883113861084 | KNN Loss: 3.1190481185913086 | BCE Loss: 1.0198349952697754\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.190145015716553 | KNN Loss: 3.133105754852295 | BCE Loss: 1.0570392608642578\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.158684253692627 | KNN Loss: 3.1279830932617188 | BCE Loss: 1.0307011604309082\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.149196624755859 | KNN Loss: 3.1193480491638184 | BCE Loss: 1.029848337173462\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.164491176605225 | KNN Loss: 3.1372804641723633 | BCE Loss: 1.0272105932235718\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.1583757400512695 | KNN Loss: 3.1267242431640625 | BCE Loss: 1.031651496887207\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.15795373916626 | KNN Loss: 3.127826452255249 | BCE Loss: 1.0301271677017212\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.172698020935059 | KNN Loss: 3.1368329524993896 | BCE Loss: 1.035865068435669\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.145971298217773 | KNN Loss: 3.125844955444336 | BCE Loss: 1.020126461982727\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.117735862731934 | KNN Loss: 3.0958309173583984 | BCE Loss: 1.021904706954956\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.152495384216309 | KNN Loss: 3.108079195022583 | BCE Loss: 1.044416069984436\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.13289213180542 | KNN Loss: 3.0997166633605957 | BCE Loss: 1.0331753492355347\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.139163017272949 | KNN Loss: 3.0859568119049072 | BCE Loss: 1.053205966949463\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.1417694091796875 | KNN Loss: 3.1274681091308594 | BCE Loss: 1.0143015384674072\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.143898010253906 | KNN Loss: 3.0974323749542236 | BCE Loss: 1.0464656352996826\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.071857452392578 | KNN Loss: 3.047417402267456 | BCE Loss: 1.024440050125122\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.108194828033447 | KNN Loss: 3.090566873550415 | BCE Loss: 1.0176280736923218\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.158309459686279 | KNN Loss: 3.11712908744812 | BCE Loss: 1.0411803722381592\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.17568302154541 | KNN Loss: 3.122288465499878 | BCE Loss: 1.0533945560455322\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.1334052085876465 | KNN Loss: 3.0911097526550293 | BCE Loss: 1.0422954559326172\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.1608123779296875 | KNN Loss: 3.1098687648773193 | BCE Loss: 1.0509436130523682\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.129583358764648 | KNN Loss: 3.111938953399658 | BCE Loss: 1.0176446437835693\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.149435997009277 | KNN Loss: 3.1324679851531982 | BCE Loss: 1.016968011856079\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.129530906677246 | KNN Loss: 3.1298112869262695 | BCE Loss: 0.9997196197509766\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.136309623718262 | KNN Loss: 3.109976053237915 | BCE Loss: 1.0263338088989258\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.117500305175781 | KNN Loss: 3.1031856536865234 | BCE Loss: 1.014314889907837\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.142777442932129 | KNN Loss: 3.1450836658477783 | BCE Loss: 0.9976937770843506\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.107481002807617 | KNN Loss: 3.105062246322632 | BCE Loss: 1.002418875694275\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.100986480712891 | KNN Loss: 3.0913968086242676 | BCE Loss: 1.009589672088623\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.14040470123291 | KNN Loss: 3.1201016902923584 | BCE Loss: 1.0203030109405518\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.118449687957764 | KNN Loss: 3.0856666564941406 | BCE Loss: 1.032783031463623\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.131448745727539 | KNN Loss: 3.092679023742676 | BCE Loss: 1.0387697219848633\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.122927665710449 | KNN Loss: 3.0903515815734863 | BCE Loss: 1.0325758457183838\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.143276214599609 | KNN Loss: 3.1132192611694336 | BCE Loss: 1.0300571918487549\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.192075729370117 | KNN Loss: 3.1303086280822754 | BCE Loss: 1.0617671012878418\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.180001735687256 | KNN Loss: 3.13187575340271 | BCE Loss: 1.048125982284546\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.122845649719238 | KNN Loss: 3.1118695735931396 | BCE Loss: 1.0109760761260986\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.107250213623047 | KNN Loss: 3.0571916103363037 | BCE Loss: 1.0500588417053223\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.095990180969238 | KNN Loss: 3.0802509784698486 | BCE Loss: 1.0157394409179688\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.070903301239014 | KNN Loss: 3.0502607822418213 | BCE Loss: 1.0206425189971924\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.083768367767334 | KNN Loss: 3.0519914627075195 | BCE Loss: 1.0317769050598145\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.154996871948242 | KNN Loss: 3.124300956726074 | BCE Loss: 1.030696153640747\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.133162975311279 | KNN Loss: 3.1070878505706787 | BCE Loss: 1.0260752439498901\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.11175537109375 | KNN Loss: 3.0848724842071533 | BCE Loss: 1.0268831253051758\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.1198906898498535 | KNN Loss: 3.089144468307495 | BCE Loss: 1.0307462215423584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.137509822845459 | KNN Loss: 3.108593225479126 | BCE Loss: 1.0289167165756226\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.164376258850098 | KNN Loss: 3.126413345336914 | BCE Loss: 1.0379631519317627\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.120737075805664 | KNN Loss: 3.098545789718628 | BCE Loss: 1.022191047668457\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.113468170166016 | KNN Loss: 3.087542772293091 | BCE Loss: 1.0259253978729248\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.144238471984863 | KNN Loss: 3.13447642326355 | BCE Loss: 1.0097622871398926\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.091134548187256 | KNN Loss: 3.071777105331421 | BCE Loss: 1.0193575620651245\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.081925392150879 | KNN Loss: 3.073878049850464 | BCE Loss: 1.0080475807189941\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.124319076538086 | KNN Loss: 3.097728967666626 | BCE Loss: 1.0265898704528809\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.143649101257324 | KNN Loss: 3.1191792488098145 | BCE Loss: 1.0244696140289307\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.132167816162109 | KNN Loss: 3.1180319786071777 | BCE Loss: 1.0141360759735107\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.13278865814209 | KNN Loss: 3.1167409420013428 | BCE Loss: 1.016047716140747\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.136252403259277 | KNN Loss: 3.1004483699798584 | BCE Loss: 1.035804271697998\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.151314735412598 | KNN Loss: 3.1131319999694824 | BCE Loss: 1.0381826162338257\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.115961074829102 | KNN Loss: 3.0855627059936523 | BCE Loss: 1.0303986072540283\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.144674301147461 | KNN Loss: 3.0969560146331787 | BCE Loss: 1.0477185249328613\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.1543378829956055 | KNN Loss: 3.1212503910064697 | BCE Loss: 1.0330874919891357\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.108606815338135 | KNN Loss: 3.1161301136016846 | BCE Loss: 0.9924766421318054\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.118979454040527 | KNN Loss: 3.1011016368865967 | BCE Loss: 1.0178778171539307\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.151270389556885 | KNN Loss: 3.1126768589019775 | BCE Loss: 1.0385935306549072\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.174152374267578 | KNN Loss: 3.1373183727264404 | BCE Loss: 1.0368337631225586\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.1777496337890625 | KNN Loss: 3.1377296447753906 | BCE Loss: 1.0400199890136719\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.156885147094727 | KNN Loss: 3.120696544647217 | BCE Loss: 1.0361888408660889\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.137129783630371 | KNN Loss: 3.1041159629821777 | BCE Loss: 1.0330135822296143\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.169248104095459 | KNN Loss: 3.133410930633545 | BCE Loss: 1.0358370542526245\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.144585609436035 | KNN Loss: 3.123997449874878 | BCE Loss: 1.0205880403518677\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.165526390075684 | KNN Loss: 3.146148681640625 | BCE Loss: 1.0193779468536377\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.154815673828125 | KNN Loss: 3.1490039825439453 | BCE Loss: 1.0058116912841797\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.191837787628174 | KNN Loss: 3.131925344467163 | BCE Loss: 1.0599123239517212\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.149249076843262 | KNN Loss: 3.1176376342773438 | BCE Loss: 1.0316115617752075\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.141117095947266 | KNN Loss: 3.098510265350342 | BCE Loss: 1.0426065921783447\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.134171962738037 | KNN Loss: 3.105534076690674 | BCE Loss: 1.0286378860473633\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.1720733642578125 | KNN Loss: 3.140441417694092 | BCE Loss: 1.0316319465637207\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.156922340393066 | KNN Loss: 3.114097833633423 | BCE Loss: 1.042824387550354\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.159074783325195 | KNN Loss: 3.135277509689331 | BCE Loss: 1.0237975120544434\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.190973281860352 | KNN Loss: 3.1302857398986816 | BCE Loss: 1.0606876611709595\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.130208969116211 | KNN Loss: 3.105170965194702 | BCE Loss: 1.025038242340088\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.149310111999512 | KNN Loss: 3.1159744262695312 | BCE Loss: 1.0333354473114014\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.069357872009277 | KNN Loss: 3.0732595920562744 | BCE Loss: 0.9960983991622925\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.136607646942139 | KNN Loss: 3.118335247039795 | BCE Loss: 1.0182723999023438\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.122398853302002 | KNN Loss: 3.1163837909698486 | BCE Loss: 1.0060150623321533\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.123782157897949 | KNN Loss: 3.10408091545105 | BCE Loss: 1.0197014808654785\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.153899669647217 | KNN Loss: 3.1481730937957764 | BCE Loss: 1.0057264566421509\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.128734588623047 | KNN Loss: 3.0942022800445557 | BCE Loss: 1.034532070159912\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.128493309020996 | KNN Loss: 3.1206698417663574 | BCE Loss: 1.0078234672546387\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.143779754638672 | KNN Loss: 3.1047041416168213 | BCE Loss: 1.0390753746032715\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.140971660614014 | KNN Loss: 3.1321072578430176 | BCE Loss: 1.008864402770996\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.111244201660156 | KNN Loss: 3.081556797027588 | BCE Loss: 1.029687523841858\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.106666564941406 | KNN Loss: 3.0709338188171387 | BCE Loss: 1.0357328653335571\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.100071907043457 | KNN Loss: 3.0606846809387207 | BCE Loss: 1.0393872261047363\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.1507368087768555 | KNN Loss: 3.1230146884918213 | BCE Loss: 1.0277220010757446\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.139266014099121 | KNN Loss: 3.137152910232544 | BCE Loss: 1.002112865447998\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.1358489990234375 | KNN Loss: 3.106748104095459 | BCE Loss: 1.0291011333465576\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.1787285804748535 | KNN Loss: 3.128899097442627 | BCE Loss: 1.0498294830322266\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.132641792297363 | KNN Loss: 3.1243717670440674 | BCE Loss: 1.008270025253296\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.128856182098389 | KNN Loss: 3.1035726070404053 | BCE Loss: 1.0252835750579834\n",
      "Epoch    74: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.158502101898193 | KNN Loss: 3.092256546020508 | BCE Loss: 1.0662455558776855\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.1512603759765625 | KNN Loss: 3.1242363452911377 | BCE Loss: 1.0270241498947144\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.1448469161987305 | KNN Loss: 3.103159189224243 | BCE Loss: 1.0416874885559082\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.117711544036865 | KNN Loss: 3.0907936096191406 | BCE Loss: 1.0269179344177246\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.118104934692383 | KNN Loss: 3.093489646911621 | BCE Loss: 1.0246154069900513\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.081051826477051 | KNN Loss: 3.1027393341064453 | BCE Loss: 0.9783124923706055\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.097592353820801 | KNN Loss: 3.0870096683502197 | BCE Loss: 1.0105829238891602\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.146601676940918 | KNN Loss: 3.1170220375061035 | BCE Loss: 1.0295794010162354\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.113105773925781 | KNN Loss: 3.0896637439727783 | BCE Loss: 1.023442268371582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.102123260498047 | KNN Loss: 3.0964179039001465 | BCE Loss: 1.0057055950164795\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.149783134460449 | KNN Loss: 3.1033875942230225 | BCE Loss: 1.0463953018188477\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.123112201690674 | KNN Loss: 3.090872287750244 | BCE Loss: 1.0322397947311401\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.051721096038818 | KNN Loss: 3.0492944717407227 | BCE Loss: 1.0024265050888062\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.124121189117432 | KNN Loss: 3.087127208709717 | BCE Loss: 1.0369940996170044\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.096490859985352 | KNN Loss: 3.0937042236328125 | BCE Loss: 1.00278639793396\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.122089385986328 | KNN Loss: 3.1060736179351807 | BCE Loss: 1.0160157680511475\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.123082160949707 | KNN Loss: 3.0880813598632812 | BCE Loss: 1.0350008010864258\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.103974342346191 | KNN Loss: 3.0646204948425293 | BCE Loss: 1.039353847503662\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.095821857452393 | KNN Loss: 3.0794692039489746 | BCE Loss: 1.0163527727127075\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.103253364562988 | KNN Loss: 3.0816144943237305 | BCE Loss: 1.0216387510299683\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.124342918395996 | KNN Loss: 3.0858378410339355 | BCE Loss: 1.0385050773620605\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.126609802246094 | KNN Loss: 3.0912952423095703 | BCE Loss: 1.0353143215179443\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.085629940032959 | KNN Loss: 3.087252140045166 | BCE Loss: 0.9983776807785034\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.142917633056641 | KNN Loss: 3.1099276542663574 | BCE Loss: 1.0329899787902832\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.1007537841796875 | KNN Loss: 3.0992813110351562 | BCE Loss: 1.0014724731445312\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.162797451019287 | KNN Loss: 3.1047751903533936 | BCE Loss: 1.058022379875183\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.1273603439331055 | KNN Loss: 3.122131109237671 | BCE Loss: 1.0052294731140137\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.184603214263916 | KNN Loss: 3.102769374847412 | BCE Loss: 1.0818337202072144\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.176013946533203 | KNN Loss: 3.1652462482452393 | BCE Loss: 1.0107674598693848\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.137016773223877 | KNN Loss: 3.100346803665161 | BCE Loss: 1.0366699695587158\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.158911228179932 | KNN Loss: 3.1161599159240723 | BCE Loss: 1.0427513122558594\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.128532409667969 | KNN Loss: 3.114837646484375 | BCE Loss: 1.0136947631835938\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.163455009460449 | KNN Loss: 3.142642021179199 | BCE Loss: 1.0208131074905396\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.103723049163818 | KNN Loss: 3.0808358192443848 | BCE Loss: 1.0228872299194336\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.160569667816162 | KNN Loss: 3.115204334259033 | BCE Loss: 1.0453654527664185\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.090198516845703 | KNN Loss: 3.096388578414917 | BCE Loss: 0.9938097596168518\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.093625068664551 | KNN Loss: 3.0640594959259033 | BCE Loss: 1.0295653343200684\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.104507923126221 | KNN Loss: 3.0726757049560547 | BCE Loss: 1.031832218170166\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.114771842956543 | KNN Loss: 3.092468023300171 | BCE Loss: 1.0223040580749512\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.120003700256348 | KNN Loss: 3.1109955310821533 | BCE Loss: 1.0090079307556152\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.10627555847168 | KNN Loss: 3.0638649463653564 | BCE Loss: 1.0424104928970337\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.0896806716918945 | KNN Loss: 3.097855567932129 | BCE Loss: 0.9918249845504761\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.099748611450195 | KNN Loss: 3.096083402633667 | BCE Loss: 1.0036649703979492\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.135353088378906 | KNN Loss: 3.0988922119140625 | BCE Loss: 1.0364606380462646\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.11961555480957 | KNN Loss: 3.0950863361358643 | BCE Loss: 1.0245293378829956\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.074563026428223 | KNN Loss: 3.0845162868499756 | BCE Loss: 0.9900467991828918\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.092145919799805 | KNN Loss: 3.0703530311584473 | BCE Loss: 1.0217928886413574\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.089400291442871 | KNN Loss: 3.0843193531036377 | BCE Loss: 1.0050811767578125\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.142244338989258 | KNN Loss: 3.110764503479004 | BCE Loss: 1.031480073928833\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.095857620239258 | KNN Loss: 3.08296537399292 | BCE Loss: 1.012892484664917\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.1580634117126465 | KNN Loss: 3.10440993309021 | BCE Loss: 1.0536534786224365\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.134076118469238 | KNN Loss: 3.102922201156616 | BCE Loss: 1.031153678894043\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.110339641571045 | KNN Loss: 3.087455987930298 | BCE Loss: 1.0228835344314575\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.120077610015869 | KNN Loss: 3.086148738861084 | BCE Loss: 1.0339287519454956\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.108351230621338 | KNN Loss: 3.100466251373291 | BCE Loss: 1.0078849792480469\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.101712226867676 | KNN Loss: 3.0679781436920166 | BCE Loss: 1.03373384475708\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.068164825439453 | KNN Loss: 3.0764219760894775 | BCE Loss: 0.9917429685592651\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.111083030700684 | KNN Loss: 3.089686155319214 | BCE Loss: 1.0213967561721802\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.052800178527832 | KNN Loss: 3.040947198867798 | BCE Loss: 1.0118532180786133\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.088232040405273 | KNN Loss: 3.069114923477173 | BCE Loss: 1.0191171169281006\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.12666654586792 | KNN Loss: 3.095824718475342 | BCE Loss: 1.0308418273925781\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.11685848236084 | KNN Loss: 3.1026973724365234 | BCE Loss: 1.0141613483428955\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.176601886749268 | KNN Loss: 3.113511800765991 | BCE Loss: 1.0630899667739868\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.1201653480529785 | KNN Loss: 3.1004638671875 | BCE Loss: 1.019701361656189\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.097829818725586 | KNN Loss: 3.0663697719573975 | BCE Loss: 1.0314598083496094\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.127137184143066 | KNN Loss: 3.0892350673675537 | BCE Loss: 1.0379023551940918\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.098932266235352 | KNN Loss: 3.09155011177063 | BCE Loss: 1.0073823928833008\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.177289009094238 | KNN Loss: 3.120025634765625 | BCE Loss: 1.0572636127471924\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.093606472015381 | KNN Loss: 3.0910377502441406 | BCE Loss: 1.0025687217712402\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.117326259613037 | KNN Loss: 3.080665349960327 | BCE Loss: 1.0366607904434204\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.051578044891357 | KNN Loss: 3.039597749710083 | BCE Loss: 1.0119802951812744\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.125338554382324 | KNN Loss: 3.0844106674194336 | BCE Loss: 1.0409276485443115\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.141112804412842 | KNN Loss: 3.1154332160949707 | BCE Loss: 1.0256797075271606\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.158564567565918 | KNN Loss: 3.1126272678375244 | BCE Loss: 1.0459375381469727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.119910717010498 | KNN Loss: 3.100619316101074 | BCE Loss: 1.0192914009094238\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.1505584716796875 | KNN Loss: 3.093820095062256 | BCE Loss: 1.0567383766174316\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.106929302215576 | KNN Loss: 3.075411081314087 | BCE Loss: 1.0315182209014893\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.141281604766846 | KNN Loss: 3.1092159748077393 | BCE Loss: 1.0320656299591064\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.099991798400879 | KNN Loss: 3.063936233520508 | BCE Loss: 1.0360554456710815\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.088787078857422 | KNN Loss: 3.0723607540130615 | BCE Loss: 1.0164262056350708\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.074583530426025 | KNN Loss: 3.0688979625701904 | BCE Loss: 1.005685567855835\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.162245750427246 | KNN Loss: 3.1315207481384277 | BCE Loss: 1.0307247638702393\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.1312408447265625 | KNN Loss: 3.0830414295196533 | BCE Loss: 1.0481994152069092\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.132962226867676 | KNN Loss: 3.123339891433716 | BCE Loss: 1.009622573852539\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.092127799987793 | KNN Loss: 3.085054397583008 | BCE Loss: 1.0070734024047852\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.148952960968018 | KNN Loss: 3.086857795715332 | BCE Loss: 1.0620951652526855\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.118488311767578 | KNN Loss: 3.1057591438293457 | BCE Loss: 1.0127291679382324\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.120517253875732 | KNN Loss: 3.0765488147735596 | BCE Loss: 1.0439683198928833\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.112078666687012 | KNN Loss: 3.089459180831909 | BCE Loss: 1.0226194858551025\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.146546840667725 | KNN Loss: 3.1080095767974854 | BCE Loss: 1.0385371446609497\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.120579242706299 | KNN Loss: 3.0825347900390625 | BCE Loss: 1.0380444526672363\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.153199672698975 | KNN Loss: 3.117367744445801 | BCE Loss: 1.0358319282531738\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.118685722351074 | KNN Loss: 3.0737876892089844 | BCE Loss: 1.044898271560669\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.110958576202393 | KNN Loss: 3.057297706604004 | BCE Loss: 1.0536607503890991\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.1169023513793945 | KNN Loss: 3.1054773330688477 | BCE Loss: 1.0114247798919678\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.131648063659668 | KNN Loss: 3.096956968307495 | BCE Loss: 1.034691333770752\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.14006233215332 | KNN Loss: 3.080937147140503 | BCE Loss: 1.0591251850128174\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.154369831085205 | KNN Loss: 3.1258695125579834 | BCE Loss: 1.0285003185272217\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.124295234680176 | KNN Loss: 3.1055781841278076 | BCE Loss: 1.0187170505523682\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.085235595703125 | KNN Loss: 3.0758345127105713 | BCE Loss: 1.0094010829925537\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.103768825531006 | KNN Loss: 3.0819857120513916 | BCE Loss: 1.0217832326889038\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.105886459350586 | KNN Loss: 3.072270631790161 | BCE Loss: 1.033616065979004\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.085078239440918 | KNN Loss: 3.063952922821045 | BCE Loss: 1.021125078201294\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.097509860992432 | KNN Loss: 3.0816991329193115 | BCE Loss: 1.0158107280731201\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.140386581420898 | KNN Loss: 3.103037118911743 | BCE Loss: 1.0373494625091553\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.126070022583008 | KNN Loss: 3.0892457962036133 | BCE Loss: 1.0368242263793945\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.111466884613037 | KNN Loss: 3.084359884262085 | BCE Loss: 1.0271070003509521\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.13220739364624 | KNN Loss: 3.107724666595459 | BCE Loss: 1.0244828462600708\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.129210472106934 | KNN Loss: 3.094529628753662 | BCE Loss: 1.034680962562561\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.120594501495361 | KNN Loss: 3.105891466140747 | BCE Loss: 1.0147030353546143\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.134356498718262 | KNN Loss: 3.0920257568359375 | BCE Loss: 1.0423309803009033\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.149565696716309 | KNN Loss: 3.1125104427337646 | BCE Loss: 1.0370550155639648\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.123703956604004 | KNN Loss: 3.107494592666626 | BCE Loss: 1.0162091255187988\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.101276397705078 | KNN Loss: 3.095647096633911 | BCE Loss: 1.005629539489746\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.0956315994262695 | KNN Loss: 3.0606935024261475 | BCE Loss: 1.034937858581543\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.131093502044678 | KNN Loss: 3.0876686573028564 | BCE Loss: 1.0434247255325317\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.105027675628662 | KNN Loss: 3.110377788543701 | BCE Loss: 0.9946500658988953\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.164697647094727 | KNN Loss: 3.1038620471954346 | BCE Loss: 1.060835838317871\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.124919891357422 | KNN Loss: 3.0971128940582275 | BCE Loss: 1.0278071165084839\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.052656173706055 | KNN Loss: 3.052051544189453 | BCE Loss: 1.000604510307312\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.113971710205078 | KNN Loss: 3.098696708679199 | BCE Loss: 1.0152747631072998\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.102862358093262 | KNN Loss: 3.086113452911377 | BCE Loss: 1.0167491436004639\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.109085559844971 | KNN Loss: 3.0891129970550537 | BCE Loss: 1.019972562789917\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.09639835357666 | KNN Loss: 3.0695157051086426 | BCE Loss: 1.0268824100494385\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.089839458465576 | KNN Loss: 3.076950788497925 | BCE Loss: 1.012888789176941\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.130417346954346 | KNN Loss: 3.108264684677124 | BCE Loss: 1.0221526622772217\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.096631050109863 | KNN Loss: 3.061042308807373 | BCE Loss: 1.0355887413024902\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.153347492218018 | KNN Loss: 3.11309814453125 | BCE Loss: 1.0402493476867676\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.1694746017456055 | KNN Loss: 3.102494239807129 | BCE Loss: 1.0669801235198975\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.1537322998046875 | KNN Loss: 3.125426769256592 | BCE Loss: 1.0283057689666748\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.128975868225098 | KNN Loss: 3.088014602661133 | BCE Loss: 1.0409610271453857\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.124238014221191 | KNN Loss: 3.084644079208374 | BCE Loss: 1.0395941734313965\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.10477352142334 | KNN Loss: 3.0976734161376953 | BCE Loss: 1.0070998668670654\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.143399715423584 | KNN Loss: 3.1200263500213623 | BCE Loss: 1.0233733654022217\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.1674652099609375 | KNN Loss: 3.119375228881836 | BCE Loss: 1.0480902194976807\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.106739044189453 | KNN Loss: 3.080322504043579 | BCE Loss: 1.0264167785644531\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.130428791046143 | KNN Loss: 3.0840904712677 | BCE Loss: 1.0463382005691528\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.121459484100342 | KNN Loss: 3.111297845840454 | BCE Loss: 1.0101617574691772\n",
      "Epoch    97: reducing learning rate of group 0 to 1.7150e-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.122441291809082 | KNN Loss: 3.077540397644043 | BCE Loss: 1.0449007749557495\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.1388678550720215 | KNN Loss: 3.0992136001586914 | BCE Loss: 1.0396541357040405\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.140501976013184 | KNN Loss: 3.0726561546325684 | BCE Loss: 1.0678460597991943\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.123679161071777 | KNN Loss: 3.110231637954712 | BCE Loss: 1.0134477615356445\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.1192121505737305 | KNN Loss: 3.0950584411621094 | BCE Loss: 1.0241539478302002\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.102678298950195 | KNN Loss: 3.0854270458221436 | BCE Loss: 1.0172513723373413\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.08939266204834 | KNN Loss: 3.084026575088501 | BCE Loss: 1.0053658485412598\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.1371002197265625 | KNN Loss: 3.089794635772705 | BCE Loss: 1.0473055839538574\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.105494499206543 | KNN Loss: 3.072256088256836 | BCE Loss: 1.0332386493682861\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.07027006149292 | KNN Loss: 3.0449059009552 | BCE Loss: 1.0253641605377197\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.14638614654541 | KNN Loss: 3.124812126159668 | BCE Loss: 1.021573781967163\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.064767837524414 | KNN Loss: 3.0573205947875977 | BCE Loss: 1.0074470043182373\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.128354072570801 | KNN Loss: 3.0868659019470215 | BCE Loss: 1.0414879322052002\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.147272109985352 | KNN Loss: 3.1128337383270264 | BCE Loss: 1.0344384908676147\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.1293745040893555 | KNN Loss: 3.101181745529175 | BCE Loss: 1.0281925201416016\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.101380348205566 | KNN Loss: 3.079540967941284 | BCE Loss: 1.0218392610549927\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.102046966552734 | KNN Loss: 3.0594165325164795 | BCE Loss: 1.0426303148269653\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.096280097961426 | KNN Loss: 3.0885376930236816 | BCE Loss: 1.0077426433563232\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.082056999206543 | KNN Loss: 3.057452917098999 | BCE Loss: 1.0246038436889648\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.094169616699219 | KNN Loss: 3.0863354206085205 | BCE Loss: 1.0078344345092773\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.1181535720825195 | KNN Loss: 3.09132719039917 | BCE Loss: 1.02682626247406\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.11027193069458 | KNN Loss: 3.096409320831299 | BCE Loss: 1.0138626098632812\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.105339050292969 | KNN Loss: 3.1024258136749268 | BCE Loss: 1.002913236618042\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.110118389129639 | KNN Loss: 3.081007480621338 | BCE Loss: 1.0291107892990112\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.05274772644043 | KNN Loss: 3.058762550354004 | BCE Loss: 0.9939852356910706\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.141414642333984 | KNN Loss: 3.123298168182373 | BCE Loss: 1.0181164741516113\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.086043357849121 | KNN Loss: 3.082451820373535 | BCE Loss: 1.003591775894165\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.129201889038086 | KNN Loss: 3.1105966567993164 | BCE Loss: 1.01860511302948\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.131880760192871 | KNN Loss: 3.0996909141540527 | BCE Loss: 1.0321896076202393\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.107439041137695 | KNN Loss: 3.0732507705688477 | BCE Loss: 1.0341882705688477\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.09851598739624 | KNN Loss: 3.0815281867980957 | BCE Loss: 1.0169878005981445\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.091981410980225 | KNN Loss: 3.0838513374328613 | BCE Loss: 1.0081301927566528\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.094681262969971 | KNN Loss: 3.0783767700195312 | BCE Loss: 1.01630437374115\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.195849418640137 | KNN Loss: 3.1605417728424072 | BCE Loss: 1.0353076457977295\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.1083478927612305 | KNN Loss: 3.0873842239379883 | BCE Loss: 1.0209636688232422\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.174918174743652 | KNN Loss: 3.1203696727752686 | BCE Loss: 1.0545482635498047\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.121647834777832 | KNN Loss: 3.0609796047210693 | BCE Loss: 1.0606682300567627\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.091362476348877 | KNN Loss: 3.0672948360443115 | BCE Loss: 1.0240676403045654\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.129772186279297 | KNN Loss: 3.097581386566162 | BCE Loss: 1.0321905612945557\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.133935928344727 | KNN Loss: 3.1036412715911865 | BCE Loss: 1.0302945375442505\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.119375228881836 | KNN Loss: 3.086780309677124 | BCE Loss: 1.0325947999954224\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.071567058563232 | KNN Loss: 3.0618441104888916 | BCE Loss: 1.0097229480743408\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.138737678527832 | KNN Loss: 3.081402540206909 | BCE Loss: 1.0573351383209229\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.127717018127441 | KNN Loss: 3.081096887588501 | BCE Loss: 1.0466203689575195\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.127259254455566 | KNN Loss: 3.101515054702759 | BCE Loss: 1.0257443189620972\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.139516830444336 | KNN Loss: 3.1248342990875244 | BCE Loss: 1.0146822929382324\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.090166091918945 | KNN Loss: 3.045816659927368 | BCE Loss: 1.0443494319915771\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.085984230041504 | KNN Loss: 3.056969165802002 | BCE Loss: 1.0290148258209229\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.157098770141602 | KNN Loss: 3.104201555252075 | BCE Loss: 1.0528972148895264\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.160557746887207 | KNN Loss: 3.1076109409332275 | BCE Loss: 1.0529465675354004\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.115057945251465 | KNN Loss: 3.094219446182251 | BCE Loss: 1.0208384990692139\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.062792778015137 | KNN Loss: 3.053338050842285 | BCE Loss: 1.0094548463821411\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.099457740783691 | KNN Loss: 3.091320276260376 | BCE Loss: 1.0081373453140259\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.111481189727783 | KNN Loss: 3.085334539413452 | BCE Loss: 1.0261467695236206\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.172619342803955 | KNN Loss: 3.1031579971313477 | BCE Loss: 1.0694613456726074\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.107542991638184 | KNN Loss: 3.079697608947754 | BCE Loss: 1.0278452634811401\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.137635231018066 | KNN Loss: 3.1046760082244873 | BCE Loss: 1.0329594612121582\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.086180210113525 | KNN Loss: 3.063880443572998 | BCE Loss: 1.0222996473312378\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.132149696350098 | KNN Loss: 3.0872535705566406 | BCE Loss: 1.044895887374878\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.090677261352539 | KNN Loss: 3.0685625076293945 | BCE Loss: 1.0221145153045654\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.0517683029174805 | KNN Loss: 3.0434868335723877 | BCE Loss: 1.0082812309265137\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.052945137023926 | KNN Loss: 3.055708169937134 | BCE Loss: 0.9972371459007263\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.107062816619873 | KNN Loss: 3.0808048248291016 | BCE Loss: 1.026257872581482\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.114968299865723 | KNN Loss: 3.075749635696411 | BCE Loss: 1.0392189025878906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.1084113121032715 | KNN Loss: 3.070949077606201 | BCE Loss: 1.0374621152877808\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.055578708648682 | KNN Loss: 3.0447654724121094 | BCE Loss: 1.0108132362365723\n",
      "Epoch   108: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.096551895141602 | KNN Loss: 3.067798614501953 | BCE Loss: 1.0287530422210693\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.086984157562256 | KNN Loss: 3.0695960521698 | BCE Loss: 1.017388105392456\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.115662574768066 | KNN Loss: 3.079214572906494 | BCE Loss: 1.0364482402801514\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.149948596954346 | KNN Loss: 3.109612226486206 | BCE Loss: 1.0403363704681396\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.130278587341309 | KNN Loss: 3.078092098236084 | BCE Loss: 1.052186369895935\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.066539764404297 | KNN Loss: 3.065347909927368 | BCE Loss: 1.0011920928955078\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.113208293914795 | KNN Loss: 3.1117899417877197 | BCE Loss: 1.0014183521270752\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.084552764892578 | KNN Loss: 3.070420026779175 | BCE Loss: 1.0141326189041138\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.1154022216796875 | KNN Loss: 3.099289894104004 | BCE Loss: 1.0161125659942627\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.075878143310547 | KNN Loss: 3.0650522708892822 | BCE Loss: 1.0108261108398438\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.045741081237793 | KNN Loss: 3.035933256149292 | BCE Loss: 1.009807825088501\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.11792516708374 | KNN Loss: 3.078669309616089 | BCE Loss: 1.0392558574676514\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.073472023010254 | KNN Loss: 3.0419347286224365 | BCE Loss: 1.0315372943878174\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.1020917892456055 | KNN Loss: 3.047841787338257 | BCE Loss: 1.0542497634887695\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.067752361297607 | KNN Loss: 3.054194927215576 | BCE Loss: 1.0135573148727417\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.146148204803467 | KNN Loss: 3.0849626064300537 | BCE Loss: 1.061185598373413\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.115507125854492 | KNN Loss: 3.088188886642456 | BCE Loss: 1.0273182392120361\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.055192470550537 | KNN Loss: 3.063354015350342 | BCE Loss: 0.9918384552001953\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.113010406494141 | KNN Loss: 3.1007511615753174 | BCE Loss: 1.0122590065002441\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.104269027709961 | KNN Loss: 3.092730760574341 | BCE Loss: 1.011538028717041\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.099792957305908 | KNN Loss: 3.0779218673706055 | BCE Loss: 1.0218709707260132\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.132079124450684 | KNN Loss: 3.0950703620910645 | BCE Loss: 1.0370090007781982\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.132251739501953 | KNN Loss: 3.0721538066864014 | BCE Loss: 1.0600979328155518\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.085329055786133 | KNN Loss: 3.0481815338134766 | BCE Loss: 1.0371475219726562\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.110391616821289 | KNN Loss: 3.069505214691162 | BCE Loss: 1.040886402130127\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.106279373168945 | KNN Loss: 3.087764024734497 | BCE Loss: 1.0185151100158691\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.077109336853027 | KNN Loss: 3.0726237297058105 | BCE Loss: 1.0044853687286377\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.146578311920166 | KNN Loss: 3.117851495742798 | BCE Loss: 1.0287269353866577\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.097535133361816 | KNN Loss: 3.096660614013672 | BCE Loss: 1.0008745193481445\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.059734344482422 | KNN Loss: 3.055116653442383 | BCE Loss: 1.00461745262146\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.1178178787231445 | KNN Loss: 3.096946954727173 | BCE Loss: 1.0208711624145508\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.131433486938477 | KNN Loss: 3.0753161907196045 | BCE Loss: 1.0561175346374512\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.084010601043701 | KNN Loss: 3.0640923976898193 | BCE Loss: 1.0199182033538818\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.074682235717773 | KNN Loss: 3.0659430027008057 | BCE Loss: 1.0087389945983887\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.131007671356201 | KNN Loss: 3.1142795085906982 | BCE Loss: 1.016728162765503\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.082863807678223 | KNN Loss: 3.0534508228302 | BCE Loss: 1.0294129848480225\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.061884880065918 | KNN Loss: 3.040090322494507 | BCE Loss: 1.0217947959899902\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.142740249633789 | KNN Loss: 3.0933938026428223 | BCE Loss: 1.0493462085723877\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.077399253845215 | KNN Loss: 3.0679337978363037 | BCE Loss: 1.009465217590332\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.1129560470581055 | KNN Loss: 3.0945727825164795 | BCE Loss: 1.0183830261230469\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.134713649749756 | KNN Loss: 3.1012496948242188 | BCE Loss: 1.033463954925537\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.076503753662109 | KNN Loss: 3.0520830154418945 | BCE Loss: 1.0244208574295044\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.131397247314453 | KNN Loss: 3.0920584201812744 | BCE Loss: 1.0393388271331787\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.086792945861816 | KNN Loss: 3.0830347537994385 | BCE Loss: 1.0037579536437988\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.092803001403809 | KNN Loss: 3.079881191253662 | BCE Loss: 1.0129220485687256\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.075483798980713 | KNN Loss: 3.0644984245300293 | BCE Loss: 1.0109853744506836\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.117940902709961 | KNN Loss: 3.0745928287506104 | BCE Loss: 1.0433483123779297\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.157654762268066 | KNN Loss: 3.1275227069854736 | BCE Loss: 1.0301318168640137\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.119350433349609 | KNN Loss: 3.091240406036377 | BCE Loss: 1.0281100273132324\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.0790910720825195 | KNN Loss: 3.0573530197143555 | BCE Loss: 1.021737813949585\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.133177757263184 | KNN Loss: 3.1098806858062744 | BCE Loss: 1.0232973098754883\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.080037593841553 | KNN Loss: 3.0574564933776855 | BCE Loss: 1.0225812196731567\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.057607173919678 | KNN Loss: 3.0487961769104004 | BCE Loss: 1.0088109970092773\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.055981636047363 | KNN Loss: 3.036651611328125 | BCE Loss: 1.0193300247192383\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.096899032592773 | KNN Loss: 3.0611729621887207 | BCE Loss: 1.0357259511947632\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.115346908569336 | KNN Loss: 3.065767288208008 | BCE Loss: 1.0495797395706177\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.079715728759766 | KNN Loss: 3.0673184394836426 | BCE Loss: 1.0123971700668335\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.108541488647461 | KNN Loss: 3.093665838241577 | BCE Loss: 1.0148755311965942\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.08845329284668 | KNN Loss: 3.0516514778137207 | BCE Loss: 1.036802053451538\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.088860511779785 | KNN Loss: 3.056865930557251 | BCE Loss: 1.0319945812225342\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.131721496582031 | KNN Loss: 3.097074031829834 | BCE Loss: 1.0346477031707764\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.113888263702393 | KNN Loss: 3.0751988887786865 | BCE Loss: 1.038689374923706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.115325927734375 | KNN Loss: 3.0848610401153564 | BCE Loss: 1.0304651260375977\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.088039398193359 | KNN Loss: 3.0564053058624268 | BCE Loss: 1.0316338539123535\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.080898284912109 | KNN Loss: 3.0773017406463623 | BCE Loss: 1.003596544265747\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.082291603088379 | KNN Loss: 3.0610828399658203 | BCE Loss: 1.0212087631225586\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.174953460693359 | KNN Loss: 3.1253714561462402 | BCE Loss: 1.04958176612854\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.093533515930176 | KNN Loss: 3.0718834400177 | BCE Loss: 1.0216498374938965\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.093579292297363 | KNN Loss: 3.0596117973327637 | BCE Loss: 1.0339672565460205\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.072288990020752 | KNN Loss: 3.068572998046875 | BCE Loss: 1.003715991973877\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.107882499694824 | KNN Loss: 3.076575517654419 | BCE Loss: 1.0313072204589844\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.121020793914795 | KNN Loss: 3.0743656158447266 | BCE Loss: 1.0466551780700684\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.1230549812316895 | KNN Loss: 3.0897679328918457 | BCE Loss: 1.0332869291305542\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.124635696411133 | KNN Loss: 3.0740487575531006 | BCE Loss: 1.0505868196487427\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.112450122833252 | KNN Loss: 3.0648951530456543 | BCE Loss: 1.0475549697875977\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.0587382316589355 | KNN Loss: 3.046818494796753 | BCE Loss: 1.011919617652893\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.075638294219971 | KNN Loss: 3.0575785636901855 | BCE Loss: 1.0180597305297852\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.0427117347717285 | KNN Loss: 3.036186456680298 | BCE Loss: 1.0065252780914307\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.100528240203857 | KNN Loss: 3.060133695602417 | BCE Loss: 1.04039466381073\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.083779335021973 | KNN Loss: 3.060187578201294 | BCE Loss: 1.0235916376113892\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.122481346130371 | KNN Loss: 3.1087403297424316 | BCE Loss: 1.0137410163879395\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.134239196777344 | KNN Loss: 3.1003472805023193 | BCE Loss: 1.0338921546936035\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.133894920349121 | KNN Loss: 3.0627384185791016 | BCE Loss: 1.0711567401885986\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.066506862640381 | KNN Loss: 3.083590269088745 | BCE Loss: 0.9829165935516357\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.0876007080078125 | KNN Loss: 3.082123041152954 | BCE Loss: 1.0054779052734375\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.12941312789917 | KNN Loss: 3.105809211730957 | BCE Loss: 1.0236040353775024\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.122394561767578 | KNN Loss: 3.084599256515503 | BCE Loss: 1.0377954244613647\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.115904808044434 | KNN Loss: 3.090850591659546 | BCE Loss: 1.0250540971755981\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.139945983886719 | KNN Loss: 3.1169168949127197 | BCE Loss: 1.02302885055542\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.061226844787598 | KNN Loss: 3.057216167449951 | BCE Loss: 1.0040109157562256\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.126906394958496 | KNN Loss: 3.070059299468994 | BCE Loss: 1.056847333908081\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.11275053024292 | KNN Loss: 3.0824193954467773 | BCE Loss: 1.0303311347961426\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.096609115600586 | KNN Loss: 3.0645101070404053 | BCE Loss: 1.0320987701416016\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.112258434295654 | KNN Loss: 3.100044012069702 | BCE Loss: 1.0122145414352417\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.074923992156982 | KNN Loss: 3.0599751472473145 | BCE Loss: 1.0149489641189575\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.096235275268555 | KNN Loss: 3.07538104057312 | BCE Loss: 1.0208544731140137\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.093688488006592 | KNN Loss: 3.099506378173828 | BCE Loss: 0.9941819906234741\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.141968727111816 | KNN Loss: 3.0902342796325684 | BCE Loss: 1.051734447479248\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.077327728271484 | KNN Loss: 3.0528807640075684 | BCE Loss: 1.024446725845337\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.065792083740234 | KNN Loss: 3.041553258895874 | BCE Loss: 1.02423894405365\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.05134916305542 | KNN Loss: 3.0561211109161377 | BCE Loss: 0.9952282309532166\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.089459419250488 | KNN Loss: 3.080289840698242 | BCE Loss: 1.009169578552246\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.128466606140137 | KNN Loss: 3.10473895072937 | BCE Loss: 1.0237277746200562\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.112020492553711 | KNN Loss: 3.0749804973602295 | BCE Loss: 1.0370397567749023\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.114678859710693 | KNN Loss: 3.078425168991089 | BCE Loss: 1.036253809928894\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.0716552734375 | KNN Loss: 3.061819076538086 | BCE Loss: 1.009835958480835\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.103014945983887 | KNN Loss: 3.0729668140411377 | BCE Loss: 1.03004789352417\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.091615676879883 | KNN Loss: 3.0389389991760254 | BCE Loss: 1.0526766777038574\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.081167221069336 | KNN Loss: 3.0819406509399414 | BCE Loss: 0.9992265701293945\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.150749683380127 | KNN Loss: 3.09273099899292 | BCE Loss: 1.0580185651779175\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.078551292419434 | KNN Loss: 3.0613222122192383 | BCE Loss: 1.0172289609909058\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.0584397315979 | KNN Loss: 3.0451879501342773 | BCE Loss: 1.013251781463623\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.08609676361084 | KNN Loss: 3.058957099914551 | BCE Loss: 1.0271395444869995\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.085752010345459 | KNN Loss: 3.056886672973633 | BCE Loss: 1.0288653373718262\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.123626232147217 | KNN Loss: 3.0767197608947754 | BCE Loss: 1.0469064712524414\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.100736618041992 | KNN Loss: 3.0765278339385986 | BCE Loss: 1.0242085456848145\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.075126647949219 | KNN Loss: 3.0485212802886963 | BCE Loss: 1.0266051292419434\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.105908393859863 | KNN Loss: 3.0613017082214355 | BCE Loss: 1.0446066856384277\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.096998691558838 | KNN Loss: 3.0745456218719482 | BCE Loss: 1.0224530696868896\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.0563225746154785 | KNN Loss: 3.054124355316162 | BCE Loss: 1.0021982192993164\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.13254451751709 | KNN Loss: 3.077080249786377 | BCE Loss: 1.0554643869400024\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.084950923919678 | KNN Loss: 3.0590150356292725 | BCE Loss: 1.0259360074996948\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.120031356811523 | KNN Loss: 3.082247018814087 | BCE Loss: 1.0377840995788574\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.094239234924316 | KNN Loss: 3.072113275527954 | BCE Loss: 1.0221261978149414\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.0331268310546875 | KNN Loss: 3.029270887374878 | BCE Loss: 1.0038559436798096\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.137619972229004 | KNN Loss: 3.108100652694702 | BCE Loss: 1.0295195579528809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.124573707580566 | KNN Loss: 3.1080117225646973 | BCE Loss: 1.0165618658065796\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.068890571594238 | KNN Loss: 3.0575110912323 | BCE Loss: 1.0113792419433594\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.150649547576904 | KNN Loss: 3.103487253189087 | BCE Loss: 1.0471622943878174\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.114579677581787 | KNN Loss: 3.076709508895874 | BCE Loss: 1.037870168685913\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.137100696563721 | KNN Loss: 3.0883638858795166 | BCE Loss: 1.048736810684204\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.103177547454834 | KNN Loss: 3.1135854721069336 | BCE Loss: 0.9895921945571899\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.092764854431152 | KNN Loss: 3.063499689102173 | BCE Loss: 1.0292654037475586\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.114772796630859 | KNN Loss: 3.0711395740509033 | BCE Loss: 1.043632984161377\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.0992937088012695 | KNN Loss: 3.0755372047424316 | BCE Loss: 1.0237566232681274\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.128967761993408 | KNN Loss: 3.100825071334839 | BCE Loss: 1.0281426906585693\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.0547709465026855 | KNN Loss: 3.050157070159912 | BCE Loss: 1.0046138763427734\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.109694957733154 | KNN Loss: 3.0824360847473145 | BCE Loss: 1.0272589921951294\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.041595458984375 | KNN Loss: 3.0327484607696533 | BCE Loss: 1.0088467597961426\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.085227966308594 | KNN Loss: 3.064120054244995 | BCE Loss: 1.0211081504821777\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.14609432220459 | KNN Loss: 3.0795955657958984 | BCE Loss: 1.0664989948272705\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.111563682556152 | KNN Loss: 3.0810582637786865 | BCE Loss: 1.0305051803588867\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.061688423156738 | KNN Loss: 3.051447629928589 | BCE Loss: 1.0102410316467285\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.125705242156982 | KNN Loss: 3.081165075302124 | BCE Loss: 1.0445400476455688\n",
      "Epoch   132: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.096339702606201 | KNN Loss: 3.072819709777832 | BCE Loss: 1.0235201120376587\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.08646297454834 | KNN Loss: 3.063702344894409 | BCE Loss: 1.0227606296539307\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.071488857269287 | KNN Loss: 3.046337127685547 | BCE Loss: 1.0251517295837402\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.103120803833008 | KNN Loss: 3.0706300735473633 | BCE Loss: 1.032490849494934\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.093752861022949 | KNN Loss: 3.068739414215088 | BCE Loss: 1.0250134468078613\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.058253288269043 | KNN Loss: 3.055680990219116 | BCE Loss: 1.0025720596313477\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.116641998291016 | KNN Loss: 3.0924136638641357 | BCE Loss: 1.024228572845459\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.07635498046875 | KNN Loss: 3.072533130645752 | BCE Loss: 1.003821849822998\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.102290153503418 | KNN Loss: 3.0764670372009277 | BCE Loss: 1.0258232355117798\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.084108352661133 | KNN Loss: 3.0631020069122314 | BCE Loss: 1.0210063457489014\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.08221960067749 | KNN Loss: 3.077005624771118 | BCE Loss: 1.005213975906372\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.075446128845215 | KNN Loss: 3.0740151405334473 | BCE Loss: 1.0014312267303467\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.124666213989258 | KNN Loss: 3.087847948074341 | BCE Loss: 1.036818265914917\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.124899864196777 | KNN Loss: 3.0901191234588623 | BCE Loss: 1.0347808599472046\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.0638580322265625 | KNN Loss: 3.0437612533569336 | BCE Loss: 1.0200968980789185\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.154767990112305 | KNN Loss: 3.1064364910125732 | BCE Loss: 1.0483312606811523\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.041698932647705 | KNN Loss: 3.041123390197754 | BCE Loss: 1.0005755424499512\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.083306312561035 | KNN Loss: 3.049057960510254 | BCE Loss: 1.0342482328414917\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.060126781463623 | KNN Loss: 3.0533552169799805 | BCE Loss: 1.0067716836929321\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.0717363357543945 | KNN Loss: 3.0549919605255127 | BCE Loss: 1.0167441368103027\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.1209306716918945 | KNN Loss: 3.1020538806915283 | BCE Loss: 1.0188769102096558\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.07639741897583 | KNN Loss: 3.0325090885162354 | BCE Loss: 1.0438884496688843\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.133569240570068 | KNN Loss: 3.103944778442383 | BCE Loss: 1.0296244621276855\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.057582855224609 | KNN Loss: 3.039736747741699 | BCE Loss: 1.0178462266921997\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.071497440338135 | KNN Loss: 3.054056406021118 | BCE Loss: 1.0174410343170166\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.124966144561768 | KNN Loss: 3.064831018447876 | BCE Loss: 1.0601352453231812\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.1259989738464355 | KNN Loss: 3.0858514308929443 | BCE Loss: 1.0401475429534912\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.109895706176758 | KNN Loss: 3.0888404846191406 | BCE Loss: 1.021054983139038\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.09490442276001 | KNN Loss: 3.098956823348999 | BCE Loss: 0.9959476590156555\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.078773498535156 | KNN Loss: 3.042384386062622 | BCE Loss: 1.0363889932632446\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.079233646392822 | KNN Loss: 3.086811065673828 | BCE Loss: 0.9924224615097046\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.152987480163574 | KNN Loss: 3.109229803085327 | BCE Loss: 1.043757677078247\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.074504852294922 | KNN Loss: 3.0614819526672363 | BCE Loss: 1.013023018836975\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.141930103302002 | KNN Loss: 3.0955069065093994 | BCE Loss: 1.0464231967926025\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.052957057952881 | KNN Loss: 3.052898406982422 | BCE Loss: 1.000058650970459\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.081921577453613 | KNN Loss: 3.0659148693084717 | BCE Loss: 1.0160069465637207\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.133785247802734 | KNN Loss: 3.09938907623291 | BCE Loss: 1.0343961715698242\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.081788539886475 | KNN Loss: 3.057080030441284 | BCE Loss: 1.0247085094451904\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.070337295532227 | KNN Loss: 3.061326742172241 | BCE Loss: 1.0090107917785645\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.124511241912842 | KNN Loss: 3.107569456100464 | BCE Loss: 1.016941785812378\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.107921600341797 | KNN Loss: 3.090183734893799 | BCE Loss: 1.017737865447998\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.139163970947266 | KNN Loss: 3.06789231300354 | BCE Loss: 1.0712718963623047\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.114118576049805 | KNN Loss: 3.081688642501831 | BCE Loss: 1.0324301719665527\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.042596340179443 | KNN Loss: 3.050203561782837 | BCE Loss: 0.9923928380012512\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.089901924133301 | KNN Loss: 3.097109794616699 | BCE Loss: 0.992792010307312\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.141538619995117 | KNN Loss: 3.0947265625 | BCE Loss: 1.0468120574951172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.089846134185791 | KNN Loss: 3.065338373184204 | BCE Loss: 1.0245076417922974\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.053929328918457 | KNN Loss: 3.0428435802459717 | BCE Loss: 1.0110857486724854\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.047557830810547 | KNN Loss: 3.0383076667785645 | BCE Loss: 1.0092504024505615\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.083378791809082 | KNN Loss: 3.074779510498047 | BCE Loss: 1.0085991621017456\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.130561351776123 | KNN Loss: 3.094024419784546 | BCE Loss: 1.0365370512008667\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.155498027801514 | KNN Loss: 3.086491346359253 | BCE Loss: 1.0690068006515503\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.060441970825195 | KNN Loss: 3.0615506172180176 | BCE Loss: 0.9988915920257568\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.061905384063721 | KNN Loss: 3.0717945098876953 | BCE Loss: 0.9901106953620911\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.146703243255615 | KNN Loss: 3.1153154373168945 | BCE Loss: 1.0313876867294312\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.089696884155273 | KNN Loss: 3.0745599269866943 | BCE Loss: 1.0151371955871582\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.076371192932129 | KNN Loss: 3.0615074634552 | BCE Loss: 1.0148634910583496\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.060926914215088 | KNN Loss: 3.056913375854492 | BCE Loss: 1.0040134191513062\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.074075698852539 | KNN Loss: 3.023688316345215 | BCE Loss: 1.0503871440887451\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.184844970703125 | KNN Loss: 3.1121480464935303 | BCE Loss: 1.0726968050003052\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.065107345581055 | KNN Loss: 3.038017749786377 | BCE Loss: 1.0270893573760986\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.112954139709473 | KNN Loss: 3.0831332206726074 | BCE Loss: 1.0298211574554443\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.142792224884033 | KNN Loss: 3.1056957244873047 | BCE Loss: 1.037096619606018\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.076026916503906 | KNN Loss: 3.054443597793579 | BCE Loss: 1.0215833187103271\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.0722222328186035 | KNN Loss: 3.066458225250244 | BCE Loss: 1.005764126777649\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.060156345367432 | KNN Loss: 3.0166938304901123 | BCE Loss: 1.0434625148773193\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.105628490447998 | KNN Loss: 3.0587618350982666 | BCE Loss: 1.0468666553497314\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.084158897399902 | KNN Loss: 3.0586700439453125 | BCE Loss: 1.0254887342453003\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.142083168029785 | KNN Loss: 3.0799367427825928 | BCE Loss: 1.0621466636657715\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.096755027770996 | KNN Loss: 3.08335280418396 | BCE Loss: 1.013401985168457\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.095268249511719 | KNN Loss: 3.056226968765259 | BCE Loss: 1.0390410423278809\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.048133373260498 | KNN Loss: 3.0398528575897217 | BCE Loss: 1.0082803964614868\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.116387367248535 | KNN Loss: 3.076254367828369 | BCE Loss: 1.0401332378387451\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.100364685058594 | KNN Loss: 3.083096742630005 | BCE Loss: 1.0172679424285889\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.112762451171875 | KNN Loss: 3.0953283309936523 | BCE Loss: 1.0174341201782227\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.09647798538208 | KNN Loss: 3.059962034225464 | BCE Loss: 1.0365160703659058\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.112781524658203 | KNN Loss: 3.0705370903015137 | BCE Loss: 1.0422441959381104\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.083279132843018 | KNN Loss: 3.045664072036743 | BCE Loss: 1.0376149415969849\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.048698425292969 | KNN Loss: 3.041884660720825 | BCE Loss: 1.0068137645721436\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.057580471038818 | KNN Loss: 3.0463526248931885 | BCE Loss: 1.0112278461456299\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.063434600830078 | KNN Loss: 3.049210786819458 | BCE Loss: 1.0142240524291992\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.128931045532227 | KNN Loss: 3.090336322784424 | BCE Loss: 1.0385944843292236\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.093918800354004 | KNN Loss: 3.066318988800049 | BCE Loss: 1.027599573135376\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.128793716430664 | KNN Loss: 3.0824289321899414 | BCE Loss: 1.046364665031433\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.148049831390381 | KNN Loss: 3.09694504737854 | BCE Loss: 1.0511049032211304\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.056961536407471 | KNN Loss: 3.0750014781951904 | BCE Loss: 0.981959879398346\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.1251091957092285 | KNN Loss: 3.065513849258423 | BCE Loss: 1.0595954656600952\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.105037689208984 | KNN Loss: 3.0842061042785645 | BCE Loss: 1.020831823348999\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.069387912750244 | KNN Loss: 3.049501895904541 | BCE Loss: 1.0198860168457031\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.090425491333008 | KNN Loss: 3.066373586654663 | BCE Loss: 1.0240519046783447\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.062054634094238 | KNN Loss: 3.066118001937866 | BCE Loss: 0.9959366321563721\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.111888885498047 | KNN Loss: 3.0785977840423584 | BCE Loss: 1.0332913398742676\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.114434719085693 | KNN Loss: 3.1020328998565674 | BCE Loss: 1.0124019384384155\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.102325439453125 | KNN Loss: 3.0742053985595703 | BCE Loss: 1.0281202793121338\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.154817581176758 | KNN Loss: 3.0996835231781006 | BCE Loss: 1.0551342964172363\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.037799835205078 | KNN Loss: 3.028683662414551 | BCE Loss: 1.0091161727905273\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.092761516571045 | KNN Loss: 3.051050901412964 | BCE Loss: 1.0417107343673706\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.087540149688721 | KNN Loss: 3.067758798599243 | BCE Loss: 1.019781470298767\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.056789875030518 | KNN Loss: 3.056807041168213 | BCE Loss: 0.9999828338623047\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.060648441314697 | KNN Loss: 3.0372018814086914 | BCE Loss: 1.0234466791152954\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.109339714050293 | KNN Loss: 3.0845887660980225 | BCE Loss: 1.0247507095336914\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.069371223449707 | KNN Loss: 3.0402238368988037 | BCE Loss: 1.0291476249694824\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.095585823059082 | KNN Loss: 3.070100784301758 | BCE Loss: 1.0254852771759033\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.098583221435547 | KNN Loss: 3.082301616668701 | BCE Loss: 1.0162813663482666\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.100379943847656 | KNN Loss: 3.0874812602996826 | BCE Loss: 1.0128984451293945\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.0841169357299805 | KNN Loss: 3.065354347229004 | BCE Loss: 1.0187628269195557\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.06570291519165 | KNN Loss: 3.0517520904541016 | BCE Loss: 1.0139508247375488\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.1133904457092285 | KNN Loss: 3.064894437789917 | BCE Loss: 1.0484960079193115\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.134545803070068 | KNN Loss: 3.1015188694000244 | BCE Loss: 1.033026933670044\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.1165924072265625 | KNN Loss: 3.0784099102020264 | BCE Loss: 1.0381824970245361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.084072113037109 | KNN Loss: 3.0746548175811768 | BCE Loss: 1.0094174146652222\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.096889972686768 | KNN Loss: 3.0281858444213867 | BCE Loss: 1.0687041282653809\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.092436790466309 | KNN Loss: 3.0867021083831787 | BCE Loss: 1.0057344436645508\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.099284648895264 | KNN Loss: 3.0897903442382812 | BCE Loss: 1.0094943046569824\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.085549354553223 | KNN Loss: 3.0559909343719482 | BCE Loss: 1.0295581817626953\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.068517684936523 | KNN Loss: 3.057922840118408 | BCE Loss: 1.0105946063995361\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.104161739349365 | KNN Loss: 3.0666351318359375 | BCE Loss: 1.0375266075134277\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.098696708679199 | KNN Loss: 3.100743532180786 | BCE Loss: 0.9979534149169922\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.053186416625977 | KNN Loss: 3.060858726501465 | BCE Loss: 0.9923274517059326\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.123830795288086 | KNN Loss: 3.1024086475372314 | BCE Loss: 1.0214219093322754\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.125345230102539 | KNN Loss: 3.088972330093384 | BCE Loss: 1.0363726615905762\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.095207214355469 | KNN Loss: 3.0672829151153564 | BCE Loss: 1.0279240608215332\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.072131156921387 | KNN Loss: 3.060514211654663 | BCE Loss: 1.0116170644760132\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.117575645446777 | KNN Loss: 3.087371349334717 | BCE Loss: 1.0302040576934814\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.079968452453613 | KNN Loss: 3.0507748126983643 | BCE Loss: 1.029193639755249\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.058864593505859 | KNN Loss: 3.0534615516662598 | BCE Loss: 1.0054031610488892\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.158611297607422 | KNN Loss: 3.1031856536865234 | BCE Loss: 1.0554254055023193\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.058589935302734 | KNN Loss: 3.0573599338531494 | BCE Loss: 1.001230001449585\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.108344078063965 | KNN Loss: 3.0796706676483154 | BCE Loss: 1.0286736488342285\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.121781349182129 | KNN Loss: 3.091372489929199 | BCE Loss: 1.0304090976715088\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.071066856384277 | KNN Loss: 3.0633323192596436 | BCE Loss: 1.0077344179153442\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.084596157073975 | KNN Loss: 3.060953140258789 | BCE Loss: 1.0236430168151855\n",
      "Epoch   154: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.138585567474365 | KNN Loss: 3.0960164070129395 | BCE Loss: 1.0425692796707153\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.078268527984619 | KNN Loss: 3.0509378910064697 | BCE Loss: 1.0273306369781494\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.091036319732666 | KNN Loss: 3.045189380645752 | BCE Loss: 1.045846939086914\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.095788955688477 | KNN Loss: 3.0813777446746826 | BCE Loss: 1.014411449432373\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.108560085296631 | KNN Loss: 3.0807313919067383 | BCE Loss: 1.0278286933898926\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.10720157623291 | KNN Loss: 3.090036630630493 | BCE Loss: 1.017165184020996\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.039135456085205 | KNN Loss: 3.0454607009887695 | BCE Loss: 0.9936748743057251\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.107460021972656 | KNN Loss: 3.080561399459839 | BCE Loss: 1.0268983840942383\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.078132152557373 | KNN Loss: 3.0257673263549805 | BCE Loss: 1.052364706993103\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.1076860427856445 | KNN Loss: 3.0736160278320312 | BCE Loss: 1.0340697765350342\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.14388370513916 | KNN Loss: 3.1172232627868652 | BCE Loss: 1.026660442352295\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.086196422576904 | KNN Loss: 3.049136161804199 | BCE Loss: 1.0370603799819946\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.098116874694824 | KNN Loss: 3.0477895736694336 | BCE Loss: 1.0503270626068115\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.071028232574463 | KNN Loss: 3.044570207595825 | BCE Loss: 1.0264581441879272\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.083253383636475 | KNN Loss: 3.058312177658081 | BCE Loss: 1.024941325187683\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.0919084548950195 | KNN Loss: 3.068633556365967 | BCE Loss: 1.0232748985290527\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.0531840324401855 | KNN Loss: 3.041949987411499 | BCE Loss: 1.0112340450286865\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.082817554473877 | KNN Loss: 3.0529417991638184 | BCE Loss: 1.0298757553100586\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.107717514038086 | KNN Loss: 3.090930223464966 | BCE Loss: 1.0167875289916992\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.099184513092041 | KNN Loss: 3.055037021636963 | BCE Loss: 1.0441473722457886\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.036421775817871 | KNN Loss: 3.015380859375 | BCE Loss: 1.021040678024292\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.072483062744141 | KNN Loss: 3.0733721256256104 | BCE Loss: 0.999110996723175\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.073391914367676 | KNN Loss: 3.0642807483673096 | BCE Loss: 1.0091114044189453\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.087342739105225 | KNN Loss: 3.0898056030273438 | BCE Loss: 0.9975371360778809\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.1907501220703125 | KNN Loss: 3.1525495052337646 | BCE Loss: 1.038200855255127\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.116456985473633 | KNN Loss: 3.0975401401519775 | BCE Loss: 1.0189167261123657\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.107379913330078 | KNN Loss: 3.0951154232025146 | BCE Loss: 1.0122644901275635\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.0720367431640625 | KNN Loss: 3.071169376373291 | BCE Loss: 1.0008671283721924\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.1042046546936035 | KNN Loss: 3.0529239177703857 | BCE Loss: 1.0512807369232178\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.13700008392334 | KNN Loss: 3.094599723815918 | BCE Loss: 1.0424003601074219\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.066862106323242 | KNN Loss: 3.0525786876678467 | BCE Loss: 1.0142834186553955\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.09085750579834 | KNN Loss: 3.048243522644043 | BCE Loss: 1.042614221572876\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.072132110595703 | KNN Loss: 3.055119037628174 | BCE Loss: 1.0170129537582397\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.158971786499023 | KNN Loss: 3.111844301223755 | BCE Loss: 1.0471277236938477\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.085055828094482 | KNN Loss: 3.0805788040161133 | BCE Loss: 1.0044770240783691\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.110073566436768 | KNN Loss: 3.0894503593444824 | BCE Loss: 1.0206230878829956\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.112063407897949 | KNN Loss: 3.0714659690856934 | BCE Loss: 1.0405975580215454\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.080972671508789 | KNN Loss: 3.0495851039886475 | BCE Loss: 1.0313875675201416\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.05545711517334 | KNN Loss: 3.0500059127807617 | BCE Loss: 1.0054514408111572\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.052606105804443 | KNN Loss: 3.0388553142547607 | BCE Loss: 1.0137507915496826\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.104616165161133 | KNN Loss: 3.0780272483825684 | BCE Loss: 1.0265889167785645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.118847846984863 | KNN Loss: 3.0953078269958496 | BCE Loss: 1.0235401391983032\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.104015350341797 | KNN Loss: 3.065640926361084 | BCE Loss: 1.0383741855621338\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.121308326721191 | KNN Loss: 3.08447265625 | BCE Loss: 1.036835789680481\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.085082054138184 | KNN Loss: 3.0700488090515137 | BCE Loss: 1.01503324508667\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.08622932434082 | KNN Loss: 3.05708646774292 | BCE Loss: 1.02914297580719\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.074995994567871 | KNN Loss: 3.0457217693328857 | BCE Loss: 1.0292744636535645\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.192248344421387 | KNN Loss: 3.115696430206299 | BCE Loss: 1.0765516757965088\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.0836076736450195 | KNN Loss: 3.072368621826172 | BCE Loss: 1.0112388134002686\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.083106517791748 | KNN Loss: 3.059597969055176 | BCE Loss: 1.0235086679458618\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.112994194030762 | KNN Loss: 3.0710701942443848 | BCE Loss: 1.041924238204956\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.06248664855957 | KNN Loss: 3.040717601776123 | BCE Loss: 1.0217692852020264\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.083708763122559 | KNN Loss: 3.0708627700805664 | BCE Loss: 1.0128459930419922\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.080289363861084 | KNN Loss: 3.065748691558838 | BCE Loss: 1.014540672302246\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.126303672790527 | KNN Loss: 3.1156437397003174 | BCE Loss: 1.0106600522994995\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.132439136505127 | KNN Loss: 3.0940685272216797 | BCE Loss: 1.0383704900741577\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.1167097091674805 | KNN Loss: 3.0990214347839355 | BCE Loss: 1.0176881551742554\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.105287551879883 | KNN Loss: 3.071880340576172 | BCE Loss: 1.03340744972229\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.112738609313965 | KNN Loss: 3.1078951358795166 | BCE Loss: 1.0048434734344482\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.047613143920898 | KNN Loss: 3.0402090549468994 | BCE Loss: 1.00740385055542\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.017750263214111 | KNN Loss: 3.025791883468628 | BCE Loss: 0.9919583201408386\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.098991870880127 | KNN Loss: 3.054659605026245 | BCE Loss: 1.0443321466445923\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.081988334655762 | KNN Loss: 3.0656003952026367 | BCE Loss: 1.016387939453125\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.08620548248291 | KNN Loss: 3.0665054321289062 | BCE Loss: 1.0196998119354248\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.125013828277588 | KNN Loss: 3.0827198028564453 | BCE Loss: 1.0422940254211426\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.074697971343994 | KNN Loss: 3.0502941608428955 | BCE Loss: 1.0244039297103882\n",
      "Epoch   165: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.128120422363281 | KNN Loss: 3.0941412448883057 | BCE Loss: 1.0339791774749756\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.095075607299805 | KNN Loss: 3.0756945610046387 | BCE Loss: 1.019381046295166\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.1050801277160645 | KNN Loss: 3.0696539878845215 | BCE Loss: 1.0354260206222534\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.053380489349365 | KNN Loss: 3.0588555335998535 | BCE Loss: 0.9945248961448669\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.098010540008545 | KNN Loss: 3.0711140632629395 | BCE Loss: 1.026896357536316\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.102411270141602 | KNN Loss: 3.056830883026123 | BCE Loss: 1.0455806255340576\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.084608554840088 | KNN Loss: 3.037140369415283 | BCE Loss: 1.0474683046340942\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.073357105255127 | KNN Loss: 3.061138153076172 | BCE Loss: 1.0122190713882446\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.108345985412598 | KNN Loss: 3.078666925430298 | BCE Loss: 1.029679298400879\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.105545997619629 | KNN Loss: 3.071626901626587 | BCE Loss: 1.033919334411621\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.080606460571289 | KNN Loss: 3.060606002807617 | BCE Loss: 1.0200003385543823\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.047837257385254 | KNN Loss: 3.0405380725860596 | BCE Loss: 1.0072990655899048\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.08303165435791 | KNN Loss: 3.072995901107788 | BCE Loss: 1.0100358724594116\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.107853889465332 | KNN Loss: 3.0538277626037598 | BCE Loss: 1.0540261268615723\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.115534782409668 | KNN Loss: 3.103597402572632 | BCE Loss: 1.0119376182556152\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.064145088195801 | KNN Loss: 3.0541610717773438 | BCE Loss: 1.0099838972091675\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.117936134338379 | KNN Loss: 3.098818778991699 | BCE Loss: 1.0191175937652588\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.088320732116699 | KNN Loss: 3.072092294692993 | BCE Loss: 1.0162286758422852\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.136307716369629 | KNN Loss: 3.093982458114624 | BCE Loss: 1.0423250198364258\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.094986915588379 | KNN Loss: 3.0772178173065186 | BCE Loss: 1.0177693367004395\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.0686211585998535 | KNN Loss: 3.0640649795532227 | BCE Loss: 1.0045561790466309\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.105644702911377 | KNN Loss: 3.1015141010284424 | BCE Loss: 1.0041306018829346\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.119083404541016 | KNN Loss: 3.0751359462738037 | BCE Loss: 1.043947696685791\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.111936092376709 | KNN Loss: 3.0732781887054443 | BCE Loss: 1.0386579036712646\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.094536304473877 | KNN Loss: 3.0586130619049072 | BCE Loss: 1.0359232425689697\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.0851545333862305 | KNN Loss: 3.0667550563812256 | BCE Loss: 1.0183992385864258\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.097573280334473 | KNN Loss: 3.05835223197937 | BCE Loss: 1.0392212867736816\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.083431720733643 | KNN Loss: 3.0654489994049072 | BCE Loss: 1.0179827213287354\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.038968086242676 | KNN Loss: 3.032073974609375 | BCE Loss: 1.0068943500518799\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.112600803375244 | KNN Loss: 3.0861918926239014 | BCE Loss: 1.0264089107513428\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.11165189743042 | KNN Loss: 3.073366165161133 | BCE Loss: 1.0382858514785767\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.111804962158203 | KNN Loss: 3.0771026611328125 | BCE Loss: 1.0347023010253906\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.116237163543701 | KNN Loss: 3.0836856365203857 | BCE Loss: 1.0325514078140259\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.084144115447998 | KNN Loss: 3.0419154167175293 | BCE Loss: 1.0422288179397583\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.049401760101318 | KNN Loss: 3.0395395755767822 | BCE Loss: 1.0098621845245361\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.088078498840332 | KNN Loss: 3.0531702041625977 | BCE Loss: 1.0349081754684448\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.112143516540527 | KNN Loss: 3.07061505317688 | BCE Loss: 1.0415282249450684\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.153620719909668 | KNN Loss: 3.1040148735046387 | BCE Loss: 1.0496056079864502\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.1256303787231445 | KNN Loss: 3.063016653060913 | BCE Loss: 1.0626139640808105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.074243545532227 | KNN Loss: 3.054414749145508 | BCE Loss: 1.0198285579681396\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.090884208679199 | KNN Loss: 3.070280075073242 | BCE Loss: 1.020604133605957\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.080811023712158 | KNN Loss: 3.0609686374664307 | BCE Loss: 1.0198423862457275\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.056197643280029 | KNN Loss: 3.0462186336517334 | BCE Loss: 1.0099788904190063\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.092959403991699 | KNN Loss: 3.0743303298950195 | BCE Loss: 1.0186288356781006\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.103797435760498 | KNN Loss: 3.069753885269165 | BCE Loss: 1.034043550491333\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.095306873321533 | KNN Loss: 3.078124523162842 | BCE Loss: 1.0171823501586914\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.088261604309082 | KNN Loss: 3.0722692012786865 | BCE Loss: 1.0159926414489746\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.063467979431152 | KNN Loss: 3.050976514816284 | BCE Loss: 1.0124914646148682\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.068691730499268 | KNN Loss: 3.076348066329956 | BCE Loss: 0.9923436045646667\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.108346939086914 | KNN Loss: 3.1041767597198486 | BCE Loss: 1.0041701793670654\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.104279041290283 | KNN Loss: 3.0669844150543213 | BCE Loss: 1.037294626235962\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.117922782897949 | KNN Loss: 3.068251132965088 | BCE Loss: 1.0496715307235718\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.0743513107299805 | KNN Loss: 3.0497241020202637 | BCE Loss: 1.0246269702911377\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.109224319458008 | KNN Loss: 3.064316987991333 | BCE Loss: 1.0449073314666748\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.08777379989624 | KNN Loss: 3.048056125640869 | BCE Loss: 1.039717674255371\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.085320949554443 | KNN Loss: 3.0545120239257812 | BCE Loss: 1.030808925628662\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.058046817779541 | KNN Loss: 3.0638139247894287 | BCE Loss: 0.9942328929901123\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.135931015014648 | KNN Loss: 3.1061224937438965 | BCE Loss: 1.0298082828521729\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.12821102142334 | KNN Loss: 3.0903685092926025 | BCE Loss: 1.0378422737121582\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.099379539489746 | KNN Loss: 3.087616443634033 | BCE Loss: 1.0117629766464233\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.063244342803955 | KNN Loss: 3.046243906021118 | BCE Loss: 1.017000436782837\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.0379743576049805 | KNN Loss: 3.028477907180786 | BCE Loss: 1.0094964504241943\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.032634735107422 | KNN Loss: 3.0428874492645264 | BCE Loss: 0.9897475242614746\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.071699142456055 | KNN Loss: 3.057917356491089 | BCE Loss: 1.0137816667556763\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.086466312408447 | KNN Loss: 3.077528715133667 | BCE Loss: 1.0089374780654907\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.086740493774414 | KNN Loss: 3.070848226547241 | BCE Loss: 1.0158922672271729\n",
      "Epoch   176: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.120896816253662 | KNN Loss: 3.0852930545806885 | BCE Loss: 1.0356038808822632\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.075887680053711 | KNN Loss: 3.0368592739105225 | BCE Loss: 1.0390286445617676\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.082756996154785 | KNN Loss: 3.0734314918518066 | BCE Loss: 1.0093257427215576\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.101584434509277 | KNN Loss: 3.0701041221618652 | BCE Loss: 1.0314805507659912\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.141757011413574 | KNN Loss: 3.090034246444702 | BCE Loss: 1.0517230033874512\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.142331123352051 | KNN Loss: 3.0974175930023193 | BCE Loss: 1.0449132919311523\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.1151957511901855 | KNN Loss: 3.0935654640197754 | BCE Loss: 1.0216302871704102\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.072916030883789 | KNN Loss: 3.063603639602661 | BCE Loss: 1.009312629699707\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.068802833557129 | KNN Loss: 3.0487685203552246 | BCE Loss: 1.0200345516204834\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.093491554260254 | KNN Loss: 3.082305431365967 | BCE Loss: 1.011186122894287\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.108508586883545 | KNN Loss: 3.0637683868408203 | BCE Loss: 1.0447402000427246\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.063411235809326 | KNN Loss: 3.0361602306365967 | BCE Loss: 1.02725088596344\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.071700096130371 | KNN Loss: 3.0437240600585938 | BCE Loss: 1.0279762744903564\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.133980751037598 | KNN Loss: 3.0917797088623047 | BCE Loss: 1.042201042175293\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.098583698272705 | KNN Loss: 3.07094144821167 | BCE Loss: 1.0276423692703247\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.070863246917725 | KNN Loss: 3.060070514678955 | BCE Loss: 1.0107927322387695\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.065498352050781 | KNN Loss: 3.0640835762023926 | BCE Loss: 1.0014150142669678\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.09807014465332 | KNN Loss: 3.0749409198760986 | BCE Loss: 1.0231292247772217\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.10606050491333 | KNN Loss: 3.106266736984253 | BCE Loss: 0.9997937679290771\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.0412750244140625 | KNN Loss: 3.0507898330688477 | BCE Loss: 0.9904851913452148\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.146997451782227 | KNN Loss: 3.0906245708465576 | BCE Loss: 1.056372880935669\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.120967864990234 | KNN Loss: 3.0960006713867188 | BCE Loss: 1.0249671936035156\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.080663681030273 | KNN Loss: 3.075160503387451 | BCE Loss: 1.0055034160614014\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.105313777923584 | KNN Loss: 3.0878148078918457 | BCE Loss: 1.0174989700317383\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.133556365966797 | KNN Loss: 3.090999126434326 | BCE Loss: 1.0425572395324707\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.056792736053467 | KNN Loss: 3.0416641235351562 | BCE Loss: 1.0151287317276\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.099889278411865 | KNN Loss: 3.069434404373169 | BCE Loss: 1.0304548740386963\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.089054584503174 | KNN Loss: 3.0534398555755615 | BCE Loss: 1.0356147289276123\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.038719654083252 | KNN Loss: 3.0358805656433105 | BCE Loss: 1.002839207649231\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.097131729125977 | KNN Loss: 3.0754427909851074 | BCE Loss: 1.0216888189315796\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.046854019165039 | KNN Loss: 3.0358049869537354 | BCE Loss: 1.0110489130020142\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.112677097320557 | KNN Loss: 3.097691059112549 | BCE Loss: 1.0149860382080078\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.116533279418945 | KNN Loss: 3.0828356742858887 | BCE Loss: 1.0336976051330566\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.03955078125 | KNN Loss: 3.0345187187194824 | BCE Loss: 1.0050323009490967\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.122776508331299 | KNN Loss: 3.092559814453125 | BCE Loss: 1.0302168130874634\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.1143903732299805 | KNN Loss: 3.083568572998047 | BCE Loss: 1.0308215618133545\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.101598739624023 | KNN Loss: 3.0828633308410645 | BCE Loss: 1.018735408782959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.134346961975098 | KNN Loss: 3.089235544204712 | BCE Loss: 1.0451116561889648\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.103343963623047 | KNN Loss: 3.10176157951355 | BCE Loss: 1.001582384109497\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.095044136047363 | KNN Loss: 3.078857421875 | BCE Loss: 1.0161864757537842\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.05525016784668 | KNN Loss: 3.0586464405059814 | BCE Loss: 0.9966038465499878\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.088606834411621 | KNN Loss: 3.0439577102661133 | BCE Loss: 1.0446490049362183\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.073754787445068 | KNN Loss: 3.063962697982788 | BCE Loss: 1.0097920894622803\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.089110374450684 | KNN Loss: 3.0878055095672607 | BCE Loss: 1.001305103302002\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.104755401611328 | KNN Loss: 3.103487730026245 | BCE Loss: 1.0012677907943726\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.082903861999512 | KNN Loss: 3.0811519622802734 | BCE Loss: 1.0017517805099487\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.070512294769287 | KNN Loss: 3.040421724319458 | BCE Loss: 1.030090570449829\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.067720890045166 | KNN Loss: 3.04268217086792 | BCE Loss: 1.0250388383865356\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.127758026123047 | KNN Loss: 3.0907695293426514 | BCE Loss: 1.0369887351989746\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.070135116577148 | KNN Loss: 3.043384075164795 | BCE Loss: 1.0267508029937744\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.054443359375 | KNN Loss: 3.0392861366271973 | BCE Loss: 1.0151574611663818\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.095134258270264 | KNN Loss: 3.0545740127563477 | BCE Loss: 1.040560245513916\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.075869560241699 | KNN Loss: 3.0544793605804443 | BCE Loss: 1.021390438079834\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.091766834259033 | KNN Loss: 3.069196939468384 | BCE Loss: 1.0225697755813599\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.13569450378418 | KNN Loss: 3.064208745956421 | BCE Loss: 1.0714857578277588\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.126565456390381 | KNN Loss: 3.090604305267334 | BCE Loss: 1.0359611511230469\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.070084095001221 | KNN Loss: 3.0765817165374756 | BCE Loss: 0.9935022592544556\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.104671955108643 | KNN Loss: 3.0745487213134766 | BCE Loss: 1.030123233795166\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.043278217315674 | KNN Loss: 3.0420985221862793 | BCE Loss: 1.001179575920105\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.066396236419678 | KNN Loss: 3.058450698852539 | BCE Loss: 1.0079455375671387\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.114141464233398 | KNN Loss: 3.0636825561523438 | BCE Loss: 1.0504591464996338\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.1335601806640625 | KNN Loss: 3.1062417030334473 | BCE Loss: 1.0273182392120361\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.055238723754883 | KNN Loss: 3.042426824569702 | BCE Loss: 1.0128118991851807\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.043109893798828 | KNN Loss: 3.0102076530456543 | BCE Loss: 1.0329023599624634\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.088895797729492 | KNN Loss: 3.064466714859009 | BCE Loss: 1.0244290828704834\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.101177215576172 | KNN Loss: 3.0626769065856934 | BCE Loss: 1.0385000705718994\n",
      "Epoch   187: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.110885143280029 | KNN Loss: 3.0721049308776855 | BCE Loss: 1.0387803316116333\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.037993431091309 | KNN Loss: 3.0332980155944824 | BCE Loss: 1.0046955347061157\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.130813121795654 | KNN Loss: 3.071685314178467 | BCE Loss: 1.0591278076171875\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.088125705718994 | KNN Loss: 3.055952787399292 | BCE Loss: 1.0321727991104126\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.152911186218262 | KNN Loss: 3.0912187099456787 | BCE Loss: 1.061692237854004\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.134922981262207 | KNN Loss: 3.0930676460266113 | BCE Loss: 1.0418555736541748\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.095860481262207 | KNN Loss: 3.0650601387023926 | BCE Loss: 1.030800461769104\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.146989822387695 | KNN Loss: 3.085832357406616 | BCE Loss: 1.0611577033996582\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.042940139770508 | KNN Loss: 3.047654390335083 | BCE Loss: 0.99528568983078\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.061327934265137 | KNN Loss: 3.052333116531372 | BCE Loss: 1.0089945793151855\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.127408981323242 | KNN Loss: 3.0905680656433105 | BCE Loss: 1.0368410348892212\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.09597110748291 | KNN Loss: 3.085876226425171 | BCE Loss: 1.0100946426391602\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.12368106842041 | KNN Loss: 3.1142539978027344 | BCE Loss: 1.0094268321990967\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.103114128112793 | KNN Loss: 3.082098960876465 | BCE Loss: 1.021014928817749\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.125679016113281 | KNN Loss: 3.0860843658447266 | BCE Loss: 1.0395944118499756\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.138580799102783 | KNN Loss: 3.0882937908172607 | BCE Loss: 1.0502870082855225\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.065276622772217 | KNN Loss: 3.0547733306884766 | BCE Loss: 1.0105032920837402\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.094806671142578 | KNN Loss: 3.0549046993255615 | BCE Loss: 1.0399022102355957\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.105271339416504 | KNN Loss: 3.0664098262786865 | BCE Loss: 1.0388612747192383\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.121872425079346 | KNN Loss: 3.068495273590088 | BCE Loss: 1.0533771514892578\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.075935363769531 | KNN Loss: 3.051072835922241 | BCE Loss: 1.02486252784729\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.075682640075684 | KNN Loss: 3.069901704788208 | BCE Loss: 1.0057806968688965\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.094808578491211 | KNN Loss: 3.075002431869507 | BCE Loss: 1.0198060274124146\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.093607425689697 | KNN Loss: 3.045708656311035 | BCE Loss: 1.047898769378662\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.091627597808838 | KNN Loss: 3.0956194400787354 | BCE Loss: 0.9960082769393921\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.089938163757324 | KNN Loss: 3.069995641708374 | BCE Loss: 1.0199425220489502\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.042616844177246 | KNN Loss: 3.033379077911377 | BCE Loss: 1.00923752784729\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.115425109863281 | KNN Loss: 3.0918385982513428 | BCE Loss: 1.023586392402649\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.122679710388184 | KNN Loss: 3.106639862060547 | BCE Loss: 1.0160398483276367\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.064769744873047 | KNN Loss: 3.063007354736328 | BCE Loss: 1.0017625093460083\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.13244104385376 | KNN Loss: 3.0948688983917236 | BCE Loss: 1.0375720262527466\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.12172269821167 | KNN Loss: 3.071845293045044 | BCE Loss: 1.0498775243759155\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.0634636878967285 | KNN Loss: 3.098597764968872 | BCE Loss: 0.9648659825325012\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.046477317810059 | KNN Loss: 3.0460829734802246 | BCE Loss: 1.000394344329834\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.110082149505615 | KNN Loss: 3.079505443572998 | BCE Loss: 1.0305767059326172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.064971923828125 | KNN Loss: 3.038792133331299 | BCE Loss: 1.026179552078247\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.052854061126709 | KNN Loss: 3.080481767654419 | BCE Loss: 0.9723724722862244\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.069075584411621 | KNN Loss: 3.0547313690185547 | BCE Loss: 1.0143440961837769\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.0598249435424805 | KNN Loss: 3.02384614944458 | BCE Loss: 1.0359790325164795\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.0571770668029785 | KNN Loss: 3.055776596069336 | BCE Loss: 1.0014004707336426\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.059244632720947 | KNN Loss: 3.03291654586792 | BCE Loss: 1.0263279676437378\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.12431001663208 | KNN Loss: 3.0812485218048096 | BCE Loss: 1.043061375617981\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.089166641235352 | KNN Loss: 3.0641841888427734 | BCE Loss: 1.0249824523925781\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.088949203491211 | KNN Loss: 3.0572104454040527 | BCE Loss: 1.0317389965057373\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.087880611419678 | KNN Loss: 3.0609569549560547 | BCE Loss: 1.0269235372543335\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.089136123657227 | KNN Loss: 3.069281578063965 | BCE Loss: 1.0198543071746826\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.0968017578125 | KNN Loss: 3.089049816131592 | BCE Loss: 1.0077519416809082\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.077803611755371 | KNN Loss: 3.0575428009033203 | BCE Loss: 1.0202606916427612\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.104111671447754 | KNN Loss: 3.0671627521514893 | BCE Loss: 1.0369491577148438\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.087165832519531 | KNN Loss: 3.0677640438079834 | BCE Loss: 1.0194017887115479\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.108415603637695 | KNN Loss: 3.0862631797790527 | BCE Loss: 1.0221521854400635\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.05869197845459 | KNN Loss: 3.0374178886413574 | BCE Loss: 1.0212740898132324\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.101289749145508 | KNN Loss: 3.075680732727051 | BCE Loss: 1.025609016418457\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.097997665405273 | KNN Loss: 3.0860347747802734 | BCE Loss: 1.0119630098342896\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.068167209625244 | KNN Loss: 3.047600030899048 | BCE Loss: 1.0205672979354858\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.064200401306152 | KNN Loss: 3.0374906063079834 | BCE Loss: 1.026709794998169\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.117879390716553 | KNN Loss: 3.091799259185791 | BCE Loss: 1.0260802507400513\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.089716911315918 | KNN Loss: 3.038081169128418 | BCE Loss: 1.051635980606079\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.076962471008301 | KNN Loss: 3.0541181564331055 | BCE Loss: 1.0228441953659058\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.1031389236450195 | KNN Loss: 3.0658233165740967 | BCE Loss: 1.0373153686523438\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.103156089782715 | KNN Loss: 3.069939374923706 | BCE Loss: 1.0332167148590088\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.061761856079102 | KNN Loss: 3.0663657188415527 | BCE Loss: 0.9953958988189697\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.077976226806641 | KNN Loss: 3.057875633239746 | BCE Loss: 1.0201008319854736\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.051839828491211 | KNN Loss: 3.0245778560638428 | BCE Loss: 1.0272618532180786\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.094244003295898 | KNN Loss: 3.0759119987487793 | BCE Loss: 1.0183318853378296\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.11651611328125 | KNN Loss: 3.058422327041626 | BCE Loss: 1.0580940246582031\n",
      "Epoch   198: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.1262102127075195 | KNN Loss: 3.0586578845977783 | BCE Loss: 1.0675523281097412\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.070329189300537 | KNN Loss: 3.0775671005249023 | BCE Loss: 0.99276202917099\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.1091718673706055 | KNN Loss: 3.0595145225524902 | BCE Loss: 1.0496573448181152\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.088560104370117 | KNN Loss: 3.067214250564575 | BCE Loss: 1.021346092224121\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.074222087860107 | KNN Loss: 3.0342538356781006 | BCE Loss: 1.0399683713912964\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.102969646453857 | KNN Loss: 3.0646109580993652 | BCE Loss: 1.0383586883544922\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.157890319824219 | KNN Loss: 3.0980634689331055 | BCE Loss: 1.0598266124725342\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.06065559387207 | KNN Loss: 3.0628323554992676 | BCE Loss: 0.9978233575820923\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.088115692138672 | KNN Loss: 3.049942970275879 | BCE Loss: 1.038172960281372\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.092082977294922 | KNN Loss: 3.070345401763916 | BCE Loss: 1.0217373371124268\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.072671413421631 | KNN Loss: 3.0592851638793945 | BCE Loss: 1.0133861303329468\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.108207702636719 | KNN Loss: 3.070448160171509 | BCE Loss: 1.037759780883789\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.08148193359375 | KNN Loss: 3.076458692550659 | BCE Loss: 1.0050230026245117\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.060463905334473 | KNN Loss: 3.0480000972747803 | BCE Loss: 1.012463927268982\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.098283767700195 | KNN Loss: 3.0526716709136963 | BCE Loss: 1.04561185836792\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.0935893058776855 | KNN Loss: 3.0676496028900146 | BCE Loss: 1.025939702987671\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.056188583374023 | KNN Loss: 3.0381810665130615 | BCE Loss: 1.018007755279541\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.10096549987793 | KNN Loss: 3.0666611194610596 | BCE Loss: 1.034304141998291\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.09433126449585 | KNN Loss: 3.079746723175049 | BCE Loss: 1.0145845413208008\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.140842914581299 | KNN Loss: 3.0929768085479736 | BCE Loss: 1.0478659868240356\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.083616733551025 | KNN Loss: 3.0603995323181152 | BCE Loss: 1.0232173204421997\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.103177547454834 | KNN Loss: 3.0741915702819824 | BCE Loss: 1.028985857963562\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.059174537658691 | KNN Loss: 3.0510306358337402 | BCE Loss: 1.0081439018249512\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.10190486907959 | KNN Loss: 3.0551812648773193 | BCE Loss: 1.04672372341156\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.130124568939209 | KNN Loss: 3.103196859359741 | BCE Loss: 1.0269277095794678\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.048376560211182 | KNN Loss: 3.044736862182617 | BCE Loss: 1.0036396980285645\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.061566352844238 | KNN Loss: 3.0358471870422363 | BCE Loss: 1.025719404220581\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.082403182983398 | KNN Loss: 3.055270195007324 | BCE Loss: 1.0271332263946533\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.076779365539551 | KNN Loss: 3.0732192993164062 | BCE Loss: 1.0035603046417236\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.081154823303223 | KNN Loss: 3.050886392593384 | BCE Loss: 1.0302684307098389\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.086696624755859 | KNN Loss: 3.065631866455078 | BCE Loss: 1.0210647583007812\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.0915117263793945 | KNN Loss: 3.056863307952881 | BCE Loss: 1.0346482992172241\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.057520389556885 | KNN Loss: 3.033613443374634 | BCE Loss: 1.0239070653915405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.107048511505127 | KNN Loss: 3.0775537490844727 | BCE Loss: 1.0294947624206543\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.081412315368652 | KNN Loss: 3.066655397415161 | BCE Loss: 1.014756679534912\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.125693321228027 | KNN Loss: 3.0944645404815674 | BCE Loss: 1.03122878074646\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.076618194580078 | KNN Loss: 3.051234483718872 | BCE Loss: 1.025383710861206\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.080120086669922 | KNN Loss: 3.063258647918701 | BCE Loss: 1.0168616771697998\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.060517311096191 | KNN Loss: 3.029059648513794 | BCE Loss: 1.0314574241638184\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.064132213592529 | KNN Loss: 3.0640344619750977 | BCE Loss: 1.0000977516174316\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.111478805541992 | KNN Loss: 3.086312770843506 | BCE Loss: 1.0251657962799072\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.085339546203613 | KNN Loss: 3.055021286010742 | BCE Loss: 1.0303183794021606\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.130769729614258 | KNN Loss: 3.1052730083465576 | BCE Loss: 1.0254967212677002\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.104088306427002 | KNN Loss: 3.0607261657714844 | BCE Loss: 1.0433621406555176\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.108902931213379 | KNN Loss: 3.099271774291992 | BCE Loss: 1.0096313953399658\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.0573015213012695 | KNN Loss: 3.0441787242889404 | BCE Loss: 1.0131230354309082\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.10607385635376 | KNN Loss: 3.070953369140625 | BCE Loss: 1.0351204872131348\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.0912580490112305 | KNN Loss: 3.051541566848755 | BCE Loss: 1.0397164821624756\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.084285736083984 | KNN Loss: 3.0504088401794434 | BCE Loss: 1.033876657485962\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.071384906768799 | KNN Loss: 3.0352108478546143 | BCE Loss: 1.0361740589141846\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.076207160949707 | KNN Loss: 3.056349754333496 | BCE Loss: 1.0198571681976318\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.092304706573486 | KNN Loss: 3.070246458053589 | BCE Loss: 1.022058367729187\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.125190258026123 | KNN Loss: 3.1142451763153076 | BCE Loss: 1.0109450817108154\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.106739521026611 | KNN Loss: 3.0684635639190674 | BCE Loss: 1.038275957107544\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.084008693695068 | KNN Loss: 3.0579946041107178 | BCE Loss: 1.0260140895843506\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.116368293762207 | KNN Loss: 3.0733699798583984 | BCE Loss: 1.042998194694519\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.103527545928955 | KNN Loss: 3.05507493019104 | BCE Loss: 1.0484527349472046\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.069635391235352 | KNN Loss: 3.0406076908111572 | BCE Loss: 1.0290277004241943\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.12048864364624 | KNN Loss: 3.0767064094543457 | BCE Loss: 1.043782114982605\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.136855125427246 | KNN Loss: 3.101454734802246 | BCE Loss: 1.035400629043579\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.109652996063232 | KNN Loss: 3.0530788898468018 | BCE Loss: 1.0565742254257202\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.076815605163574 | KNN Loss: 3.041743278503418 | BCE Loss: 1.0350723266601562\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.128176212310791 | KNN Loss: 3.07726788520813 | BCE Loss: 1.0509083271026611\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.096165657043457 | KNN Loss: 3.082329273223877 | BCE Loss: 1.01383638381958\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.103065490722656 | KNN Loss: 3.0628511905670166 | BCE Loss: 1.04021418094635\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.097301006317139 | KNN Loss: 3.0881507396698 | BCE Loss: 1.0091502666473389\n",
      "Epoch   209: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.093053817749023 | KNN Loss: 3.0575435161590576 | BCE Loss: 1.0355101823806763\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.062494277954102 | KNN Loss: 3.0508344173431396 | BCE Loss: 1.0116596221923828\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.082121849060059 | KNN Loss: 3.0377449989318848 | BCE Loss: 1.0443768501281738\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.081949234008789 | KNN Loss: 3.0661327838897705 | BCE Loss: 1.0158166885375977\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.108432769775391 | KNN Loss: 3.078399419784546 | BCE Loss: 1.0300335884094238\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.0220465660095215 | KNN Loss: 3.026843786239624 | BCE Loss: 0.9952026605606079\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.051451206207275 | KNN Loss: 3.0661914348602295 | BCE Loss: 0.9852596521377563\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.121105194091797 | KNN Loss: 3.097712516784668 | BCE Loss: 1.023392915725708\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.10060977935791 | KNN Loss: 3.085951566696167 | BCE Loss: 1.0146582126617432\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.102863311767578 | KNN Loss: 3.0969080924987793 | BCE Loss: 1.0059549808502197\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.057973861694336 | KNN Loss: 3.035173177719116 | BCE Loss: 1.0228004455566406\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.070147514343262 | KNN Loss: 3.0687389373779297 | BCE Loss: 1.001408338546753\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.090900897979736 | KNN Loss: 3.0805130004882812 | BCE Loss: 1.0103877782821655\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.111647605895996 | KNN Loss: 3.0528459548950195 | BCE Loss: 1.058801531791687\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.130083084106445 | KNN Loss: 3.143584966659546 | BCE Loss: 0.9864981174468994\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.040033340454102 | KNN Loss: 3.0532615184783936 | BCE Loss: 0.9867720603942871\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.111478328704834 | KNN Loss: 3.089012384414673 | BCE Loss: 1.0224660634994507\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.212199687957764 | KNN Loss: 3.1445095539093018 | BCE Loss: 1.067690134048462\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.0984296798706055 | KNN Loss: 3.0558037757873535 | BCE Loss: 1.042626142501831\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.0606303215026855 | KNN Loss: 3.0461883544921875 | BCE Loss: 1.014441967010498\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.097746849060059 | KNN Loss: 3.0797648429870605 | BCE Loss: 1.017981767654419\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.118731498718262 | KNN Loss: 3.06584095954895 | BCE Loss: 1.052890658378601\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.125154495239258 | KNN Loss: 3.1066458225250244 | BCE Loss: 1.0185086727142334\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.045647621154785 | KNN Loss: 3.0204708576202393 | BCE Loss: 1.0251765251159668\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.030367851257324 | KNN Loss: 3.0091898441314697 | BCE Loss: 1.021178126335144\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.119331359863281 | KNN Loss: 3.096189498901367 | BCE Loss: 1.0231420993804932\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.076813697814941 | KNN Loss: 3.0674853324890137 | BCE Loss: 1.0093286037445068\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.13376522064209 | KNN Loss: 3.095562219619751 | BCE Loss: 1.0382027626037598\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.061156272888184 | KNN Loss: 3.0717978477478027 | BCE Loss: 0.9893583655357361\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.0880889892578125 | KNN Loss: 3.0836663246154785 | BCE Loss: 1.004422903060913\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.10444974899292 | KNN Loss: 3.067699432373047 | BCE Loss: 1.0367501974105835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.120598316192627 | KNN Loss: 3.0905704498291016 | BCE Loss: 1.0300278663635254\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.145369529724121 | KNN Loss: 3.096073627471924 | BCE Loss: 1.0492961406707764\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.148062705993652 | KNN Loss: 3.098355770111084 | BCE Loss: 1.0497069358825684\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.086316108703613 | KNN Loss: 3.0515215396881104 | BCE Loss: 1.0347943305969238\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.063759803771973 | KNN Loss: 3.067037582397461 | BCE Loss: 0.9967223405838013\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.120593547821045 | KNN Loss: 3.0807435512542725 | BCE Loss: 1.039849877357483\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.12528133392334 | KNN Loss: 3.061448574066162 | BCE Loss: 1.0638326406478882\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.0742506980896 | KNN Loss: 3.0651090145111084 | BCE Loss: 1.0091418027877808\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.067911624908447 | KNN Loss: 3.0427987575531006 | BCE Loss: 1.0251128673553467\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.077175140380859 | KNN Loss: 3.05332088470459 | BCE Loss: 1.0238540172576904\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.072458267211914 | KNN Loss: 3.0563671588897705 | BCE Loss: 1.0160908699035645\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.035457611083984 | KNN Loss: 3.017814874649048 | BCE Loss: 1.017642617225647\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.065376281738281 | KNN Loss: 3.053100824356079 | BCE Loss: 1.0122756958007812\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.035892486572266 | KNN Loss: 3.0424246788024902 | BCE Loss: 0.9934677481651306\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.087642669677734 | KNN Loss: 3.068795680999756 | BCE Loss: 1.0188469886779785\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.09495735168457 | KNN Loss: 3.07259202003479 | BCE Loss: 1.0223654508590698\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.106540203094482 | KNN Loss: 3.07657527923584 | BCE Loss: 1.0299649238586426\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.050467491149902 | KNN Loss: 3.031970977783203 | BCE Loss: 1.0184962749481201\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.100536346435547 | KNN Loss: 3.074746608734131 | BCE Loss: 1.025789737701416\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.0698161125183105 | KNN Loss: 3.050421953201294 | BCE Loss: 1.019394040107727\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.075618743896484 | KNN Loss: 3.045940399169922 | BCE Loss: 1.0296783447265625\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.064851760864258 | KNN Loss: 3.032559633255005 | BCE Loss: 1.0322922468185425\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.100612640380859 | KNN Loss: 3.069675922393799 | BCE Loss: 1.03093683719635\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.126596450805664 | KNN Loss: 3.0605835914611816 | BCE Loss: 1.0660126209259033\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.075366020202637 | KNN Loss: 3.060835838317871 | BCE Loss: 1.014530062675476\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.101933002471924 | KNN Loss: 3.067873477935791 | BCE Loss: 1.0340594053268433\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.097461700439453 | KNN Loss: 3.0548458099365234 | BCE Loss: 1.0426160097122192\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.096461296081543 | KNN Loss: 3.0694260597229004 | BCE Loss: 1.027035117149353\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.08357572555542 | KNN Loss: 3.0511646270751953 | BCE Loss: 1.032410979270935\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.08329439163208 | KNN Loss: 3.0641658306121826 | BCE Loss: 1.019128441810608\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.129696846008301 | KNN Loss: 3.0810511112213135 | BCE Loss: 1.0486457347869873\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.103707313537598 | KNN Loss: 3.0841619968414307 | BCE Loss: 1.019545316696167\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.114177703857422 | KNN Loss: 3.104243755340576 | BCE Loss: 1.0099339485168457\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.081442832946777 | KNN Loss: 3.0544681549072266 | BCE Loss: 1.0269744396209717\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.116124629974365 | KNN Loss: 3.078537940979004 | BCE Loss: 1.0375865697860718\n",
      "Epoch   220: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.124843597412109 | KNN Loss: 3.089228630065918 | BCE Loss: 1.0356147289276123\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.10251522064209 | KNN Loss: 3.074328660964966 | BCE Loss: 1.0281866788864136\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.077892303466797 | KNN Loss: 3.062849760055542 | BCE Loss: 1.0150423049926758\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.063669204711914 | KNN Loss: 3.0504398345947266 | BCE Loss: 1.013229250907898\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.102527618408203 | KNN Loss: 3.095651865005493 | BCE Loss: 1.0068756341934204\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.088420867919922 | KNN Loss: 3.078397274017334 | BCE Loss: 1.010023593902588\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.055367946624756 | KNN Loss: 3.0566320419311523 | BCE Loss: 0.9987358450889587\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.058032989501953 | KNN Loss: 3.05490779876709 | BCE Loss: 1.0031254291534424\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.06731653213501 | KNN Loss: 3.05971097946167 | BCE Loss: 1.0076055526733398\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.065996170043945 | KNN Loss: 3.0448877811431885 | BCE Loss: 1.0211083889007568\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.0817155838012695 | KNN Loss: 3.0621705055236816 | BCE Loss: 1.0195448398590088\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.143828868865967 | KNN Loss: 3.0867884159088135 | BCE Loss: 1.0570405721664429\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.134990215301514 | KNN Loss: 3.0816521644592285 | BCE Loss: 1.0533379316329956\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.084908485412598 | KNN Loss: 3.0838747024536133 | BCE Loss: 1.0010336637496948\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.03135871887207 | KNN Loss: 3.0259056091308594 | BCE Loss: 1.005453109741211\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.132697105407715 | KNN Loss: 3.1068129539489746 | BCE Loss: 1.0258843898773193\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.092682838439941 | KNN Loss: 3.063204288482666 | BCE Loss: 1.0294783115386963\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.060705184936523 | KNN Loss: 3.0275752544403076 | BCE Loss: 1.0331299304962158\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.082256317138672 | KNN Loss: 3.052565336227417 | BCE Loss: 1.029691219329834\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.080483436584473 | KNN Loss: 3.0545742511749268 | BCE Loss: 1.0259093046188354\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.028824329376221 | KNN Loss: 3.0283043384552 | BCE Loss: 1.000519871711731\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.090114593505859 | KNN Loss: 3.0858778953552246 | BCE Loss: 1.0042364597320557\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.083105087280273 | KNN Loss: 3.068964958190918 | BCE Loss: 1.0141401290893555\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.121236801147461 | KNN Loss: 3.0868401527404785 | BCE Loss: 1.034396767616272\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.101301193237305 | KNN Loss: 3.0919435024261475 | BCE Loss: 1.0093576908111572\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.096465110778809 | KNN Loss: 3.060976266860962 | BCE Loss: 1.0354889631271362\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.084346771240234 | KNN Loss: 3.088693380355835 | BCE Loss: 0.9956534504890442\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.089465618133545 | KNN Loss: 3.079427719116211 | BCE Loss: 1.0100377798080444\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.068915367126465 | KNN Loss: 3.0320777893066406 | BCE Loss: 1.0368378162384033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.076581001281738 | KNN Loss: 3.052670478820801 | BCE Loss: 1.0239107608795166\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.104897499084473 | KNN Loss: 3.0452442169189453 | BCE Loss: 1.059653401374817\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.108001708984375 | KNN Loss: 3.0808987617492676 | BCE Loss: 1.0271027088165283\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.116347312927246 | KNN Loss: 3.088130235671997 | BCE Loss: 1.028217077255249\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.117647647857666 | KNN Loss: 3.0619256496429443 | BCE Loss: 1.0557218790054321\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.098866939544678 | KNN Loss: 3.079468250274658 | BCE Loss: 1.019398808479309\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.090507507324219 | KNN Loss: 3.0597829818725586 | BCE Loss: 1.0307246446609497\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.041130542755127 | KNN Loss: 3.0301756858825684 | BCE Loss: 1.0109548568725586\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.048979759216309 | KNN Loss: 3.027906656265259 | BCE Loss: 1.0210729837417603\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.105654239654541 | KNN Loss: 3.070319652557373 | BCE Loss: 1.035334587097168\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.057984828948975 | KNN Loss: 3.07643461227417 | BCE Loss: 0.9815500378608704\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.091813564300537 | KNN Loss: 3.0713260173797607 | BCE Loss: 1.0204874277114868\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.151886940002441 | KNN Loss: 3.1128640174865723 | BCE Loss: 1.0390231609344482\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.117250919342041 | KNN Loss: 3.062079906463623 | BCE Loss: 1.055171012878418\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.079181671142578 | KNN Loss: 3.052070140838623 | BCE Loss: 1.027111530303955\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.066529273986816 | KNN Loss: 3.06671142578125 | BCE Loss: 0.9998178482055664\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.13456392288208 | KNN Loss: 3.118882417678833 | BCE Loss: 1.0156816244125366\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.078794479370117 | KNN Loss: 3.06085467338562 | BCE Loss: 1.017939567565918\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.074407577514648 | KNN Loss: 3.0668821334838867 | BCE Loss: 1.0075256824493408\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.10866641998291 | KNN Loss: 3.0696263313293457 | BCE Loss: 1.039040207862854\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.069530487060547 | KNN Loss: 3.055168628692627 | BCE Loss: 1.014362096786499\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.0668768882751465 | KNN Loss: 3.0695855617523193 | BCE Loss: 0.9972911477088928\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.093019485473633 | KNN Loss: 3.0836024284362793 | BCE Loss: 1.009417176246643\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.081984996795654 | KNN Loss: 3.078805685043335 | BCE Loss: 1.0031793117523193\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.101187705993652 | KNN Loss: 3.0977373123168945 | BCE Loss: 1.003450632095337\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.084324836730957 | KNN Loss: 3.065500259399414 | BCE Loss: 1.018824577331543\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.066869735717773 | KNN Loss: 3.0642800331115723 | BCE Loss: 1.0025897026062012\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.128790855407715 | KNN Loss: 3.0912935733795166 | BCE Loss: 1.0374972820281982\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.058535575866699 | KNN Loss: 3.0623021125793457 | BCE Loss: 0.9962332844734192\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.091804504394531 | KNN Loss: 3.0655391216278076 | BCE Loss: 1.0262653827667236\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.108120441436768 | KNN Loss: 3.0970752239227295 | BCE Loss: 1.011045217514038\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.092505931854248 | KNN Loss: 3.0581858158111572 | BCE Loss: 1.0343199968338013\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.0611348152160645 | KNN Loss: 3.0391855239868164 | BCE Loss: 1.0219494104385376\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.091432571411133 | KNN Loss: 3.0518386363983154 | BCE Loss: 1.0395939350128174\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.1068925857543945 | KNN Loss: 3.082502841949463 | BCE Loss: 1.0243897438049316\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.103219032287598 | KNN Loss: 3.071772813796997 | BCE Loss: 1.0314459800720215\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.1421122550964355 | KNN Loss: 3.0694499015808105 | BCE Loss: 1.072662353515625\n",
      "Epoch   231: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.077903747558594 | KNN Loss: 3.071561574935913 | BCE Loss: 1.0063420534133911\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.108827114105225 | KNN Loss: 3.1036267280578613 | BCE Loss: 1.0052002668380737\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.1112871170043945 | KNN Loss: 3.0871708393096924 | BCE Loss: 1.0241165161132812\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.093832015991211 | KNN Loss: 3.0695345401763916 | BCE Loss: 1.0242975950241089\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.1224894523620605 | KNN Loss: 3.0952506065368652 | BCE Loss: 1.0272387266159058\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.143531799316406 | KNN Loss: 3.086534023284912 | BCE Loss: 1.056997537612915\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.070614814758301 | KNN Loss: 3.0332448482513428 | BCE Loss: 1.037369728088379\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.110321044921875 | KNN Loss: 3.0607783794403076 | BCE Loss: 1.0495425462722778\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.116925239562988 | KNN Loss: 3.0896098613739014 | BCE Loss: 1.0273151397705078\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.077852249145508 | KNN Loss: 3.038004159927368 | BCE Loss: 1.0398478507995605\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.04841947555542 | KNN Loss: 3.070042610168457 | BCE Loss: 0.9783767461776733\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.087474822998047 | KNN Loss: 3.0789361000061035 | BCE Loss: 1.0085386037826538\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.066342353820801 | KNN Loss: 3.0603785514831543 | BCE Loss: 1.0059638023376465\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.09218692779541 | KNN Loss: 3.045668840408325 | BCE Loss: 1.0465178489685059\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.089325904846191 | KNN Loss: 3.0394396781921387 | BCE Loss: 1.0498864650726318\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.099413871765137 | KNN Loss: 3.08233904838562 | BCE Loss: 1.0170750617980957\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.103977680206299 | KNN Loss: 3.067793130874634 | BCE Loss: 1.036184549331665\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.070574760437012 | KNN Loss: 3.0703816413879395 | BCE Loss: 1.0001933574676514\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.105567932128906 | KNN Loss: 3.0778348445892334 | BCE Loss: 1.027733325958252\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.07839298248291 | KNN Loss: 3.0656700134277344 | BCE Loss: 1.0127232074737549\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.076350688934326 | KNN Loss: 3.0427722930908203 | BCE Loss: 1.0335783958435059\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.118417739868164 | KNN Loss: 3.0898823738098145 | BCE Loss: 1.0285351276397705\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.025479316711426 | KNN Loss: 3.0140702724456787 | BCE Loss: 1.0114089250564575\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.10365104675293 | KNN Loss: 3.0573489665985107 | BCE Loss: 1.0463019609451294\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.102634906768799 | KNN Loss: 3.0682828426361084 | BCE Loss: 1.0343520641326904\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.107420921325684 | KNN Loss: 3.0824170112609863 | BCE Loss: 1.0250039100646973\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.062424659729004 | KNN Loss: 3.0373685359954834 | BCE Loss: 1.0250561237335205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.11304235458374 | KNN Loss: 3.079608678817749 | BCE Loss: 1.0334336757659912\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.084815979003906 | KNN Loss: 3.0672075748443604 | BCE Loss: 1.0176081657409668\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.072519779205322 | KNN Loss: 3.0584123134613037 | BCE Loss: 1.0141074657440186\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.111647605895996 | KNN Loss: 3.061455488204956 | BCE Loss: 1.0501922369003296\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.1021270751953125 | KNN Loss: 3.0939254760742188 | BCE Loss: 1.0082013607025146\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.0620527267456055 | KNN Loss: 3.042569398880005 | BCE Loss: 1.0194833278656006\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.043419361114502 | KNN Loss: 3.017348051071167 | BCE Loss: 1.026071310043335\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.126520156860352 | KNN Loss: 3.0896265506744385 | BCE Loss: 1.0368937253952026\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.12271785736084 | KNN Loss: 3.0812408924102783 | BCE Loss: 1.041476845741272\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.057897567749023 | KNN Loss: 3.052488327026367 | BCE Loss: 1.0054091215133667\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.092483997344971 | KNN Loss: 3.086324691772461 | BCE Loss: 1.0061593055725098\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.109026908874512 | KNN Loss: 3.0923972129821777 | BCE Loss: 1.016629695892334\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.069114685058594 | KNN Loss: 3.0403542518615723 | BCE Loss: 1.0287606716156006\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.13946533203125 | KNN Loss: 3.113295793533325 | BCE Loss: 1.0261693000793457\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.103476524353027 | KNN Loss: 3.0592410564422607 | BCE Loss: 1.0442352294921875\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.030987739562988 | KNN Loss: 3.040712356567383 | BCE Loss: 0.9902753829956055\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.066053867340088 | KNN Loss: 3.044283390045166 | BCE Loss: 1.0217705965042114\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.082862377166748 | KNN Loss: 3.0687949657440186 | BCE Loss: 1.0140674114227295\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.107339382171631 | KNN Loss: 3.085725784301758 | BCE Loss: 1.021613597869873\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.082642555236816 | KNN Loss: 3.064413547515869 | BCE Loss: 1.0182292461395264\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.09389591217041 | KNN Loss: 3.0531551837921143 | BCE Loss: 1.0407406091690063\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.108442306518555 | KNN Loss: 3.05940318107605 | BCE Loss: 1.049039363861084\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.133791923522949 | KNN Loss: 3.103529453277588 | BCE Loss: 1.0302624702453613\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.077413558959961 | KNN Loss: 3.0751051902770996 | BCE Loss: 1.0023086071014404\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.113638877868652 | KNN Loss: 3.084900140762329 | BCE Loss: 1.0287389755249023\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.109854221343994 | KNN Loss: 3.055401086807251 | BCE Loss: 1.0544531345367432\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.08268928527832 | KNN Loss: 3.0620319843292236 | BCE Loss: 1.0206575393676758\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.166449546813965 | KNN Loss: 3.1326403617858887 | BCE Loss: 1.033808946609497\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.040433883666992 | KNN Loss: 3.0169851779937744 | BCE Loss: 1.0234488248825073\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.108205795288086 | KNN Loss: 3.066086530685425 | BCE Loss: 1.0421193838119507\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.0283203125 | KNN Loss: 3.023918628692627 | BCE Loss: 1.004401445388794\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.046877384185791 | KNN Loss: 3.039154291152954 | BCE Loss: 1.0077229738235474\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.091663360595703 | KNN Loss: 3.0718815326690674 | BCE Loss: 1.0197815895080566\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.037616729736328 | KNN Loss: 3.040555238723755 | BCE Loss: 0.9970617294311523\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.097705841064453 | KNN Loss: 3.0652365684509277 | BCE Loss: 1.0324690341949463\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.134925842285156 | KNN Loss: 3.0857625007629395 | BCE Loss: 1.0491633415222168\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.045960903167725 | KNN Loss: 3.0328731536865234 | BCE Loss: 1.0130877494812012\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.05726957321167 | KNN Loss: 3.0272727012634277 | BCE Loss: 1.0299968719482422\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.134005546569824 | KNN Loss: 3.0847439765930176 | BCE Loss: 1.049261450767517\n",
      "Epoch   242: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.108443260192871 | KNN Loss: 3.0647196769714355 | BCE Loss: 1.0437233448028564\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.097443580627441 | KNN Loss: 3.0774729251861572 | BCE Loss: 1.019970417022705\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.083305835723877 | KNN Loss: 3.043513536453247 | BCE Loss: 1.0397921800613403\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.109119415283203 | KNN Loss: 3.073352098464966 | BCE Loss: 1.0357670783996582\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.115313529968262 | KNN Loss: 3.086348295211792 | BCE Loss: 1.0289649963378906\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.117059230804443 | KNN Loss: 3.072275400161743 | BCE Loss: 1.0447838306427002\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.088727951049805 | KNN Loss: 3.09122896194458 | BCE Loss: 0.9974992275238037\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.102502822875977 | KNN Loss: 3.05851411819458 | BCE Loss: 1.0439889430999756\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.098952293395996 | KNN Loss: 3.079118013381958 | BCE Loss: 1.019834280014038\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.108738899230957 | KNN Loss: 3.074793577194214 | BCE Loss: 1.033945083618164\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.105100631713867 | KNN Loss: 3.0761632919311523 | BCE Loss: 1.0289373397827148\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.136268615722656 | KNN Loss: 3.11897349357605 | BCE Loss: 1.0172951221466064\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.109686851501465 | KNN Loss: 3.086270332336426 | BCE Loss: 1.023416519165039\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.068393707275391 | KNN Loss: 3.0345277786254883 | BCE Loss: 1.0338656902313232\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.074355125427246 | KNN Loss: 3.042668342590332 | BCE Loss: 1.0316869020462036\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.157736301422119 | KNN Loss: 3.126889705657959 | BCE Loss: 1.0308467149734497\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.081393241882324 | KNN Loss: 3.068570852279663 | BCE Loss: 1.0128225088119507\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.117844581604004 | KNN Loss: 3.0707805156707764 | BCE Loss: 1.0470638275146484\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.122549057006836 | KNN Loss: 3.1065852642059326 | BCE Loss: 1.0159637928009033\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.0911712646484375 | KNN Loss: 3.077866554260254 | BCE Loss: 1.0133044719696045\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.055729389190674 | KNN Loss: 3.0551493167877197 | BCE Loss: 1.0005799531936646\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.113718509674072 | KNN Loss: 3.0704870223999023 | BCE Loss: 1.0432313680648804\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.0700201988220215 | KNN Loss: 3.0397355556488037 | BCE Loss: 1.0302845239639282\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.0794854164123535 | KNN Loss: 3.0517563819885254 | BCE Loss: 1.0277289152145386\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.0922627449035645 | KNN Loss: 3.080176591873169 | BCE Loss: 1.0120861530303955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.092326641082764 | KNN Loss: 3.0443007946014404 | BCE Loss: 1.0480258464813232\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.145635604858398 | KNN Loss: 3.0890908241271973 | BCE Loss: 1.0565450191497803\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.061256408691406 | KNN Loss: 3.0601353645324707 | BCE Loss: 1.0011212825775146\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.0957112312316895 | KNN Loss: 3.0801279544830322 | BCE Loss: 1.0155832767486572\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.0715179443359375 | KNN Loss: 3.0436267852783203 | BCE Loss: 1.027890920639038\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.108040809631348 | KNN Loss: 3.070302963256836 | BCE Loss: 1.0377378463745117\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.092835903167725 | KNN Loss: 3.0975546836853027 | BCE Loss: 0.9952813386917114\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.098410129547119 | KNN Loss: 3.097414255142212 | BCE Loss: 1.0009958744049072\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.099661350250244 | KNN Loss: 3.0546679496765137 | BCE Loss: 1.04499351978302\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.097075462341309 | KNN Loss: 3.1097795963287354 | BCE Loss: 0.9872960448265076\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.132386684417725 | KNN Loss: 3.095669984817505 | BCE Loss: 1.0367166996002197\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.0568952560424805 | KNN Loss: 3.056670904159546 | BCE Loss: 1.0002243518829346\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.084078788757324 | KNN Loss: 3.0653700828552246 | BCE Loss: 1.0187089443206787\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.109726905822754 | KNN Loss: 3.094935655593872 | BCE Loss: 1.0147912502288818\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.09334659576416 | KNN Loss: 3.0481069087982178 | BCE Loss: 1.0452399253845215\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.1402130126953125 | KNN Loss: 3.1064460277557373 | BCE Loss: 1.0337672233581543\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.105827808380127 | KNN Loss: 3.0754497051239014 | BCE Loss: 1.0303781032562256\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.088380813598633 | KNN Loss: 3.039888620376587 | BCE Loss: 1.048492193222046\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.087682723999023 | KNN Loss: 3.0838584899902344 | BCE Loss: 1.003824234008789\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.104251861572266 | KNN Loss: 3.0803751945495605 | BCE Loss: 1.0238769054412842\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.084527492523193 | KNN Loss: 3.0492613315582275 | BCE Loss: 1.0352661609649658\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.073734283447266 | KNN Loss: 3.0664150714874268 | BCE Loss: 1.0073189735412598\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.079775810241699 | KNN Loss: 3.0444271564483643 | BCE Loss: 1.035348892211914\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.076740264892578 | KNN Loss: 3.069736957550049 | BCE Loss: 1.0070030689239502\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.091266632080078 | KNN Loss: 3.062364101409912 | BCE Loss: 1.028902292251587\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.084169387817383 | KNN Loss: 3.0701088905334473 | BCE Loss: 1.0140604972839355\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.071086883544922 | KNN Loss: 3.077458143234253 | BCE Loss: 0.993628978729248\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.088295936584473 | KNN Loss: 3.067289352416992 | BCE Loss: 1.0210065841674805\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.066451549530029 | KNN Loss: 3.044161796569824 | BCE Loss: 1.022289752960205\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.111407279968262 | KNN Loss: 3.1127731800079346 | BCE Loss: 0.9986342787742615\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.095881462097168 | KNN Loss: 3.039525032043457 | BCE Loss: 1.056356430053711\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.074120044708252 | KNN Loss: 3.0831902027130127 | BCE Loss: 0.9909297227859497\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.101665496826172 | KNN Loss: 3.0894482135772705 | BCE Loss: 1.0122175216674805\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.136205196380615 | KNN Loss: 3.116182327270508 | BCE Loss: 1.020022988319397\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.079495429992676 | KNN Loss: 3.0586185455322266 | BCE Loss: 1.0208770036697388\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.080738067626953 | KNN Loss: 3.0708329677581787 | BCE Loss: 1.0099050998687744\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.080557823181152 | KNN Loss: 3.06758451461792 | BCE Loss: 1.0129730701446533\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.101096153259277 | KNN Loss: 3.0624964237213135 | BCE Loss: 1.0385998487472534\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.114943027496338 | KNN Loss: 3.0852935314178467 | BCE Loss: 1.0296496152877808\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.113164901733398 | KNN Loss: 3.069715738296509 | BCE Loss: 1.0434494018554688\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.1038618087768555 | KNN Loss: 3.0617847442626953 | BCE Loss: 1.0420770645141602\n",
      "Epoch   253: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.092800617218018 | KNN Loss: 3.0643470287323 | BCE Loss: 1.0284534692764282\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.111754417419434 | KNN Loss: 3.083500862121582 | BCE Loss: 1.0282537937164307\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.0725998878479 | KNN Loss: 3.0632681846618652 | BCE Loss: 1.0093315839767456\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.118330955505371 | KNN Loss: 3.0777530670166016 | BCE Loss: 1.04057776927948\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.095397472381592 | KNN Loss: 3.064323663711548 | BCE Loss: 1.0310739278793335\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.0995073318481445 | KNN Loss: 3.0710649490356445 | BCE Loss: 1.0284422636032104\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.126161575317383 | KNN Loss: 3.0919618606567383 | BCE Loss: 1.0341994762420654\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.0918989181518555 | KNN Loss: 3.0507326126098633 | BCE Loss: 1.041166067123413\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.12365198135376 | KNN Loss: 3.0912606716156006 | BCE Loss: 1.0323911905288696\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.120651721954346 | KNN Loss: 3.091848611831665 | BCE Loss: 1.0288031101226807\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.099566459655762 | KNN Loss: 3.0761940479278564 | BCE Loss: 1.0233725309371948\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.042248249053955 | KNN Loss: 3.0346529483795166 | BCE Loss: 1.0075953006744385\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.07514762878418 | KNN Loss: 3.059234857559204 | BCE Loss: 1.0159125328063965\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.085002899169922 | KNN Loss: 3.0578243732452393 | BCE Loss: 1.0271787643432617\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.095681190490723 | KNN Loss: 3.0765159130096436 | BCE Loss: 1.0191655158996582\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.0702104568481445 | KNN Loss: 3.074734687805176 | BCE Loss: 0.9954758882522583\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.053976058959961 | KNN Loss: 3.0154688358306885 | BCE Loss: 1.0385069847106934\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.144533157348633 | KNN Loss: 3.111997604370117 | BCE Loss: 1.0325355529785156\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.117740631103516 | KNN Loss: 3.062506914138794 | BCE Loss: 1.0552337169647217\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.069806098937988 | KNN Loss: 3.047714948654175 | BCE Loss: 1.0220909118652344\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.085080146789551 | KNN Loss: 3.0590169429779053 | BCE Loss: 1.0260634422302246\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.079564571380615 | KNN Loss: 3.0311830043792725 | BCE Loss: 1.0483815670013428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.044073581695557 | KNN Loss: 3.020670175552368 | BCE Loss: 1.0234034061431885\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.086979866027832 | KNN Loss: 3.079332113265991 | BCE Loss: 1.0076477527618408\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.136874198913574 | KNN Loss: 3.0958328247070312 | BCE Loss: 1.0410414934158325\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.081936836242676 | KNN Loss: 3.065920114517212 | BCE Loss: 1.0160164833068848\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.086392402648926 | KNN Loss: 3.0447659492492676 | BCE Loss: 1.0416263341903687\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.103999614715576 | KNN Loss: 3.085829019546509 | BCE Loss: 1.0181704759597778\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.105611324310303 | KNN Loss: 3.0609638690948486 | BCE Loss: 1.044647455215454\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.081439971923828 | KNN Loss: 3.070415735244751 | BCE Loss: 1.0110244750976562\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.123736381530762 | KNN Loss: 3.104008674621582 | BCE Loss: 1.0197277069091797\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.031075477600098 | KNN Loss: 3.0132861137390137 | BCE Loss: 1.017789602279663\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.146826267242432 | KNN Loss: 3.1254825592041016 | BCE Loss: 1.0213438272476196\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.042096138000488 | KNN Loss: 3.046156883239746 | BCE Loss: 0.9959393739700317\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.080065727233887 | KNN Loss: 3.0566070079803467 | BCE Loss: 1.02345871925354\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.122219562530518 | KNN Loss: 3.087273359298706 | BCE Loss: 1.034946322441101\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.067778587341309 | KNN Loss: 3.0443999767303467 | BCE Loss: 1.0233783721923828\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.140277862548828 | KNN Loss: 3.086383581161499 | BCE Loss: 1.05389404296875\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.145340919494629 | KNN Loss: 3.0921478271484375 | BCE Loss: 1.0531928539276123\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.113532066345215 | KNN Loss: 3.0633578300476074 | BCE Loss: 1.0501742362976074\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.092930316925049 | KNN Loss: 3.0891785621643066 | BCE Loss: 1.0037516355514526\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.070930480957031 | KNN Loss: 3.057511329650879 | BCE Loss: 1.0134189128875732\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.066719055175781 | KNN Loss: 3.0530660152435303 | BCE Loss: 1.0136528015136719\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.0875654220581055 | KNN Loss: 3.069291353225708 | BCE Loss: 1.0182738304138184\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.113234996795654 | KNN Loss: 3.0866386890411377 | BCE Loss: 1.0265963077545166\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.099735260009766 | KNN Loss: 3.080958604812622 | BCE Loss: 1.0187766551971436\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.116690635681152 | KNN Loss: 3.101503372192383 | BCE Loss: 1.0151875019073486\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.09897518157959 | KNN Loss: 3.0565099716186523 | BCE Loss: 1.0424649715423584\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.121300220489502 | KNN Loss: 3.082514524459839 | BCE Loss: 1.038785696029663\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.0989603996276855 | KNN Loss: 3.052586317062378 | BCE Loss: 1.046373963356018\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.099054336547852 | KNN Loss: 3.0890614986419678 | BCE Loss: 1.0099925994873047\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.078230857849121 | KNN Loss: 3.0646443367004395 | BCE Loss: 1.0135867595672607\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.08084774017334 | KNN Loss: 3.0760135650634766 | BCE Loss: 1.0048344135284424\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.0925068855285645 | KNN Loss: 3.0653631687164307 | BCE Loss: 1.0271437168121338\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.0821638107299805 | KNN Loss: 3.0554051399230957 | BCE Loss: 1.0267585515975952\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.039254665374756 | KNN Loss: 3.0448100566864014 | BCE Loss: 0.994444727897644\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.070267677307129 | KNN Loss: 3.0429165363311768 | BCE Loss: 1.0273513793945312\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.070713996887207 | KNN Loss: 3.077501058578491 | BCE Loss: 0.9932131767272949\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.0680646896362305 | KNN Loss: 3.05767560005188 | BCE Loss: 1.0103888511657715\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.103814601898193 | KNN Loss: 3.077514410018921 | BCE Loss: 1.026300311088562\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.120885848999023 | KNN Loss: 3.062795400619507 | BCE Loss: 1.0580902099609375\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.08610725402832 | KNN Loss: 3.040905475616455 | BCE Loss: 1.0452015399932861\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.11884880065918 | KNN Loss: 3.0822594165802 | BCE Loss: 1.0365893840789795\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.120406150817871 | KNN Loss: 3.057736873626709 | BCE Loss: 1.062669038772583\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.09630012512207 | KNN Loss: 3.0733389854431152 | BCE Loss: 1.022961139678955\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.0726823806762695 | KNN Loss: 3.058697462081909 | BCE Loss: 1.0139849185943604\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.139880180358887 | KNN Loss: 3.0990633964538574 | BCE Loss: 1.0408165454864502\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.082940101623535 | KNN Loss: 3.0599942207336426 | BCE Loss: 1.0229461193084717\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.125943660736084 | KNN Loss: 3.0852701663970947 | BCE Loss: 1.0406736135482788\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.057091236114502 | KNN Loss: 3.0322985649108887 | BCE Loss: 1.0247926712036133\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.053297519683838 | KNN Loss: 3.0446736812591553 | BCE Loss: 1.0086238384246826\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.049590110778809 | KNN Loss: 3.0518839359283447 | BCE Loss: 0.9977060556411743\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.133805274963379 | KNN Loss: 3.10791277885437 | BCE Loss: 1.025892734527588\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.070567607879639 | KNN Loss: 3.0607948303222656 | BCE Loss: 1.009772777557373\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.06569766998291 | KNN Loss: 3.05179762840271 | BCE Loss: 1.0139002799987793\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.0549468994140625 | KNN Loss: 3.038198232650757 | BCE Loss: 1.0167489051818848\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.098052024841309 | KNN Loss: 3.0452511310577393 | BCE Loss: 1.0528006553649902\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.086702346801758 | KNN Loss: 3.0640223026275635 | BCE Loss: 1.0226799249649048\n",
      "Epoch   266: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.1034746170043945 | KNN Loss: 3.0666940212249756 | BCE Loss: 1.036780834197998\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.068012237548828 | KNN Loss: 3.0422849655151367 | BCE Loss: 1.0257270336151123\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.149881839752197 | KNN Loss: 3.115752696990967 | BCE Loss: 1.0341291427612305\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.101619720458984 | KNN Loss: 3.071950674057007 | BCE Loss: 1.029669165611267\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.084968566894531 | KNN Loss: 3.055163860321045 | BCE Loss: 1.0298044681549072\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.105486869812012 | KNN Loss: 3.056866407394409 | BCE Loss: 1.0486204624176025\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.09625244140625 | KNN Loss: 3.046146869659424 | BCE Loss: 1.0501055717468262\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.0821757316589355 | KNN Loss: 3.057128429412842 | BCE Loss: 1.0250474214553833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.09248685836792 | KNN Loss: 3.067960739135742 | BCE Loss: 1.0245261192321777\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.074141025543213 | KNN Loss: 3.0553879737854004 | BCE Loss: 1.0187530517578125\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.124065399169922 | KNN Loss: 3.1013076305389404 | BCE Loss: 1.0227575302124023\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.072769641876221 | KNN Loss: 3.0630996227264404 | BCE Loss: 1.0096700191497803\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.090710639953613 | KNN Loss: 3.0787253379821777 | BCE Loss: 1.0119853019714355\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.07977819442749 | KNN Loss: 3.0427346229553223 | BCE Loss: 1.037043571472168\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.120046138763428 | KNN Loss: 3.095799207687378 | BCE Loss: 1.0242469310760498\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.0994720458984375 | KNN Loss: 3.0745911598205566 | BCE Loss: 1.02488112449646\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.081515312194824 | KNN Loss: 3.057558059692383 | BCE Loss: 1.0239572525024414\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.131405830383301 | KNN Loss: 3.0853192806243896 | BCE Loss: 1.0460865497589111\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.1373796463012695 | KNN Loss: 3.1047983169555664 | BCE Loss: 1.0325813293457031\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.117412567138672 | KNN Loss: 3.0766749382019043 | BCE Loss: 1.040737509727478\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.115289688110352 | KNN Loss: 3.0757181644439697 | BCE Loss: 1.0395712852478027\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.095734119415283 | KNN Loss: 3.0416431427001953 | BCE Loss: 1.0540910959243774\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.077699661254883 | KNN Loss: 3.0641367435455322 | BCE Loss: 1.0135631561279297\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.0325117111206055 | KNN Loss: 3.0192434787750244 | BCE Loss: 1.013267993927002\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.055530548095703 | KNN Loss: 3.037489652633667 | BCE Loss: 1.018040657043457\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.098174571990967 | KNN Loss: 3.0909085273742676 | BCE Loss: 1.0072660446166992\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.126721382141113 | KNN Loss: 3.065455675125122 | BCE Loss: 1.0612659454345703\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.090821266174316 | KNN Loss: 3.058767795562744 | BCE Loss: 1.0320534706115723\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.055507659912109 | KNN Loss: 3.044970989227295 | BCE Loss: 1.0105369091033936\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.097704887390137 | KNN Loss: 3.084376335144043 | BCE Loss: 1.0133285522460938\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.137892246246338 | KNN Loss: 3.1036136150360107 | BCE Loss: 1.0342786312103271\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.100736141204834 | KNN Loss: 3.0531842708587646 | BCE Loss: 1.0475518703460693\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.177098274230957 | KNN Loss: 3.11020565032959 | BCE Loss: 1.066892385482788\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.092700958251953 | KNN Loss: 3.0825717449188232 | BCE Loss: 1.010129451751709\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.099493026733398 | KNN Loss: 3.062798261642456 | BCE Loss: 1.0366950035095215\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.083332061767578 | KNN Loss: 3.076486349105835 | BCE Loss: 1.006845474243164\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.080873489379883 | KNN Loss: 3.0557117462158203 | BCE Loss: 1.0251619815826416\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.069711685180664 | KNN Loss: 3.055072784423828 | BCE Loss: 1.0146386623382568\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.0619025230407715 | KNN Loss: 3.0405242443084717 | BCE Loss: 1.0213782787322998\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.081005096435547 | KNN Loss: 3.0538203716278076 | BCE Loss: 1.0271847248077393\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.101498603820801 | KNN Loss: 3.073972225189209 | BCE Loss: 1.0275262594223022\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.121767044067383 | KNN Loss: 3.0609874725341797 | BCE Loss: 1.0607794523239136\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.069756507873535 | KNN Loss: 3.039076089859009 | BCE Loss: 1.0306802988052368\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.112751007080078 | KNN Loss: 3.0801942348480225 | BCE Loss: 1.0325565338134766\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.110785007476807 | KNN Loss: 3.0645785331726074 | BCE Loss: 1.0462064743041992\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.072741508483887 | KNN Loss: 3.0676801204681396 | BCE Loss: 1.005061149597168\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.066131591796875 | KNN Loss: 3.064443588256836 | BCE Loss: 1.0016882419586182\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.034816741943359 | KNN Loss: 3.0382614135742188 | BCE Loss: 0.996555507183075\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.050872325897217 | KNN Loss: 3.0402705669403076 | BCE Loss: 1.0106017589569092\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.09684944152832 | KNN Loss: 3.070158004760742 | BCE Loss: 1.0266915559768677\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.024342060089111 | KNN Loss: 3.025623321533203 | BCE Loss: 0.9987188577651978\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.0653276443481445 | KNN Loss: 3.0497684478759766 | BCE Loss: 1.0155589580535889\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.122068405151367 | KNN Loss: 3.0784316062927246 | BCE Loss: 1.0436365604400635\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.121698379516602 | KNN Loss: 3.0662050247192383 | BCE Loss: 1.0554935932159424\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.062748432159424 | KNN Loss: 3.048140048980713 | BCE Loss: 1.014608383178711\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.0589399337768555 | KNN Loss: 3.0571134090423584 | BCE Loss: 1.001826524734497\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.102237224578857 | KNN Loss: 3.0740766525268555 | BCE Loss: 1.028160572052002\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.068150520324707 | KNN Loss: 3.0620036125183105 | BCE Loss: 1.006146788597107\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.092153549194336 | KNN Loss: 3.092209577560425 | BCE Loss: 0.9999440312385559\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.073697090148926 | KNN Loss: 3.0339510440826416 | BCE Loss: 1.039745807647705\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.0888142585754395 | KNN Loss: 3.0512702465057373 | BCE Loss: 1.0375440120697021\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.109720706939697 | KNN Loss: 3.0721871852874756 | BCE Loss: 1.0375334024429321\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.064558506011963 | KNN Loss: 3.023050546646118 | BCE Loss: 1.0415078401565552\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.079126358032227 | KNN Loss: 3.048699378967285 | BCE Loss: 1.0304267406463623\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.060920715332031 | KNN Loss: 3.061746835708618 | BCE Loss: 0.999173641204834\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.110093593597412 | KNN Loss: 3.06516432762146 | BCE Loss: 1.0449292659759521\n",
      "Epoch   277: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.028415679931641 | KNN Loss: 3.028644561767578 | BCE Loss: 0.999771237373352\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.117685317993164 | KNN Loss: 3.077012538909912 | BCE Loss: 1.040673017501831\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.050290107727051 | KNN Loss: 3.031473159790039 | BCE Loss: 1.0188168287277222\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.0788116455078125 | KNN Loss: 3.052558660507202 | BCE Loss: 1.0262527465820312\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.164350509643555 | KNN Loss: 3.108194589614868 | BCE Loss: 1.0561556816101074\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.132560729980469 | KNN Loss: 3.0872457027435303 | BCE Loss: 1.0453147888183594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.0761518478393555 | KNN Loss: 3.080254554748535 | BCE Loss: 0.9958970546722412\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.099803924560547 | KNN Loss: 3.07232928276062 | BCE Loss: 1.0274745225906372\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.094264030456543 | KNN Loss: 3.070964813232422 | BCE Loss: 1.0232994556427002\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.142199516296387 | KNN Loss: 3.0860135555267334 | BCE Loss: 1.0561859607696533\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.100869178771973 | KNN Loss: 3.0912442207336426 | BCE Loss: 1.0096250772476196\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.159628868103027 | KNN Loss: 3.0965328216552734 | BCE Loss: 1.0630958080291748\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.076394081115723 | KNN Loss: 3.064857006072998 | BCE Loss: 1.0115370750427246\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.083259582519531 | KNN Loss: 3.068960428237915 | BCE Loss: 1.0142991542816162\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.124169826507568 | KNN Loss: 3.079528331756592 | BCE Loss: 1.0446414947509766\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.056919574737549 | KNN Loss: 3.0564680099487305 | BCE Loss: 1.0004515647888184\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.090162754058838 | KNN Loss: 3.051870822906494 | BCE Loss: 1.0382919311523438\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.1221466064453125 | KNN Loss: 3.0887539386749268 | BCE Loss: 1.0333929061889648\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.092582702636719 | KNN Loss: 3.0463004112243652 | BCE Loss: 1.046282172203064\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.097497940063477 | KNN Loss: 3.0646588802337646 | BCE Loss: 1.032839059829712\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.125804901123047 | KNN Loss: 3.0766077041625977 | BCE Loss: 1.0491973161697388\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.082320213317871 | KNN Loss: 3.0590579509735107 | BCE Loss: 1.0232620239257812\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.045245170593262 | KNN Loss: 3.035400390625 | BCE Loss: 1.0098450183868408\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.107070446014404 | KNN Loss: 3.073606491088867 | BCE Loss: 1.0334638357162476\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.12186861038208 | KNN Loss: 3.0638234615325928 | BCE Loss: 1.0580451488494873\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.086021423339844 | KNN Loss: 3.0567684173583984 | BCE Loss: 1.0292530059814453\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.059145927429199 | KNN Loss: 3.0510334968566895 | BCE Loss: 1.0081121921539307\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.071902751922607 | KNN Loss: 3.0459041595458984 | BCE Loss: 1.025998592376709\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.073411464691162 | KNN Loss: 3.0177135467529297 | BCE Loss: 1.0556977987289429\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.0841264724731445 | KNN Loss: 3.0520615577697754 | BCE Loss: 1.0320650339126587\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.050023555755615 | KNN Loss: 3.0384252071380615 | BCE Loss: 1.0115983486175537\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.159502983093262 | KNN Loss: 3.120983600616455 | BCE Loss: 1.0385193824768066\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.072404861450195 | KNN Loss: 3.051091194152832 | BCE Loss: 1.0213136672973633\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.172968864440918 | KNN Loss: 3.1164236068725586 | BCE Loss: 1.0565450191497803\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.144780158996582 | KNN Loss: 3.1015689373016357 | BCE Loss: 1.0432111024856567\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.1073808670043945 | KNN Loss: 3.0606892108917236 | BCE Loss: 1.0466917753219604\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.105500221252441 | KNN Loss: 3.0731658935546875 | BCE Loss: 1.032334327697754\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.097136497497559 | KNN Loss: 3.0897936820983887 | BCE Loss: 1.0073429346084595\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.133100509643555 | KNN Loss: 3.1108806133270264 | BCE Loss: 1.0222197771072388\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.140186309814453 | KNN Loss: 3.091209650039673 | BCE Loss: 1.0489766597747803\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.094467639923096 | KNN Loss: 3.061755657196045 | BCE Loss: 1.0327121019363403\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.081501007080078 | KNN Loss: 3.0648560523986816 | BCE Loss: 1.0166447162628174\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.071280002593994 | KNN Loss: 3.0616307258605957 | BCE Loss: 1.009649395942688\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.138040065765381 | KNN Loss: 3.0987086296081543 | BCE Loss: 1.0393314361572266\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.05467414855957 | KNN Loss: 3.03523588180542 | BCE Loss: 1.0194380283355713\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.1013288497924805 | KNN Loss: 3.0731685161590576 | BCE Loss: 1.0281600952148438\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.133111953735352 | KNN Loss: 3.104865074157715 | BCE Loss: 1.0282466411590576\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.102536201477051 | KNN Loss: 3.0736474990844727 | BCE Loss: 1.0288889408111572\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.033493518829346 | KNN Loss: 3.043657064437866 | BCE Loss: 0.9898366332054138\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.1559014320373535 | KNN Loss: 3.118124008178711 | BCE Loss: 1.0377774238586426\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.109797477722168 | KNN Loss: 3.0949854850769043 | BCE Loss: 1.0148122310638428\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.1048407554626465 | KNN Loss: 3.0697920322418213 | BCE Loss: 1.0350487232208252\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.059391975402832 | KNN Loss: 3.0529911518096924 | BCE Loss: 1.0064010620117188\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.058310031890869 | KNN Loss: 3.050154685974121 | BCE Loss: 1.008155345916748\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.077083587646484 | KNN Loss: 3.055100440979004 | BCE Loss: 1.0219831466674805\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.116837501525879 | KNN Loss: 3.102411985397339 | BCE Loss: 1.0144257545471191\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.10109806060791 | KNN Loss: 3.058319568634033 | BCE Loss: 1.042778730392456\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.077993869781494 | KNN Loss: 3.051051378250122 | BCE Loss: 1.0269426107406616\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.083745002746582 | KNN Loss: 3.0569581985473633 | BCE Loss: 1.0267865657806396\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.082639694213867 | KNN Loss: 3.0842418670654297 | BCE Loss: 0.9983978271484375\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.103520393371582 | KNN Loss: 3.0691635608673096 | BCE Loss: 1.034356951713562\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.123006820678711 | KNN Loss: 3.094404458999634 | BCE Loss: 1.0286023616790771\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.088026523590088 | KNN Loss: 3.0605568885803223 | BCE Loss: 1.027469515800476\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.074822425842285 | KNN Loss: 3.046271800994873 | BCE Loss: 1.028550386428833\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.097400188446045 | KNN Loss: 3.063422918319702 | BCE Loss: 1.0339771509170532\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.1272873878479 | KNN Loss: 3.075462579727173 | BCE Loss: 1.051824927330017\n",
      "Epoch   288: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.124345779418945 | KNN Loss: 3.0955159664154053 | BCE Loss: 1.0288296937942505\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.110847473144531 | KNN Loss: 3.0678083896636963 | BCE Loss: 1.043039321899414\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.106611251831055 | KNN Loss: 3.059854745864868 | BCE Loss: 1.0467562675476074\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.10495138168335 | KNN Loss: 3.0729012489318848 | BCE Loss: 1.0320500135421753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.085001468658447 | KNN Loss: 3.0719473361968994 | BCE Loss: 1.0130540132522583\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.057371139526367 | KNN Loss: 3.04414701461792 | BCE Loss: 1.0132241249084473\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.0879926681518555 | KNN Loss: 3.0583269596099854 | BCE Loss: 1.0296659469604492\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.10365629196167 | KNN Loss: 3.0687003135681152 | BCE Loss: 1.0349558591842651\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.172995567321777 | KNN Loss: 3.1112582683563232 | BCE Loss: 1.061737060546875\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.088571071624756 | KNN Loss: 3.081333875656128 | BCE Loss: 1.007237195968628\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.082385063171387 | KNN Loss: 3.071139335632324 | BCE Loss: 1.0112459659576416\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.087089538574219 | KNN Loss: 3.076913595199585 | BCE Loss: 1.0101759433746338\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.040392875671387 | KNN Loss: 3.040018081665039 | BCE Loss: 1.0003749132156372\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.147391319274902 | KNN Loss: 3.116028308868408 | BCE Loss: 1.0313631296157837\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.064733505249023 | KNN Loss: 3.055985927581787 | BCE Loss: 1.0087473392486572\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.143835067749023 | KNN Loss: 3.093668222427368 | BCE Loss: 1.0501670837402344\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.139407634735107 | KNN Loss: 3.0923538208007812 | BCE Loss: 1.0470538139343262\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.124082088470459 | KNN Loss: 3.1146302223205566 | BCE Loss: 1.0094518661499023\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.099281311035156 | KNN Loss: 3.066969633102417 | BCE Loss: 1.0323114395141602\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.085916519165039 | KNN Loss: 3.0366551876068115 | BCE Loss: 1.0492610931396484\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.105069160461426 | KNN Loss: 3.0719447135925293 | BCE Loss: 1.0331244468688965\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.132780075073242 | KNN Loss: 3.119734287261963 | BCE Loss: 1.0130455493927002\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.054436683654785 | KNN Loss: 3.043823003768921 | BCE Loss: 1.0106135606765747\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.084125518798828 | KNN Loss: 3.0511186122894287 | BCE Loss: 1.0330069065093994\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.0946455001831055 | KNN Loss: 3.0522403717041016 | BCE Loss: 1.042405366897583\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.072665214538574 | KNN Loss: 3.0518674850463867 | BCE Loss: 1.0207979679107666\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.04350471496582 | KNN Loss: 3.034101963043213 | BCE Loss: 1.0094029903411865\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.119059085845947 | KNN Loss: 3.0989601612091064 | BCE Loss: 1.0200990438461304\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.099310874938965 | KNN Loss: 3.053361654281616 | BCE Loss: 1.0459492206573486\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.115764617919922 | KNN Loss: 3.0776240825653076 | BCE Loss: 1.0381402969360352\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.052206516265869 | KNN Loss: 3.044654607772827 | BCE Loss: 1.0075520277023315\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.078268527984619 | KNN Loss: 3.061429262161255 | BCE Loss: 1.0168391466140747\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.04508638381958 | KNN Loss: 3.031191349029541 | BCE Loss: 1.0138949155807495\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.114589691162109 | KNN Loss: 3.0417473316192627 | BCE Loss: 1.0728424787521362\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.059203624725342 | KNN Loss: 3.0492780208587646 | BCE Loss: 1.0099257230758667\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.082040786743164 | KNN Loss: 3.065711736679077 | BCE Loss: 1.016329288482666\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.134118556976318 | KNN Loss: 3.0604770183563232 | BCE Loss: 1.0736415386199951\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.033273220062256 | KNN Loss: 3.0319855213165283 | BCE Loss: 1.0012876987457275\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.10654878616333 | KNN Loss: 3.095268487930298 | BCE Loss: 1.0112802982330322\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.064438819885254 | KNN Loss: 3.0565333366394043 | BCE Loss: 1.0079057216644287\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.10665225982666 | KNN Loss: 3.077885866165161 | BCE Loss: 1.02876615524292\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.080670356750488 | KNN Loss: 3.058267593383789 | BCE Loss: 1.0224030017852783\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.083967208862305 | KNN Loss: 3.0563948154449463 | BCE Loss: 1.0275723934173584\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.129846572875977 | KNN Loss: 3.0581908226013184 | BCE Loss: 1.0716557502746582\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.104979515075684 | KNN Loss: 3.0798840522766113 | BCE Loss: 1.0250953435897827\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.068020820617676 | KNN Loss: 3.0633018016815186 | BCE Loss: 1.0047187805175781\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.141575813293457 | KNN Loss: 3.1066713333129883 | BCE Loss: 1.0349045991897583\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.025460720062256 | KNN Loss: 3.049237012863159 | BCE Loss: 0.9762238264083862\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.076082229614258 | KNN Loss: 3.0681257247924805 | BCE Loss: 1.0079567432403564\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.124938011169434 | KNN Loss: 3.0901503562927246 | BCE Loss: 1.0347877740859985\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.141590118408203 | KNN Loss: 3.098607301712036 | BCE Loss: 1.0429826974868774\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.084022521972656 | KNN Loss: 3.0784995555877686 | BCE Loss: 1.0055227279663086\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.070850372314453 | KNN Loss: 3.060734510421753 | BCE Loss: 1.0101161003112793\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.084327697753906 | KNN Loss: 3.0636587142944336 | BCE Loss: 1.0206689834594727\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.097170829772949 | KNN Loss: 3.050218105316162 | BCE Loss: 1.0469526052474976\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.125382423400879 | KNN Loss: 3.095449924468994 | BCE Loss: 1.0299324989318848\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.07294225692749 | KNN Loss: 3.055889844894409 | BCE Loss: 1.0170522928237915\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.13079833984375 | KNN Loss: 3.1215810775756836 | BCE Loss: 1.0092172622680664\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.077098846435547 | KNN Loss: 3.0702648162841797 | BCE Loss: 1.0068340301513672\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.0756425857543945 | KNN Loss: 3.0503382682800293 | BCE Loss: 1.0253045558929443\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.105592727661133 | KNN Loss: 3.0814294815063477 | BCE Loss: 1.0241634845733643\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.106249809265137 | KNN Loss: 3.0887320041656494 | BCE Loss: 1.0175178050994873\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.138500213623047 | KNN Loss: 3.0955564975738525 | BCE Loss: 1.0429434776306152\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.065196514129639 | KNN Loss: 3.0517876148223877 | BCE Loss: 1.0134090185165405\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.099534034729004 | KNN Loss: 3.082263469696045 | BCE Loss: 1.017270803451538\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.077259063720703 | KNN Loss: 3.0700221061706543 | BCE Loss: 1.007237195968628\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.125586032867432 | KNN Loss: 3.086658000946045 | BCE Loss: 1.0389280319213867\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.037703037261963 | KNN Loss: 3.038975715637207 | BCE Loss: 0.9987274408340454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.113282680511475 | KNN Loss: 3.076467275619507 | BCE Loss: 1.0368152856826782\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.028761386871338 | KNN Loss: 3.033860445022583 | BCE Loss: 0.9949008226394653\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.061757564544678 | KNN Loss: 3.053492784500122 | BCE Loss: 1.0082647800445557\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.099350929260254 | KNN Loss: 3.045837879180908 | BCE Loss: 1.0535131692886353\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.06881046295166 | KNN Loss: 3.0591232776641846 | BCE Loss: 1.009687066078186\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.076324462890625 | KNN Loss: 3.0659828186035156 | BCE Loss: 1.0103414058685303\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.109277248382568 | KNN Loss: 3.0929689407348633 | BCE Loss: 1.016308307647705\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.06688928604126 | KNN Loss: 3.054436206817627 | BCE Loss: 1.0124530792236328\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.073716640472412 | KNN Loss: 3.0524096488952637 | BCE Loss: 1.0213068723678589\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.099374294281006 | KNN Loss: 3.0711021423339844 | BCE Loss: 1.028272032737732\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.077324390411377 | KNN Loss: 3.069310188293457 | BCE Loss: 1.0080143213272095\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.099098205566406 | KNN Loss: 3.0732264518737793 | BCE Loss: 1.0258715152740479\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.081826686859131 | KNN Loss: 3.074277400970459 | BCE Loss: 1.0075492858886719\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.15484619140625 | KNN Loss: 3.1018807888031006 | BCE Loss: 1.0529654026031494\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.1224164962768555 | KNN Loss: 3.0686213970184326 | BCE Loss: 1.0537950992584229\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.1410017013549805 | KNN Loss: 3.0912983417510986 | BCE Loss: 1.0497031211853027\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.114164352416992 | KNN Loss: 3.0685086250305176 | BCE Loss: 1.0456557273864746\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.070722579956055 | KNN Loss: 3.03806734085083 | BCE Loss: 1.032655119895935\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.0667266845703125 | KNN Loss: 3.0585973262786865 | BCE Loss: 1.008129358291626\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.060225486755371 | KNN Loss: 3.0397636890411377 | BCE Loss: 1.0204620361328125\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.117719650268555 | KNN Loss: 3.0827417373657227 | BCE Loss: 1.0349781513214111\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.012802600860596 | KNN Loss: 3.02325177192688 | BCE Loss: 0.9895506501197815\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.0694756507873535 | KNN Loss: 3.082534074783325 | BCE Loss: 0.9869414567947388\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.143416881561279 | KNN Loss: 3.0914618968963623 | BCE Loss: 1.0519551038742065\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.137286186218262 | KNN Loss: 3.090980052947998 | BCE Loss: 1.0463058948516846\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.108147621154785 | KNN Loss: 3.072523593902588 | BCE Loss: 1.0356242656707764\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.030006408691406 | KNN Loss: 3.027193784713745 | BCE Loss: 1.0028126239776611\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.102658271789551 | KNN Loss: 3.0637967586517334 | BCE Loss: 1.0388615131378174\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.11458158493042 | KNN Loss: 3.076115608215332 | BCE Loss: 1.038465976715088\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.076056480407715 | KNN Loss: 3.068307876586914 | BCE Loss: 1.0077488422393799\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.0458831787109375 | KNN Loss: 3.0247647762298584 | BCE Loss: 1.0211181640625\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.0829010009765625 | KNN Loss: 3.069045066833496 | BCE Loss: 1.0138559341430664\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.081677436828613 | KNN Loss: 3.0580477714538574 | BCE Loss: 1.0236296653747559\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.090993881225586 | KNN Loss: 3.0763561725616455 | BCE Loss: 1.0146374702453613\n",
      "Epoch   305: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.078682899475098 | KNN Loss: 3.071152925491333 | BCE Loss: 1.0075302124023438\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.0929999351501465 | KNN Loss: 3.071931838989258 | BCE Loss: 1.0210680961608887\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.06199312210083 | KNN Loss: 3.0589966773986816 | BCE Loss: 1.0029964447021484\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.1183953285217285 | KNN Loss: 3.0502939224243164 | BCE Loss: 1.068101406097412\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.078676223754883 | KNN Loss: 3.067507028579712 | BCE Loss: 1.01116943359375\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.0526885986328125 | KNN Loss: 3.0416078567504883 | BCE Loss: 1.0110805034637451\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.067907333374023 | KNN Loss: 3.05023193359375 | BCE Loss: 1.0176752805709839\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.075403690338135 | KNN Loss: 3.0823466777801514 | BCE Loss: 0.9930570721626282\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.091096878051758 | KNN Loss: 3.075272798538208 | BCE Loss: 1.0158238410949707\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.082616806030273 | KNN Loss: 3.0717644691467285 | BCE Loss: 1.010852336883545\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.0369181632995605 | KNN Loss: 3.0570123195648193 | BCE Loss: 0.9799059629440308\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.102837562561035 | KNN Loss: 3.066741704940796 | BCE Loss: 1.0360956192016602\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.094466209411621 | KNN Loss: 3.06322979927063 | BCE Loss: 1.0312366485595703\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.071435928344727 | KNN Loss: 3.0721981525421143 | BCE Loss: 0.9992380142211914\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.1427106857299805 | KNN Loss: 3.0802626609802246 | BCE Loss: 1.0624477863311768\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.057852268218994 | KNN Loss: 3.0596272945404053 | BCE Loss: 0.9982251524925232\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.103488922119141 | KNN Loss: 3.0616166591644287 | BCE Loss: 1.0418723821640015\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.0688700675964355 | KNN Loss: 3.0739665031433105 | BCE Loss: 0.9949035048484802\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.087967872619629 | KNN Loss: 3.0761494636535645 | BCE Loss: 1.011818528175354\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.076785087585449 | KNN Loss: 3.0584006309509277 | BCE Loss: 1.0183846950531006\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.108779430389404 | KNN Loss: 3.1028940677642822 | BCE Loss: 1.005885362625122\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.064328193664551 | KNN Loss: 3.061544179916382 | BCE Loss: 1.002784252166748\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.1072564125061035 | KNN Loss: 3.0878515243530273 | BCE Loss: 1.0194048881530762\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.050051212310791 | KNN Loss: 3.0208773612976074 | BCE Loss: 1.0291738510131836\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.086941242218018 | KNN Loss: 3.090506076812744 | BCE Loss: 0.9964351654052734\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.065237998962402 | KNN Loss: 3.0530872344970703 | BCE Loss: 1.0121510028839111\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.125827789306641 | KNN Loss: 3.076122999191284 | BCE Loss: 1.0497047901153564\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.053618907928467 | KNN Loss: 3.045144557952881 | BCE Loss: 1.008474349975586\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.084014892578125 | KNN Loss: 3.0684640407562256 | BCE Loss: 1.015550971031189\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.0634284019470215 | KNN Loss: 3.0433707237243652 | BCE Loss: 1.0200576782226562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.052274703979492 | KNN Loss: 3.04594087600708 | BCE Loss: 1.006333589553833\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.08652925491333 | KNN Loss: 3.0514590740203857 | BCE Loss: 1.0350701808929443\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.111854553222656 | KNN Loss: 3.0568714141845703 | BCE Loss: 1.054983377456665\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.084563732147217 | KNN Loss: 3.060687780380249 | BCE Loss: 1.0238760709762573\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.144804954528809 | KNN Loss: 3.0996556282043457 | BCE Loss: 1.045149564743042\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.053451061248779 | KNN Loss: 3.081650495529175 | BCE Loss: 0.9718003869056702\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.079009056091309 | KNN Loss: 3.051497220993042 | BCE Loss: 1.0275115966796875\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.118320941925049 | KNN Loss: 3.078925132751465 | BCE Loss: 1.039395809173584\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.069605827331543 | KNN Loss: 3.049436092376709 | BCE Loss: 1.0201694965362549\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.1061577796936035 | KNN Loss: 3.056292772293091 | BCE Loss: 1.0498648881912231\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.038499355316162 | KNN Loss: 3.0439300537109375 | BCE Loss: 0.9945694804191589\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.094988822937012 | KNN Loss: 3.052842617034912 | BCE Loss: 1.0421464443206787\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.096471309661865 | KNN Loss: 3.0515151023864746 | BCE Loss: 1.0449562072753906\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.066764831542969 | KNN Loss: 3.052767038345337 | BCE Loss: 1.0139975547790527\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.137566566467285 | KNN Loss: 3.1061272621154785 | BCE Loss: 1.0314394235610962\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.133321762084961 | KNN Loss: 3.077916383743286 | BCE Loss: 1.0554051399230957\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.0616135597229 | KNN Loss: 3.0585434436798096 | BCE Loss: 1.0030699968338013\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.079132556915283 | KNN Loss: 3.0675859451293945 | BCE Loss: 1.0115466117858887\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.0502028465271 | KNN Loss: 3.0444319248199463 | BCE Loss: 1.0057708024978638\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.144894123077393 | KNN Loss: 3.097745418548584 | BCE Loss: 1.047148585319519\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.082282543182373 | KNN Loss: 3.0461208820343018 | BCE Loss: 1.0361616611480713\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.1422224044799805 | KNN Loss: 3.109381675720215 | BCE Loss: 1.0328409671783447\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.0812907218933105 | KNN Loss: 3.065352439880371 | BCE Loss: 1.01593816280365\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.1087327003479 | KNN Loss: 3.067375898361206 | BCE Loss: 1.0413569211959839\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.073079586029053 | KNN Loss: 3.068906784057617 | BCE Loss: 1.004172921180725\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.065935134887695 | KNN Loss: 3.03767991065979 | BCE Loss: 1.0282551050186157\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.111034393310547 | KNN Loss: 3.0883071422576904 | BCE Loss: 1.022727370262146\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.075347900390625 | KNN Loss: 3.0437941551208496 | BCE Loss: 1.0315537452697754\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.149145603179932 | KNN Loss: 3.1129791736602783 | BCE Loss: 1.0361663103103638\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.076361656188965 | KNN Loss: 3.0408008098602295 | BCE Loss: 1.0355606079101562\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.114272594451904 | KNN Loss: 3.0812809467315674 | BCE Loss: 1.032991647720337\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.097646713256836 | KNN Loss: 3.0426547527313232 | BCE Loss: 1.0549917221069336\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.118437767028809 | KNN Loss: 3.103520393371582 | BCE Loss: 1.0149173736572266\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.143498420715332 | KNN Loss: 3.0846850872039795 | BCE Loss: 1.0588133335113525\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.091728687286377 | KNN Loss: 3.0579967498779297 | BCE Loss: 1.0337319374084473\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.083272933959961 | KNN Loss: 3.0439977645874023 | BCE Loss: 1.0392749309539795\n",
      "Epoch   316: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.07403564453125 | KNN Loss: 3.049433708190918 | BCE Loss: 1.0246018171310425\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.137515544891357 | KNN Loss: 3.1120645999908447 | BCE Loss: 1.0254510641098022\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.072011947631836 | KNN Loss: 3.062713146209717 | BCE Loss: 1.00929856300354\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.089375972747803 | KNN Loss: 3.058573007583618 | BCE Loss: 1.0308030843734741\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.083693504333496 | KNN Loss: 3.053617477416992 | BCE Loss: 1.030076026916504\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.066347599029541 | KNN Loss: 3.049966812133789 | BCE Loss: 1.0163809061050415\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.1158881187438965 | KNN Loss: 3.107825756072998 | BCE Loss: 1.008062481880188\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.077117919921875 | KNN Loss: 3.0740158557891846 | BCE Loss: 1.00310218334198\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.071404457092285 | KNN Loss: 3.053650140762329 | BCE Loss: 1.017754077911377\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.064937114715576 | KNN Loss: 3.0570263862609863 | BCE Loss: 1.0079107284545898\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.099209785461426 | KNN Loss: 3.0720574855804443 | BCE Loss: 1.027152180671692\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.065112113952637 | KNN Loss: 3.047192096710205 | BCE Loss: 1.0179197788238525\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.101097106933594 | KNN Loss: 3.054964303970337 | BCE Loss: 1.0461329221725464\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.057730197906494 | KNN Loss: 3.051008701324463 | BCE Loss: 1.0067216157913208\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.058445930480957 | KNN Loss: 3.040527582168579 | BCE Loss: 1.017918586730957\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.089818000793457 | KNN Loss: 3.0786848068237305 | BCE Loss: 1.0111333131790161\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.152806758880615 | KNN Loss: 3.09053111076355 | BCE Loss: 1.062275767326355\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.058416366577148 | KNN Loss: 3.0437049865722656 | BCE Loss: 1.0147111415863037\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.1129608154296875 | KNN Loss: 3.0967581272125244 | BCE Loss: 1.0162029266357422\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.13345193862915 | KNN Loss: 3.0897955894470215 | BCE Loss: 1.043656349182129\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.110535621643066 | KNN Loss: 3.071843385696411 | BCE Loss: 1.0386922359466553\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.09373664855957 | KNN Loss: 3.0786406993865967 | BCE Loss: 1.0150959491729736\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.129401206970215 | KNN Loss: 3.0931241512298584 | BCE Loss: 1.0362772941589355\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.069905757904053 | KNN Loss: 3.063314199447632 | BCE Loss: 1.0065914392471313\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.082144737243652 | KNN Loss: 3.0486295223236084 | BCE Loss: 1.0335149765014648\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.100374221801758 | KNN Loss: 3.06038498878479 | BCE Loss: 1.0399891138076782\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.092160701751709 | KNN Loss: 3.0430610179901123 | BCE Loss: 1.0490996837615967\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.110884189605713 | KNN Loss: 3.1071763038635254 | BCE Loss: 1.003707766532898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.105191230773926 | KNN Loss: 3.1009697914123535 | BCE Loss: 1.0042213201522827\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.101876735687256 | KNN Loss: 3.0505573749542236 | BCE Loss: 1.0513193607330322\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.114068984985352 | KNN Loss: 3.10219407081604 | BCE Loss: 1.0118751525878906\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.086054801940918 | KNN Loss: 3.0530011653900146 | BCE Loss: 1.0330535173416138\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.115144729614258 | KNN Loss: 3.082744836807251 | BCE Loss: 1.032400131225586\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.095075607299805 | KNN Loss: 3.044609308242798 | BCE Loss: 1.0504661798477173\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.0922651290893555 | KNN Loss: 3.0420875549316406 | BCE Loss: 1.0501773357391357\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.108036994934082 | KNN Loss: 3.0895447731018066 | BCE Loss: 1.0184922218322754\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.151856899261475 | KNN Loss: 3.0861005783081055 | BCE Loss: 1.0657563209533691\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.125854969024658 | KNN Loss: 3.087061882019043 | BCE Loss: 1.0387930870056152\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.035398006439209 | KNN Loss: 3.0189714431762695 | BCE Loss: 1.0164265632629395\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.080954551696777 | KNN Loss: 3.070270299911499 | BCE Loss: 1.0106842517852783\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.091533660888672 | KNN Loss: 3.072316884994507 | BCE Loss: 1.019216775894165\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.0722246170043945 | KNN Loss: 3.050325870513916 | BCE Loss: 1.0218987464904785\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.1180596351623535 | KNN Loss: 3.0966525077819824 | BCE Loss: 1.021407127380371\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.102766990661621 | KNN Loss: 3.080562114715576 | BCE Loss: 1.0222046375274658\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.052349090576172 | KNN Loss: 3.058347225189209 | BCE Loss: 0.9940016269683838\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.101569175720215 | KNN Loss: 3.0542919635772705 | BCE Loss: 1.0472772121429443\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.082821846008301 | KNN Loss: 3.0381572246551514 | BCE Loss: 1.044664740562439\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.054112434387207 | KNN Loss: 3.051985740661621 | BCE Loss: 1.002126932144165\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.094554424285889 | KNN Loss: 3.0638387203216553 | BCE Loss: 1.030715823173523\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.104115962982178 | KNN Loss: 3.0729174613952637 | BCE Loss: 1.031198501586914\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.045897483825684 | KNN Loss: 3.0292670726776123 | BCE Loss: 1.0166304111480713\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.108290195465088 | KNN Loss: 3.0891685485839844 | BCE Loss: 1.019121766090393\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.071994781494141 | KNN Loss: 3.0738816261291504 | BCE Loss: 0.9981130957603455\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.096785068511963 | KNN Loss: 3.073920965194702 | BCE Loss: 1.0228641033172607\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.063088417053223 | KNN Loss: 3.054800271987915 | BCE Loss: 1.0082881450653076\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.092956066131592 | KNN Loss: 3.0520834922790527 | BCE Loss: 1.0408724546432495\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.124662399291992 | KNN Loss: 3.0730764865875244 | BCE Loss: 1.0515860319137573\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.109889507293701 | KNN Loss: 3.0829389095306396 | BCE Loss: 1.026950478553772\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.0678911209106445 | KNN Loss: 3.0506110191345215 | BCE Loss: 1.0172803401947021\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.060219764709473 | KNN Loss: 3.0472335815429688 | BCE Loss: 1.012986421585083\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.082993507385254 | KNN Loss: 3.0740129947662354 | BCE Loss: 1.008980393409729\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.066440105438232 | KNN Loss: 3.031658411026001 | BCE Loss: 1.0347816944122314\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.052651405334473 | KNN Loss: 3.0468478202819824 | BCE Loss: 1.0058033466339111\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.050016403198242 | KNN Loss: 3.0585238933563232 | BCE Loss: 0.9914922714233398\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.098491668701172 | KNN Loss: 3.0710649490356445 | BCE Loss: 1.0274269580841064\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.075313568115234 | KNN Loss: 3.0636844635009766 | BCE Loss: 1.011629343032837\n",
      "Epoch   327: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.045129776000977 | KNN Loss: 3.0385124683380127 | BCE Loss: 1.006617546081543\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.0745110511779785 | KNN Loss: 3.0708634853363037 | BCE Loss: 1.0036475658416748\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.068650245666504 | KNN Loss: 3.0552978515625 | BCE Loss: 1.0133521556854248\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.087578773498535 | KNN Loss: 3.058800458908081 | BCE Loss: 1.028778314590454\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.096796989440918 | KNN Loss: 3.0765459537506104 | BCE Loss: 1.0202512741088867\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.083308696746826 | KNN Loss: 3.083287000656128 | BCE Loss: 1.0000216960906982\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.099172115325928 | KNN Loss: 3.059276580810547 | BCE Loss: 1.0398955345153809\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.155965805053711 | KNN Loss: 3.073329448699951 | BCE Loss: 1.0826361179351807\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.171067714691162 | KNN Loss: 3.1134469509124756 | BCE Loss: 1.057620644569397\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.071643829345703 | KNN Loss: 3.0452566146850586 | BCE Loss: 1.0263874530792236\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.081360816955566 | KNN Loss: 3.069833517074585 | BCE Loss: 1.0115275382995605\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.108877182006836 | KNN Loss: 3.073101043701172 | BCE Loss: 1.0357763767242432\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.020322322845459 | KNN Loss: 3.029186487197876 | BCE Loss: 0.9911359548568726\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.092015266418457 | KNN Loss: 3.051567316055298 | BCE Loss: 1.04044771194458\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.090301513671875 | KNN Loss: 3.064330577850342 | BCE Loss: 1.0259711742401123\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.121160507202148 | KNN Loss: 3.1110477447509766 | BCE Loss: 1.010113000869751\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.10081672668457 | KNN Loss: 3.0607099533081055 | BCE Loss: 1.0401066541671753\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.086964130401611 | KNN Loss: 3.067819118499756 | BCE Loss: 1.019145131111145\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.062405586242676 | KNN Loss: 3.0420279502868652 | BCE Loss: 1.0203773975372314\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.124619483947754 | KNN Loss: 3.0741124153137207 | BCE Loss: 1.0505073070526123\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.080975532531738 | KNN Loss: 3.046149969100952 | BCE Loss: 1.0348255634307861\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.075815200805664 | KNN Loss: 3.057286024093628 | BCE Loss: 1.0185294151306152\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.096003532409668 | KNN Loss: 3.0693132877349854 | BCE Loss: 1.0266904830932617\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.074162483215332 | KNN Loss: 3.0534775257110596 | BCE Loss: 1.0206849575042725\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.090256690979004 | KNN Loss: 3.069993734359741 | BCE Loss: 1.0202627182006836\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.081239700317383 | KNN Loss: 3.0485148429870605 | BCE Loss: 1.0327250957489014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.063437461853027 | KNN Loss: 3.0654261112213135 | BCE Loss: 0.9980112314224243\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.056706428527832 | KNN Loss: 3.037548065185547 | BCE Loss: 1.0191584825515747\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.1034955978393555 | KNN Loss: 3.0682175159454346 | BCE Loss: 1.035278081893921\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.107128143310547 | KNN Loss: 3.076261281967163 | BCE Loss: 1.0308668613433838\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.061897277832031 | KNN Loss: 3.035414934158325 | BCE Loss: 1.0264822244644165\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.095222473144531 | KNN Loss: 3.063845634460449 | BCE Loss: 1.031376838684082\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.0807976722717285 | KNN Loss: 3.0590994358062744 | BCE Loss: 1.0216981172561646\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.1414971351623535 | KNN Loss: 3.107053756713867 | BCE Loss: 1.0344434976577759\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.044330596923828 | KNN Loss: 3.0357158184051514 | BCE Loss: 1.0086150169372559\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.116820812225342 | KNN Loss: 3.0840072631835938 | BCE Loss: 1.0328136682510376\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.091360569000244 | KNN Loss: 3.0619518756866455 | BCE Loss: 1.0294086933135986\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.061488151550293 | KNN Loss: 3.0627403259277344 | BCE Loss: 0.9987480044364929\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.06221342086792 | KNN Loss: 3.0452980995178223 | BCE Loss: 1.0169153213500977\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.0891523361206055 | KNN Loss: 3.0488533973693848 | BCE Loss: 1.0402989387512207\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.104035377502441 | KNN Loss: 3.0741803646087646 | BCE Loss: 1.0298550128936768\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.0806169509887695 | KNN Loss: 3.0587589740753174 | BCE Loss: 1.0218582153320312\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.091196060180664 | KNN Loss: 3.059130907058716 | BCE Loss: 1.0320653915405273\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.099433898925781 | KNN Loss: 3.0810351371765137 | BCE Loss: 1.0183987617492676\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.078714847564697 | KNN Loss: 3.028280019760132 | BCE Loss: 1.0504348278045654\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.091766357421875 | KNN Loss: 3.0705723762512207 | BCE Loss: 1.0211939811706543\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.044205665588379 | KNN Loss: 3.0477561950683594 | BCE Loss: 0.9964492917060852\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.035992622375488 | KNN Loss: 3.004840850830078 | BCE Loss: 1.031151533126831\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.054470062255859 | KNN Loss: 3.0551085472106934 | BCE Loss: 0.999361515045166\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.103829383850098 | KNN Loss: 3.070033311843872 | BCE Loss: 1.0337963104248047\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.12003231048584 | KNN Loss: 3.1043052673339844 | BCE Loss: 1.0157272815704346\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.074126243591309 | KNN Loss: 3.0993599891662598 | BCE Loss: 0.9747661352157593\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.068324089050293 | KNN Loss: 3.0428836345672607 | BCE Loss: 1.0254404544830322\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.09286642074585 | KNN Loss: 3.0499026775360107 | BCE Loss: 1.0429638624191284\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.1158294677734375 | KNN Loss: 3.0895862579345703 | BCE Loss: 1.0262432098388672\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.114592552185059 | KNN Loss: 3.077120304107666 | BCE Loss: 1.0374720096588135\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.115826606750488 | KNN Loss: 3.1009182929992676 | BCE Loss: 1.0149083137512207\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.130865573883057 | KNN Loss: 3.100485324859619 | BCE Loss: 1.030380129814148\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.044050216674805 | KNN Loss: 3.060107707977295 | BCE Loss: 0.9839422702789307\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.13723087310791 | KNN Loss: 3.1084518432617188 | BCE Loss: 1.0287790298461914\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.081159591674805 | KNN Loss: 3.0716185569763184 | BCE Loss: 1.0095410346984863\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.064610481262207 | KNN Loss: 3.040865182876587 | BCE Loss: 1.0237455368041992\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.126428127288818 | KNN Loss: 3.0919783115386963 | BCE Loss: 1.0344496965408325\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.0779314041137695 | KNN Loss: 3.053619146347046 | BCE Loss: 1.0243120193481445\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.07058048248291 | KNN Loss: 3.0489206314086914 | BCE Loss: 1.0216600894927979\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.127586841583252 | KNN Loss: 3.085268259048462 | BCE Loss: 1.0423187017440796\n",
      "Epoch   338: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.141047477722168 | KNN Loss: 3.0944695472717285 | BCE Loss: 1.0465776920318604\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.091327667236328 | KNN Loss: 3.0745365619659424 | BCE Loss: 1.0167911052703857\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.119887351989746 | KNN Loss: 3.096567153930664 | BCE Loss: 1.0233204364776611\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.102097511291504 | KNN Loss: 3.081017255783081 | BCE Loss: 1.021080493927002\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.130819320678711 | KNN Loss: 3.1031720638275146 | BCE Loss: 1.0276473760604858\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.054171562194824 | KNN Loss: 3.0350146293640137 | BCE Loss: 1.0191571712493896\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.157315254211426 | KNN Loss: 3.1063313484191895 | BCE Loss: 1.0509837865829468\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.0717973709106445 | KNN Loss: 3.059988021850586 | BCE Loss: 1.011809229850769\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.115031719207764 | KNN Loss: 3.0546648502349854 | BCE Loss: 1.0603669881820679\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.065919399261475 | KNN Loss: 3.059227466583252 | BCE Loss: 1.006691813468933\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.066900730133057 | KNN Loss: 3.048201322555542 | BCE Loss: 1.0186994075775146\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.0487542152404785 | KNN Loss: 3.0212790966033936 | BCE Loss: 1.027475118637085\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.070990562438965 | KNN Loss: 3.064939260482788 | BCE Loss: 1.0060513019561768\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.088535785675049 | KNN Loss: 3.0433170795440674 | BCE Loss: 1.045218825340271\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.114985466003418 | KNN Loss: 3.099623203277588 | BCE Loss: 1.01536226272583\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.071297645568848 | KNN Loss: 3.0497257709503174 | BCE Loss: 1.0215721130371094\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.075852394104004 | KNN Loss: 3.068085193634033 | BCE Loss: 1.0077674388885498\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.118321418762207 | KNN Loss: 3.08378529548645 | BCE Loss: 1.0345361232757568\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.109889030456543 | KNN Loss: 3.083418369293213 | BCE Loss: 1.0264705419540405\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.083889961242676 | KNN Loss: 3.053497791290283 | BCE Loss: 1.0303921699523926\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.108537673950195 | KNN Loss: 3.0757243633270264 | BCE Loss: 1.032813310623169\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.0636887550354 | KNN Loss: 3.0523641109466553 | BCE Loss: 1.0113245248794556\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.09950065612793 | KNN Loss: 3.080129623413086 | BCE Loss: 1.0193707942962646\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.075658798217773 | KNN Loss: 3.0690295696258545 | BCE Loss: 1.006629228591919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.050844669342041 | KNN Loss: 3.044635057449341 | BCE Loss: 1.0062096118927002\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.074559688568115 | KNN Loss: 3.0337564945220947 | BCE Loss: 1.04080331325531\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.130638599395752 | KNN Loss: 3.078523635864258 | BCE Loss: 1.0521150827407837\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.038408279418945 | KNN Loss: 3.057567596435547 | BCE Loss: 0.9808404445648193\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.0691142082214355 | KNN Loss: 3.0593366622924805 | BCE Loss: 1.0097774267196655\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.109340667724609 | KNN Loss: 3.075244188308716 | BCE Loss: 1.0340962409973145\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.058300018310547 | KNN Loss: 3.0556116104125977 | BCE Loss: 1.0026881694793701\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.112673759460449 | KNN Loss: 3.0901453495025635 | BCE Loss: 1.0225286483764648\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.066108226776123 | KNN Loss: 3.058898448944092 | BCE Loss: 1.0072097778320312\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.112574100494385 | KNN Loss: 3.081211566925049 | BCE Loss: 1.031362533569336\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.127987861633301 | KNN Loss: 3.0870201587677 | BCE Loss: 1.0409679412841797\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.15177059173584 | KNN Loss: 3.1039998531341553 | BCE Loss: 1.0477709770202637\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.10183048248291 | KNN Loss: 3.0727951526641846 | BCE Loss: 1.0290353298187256\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.071516990661621 | KNN Loss: 3.058809995651245 | BCE Loss: 1.0127068758010864\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.088315486907959 | KNN Loss: 3.0673320293426514 | BCE Loss: 1.020983338356018\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.092750549316406 | KNN Loss: 3.0728847980499268 | BCE Loss: 1.0198655128479004\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.102506637573242 | KNN Loss: 3.079244375228882 | BCE Loss: 1.0232620239257812\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.108726501464844 | KNN Loss: 3.0549814701080322 | BCE Loss: 1.0537450313568115\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.122543811798096 | KNN Loss: 3.1077322959899902 | BCE Loss: 1.0148115158081055\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.101387023925781 | KNN Loss: 3.066617488861084 | BCE Loss: 1.0347694158554077\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.023980140686035 | KNN Loss: 3.0280401706695557 | BCE Loss: 0.9959401488304138\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.085565090179443 | KNN Loss: 3.0830869674682617 | BCE Loss: 1.0024781227111816\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.090336799621582 | KNN Loss: 3.0578181743621826 | BCE Loss: 1.0325185060501099\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.091350555419922 | KNN Loss: 3.0586118698120117 | BCE Loss: 1.0327386856079102\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.061622619628906 | KNN Loss: 3.0529539585113525 | BCE Loss: 1.0086684226989746\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.0979461669921875 | KNN Loss: 3.0835065841674805 | BCE Loss: 1.014439344406128\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.122822284698486 | KNN Loss: 3.068166971206665 | BCE Loss: 1.0546554327011108\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.0958733558654785 | KNN Loss: 3.0617682933807373 | BCE Loss: 1.0341051816940308\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.079849720001221 | KNN Loss: 3.0590646266937256 | BCE Loss: 1.0207849740982056\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.048676013946533 | KNN Loss: 3.0405235290527344 | BCE Loss: 1.0081524848937988\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.016756057739258 | KNN Loss: 3.0005342960357666 | BCE Loss: 1.0162217617034912\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.068438529968262 | KNN Loss: 3.06684947013855 | BCE Loss: 1.0015888214111328\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.122603416442871 | KNN Loss: 3.0875515937805176 | BCE Loss: 1.0350518226623535\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.035463809967041 | KNN Loss: 3.014294147491455 | BCE Loss: 1.021169662475586\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.063948631286621 | KNN Loss: 3.053800344467163 | BCE Loss: 1.010148048400879\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.062745571136475 | KNN Loss: 3.0352234840393066 | BCE Loss: 1.027522087097168\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.076544284820557 | KNN Loss: 3.058476209640503 | BCE Loss: 1.0180680751800537\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.069405555725098 | KNN Loss: 3.047926902770996 | BCE Loss: 1.021478533744812\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.0373029708862305 | KNN Loss: 3.0610930919647217 | BCE Loss: 0.9762099981307983\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.096710205078125 | KNN Loss: 3.065180540084839 | BCE Loss: 1.0315297842025757\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.059545516967773 | KNN Loss: 3.048973798751831 | BCE Loss: 1.0105717182159424\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.131147861480713 | KNN Loss: 3.1014466285705566 | BCE Loss: 1.0297012329101562\n",
      "Epoch   349: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.027567386627197 | KNN Loss: 3.0254061222076416 | BCE Loss: 1.0021611452102661\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.149845123291016 | KNN Loss: 3.082059144973755 | BCE Loss: 1.0677857398986816\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.081735610961914 | KNN Loss: 3.067406415939331 | BCE Loss: 1.014328956604004\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.088061809539795 | KNN Loss: 3.0731570720672607 | BCE Loss: 1.0149047374725342\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.09288215637207 | KNN Loss: 3.0748953819274902 | BCE Loss: 1.01798677444458\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.066938400268555 | KNN Loss: 3.031749963760376 | BCE Loss: 1.0351885557174683\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.056831359863281 | KNN Loss: 3.026423215866089 | BCE Loss: 1.0304080247879028\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.145480632781982 | KNN Loss: 3.083806276321411 | BCE Loss: 1.0616743564605713\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.074362754821777 | KNN Loss: 3.0709311962127686 | BCE Loss: 1.0034313201904297\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.062226295471191 | KNN Loss: 3.028641939163208 | BCE Loss: 1.0335841178894043\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.087762355804443 | KNN Loss: 3.0733420848846436 | BCE Loss: 1.0144202709197998\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.104143142700195 | KNN Loss: 3.07785964012146 | BCE Loss: 1.0262832641601562\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.089567184448242 | KNN Loss: 3.08518123626709 | BCE Loss: 1.0043858289718628\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.144545555114746 | KNN Loss: 3.11470890045166 | BCE Loss: 1.029836654663086\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.127964019775391 | KNN Loss: 3.081041097640991 | BCE Loss: 1.046923041343689\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.1331939697265625 | KNN Loss: 3.0942254066467285 | BCE Loss: 1.038968563079834\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.10421085357666 | KNN Loss: 3.0762805938720703 | BCE Loss: 1.0279302597045898\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.137702941894531 | KNN Loss: 3.1047627925872803 | BCE Loss: 1.0329399108886719\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.1590142250061035 | KNN Loss: 3.1383001804351807 | BCE Loss: 1.0207140445709229\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.049366474151611 | KNN Loss: 3.051953077316284 | BCE Loss: 0.9974135160446167\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.128541946411133 | KNN Loss: 3.0799143314361572 | BCE Loss: 1.0486273765563965\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.087844371795654 | KNN Loss: 3.0541887283325195 | BCE Loss: 1.0336557626724243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.095426082611084 | KNN Loss: 3.068218231201172 | BCE Loss: 1.0272077322006226\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.061656951904297 | KNN Loss: 3.052975654602051 | BCE Loss: 1.008681297302246\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.066148281097412 | KNN Loss: 3.057011127471924 | BCE Loss: 1.0091372728347778\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.118610382080078 | KNN Loss: 3.0762710571289062 | BCE Loss: 1.0423392057418823\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.117094039916992 | KNN Loss: 3.0718977451324463 | BCE Loss: 1.045196294784546\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.107043266296387 | KNN Loss: 3.105374813079834 | BCE Loss: 1.0016683340072632\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.0703887939453125 | KNN Loss: 3.075707197189331 | BCE Loss: 0.9946817755699158\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.073463439941406 | KNN Loss: 3.07967209815979 | BCE Loss: 0.9937911033630371\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.125600814819336 | KNN Loss: 3.1064400672912598 | BCE Loss: 1.0191608667373657\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.113609313964844 | KNN Loss: 3.0664825439453125 | BCE Loss: 1.0471270084381104\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.071595191955566 | KNN Loss: 3.071460723876953 | BCE Loss: 1.0001344680786133\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.051233768463135 | KNN Loss: 3.0470783710479736 | BCE Loss: 1.0041553974151611\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.086188316345215 | KNN Loss: 3.0658318996429443 | BCE Loss: 1.0203561782836914\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.140448093414307 | KNN Loss: 3.077702760696411 | BCE Loss: 1.0627453327178955\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.07114315032959 | KNN Loss: 3.06371808052063 | BCE Loss: 1.007425308227539\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.094037055969238 | KNN Loss: 3.078216314315796 | BCE Loss: 1.0158207416534424\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.048946857452393 | KNN Loss: 3.04128098487854 | BCE Loss: 1.0076658725738525\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.090840816497803 | KNN Loss: 3.0780837535858154 | BCE Loss: 1.0127570629119873\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.102397918701172 | KNN Loss: 3.0738840103149414 | BCE Loss: 1.0285141468048096\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.055004119873047 | KNN Loss: 3.032764434814453 | BCE Loss: 1.0222399234771729\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.092476844787598 | KNN Loss: 3.086362600326538 | BCE Loss: 1.0061144828796387\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.079370498657227 | KNN Loss: 3.0375938415527344 | BCE Loss: 1.0417766571044922\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.0896735191345215 | KNN Loss: 3.0476784706115723 | BCE Loss: 1.0419949293136597\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.095084190368652 | KNN Loss: 3.0834178924560547 | BCE Loss: 1.0116662979125977\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.05063009262085 | KNN Loss: 3.0569891929626465 | BCE Loss: 0.9936408996582031\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.122886657714844 | KNN Loss: 3.074201822280884 | BCE Loss: 1.0486845970153809\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.1358208656311035 | KNN Loss: 3.079848527908325 | BCE Loss: 1.0559723377227783\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.126770973205566 | KNN Loss: 3.0700316429138184 | BCE Loss: 1.056739330291748\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.080211162567139 | KNN Loss: 3.0623161792755127 | BCE Loss: 1.0178948640823364\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.058441162109375 | KNN Loss: 3.0536513328552246 | BCE Loss: 1.0047898292541504\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.086913585662842 | KNN Loss: 3.0698883533477783 | BCE Loss: 1.017025351524353\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.0911455154418945 | KNN Loss: 3.058515787124634 | BCE Loss: 1.0326299667358398\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.080964088439941 | KNN Loss: 3.0629091262817383 | BCE Loss: 1.0180550813674927\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.1285529136657715 | KNN Loss: 3.0899460315704346 | BCE Loss: 1.0386067628860474\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.092560768127441 | KNN Loss: 3.0291712284088135 | BCE Loss: 1.063389539718628\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.0748701095581055 | KNN Loss: 3.04343581199646 | BCE Loss: 1.0314340591430664\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.0860795974731445 | KNN Loss: 3.0491487979888916 | BCE Loss: 1.036930799484253\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.092593193054199 | KNN Loss: 3.0553925037384033 | BCE Loss: 1.037200927734375\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.070446968078613 | KNN Loss: 3.058561086654663 | BCE Loss: 1.0118857622146606\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.041390419006348 | KNN Loss: 3.0413928031921387 | BCE Loss: 0.9999973773956299\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.085392951965332 | KNN Loss: 3.0827744007110596 | BCE Loss: 1.0026183128356934\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.129709243774414 | KNN Loss: 3.093848943710327 | BCE Loss: 1.0358604192733765\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.088295936584473 | KNN Loss: 3.065053701400757 | BCE Loss: 1.0232421159744263\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.078110218048096 | KNN Loss: 3.062244176864624 | BCE Loss: 1.0158659219741821\n",
      "Epoch   360: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.087418556213379 | KNN Loss: 3.047527313232422 | BCE Loss: 1.039891242980957\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.117311000823975 | KNN Loss: 3.0655057430267334 | BCE Loss: 1.0518052577972412\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.093296051025391 | KNN Loss: 3.0586352348327637 | BCE Loss: 1.034661054611206\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.077068328857422 | KNN Loss: 3.042985439300537 | BCE Loss: 1.0340828895568848\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.066466808319092 | KNN Loss: 3.0666751861572266 | BCE Loss: 0.99979168176651\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.106999397277832 | KNN Loss: 3.0955982208251953 | BCE Loss: 1.0114014148712158\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.116957664489746 | KNN Loss: 3.086452007293701 | BCE Loss: 1.030505657196045\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.077934265136719 | KNN Loss: 3.06048583984375 | BCE Loss: 1.0174485445022583\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.094508647918701 | KNN Loss: 3.0543770790100098 | BCE Loss: 1.0401315689086914\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 3.996397018432617 | KNN Loss: 3.0060291290283203 | BCE Loss: 0.9903680086135864\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.045534133911133 | KNN Loss: 3.0456879138946533 | BCE Loss: 0.9998463988304138\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.068357467651367 | KNN Loss: 3.06375789642334 | BCE Loss: 1.0045993328094482\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.083045959472656 | KNN Loss: 3.053046226501465 | BCE Loss: 1.0299996137619019\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.136636734008789 | KNN Loss: 3.101026773452759 | BCE Loss: 1.0356098413467407\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.104404449462891 | KNN Loss: 3.081458330154419 | BCE Loss: 1.0229461193084717\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.074213027954102 | KNN Loss: 3.0503015518188477 | BCE Loss: 1.023911476135254\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.085492134094238 | KNN Loss: 3.0629396438598633 | BCE Loss: 1.0225526094436646\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.108214378356934 | KNN Loss: 3.0685439109802246 | BCE Loss: 1.039670705795288\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.0706377029418945 | KNN Loss: 3.0332772731781006 | BCE Loss: 1.0373603105545044\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.150312423706055 | KNN Loss: 3.0732951164245605 | BCE Loss: 1.077017068862915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.097957611083984 | KNN Loss: 3.0697274208068848 | BCE Loss: 1.0282303094863892\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.036758899688721 | KNN Loss: 3.024502992630005 | BCE Loss: 1.0122559070587158\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.057812690734863 | KNN Loss: 3.05973744392395 | BCE Loss: 0.9980754852294922\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.093024253845215 | KNN Loss: 3.085183620452881 | BCE Loss: 1.007840871810913\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.045061111450195 | KNN Loss: 3.0348076820373535 | BCE Loss: 1.0102531909942627\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.059871196746826 | KNN Loss: 3.0367536544799805 | BCE Loss: 1.0231174230575562\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.057836532592773 | KNN Loss: 3.048776388168335 | BCE Loss: 1.0090601444244385\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.067967414855957 | KNN Loss: 3.0481982231140137 | BCE Loss: 1.0197689533233643\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.062525749206543 | KNN Loss: 3.0230417251586914 | BCE Loss: 1.0394842624664307\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.110130310058594 | KNN Loss: 3.1024973392486572 | BCE Loss: 1.0076332092285156\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.087879180908203 | KNN Loss: 3.0723838806152344 | BCE Loss: 1.0154950618743896\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.1523942947387695 | KNN Loss: 3.09973406791687 | BCE Loss: 1.0526599884033203\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.082305431365967 | KNN Loss: 3.0399303436279297 | BCE Loss: 1.042375087738037\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.059708595275879 | KNN Loss: 3.0481553077697754 | BCE Loss: 1.011553406715393\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.099570274353027 | KNN Loss: 3.0494415760040283 | BCE Loss: 1.050128698348999\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.088476657867432 | KNN Loss: 3.0640087127685547 | BCE Loss: 1.024467945098877\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.11090087890625 | KNN Loss: 3.070798873901367 | BCE Loss: 1.0401021242141724\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.058757781982422 | KNN Loss: 3.0612173080444336 | BCE Loss: 0.9975407123565674\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.096096992492676 | KNN Loss: 3.080530881881714 | BCE Loss: 1.0155659914016724\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.083746910095215 | KNN Loss: 3.071155548095703 | BCE Loss: 1.0125911235809326\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.09876012802124 | KNN Loss: 3.070214033126831 | BCE Loss: 1.0285460948944092\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.0819573402404785 | KNN Loss: 3.0877034664154053 | BCE Loss: 0.9942538738250732\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.095232963562012 | KNN Loss: 3.0836002826690674 | BCE Loss: 1.0116329193115234\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.091362476348877 | KNN Loss: 3.0985209941864014 | BCE Loss: 0.9928416013717651\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.0706000328063965 | KNN Loss: 3.0623788833618164 | BCE Loss: 1.00822114944458\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.120175838470459 | KNN Loss: 3.067523956298828 | BCE Loss: 1.0526518821716309\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.087488174438477 | KNN Loss: 3.0750701427459717 | BCE Loss: 1.012418270111084\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.037140369415283 | KNN Loss: 3.0266337394714355 | BCE Loss: 1.0105066299438477\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.12437105178833 | KNN Loss: 3.0703237056732178 | BCE Loss: 1.0540473461151123\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.0520339012146 | KNN Loss: 3.056117057800293 | BCE Loss: 0.9959167838096619\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.123257637023926 | KNN Loss: 3.100738286972046 | BCE Loss: 1.0225191116333008\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.029199123382568 | KNN Loss: 3.0287210941314697 | BCE Loss: 1.0004780292510986\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.0513691902160645 | KNN Loss: 3.044891834259033 | BCE Loss: 1.0064774751663208\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.035184860229492 | KNN Loss: 3.0149624347686768 | BCE Loss: 1.0202223062515259\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.098163604736328 | KNN Loss: 3.0863120555877686 | BCE Loss: 1.0118517875671387\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.0657854080200195 | KNN Loss: 3.0404117107391357 | BCE Loss: 1.0253736972808838\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.083273887634277 | KNN Loss: 3.0580005645751953 | BCE Loss: 1.0252734422683716\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.0895586013793945 | KNN Loss: 3.0778818130493164 | BCE Loss: 1.0116767883300781\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.106510162353516 | KNN Loss: 3.0799431800842285 | BCE Loss: 1.026566982269287\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.094712257385254 | KNN Loss: 3.0461716651916504 | BCE Loss: 1.0485408306121826\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.097809791564941 | KNN Loss: 3.0705726146698 | BCE Loss: 1.027237057685852\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.1139655113220215 | KNN Loss: 3.0718624591827393 | BCE Loss: 1.0421029329299927\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.082429885864258 | KNN Loss: 3.042633533477783 | BCE Loss: 1.0397961139678955\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.043069839477539 | KNN Loss: 3.0558507442474365 | BCE Loss: 0.9872193336486816\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.1012678146362305 | KNN Loss: 3.066016435623169 | BCE Loss: 1.035251259803772\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.104835510253906 | KNN Loss: 3.0893383026123047 | BCE Loss: 1.0154973268508911\n",
      "Epoch   371: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.060492038726807 | KNN Loss: 3.0390048027038574 | BCE Loss: 1.0214872360229492\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.114107608795166 | KNN Loss: 3.0754666328430176 | BCE Loss: 1.0386409759521484\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.084921360015869 | KNN Loss: 3.065345525741577 | BCE Loss: 1.019575834274292\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.07265043258667 | KNN Loss: 3.062882423400879 | BCE Loss: 1.0097678899765015\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.085477352142334 | KNN Loss: 3.049717903137207 | BCE Loss: 1.0357593297958374\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.146827697753906 | KNN Loss: 3.113476514816284 | BCE Loss: 1.0333510637283325\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.106801986694336 | KNN Loss: 3.065235137939453 | BCE Loss: 1.0415668487548828\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.081141948699951 | KNN Loss: 3.059835433959961 | BCE Loss: 1.0213065147399902\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.067170143127441 | KNN Loss: 3.0532023906707764 | BCE Loss: 1.013967752456665\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.105152130126953 | KNN Loss: 3.0659899711608887 | BCE Loss: 1.0391623973846436\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.113118648529053 | KNN Loss: 3.1008658409118652 | BCE Loss: 1.012252688407898\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.06604528427124 | KNN Loss: 3.0517730712890625 | BCE Loss: 1.0142720937728882\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.056605815887451 | KNN Loss: 3.032195806503296 | BCE Loss: 1.0244101285934448\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.155619144439697 | KNN Loss: 3.1113460063934326 | BCE Loss: 1.0442731380462646\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.048417568206787 | KNN Loss: 3.046112537384033 | BCE Loss: 1.002305030822754\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.111963748931885 | KNN Loss: 3.0855863094329834 | BCE Loss: 1.026377558708191\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.074891090393066 | KNN Loss: 3.0685079097747803 | BCE Loss: 1.0063834190368652\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.093433380126953 | KNN Loss: 3.0473670959472656 | BCE Loss: 1.0460660457611084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.09586238861084 | KNN Loss: 3.044299840927124 | BCE Loss: 1.0515625476837158\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.114771842956543 | KNN Loss: 3.099344491958618 | BCE Loss: 1.0154271125793457\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.0606184005737305 | KNN Loss: 3.047515392303467 | BCE Loss: 1.0131032466888428\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.095880508422852 | KNN Loss: 3.042097568511963 | BCE Loss: 1.0537828207015991\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.054931163787842 | KNN Loss: 3.0412046909332275 | BCE Loss: 1.0137264728546143\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.100155353546143 | KNN Loss: 3.0954654216766357 | BCE Loss: 1.0046899318695068\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.071311950683594 | KNN Loss: 3.040191411972046 | BCE Loss: 1.0311205387115479\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.074844837188721 | KNN Loss: 3.06019926071167 | BCE Loss: 1.0146454572677612\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.107029438018799 | KNN Loss: 3.093579053878784 | BCE Loss: 1.0134503841400146\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.065061569213867 | KNN Loss: 3.0580575466156006 | BCE Loss: 1.0070037841796875\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.12774133682251 | KNN Loss: 3.0935614109039307 | BCE Loss: 1.0341800451278687\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.128600120544434 | KNN Loss: 3.09631085395813 | BCE Loss: 1.0322891473770142\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.076756954193115 | KNN Loss: 3.0374209880828857 | BCE Loss: 1.0393359661102295\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.0557403564453125 | KNN Loss: 3.0678036212921143 | BCE Loss: 0.9879369735717773\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.064676761627197 | KNN Loss: 3.0434210300445557 | BCE Loss: 1.0212557315826416\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.072625160217285 | KNN Loss: 3.0657615661621094 | BCE Loss: 1.0068635940551758\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.084775447845459 | KNN Loss: 3.0480868816375732 | BCE Loss: 1.0366885662078857\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.104269027709961 | KNN Loss: 3.0660481452941895 | BCE Loss: 1.0382208824157715\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.108795642852783 | KNN Loss: 3.077831745147705 | BCE Loss: 1.0309638977050781\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.128260612487793 | KNN Loss: 3.1044814586639404 | BCE Loss: 1.0237789154052734\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.1271281242370605 | KNN Loss: 3.0871288776397705 | BCE Loss: 1.0399993658065796\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.101057052612305 | KNN Loss: 3.0690927505493164 | BCE Loss: 1.0319640636444092\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.12190055847168 | KNN Loss: 3.0786681175231934 | BCE Loss: 1.0432325601577759\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.116560935974121 | KNN Loss: 3.0766382217407227 | BCE Loss: 1.0399224758148193\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.105991363525391 | KNN Loss: 3.0645785331726074 | BCE Loss: 1.0414130687713623\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.152298450469971 | KNN Loss: 3.0942606925964355 | BCE Loss: 1.0580377578735352\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.070981025695801 | KNN Loss: 3.054565191268921 | BCE Loss: 1.0164159536361694\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.050491809844971 | KNN Loss: 3.046260356903076 | BCE Loss: 1.004231572151184\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.116326332092285 | KNN Loss: 3.0786209106445312 | BCE Loss: 1.037705659866333\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.05034875869751 | KNN Loss: 3.0473597049713135 | BCE Loss: 1.0029890537261963\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.042548656463623 | KNN Loss: 3.0499424934387207 | BCE Loss: 0.9926061034202576\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.067441940307617 | KNN Loss: 3.052744150161743 | BCE Loss: 1.0146980285644531\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.047364234924316 | KNN Loss: 3.0605127811431885 | BCE Loss: 0.9868515729904175\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.113617420196533 | KNN Loss: 3.0827174186706543 | BCE Loss: 1.030900001525879\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.106175422668457 | KNN Loss: 3.051107883453369 | BCE Loss: 1.0550676584243774\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.141393661499023 | KNN Loss: 3.1023426055908203 | BCE Loss: 1.0390512943267822\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.111528396606445 | KNN Loss: 3.0839390754699707 | BCE Loss: 1.0275895595550537\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.079560279846191 | KNN Loss: 3.0733094215393066 | BCE Loss: 1.0062507390975952\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.102478981018066 | KNN Loss: 3.072997808456421 | BCE Loss: 1.0294811725616455\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.100613594055176 | KNN Loss: 3.0771098136901855 | BCE Loss: 1.0235036611557007\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.071511745452881 | KNN Loss: 3.0516178607940674 | BCE Loss: 1.0198938846588135\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.057300567626953 | KNN Loss: 3.0373191833496094 | BCE Loss: 1.0199811458587646\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.097729206085205 | KNN Loss: 3.065937042236328 | BCE Loss: 1.031792163848877\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.098813056945801 | KNN Loss: 3.059335231781006 | BCE Loss: 1.0394777059555054\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.064479351043701 | KNN Loss: 3.0586435794830322 | BCE Loss: 1.0058356523513794\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.097932815551758 | KNN Loss: 3.07682466506958 | BCE Loss: 1.0211079120635986\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.075337886810303 | KNN Loss: 3.050039052963257 | BCE Loss: 1.0252989530563354\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.083497524261475 | KNN Loss: 3.0806355476379395 | BCE Loss: 1.0028618574142456\n",
      "Epoch   382: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.095821380615234 | KNN Loss: 3.0768866539001465 | BCE Loss: 1.0189344882965088\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.0555315017700195 | KNN Loss: 3.06447172164917 | BCE Loss: 0.9910595417022705\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.1101298332214355 | KNN Loss: 3.0843698978424072 | BCE Loss: 1.0257600545883179\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.086125373840332 | KNN Loss: 3.0462944507598877 | BCE Loss: 1.0398309230804443\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.150351524353027 | KNN Loss: 3.1117420196533203 | BCE Loss: 1.0386097431182861\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.072042465209961 | KNN Loss: 3.07413649559021 | BCE Loss: 0.9979058504104614\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.0792131423950195 | KNN Loss: 3.065051794052124 | BCE Loss: 1.0141615867614746\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.091549873352051 | KNN Loss: 3.049968957901001 | BCE Loss: 1.0415809154510498\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.08165979385376 | KNN Loss: 3.0704283714294434 | BCE Loss: 1.0112314224243164\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.05810546875 | KNN Loss: 3.0436184406280518 | BCE Loss: 1.0144870281219482\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.062404155731201 | KNN Loss: 3.0543127059936523 | BCE Loss: 1.0080914497375488\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.104266166687012 | KNN Loss: 3.06955885887146 | BCE Loss: 1.0347073078155518\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.085818290710449 | KNN Loss: 3.048518180847168 | BCE Loss: 1.0373001098632812\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.071314334869385 | KNN Loss: 3.0649142265319824 | BCE Loss: 1.006400227546692\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.050410270690918 | KNN Loss: 3.0487849712371826 | BCE Loss: 1.001625418663025\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.079684257507324 | KNN Loss: 3.0541491508483887 | BCE Loss: 1.0255348682403564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.104311943054199 | KNN Loss: 3.065690040588379 | BCE Loss: 1.0386216640472412\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.118304252624512 | KNN Loss: 3.0907137393951416 | BCE Loss: 1.0275905132293701\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.126136779785156 | KNN Loss: 3.0589377880096436 | BCE Loss: 1.0671991109848022\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.075124740600586 | KNN Loss: 3.0616018772125244 | BCE Loss: 1.0135226249694824\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.078861236572266 | KNN Loss: 3.062480926513672 | BCE Loss: 1.0163803100585938\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.083714962005615 | KNN Loss: 3.0681660175323486 | BCE Loss: 1.015548825263977\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.084773540496826 | KNN Loss: 3.065049648284912 | BCE Loss: 1.0197240114212036\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.093136310577393 | KNN Loss: 3.0768537521362305 | BCE Loss: 1.0162824392318726\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.083568572998047 | KNN Loss: 3.0277822017669678 | BCE Loss: 1.0557861328125\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.09874963760376 | KNN Loss: 3.062931537628174 | BCE Loss: 1.0358179807662964\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.051475524902344 | KNN Loss: 3.067779064178467 | BCE Loss: 0.9836966395378113\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.11096715927124 | KNN Loss: 3.0762479305267334 | BCE Loss: 1.0347192287445068\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.147097587585449 | KNN Loss: 3.0871431827545166 | BCE Loss: 1.0599546432495117\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.1202592849731445 | KNN Loss: 3.076927900314331 | BCE Loss: 1.0433311462402344\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.130781650543213 | KNN Loss: 3.077298402786255 | BCE Loss: 1.0534831285476685\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.111309051513672 | KNN Loss: 3.0859718322753906 | BCE Loss: 1.0253369808197021\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.085824966430664 | KNN Loss: 3.0493412017822266 | BCE Loss: 1.0364840030670166\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.094876766204834 | KNN Loss: 3.0715370178222656 | BCE Loss: 1.0233397483825684\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.125124931335449 | KNN Loss: 3.080232858657837 | BCE Loss: 1.0448920726776123\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.080387592315674 | KNN Loss: 3.0734381675720215 | BCE Loss: 1.0069493055343628\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.091516494750977 | KNN Loss: 3.082054376602173 | BCE Loss: 1.0094618797302246\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.051581382751465 | KNN Loss: 3.0214602947235107 | BCE Loss: 1.030121088027954\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.046428203582764 | KNN Loss: 3.0386641025543213 | BCE Loss: 1.0077641010284424\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.067687034606934 | KNN Loss: 3.0659992694854736 | BCE Loss: 1.0016875267028809\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.1389384269714355 | KNN Loss: 3.095334529876709 | BCE Loss: 1.043603777885437\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.086624622344971 | KNN Loss: 3.0626566410064697 | BCE Loss: 1.0239678621292114\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.11458683013916 | KNN Loss: 3.078568935394287 | BCE Loss: 1.036017894744873\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.104861736297607 | KNN Loss: 3.058586835861206 | BCE Loss: 1.0462749004364014\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.134303092956543 | KNN Loss: 3.0993194580078125 | BCE Loss: 1.0349836349487305\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.113020896911621 | KNN Loss: 3.049283981323242 | BCE Loss: 1.063736915588379\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.106812000274658 | KNN Loss: 3.071681261062622 | BCE Loss: 1.0351308584213257\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.086877822875977 | KNN Loss: 3.07033371925354 | BCE Loss: 1.0165443420410156\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.103977203369141 | KNN Loss: 3.0910611152648926 | BCE Loss: 1.0129159688949585\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.056095600128174 | KNN Loss: 3.0406336784362793 | BCE Loss: 1.0154619216918945\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.0713396072387695 | KNN Loss: 3.047605514526367 | BCE Loss: 1.0237340927124023\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.083676815032959 | KNN Loss: 3.0738658905029297 | BCE Loss: 1.0098110437393188\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.078886985778809 | KNN Loss: 3.063201665878296 | BCE Loss: 1.0156850814819336\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.1222944259643555 | KNN Loss: 3.0829246044158936 | BCE Loss: 1.0393695831298828\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.121481895446777 | KNN Loss: 3.090869903564453 | BCE Loss: 1.0306122303009033\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.060216903686523 | KNN Loss: 3.0595085620880127 | BCE Loss: 1.0007082223892212\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.056564807891846 | KNN Loss: 3.0535571575164795 | BCE Loss: 1.0030077695846558\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.128978252410889 | KNN Loss: 3.07474684715271 | BCE Loss: 1.0542315244674683\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.080578327178955 | KNN Loss: 3.0788731575012207 | BCE Loss: 1.0017051696777344\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.061707973480225 | KNN Loss: 3.0365748405456543 | BCE Loss: 1.0251331329345703\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.141172409057617 | KNN Loss: 3.1009042263031006 | BCE Loss: 1.0402681827545166\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.068757057189941 | KNN Loss: 3.0390703678131104 | BCE Loss: 1.029686450958252\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.0966386795043945 | KNN Loss: 3.090282678604126 | BCE Loss: 1.0063562393188477\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.077783584594727 | KNN Loss: 3.0384747982025146 | BCE Loss: 1.0393089056015015\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.110767364501953 | KNN Loss: 3.0830318927764893 | BCE Loss: 1.0277352333068848\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.0670270919799805 | KNN Loss: 3.0511202812194824 | BCE Loss: 1.0159069299697876\n",
      "Epoch   393: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.0605573654174805 | KNN Loss: 3.057229995727539 | BCE Loss: 1.0033271312713623\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.085054397583008 | KNN Loss: 3.053041458129883 | BCE Loss: 1.032012939453125\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.058126926422119 | KNN Loss: 3.0549044609069824 | BCE Loss: 1.0032224655151367\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.10227632522583 | KNN Loss: 3.0670323371887207 | BCE Loss: 1.0352439880371094\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.070674896240234 | KNN Loss: 3.0750179290771484 | BCE Loss: 0.9956569075584412\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.114046096801758 | KNN Loss: 3.0928845405578613 | BCE Loss: 1.0211615562438965\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.121884346008301 | KNN Loss: 3.070671558380127 | BCE Loss: 1.0512126684188843\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.08685302734375 | KNN Loss: 3.0603444576263428 | BCE Loss: 1.0265088081359863\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.1261162757873535 | KNN Loss: 3.1106159687042236 | BCE Loss: 1.0155003070831299\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.079751968383789 | KNN Loss: 3.047415018081665 | BCE Loss: 1.032336950302124\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.101877212524414 | KNN Loss: 3.0909457206726074 | BCE Loss: 1.0109317302703857\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.142492771148682 | KNN Loss: 3.0931599140167236 | BCE Loss: 1.049332857131958\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.090744972229004 | KNN Loss: 3.0518109798431396 | BCE Loss: 1.0389337539672852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.08995246887207 | KNN Loss: 3.0600340366363525 | BCE Loss: 1.0299181938171387\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.074589729309082 | KNN Loss: 3.059720754623413 | BCE Loss: 1.0148688554763794\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.126965522766113 | KNN Loss: 3.071751832962036 | BCE Loss: 1.055213451385498\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.087882041931152 | KNN Loss: 3.066810369491577 | BCE Loss: 1.0210715532302856\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.05764102935791 | KNN Loss: 3.049177646636963 | BCE Loss: 1.0084635019302368\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.09837532043457 | KNN Loss: 3.0724523067474365 | BCE Loss: 1.025923252105713\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.100473880767822 | KNN Loss: 3.0920450687408447 | BCE Loss: 1.0084288120269775\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.043381690979004 | KNN Loss: 3.041574716567993 | BCE Loss: 1.0018067359924316\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.120945930480957 | KNN Loss: 3.0894076824188232 | BCE Loss: 1.0315380096435547\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.104341506958008 | KNN Loss: 3.069455623626709 | BCE Loss: 1.034886121749878\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.084481239318848 | KNN Loss: 3.076833724975586 | BCE Loss: 1.0076477527618408\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.109004974365234 | KNN Loss: 3.068648338317871 | BCE Loss: 1.0403568744659424\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.086909770965576 | KNN Loss: 3.0514023303985596 | BCE Loss: 1.0355075597763062\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.089730262756348 | KNN Loss: 3.0814621448516846 | BCE Loss: 1.008267879486084\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.0965986251831055 | KNN Loss: 3.0959813594818115 | BCE Loss: 1.000617265701294\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.092565536499023 | KNN Loss: 3.0529162883758545 | BCE Loss: 1.039649248123169\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.138350009918213 | KNN Loss: 3.105382204055786 | BCE Loss: 1.0329679250717163\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.055885314941406 | KNN Loss: 3.060033082962036 | BCE Loss: 0.9958524703979492\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.038059234619141 | KNN Loss: 3.0427513122558594 | BCE Loss: 0.9953078031539917\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.0984206199646 | KNN Loss: 3.0728049278259277 | BCE Loss: 1.0256155729293823\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.047938346862793 | KNN Loss: 3.041714906692505 | BCE Loss: 1.006223201751709\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.106054306030273 | KNN Loss: 3.061084747314453 | BCE Loss: 1.0449693202972412\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.064271926879883 | KNN Loss: 3.0332446098327637 | BCE Loss: 1.0310275554656982\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.015429496765137 | KNN Loss: 3.054547071456909 | BCE Loss: 0.9608824849128723\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.030435085296631 | KNN Loss: 3.011826992034912 | BCE Loss: 1.0186080932617188\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.115629196166992 | KNN Loss: 3.0943028926849365 | BCE Loss: 1.0213265419006348\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.1012725830078125 | KNN Loss: 3.0739500522613525 | BCE Loss: 1.0273226499557495\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.108748912811279 | KNN Loss: 3.0772862434387207 | BCE Loss: 1.031462550163269\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.141061305999756 | KNN Loss: 3.115922451019287 | BCE Loss: 1.0251388549804688\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.106904983520508 | KNN Loss: 3.0825278759002686 | BCE Loss: 1.0243768692016602\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.0682692527771 | KNN Loss: 3.0522048473358154 | BCE Loss: 1.0160645246505737\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.043754577636719 | KNN Loss: 3.052819013595581 | BCE Loss: 0.9909356832504272\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.079950332641602 | KNN Loss: 3.0610132217407227 | BCE Loss: 1.018937349319458\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.064852714538574 | KNN Loss: 3.0315232276916504 | BCE Loss: 1.0333294868469238\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.110873222351074 | KNN Loss: 3.086843252182007 | BCE Loss: 1.0240299701690674\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.119375228881836 | KNN Loss: 3.0699782371520996 | BCE Loss: 1.0493969917297363\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.088009834289551 | KNN Loss: 3.069967746734619 | BCE Loss: 1.0180423259735107\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.1203155517578125 | KNN Loss: 3.0692648887634277 | BCE Loss: 1.0510507822036743\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.156184196472168 | KNN Loss: 3.088923931121826 | BCE Loss: 1.0672602653503418\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.109923839569092 | KNN Loss: 3.0861873626708984 | BCE Loss: 1.023736596107483\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.085978031158447 | KNN Loss: 3.0927228927612305 | BCE Loss: 0.9932551980018616\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.121861457824707 | KNN Loss: 3.070296287536621 | BCE Loss: 1.051565170288086\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.059968948364258 | KNN Loss: 3.0222015380859375 | BCE Loss: 1.0377674102783203\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.064939498901367 | KNN Loss: 3.0451557636260986 | BCE Loss: 1.019783616065979\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.074047565460205 | KNN Loss: 3.046444892883301 | BCE Loss: 1.0276026725769043\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.086317539215088 | KNN Loss: 3.059781312942505 | BCE Loss: 1.026536226272583\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.117056369781494 | KNN Loss: 3.085303544998169 | BCE Loss: 1.0317528247833252\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.123626708984375 | KNN Loss: 3.068394422531128 | BCE Loss: 1.055232286453247\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.1143717765808105 | KNN Loss: 3.0925652980804443 | BCE Loss: 1.0218064785003662\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.075056076049805 | KNN Loss: 3.045797348022461 | BCE Loss: 1.0292587280273438\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.109996795654297 | KNN Loss: 3.095841407775879 | BCE Loss: 1.0141551494598389\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.03203821182251 | KNN Loss: 3.040759325027466 | BCE Loss: 0.9912787675857544\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.080986022949219 | KNN Loss: 3.056586503982544 | BCE Loss: 1.0243996381759644\n",
      "Epoch   404: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.103200912475586 | KNN Loss: 3.0771169662475586 | BCE Loss: 1.0260841846466064\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.0592756271362305 | KNN Loss: 3.0589702129364014 | BCE Loss: 1.000305414199829\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.0982890129089355 | KNN Loss: 3.0553998947143555 | BCE Loss: 1.04288911819458\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.09685754776001 | KNN Loss: 3.091096878051758 | BCE Loss: 1.005760669708252\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.104137897491455 | KNN Loss: 3.071930170059204 | BCE Loss: 1.032207727432251\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.102503299713135 | KNN Loss: 3.075955867767334 | BCE Loss: 1.0265474319458008\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.080454349517822 | KNN Loss: 3.061291217803955 | BCE Loss: 1.0191631317138672\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.057657718658447 | KNN Loss: 3.0370728969573975 | BCE Loss: 1.0205849409103394\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.085665702819824 | KNN Loss: 3.0720345973968506 | BCE Loss: 1.0136312246322632\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.13624382019043 | KNN Loss: 3.1019062995910645 | BCE Loss: 1.0343372821807861\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.099761009216309 | KNN Loss: 3.0726475715637207 | BCE Loss: 1.027113676071167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.067869186401367 | KNN Loss: 3.048295497894287 | BCE Loss: 1.019573450088501\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.11198091506958 | KNN Loss: 3.087237596511841 | BCE Loss: 1.0247431993484497\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.202536106109619 | KNN Loss: 3.15704607963562 | BCE Loss: 1.0454899072647095\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.065980434417725 | KNN Loss: 3.0564651489257812 | BCE Loss: 1.0095152854919434\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.078166961669922 | KNN Loss: 3.0687882900238037 | BCE Loss: 1.0093787908554077\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.106908798217773 | KNN Loss: 3.0565907955169678 | BCE Loss: 1.0503180027008057\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.096429824829102 | KNN Loss: 3.082143783569336 | BCE Loss: 1.0142862796783447\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.168573379516602 | KNN Loss: 3.0847578048706055 | BCE Loss: 1.083815574645996\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.056042671203613 | KNN Loss: 3.0538463592529297 | BCE Loss: 1.0021960735321045\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.004481792449951 | KNN Loss: 3.018927812576294 | BCE Loss: 0.9855541586875916\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.108621597290039 | KNN Loss: 3.094468832015991 | BCE Loss: 1.014153003692627\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.1189141273498535 | KNN Loss: 3.0940229892730713 | BCE Loss: 1.0248911380767822\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.050599098205566 | KNN Loss: 3.0454373359680176 | BCE Loss: 1.0051617622375488\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.05460262298584 | KNN Loss: 3.046441078186035 | BCE Loss: 1.0081614255905151\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.090073585510254 | KNN Loss: 3.0720560550689697 | BCE Loss: 1.0180175304412842\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.153345584869385 | KNN Loss: 3.0877208709716797 | BCE Loss: 1.065624713897705\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.04581880569458 | KNN Loss: 3.0446369647979736 | BCE Loss: 1.0011818408966064\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.091784477233887 | KNN Loss: 3.0693745613098145 | BCE Loss: 1.0224101543426514\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.087909698486328 | KNN Loss: 3.073439836502075 | BCE Loss: 1.014470100402832\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.10185432434082 | KNN Loss: 3.0676631927490234 | BCE Loss: 1.034191370010376\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.094915390014648 | KNN Loss: 3.080256462097168 | BCE Loss: 1.014658808708191\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.099686622619629 | KNN Loss: 3.0589652061462402 | BCE Loss: 1.0407214164733887\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.063144207000732 | KNN Loss: 3.0613021850585938 | BCE Loss: 1.0018421411514282\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.087343215942383 | KNN Loss: 3.0595407485961914 | BCE Loss: 1.0278024673461914\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.088198661804199 | KNN Loss: 3.0781939029693604 | BCE Loss: 1.0100046396255493\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.079916477203369 | KNN Loss: 3.062868595123291 | BCE Loss: 1.0170477628707886\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.077401161193848 | KNN Loss: 3.059981346130371 | BCE Loss: 1.0174198150634766\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.09271240234375 | KNN Loss: 3.071631908416748 | BCE Loss: 1.0210802555084229\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.087631702423096 | KNN Loss: 3.0542027950286865 | BCE Loss: 1.0334289073944092\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.074151039123535 | KNN Loss: 3.050570487976074 | BCE Loss: 1.023580551147461\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.085834503173828 | KNN Loss: 3.09424090385437 | BCE Loss: 0.9915938377380371\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.115938186645508 | KNN Loss: 3.052489757537842 | BCE Loss: 1.0634486675262451\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.125639915466309 | KNN Loss: 3.094623565673828 | BCE Loss: 1.0310163497924805\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.050459384918213 | KNN Loss: 3.0321929454803467 | BCE Loss: 1.0182664394378662\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.0876946449279785 | KNN Loss: 3.0705201625823975 | BCE Loss: 1.0171743631362915\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.1078033447265625 | KNN Loss: 3.0957045555114746 | BCE Loss: 1.0120985507965088\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.099246025085449 | KNN Loss: 3.090839385986328 | BCE Loss: 1.0084068775177002\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.080093860626221 | KNN Loss: 3.0550918579101562 | BCE Loss: 1.0250020027160645\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.106767654418945 | KNN Loss: 3.054835319519043 | BCE Loss: 1.051932454109192\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.049692630767822 | KNN Loss: 3.061004400253296 | BCE Loss: 0.9886882305145264\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.147848606109619 | KNN Loss: 3.1096599102020264 | BCE Loss: 1.0381886959075928\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.069314002990723 | KNN Loss: 3.0586090087890625 | BCE Loss: 1.0107051134109497\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.132967948913574 | KNN Loss: 3.105407476425171 | BCE Loss: 1.0275604724884033\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.123305320739746 | KNN Loss: 3.0533056259155273 | BCE Loss: 1.0699996948242188\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.075833320617676 | KNN Loss: 3.070740222930908 | BCE Loss: 1.0050930976867676\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.107692718505859 | KNN Loss: 3.0756633281707764 | BCE Loss: 1.0320295095443726\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.086464881896973 | KNN Loss: 3.0474908351898193 | BCE Loss: 1.0389740467071533\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.113272666931152 | KNN Loss: 3.0528292655944824 | BCE Loss: 1.06044340133667\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.12311315536499 | KNN Loss: 3.1078598499298096 | BCE Loss: 1.0152531862258911\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.0986328125 | KNN Loss: 3.0735092163085938 | BCE Loss: 1.0251235961914062\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.133292198181152 | KNN Loss: 3.0910515785217285 | BCE Loss: 1.042240858078003\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.052387714385986 | KNN Loss: 3.0269076824188232 | BCE Loss: 1.025480031967163\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.075539588928223 | KNN Loss: 3.064239740371704 | BCE Loss: 1.0112998485565186\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.137279033660889 | KNN Loss: 3.0832324028015137 | BCE Loss: 1.0540467500686646\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.075197219848633 | KNN Loss: 3.062551498413086 | BCE Loss: 1.0126457214355469\n",
      "Epoch   415: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.077527046203613 | KNN Loss: 3.061178684234619 | BCE Loss: 1.016348123550415\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.144507884979248 | KNN Loss: 3.090550184249878 | BCE Loss: 1.0539577007293701\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.124629974365234 | KNN Loss: 3.1117982864379883 | BCE Loss: 1.012831687927246\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.069035530090332 | KNN Loss: 3.0418388843536377 | BCE Loss: 1.0271967649459839\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.089067459106445 | KNN Loss: 3.0687718391418457 | BCE Loss: 1.0202956199645996\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.125824928283691 | KNN Loss: 3.0868794918060303 | BCE Loss: 1.0389454364776611\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.121518135070801 | KNN Loss: 3.1198737621307373 | BCE Loss: 1.0016446113586426\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.124380588531494 | KNN Loss: 3.078639268875122 | BCE Loss: 1.045741319656372\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.112441062927246 | KNN Loss: 3.0541722774505615 | BCE Loss: 1.0582685470581055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.120395660400391 | KNN Loss: 3.1064107418060303 | BCE Loss: 1.0139851570129395\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.084008693695068 | KNN Loss: 3.049210548400879 | BCE Loss: 1.0347980260849\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.0950822830200195 | KNN Loss: 3.098548650741577 | BCE Loss: 0.9965334534645081\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.077378749847412 | KNN Loss: 3.0431835651397705 | BCE Loss: 1.0341951847076416\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.0467729568481445 | KNN Loss: 3.040727138519287 | BCE Loss: 1.0060458183288574\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.101114273071289 | KNN Loss: 3.074624538421631 | BCE Loss: 1.0264899730682373\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.156754016876221 | KNN Loss: 3.1156563758850098 | BCE Loss: 1.0410975217819214\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.077102184295654 | KNN Loss: 3.054123878479004 | BCE Loss: 1.0229781866073608\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.045964241027832 | KNN Loss: 3.0367257595062256 | BCE Loss: 1.0092384815216064\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.065849781036377 | KNN Loss: 3.047215700149536 | BCE Loss: 1.0186340808868408\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.091527938842773 | KNN Loss: 3.0676822662353516 | BCE Loss: 1.0238457918167114\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.0800981521606445 | KNN Loss: 3.059561014175415 | BCE Loss: 1.020537257194519\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.086260795593262 | KNN Loss: 3.0572268962860107 | BCE Loss: 1.0290340185165405\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.123434066772461 | KNN Loss: 3.090425491333008 | BCE Loss: 1.033008337020874\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.05951452255249 | KNN Loss: 3.0429165363311768 | BCE Loss: 1.016598105430603\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.104883193969727 | KNN Loss: 3.0921738147735596 | BCE Loss: 1.0127092599868774\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.108948230743408 | KNN Loss: 3.111135721206665 | BCE Loss: 0.9978126883506775\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.112892150878906 | KNN Loss: 3.060962200164795 | BCE Loss: 1.0519301891326904\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.091609001159668 | KNN Loss: 3.0652096271514893 | BCE Loss: 1.0263991355895996\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.0396342277526855 | KNN Loss: 3.0322492122650146 | BCE Loss: 1.0073848962783813\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.071089744567871 | KNN Loss: 3.072328805923462 | BCE Loss: 0.9987611174583435\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.061121940612793 | KNN Loss: 3.0617311000823975 | BCE Loss: 0.9993906021118164\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.089322090148926 | KNN Loss: 3.0486698150634766 | BCE Loss: 1.0406520366668701\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.062870502471924 | KNN Loss: 3.0643515586853027 | BCE Loss: 0.9985188245773315\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.054439067840576 | KNN Loss: 3.064073085784912 | BCE Loss: 0.9903658032417297\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.095417022705078 | KNN Loss: 3.067666530609131 | BCE Loss: 1.0277504920959473\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.085765838623047 | KNN Loss: 3.060889482498169 | BCE Loss: 1.024876594543457\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.064707279205322 | KNN Loss: 3.031529426574707 | BCE Loss: 1.0331779718399048\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.091239929199219 | KNN Loss: 3.053969383239746 | BCE Loss: 1.0372705459594727\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.092945098876953 | KNN Loss: 3.081465482711792 | BCE Loss: 1.0114798545837402\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.090428352355957 | KNN Loss: 3.05820369720459 | BCE Loss: 1.032224416732788\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.09657096862793 | KNN Loss: 3.0749917030334473 | BCE Loss: 1.0215790271759033\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.067060470581055 | KNN Loss: 3.0455198287963867 | BCE Loss: 1.0215404033660889\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.046382904052734 | KNN Loss: 3.0365047454833984 | BCE Loss: 1.0098779201507568\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.118353366851807 | KNN Loss: 3.11440372467041 | BCE Loss: 1.0039496421813965\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.125253677368164 | KNN Loss: 3.0737557411193848 | BCE Loss: 1.0514981746673584\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.106675148010254 | KNN Loss: 3.0861470699310303 | BCE Loss: 1.0205281972885132\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.104179859161377 | KNN Loss: 3.073058843612671 | BCE Loss: 1.031121015548706\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.07008171081543 | KNN Loss: 3.034771203994751 | BCE Loss: 1.0353107452392578\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.063918113708496 | KNN Loss: 3.0858144760131836 | BCE Loss: 0.9781036972999573\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.110844612121582 | KNN Loss: 3.0742874145507812 | BCE Loss: 1.0365574359893799\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.0979132652282715 | KNN Loss: 3.077986478805542 | BCE Loss: 1.01992666721344\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.139943599700928 | KNN Loss: 3.0917418003082275 | BCE Loss: 1.0482016801834106\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.107350826263428 | KNN Loss: 3.091669797897339 | BCE Loss: 1.0156810283660889\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.072533130645752 | KNN Loss: 3.0824804306030273 | BCE Loss: 0.9900527596473694\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.100805282592773 | KNN Loss: 3.0841217041015625 | BCE Loss: 1.0166834592819214\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.078244209289551 | KNN Loss: 3.0558409690856934 | BCE Loss: 1.022403359413147\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.082345008850098 | KNN Loss: 3.0617995262145996 | BCE Loss: 1.0205453634262085\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.093881130218506 | KNN Loss: 3.074681520462036 | BCE Loss: 1.0191997289657593\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.113443374633789 | KNN Loss: 3.0734593868255615 | BCE Loss: 1.0399839878082275\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.095999717712402 | KNN Loss: 3.0484399795532227 | BCE Loss: 1.0475594997406006\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.138681888580322 | KNN Loss: 3.089416265487671 | BCE Loss: 1.0492656230926514\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.058530807495117 | KNN Loss: 3.060535430908203 | BCE Loss: 0.9979953765869141\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.071889400482178 | KNN Loss: 3.071719169616699 | BCE Loss: 1.0001702308654785\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.1418843269348145 | KNN Loss: 3.1001737117767334 | BCE Loss: 1.041710615158081\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.081226825714111 | KNN Loss: 3.0747756958007812 | BCE Loss: 1.0064510107040405\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.099038124084473 | KNN Loss: 3.0617923736572266 | BCE Loss: 1.0372456312179565\n",
      "Epoch   426: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.092184066772461 | KNN Loss: 3.0672264099121094 | BCE Loss: 1.024957537651062\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.096897125244141 | KNN Loss: 3.063951253890991 | BCE Loss: 1.0329458713531494\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.116034030914307 | KNN Loss: 3.0988709926605225 | BCE Loss: 1.0171630382537842\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.104152202606201 | KNN Loss: 3.0768542289733887 | BCE Loss: 1.0272979736328125\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.066401958465576 | KNN Loss: 3.07338809967041 | BCE Loss: 0.9930139780044556\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.107046127319336 | KNN Loss: 3.0678083896636963 | BCE Loss: 1.0392379760742188\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.073813438415527 | KNN Loss: 3.0517642498016357 | BCE Loss: 1.0220489501953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.063459396362305 | KNN Loss: 3.0340020656585693 | BCE Loss: 1.0294570922851562\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.106435775756836 | KNN Loss: 3.0735971927642822 | BCE Loss: 1.0328385829925537\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.092832565307617 | KNN Loss: 3.0779454708099365 | BCE Loss: 1.0148870944976807\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.1314377784729 | KNN Loss: 3.1037068367004395 | BCE Loss: 1.027730941772461\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.132693290710449 | KNN Loss: 3.099058151245117 | BCE Loss: 1.033634901046753\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.119777679443359 | KNN Loss: 3.095893621444702 | BCE Loss: 1.0238842964172363\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.090920925140381 | KNN Loss: 3.0997676849365234 | BCE Loss: 0.9911531805992126\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.113251209259033 | KNN Loss: 3.081963300704956 | BCE Loss: 1.0312879085540771\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.128566741943359 | KNN Loss: 3.0543675422668457 | BCE Loss: 1.0741994380950928\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.059810161590576 | KNN Loss: 3.055837631225586 | BCE Loss: 1.0039725303649902\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.152594089508057 | KNN Loss: 3.1191368103027344 | BCE Loss: 1.0334573984146118\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.091107368469238 | KNN Loss: 3.0663416385650635 | BCE Loss: 1.0247656106948853\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.092337608337402 | KNN Loss: 3.0789077281951904 | BCE Loss: 1.013430118560791\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.072342395782471 | KNN Loss: 3.0661191940307617 | BCE Loss: 1.0062230825424194\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.101430416107178 | KNN Loss: 3.06630539894104 | BCE Loss: 1.0351251363754272\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.157159805297852 | KNN Loss: 3.1137869358062744 | BCE Loss: 1.043372631072998\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.090139389038086 | KNN Loss: 3.0779170989990234 | BCE Loss: 1.012222170829773\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.114440441131592 | KNN Loss: 3.0941216945648193 | BCE Loss: 1.0203187465667725\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.083646774291992 | KNN Loss: 3.040266990661621 | BCE Loss: 1.0433796644210815\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.125213623046875 | KNN Loss: 3.065290689468384 | BCE Loss: 1.0599229335784912\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.068365097045898 | KNN Loss: 3.0361075401306152 | BCE Loss: 1.0322577953338623\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.0576491355896 | KNN Loss: 3.0485637187957764 | BCE Loss: 1.0090854167938232\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.077593803405762 | KNN Loss: 3.059635877609253 | BCE Loss: 1.017958164215088\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.0512824058532715 | KNN Loss: 3.0382845401763916 | BCE Loss: 1.0129979848861694\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.0770463943481445 | KNN Loss: 3.070157051086426 | BCE Loss: 1.0068892240524292\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.129452705383301 | KNN Loss: 3.1094040870666504 | BCE Loss: 1.0200483798980713\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.057303428649902 | KNN Loss: 3.037487268447876 | BCE Loss: 1.0198159217834473\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.090230941772461 | KNN Loss: 3.0467116832733154 | BCE Loss: 1.0435192584991455\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.134591579437256 | KNN Loss: 3.082812786102295 | BCE Loss: 1.051778793334961\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.158591270446777 | KNN Loss: 3.1077067852020264 | BCE Loss: 1.05088472366333\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.104913234710693 | KNN Loss: 3.063906192779541 | BCE Loss: 1.0410070419311523\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.0696797370910645 | KNN Loss: 3.07492995262146 | BCE Loss: 0.9947497248649597\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.118405342102051 | KNN Loss: 3.0747838020324707 | BCE Loss: 1.04362154006958\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.116178035736084 | KNN Loss: 3.088696241378784 | BCE Loss: 1.0274816751480103\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.101757526397705 | KNN Loss: 3.0601325035095215 | BCE Loss: 1.0416250228881836\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.085414886474609 | KNN Loss: 3.0776073932647705 | BCE Loss: 1.0078074932098389\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.103721618652344 | KNN Loss: 3.074211597442627 | BCE Loss: 1.029510259628296\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.0807929039001465 | KNN Loss: 3.041720151901245 | BCE Loss: 1.039072871208191\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.095240592956543 | KNN Loss: 3.0658984184265137 | BCE Loss: 1.0293424129486084\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.129204750061035 | KNN Loss: 3.0840346813201904 | BCE Loss: 1.0451700687408447\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.065764427185059 | KNN Loss: 3.034468412399292 | BCE Loss: 1.031295895576477\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.076282501220703 | KNN Loss: 3.049834966659546 | BCE Loss: 1.0264474153518677\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.063671112060547 | KNN Loss: 3.063321590423584 | BCE Loss: 1.0003492832183838\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.041706085205078 | KNN Loss: 3.023660182952881 | BCE Loss: 1.0180459022521973\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.058186054229736 | KNN Loss: 3.052483320236206 | BCE Loss: 1.0057028532028198\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.031526565551758 | KNN Loss: 3.031461715698242 | BCE Loss: 1.0000646114349365\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.100886821746826 | KNN Loss: 3.0657906532287598 | BCE Loss: 1.035096287727356\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.086961269378662 | KNN Loss: 3.0800747871398926 | BCE Loss: 1.00688636302948\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.142120361328125 | KNN Loss: 3.1114907264709473 | BCE Loss: 1.0306293964385986\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.126384735107422 | KNN Loss: 3.0700576305389404 | BCE Loss: 1.0563268661499023\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.089966297149658 | KNN Loss: 3.067643642425537 | BCE Loss: 1.022322654724121\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.045877456665039 | KNN Loss: 3.046060085296631 | BCE Loss: 0.9998174905776978\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.0917463302612305 | KNN Loss: 3.0671298503875732 | BCE Loss: 1.0246164798736572\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.061707496643066 | KNN Loss: 3.048137664794922 | BCE Loss: 1.0135700702667236\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.085994243621826 | KNN Loss: 3.0576155185699463 | BCE Loss: 1.0283788442611694\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.087205410003662 | KNN Loss: 3.0695674419403076 | BCE Loss: 1.017637848854065\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.120226860046387 | KNN Loss: 3.0806822776794434 | BCE Loss: 1.0395448207855225\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.118420124053955 | KNN Loss: 3.111853837966919 | BCE Loss: 1.0065664052963257\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.092998504638672 | KNN Loss: 3.1125690937042236 | BCE Loss: 0.9804292917251587\n",
      "Epoch   437: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.094869613647461 | KNN Loss: 3.0946125984191895 | BCE Loss: 1.0002567768096924\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.097187042236328 | KNN Loss: 3.076913833618164 | BCE Loss: 1.0202730894088745\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.091224193572998 | KNN Loss: 3.0588796138763428 | BCE Loss: 1.0323445796966553\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.108131408691406 | KNN Loss: 3.083613157272339 | BCE Loss: 1.0245182514190674\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.100835800170898 | KNN Loss: 3.0801844596862793 | BCE Loss: 1.0206514596939087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.0708794593811035 | KNN Loss: 3.041016101837158 | BCE Loss: 1.0298633575439453\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.063405990600586 | KNN Loss: 3.059786558151245 | BCE Loss: 1.0036191940307617\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.121001243591309 | KNN Loss: 3.0869781970977783 | BCE Loss: 1.0340228080749512\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.089197635650635 | KNN Loss: 3.061354875564575 | BCE Loss: 1.02784264087677\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.094061374664307 | KNN Loss: 3.0630154609680176 | BCE Loss: 1.031045913696289\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.052301406860352 | KNN Loss: 3.0573949813842773 | BCE Loss: 0.9949064254760742\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.104227066040039 | KNN Loss: 3.0930612087249756 | BCE Loss: 1.0111660957336426\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.105855941772461 | KNN Loss: 3.0757195949554443 | BCE Loss: 1.0301363468170166\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.062112331390381 | KNN Loss: 3.0555450916290283 | BCE Loss: 1.006567358970642\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.079668998718262 | KNN Loss: 3.074151039123535 | BCE Loss: 1.0055179595947266\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.068800449371338 | KNN Loss: 3.0458977222442627 | BCE Loss: 1.0229028463363647\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.066186428070068 | KNN Loss: 3.0661978721618652 | BCE Loss: 0.9999886155128479\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.072644233703613 | KNN Loss: 3.05483341217041 | BCE Loss: 1.017810583114624\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.137933254241943 | KNN Loss: 3.09028959274292 | BCE Loss: 1.0476436614990234\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.093757152557373 | KNN Loss: 3.0605263710021973 | BCE Loss: 1.0332306623458862\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.079529762268066 | KNN Loss: 3.0657873153686523 | BCE Loss: 1.013742208480835\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.118714809417725 | KNN Loss: 3.071619749069214 | BCE Loss: 1.0470950603485107\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.093494415283203 | KNN Loss: 3.0810110569000244 | BCE Loss: 1.0124835968017578\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.103431701660156 | KNN Loss: 3.0972952842712402 | BCE Loss: 1.0061366558074951\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.099643230438232 | KNN Loss: 3.057926654815674 | BCE Loss: 1.041716456413269\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.108202934265137 | KNN Loss: 3.0694234371185303 | BCE Loss: 1.038779616355896\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.126348495483398 | KNN Loss: 3.085925817489624 | BCE Loss: 1.0404229164123535\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.063056468963623 | KNN Loss: 3.0392463207244873 | BCE Loss: 1.0238102674484253\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.073535442352295 | KNN Loss: 3.0513570308685303 | BCE Loss: 1.0221785306930542\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.094966411590576 | KNN Loss: 3.06432843208313 | BCE Loss: 1.0306379795074463\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.057738780975342 | KNN Loss: 3.0673482418060303 | BCE Loss: 0.9903904795646667\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.101642608642578 | KNN Loss: 3.075226068496704 | BCE Loss: 1.026416540145874\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.121868133544922 | KNN Loss: 3.0916666984558105 | BCE Loss: 1.0302014350891113\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.10698938369751 | KNN Loss: 3.08132004737854 | BCE Loss: 1.0256692171096802\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.059582233428955 | KNN Loss: 3.0512852668762207 | BCE Loss: 1.008297085762024\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.098221778869629 | KNN Loss: 3.062574625015259 | BCE Loss: 1.0356472730636597\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.1234917640686035 | KNN Loss: 3.066840648651123 | BCE Loss: 1.05665123462677\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.065442085266113 | KNN Loss: 3.0305657386779785 | BCE Loss: 1.0348765850067139\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.088637828826904 | KNN Loss: 3.0746371746063232 | BCE Loss: 1.014000654220581\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.048311710357666 | KNN Loss: 3.0367138385772705 | BCE Loss: 1.0115978717803955\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.029741287231445 | KNN Loss: 3.0340418815612793 | BCE Loss: 0.9956996440887451\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.102284908294678 | KNN Loss: 3.0738887786865234 | BCE Loss: 1.0283961296081543\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.119622230529785 | KNN Loss: 3.0895116329193115 | BCE Loss: 1.0301103591918945\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.101702690124512 | KNN Loss: 3.0663902759552 | BCE Loss: 1.0353126525878906\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.049062252044678 | KNN Loss: 3.037200927734375 | BCE Loss: 1.0118612051010132\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.031570911407471 | KNN Loss: 3.044947624206543 | BCE Loss: 0.9866234064102173\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.064998149871826 | KNN Loss: 3.040163278579712 | BCE Loss: 1.0248348712921143\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.071863651275635 | KNN Loss: 3.054687738418579 | BCE Loss: 1.0171759128570557\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.111515045166016 | KNN Loss: 3.069664239883423 | BCE Loss: 1.0418506860733032\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.104639053344727 | KNN Loss: 3.061034917831421 | BCE Loss: 1.0436038970947266\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.1538591384887695 | KNN Loss: 3.128959894180298 | BCE Loss: 1.0248992443084717\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.111296653747559 | KNN Loss: 3.071763515472412 | BCE Loss: 1.0395333766937256\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.119728088378906 | KNN Loss: 3.0576834678649902 | BCE Loss: 1.0620448589324951\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.061184406280518 | KNN Loss: 3.0574610233306885 | BCE Loss: 1.0037235021591187\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.12030553817749 | KNN Loss: 3.0773682594299316 | BCE Loss: 1.0429372787475586\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.1561713218688965 | KNN Loss: 3.11556339263916 | BCE Loss: 1.0406079292297363\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.099531173706055 | KNN Loss: 3.074171543121338 | BCE Loss: 1.0253593921661377\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.0871052742004395 | KNN Loss: 3.049952983856201 | BCE Loss: 1.0371521711349487\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.097844123840332 | KNN Loss: 3.070880651473999 | BCE Loss: 1.026963710784912\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.113558769226074 | KNN Loss: 3.0794291496276855 | BCE Loss: 1.0341296195983887\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.101751327514648 | KNN Loss: 3.0617191791534424 | BCE Loss: 1.040031909942627\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.058919429779053 | KNN Loss: 3.043429374694824 | BCE Loss: 1.0154900550842285\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.102153778076172 | KNN Loss: 3.0510189533233643 | BCE Loss: 1.0511348247528076\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.025137424468994 | KNN Loss: 3.049818515777588 | BCE Loss: 0.9753188490867615\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.091533184051514 | KNN Loss: 3.080289125442505 | BCE Loss: 1.0112440586090088\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.051692008972168 | KNN Loss: 3.0424816608428955 | BCE Loss: 1.009210228919983\n",
      "Epoch   448: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.124253749847412 | KNN Loss: 3.1095218658447266 | BCE Loss: 1.014731764793396\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.101619720458984 | KNN Loss: 3.063122034072876 | BCE Loss: 1.0384979248046875\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.067489147186279 | KNN Loss: 3.032147169113159 | BCE Loss: 1.0353420972824097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.089088439941406 | KNN Loss: 3.071681499481201 | BCE Loss: 1.0174068212509155\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.125425338745117 | KNN Loss: 3.0975501537323 | BCE Loss: 1.027875304222107\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.044632434844971 | KNN Loss: 3.066856861114502 | BCE Loss: 0.9777754545211792\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.0968170166015625 | KNN Loss: 3.0623044967651367 | BCE Loss: 1.0345127582550049\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.073984146118164 | KNN Loss: 3.0623459815979004 | BCE Loss: 1.0116382837295532\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.081967830657959 | KNN Loss: 3.0752947330474854 | BCE Loss: 1.0066732168197632\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.1162848472595215 | KNN Loss: 3.1017158031463623 | BCE Loss: 1.0145691633224487\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.095612049102783 | KNN Loss: 3.0821166038513184 | BCE Loss: 1.0134955644607544\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.119370937347412 | KNN Loss: 3.0912232398986816 | BCE Loss: 1.028147578239441\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.030693054199219 | KNN Loss: 3.0377302169799805 | BCE Loss: 0.9929629564285278\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.081981182098389 | KNN Loss: 3.061607837677002 | BCE Loss: 1.0203733444213867\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.091217041015625 | KNN Loss: 3.101095676422119 | BCE Loss: 0.9901211261749268\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.067147254943848 | KNN Loss: 3.0715880393981934 | BCE Loss: 0.9955594539642334\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.10350227355957 | KNN Loss: 3.0648765563964844 | BCE Loss: 1.038625717163086\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.077726364135742 | KNN Loss: 3.0510787963867188 | BCE Loss: 1.0266475677490234\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.179628372192383 | KNN Loss: 3.1534852981567383 | BCE Loss: 1.026143193244934\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.115981101989746 | KNN Loss: 3.0749940872192383 | BCE Loss: 1.0409867763519287\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.076467514038086 | KNN Loss: 3.061257839202881 | BCE Loss: 1.0152099132537842\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.063436985015869 | KNN Loss: 3.0536234378814697 | BCE Loss: 1.0098135471343994\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.0975871086120605 | KNN Loss: 3.0700557231903076 | BCE Loss: 1.0275312662124634\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.100249290466309 | KNN Loss: 3.067556619644165 | BCE Loss: 1.032692551612854\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.092318534851074 | KNN Loss: 3.0535287857055664 | BCE Loss: 1.0387896299362183\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.04818058013916 | KNN Loss: 3.032338857650757 | BCE Loss: 1.0158419609069824\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.081857681274414 | KNN Loss: 3.0600616931915283 | BCE Loss: 1.0217959880828857\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.13844108581543 | KNN Loss: 3.089920997619629 | BCE Loss: 1.0485203266143799\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.1216254234313965 | KNN Loss: 3.09004282951355 | BCE Loss: 1.0315824747085571\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.131588459014893 | KNN Loss: 3.097017288208008 | BCE Loss: 1.0345711708068848\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.087933540344238 | KNN Loss: 3.071542978286743 | BCE Loss: 1.0163905620574951\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.117225646972656 | KNN Loss: 3.102795362472534 | BCE Loss: 1.014430046081543\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.123459815979004 | KNN Loss: 3.075462818145752 | BCE Loss: 1.047997236251831\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.106959342956543 | KNN Loss: 3.094311475753784 | BCE Loss: 1.0126476287841797\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.11325740814209 | KNN Loss: 3.090350866317749 | BCE Loss: 1.0229065418243408\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.081015586853027 | KNN Loss: 3.061344861984253 | BCE Loss: 1.0196707248687744\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.042077541351318 | KNN Loss: 3.060987710952759 | BCE Loss: 0.98108971118927\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.091656684875488 | KNN Loss: 3.1007237434387207 | BCE Loss: 0.990932822227478\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.100864887237549 | KNN Loss: 3.0837249755859375 | BCE Loss: 1.0171397924423218\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.122625827789307 | KNN Loss: 3.074968099594116 | BCE Loss: 1.0476576089859009\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.062809467315674 | KNN Loss: 3.074022054672241 | BCE Loss: 0.9887872338294983\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.098271369934082 | KNN Loss: 3.0658788681030273 | BCE Loss: 1.0323922634124756\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.129583358764648 | KNN Loss: 3.092284917831421 | BCE Loss: 1.037298560142517\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.104011058807373 | KNN Loss: 3.068586826324463 | BCE Loss: 1.0354242324829102\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.084776878356934 | KNN Loss: 3.0660879611968994 | BCE Loss: 1.0186889171600342\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.09073543548584 | KNN Loss: 3.091350793838501 | BCE Loss: 0.9993845224380493\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.093932151794434 | KNN Loss: 3.068847417831421 | BCE Loss: 1.0250844955444336\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.084911346435547 | KNN Loss: 3.0652880668640137 | BCE Loss: 1.0196232795715332\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.135293960571289 | KNN Loss: 3.097608804702759 | BCE Loss: 1.0376850366592407\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.12558650970459 | KNN Loss: 3.0849461555480957 | BCE Loss: 1.0406403541564941\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.093211650848389 | KNN Loss: 3.076624631881714 | BCE Loss: 1.0165868997573853\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.033489227294922 | KNN Loss: 3.0400047302246094 | BCE Loss: 0.9934843182563782\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.072099208831787 | KNN Loss: 3.027174711227417 | BCE Loss: 1.0449244976043701\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.089231967926025 | KNN Loss: 3.038426637649536 | BCE Loss: 1.0508054494857788\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.107251167297363 | KNN Loss: 3.0678164958953857 | BCE Loss: 1.0394344329833984\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.097395896911621 | KNN Loss: 3.062300443649292 | BCE Loss: 1.035095453262329\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.097904205322266 | KNN Loss: 3.0805883407592773 | BCE Loss: 1.0173159837722778\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.093758583068848 | KNN Loss: 3.064034938812256 | BCE Loss: 1.0297234058380127\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.104043960571289 | KNN Loss: 3.070873498916626 | BCE Loss: 1.033170223236084\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.113141059875488 | KNN Loss: 3.046351194381714 | BCE Loss: 1.0667901039123535\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.142487525939941 | KNN Loss: 3.084381341934204 | BCE Loss: 1.0581064224243164\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.120408058166504 | KNN Loss: 3.1031646728515625 | BCE Loss: 1.0172432661056519\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.084024429321289 | KNN Loss: 3.065889835357666 | BCE Loss: 1.0181344747543335\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.05375862121582 | KNN Loss: 3.0418107509613037 | BCE Loss: 1.0119481086730957\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.114624977111816 | KNN Loss: 3.0514075756073 | BCE Loss: 1.0632174015045166\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.11094856262207 | KNN Loss: 3.057313919067383 | BCE Loss: 1.0536346435546875\n",
      "Epoch   459: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.047690391540527 | KNN Loss: 3.0249366760253906 | BCE Loss: 1.0227539539337158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.108442783355713 | KNN Loss: 3.052743434906006 | BCE Loss: 1.055699348449707\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.067770004272461 | KNN Loss: 3.056199073791504 | BCE Loss: 1.011570930480957\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.112699508666992 | KNN Loss: 3.0618655681610107 | BCE Loss: 1.0508339405059814\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.136714458465576 | KNN Loss: 3.108010768890381 | BCE Loss: 1.0287036895751953\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.08895206451416 | KNN Loss: 3.0643649101257324 | BCE Loss: 1.0245869159698486\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.0924224853515625 | KNN Loss: 3.0653116703033447 | BCE Loss: 1.0271108150482178\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.111757755279541 | KNN Loss: 3.0552256107330322 | BCE Loss: 1.0565322637557983\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.106442451477051 | KNN Loss: 3.0806546211242676 | BCE Loss: 1.0257878303527832\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.052565097808838 | KNN Loss: 3.0407555103302 | BCE Loss: 1.0118097066879272\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.139352321624756 | KNN Loss: 3.089423894882202 | BCE Loss: 1.0499284267425537\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.115719795227051 | KNN Loss: 3.1080620288848877 | BCE Loss: 1.007657766342163\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.061490058898926 | KNN Loss: 3.042661190032959 | BCE Loss: 1.018829107284546\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.1294660568237305 | KNN Loss: 3.096733331680298 | BCE Loss: 1.0327324867248535\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.123943328857422 | KNN Loss: 3.0855603218078613 | BCE Loss: 1.0383830070495605\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.080374717712402 | KNN Loss: 3.056959629058838 | BCE Loss: 1.023414969444275\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.12760066986084 | KNN Loss: 3.0824320316314697 | BCE Loss: 1.0451688766479492\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.0530829429626465 | KNN Loss: 3.0480637550354004 | BCE Loss: 1.005019187927246\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.0819926261901855 | KNN Loss: 3.048804759979248 | BCE Loss: 1.0331878662109375\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.07328462600708 | KNN Loss: 3.07889723777771 | BCE Loss: 0.9943873286247253\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.182634353637695 | KNN Loss: 3.1495022773742676 | BCE Loss: 1.0331319570541382\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.114768028259277 | KNN Loss: 3.086080551147461 | BCE Loss: 1.0286873579025269\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.08314847946167 | KNN Loss: 3.050240993499756 | BCE Loss: 1.0329076051712036\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.098543167114258 | KNN Loss: 3.0966391563415527 | BCE Loss: 1.001903772354126\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.078313827514648 | KNN Loss: 3.0533559322357178 | BCE Loss: 1.0249578952789307\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.116353988647461 | KNN Loss: 3.0838990211486816 | BCE Loss: 1.0324548482894897\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.102146148681641 | KNN Loss: 3.054314613342285 | BCE Loss: 1.047831416130066\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.085726737976074 | KNN Loss: 3.063202381134033 | BCE Loss: 1.0225245952606201\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.132937431335449 | KNN Loss: 3.1103570461273193 | BCE Loss: 1.0225805044174194\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.118212699890137 | KNN Loss: 3.1117303371429443 | BCE Loss: 1.0064823627471924\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.080835819244385 | KNN Loss: 3.0467684268951416 | BCE Loss: 1.0340673923492432\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.106956958770752 | KNN Loss: 3.0717077255249023 | BCE Loss: 1.0352492332458496\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.072938442230225 | KNN Loss: 3.0734946727752686 | BCE Loss: 0.9994438886642456\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.144689559936523 | KNN Loss: 3.0827291011810303 | BCE Loss: 1.0619604587554932\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.114142417907715 | KNN Loss: 3.0725057125091553 | BCE Loss: 1.0416367053985596\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.083690166473389 | KNN Loss: 3.0579185485839844 | BCE Loss: 1.0257716178894043\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.067704677581787 | KNN Loss: 3.037727117538452 | BCE Loss: 1.0299776792526245\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.10295295715332 | KNN Loss: 3.0706682205200195 | BCE Loss: 1.0322847366333008\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.063985824584961 | KNN Loss: 3.054847478866577 | BCE Loss: 1.0091383457183838\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.164618968963623 | KNN Loss: 3.119204521179199 | BCE Loss: 1.0454143285751343\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.054154396057129 | KNN Loss: 3.055690288543701 | BCE Loss: 0.9984643459320068\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.107330799102783 | KNN Loss: 3.0768816471099854 | BCE Loss: 1.0304491519927979\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.101916790008545 | KNN Loss: 3.0535356998443604 | BCE Loss: 1.0483812093734741\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.041980743408203 | KNN Loss: 3.034528970718384 | BCE Loss: 1.0074517726898193\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.096160888671875 | KNN Loss: 3.0642831325531006 | BCE Loss: 1.0318779945373535\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.081664085388184 | KNN Loss: 3.0453810691833496 | BCE Loss: 1.0362828969955444\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.081609725952148 | KNN Loss: 3.0448412895202637 | BCE Loss: 1.0367681980133057\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.13636589050293 | KNN Loss: 3.1043641567230225 | BCE Loss: 1.0320019721984863\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.052890777587891 | KNN Loss: 3.0539791584014893 | BCE Loss: 0.9989114999771118\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.120255470275879 | KNN Loss: 3.085533857345581 | BCE Loss: 1.0347216129302979\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.111054420471191 | KNN Loss: 3.07584547996521 | BCE Loss: 1.0352089405059814\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.051603317260742 | KNN Loss: 3.070265054702759 | BCE Loss: 0.9813382029533386\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.103392601013184 | KNN Loss: 3.055330276489258 | BCE Loss: 1.0480620861053467\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.075939178466797 | KNN Loss: 3.0584189891815186 | BCE Loss: 1.0175204277038574\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.101637840270996 | KNN Loss: 3.0783870220184326 | BCE Loss: 1.0232508182525635\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.095104217529297 | KNN Loss: 3.063093423843384 | BCE Loss: 1.032010793685913\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.071480751037598 | KNN Loss: 3.038881540298462 | BCE Loss: 1.0325993299484253\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.093210220336914 | KNN Loss: 3.0629172325134277 | BCE Loss: 1.0302929878234863\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.025063514709473 | KNN Loss: 3.0233852863311768 | BCE Loss: 1.0016779899597168\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.114473819732666 | KNN Loss: 3.079535484313965 | BCE Loss: 1.0349383354187012\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.0614752769470215 | KNN Loss: 3.0333261489868164 | BCE Loss: 1.0281490087509155\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.108312606811523 | KNN Loss: 3.068587064743042 | BCE Loss: 1.0397253036499023\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.08119535446167 | KNN Loss: 3.035797357559204 | BCE Loss: 1.0453979969024658\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.069664478302002 | KNN Loss: 3.06284236907959 | BCE Loss: 1.0068219900131226\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.079015254974365 | KNN Loss: 3.049532413482666 | BCE Loss: 1.0294827222824097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.0749053955078125 | KNN Loss: 3.053128242492676 | BCE Loss: 1.0217769145965576\n",
      "Epoch   470: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.154278755187988 | KNN Loss: 3.1213719844818115 | BCE Loss: 1.0329068899154663\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.072127819061279 | KNN Loss: 3.065067768096924 | BCE Loss: 1.0070600509643555\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.122323036193848 | KNN Loss: 3.084078550338745 | BCE Loss: 1.038244605064392\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.089859962463379 | KNN Loss: 3.0588338375091553 | BCE Loss: 1.0310263633728027\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.087821006774902 | KNN Loss: 3.0655465126037598 | BCE Loss: 1.0222746133804321\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.069508075714111 | KNN Loss: 3.0296149253845215 | BCE Loss: 1.0398931503295898\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.07293701171875 | KNN Loss: 3.069488286972046 | BCE Loss: 1.003448486328125\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.1360883712768555 | KNN Loss: 3.098449468612671 | BCE Loss: 1.0376386642456055\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.061273574829102 | KNN Loss: 3.039215087890625 | BCE Loss: 1.022058367729187\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.074860095977783 | KNN Loss: 3.0595498085021973 | BCE Loss: 1.0153101682662964\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.12384557723999 | KNN Loss: 3.0947790145874023 | BCE Loss: 1.029066562652588\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.116787910461426 | KNN Loss: 3.0829620361328125 | BCE Loss: 1.0338261127471924\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.08341121673584 | KNN Loss: 3.074277877807617 | BCE Loss: 1.0091335773468018\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.069151878356934 | KNN Loss: 3.0774617195129395 | BCE Loss: 0.9916901588439941\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.1062703132629395 | KNN Loss: 3.075690984725952 | BCE Loss: 1.0305793285369873\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.113073348999023 | KNN Loss: 3.091193199157715 | BCE Loss: 1.0218801498413086\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.080566883087158 | KNN Loss: 3.046595573425293 | BCE Loss: 1.0339711904525757\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.07033109664917 | KNN Loss: 3.053931713104248 | BCE Loss: 1.0163992643356323\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.080907821655273 | KNN Loss: 3.0572993755340576 | BCE Loss: 1.0236082077026367\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.060677528381348 | KNN Loss: 3.0318446159362793 | BCE Loss: 1.0288326740264893\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.074145317077637 | KNN Loss: 3.039085865020752 | BCE Loss: 1.0350593328475952\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.114752292633057 | KNN Loss: 3.0904738903045654 | BCE Loss: 1.0242784023284912\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.153714179992676 | KNN Loss: 3.125394105911255 | BCE Loss: 1.028320074081421\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.067800521850586 | KNN Loss: 3.045261859893799 | BCE Loss: 1.022538423538208\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.106494903564453 | KNN Loss: 3.0861260890960693 | BCE Loss: 1.0203685760498047\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.1161909103393555 | KNN Loss: 3.0890276432037354 | BCE Loss: 1.0271633863449097\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.091312408447266 | KNN Loss: 3.0780580043792725 | BCE Loss: 1.013254165649414\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.123972415924072 | KNN Loss: 3.0931458473205566 | BCE Loss: 1.0308265686035156\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.102361679077148 | KNN Loss: 3.0899412631988525 | BCE Loss: 1.0124201774597168\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.078181266784668 | KNN Loss: 3.046715497970581 | BCE Loss: 1.031465768814087\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.061529636383057 | KNN Loss: 3.0712647438049316 | BCE Loss: 0.9902650117874146\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.07034969329834 | KNN Loss: 3.026376724243164 | BCE Loss: 1.0439729690551758\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.105279445648193 | KNN Loss: 3.0774269104003906 | BCE Loss: 1.0278526544570923\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.1105146408081055 | KNN Loss: 3.069401741027832 | BCE Loss: 1.0411126613616943\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.099704742431641 | KNN Loss: 3.0801138877868652 | BCE Loss: 1.0195910930633545\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.1059088706970215 | KNN Loss: 3.075839042663574 | BCE Loss: 1.0300698280334473\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.028215408325195 | KNN Loss: 3.025700807571411 | BCE Loss: 1.0025144815444946\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.070225715637207 | KNN Loss: 3.0495965480804443 | BCE Loss: 1.0206289291381836\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.077856063842773 | KNN Loss: 3.053840160369873 | BCE Loss: 1.0240156650543213\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.1872687339782715 | KNN Loss: 3.1679933071136475 | BCE Loss: 1.0192753076553345\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.133641242980957 | KNN Loss: 3.0862984657287598 | BCE Loss: 1.0473430156707764\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.0550537109375 | KNN Loss: 3.0523998737335205 | BCE Loss: 1.0026535987854004\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.102258205413818 | KNN Loss: 3.0543081760406494 | BCE Loss: 1.047950029373169\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.071019649505615 | KNN Loss: 3.0423686504364014 | BCE Loss: 1.0286509990692139\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.061389923095703 | KNN Loss: 3.04917049407959 | BCE Loss: 1.0122191905975342\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.0581183433532715 | KNN Loss: 3.044160842895508 | BCE Loss: 1.0139575004577637\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.087904930114746 | KNN Loss: 3.060166120529175 | BCE Loss: 1.0277386903762817\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.130409240722656 | KNN Loss: 3.109663724899292 | BCE Loss: 1.0207452774047852\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.0911946296691895 | KNN Loss: 3.0656843185424805 | BCE Loss: 1.025510311126709\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.068439960479736 | KNN Loss: 3.0637645721435547 | BCE Loss: 1.004675269126892\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.122901439666748 | KNN Loss: 3.0931262969970703 | BCE Loss: 1.0297751426696777\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.106887340545654 | KNN Loss: 3.0801374912261963 | BCE Loss: 1.026749849319458\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.1498308181762695 | KNN Loss: 3.1318469047546387 | BCE Loss: 1.0179836750030518\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.096628665924072 | KNN Loss: 3.061957836151123 | BCE Loss: 1.0346708297729492\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.096301078796387 | KNN Loss: 3.0737977027893066 | BCE Loss: 1.022503137588501\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.079038619995117 | KNN Loss: 3.0529606342315674 | BCE Loss: 1.0260781049728394\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.08896541595459 | KNN Loss: 3.0525124073028564 | BCE Loss: 1.0364532470703125\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.118949890136719 | KNN Loss: 3.087667465209961 | BCE Loss: 1.0312823057174683\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.159527778625488 | KNN Loss: 3.1277565956115723 | BCE Loss: 1.031770944595337\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.157244682312012 | KNN Loss: 3.120237112045288 | BCE Loss: 1.0370073318481445\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.0807623863220215 | KNN Loss: 3.0388734340667725 | BCE Loss: 1.0418890714645386\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.063584327697754 | KNN Loss: 3.028376817703247 | BCE Loss: 1.0352073907852173\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.060637474060059 | KNN Loss: 3.0548293590545654 | BCE Loss: 1.005807876586914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.045282363891602 | KNN Loss: 3.0295751094818115 | BCE Loss: 1.0157071352005005\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.100578308105469 | KNN Loss: 3.0946602821350098 | BCE Loss: 1.0059177875518799\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.094522953033447 | KNN Loss: 3.0792617797851562 | BCE Loss: 1.015261173248291\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.060585021972656 | KNN Loss: 3.048348903656006 | BCE Loss: 1.0122363567352295\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.15877103805542 | KNN Loss: 3.103651523590088 | BCE Loss: 1.055119514465332\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.100108623504639 | KNN Loss: 3.0888283252716064 | BCE Loss: 1.0112801790237427\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.133885383605957 | KNN Loss: 3.100940227508545 | BCE Loss: 1.032944917678833\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.09637975692749 | KNN Loss: 3.0578696727752686 | BCE Loss: 1.0385100841522217\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.094751834869385 | KNN Loss: 3.0659937858581543 | BCE Loss: 1.028757929801941\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.083534240722656 | KNN Loss: 3.0842652320861816 | BCE Loss: 0.9992690086364746\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.0658416748046875 | KNN Loss: 3.046036958694458 | BCE Loss: 1.0198047161102295\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.135488510131836 | KNN Loss: 3.0992982387542725 | BCE Loss: 1.0361905097961426\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.090295791625977 | KNN Loss: 3.0742526054382324 | BCE Loss: 1.016042947769165\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.088375091552734 | KNN Loss: 3.0680160522460938 | BCE Loss: 1.0203590393066406\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.174731731414795 | KNN Loss: 3.1113181114196777 | BCE Loss: 1.0634135007858276\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.067775726318359 | KNN Loss: 3.0446696281433105 | BCE Loss: 1.023106336593628\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.1278533935546875 | KNN Loss: 3.0679240226745605 | BCE Loss: 1.059929609298706\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.124332904815674 | KNN Loss: 3.0967485904693604 | BCE Loss: 1.0275843143463135\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.06920051574707 | KNN Loss: 3.0444424152374268 | BCE Loss: 1.0247578620910645\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.0919189453125 | KNN Loss: 3.084359645843506 | BCE Loss: 1.0075595378875732\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.110790252685547 | KNN Loss: 3.0618460178375244 | BCE Loss: 1.048944354057312\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.085127353668213 | KNN Loss: 3.0733141899108887 | BCE Loss: 1.0118130445480347\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.0985307693481445 | KNN Loss: 3.0639395713806152 | BCE Loss: 1.0345913171768188\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.0782670974731445 | KNN Loss: 3.0474750995635986 | BCE Loss: 1.030791997909546\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.07758903503418 | KNN Loss: 3.0666468143463135 | BCE Loss: 1.010941982269287\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.0630598068237305 | KNN Loss: 3.0374755859375 | BCE Loss: 1.0255844593048096\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.104637145996094 | KNN Loss: 3.080427646636963 | BCE Loss: 1.0242096185684204\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.0908918380737305 | KNN Loss: 3.0687901973724365 | BCE Loss: 1.022101640701294\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.105714797973633 | KNN Loss: 3.0721888542175293 | BCE Loss: 1.0335259437561035\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.087831974029541 | KNN Loss: 3.0703766345977783 | BCE Loss: 1.0174553394317627\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.0899224281311035 | KNN Loss: 3.075277328491211 | BCE Loss: 1.014644980430603\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.109993934631348 | KNN Loss: 3.075615882873535 | BCE Loss: 1.0343780517578125\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.063329696655273 | KNN Loss: 3.0481772422790527 | BCE Loss: 1.0151525735855103\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.161862850189209 | KNN Loss: 3.13661789894104 | BCE Loss: 1.0252450704574585\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.087831497192383 | KNN Loss: 3.067387819290161 | BCE Loss: 1.0204437971115112\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.077090263366699 | KNN Loss: 3.0524518489837646 | BCE Loss: 1.024638295173645\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.037014961242676 | KNN Loss: 3.0333666801452637 | BCE Loss: 1.0036481618881226\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.113778114318848 | KNN Loss: 3.0729410648345947 | BCE Loss: 1.0408368110656738\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.071550369262695 | KNN Loss: 3.0634384155273438 | BCE Loss: 1.0081120729446411\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.10111665725708 | KNN Loss: 3.0783162117004395 | BCE Loss: 1.0228004455566406\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.105666637420654 | KNN Loss: 3.0598134994506836 | BCE Loss: 1.0458531379699707\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.052030563354492 | KNN Loss: 3.0455739498138428 | BCE Loss: 1.0064568519592285\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.099802017211914 | KNN Loss: 3.0467846393585205 | BCE Loss: 1.0530171394348145\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.0431742668151855 | KNN Loss: 3.057875871658325 | BCE Loss: 0.9852982759475708\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.099885940551758 | KNN Loss: 3.064931869506836 | BCE Loss: 1.0349540710449219\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.087290287017822 | KNN Loss: 3.0504016876220703 | BCE Loss: 1.036888599395752\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.0663371086120605 | KNN Loss: 3.0430142879486084 | BCE Loss: 1.0233227014541626\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.104767799377441 | KNN Loss: 3.0816712379455566 | BCE Loss: 1.0230967998504639\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.160501480102539 | KNN Loss: 3.120494842529297 | BCE Loss: 1.0400065183639526\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.082967758178711 | KNN Loss: 3.0461533069610596 | BCE Loss: 1.0368146896362305\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.0857462882995605 | KNN Loss: 3.071011781692505 | BCE Loss: 1.0147346258163452\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.091821670532227 | KNN Loss: 3.0876967906951904 | BCE Loss: 1.004124641418457\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.07534122467041 | KNN Loss: 3.0487239360809326 | BCE Loss: 1.026617169380188\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.069729328155518 | KNN Loss: 3.067655086517334 | BCE Loss: 1.0020742416381836\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.150794506072998 | KNN Loss: 3.0965311527252197 | BCE Loss: 1.0542634725570679\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.135459899902344 | KNN Loss: 3.1108970642089844 | BCE Loss: 1.0245630741119385\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.070163726806641 | KNN Loss: 3.0620248317718506 | BCE Loss: 1.00813889503479\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.142533302307129 | KNN Loss: 3.0865859985351562 | BCE Loss: 1.0559470653533936\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.1242170333862305 | KNN Loss: 3.11533784866333 | BCE Loss: 1.0088791847229004\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.035298824310303 | KNN Loss: 3.0392634868621826 | BCE Loss: 0.9960353374481201\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.112689971923828 | KNN Loss: 3.0860979557037354 | BCE Loss: 1.0265917778015137\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.090959072113037 | KNN Loss: 3.0675504207611084 | BCE Loss: 1.0234086513519287\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.100110054016113 | KNN Loss: 3.0763111114501953 | BCE Loss: 1.0237987041473389\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.082186698913574 | KNN Loss: 3.0483956336975098 | BCE Loss: 1.0337910652160645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.086661338806152 | KNN Loss: 3.0787205696105957 | BCE Loss: 1.0079410076141357\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.0501017570495605 | KNN Loss: 3.0588767528533936 | BCE Loss: 0.9912250638008118\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.078587055206299 | KNN Loss: 3.0444648265838623 | BCE Loss: 1.0341222286224365\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.038388252258301 | KNN Loss: 3.0538175106048584 | BCE Loss: 0.9845705032348633\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.095339775085449 | KNN Loss: 3.0685949325561523 | BCE Loss: 1.0267446041107178\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.064320087432861 | KNN Loss: 3.0709352493286133 | BCE Loss: 0.993384838104248\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.153305530548096 | KNN Loss: 3.1162731647491455 | BCE Loss: 1.0370323657989502\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.1040754318237305 | KNN Loss: 3.054288148880005 | BCE Loss: 1.0497870445251465\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.030147552490234 | KNN Loss: 3.0495681762695312 | BCE Loss: 0.980579137802124\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.082042217254639 | KNN Loss: 3.039691209793091 | BCE Loss: 1.0423510074615479\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.094003677368164 | KNN Loss: 3.0657958984375 | BCE Loss: 1.028207540512085\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.125111103057861 | KNN Loss: 3.105712652206421 | BCE Loss: 1.0193984508514404\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.110071182250977 | KNN Loss: 3.0993189811706543 | BCE Loss: 1.0107520818710327\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.066709041595459 | KNN Loss: 3.031616687774658 | BCE Loss: 1.0350923538208008\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.053142070770264 | KNN Loss: 3.031003952026367 | BCE Loss: 1.022137999534607\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.108710765838623 | KNN Loss: 3.065288543701172 | BCE Loss: 1.0434222221374512\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.102355003356934 | KNN Loss: 3.065246105194092 | BCE Loss: 1.0371086597442627\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.128955841064453 | KNN Loss: 3.111905336380005 | BCE Loss: 1.0170505046844482\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.107466697692871 | KNN Loss: 3.07867431640625 | BCE Loss: 1.0287925004959106\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.0953779220581055 | KNN Loss: 3.0792112350463867 | BCE Loss: 1.0161669254302979\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.084157466888428 | KNN Loss: 3.059757947921753 | BCE Loss: 1.0243995189666748\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.08858585357666 | KNN Loss: 3.062551975250244 | BCE Loss: 1.0260339975357056\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.080523490905762 | KNN Loss: 3.059248924255371 | BCE Loss: 1.0212743282318115\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.124073505401611 | KNN Loss: 3.1010003089904785 | BCE Loss: 1.0230731964111328\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.122656345367432 | KNN Loss: 3.0655996799468994 | BCE Loss: 1.0570566654205322\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.05448579788208 | KNN Loss: 3.063446283340454 | BCE Loss: 0.9910396337509155\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.087368488311768 | KNN Loss: 3.0692968368530273 | BCE Loss: 1.0180717706680298\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.102171897888184 | KNN Loss: 3.072050094604492 | BCE Loss: 1.0301215648651123\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.030229568481445 | KNN Loss: 3.0381460189819336 | BCE Loss: 0.9920833706855774\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.082747459411621 | KNN Loss: 3.0503671169281006 | BCE Loss: 1.0323805809020996\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.076911449432373 | KNN Loss: 3.053582191467285 | BCE Loss: 1.0233291387557983\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.093379020690918 | KNN Loss: 3.068690061569214 | BCE Loss: 1.024688720703125\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.150900840759277 | KNN Loss: 3.1220695972442627 | BCE Loss: 1.0288310050964355\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.1301751136779785 | KNN Loss: 3.1026127338409424 | BCE Loss: 1.0275623798370361\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.077918529510498 | KNN Loss: 3.034104108810425 | BCE Loss: 1.0438144207000732\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.041039943695068 | KNN Loss: 3.0417981147766113 | BCE Loss: 0.999241828918457\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.07314395904541 | KNN Loss: 3.0467910766601562 | BCE Loss: 1.026353120803833\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.0895891189575195 | KNN Loss: 3.0620412826538086 | BCE Loss: 1.02754807472229\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.123604774475098 | KNN Loss: 3.1028244495391846 | BCE Loss: 1.0207804441452026\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.046384811401367 | KNN Loss: 3.027477741241455 | BCE Loss: 1.018906831741333\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.0883588790893555 | KNN Loss: 3.0408852100372314 | BCE Loss: 1.0474739074707031\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.081220626831055 | KNN Loss: 3.0732500553131104 | BCE Loss: 1.0079704523086548\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.032217979431152 | KNN Loss: 3.039290189743042 | BCE Loss: 0.9929280281066895\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.095410346984863 | KNN Loss: 3.0568039417266846 | BCE Loss: 1.0386065244674683\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.122291564941406 | KNN Loss: 3.085131883621216 | BCE Loss: 1.0371599197387695\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.120223045349121 | KNN Loss: 3.076632022857666 | BCE Loss: 1.043591022491455\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.04156494140625 | KNN Loss: 3.0142223834991455 | BCE Loss: 1.0273423194885254\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.0925612449646 | KNN Loss: 3.096964120864868 | BCE Loss: 0.9955969452857971\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.115769386291504 | KNN Loss: 3.09651780128479 | BCE Loss: 1.019251823425293\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.118173122406006 | KNN Loss: 3.0771117210388184 | BCE Loss: 1.0410614013671875\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.058927536010742 | KNN Loss: 3.0496461391448975 | BCE Loss: 1.0092815160751343\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.073599815368652 | KNN Loss: 3.0367376804351807 | BCE Loss: 1.0368623733520508\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.161610126495361 | KNN Loss: 3.1192684173583984 | BCE Loss: 1.042341709136963\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2370,  2.7679,  2.7122,  3.9908,  3.8847,  0.8725,  3.0327,  2.5618,\n",
      "          2.2363,  1.6938,  2.2864,  2.5082,  0.6424,  2.1298,  1.3242,  1.0866,\n",
      "          2.1310,  2.3357,  2.0970,  2.6638,  1.7034,  2.1554,  1.3457,  1.5651,\n",
      "          2.0842,  1.5516,  1.5837,  1.6459,  1.2925,  0.4660, -0.3103,  0.5602,\n",
      "          0.1078,  1.1122,  1.7155,  1.2236,  0.4354,  2.3027,  0.8398,  1.3784,\n",
      "          0.6218, -0.6223, -0.1409,  2.6776,  2.5059,  0.5122, -0.3030,  0.2509,\n",
      "          1.6245,  1.8960,  2.0517, -0.2963,  1.5548,  0.5244, -0.7659,  1.3300,\n",
      "          1.7193,  1.1864,  1.4329,  1.3418,  0.5420,  0.9076, -0.0140,  1.9244,\n",
      "          1.2947,  1.9178, -2.1642,  0.5483,  2.1487,  2.4974,  2.8048,  0.6341,\n",
      "          1.2793,  2.8174,  1.6929,  0.9510, -0.0296,  0.4604,  0.0906,  1.9074,\n",
      "         -0.1839,  0.4141,  0.8577, -0.2542,  0.4213, -1.1251, -2.7708, -0.2355,\n",
      "          0.6990, -2.0052,  0.0347, -0.0140, -0.6624, -1.0185,  0.7804,  0.9752,\n",
      "         -0.7505, -0.7863,  0.3757,  1.0268,  0.8330, -1.7363,  0.6656,  1.4171,\n",
      "         -1.4017, -0.9764, -0.3806,  0.1101, -1.0788, -1.5446, -0.9706, -2.7483,\n",
      "         -0.3309,  1.8980,  1.5955, -0.3631, -0.5665,  0.2001,  1.7736, -3.0561,\n",
      "          0.2774, -0.3077,  0.1769, -0.4957,  0.2214, -0.8488, -1.0197,  1.1998,\n",
      "          0.4430, -0.5396,  0.4945, -0.5320, -1.3027, -0.3838, -0.3918,  0.8373,\n",
      "         -0.4206,  0.0667, -2.4023, -0.9393, -1.2944,  0.6842, -2.4250, -0.9603,\n",
      "         -1.0896, -0.5812, -1.8777, -0.7687, -2.8369, -1.8985, -1.4396, -0.3654,\n",
      "         -1.7350,  0.3684, -1.7547, -0.4894, -3.9362, -0.0566, -0.0107, -0.7188,\n",
      "         -2.6747, -1.9519, -1.4572, -1.5946, -2.1813, -2.2477, -4.2104]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-4.2104, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.9908, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0666cf6277c1464ea905723bbb35d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 81.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267f722805884a078c9c2e4da2ffdbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328e89183d8046d3bbf69c0b77bc74eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f169e503f04b41993bd2115728869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 018 | Total loss: 9.623 | Reg loss: 0.009 | Tree loss: 9.623 | Accuracy: 0.000000 | 0.241 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 018 | Total loss: 9.620 | Reg loss: 0.009 | Tree loss: 9.620 | Accuracy: 0.000000 | 0.229 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 018 | Total loss: 9.619 | Reg loss: 0.008 | Tree loss: 9.619 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 018 | Total loss: 9.612 | Reg loss: 0.008 | Tree loss: 9.612 | Accuracy: 0.000000 | 0.225 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 018 | Total loss: 9.609 | Reg loss: 0.008 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 018 | Total loss: 9.609 | Reg loss: 0.007 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 018 | Total loss: 9.600 | Reg loss: 0.007 | Tree loss: 9.600 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 018 | Total loss: 9.601 | Reg loss: 0.007 | Tree loss: 9.601 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 018 | Total loss: 9.600 | Reg loss: 0.007 | Tree loss: 9.600 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 018 | Total loss: 9.597 | Reg loss: 0.006 | Tree loss: 9.597 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 018 | Total loss: 9.589 | Reg loss: 0.006 | Tree loss: 9.589 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 018 | Total loss: 9.594 | Reg loss: 0.006 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 018 | Total loss: 9.586 | Reg loss: 0.006 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 018 | Total loss: 9.587 | Reg loss: 0.006 | Tree loss: 9.587 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 018 | Total loss: 9.588 | Reg loss: 0.006 | Tree loss: 9.588 | Accuracy: 0.000000 | 0.225 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 018 | Total loss: 9.585 | Reg loss: 0.006 | Tree loss: 9.585 | Accuracy: 0.000000 | 0.225 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 018 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.000000 | 0.225 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 018 | Total loss: 9.572 | Reg loss: 0.006 | Tree loss: 9.572 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 01 | Batch: 000 / 018 | Total loss: 9.593 | Reg loss: 0.003 | Tree loss: 9.593 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 018 | Total loss: 9.586 | Reg loss: 0.003 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 018 | Total loss: 9.586 | Reg loss: 0.003 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 018 | Total loss: 9.584 | Reg loss: 0.003 | Tree loss: 9.584 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 018 | Total loss: 9.583 | Reg loss: 0.003 | Tree loss: 9.583 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 018 | Total loss: 9.575 | Reg loss: 0.004 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.226 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 018 | Total loss: 9.577 | Reg loss: 0.004 | Tree loss: 9.577 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 018 | Total loss: 9.573 | Reg loss: 0.004 | Tree loss: 9.573 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 018 | Total loss: 9.573 | Reg loss: 0.004 | Tree loss: 9.573 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 018 | Total loss: 9.569 | Reg loss: 0.005 | Tree loss: 9.569 | Accuracy: 0.000000 | 0.227 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 018 | Total loss: 9.562 | Reg loss: 0.005 | Tree loss: 9.562 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 018 | Total loss: 9.564 | Reg loss: 0.005 | Tree loss: 9.564 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 018 | Total loss: 9.562 | Reg loss: 0.005 | Tree loss: 9.562 | Accuracy: 0.001953 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 018 | Total loss: 9.554 | Reg loss: 0.006 | Tree loss: 9.554 | Accuracy: 0.000000 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 018 | Total loss: 9.557 | Reg loss: 0.006 | Tree loss: 9.557 | Accuracy: 0.001953 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 018 | Total loss: 9.557 | Reg loss: 0.006 | Tree loss: 9.557 | Accuracy: 0.003906 | 0.228 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 018 | Total loss: 9.551 | Reg loss: 0.006 | Tree loss: 9.551 | Accuracy: 0.015625 | 0.229 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 018 | Total loss: 9.549 | Reg loss: 0.006 | Tree loss: 9.549 | Accuracy: 0.012422 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 02 | Batch: 000 / 018 | Total loss: 9.563 | Reg loss: 0.004 | Tree loss: 9.563 | Accuracy: 0.001953 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 018 | Total loss: 9.560 | Reg loss: 0.004 | Tree loss: 9.560 | Accuracy: 0.000000 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 018 | Total loss: 9.566 | Reg loss: 0.004 | Tree loss: 9.566 | Accuracy: 0.003906 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 018 | Total loss: 9.559 | Reg loss: 0.004 | Tree loss: 9.559 | Accuracy: 0.005859 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 018 | Total loss: 9.555 | Reg loss: 0.004 | Tree loss: 9.555 | Accuracy: 0.019531 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 018 | Total loss: 9.553 | Reg loss: 0.005 | Tree loss: 9.553 | Accuracy: 0.023438 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 018 | Total loss: 9.553 | Reg loss: 0.005 | Tree loss: 9.553 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 018 | Total loss: 9.550 | Reg loss: 0.005 | Tree loss: 9.550 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 018 | Total loss: 9.544 | Reg loss: 0.005 | Tree loss: 9.544 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 018 | Total loss: 9.543 | Reg loss: 0.005 | Tree loss: 9.543 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 018 | Total loss: 9.543 | Reg loss: 0.006 | Tree loss: 9.543 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 018 | Total loss: 9.533 | Reg loss: 0.006 | Tree loss: 9.533 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 018 | Total loss: 9.534 | Reg loss: 0.006 | Tree loss: 9.534 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 018 | Total loss: 9.539 | Reg loss: 0.006 | Tree loss: 9.539 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 018 | Total loss: 9.535 | Reg loss: 0.006 | Tree loss: 9.535 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 018 | Total loss: 9.528 | Reg loss: 0.007 | Tree loss: 9.528 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 018 | Total loss: 9.524 | Reg loss: 0.007 | Tree loss: 9.524 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 018 | Total loss: 9.529 | Reg loss: 0.007 | Tree loss: 9.529 | Accuracy: 0.057971 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 03 | Batch: 000 / 018 | Total loss: 9.542 | Reg loss: 0.005 | Tree loss: 9.542 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 018 | Total loss: 9.542 | Reg loss: 0.005 | Tree loss: 9.542 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 018 | Total loss: 9.536 | Reg loss: 0.005 | Tree loss: 9.536 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 018 | Total loss: 9.533 | Reg loss: 0.005 | Tree loss: 9.533 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 018 | Total loss: 9.531 | Reg loss: 0.005 | Tree loss: 9.531 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 018 | Total loss: 9.531 | Reg loss: 0.006 | Tree loss: 9.531 | Accuracy: 0.091797 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Batch: 006 / 018 | Total loss: 9.526 | Reg loss: 0.006 | Tree loss: 9.526 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 018 | Total loss: 9.529 | Reg loss: 0.006 | Tree loss: 9.529 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 018 | Total loss: 9.515 | Reg loss: 0.006 | Tree loss: 9.515 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 018 | Total loss: 9.517 | Reg loss: 0.006 | Tree loss: 9.517 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 018 | Total loss: 9.521 | Reg loss: 0.006 | Tree loss: 9.521 | Accuracy: 0.050781 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 018 | Total loss: 9.517 | Reg loss: 0.007 | Tree loss: 9.517 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 018 | Total loss: 9.517 | Reg loss: 0.007 | Tree loss: 9.517 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 018 | Total loss: 9.510 | Reg loss: 0.007 | Tree loss: 9.510 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 018 | Total loss: 9.503 | Reg loss: 0.007 | Tree loss: 9.503 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 018 | Total loss: 9.503 | Reg loss: 0.008 | Tree loss: 9.503 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 018 | Total loss: 9.498 | Reg loss: 0.008 | Tree loss: 9.498 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 018 | Total loss: 9.493 | Reg loss: 0.008 | Tree loss: 9.493 | Accuracy: 0.093168 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 04 | Batch: 000 / 018 | Total loss: 9.515 | Reg loss: 0.006 | Tree loss: 9.515 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 018 | Total loss: 9.511 | Reg loss: 0.006 | Tree loss: 9.511 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 018 | Total loss: 9.512 | Reg loss: 0.006 | Tree loss: 9.512 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 018 | Total loss: 9.513 | Reg loss: 0.006 | Tree loss: 9.513 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 018 | Total loss: 9.510 | Reg loss: 0.006 | Tree loss: 9.510 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 018 | Total loss: 9.502 | Reg loss: 0.007 | Tree loss: 9.502 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 018 | Total loss: 9.503 | Reg loss: 0.007 | Tree loss: 9.503 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 018 | Total loss: 9.501 | Reg loss: 0.007 | Tree loss: 9.501 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 018 | Total loss: 9.500 | Reg loss: 0.007 | Tree loss: 9.500 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 018 | Total loss: 9.488 | Reg loss: 0.007 | Tree loss: 9.488 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 018 | Total loss: 9.495 | Reg loss: 0.007 | Tree loss: 9.495 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 018 | Total loss: 9.482 | Reg loss: 0.008 | Tree loss: 9.482 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 018 | Total loss: 9.483 | Reg loss: 0.008 | Tree loss: 9.483 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 018 | Total loss: 9.485 | Reg loss: 0.008 | Tree loss: 9.485 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 018 | Total loss: 9.477 | Reg loss: 0.008 | Tree loss: 9.477 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 018 | Total loss: 9.478 | Reg loss: 0.009 | Tree loss: 9.478 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 018 | Total loss: 9.469 | Reg loss: 0.009 | Tree loss: 9.469 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 018 | Total loss: 9.464 | Reg loss: 0.009 | Tree loss: 9.464 | Accuracy: 0.080745 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 05 | Batch: 000 / 018 | Total loss: 9.498 | Reg loss: 0.007 | Tree loss: 9.498 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 018 | Total loss: 9.489 | Reg loss: 0.007 | Tree loss: 9.489 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 018 | Total loss: 9.487 | Reg loss: 0.007 | Tree loss: 9.487 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 018 | Total loss: 9.480 | Reg loss: 0.007 | Tree loss: 9.480 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 018 | Total loss: 9.481 | Reg loss: 0.007 | Tree loss: 9.481 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 018 | Total loss: 9.478 | Reg loss: 0.008 | Tree loss: 9.478 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 018 | Total loss: 9.477 | Reg loss: 0.008 | Tree loss: 9.477 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 018 | Total loss: 9.469 | Reg loss: 0.008 | Tree loss: 9.469 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 018 | Total loss: 9.464 | Reg loss: 0.008 | Tree loss: 9.464 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 018 | Total loss: 9.462 | Reg loss: 0.008 | Tree loss: 9.462 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 018 | Total loss: 9.462 | Reg loss: 0.009 | Tree loss: 9.462 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 018 | Total loss: 9.448 | Reg loss: 0.009 | Tree loss: 9.448 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 018 | Total loss: 9.450 | Reg loss: 0.009 | Tree loss: 9.450 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 018 | Total loss: 9.447 | Reg loss: 0.009 | Tree loss: 9.447 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 018 | Total loss: 9.441 | Reg loss: 0.009 | Tree loss: 9.441 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 018 | Total loss: 9.448 | Reg loss: 0.010 | Tree loss: 9.448 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 018 | Total loss: 9.437 | Reg loss: 0.010 | Tree loss: 9.437 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 018 | Total loss: 9.424 | Reg loss: 0.010 | Tree loss: 9.424 | Accuracy: 0.101449 | 0.228 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 06 | Batch: 000 / 018 | Total loss: 9.461 | Reg loss: 0.008 | Tree loss: 9.461 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 018 | Total loss: 9.453 | Reg loss: 0.008 | Tree loss: 9.453 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 018 | Total loss: 9.458 | Reg loss: 0.008 | Tree loss: 9.458 | Accuracy: 0.048828 | 0.229 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 018 | Total loss: 9.451 | Reg loss: 0.008 | Tree loss: 9.451 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 018 | Total loss: 9.444 | Reg loss: 0.009 | Tree loss: 9.444 | Accuracy: 0.080078 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 018 | Total loss: 9.440 | Reg loss: 0.009 | Tree loss: 9.440 | Accuracy: 0.070312 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 018 | Total loss: 9.438 | Reg loss: 0.009 | Tree loss: 9.438 | Accuracy: 0.080078 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 018 | Total loss: 9.428 | Reg loss: 0.009 | Tree loss: 9.428 | Accuracy: 0.076172 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 018 | Total loss: 9.438 | Reg loss: 0.009 | Tree loss: 9.438 | Accuracy: 0.058594 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 018 | Total loss: 9.427 | Reg loss: 0.009 | Tree loss: 9.427 | Accuracy: 0.060547 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 018 | Total loss: 9.418 | Reg loss: 0.010 | Tree loss: 9.418 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 018 | Total loss: 9.408 | Reg loss: 0.010 | Tree loss: 9.408 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 018 | Total loss: 9.418 | Reg loss: 0.010 | Tree loss: 9.418 | Accuracy: 0.064453 | 0.228 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 013 / 018 | Total loss: 9.405 | Reg loss: 0.010 | Tree loss: 9.405 | Accuracy: 0.074219 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 018 | Total loss: 9.405 | Reg loss: 0.011 | Tree loss: 9.405 | Accuracy: 0.076172 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 018 | Total loss: 9.404 | Reg loss: 0.011 | Tree loss: 9.404 | Accuracy: 0.068359 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 018 | Total loss: 9.385 | Reg loss: 0.011 | Tree loss: 9.385 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 018 | Total loss: 9.388 | Reg loss: 0.012 | Tree loss: 9.388 | Accuracy: 0.066253 | 0.228 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 07 | Batch: 000 / 018 | Total loss: 9.429 | Reg loss: 0.009 | Tree loss: 9.429 | Accuracy: 0.078125 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 018 | Total loss: 9.422 | Reg loss: 0.009 | Tree loss: 9.422 | Accuracy: 0.062500 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 018 | Total loss: 9.415 | Reg loss: 0.009 | Tree loss: 9.415 | Accuracy: 0.070312 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 018 | Total loss: 9.414 | Reg loss: 0.010 | Tree loss: 9.414 | Accuracy: 0.054688 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 018 | Total loss: 9.416 | Reg loss: 0.010 | Tree loss: 9.416 | Accuracy: 0.058594 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 018 | Total loss: 9.397 | Reg loss: 0.010 | Tree loss: 9.397 | Accuracy: 0.056641 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 018 | Total loss: 9.393 | Reg loss: 0.010 | Tree loss: 9.393 | Accuracy: 0.074219 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 018 | Total loss: 9.383 | Reg loss: 0.010 | Tree loss: 9.383 | Accuracy: 0.082031 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 018 | Total loss: 9.399 | Reg loss: 0.011 | Tree loss: 9.399 | Accuracy: 0.052734 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 018 | Total loss: 9.380 | Reg loss: 0.011 | Tree loss: 9.380 | Accuracy: 0.060547 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 018 | Total loss: 9.367 | Reg loss: 0.011 | Tree loss: 9.367 | Accuracy: 0.083984 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 018 | Total loss: 9.360 | Reg loss: 0.011 | Tree loss: 9.360 | Accuracy: 0.078125 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 018 | Total loss: 9.361 | Reg loss: 0.012 | Tree loss: 9.361 | Accuracy: 0.070312 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 018 | Total loss: 9.350 | Reg loss: 0.012 | Tree loss: 9.350 | Accuracy: 0.076172 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 018 | Total loss: 9.349 | Reg loss: 0.012 | Tree loss: 9.349 | Accuracy: 0.064453 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 018 | Total loss: 9.334 | Reg loss: 0.013 | Tree loss: 9.334 | Accuracy: 0.074219 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 018 | Total loss: 9.310 | Reg loss: 0.013 | Tree loss: 9.310 | Accuracy: 0.099609 | 0.228 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 018 | Total loss: 9.313 | Reg loss: 0.013 | Tree loss: 9.313 | Accuracy: 0.066253 | 0.228 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 08 | Batch: 000 / 018 | Total loss: 9.380 | Reg loss: 0.011 | Tree loss: 9.380 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 018 | Total loss: 9.373 | Reg loss: 0.011 | Tree loss: 9.373 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 018 | Total loss: 9.362 | Reg loss: 0.011 | Tree loss: 9.362 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 018 | Total loss: 9.360 | Reg loss: 0.011 | Tree loss: 9.360 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 018 | Total loss: 9.345 | Reg loss: 0.011 | Tree loss: 9.345 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 018 | Total loss: 9.341 | Reg loss: 0.011 | Tree loss: 9.341 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 018 | Total loss: 9.335 | Reg loss: 0.012 | Tree loss: 9.335 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 018 | Total loss: 9.330 | Reg loss: 0.012 | Tree loss: 9.330 | Accuracy: 0.066406 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 018 | Total loss: 9.319 | Reg loss: 0.012 | Tree loss: 9.319 | Accuracy: 0.070312 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 018 | Total loss: 9.307 | Reg loss: 0.012 | Tree loss: 9.307 | Accuracy: 0.064453 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 018 | Total loss: 9.304 | Reg loss: 0.013 | Tree loss: 9.304 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 018 | Total loss: 9.288 | Reg loss: 0.013 | Tree loss: 9.288 | Accuracy: 0.076172 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 018 | Total loss: 9.284 | Reg loss: 0.013 | Tree loss: 9.284 | Accuracy: 0.066406 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 018 | Total loss: 9.285 | Reg loss: 0.014 | Tree loss: 9.285 | Accuracy: 0.064453 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 018 | Total loss: 9.282 | Reg loss: 0.014 | Tree loss: 9.282 | Accuracy: 0.062500 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 018 | Total loss: 9.246 | Reg loss: 0.014 | Tree loss: 9.246 | Accuracy: 0.060547 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 018 | Total loss: 9.233 | Reg loss: 0.015 | Tree loss: 9.233 | Accuracy: 0.085938 | 0.228 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 018 | Total loss: 9.226 | Reg loss: 0.015 | Tree loss: 9.226 | Accuracy: 0.064182 | 0.228 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 09 | Batch: 000 / 018 | Total loss: 9.315 | Reg loss: 0.012 | Tree loss: 9.315 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 018 | Total loss: 9.304 | Reg loss: 0.012 | Tree loss: 9.304 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 018 | Total loss: 9.296 | Reg loss: 0.012 | Tree loss: 9.296 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 018 | Total loss: 9.280 | Reg loss: 0.012 | Tree loss: 9.280 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 018 | Total loss: 9.279 | Reg loss: 0.013 | Tree loss: 9.279 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 018 | Total loss: 9.259 | Reg loss: 0.013 | Tree loss: 9.259 | Accuracy: 0.095703 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 018 | Total loss: 9.268 | Reg loss: 0.013 | Tree loss: 9.268 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 018 | Total loss: 9.238 | Reg loss: 0.013 | Tree loss: 9.238 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 018 | Total loss: 9.241 | Reg loss: 0.014 | Tree loss: 9.241 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 018 | Total loss: 9.225 | Reg loss: 0.014 | Tree loss: 9.225 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 018 | Total loss: 9.209 | Reg loss: 0.014 | Tree loss: 9.209 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 018 | Total loss: 9.197 | Reg loss: 0.015 | Tree loss: 9.197 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 018 | Total loss: 9.196 | Reg loss: 0.015 | Tree loss: 9.196 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 018 | Total loss: 9.173 | Reg loss: 0.015 | Tree loss: 9.173 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 018 | Total loss: 9.173 | Reg loss: 0.016 | Tree loss: 9.173 | Accuracy: 0.056641 | 0.228 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 018 | Total loss: 9.149 | Reg loss: 0.016 | Tree loss: 9.149 | Accuracy: 0.062500 | 0.228 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 018 | Total loss: 9.119 | Reg loss: 0.017 | Tree loss: 9.119 | Accuracy: 0.072266 | 0.228 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 018 | Total loss: 9.129 | Reg loss: 0.017 | Tree loss: 9.129 | Accuracy: 0.064182 | 0.228 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 000 / 018 | Total loss: 9.246 | Reg loss: 0.014 | Tree loss: 9.246 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 018 | Total loss: 9.230 | Reg loss: 0.014 | Tree loss: 9.230 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 018 | Total loss: 9.204 | Reg loss: 0.014 | Tree loss: 9.204 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 018 | Total loss: 9.193 | Reg loss: 0.014 | Tree loss: 9.193 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 018 | Total loss: 9.177 | Reg loss: 0.014 | Tree loss: 9.177 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 018 | Total loss: 9.167 | Reg loss: 0.015 | Tree loss: 9.167 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 018 | Total loss: 9.158 | Reg loss: 0.015 | Tree loss: 9.158 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 018 | Total loss: 9.152 | Reg loss: 0.015 | Tree loss: 9.152 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 018 | Total loss: 9.121 | Reg loss: 0.015 | Tree loss: 9.121 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 018 | Total loss: 9.099 | Reg loss: 0.016 | Tree loss: 9.099 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 018 | Total loss: 9.117 | Reg loss: 0.016 | Tree loss: 9.117 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 018 | Total loss: 9.095 | Reg loss: 0.017 | Tree loss: 9.095 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 018 | Total loss: 9.060 | Reg loss: 0.017 | Tree loss: 9.060 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 018 | Total loss: 9.065 | Reg loss: 0.017 | Tree loss: 9.065 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 018 | Total loss: 9.068 | Reg loss: 0.018 | Tree loss: 9.068 | Accuracy: 0.042969 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 018 | Total loss: 9.009 | Reg loss: 0.018 | Tree loss: 9.009 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 018 | Total loss: 9.025 | Reg loss: 0.019 | Tree loss: 9.025 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 018 | Total loss: 8.965 | Reg loss: 0.019 | Tree loss: 8.965 | Accuracy: 0.097308 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 11 | Batch: 000 / 018 | Total loss: 9.154 | Reg loss: 0.016 | Tree loss: 9.154 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 018 | Total loss: 9.129 | Reg loss: 0.016 | Tree loss: 9.129 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 018 | Total loss: 9.102 | Reg loss: 0.016 | Tree loss: 9.102 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 018 | Total loss: 9.097 | Reg loss: 0.016 | Tree loss: 9.097 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 018 | Total loss: 9.081 | Reg loss: 0.016 | Tree loss: 9.081 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 018 | Total loss: 9.038 | Reg loss: 0.016 | Tree loss: 9.038 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 018 | Total loss: 9.039 | Reg loss: 0.017 | Tree loss: 9.039 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 018 | Total loss: 9.030 | Reg loss: 0.017 | Tree loss: 9.030 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 018 | Total loss: 9.010 | Reg loss: 0.017 | Tree loss: 9.010 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 018 | Total loss: 9.001 | Reg loss: 0.018 | Tree loss: 9.001 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 018 | Total loss: 8.966 | Reg loss: 0.018 | Tree loss: 8.966 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 018 | Total loss: 8.963 | Reg loss: 0.018 | Tree loss: 8.963 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 018 | Total loss: 8.946 | Reg loss: 0.019 | Tree loss: 8.946 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 018 | Total loss: 8.921 | Reg loss: 0.019 | Tree loss: 8.921 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 018 | Total loss: 8.892 | Reg loss: 0.020 | Tree loss: 8.892 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 018 | Total loss: 8.893 | Reg loss: 0.020 | Tree loss: 8.893 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 018 | Total loss: 8.857 | Reg loss: 0.020 | Tree loss: 8.857 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 018 | Total loss: 8.822 | Reg loss: 0.021 | Tree loss: 8.822 | Accuracy: 0.080745 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 12 | Batch: 000 / 018 | Total loss: 9.023 | Reg loss: 0.017 | Tree loss: 9.023 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 018 | Total loss: 8.998 | Reg loss: 0.017 | Tree loss: 8.998 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 018 | Total loss: 8.976 | Reg loss: 0.018 | Tree loss: 8.976 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 018 | Total loss: 8.986 | Reg loss: 0.018 | Tree loss: 8.986 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 018 | Total loss: 8.955 | Reg loss: 0.018 | Tree loss: 8.955 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 018 | Total loss: 8.946 | Reg loss: 0.018 | Tree loss: 8.946 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 018 | Total loss: 8.910 | Reg loss: 0.018 | Tree loss: 8.910 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 018 | Total loss: 8.872 | Reg loss: 0.019 | Tree loss: 8.872 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 018 | Total loss: 8.873 | Reg loss: 0.019 | Tree loss: 8.873 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 018 | Total loss: 8.856 | Reg loss: 0.019 | Tree loss: 8.856 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 018 | Total loss: 8.808 | Reg loss: 0.020 | Tree loss: 8.808 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 018 | Total loss: 8.833 | Reg loss: 0.020 | Tree loss: 8.833 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 018 | Total loss: 8.808 | Reg loss: 0.021 | Tree loss: 8.808 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 018 | Total loss: 8.764 | Reg loss: 0.021 | Tree loss: 8.764 | Accuracy: 0.091797 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 018 | Total loss: 8.737 | Reg loss: 0.021 | Tree loss: 8.737 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 018 | Total loss: 8.737 | Reg loss: 0.022 | Tree loss: 8.737 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 018 | Total loss: 8.719 | Reg loss: 0.022 | Tree loss: 8.719 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 018 | Total loss: 8.694 | Reg loss: 0.023 | Tree loss: 8.694 | Accuracy: 0.068323 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 13 | Batch: 000 / 018 | Total loss: 8.893 | Reg loss: 0.019 | Tree loss: 8.893 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 018 | Total loss: 8.880 | Reg loss: 0.019 | Tree loss: 8.880 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 018 | Total loss: 8.860 | Reg loss: 0.019 | Tree loss: 8.860 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 018 | Total loss: 8.832 | Reg loss: 0.020 | Tree loss: 8.832 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 018 | Total loss: 8.803 | Reg loss: 0.020 | Tree loss: 8.803 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 018 | Total loss: 8.792 | Reg loss: 0.020 | Tree loss: 8.792 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 018 | Total loss: 8.803 | Reg loss: 0.020 | Tree loss: 8.803 | Accuracy: 0.058594 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 007 / 018 | Total loss: 8.751 | Reg loss: 0.020 | Tree loss: 8.751 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 018 | Total loss: 8.734 | Reg loss: 0.021 | Tree loss: 8.734 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 018 | Total loss: 8.686 | Reg loss: 0.021 | Tree loss: 8.686 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 018 | Total loss: 8.690 | Reg loss: 0.021 | Tree loss: 8.690 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 018 | Total loss: 8.651 | Reg loss: 0.022 | Tree loss: 8.651 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 018 | Total loss: 8.628 | Reg loss: 0.022 | Tree loss: 8.628 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 018 | Total loss: 8.623 | Reg loss: 0.023 | Tree loss: 8.623 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 018 | Total loss: 8.597 | Reg loss: 0.023 | Tree loss: 8.597 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 018 | Total loss: 8.569 | Reg loss: 0.023 | Tree loss: 8.569 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 018 | Total loss: 8.546 | Reg loss: 0.024 | Tree loss: 8.546 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 018 | Total loss: 8.533 | Reg loss: 0.024 | Tree loss: 8.533 | Accuracy: 0.080745 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 14 | Batch: 000 / 018 | Total loss: 8.750 | Reg loss: 0.021 | Tree loss: 8.750 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 018 | Total loss: 8.727 | Reg loss: 0.021 | Tree loss: 8.727 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 018 | Total loss: 8.720 | Reg loss: 0.021 | Tree loss: 8.720 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 018 | Total loss: 8.686 | Reg loss: 0.021 | Tree loss: 8.686 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 018 | Total loss: 8.649 | Reg loss: 0.021 | Tree loss: 8.649 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 018 | Total loss: 8.645 | Reg loss: 0.022 | Tree loss: 8.645 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 018 | Total loss: 8.628 | Reg loss: 0.022 | Tree loss: 8.628 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 018 | Total loss: 8.609 | Reg loss: 0.022 | Tree loss: 8.609 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 018 | Total loss: 8.567 | Reg loss: 0.022 | Tree loss: 8.567 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 018 | Total loss: 8.555 | Reg loss: 0.023 | Tree loss: 8.555 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 018 | Total loss: 8.514 | Reg loss: 0.023 | Tree loss: 8.514 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 018 | Total loss: 8.513 | Reg loss: 0.023 | Tree loss: 8.513 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 018 | Total loss: 8.488 | Reg loss: 0.024 | Tree loss: 8.488 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 018 | Total loss: 8.486 | Reg loss: 0.024 | Tree loss: 8.486 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 018 | Total loss: 8.428 | Reg loss: 0.024 | Tree loss: 8.428 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 018 | Total loss: 8.411 | Reg loss: 0.025 | Tree loss: 8.411 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 018 | Total loss: 8.394 | Reg loss: 0.025 | Tree loss: 8.394 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 018 | Total loss: 8.359 | Reg loss: 0.025 | Tree loss: 8.359 | Accuracy: 0.066253 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 15 | Batch: 000 / 018 | Total loss: 8.613 | Reg loss: 0.022 | Tree loss: 8.613 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 018 | Total loss: 8.590 | Reg loss: 0.022 | Tree loss: 8.590 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 018 | Total loss: 8.559 | Reg loss: 0.023 | Tree loss: 8.559 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 018 | Total loss: 8.557 | Reg loss: 0.023 | Tree loss: 8.557 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 018 | Total loss: 8.490 | Reg loss: 0.023 | Tree loss: 8.490 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 018 | Total loss: 8.489 | Reg loss: 0.023 | Tree loss: 8.489 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 018 | Total loss: 8.500 | Reg loss: 0.023 | Tree loss: 8.500 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 018 | Total loss: 8.451 | Reg loss: 0.023 | Tree loss: 8.451 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 018 | Total loss: 8.425 | Reg loss: 0.024 | Tree loss: 8.425 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 018 | Total loss: 8.415 | Reg loss: 0.024 | Tree loss: 8.415 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 018 | Total loss: 8.354 | Reg loss: 0.024 | Tree loss: 8.354 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 018 | Total loss: 8.335 | Reg loss: 0.025 | Tree loss: 8.335 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 018 | Total loss: 8.344 | Reg loss: 0.025 | Tree loss: 8.344 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 018 | Total loss: 8.279 | Reg loss: 0.025 | Tree loss: 8.279 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 018 | Total loss: 8.259 | Reg loss: 0.025 | Tree loss: 8.259 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 018 | Total loss: 8.251 | Reg loss: 0.026 | Tree loss: 8.251 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 018 | Total loss: 8.219 | Reg loss: 0.026 | Tree loss: 8.219 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 018 | Total loss: 8.207 | Reg loss: 0.026 | Tree loss: 8.207 | Accuracy: 0.080745 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 16 | Batch: 000 / 018 | Total loss: 8.441 | Reg loss: 0.024 | Tree loss: 8.441 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 018 | Total loss: 8.439 | Reg loss: 0.024 | Tree loss: 8.439 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 018 | Total loss: 8.386 | Reg loss: 0.024 | Tree loss: 8.386 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 018 | Total loss: 8.363 | Reg loss: 0.024 | Tree loss: 8.363 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 018 | Total loss: 8.352 | Reg loss: 0.024 | Tree loss: 8.352 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 018 | Total loss: 8.347 | Reg loss: 0.024 | Tree loss: 8.347 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 018 | Total loss: 8.305 | Reg loss: 0.024 | Tree loss: 8.305 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 018 | Total loss: 8.274 | Reg loss: 0.025 | Tree loss: 8.274 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 018 | Total loss: 8.269 | Reg loss: 0.025 | Tree loss: 8.269 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 018 | Total loss: 8.252 | Reg loss: 0.025 | Tree loss: 8.252 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 018 | Total loss: 8.223 | Reg loss: 0.025 | Tree loss: 8.223 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 018 | Total loss: 8.212 | Reg loss: 0.026 | Tree loss: 8.212 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 018 | Total loss: 8.138 | Reg loss: 0.026 | Tree loss: 8.138 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 018 | Total loss: 8.145 | Reg loss: 0.026 | Tree loss: 8.145 | Accuracy: 0.095703 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 014 / 018 | Total loss: 8.108 | Reg loss: 0.026 | Tree loss: 8.108 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 018 | Total loss: 8.128 | Reg loss: 0.027 | Tree loss: 8.128 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 018 | Total loss: 8.071 | Reg loss: 0.027 | Tree loss: 8.071 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 018 | Total loss: 8.070 | Reg loss: 0.027 | Tree loss: 8.070 | Accuracy: 0.049689 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 17 | Batch: 000 / 018 | Total loss: 8.305 | Reg loss: 0.025 | Tree loss: 8.305 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 018 | Total loss: 8.274 | Reg loss: 0.025 | Tree loss: 8.274 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 018 | Total loss: 8.257 | Reg loss: 0.025 | Tree loss: 8.257 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 018 | Total loss: 8.237 | Reg loss: 0.025 | Tree loss: 8.237 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 018 | Total loss: 8.174 | Reg loss: 0.025 | Tree loss: 8.174 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 018 | Total loss: 8.192 | Reg loss: 0.025 | Tree loss: 8.192 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 018 | Total loss: 8.180 | Reg loss: 0.026 | Tree loss: 8.180 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 018 | Total loss: 8.117 | Reg loss: 0.026 | Tree loss: 8.117 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 018 | Total loss: 8.090 | Reg loss: 0.026 | Tree loss: 8.090 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 018 | Total loss: 8.096 | Reg loss: 0.026 | Tree loss: 8.096 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 018 | Total loss: 8.054 | Reg loss: 0.026 | Tree loss: 8.054 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 018 | Total loss: 8.079 | Reg loss: 0.027 | Tree loss: 8.079 | Accuracy: 0.044922 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 018 | Total loss: 8.022 | Reg loss: 0.027 | Tree loss: 8.022 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 018 | Total loss: 7.953 | Reg loss: 0.027 | Tree loss: 7.953 | Accuracy: 0.097656 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 018 | Total loss: 7.949 | Reg loss: 0.027 | Tree loss: 7.949 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 018 | Total loss: 7.924 | Reg loss: 0.028 | Tree loss: 7.924 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 018 | Total loss: 7.912 | Reg loss: 0.028 | Tree loss: 7.912 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 018 | Total loss: 7.897 | Reg loss: 0.028 | Tree loss: 7.897 | Accuracy: 0.078675 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 18 | Batch: 000 / 018 | Total loss: 8.108 | Reg loss: 0.026 | Tree loss: 8.108 | Accuracy: 0.099609 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 018 | Total loss: 8.130 | Reg loss: 0.026 | Tree loss: 8.130 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 018 | Total loss: 8.060 | Reg loss: 0.026 | Tree loss: 8.060 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 018 | Total loss: 8.058 | Reg loss: 0.026 | Tree loss: 8.058 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 018 | Total loss: 8.044 | Reg loss: 0.026 | Tree loss: 8.044 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 018 | Total loss: 8.038 | Reg loss: 0.026 | Tree loss: 8.038 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 018 | Total loss: 7.988 | Reg loss: 0.026 | Tree loss: 7.988 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 018 | Total loss: 7.945 | Reg loss: 0.027 | Tree loss: 7.945 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 018 | Total loss: 7.952 | Reg loss: 0.027 | Tree loss: 7.952 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 018 | Total loss: 7.905 | Reg loss: 0.027 | Tree loss: 7.905 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 018 | Total loss: 7.908 | Reg loss: 0.027 | Tree loss: 7.908 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 018 | Total loss: 7.870 | Reg loss: 0.027 | Tree loss: 7.870 | Accuracy: 0.091797 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 018 | Total loss: 7.883 | Reg loss: 0.028 | Tree loss: 7.883 | Accuracy: 0.042969 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 018 | Total loss: 7.862 | Reg loss: 0.028 | Tree loss: 7.862 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 018 | Total loss: 7.845 | Reg loss: 0.028 | Tree loss: 7.845 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 018 | Total loss: 7.806 | Reg loss: 0.028 | Tree loss: 7.806 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 018 | Total loss: 7.775 | Reg loss: 0.028 | Tree loss: 7.775 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 018 | Total loss: 7.771 | Reg loss: 0.029 | Tree loss: 7.771 | Accuracy: 0.064182 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 19 | Batch: 000 / 018 | Total loss: 7.984 | Reg loss: 0.027 | Tree loss: 7.984 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 018 | Total loss: 7.974 | Reg loss: 0.027 | Tree loss: 7.974 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 018 | Total loss: 7.900 | Reg loss: 0.027 | Tree loss: 7.900 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 018 | Total loss: 7.920 | Reg loss: 0.027 | Tree loss: 7.920 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 018 | Total loss: 7.884 | Reg loss: 0.027 | Tree loss: 7.884 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 018 | Total loss: 7.891 | Reg loss: 0.027 | Tree loss: 7.891 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 018 | Total loss: 7.876 | Reg loss: 0.027 | Tree loss: 7.876 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 018 | Total loss: 7.809 | Reg loss: 0.027 | Tree loss: 7.809 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 018 | Total loss: 7.818 | Reg loss: 0.027 | Tree loss: 7.818 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 018 | Total loss: 7.756 | Reg loss: 0.028 | Tree loss: 7.756 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 018 | Total loss: 7.757 | Reg loss: 0.028 | Tree loss: 7.757 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 018 | Total loss: 7.734 | Reg loss: 0.028 | Tree loss: 7.734 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 018 | Total loss: 7.717 | Reg loss: 0.028 | Tree loss: 7.717 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 018 | Total loss: 7.699 | Reg loss: 0.028 | Tree loss: 7.699 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 018 | Total loss: 7.657 | Reg loss: 0.029 | Tree loss: 7.657 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 018 | Total loss: 7.592 | Reg loss: 0.029 | Tree loss: 7.592 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 018 | Total loss: 7.647 | Reg loss: 0.029 | Tree loss: 7.647 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 018 | Total loss: 7.607 | Reg loss: 0.029 | Tree loss: 7.607 | Accuracy: 0.057971 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 20 | Batch: 000 / 018 | Total loss: 7.804 | Reg loss: 0.027 | Tree loss: 7.804 | Accuracy: 0.078125 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 001 / 018 | Total loss: 7.786 | Reg loss: 0.027 | Tree loss: 7.786 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 018 | Total loss: 7.817 | Reg loss: 0.028 | Tree loss: 7.817 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 018 | Total loss: 7.795 | Reg loss: 0.028 | Tree loss: 7.795 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 018 | Total loss: 7.715 | Reg loss: 0.028 | Tree loss: 7.715 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 018 | Total loss: 7.742 | Reg loss: 0.028 | Tree loss: 7.742 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 018 | Total loss: 7.679 | Reg loss: 0.028 | Tree loss: 7.679 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 018 | Total loss: 7.675 | Reg loss: 0.028 | Tree loss: 7.675 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 018 | Total loss: 7.649 | Reg loss: 0.028 | Tree loss: 7.649 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 018 | Total loss: 7.625 | Reg loss: 0.028 | Tree loss: 7.625 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 018 | Total loss: 7.618 | Reg loss: 0.028 | Tree loss: 7.618 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 018 | Total loss: 7.589 | Reg loss: 0.029 | Tree loss: 7.589 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 018 | Total loss: 7.528 | Reg loss: 0.029 | Tree loss: 7.528 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 018 | Total loss: 7.522 | Reg loss: 0.029 | Tree loss: 7.522 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 018 | Total loss: 7.513 | Reg loss: 0.029 | Tree loss: 7.513 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 018 | Total loss: 7.531 | Reg loss: 0.029 | Tree loss: 7.531 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 018 | Total loss: 7.516 | Reg loss: 0.029 | Tree loss: 7.516 | Accuracy: 0.048828 | 0.229 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 018 | Total loss: 7.454 | Reg loss: 0.030 | Tree loss: 7.454 | Accuracy: 0.070393 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 21 | Batch: 000 / 018 | Total loss: 7.678 | Reg loss: 0.028 | Tree loss: 7.678 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 018 | Total loss: 7.658 | Reg loss: 0.028 | Tree loss: 7.658 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 018 | Total loss: 7.630 | Reg loss: 0.028 | Tree loss: 7.630 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 018 | Total loss: 7.621 | Reg loss: 0.028 | Tree loss: 7.621 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 018 | Total loss: 7.585 | Reg loss: 0.028 | Tree loss: 7.585 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 018 | Total loss: 7.570 | Reg loss: 0.028 | Tree loss: 7.570 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 018 | Total loss: 7.560 | Reg loss: 0.028 | Tree loss: 7.560 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 018 | Total loss: 7.583 | Reg loss: 0.029 | Tree loss: 7.583 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 018 | Total loss: 7.531 | Reg loss: 0.029 | Tree loss: 7.531 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 018 | Total loss: 7.464 | Reg loss: 0.029 | Tree loss: 7.464 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 018 | Total loss: 7.434 | Reg loss: 0.029 | Tree loss: 7.434 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 018 | Total loss: 7.442 | Reg loss: 0.029 | Tree loss: 7.442 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 018 | Total loss: 7.407 | Reg loss: 0.029 | Tree loss: 7.407 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 018 | Total loss: 7.396 | Reg loss: 0.029 | Tree loss: 7.396 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 018 | Total loss: 7.383 | Reg loss: 0.029 | Tree loss: 7.383 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 018 | Total loss: 7.353 | Reg loss: 0.030 | Tree loss: 7.353 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 018 | Total loss: 7.346 | Reg loss: 0.030 | Tree loss: 7.346 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 018 | Total loss: 7.318 | Reg loss: 0.030 | Tree loss: 7.318 | Accuracy: 0.076605 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 22 | Batch: 000 / 018 | Total loss: 7.518 | Reg loss: 0.029 | Tree loss: 7.518 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 018 | Total loss: 7.478 | Reg loss: 0.029 | Tree loss: 7.478 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 018 | Total loss: 7.499 | Reg loss: 0.029 | Tree loss: 7.499 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 018 | Total loss: 7.437 | Reg loss: 0.029 | Tree loss: 7.437 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 018 | Total loss: 7.493 | Reg loss: 0.029 | Tree loss: 7.493 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 018 | Total loss: 7.401 | Reg loss: 0.029 | Tree loss: 7.401 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 018 | Total loss: 7.393 | Reg loss: 0.029 | Tree loss: 7.393 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 018 | Total loss: 7.387 | Reg loss: 0.029 | Tree loss: 7.387 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 018 | Total loss: 7.380 | Reg loss: 0.029 | Tree loss: 7.380 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 018 | Total loss: 7.404 | Reg loss: 0.029 | Tree loss: 7.404 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 018 | Total loss: 7.298 | Reg loss: 0.029 | Tree loss: 7.298 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 018 | Total loss: 7.319 | Reg loss: 0.029 | Tree loss: 7.319 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 018 | Total loss: 7.265 | Reg loss: 0.030 | Tree loss: 7.265 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 018 | Total loss: 7.264 | Reg loss: 0.030 | Tree loss: 7.264 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 018 | Total loss: 7.252 | Reg loss: 0.030 | Tree loss: 7.252 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 018 | Total loss: 7.252 | Reg loss: 0.030 | Tree loss: 7.252 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 018 | Total loss: 7.202 | Reg loss: 0.030 | Tree loss: 7.202 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 018 | Total loss: 7.187 | Reg loss: 0.030 | Tree loss: 7.187 | Accuracy: 0.064182 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 23 | Batch: 000 / 018 | Total loss: 7.373 | Reg loss: 0.029 | Tree loss: 7.373 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 018 | Total loss: 7.369 | Reg loss: 0.029 | Tree loss: 7.369 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 018 | Total loss: 7.335 | Reg loss: 0.029 | Tree loss: 7.335 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 018 | Total loss: 7.326 | Reg loss: 0.029 | Tree loss: 7.326 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 018 | Total loss: 7.306 | Reg loss: 0.029 | Tree loss: 7.306 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 018 | Total loss: 7.289 | Reg loss: 0.029 | Tree loss: 7.289 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 018 | Total loss: 7.288 | Reg loss: 0.029 | Tree loss: 7.288 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 018 | Total loss: 7.305 | Reg loss: 0.029 | Tree loss: 7.305 | Accuracy: 0.048828 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 008 / 018 | Total loss: 7.245 | Reg loss: 0.029 | Tree loss: 7.245 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 018 | Total loss: 7.199 | Reg loss: 0.030 | Tree loss: 7.199 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 018 | Total loss: 7.229 | Reg loss: 0.030 | Tree loss: 7.229 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 018 | Total loss: 7.122 | Reg loss: 0.030 | Tree loss: 7.122 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 018 | Total loss: 7.137 | Reg loss: 0.030 | Tree loss: 7.137 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 018 | Total loss: 7.106 | Reg loss: 0.030 | Tree loss: 7.106 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 018 | Total loss: 7.120 | Reg loss: 0.030 | Tree loss: 7.120 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 018 | Total loss: 7.085 | Reg loss: 0.030 | Tree loss: 7.085 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 018 | Total loss: 7.079 | Reg loss: 0.030 | Tree loss: 7.079 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 018 | Total loss: 7.045 | Reg loss: 0.031 | Tree loss: 7.045 | Accuracy: 0.078675 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 24 | Batch: 000 / 018 | Total loss: 7.267 | Reg loss: 0.029 | Tree loss: 7.267 | Accuracy: 0.099609 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 018 | Total loss: 7.194 | Reg loss: 0.029 | Tree loss: 7.194 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 018 | Total loss: 7.192 | Reg loss: 0.029 | Tree loss: 7.192 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 018 | Total loss: 7.209 | Reg loss: 0.029 | Tree loss: 7.209 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 018 | Total loss: 7.136 | Reg loss: 0.030 | Tree loss: 7.136 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 018 | Total loss: 7.135 | Reg loss: 0.030 | Tree loss: 7.135 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 018 | Total loss: 7.100 | Reg loss: 0.030 | Tree loss: 7.100 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 018 | Total loss: 7.119 | Reg loss: 0.030 | Tree loss: 7.119 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 018 | Total loss: 7.115 | Reg loss: 0.030 | Tree loss: 7.115 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 018 | Total loss: 7.048 | Reg loss: 0.030 | Tree loss: 7.048 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 018 | Total loss: 7.067 | Reg loss: 0.030 | Tree loss: 7.067 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 018 | Total loss: 6.997 | Reg loss: 0.030 | Tree loss: 6.997 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 018 | Total loss: 7.030 | Reg loss: 0.030 | Tree loss: 7.030 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 018 | Total loss: 7.070 | Reg loss: 0.030 | Tree loss: 7.070 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 018 | Total loss: 7.013 | Reg loss: 0.030 | Tree loss: 7.013 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 018 | Total loss: 6.960 | Reg loss: 0.031 | Tree loss: 6.960 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 018 | Total loss: 6.944 | Reg loss: 0.031 | Tree loss: 6.944 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 018 | Total loss: 6.966 | Reg loss: 0.031 | Tree loss: 6.966 | Accuracy: 0.062112 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 25 | Batch: 000 / 018 | Total loss: 7.121 | Reg loss: 0.030 | Tree loss: 7.121 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 018 | Total loss: 7.132 | Reg loss: 0.030 | Tree loss: 7.132 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 018 | Total loss: 7.095 | Reg loss: 0.030 | Tree loss: 7.095 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 018 | Total loss: 7.052 | Reg loss: 0.030 | Tree loss: 7.052 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 018 | Total loss: 7.063 | Reg loss: 0.030 | Tree loss: 7.063 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 018 | Total loss: 7.077 | Reg loss: 0.030 | Tree loss: 7.077 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 018 | Total loss: 6.991 | Reg loss: 0.030 | Tree loss: 6.991 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 018 | Total loss: 6.995 | Reg loss: 0.030 | Tree loss: 6.995 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 018 | Total loss: 6.907 | Reg loss: 0.030 | Tree loss: 6.907 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 018 | Total loss: 6.953 | Reg loss: 0.030 | Tree loss: 6.953 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 018 | Total loss: 6.942 | Reg loss: 0.030 | Tree loss: 6.942 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 018 | Total loss: 6.884 | Reg loss: 0.030 | Tree loss: 6.884 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 018 | Total loss: 6.916 | Reg loss: 0.030 | Tree loss: 6.916 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 018 | Total loss: 6.862 | Reg loss: 0.031 | Tree loss: 6.862 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 018 | Total loss: 6.801 | Reg loss: 0.031 | Tree loss: 6.801 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 018 | Total loss: 6.821 | Reg loss: 0.031 | Tree loss: 6.821 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 018 | Total loss: 6.817 | Reg loss: 0.031 | Tree loss: 6.817 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 018 | Total loss: 6.792 | Reg loss: 0.031 | Tree loss: 6.792 | Accuracy: 0.072464 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 26 | Batch: 000 / 018 | Total loss: 7.011 | Reg loss: 0.030 | Tree loss: 7.011 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 018 | Total loss: 6.982 | Reg loss: 0.030 | Tree loss: 6.982 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 018 | Total loss: 6.961 | Reg loss: 0.030 | Tree loss: 6.961 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 018 | Total loss: 6.921 | Reg loss: 0.030 | Tree loss: 6.921 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 018 | Total loss: 6.910 | Reg loss: 0.030 | Tree loss: 6.910 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 018 | Total loss: 6.899 | Reg loss: 0.030 | Tree loss: 6.899 | Accuracy: 0.048828 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 018 | Total loss: 6.865 | Reg loss: 0.030 | Tree loss: 6.865 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 018 | Total loss: 6.859 | Reg loss: 0.030 | Tree loss: 6.859 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 018 | Total loss: 6.830 | Reg loss: 0.030 | Tree loss: 6.830 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 018 | Total loss: 6.755 | Reg loss: 0.030 | Tree loss: 6.755 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 018 | Total loss: 6.761 | Reg loss: 0.031 | Tree loss: 6.761 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 018 | Total loss: 6.785 | Reg loss: 0.031 | Tree loss: 6.785 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 018 | Total loss: 6.789 | Reg loss: 0.031 | Tree loss: 6.789 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 018 | Total loss: 6.720 | Reg loss: 0.031 | Tree loss: 6.720 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 018 | Total loss: 6.757 | Reg loss: 0.031 | Tree loss: 6.757 | Accuracy: 0.064453 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 015 / 018 | Total loss: 6.740 | Reg loss: 0.031 | Tree loss: 6.740 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 018 | Total loss: 6.697 | Reg loss: 0.031 | Tree loss: 6.697 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 018 | Total loss: 6.721 | Reg loss: 0.031 | Tree loss: 6.721 | Accuracy: 0.070393 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 27 | Batch: 000 / 018 | Total loss: 6.855 | Reg loss: 0.030 | Tree loss: 6.855 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 018 | Total loss: 6.908 | Reg loss: 0.030 | Tree loss: 6.908 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 018 | Total loss: 6.876 | Reg loss: 0.030 | Tree loss: 6.876 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 018 | Total loss: 6.793 | Reg loss: 0.030 | Tree loss: 6.793 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 018 | Total loss: 6.761 | Reg loss: 0.030 | Tree loss: 6.761 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 018 | Total loss: 6.778 | Reg loss: 0.030 | Tree loss: 6.778 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 018 | Total loss: 6.737 | Reg loss: 0.030 | Tree loss: 6.737 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 018 | Total loss: 6.742 | Reg loss: 0.031 | Tree loss: 6.742 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 018 | Total loss: 6.709 | Reg loss: 0.031 | Tree loss: 6.709 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 018 | Total loss: 6.697 | Reg loss: 0.031 | Tree loss: 6.697 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 018 | Total loss: 6.727 | Reg loss: 0.031 | Tree loss: 6.727 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 018 | Total loss: 6.632 | Reg loss: 0.031 | Tree loss: 6.632 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 018 | Total loss: 6.616 | Reg loss: 0.031 | Tree loss: 6.616 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 018 | Total loss: 6.587 | Reg loss: 0.031 | Tree loss: 6.587 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 018 | Total loss: 6.580 | Reg loss: 0.031 | Tree loss: 6.580 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 018 | Total loss: 6.610 | Reg loss: 0.031 | Tree loss: 6.610 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 018 | Total loss: 6.584 | Reg loss: 0.031 | Tree loss: 6.584 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 018 | Total loss: 6.565 | Reg loss: 0.031 | Tree loss: 6.565 | Accuracy: 0.068323 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 28 | Batch: 000 / 018 | Total loss: 6.735 | Reg loss: 0.030 | Tree loss: 6.735 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 018 | Total loss: 6.721 | Reg loss: 0.030 | Tree loss: 6.721 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 018 | Total loss: 6.683 | Reg loss: 0.030 | Tree loss: 6.683 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 018 | Total loss: 6.719 | Reg loss: 0.031 | Tree loss: 6.719 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 018 | Total loss: 6.664 | Reg loss: 0.031 | Tree loss: 6.664 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 018 | Total loss: 6.646 | Reg loss: 0.031 | Tree loss: 6.646 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 018 | Total loss: 6.675 | Reg loss: 0.031 | Tree loss: 6.675 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 018 | Total loss: 6.624 | Reg loss: 0.031 | Tree loss: 6.624 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 018 | Total loss: 6.529 | Reg loss: 0.031 | Tree loss: 6.529 | Accuracy: 0.097656 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 018 | Total loss: 6.581 | Reg loss: 0.031 | Tree loss: 6.581 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 018 | Total loss: 6.551 | Reg loss: 0.031 | Tree loss: 6.551 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 018 | Total loss: 6.546 | Reg loss: 0.031 | Tree loss: 6.546 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 018 | Total loss: 6.555 | Reg loss: 0.031 | Tree loss: 6.555 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 018 | Total loss: 6.523 | Reg loss: 0.031 | Tree loss: 6.523 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 018 | Total loss: 6.452 | Reg loss: 0.031 | Tree loss: 6.452 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 018 | Total loss: 6.521 | Reg loss: 0.031 | Tree loss: 6.521 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 018 | Total loss: 6.460 | Reg loss: 0.032 | Tree loss: 6.460 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 018 | Total loss: 6.441 | Reg loss: 0.032 | Tree loss: 6.441 | Accuracy: 0.070393 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 29 | Batch: 000 / 018 | Total loss: 6.632 | Reg loss: 0.031 | Tree loss: 6.632 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 018 | Total loss: 6.607 | Reg loss: 0.031 | Tree loss: 6.607 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 018 | Total loss: 6.596 | Reg loss: 0.031 | Tree loss: 6.596 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 018 | Total loss: 6.567 | Reg loss: 0.031 | Tree loss: 6.567 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 018 | Total loss: 6.531 | Reg loss: 0.031 | Tree loss: 6.531 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 018 | Total loss: 6.548 | Reg loss: 0.031 | Tree loss: 6.548 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 018 | Total loss: 6.530 | Reg loss: 0.031 | Tree loss: 6.530 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 018 | Total loss: 6.532 | Reg loss: 0.031 | Tree loss: 6.532 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 018 | Total loss: 6.478 | Reg loss: 0.031 | Tree loss: 6.478 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 018 | Total loss: 6.448 | Reg loss: 0.031 | Tree loss: 6.448 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 018 | Total loss: 6.460 | Reg loss: 0.031 | Tree loss: 6.460 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 018 | Total loss: 6.430 | Reg loss: 0.031 | Tree loss: 6.430 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 018 | Total loss: 6.438 | Reg loss: 0.031 | Tree loss: 6.438 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 018 | Total loss: 6.397 | Reg loss: 0.031 | Tree loss: 6.397 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 018 | Total loss: 6.327 | Reg loss: 0.031 | Tree loss: 6.327 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 018 | Total loss: 6.361 | Reg loss: 0.032 | Tree loss: 6.361 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 018 | Total loss: 6.281 | Reg loss: 0.032 | Tree loss: 6.281 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 018 | Total loss: 6.300 | Reg loss: 0.032 | Tree loss: 6.300 | Accuracy: 0.078675 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 30 | Batch: 000 / 018 | Total loss: 6.494 | Reg loss: 0.031 | Tree loss: 6.494 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 018 | Total loss: 6.514 | Reg loss: 0.031 | Tree loss: 6.514 | Accuracy: 0.058594 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 002 / 018 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 018 | Total loss: 6.446 | Reg loss: 0.031 | Tree loss: 6.446 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 018 | Total loss: 6.426 | Reg loss: 0.031 | Tree loss: 6.426 | Accuracy: 0.091797 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 018 | Total loss: 6.396 | Reg loss: 0.031 | Tree loss: 6.396 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 018 | Total loss: 6.397 | Reg loss: 0.031 | Tree loss: 6.397 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 018 | Total loss: 6.339 | Reg loss: 0.031 | Tree loss: 6.339 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 018 | Total loss: 6.326 | Reg loss: 0.031 | Tree loss: 6.326 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 018 | Total loss: 6.380 | Reg loss: 0.031 | Tree loss: 6.380 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 018 | Total loss: 6.297 | Reg loss: 0.031 | Tree loss: 6.297 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 018 | Total loss: 6.328 | Reg loss: 0.031 | Tree loss: 6.328 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 018 | Total loss: 6.294 | Reg loss: 0.031 | Tree loss: 6.294 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 018 | Total loss: 6.299 | Reg loss: 0.032 | Tree loss: 6.299 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 018 | Total loss: 6.209 | Reg loss: 0.032 | Tree loss: 6.209 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 018 | Total loss: 6.222 | Reg loss: 0.032 | Tree loss: 6.222 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 018 | Total loss: 6.213 | Reg loss: 0.032 | Tree loss: 6.213 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 018 | Total loss: 6.236 | Reg loss: 0.032 | Tree loss: 6.236 | Accuracy: 0.070393 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 31 | Batch: 000 / 018 | Total loss: 6.366 | Reg loss: 0.031 | Tree loss: 6.366 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 018 | Total loss: 6.366 | Reg loss: 0.031 | Tree loss: 6.366 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 018 | Total loss: 6.316 | Reg loss: 0.031 | Tree loss: 6.316 | Accuracy: 0.091797 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 018 | Total loss: 6.361 | Reg loss: 0.031 | Tree loss: 6.361 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 018 | Total loss: 6.283 | Reg loss: 0.031 | Tree loss: 6.283 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 018 | Total loss: 6.284 | Reg loss: 0.031 | Tree loss: 6.284 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 018 | Total loss: 6.264 | Reg loss: 0.031 | Tree loss: 6.264 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 018 | Total loss: 6.290 | Reg loss: 0.031 | Tree loss: 6.290 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 018 | Total loss: 6.253 | Reg loss: 0.031 | Tree loss: 6.253 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 018 | Total loss: 6.228 | Reg loss: 0.031 | Tree loss: 6.228 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 018 | Total loss: 6.273 | Reg loss: 0.031 | Tree loss: 6.273 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 018 | Total loss: 6.189 | Reg loss: 0.031 | Tree loss: 6.189 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 018 | Total loss: 6.192 | Reg loss: 0.032 | Tree loss: 6.192 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 018 | Total loss: 6.149 | Reg loss: 0.032 | Tree loss: 6.149 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 018 | Total loss: 6.126 | Reg loss: 0.032 | Tree loss: 6.126 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 018 | Total loss: 6.136 | Reg loss: 0.032 | Tree loss: 6.136 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 018 | Total loss: 6.132 | Reg loss: 0.032 | Tree loss: 6.132 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 018 | Total loss: 6.041 | Reg loss: 0.032 | Tree loss: 6.041 | Accuracy: 0.076605 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 32 | Batch: 000 / 018 | Total loss: 6.247 | Reg loss: 0.031 | Tree loss: 6.247 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 018 | Total loss: 6.208 | Reg loss: 0.031 | Tree loss: 6.208 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 018 | Total loss: 6.228 | Reg loss: 0.031 | Tree loss: 6.228 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 018 | Total loss: 6.231 | Reg loss: 0.031 | Tree loss: 6.231 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 018 | Total loss: 6.204 | Reg loss: 0.031 | Tree loss: 6.204 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 018 | Total loss: 6.215 | Reg loss: 0.031 | Tree loss: 6.215 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 018 | Total loss: 6.179 | Reg loss: 0.031 | Tree loss: 6.179 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 018 | Total loss: 6.095 | Reg loss: 0.031 | Tree loss: 6.095 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 018 | Total loss: 6.149 | Reg loss: 0.031 | Tree loss: 6.149 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 018 | Total loss: 6.114 | Reg loss: 0.031 | Tree loss: 6.114 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 018 | Total loss: 6.088 | Reg loss: 0.031 | Tree loss: 6.088 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 018 | Total loss: 6.075 | Reg loss: 0.032 | Tree loss: 6.075 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 018 | Total loss: 6.139 | Reg loss: 0.032 | Tree loss: 6.139 | Accuracy: 0.046875 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 018 | Total loss: 6.071 | Reg loss: 0.032 | Tree loss: 6.071 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 018 | Total loss: 6.046 | Reg loss: 0.032 | Tree loss: 6.046 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 018 | Total loss: 6.020 | Reg loss: 0.032 | Tree loss: 6.020 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 018 | Total loss: 5.988 | Reg loss: 0.032 | Tree loss: 5.988 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 018 | Total loss: 5.939 | Reg loss: 0.032 | Tree loss: 5.939 | Accuracy: 0.086957 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 33 | Batch: 000 / 018 | Total loss: 6.214 | Reg loss: 0.031 | Tree loss: 6.214 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 018 | Total loss: 6.084 | Reg loss: 0.031 | Tree loss: 6.084 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 018 | Total loss: 6.095 | Reg loss: 0.031 | Tree loss: 6.095 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 018 | Total loss: 6.123 | Reg loss: 0.031 | Tree loss: 6.123 | Accuracy: 0.044922 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 018 | Total loss: 6.093 | Reg loss: 0.031 | Tree loss: 6.093 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 018 | Total loss: 6.058 | Reg loss: 0.031 | Tree loss: 6.058 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 018 | Total loss: 6.072 | Reg loss: 0.031 | Tree loss: 6.072 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 018 | Total loss: 6.041 | Reg loss: 0.031 | Tree loss: 6.041 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 018 | Total loss: 6.003 | Reg loss: 0.031 | Tree loss: 6.003 | Accuracy: 0.091797 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 009 / 018 | Total loss: 5.961 | Reg loss: 0.032 | Tree loss: 5.961 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 018 | Total loss: 6.006 | Reg loss: 0.032 | Tree loss: 6.006 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 018 | Total loss: 5.968 | Reg loss: 0.032 | Tree loss: 5.968 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 018 | Total loss: 5.933 | Reg loss: 0.032 | Tree loss: 5.933 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 018 | Total loss: 5.944 | Reg loss: 0.032 | Tree loss: 5.944 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 018 | Total loss: 5.895 | Reg loss: 0.032 | Tree loss: 5.895 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 018 | Total loss: 5.915 | Reg loss: 0.032 | Tree loss: 5.915 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 018 | Total loss: 5.960 | Reg loss: 0.032 | Tree loss: 5.960 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 018 | Total loss: 5.927 | Reg loss: 0.032 | Tree loss: 5.927 | Accuracy: 0.074534 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 34 | Batch: 000 / 018 | Total loss: 6.040 | Reg loss: 0.031 | Tree loss: 6.040 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 018 | Total loss: 6.032 | Reg loss: 0.031 | Tree loss: 6.032 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 018 | Total loss: 6.016 | Reg loss: 0.031 | Tree loss: 6.016 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 018 | Total loss: 5.993 | Reg loss: 0.031 | Tree loss: 5.993 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 018 | Total loss: 5.982 | Reg loss: 0.031 | Tree loss: 5.982 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 018 | Total loss: 5.930 | Reg loss: 0.031 | Tree loss: 5.930 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 018 | Total loss: 5.964 | Reg loss: 0.031 | Tree loss: 5.964 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 018 | Total loss: 5.917 | Reg loss: 0.032 | Tree loss: 5.917 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 018 | Total loss: 5.918 | Reg loss: 0.032 | Tree loss: 5.918 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 018 | Total loss: 5.914 | Reg loss: 0.032 | Tree loss: 5.914 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 018 | Total loss: 5.891 | Reg loss: 0.032 | Tree loss: 5.891 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 018 | Total loss: 5.868 | Reg loss: 0.032 | Tree loss: 5.868 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 018 | Total loss: 5.845 | Reg loss: 0.032 | Tree loss: 5.845 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 018 | Total loss: 5.838 | Reg loss: 0.032 | Tree loss: 5.838 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 018 | Total loss: 5.815 | Reg loss: 0.032 | Tree loss: 5.815 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 018 | Total loss: 5.809 | Reg loss: 0.032 | Tree loss: 5.809 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 018 | Total loss: 5.811 | Reg loss: 0.032 | Tree loss: 5.811 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 018 | Total loss: 5.820 | Reg loss: 0.032 | Tree loss: 5.820 | Accuracy: 0.064182 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 35 | Batch: 000 / 018 | Total loss: 5.920 | Reg loss: 0.031 | Tree loss: 5.920 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 018 | Total loss: 5.926 | Reg loss: 0.031 | Tree loss: 5.926 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 018 | Total loss: 5.879 | Reg loss: 0.031 | Tree loss: 5.879 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 018 | Total loss: 5.885 | Reg loss: 0.031 | Tree loss: 5.885 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 018 | Total loss: 5.875 | Reg loss: 0.031 | Tree loss: 5.875 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 018 | Total loss: 5.867 | Reg loss: 0.032 | Tree loss: 5.867 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 018 | Total loss: 5.852 | Reg loss: 0.032 | Tree loss: 5.852 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 018 | Total loss: 5.833 | Reg loss: 0.032 | Tree loss: 5.833 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 018 | Total loss: 5.808 | Reg loss: 0.032 | Tree loss: 5.808 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 018 | Total loss: 5.810 | Reg loss: 0.032 | Tree loss: 5.810 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 018 | Total loss: 5.793 | Reg loss: 0.032 | Tree loss: 5.793 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 018 | Total loss: 5.821 | Reg loss: 0.032 | Tree loss: 5.821 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 018 | Total loss: 5.730 | Reg loss: 0.032 | Tree loss: 5.730 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 018 | Total loss: 5.767 | Reg loss: 0.032 | Tree loss: 5.767 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 018 | Total loss: 5.775 | Reg loss: 0.032 | Tree loss: 5.775 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 018 | Total loss: 5.692 | Reg loss: 0.032 | Tree loss: 5.692 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 018 | Total loss: 5.706 | Reg loss: 0.032 | Tree loss: 5.706 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 018 | Total loss: 5.649 | Reg loss: 0.032 | Tree loss: 5.649 | Accuracy: 0.080745 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 36 | Batch: 000 / 018 | Total loss: 5.847 | Reg loss: 0.032 | Tree loss: 5.847 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 018 | Total loss: 5.810 | Reg loss: 0.032 | Tree loss: 5.810 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 018 | Total loss: 5.788 | Reg loss: 0.032 | Tree loss: 5.788 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 018 | Total loss: 5.734 | Reg loss: 0.032 | Tree loss: 5.734 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 018 | Total loss: 5.761 | Reg loss: 0.032 | Tree loss: 5.761 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 018 | Total loss: 5.762 | Reg loss: 0.032 | Tree loss: 5.762 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 018 | Total loss: 5.698 | Reg loss: 0.032 | Tree loss: 5.698 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 018 | Total loss: 5.727 | Reg loss: 0.032 | Tree loss: 5.727 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 018 | Total loss: 5.743 | Reg loss: 0.032 | Tree loss: 5.743 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 018 | Total loss: 5.677 | Reg loss: 0.032 | Tree loss: 5.677 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 018 | Total loss: 5.729 | Reg loss: 0.032 | Tree loss: 5.729 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 018 | Total loss: 5.683 | Reg loss: 0.032 | Tree loss: 5.683 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 018 | Total loss: 5.643 | Reg loss: 0.032 | Tree loss: 5.643 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 018 | Total loss: 5.714 | Reg loss: 0.032 | Tree loss: 5.714 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 018 | Total loss: 5.694 | Reg loss: 0.032 | Tree loss: 5.694 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 018 | Total loss: 5.589 | Reg loss: 0.032 | Tree loss: 5.589 | Accuracy: 0.082031 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 016 / 018 | Total loss: 5.650 | Reg loss: 0.032 | Tree loss: 5.650 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 018 | Total loss: 5.606 | Reg loss: 0.032 | Tree loss: 5.606 | Accuracy: 0.074534 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 37 | Batch: 000 / 018 | Total loss: 5.701 | Reg loss: 0.032 | Tree loss: 5.701 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 018 | Total loss: 5.701 | Reg loss: 0.032 | Tree loss: 5.701 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 018 | Total loss: 5.752 | Reg loss: 0.032 | Tree loss: 5.752 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 018 | Total loss: 5.694 | Reg loss: 0.032 | Tree loss: 5.694 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 018 | Total loss: 5.597 | Reg loss: 0.032 | Tree loss: 5.597 | Accuracy: 0.091797 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 018 | Total loss: 5.649 | Reg loss: 0.032 | Tree loss: 5.649 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 018 | Total loss: 5.681 | Reg loss: 0.032 | Tree loss: 5.681 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 018 | Total loss: 5.619 | Reg loss: 0.032 | Tree loss: 5.619 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 018 | Total loss: 5.651 | Reg loss: 0.032 | Tree loss: 5.651 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 018 | Total loss: 5.638 | Reg loss: 0.032 | Tree loss: 5.638 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 018 | Total loss: 5.527 | Reg loss: 0.032 | Tree loss: 5.527 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 018 | Total loss: 5.598 | Reg loss: 0.032 | Tree loss: 5.598 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 018 | Total loss: 5.606 | Reg loss: 0.032 | Tree loss: 5.606 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 018 | Total loss: 5.556 | Reg loss: 0.032 | Tree loss: 5.556 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 018 | Total loss: 5.593 | Reg loss: 0.032 | Tree loss: 5.593 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 018 | Total loss: 5.591 | Reg loss: 0.032 | Tree loss: 5.591 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 018 | Total loss: 5.559 | Reg loss: 0.032 | Tree loss: 5.559 | Accuracy: 0.044922 | 0.229 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 018 | Total loss: 5.483 | Reg loss: 0.032 | Tree loss: 5.483 | Accuracy: 0.101449 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 38 | Batch: 000 / 018 | Total loss: 5.689 | Reg loss: 0.032 | Tree loss: 5.689 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 018 | Total loss: 5.599 | Reg loss: 0.032 | Tree loss: 5.599 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 018 | Total loss: 5.584 | Reg loss: 0.032 | Tree loss: 5.584 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 018 | Total loss: 5.557 | Reg loss: 0.032 | Tree loss: 5.557 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 018 | Total loss: 5.571 | Reg loss: 0.032 | Tree loss: 5.571 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 018 | Total loss: 5.595 | Reg loss: 0.032 | Tree loss: 5.595 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 018 | Total loss: 5.525 | Reg loss: 0.032 | Tree loss: 5.525 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 018 | Total loss: 5.551 | Reg loss: 0.032 | Tree loss: 5.551 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 018 | Total loss: 5.571 | Reg loss: 0.032 | Tree loss: 5.571 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 018 | Total loss: 5.515 | Reg loss: 0.032 | Tree loss: 5.515 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 018 | Total loss: 5.541 | Reg loss: 0.032 | Tree loss: 5.541 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 018 | Total loss: 5.451 | Reg loss: 0.032 | Tree loss: 5.451 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 018 | Total loss: 5.501 | Reg loss: 0.032 | Tree loss: 5.501 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 018 | Total loss: 5.464 | Reg loss: 0.032 | Tree loss: 5.464 | Accuracy: 0.093750 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 018 | Total loss: 5.451 | Reg loss: 0.032 | Tree loss: 5.451 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 018 | Total loss: 5.470 | Reg loss: 0.032 | Tree loss: 5.470 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 018 | Total loss: 5.480 | Reg loss: 0.032 | Tree loss: 5.480 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 018 | Total loss: 5.484 | Reg loss: 0.032 | Tree loss: 5.484 | Accuracy: 0.064182 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 39 | Batch: 000 / 018 | Total loss: 5.586 | Reg loss: 0.032 | Tree loss: 5.586 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 018 | Total loss: 5.531 | Reg loss: 0.032 | Tree loss: 5.531 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 018 | Total loss: 5.529 | Reg loss: 0.032 | Tree loss: 5.529 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 018 | Total loss: 5.469 | Reg loss: 0.032 | Tree loss: 5.469 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 018 | Total loss: 5.469 | Reg loss: 0.032 | Tree loss: 5.469 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 018 | Total loss: 5.495 | Reg loss: 0.032 | Tree loss: 5.495 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 018 | Total loss: 5.510 | Reg loss: 0.032 | Tree loss: 5.510 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 018 | Total loss: 5.525 | Reg loss: 0.032 | Tree loss: 5.525 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 018 | Total loss: 5.410 | Reg loss: 0.032 | Tree loss: 5.410 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 018 | Total loss: 5.464 | Reg loss: 0.032 | Tree loss: 5.464 | Accuracy: 0.070312 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 018 | Total loss: 5.389 | Reg loss: 0.032 | Tree loss: 5.389 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 018 | Total loss: 5.405 | Reg loss: 0.032 | Tree loss: 5.405 | Accuracy: 0.076172 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 018 | Total loss: 5.376 | Reg loss: 0.032 | Tree loss: 5.376 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 018 | Total loss: 5.428 | Reg loss: 0.032 | Tree loss: 5.428 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 018 | Total loss: 5.395 | Reg loss: 0.032 | Tree loss: 5.395 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 018 | Total loss: 5.386 | Reg loss: 0.032 | Tree loss: 5.386 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 018 | Total loss: 5.399 | Reg loss: 0.032 | Tree loss: 5.399 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 018 | Total loss: 5.342 | Reg loss: 0.032 | Tree loss: 5.342 | Accuracy: 0.070393 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 40 | Batch: 000 / 018 | Total loss: 5.547 | Reg loss: 0.032 | Tree loss: 5.547 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 018 | Total loss: 5.458 | Reg loss: 0.032 | Tree loss: 5.458 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 018 | Total loss: 5.432 | Reg loss: 0.032 | Tree loss: 5.432 | Accuracy: 0.082031 | 0.229 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 003 / 018 | Total loss: 5.452 | Reg loss: 0.032 | Tree loss: 5.452 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 018 | Total loss: 5.391 | Reg loss: 0.032 | Tree loss: 5.391 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 018 | Total loss: 5.408 | Reg loss: 0.032 | Tree loss: 5.408 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 018 | Total loss: 5.387 | Reg loss: 0.032 | Tree loss: 5.387 | Accuracy: 0.083984 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 018 | Total loss: 5.357 | Reg loss: 0.032 | Tree loss: 5.357 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 018 | Total loss: 5.380 | Reg loss: 0.032 | Tree loss: 5.380 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 018 | Total loss: 5.364 | Reg loss: 0.032 | Tree loss: 5.364 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 018 | Total loss: 5.326 | Reg loss: 0.032 | Tree loss: 5.326 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 018 | Total loss: 5.388 | Reg loss: 0.032 | Tree loss: 5.388 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 018 | Total loss: 5.333 | Reg loss: 0.032 | Tree loss: 5.333 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 018 | Total loss: 5.346 | Reg loss: 0.032 | Tree loss: 5.346 | Accuracy: 0.052734 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 018 | Total loss: 5.235 | Reg loss: 0.032 | Tree loss: 5.235 | Accuracy: 0.089844 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 018 | Total loss: 5.244 | Reg loss: 0.032 | Tree loss: 5.244 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 018 | Total loss: 5.313 | Reg loss: 0.032 | Tree loss: 5.313 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 018 | Total loss: 5.340 | Reg loss: 0.032 | Tree loss: 5.340 | Accuracy: 0.049689 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 41 | Batch: 000 / 018 | Total loss: 5.408 | Reg loss: 0.032 | Tree loss: 5.408 | Accuracy: 0.056641 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 018 | Total loss: 5.394 | Reg loss: 0.032 | Tree loss: 5.394 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 018 | Total loss: 5.315 | Reg loss: 0.032 | Tree loss: 5.315 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 018 | Total loss: 5.378 | Reg loss: 0.032 | Tree loss: 5.378 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 018 | Total loss: 5.346 | Reg loss: 0.032 | Tree loss: 5.346 | Accuracy: 0.085938 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 018 | Total loss: 5.345 | Reg loss: 0.032 | Tree loss: 5.345 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 018 | Total loss: 5.329 | Reg loss: 0.032 | Tree loss: 5.329 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 018 | Total loss: 5.303 | Reg loss: 0.032 | Tree loss: 5.303 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 018 | Total loss: 5.292 | Reg loss: 0.032 | Tree loss: 5.292 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 018 | Total loss: 5.237 | Reg loss: 0.032 | Tree loss: 5.237 | Accuracy: 0.082031 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 018 | Total loss: 5.293 | Reg loss: 0.032 | Tree loss: 5.293 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 018 | Total loss: 5.274 | Reg loss: 0.032 | Tree loss: 5.274 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 018 | Total loss: 5.256 | Reg loss: 0.032 | Tree loss: 5.256 | Accuracy: 0.062500 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 018 | Total loss: 5.291 | Reg loss: 0.032 | Tree loss: 5.291 | Accuracy: 0.058594 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 018 | Total loss: 5.286 | Reg loss: 0.032 | Tree loss: 5.286 | Accuracy: 0.037109 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 018 | Total loss: 5.203 | Reg loss: 0.032 | Tree loss: 5.203 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 018 | Total loss: 5.247 | Reg loss: 0.032 | Tree loss: 5.247 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 018 | Total loss: 5.159 | Reg loss: 0.032 | Tree loss: 5.159 | Accuracy: 0.074534 | 0.229 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 42 | Batch: 000 / 018 | Total loss: 5.345 | Reg loss: 0.031 | Tree loss: 5.345 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 018 | Total loss: 5.320 | Reg loss: 0.031 | Tree loss: 5.320 | Accuracy: 0.060547 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 018 | Total loss: 5.341 | Reg loss: 0.031 | Tree loss: 5.341 | Accuracy: 0.074219 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 018 | Total loss: 5.269 | Reg loss: 0.031 | Tree loss: 5.269 | Accuracy: 0.080078 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 018 | Total loss: 5.236 | Reg loss: 0.031 | Tree loss: 5.236 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 018 | Total loss: 5.305 | Reg loss: 0.031 | Tree loss: 5.305 | Accuracy: 0.066406 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 018 | Total loss: 5.276 | Reg loss: 0.031 | Tree loss: 5.276 | Accuracy: 0.068359 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 018 | Total loss: 5.261 | Reg loss: 0.031 | Tree loss: 5.261 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 018 | Total loss: 5.209 | Reg loss: 0.031 | Tree loss: 5.209 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 018 | Total loss: 5.265 | Reg loss: 0.031 | Tree loss: 5.265 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 018 | Total loss: 5.171 | Reg loss: 0.031 | Tree loss: 5.171 | Accuracy: 0.064453 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 018 | Total loss: 5.227 | Reg loss: 0.032 | Tree loss: 5.227 | Accuracy: 0.072266 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 018 | Total loss: 5.174 | Reg loss: 0.032 | Tree loss: 5.174 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 018 | Total loss: 5.196 | Reg loss: 0.032 | Tree loss: 5.196 | Accuracy: 0.050781 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 018 | Total loss: 5.149 | Reg loss: 0.032 | Tree loss: 5.149 | Accuracy: 0.078125 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 018 | Total loss: 5.161 | Reg loss: 0.032 | Tree loss: 5.161 | Accuracy: 0.054688 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 018 | Total loss: 5.085 | Reg loss: 0.032 | Tree loss: 5.085 | Accuracy: 0.087891 | 0.229 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 018 | Total loss: 5.102 | Reg loss: 0.032 | Tree loss: 5.102 | Accuracy: 0.080745 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 43 | Batch: 000 / 018 | Total loss: 5.331 | Reg loss: 0.031 | Tree loss: 5.331 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 018 | Total loss: 5.239 | Reg loss: 0.031 | Tree loss: 5.239 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 018 | Total loss: 5.222 | Reg loss: 0.031 | Tree loss: 5.222 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 018 | Total loss: 5.196 | Reg loss: 0.031 | Tree loss: 5.196 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 018 | Total loss: 5.236 | Reg loss: 0.031 | Tree loss: 5.236 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 018 | Total loss: 5.167 | Reg loss: 0.031 | Tree loss: 5.167 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 018 | Total loss: 5.210 | Reg loss: 0.031 | Tree loss: 5.210 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 018 | Total loss: 5.190 | Reg loss: 0.031 | Tree loss: 5.190 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 018 | Total loss: 5.189 | Reg loss: 0.031 | Tree loss: 5.189 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 018 | Total loss: 5.180 | Reg loss: 0.031 | Tree loss: 5.180 | Accuracy: 0.082031 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 018 | Total loss: 5.146 | Reg loss: 0.031 | Tree loss: 5.146 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 018 | Total loss: 5.084 | Reg loss: 0.031 | Tree loss: 5.084 | Accuracy: 0.089844 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 018 | Total loss: 5.079 | Reg loss: 0.031 | Tree loss: 5.079 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 018 | Total loss: 5.125 | Reg loss: 0.031 | Tree loss: 5.125 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 018 | Total loss: 5.045 | Reg loss: 0.031 | Tree loss: 5.045 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 018 | Total loss: 5.109 | Reg loss: 0.031 | Tree loss: 5.109 | Accuracy: 0.048828 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 018 | Total loss: 5.049 | Reg loss: 0.031 | Tree loss: 5.049 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 018 | Total loss: 5.117 | Reg loss: 0.032 | Tree loss: 5.117 | Accuracy: 0.043478 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 44 | Batch: 000 / 018 | Total loss: 5.237 | Reg loss: 0.031 | Tree loss: 5.237 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 018 | Total loss: 5.201 | Reg loss: 0.031 | Tree loss: 5.201 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 018 | Total loss: 5.125 | Reg loss: 0.031 | Tree loss: 5.125 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 018 | Total loss: 5.148 | Reg loss: 0.031 | Tree loss: 5.148 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 018 | Total loss: 5.094 | Reg loss: 0.031 | Tree loss: 5.094 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 018 | Total loss: 5.174 | Reg loss: 0.031 | Tree loss: 5.174 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 018 | Total loss: 5.144 | Reg loss: 0.031 | Tree loss: 5.144 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 018 | Total loss: 5.124 | Reg loss: 0.031 | Tree loss: 5.124 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 018 | Total loss: 5.042 | Reg loss: 0.031 | Tree loss: 5.042 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 018 | Total loss: 5.152 | Reg loss: 0.031 | Tree loss: 5.152 | Accuracy: 0.042969 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 018 | Total loss: 5.099 | Reg loss: 0.031 | Tree loss: 5.099 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 018 | Total loss: 5.053 | Reg loss: 0.031 | Tree loss: 5.053 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 018 | Total loss: 5.027 | Reg loss: 0.031 | Tree loss: 5.027 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 018 | Total loss: 5.144 | Reg loss: 0.031 | Tree loss: 5.144 | Accuracy: 0.048828 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 018 | Total loss: 5.043 | Reg loss: 0.031 | Tree loss: 5.043 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 018 | Total loss: 4.992 | Reg loss: 0.031 | Tree loss: 4.992 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 018 | Total loss: 4.995 | Reg loss: 0.031 | Tree loss: 4.995 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 018 | Total loss: 4.993 | Reg loss: 0.031 | Tree loss: 4.993 | Accuracy: 0.082816 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 45 | Batch: 000 / 018 | Total loss: 5.132 | Reg loss: 0.031 | Tree loss: 5.132 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 018 | Total loss: 5.147 | Reg loss: 0.031 | Tree loss: 5.147 | Accuracy: 0.048828 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 018 | Total loss: 5.105 | Reg loss: 0.031 | Tree loss: 5.105 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 018 | Total loss: 5.041 | Reg loss: 0.031 | Tree loss: 5.041 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 018 | Total loss: 5.058 | Reg loss: 0.031 | Tree loss: 5.058 | Accuracy: 0.097656 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 018 | Total loss: 5.020 | Reg loss: 0.031 | Tree loss: 5.020 | Accuracy: 0.089844 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 018 | Total loss: 5.075 | Reg loss: 0.031 | Tree loss: 5.075 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 018 | Total loss: 5.043 | Reg loss: 0.031 | Tree loss: 5.043 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 018 | Total loss: 5.053 | Reg loss: 0.031 | Tree loss: 5.053 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 018 | Total loss: 5.087 | Reg loss: 0.031 | Tree loss: 5.087 | Accuracy: 0.042969 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 018 | Total loss: 5.076 | Reg loss: 0.031 | Tree loss: 5.076 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 018 | Total loss: 5.007 | Reg loss: 0.031 | Tree loss: 5.007 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 018 | Total loss: 4.969 | Reg loss: 0.031 | Tree loss: 4.969 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 018 | Total loss: 5.001 | Reg loss: 0.031 | Tree loss: 5.001 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 018 | Total loss: 4.965 | Reg loss: 0.031 | Tree loss: 4.965 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 018 | Total loss: 4.999 | Reg loss: 0.031 | Tree loss: 4.999 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 018 | Total loss: 5.019 | Reg loss: 0.031 | Tree loss: 5.019 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 018 | Total loss: 4.932 | Reg loss: 0.031 | Tree loss: 4.932 | Accuracy: 0.078675 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 46 | Batch: 000 / 018 | Total loss: 5.090 | Reg loss: 0.031 | Tree loss: 5.090 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 018 | Total loss: 5.049 | Reg loss: 0.031 | Tree loss: 5.049 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 018 | Total loss: 5.032 | Reg loss: 0.030 | Tree loss: 5.032 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 018 | Total loss: 5.023 | Reg loss: 0.030 | Tree loss: 5.023 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 018 | Total loss: 5.060 | Reg loss: 0.030 | Tree loss: 5.060 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 018 | Total loss: 5.020 | Reg loss: 0.030 | Tree loss: 5.020 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 018 | Total loss: 5.041 | Reg loss: 0.030 | Tree loss: 5.041 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 018 | Total loss: 5.069 | Reg loss: 0.030 | Tree loss: 5.069 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 018 | Total loss: 4.993 | Reg loss: 0.030 | Tree loss: 4.993 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 018 | Total loss: 5.010 | Reg loss: 0.030 | Tree loss: 5.010 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 018 | Total loss: 4.940 | Reg loss: 0.030 | Tree loss: 4.940 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 018 | Total loss: 4.925 | Reg loss: 0.030 | Tree loss: 4.925 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 018 | Total loss: 4.954 | Reg loss: 0.031 | Tree loss: 4.954 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 018 | Total loss: 4.993 | Reg loss: 0.031 | Tree loss: 4.993 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 018 | Total loss: 4.875 | Reg loss: 0.031 | Tree loss: 4.875 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 018 | Total loss: 4.886 | Reg loss: 0.031 | Tree loss: 4.886 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 018 | Total loss: 4.892 | Reg loss: 0.031 | Tree loss: 4.892 | Accuracy: 0.070312 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 017 / 018 | Total loss: 4.878 | Reg loss: 0.031 | Tree loss: 4.878 | Accuracy: 0.064182 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 47 | Batch: 000 / 018 | Total loss: 5.053 | Reg loss: 0.030 | Tree loss: 5.053 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 018 | Total loss: 5.007 | Reg loss: 0.030 | Tree loss: 5.007 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 018 | Total loss: 5.020 | Reg loss: 0.030 | Tree loss: 5.020 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 018 | Total loss: 4.982 | Reg loss: 0.030 | Tree loss: 4.982 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 018 | Total loss: 4.966 | Reg loss: 0.030 | Tree loss: 4.966 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 018 | Total loss: 4.925 | Reg loss: 0.030 | Tree loss: 4.925 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 018 | Total loss: 4.985 | Reg loss: 0.030 | Tree loss: 4.985 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 018 | Total loss: 4.919 | Reg loss: 0.030 | Tree loss: 4.919 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 018 | Total loss: 4.923 | Reg loss: 0.030 | Tree loss: 4.923 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 018 | Total loss: 4.953 | Reg loss: 0.030 | Tree loss: 4.953 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 018 | Total loss: 4.865 | Reg loss: 0.030 | Tree loss: 4.865 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 018 | Total loss: 4.918 | Reg loss: 0.030 | Tree loss: 4.918 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 018 | Total loss: 4.971 | Reg loss: 0.030 | Tree loss: 4.971 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 018 | Total loss: 4.910 | Reg loss: 0.030 | Tree loss: 4.910 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 018 | Total loss: 4.828 | Reg loss: 0.030 | Tree loss: 4.828 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 018 | Total loss: 4.888 | Reg loss: 0.030 | Tree loss: 4.888 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 018 | Total loss: 4.848 | Reg loss: 0.030 | Tree loss: 4.848 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 018 | Total loss: 4.824 | Reg loss: 0.030 | Tree loss: 4.824 | Accuracy: 0.070393 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 48 | Batch: 000 / 018 | Total loss: 4.917 | Reg loss: 0.030 | Tree loss: 4.917 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 018 | Total loss: 4.993 | Reg loss: 0.030 | Tree loss: 4.993 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 018 | Total loss: 4.881 | Reg loss: 0.030 | Tree loss: 4.881 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 018 | Total loss: 4.945 | Reg loss: 0.030 | Tree loss: 4.945 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 018 | Total loss: 4.979 | Reg loss: 0.030 | Tree loss: 4.979 | Accuracy: 0.042969 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 018 | Total loss: 4.905 | Reg loss: 0.030 | Tree loss: 4.905 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 018 | Total loss: 4.969 | Reg loss: 0.030 | Tree loss: 4.969 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 018 | Total loss: 4.855 | Reg loss: 0.030 | Tree loss: 4.855 | Accuracy: 0.089844 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 018 | Total loss: 4.851 | Reg loss: 0.030 | Tree loss: 4.851 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 018 | Total loss: 4.834 | Reg loss: 0.030 | Tree loss: 4.834 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 018 | Total loss: 4.835 | Reg loss: 0.030 | Tree loss: 4.835 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 018 | Total loss: 4.873 | Reg loss: 0.030 | Tree loss: 4.873 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 018 | Total loss: 4.851 | Reg loss: 0.030 | Tree loss: 4.851 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 018 | Total loss: 4.895 | Reg loss: 0.030 | Tree loss: 4.895 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 018 | Total loss: 4.831 | Reg loss: 0.030 | Tree loss: 4.831 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 018 | Total loss: 4.862 | Reg loss: 0.030 | Tree loss: 4.862 | Accuracy: 0.044922 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 018 | Total loss: 4.809 | Reg loss: 0.030 | Tree loss: 4.809 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 018 | Total loss: 4.805 | Reg loss: 0.030 | Tree loss: 4.805 | Accuracy: 0.086957 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 49 | Batch: 000 / 018 | Total loss: 4.963 | Reg loss: 0.029 | Tree loss: 4.963 | Accuracy: 0.041016 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 018 | Total loss: 4.897 | Reg loss: 0.029 | Tree loss: 4.897 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 018 | Total loss: 4.868 | Reg loss: 0.029 | Tree loss: 4.868 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 018 | Total loss: 4.912 | Reg loss: 0.029 | Tree loss: 4.912 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 018 | Total loss: 4.881 | Reg loss: 0.029 | Tree loss: 4.881 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 018 | Total loss: 4.920 | Reg loss: 0.029 | Tree loss: 4.920 | Accuracy: 0.050781 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 018 | Total loss: 4.802 | Reg loss: 0.029 | Tree loss: 4.802 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 018 | Total loss: 4.866 | Reg loss: 0.029 | Tree loss: 4.866 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 018 | Total loss: 4.837 | Reg loss: 0.029 | Tree loss: 4.837 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 018 | Total loss: 4.765 | Reg loss: 0.029 | Tree loss: 4.765 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 018 | Total loss: 4.795 | Reg loss: 0.029 | Tree loss: 4.795 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 018 | Total loss: 4.832 | Reg loss: 0.029 | Tree loss: 4.832 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 018 | Total loss: 4.814 | Reg loss: 0.029 | Tree loss: 4.814 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 018 | Total loss: 4.824 | Reg loss: 0.029 | Tree loss: 4.824 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 018 | Total loss: 4.773 | Reg loss: 0.029 | Tree loss: 4.773 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 018 | Total loss: 4.797 | Reg loss: 0.029 | Tree loss: 4.797 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 018 | Total loss: 4.731 | Reg loss: 0.029 | Tree loss: 4.731 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 018 | Total loss: 4.756 | Reg loss: 0.029 | Tree loss: 4.756 | Accuracy: 0.068323 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 50 | Batch: 000 / 018 | Total loss: 4.835 | Reg loss: 0.029 | Tree loss: 4.835 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 018 | Total loss: 4.887 | Reg loss: 0.029 | Tree loss: 4.887 | Accuracy: 0.046875 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 018 | Total loss: 4.843 | Reg loss: 0.029 | Tree loss: 4.843 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 018 | Total loss: 4.866 | Reg loss: 0.029 | Tree loss: 4.866 | Accuracy: 0.076172 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 004 / 018 | Total loss: 4.798 | Reg loss: 0.029 | Tree loss: 4.798 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 018 | Total loss: 4.830 | Reg loss: 0.029 | Tree loss: 4.830 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 018 | Total loss: 4.833 | Reg loss: 0.029 | Tree loss: 4.833 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 018 | Total loss: 4.812 | Reg loss: 0.029 | Tree loss: 4.812 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 018 | Total loss: 4.839 | Reg loss: 0.029 | Tree loss: 4.839 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 018 | Total loss: 4.793 | Reg loss: 0.029 | Tree loss: 4.793 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 018 | Total loss: 4.783 | Reg loss: 0.029 | Tree loss: 4.783 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 018 | Total loss: 4.739 | Reg loss: 0.029 | Tree loss: 4.739 | Accuracy: 0.087891 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 018 | Total loss: 4.811 | Reg loss: 0.029 | Tree loss: 4.811 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 018 | Total loss: 4.808 | Reg loss: 0.029 | Tree loss: 4.808 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 018 | Total loss: 4.683 | Reg loss: 0.029 | Tree loss: 4.683 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 018 | Total loss: 4.679 | Reg loss: 0.029 | Tree loss: 4.679 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 018 | Total loss: 4.651 | Reg loss: 0.029 | Tree loss: 4.651 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 018 | Total loss: 4.736 | Reg loss: 0.029 | Tree loss: 4.736 | Accuracy: 0.070393 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 51 | Batch: 000 / 018 | Total loss: 4.797 | Reg loss: 0.028 | Tree loss: 4.797 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 018 | Total loss: 4.858 | Reg loss: 0.028 | Tree loss: 4.858 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 018 | Total loss: 4.833 | Reg loss: 0.028 | Tree loss: 4.833 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 018 | Total loss: 4.819 | Reg loss: 0.028 | Tree loss: 4.819 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 018 | Total loss: 4.838 | Reg loss: 0.028 | Tree loss: 4.838 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 018 | Total loss: 4.765 | Reg loss: 0.028 | Tree loss: 4.765 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 018 | Total loss: 4.801 | Reg loss: 0.028 | Tree loss: 4.801 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 018 | Total loss: 4.766 | Reg loss: 0.028 | Tree loss: 4.766 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 018 | Total loss: 4.759 | Reg loss: 0.028 | Tree loss: 4.759 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 018 | Total loss: 4.705 | Reg loss: 0.028 | Tree loss: 4.705 | Accuracy: 0.095703 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 018 | Total loss: 4.740 | Reg loss: 0.028 | Tree loss: 4.740 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 018 | Total loss: 4.677 | Reg loss: 0.028 | Tree loss: 4.677 | Accuracy: 0.087891 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 018 | Total loss: 4.708 | Reg loss: 0.028 | Tree loss: 4.708 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 018 | Total loss: 4.662 | Reg loss: 0.028 | Tree loss: 4.662 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 018 | Total loss: 4.732 | Reg loss: 0.028 | Tree loss: 4.732 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 018 | Total loss: 4.634 | Reg loss: 0.028 | Tree loss: 4.634 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 018 | Total loss: 4.654 | Reg loss: 0.028 | Tree loss: 4.654 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 018 | Total loss: 4.699 | Reg loss: 0.028 | Tree loss: 4.699 | Accuracy: 0.053830 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 52 | Batch: 000 / 018 | Total loss: 4.782 | Reg loss: 0.028 | Tree loss: 4.782 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 018 | Total loss: 4.745 | Reg loss: 0.027 | Tree loss: 4.745 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 018 | Total loss: 4.789 | Reg loss: 0.027 | Tree loss: 4.789 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 018 | Total loss: 4.741 | Reg loss: 0.027 | Tree loss: 4.741 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 018 | Total loss: 4.790 | Reg loss: 0.027 | Tree loss: 4.790 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 018 | Total loss: 4.755 | Reg loss: 0.027 | Tree loss: 4.755 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 018 | Total loss: 4.722 | Reg loss: 0.027 | Tree loss: 4.722 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 018 | Total loss: 4.716 | Reg loss: 0.027 | Tree loss: 4.716 | Accuracy: 0.050781 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 018 | Total loss: 4.672 | Reg loss: 0.028 | Tree loss: 4.672 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 018 | Total loss: 4.707 | Reg loss: 0.028 | Tree loss: 4.707 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 018 | Total loss: 4.651 | Reg loss: 0.028 | Tree loss: 4.651 | Accuracy: 0.093750 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 018 | Total loss: 4.678 | Reg loss: 0.028 | Tree loss: 4.678 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 018 | Total loss: 4.728 | Reg loss: 0.028 | Tree loss: 4.728 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 018 | Total loss: 4.606 | Reg loss: 0.028 | Tree loss: 4.606 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 018 | Total loss: 4.604 | Reg loss: 0.028 | Tree loss: 4.604 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 018 | Total loss: 4.696 | Reg loss: 0.028 | Tree loss: 4.696 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 018 | Total loss: 4.672 | Reg loss: 0.028 | Tree loss: 4.672 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 018 | Total loss: 4.651 | Reg loss: 0.028 | Tree loss: 4.651 | Accuracy: 0.060041 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 53 | Batch: 000 / 018 | Total loss: 4.749 | Reg loss: 0.027 | Tree loss: 4.749 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 018 | Total loss: 4.803 | Reg loss: 0.027 | Tree loss: 4.803 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 018 | Total loss: 4.688 | Reg loss: 0.027 | Tree loss: 4.688 | Accuracy: 0.087891 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 018 | Total loss: 4.751 | Reg loss: 0.027 | Tree loss: 4.751 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 018 | Total loss: 4.700 | Reg loss: 0.027 | Tree loss: 4.700 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 018 | Total loss: 4.685 | Reg loss: 0.027 | Tree loss: 4.685 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 018 | Total loss: 4.647 | Reg loss: 0.027 | Tree loss: 4.647 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 018 | Total loss: 4.624 | Reg loss: 0.027 | Tree loss: 4.624 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 018 | Total loss: 4.681 | Reg loss: 0.027 | Tree loss: 4.681 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 018 | Total loss: 4.673 | Reg loss: 0.027 | Tree loss: 4.673 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 018 | Total loss: 4.677 | Reg loss: 0.027 | Tree loss: 4.677 | Accuracy: 0.058594 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 011 / 018 | Total loss: 4.576 | Reg loss: 0.028 | Tree loss: 4.576 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 018 | Total loss: 4.622 | Reg loss: 0.028 | Tree loss: 4.622 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 018 | Total loss: 4.629 | Reg loss: 0.028 | Tree loss: 4.629 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 018 | Total loss: 4.590 | Reg loss: 0.028 | Tree loss: 4.590 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 018 | Total loss: 4.629 | Reg loss: 0.028 | Tree loss: 4.629 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 018 | Total loss: 4.643 | Reg loss: 0.028 | Tree loss: 4.643 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 018 | Total loss: 4.600 | Reg loss: 0.028 | Tree loss: 4.600 | Accuracy: 0.072464 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 54 | Batch: 000 / 018 | Total loss: 4.703 | Reg loss: 0.027 | Tree loss: 4.703 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 018 | Total loss: 4.634 | Reg loss: 0.027 | Tree loss: 4.634 | Accuracy: 0.095703 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 018 | Total loss: 4.693 | Reg loss: 0.027 | Tree loss: 4.693 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 018 | Total loss: 4.724 | Reg loss: 0.027 | Tree loss: 4.724 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 018 | Total loss: 4.678 | Reg loss: 0.027 | Tree loss: 4.678 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 018 | Total loss: 4.654 | Reg loss: 0.027 | Tree loss: 4.654 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 018 | Total loss: 4.617 | Reg loss: 0.027 | Tree loss: 4.617 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 018 | Total loss: 4.655 | Reg loss: 0.027 | Tree loss: 4.655 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 018 | Total loss: 4.638 | Reg loss: 0.027 | Tree loss: 4.638 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 018 | Total loss: 4.633 | Reg loss: 0.028 | Tree loss: 4.633 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 018 | Total loss: 4.656 | Reg loss: 0.028 | Tree loss: 4.656 | Accuracy: 0.050781 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 018 | Total loss: 4.564 | Reg loss: 0.028 | Tree loss: 4.564 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 018 | Total loss: 4.643 | Reg loss: 0.028 | Tree loss: 4.643 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 018 | Total loss: 4.625 | Reg loss: 0.028 | Tree loss: 4.625 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 018 | Total loss: 4.544 | Reg loss: 0.028 | Tree loss: 4.544 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 018 | Total loss: 4.570 | Reg loss: 0.028 | Tree loss: 4.570 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 018 | Total loss: 4.516 | Reg loss: 0.029 | Tree loss: 4.516 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 018 | Total loss: 4.523 | Reg loss: 0.029 | Tree loss: 4.523 | Accuracy: 0.068323 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 55 | Batch: 000 / 018 | Total loss: 4.786 | Reg loss: 0.027 | Tree loss: 4.786 | Accuracy: 0.044922 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 018 | Total loss: 4.665 | Reg loss: 0.027 | Tree loss: 4.665 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 018 | Total loss: 4.691 | Reg loss: 0.028 | Tree loss: 4.691 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 018 | Total loss: 4.630 | Reg loss: 0.028 | Tree loss: 4.630 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 018 | Total loss: 4.668 | Reg loss: 0.028 | Tree loss: 4.668 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 018 | Total loss: 4.666 | Reg loss: 0.028 | Tree loss: 4.666 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 018 | Total loss: 4.623 | Reg loss: 0.028 | Tree loss: 4.623 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 018 | Total loss: 4.633 | Reg loss: 0.028 | Tree loss: 4.633 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 018 | Total loss: 4.548 | Reg loss: 0.028 | Tree loss: 4.548 | Accuracy: 0.089844 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 018 | Total loss: 4.533 | Reg loss: 0.028 | Tree loss: 4.533 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 018 | Total loss: 4.558 | Reg loss: 0.028 | Tree loss: 4.558 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 018 | Total loss: 4.535 | Reg loss: 0.028 | Tree loss: 4.535 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 018 | Total loss: 4.552 | Reg loss: 0.029 | Tree loss: 4.552 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 018 | Total loss: 4.551 | Reg loss: 0.029 | Tree loss: 4.551 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 018 | Total loss: 4.523 | Reg loss: 0.029 | Tree loss: 4.523 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 018 | Total loss: 4.464 | Reg loss: 0.029 | Tree loss: 4.464 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 018 | Total loss: 4.463 | Reg loss: 0.029 | Tree loss: 4.463 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 018 | Total loss: 4.523 | Reg loss: 0.029 | Tree loss: 4.523 | Accuracy: 0.074534 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 56 | Batch: 000 / 018 | Total loss: 4.630 | Reg loss: 0.028 | Tree loss: 4.630 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 018 | Total loss: 4.672 | Reg loss: 0.028 | Tree loss: 4.672 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 018 | Total loss: 4.572 | Reg loss: 0.028 | Tree loss: 4.572 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 018 | Total loss: 4.560 | Reg loss: 0.028 | Tree loss: 4.560 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 018 | Total loss: 4.623 | Reg loss: 0.028 | Tree loss: 4.623 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 018 | Total loss: 4.596 | Reg loss: 0.028 | Tree loss: 4.596 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 018 | Total loss: 4.588 | Reg loss: 0.029 | Tree loss: 4.588 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 018 | Total loss: 4.650 | Reg loss: 0.029 | Tree loss: 4.650 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 018 | Total loss: 4.575 | Reg loss: 0.029 | Tree loss: 4.575 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 018 | Total loss: 4.538 | Reg loss: 0.029 | Tree loss: 4.538 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 018 | Total loss: 4.506 | Reg loss: 0.029 | Tree loss: 4.506 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 018 | Total loss: 4.502 | Reg loss: 0.029 | Tree loss: 4.502 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 018 | Total loss: 4.508 | Reg loss: 0.029 | Tree loss: 4.508 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 018 | Total loss: 4.551 | Reg loss: 0.029 | Tree loss: 4.551 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 018 | Total loss: 4.494 | Reg loss: 0.029 | Tree loss: 4.494 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 018 | Total loss: 4.480 | Reg loss: 0.030 | Tree loss: 4.480 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 018 | Total loss: 4.508 | Reg loss: 0.030 | Tree loss: 4.508 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 018 | Total loss: 4.452 | Reg loss: 0.030 | Tree loss: 4.452 | Accuracy: 0.089027 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 000 / 018 | Total loss: 4.606 | Reg loss: 0.029 | Tree loss: 4.606 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 018 | Total loss: 4.660 | Reg loss: 0.029 | Tree loss: 4.660 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 018 | Total loss: 4.599 | Reg loss: 0.029 | Tree loss: 4.599 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 018 | Total loss: 4.588 | Reg loss: 0.029 | Tree loss: 4.588 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 018 | Total loss: 4.567 | Reg loss: 0.029 | Tree loss: 4.567 | Accuracy: 0.089844 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 018 | Total loss: 4.551 | Reg loss: 0.029 | Tree loss: 4.551 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 018 | Total loss: 4.485 | Reg loss: 0.029 | Tree loss: 4.485 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 018 | Total loss: 4.558 | Reg loss: 0.029 | Tree loss: 4.558 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 018 | Total loss: 4.553 | Reg loss: 0.029 | Tree loss: 4.553 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 018 | Total loss: 4.529 | Reg loss: 0.029 | Tree loss: 4.529 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 018 | Total loss: 4.545 | Reg loss: 0.030 | Tree loss: 4.545 | Accuracy: 0.050781 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 018 | Total loss: 4.519 | Reg loss: 0.030 | Tree loss: 4.519 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 018 | Total loss: 4.524 | Reg loss: 0.030 | Tree loss: 4.524 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 018 | Total loss: 4.471 | Reg loss: 0.030 | Tree loss: 4.471 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 018 | Total loss: 4.481 | Reg loss: 0.030 | Tree loss: 4.481 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 018 | Total loss: 4.381 | Reg loss: 0.030 | Tree loss: 4.381 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 018 | Total loss: 4.417 | Reg loss: 0.030 | Tree loss: 4.417 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 018 | Total loss: 4.402 | Reg loss: 0.030 | Tree loss: 4.402 | Accuracy: 0.093168 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 58 | Batch: 000 / 018 | Total loss: 4.542 | Reg loss: 0.029 | Tree loss: 4.542 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 018 | Total loss: 4.604 | Reg loss: 0.029 | Tree loss: 4.604 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 018 | Total loss: 4.546 | Reg loss: 0.029 | Tree loss: 4.546 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 018 | Total loss: 4.555 | Reg loss: 0.029 | Tree loss: 4.555 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 018 | Total loss: 4.568 | Reg loss: 0.030 | Tree loss: 4.568 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 018 | Total loss: 4.532 | Reg loss: 0.030 | Tree loss: 4.532 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 018 | Total loss: 4.474 | Reg loss: 0.030 | Tree loss: 4.474 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 018 | Total loss: 4.454 | Reg loss: 0.030 | Tree loss: 4.454 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 018 | Total loss: 4.519 | Reg loss: 0.030 | Tree loss: 4.519 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 018 | Total loss: 4.501 | Reg loss: 0.030 | Tree loss: 4.501 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 018 | Total loss: 4.487 | Reg loss: 0.030 | Tree loss: 4.487 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 018 | Total loss: 4.504 | Reg loss: 0.030 | Tree loss: 4.504 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 018 | Total loss: 4.499 | Reg loss: 0.030 | Tree loss: 4.499 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 018 | Total loss: 4.425 | Reg loss: 0.030 | Tree loss: 4.425 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 018 | Total loss: 4.434 | Reg loss: 0.031 | Tree loss: 4.434 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 018 | Total loss: 4.464 | Reg loss: 0.031 | Tree loss: 4.464 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 018 | Total loss: 4.410 | Reg loss: 0.031 | Tree loss: 4.410 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 018 | Total loss: 4.399 | Reg loss: 0.031 | Tree loss: 4.399 | Accuracy: 0.066253 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 59 | Batch: 000 / 018 | Total loss: 4.583 | Reg loss: 0.030 | Tree loss: 4.583 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 018 | Total loss: 4.543 | Reg loss: 0.030 | Tree loss: 4.543 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 018 | Total loss: 4.564 | Reg loss: 0.030 | Tree loss: 4.564 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 018 | Total loss: 4.505 | Reg loss: 0.030 | Tree loss: 4.505 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 018 | Total loss: 4.505 | Reg loss: 0.030 | Tree loss: 4.505 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 018 | Total loss: 4.554 | Reg loss: 0.030 | Tree loss: 4.554 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 018 | Total loss: 4.510 | Reg loss: 0.030 | Tree loss: 4.510 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 018 | Total loss: 4.523 | Reg loss: 0.030 | Tree loss: 4.523 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 018 | Total loss: 4.457 | Reg loss: 0.030 | Tree loss: 4.457 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 018 | Total loss: 4.441 | Reg loss: 0.030 | Tree loss: 4.441 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 018 | Total loss: 4.548 | Reg loss: 0.031 | Tree loss: 4.548 | Accuracy: 0.048828 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 018 | Total loss: 4.429 | Reg loss: 0.031 | Tree loss: 4.429 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 018 | Total loss: 4.431 | Reg loss: 0.031 | Tree loss: 4.431 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 018 | Total loss: 4.390 | Reg loss: 0.031 | Tree loss: 4.390 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 018 | Total loss: 4.384 | Reg loss: 0.031 | Tree loss: 4.384 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 018 | Total loss: 4.360 | Reg loss: 0.031 | Tree loss: 4.360 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 018 | Total loss: 4.295 | Reg loss: 0.031 | Tree loss: 4.295 | Accuracy: 0.085938 | 0.23 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 018 | Total loss: 4.417 | Reg loss: 0.031 | Tree loss: 4.417 | Accuracy: 0.072464 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 60 | Batch: 000 / 018 | Total loss: 4.520 | Reg loss: 0.030 | Tree loss: 4.520 | Accuracy: 0.091797 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 018 | Total loss: 4.536 | Reg loss: 0.030 | Tree loss: 4.536 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 018 | Total loss: 4.485 | Reg loss: 0.030 | Tree loss: 4.485 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 018 | Total loss: 4.494 | Reg loss: 0.031 | Tree loss: 4.494 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 018 | Total loss: 4.539 | Reg loss: 0.031 | Tree loss: 4.539 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 018 | Total loss: 4.440 | Reg loss: 0.031 | Tree loss: 4.440 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 018 | Total loss: 4.470 | Reg loss: 0.031 | Tree loss: 4.470 | Accuracy: 0.044922 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 007 / 018 | Total loss: 4.509 | Reg loss: 0.031 | Tree loss: 4.509 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 018 | Total loss: 4.459 | Reg loss: 0.031 | Tree loss: 4.459 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 018 | Total loss: 4.466 | Reg loss: 0.031 | Tree loss: 4.466 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 018 | Total loss: 4.438 | Reg loss: 0.031 | Tree loss: 4.438 | Accuracy: 0.072266 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 018 | Total loss: 4.364 | Reg loss: 0.031 | Tree loss: 4.364 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 018 | Total loss: 4.441 | Reg loss: 0.031 | Tree loss: 4.441 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 018 | Total loss: 4.367 | Reg loss: 0.031 | Tree loss: 4.367 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 018 | Total loss: 4.334 | Reg loss: 0.031 | Tree loss: 4.334 | Accuracy: 0.103516 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 018 | Total loss: 4.382 | Reg loss: 0.032 | Tree loss: 4.382 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 018 | Total loss: 4.385 | Reg loss: 0.032 | Tree loss: 4.385 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 018 | Total loss: 4.367 | Reg loss: 0.032 | Tree loss: 4.367 | Accuracy: 0.064182 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 61 | Batch: 000 / 018 | Total loss: 4.545 | Reg loss: 0.031 | Tree loss: 4.545 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 018 | Total loss: 4.481 | Reg loss: 0.031 | Tree loss: 4.481 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 018 | Total loss: 4.485 | Reg loss: 0.031 | Tree loss: 4.485 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 018 | Total loss: 4.458 | Reg loss: 0.031 | Tree loss: 4.458 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 018 | Total loss: 4.486 | Reg loss: 0.031 | Tree loss: 4.486 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 018 | Total loss: 4.462 | Reg loss: 0.031 | Tree loss: 4.462 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 018 | Total loss: 4.449 | Reg loss: 0.031 | Tree loss: 4.449 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 018 | Total loss: 4.462 | Reg loss: 0.031 | Tree loss: 4.462 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 018 | Total loss: 4.372 | Reg loss: 0.031 | Tree loss: 4.372 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 018 | Total loss: 4.408 | Reg loss: 0.031 | Tree loss: 4.408 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 018 | Total loss: 4.408 | Reg loss: 0.031 | Tree loss: 4.408 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 018 | Total loss: 4.375 | Reg loss: 0.032 | Tree loss: 4.375 | Accuracy: 0.103516 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 018 | Total loss: 4.394 | Reg loss: 0.032 | Tree loss: 4.394 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 018 | Total loss: 4.370 | Reg loss: 0.032 | Tree loss: 4.370 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 018 | Total loss: 4.401 | Reg loss: 0.032 | Tree loss: 4.401 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 018 | Total loss: 4.380 | Reg loss: 0.032 | Tree loss: 4.380 | Accuracy: 0.054688 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 018 | Total loss: 4.297 | Reg loss: 0.032 | Tree loss: 4.297 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 018 | Total loss: 4.350 | Reg loss: 0.032 | Tree loss: 4.350 | Accuracy: 0.053830 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 62 | Batch: 000 / 018 | Total loss: 4.439 | Reg loss: 0.031 | Tree loss: 4.439 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 018 | Total loss: 4.532 | Reg loss: 0.031 | Tree loss: 4.532 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 018 | Total loss: 4.517 | Reg loss: 0.031 | Tree loss: 4.517 | Accuracy: 0.074219 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 018 | Total loss: 4.490 | Reg loss: 0.031 | Tree loss: 4.490 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 018 | Total loss: 4.459 | Reg loss: 0.031 | Tree loss: 4.459 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 018 | Total loss: 4.492 | Reg loss: 0.032 | Tree loss: 4.492 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 018 | Total loss: 4.418 | Reg loss: 0.032 | Tree loss: 4.418 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 018 | Total loss: 4.405 | Reg loss: 0.032 | Tree loss: 4.405 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 018 | Total loss: 4.377 | Reg loss: 0.032 | Tree loss: 4.377 | Accuracy: 0.066406 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 018 | Total loss: 4.408 | Reg loss: 0.032 | Tree loss: 4.408 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 018 | Total loss: 4.387 | Reg loss: 0.032 | Tree loss: 4.387 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 018 | Total loss: 4.397 | Reg loss: 0.032 | Tree loss: 4.397 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 018 | Total loss: 4.405 | Reg loss: 0.032 | Tree loss: 4.405 | Accuracy: 0.056641 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 018 | Total loss: 4.303 | Reg loss: 0.032 | Tree loss: 4.303 | Accuracy: 0.095703 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 018 | Total loss: 4.291 | Reg loss: 0.032 | Tree loss: 4.291 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 018 | Total loss: 4.353 | Reg loss: 0.032 | Tree loss: 4.353 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 018 | Total loss: 4.258 | Reg loss: 0.033 | Tree loss: 4.258 | Accuracy: 0.095703 | 0.23 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 018 | Total loss: 4.272 | Reg loss: 0.033 | Tree loss: 4.272 | Accuracy: 0.072464 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 63 | Batch: 000 / 018 | Total loss: 4.479 | Reg loss: 0.032 | Tree loss: 4.479 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 018 | Total loss: 4.403 | Reg loss: 0.032 | Tree loss: 4.403 | Accuracy: 0.083984 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 018 | Total loss: 4.426 | Reg loss: 0.032 | Tree loss: 4.426 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 018 | Total loss: 4.411 | Reg loss: 0.032 | Tree loss: 4.411 | Accuracy: 0.080078 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 018 | Total loss: 4.357 | Reg loss: 0.032 | Tree loss: 4.357 | Accuracy: 0.097656 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 018 | Total loss: 4.434 | Reg loss: 0.032 | Tree loss: 4.434 | Accuracy: 0.087891 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 018 | Total loss: 4.485 | Reg loss: 0.032 | Tree loss: 4.485 | Accuracy: 0.052734 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 018 | Total loss: 4.389 | Reg loss: 0.032 | Tree loss: 4.389 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 018 | Total loss: 4.418 | Reg loss: 0.032 | Tree loss: 4.418 | Accuracy: 0.060547 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 018 | Total loss: 4.393 | Reg loss: 0.032 | Tree loss: 4.393 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 018 | Total loss: 4.464 | Reg loss: 0.032 | Tree loss: 4.464 | Accuracy: 0.039062 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 018 | Total loss: 4.333 | Reg loss: 0.032 | Tree loss: 4.333 | Accuracy: 0.070312 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 018 | Total loss: 4.297 | Reg loss: 0.032 | Tree loss: 4.297 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 018 | Total loss: 4.362 | Reg loss: 0.033 | Tree loss: 4.362 | Accuracy: 0.074219 | 0.23 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 014 / 018 | Total loss: 4.256 | Reg loss: 0.033 | Tree loss: 4.256 | Accuracy: 0.076172 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 018 | Total loss: 4.346 | Reg loss: 0.033 | Tree loss: 4.346 | Accuracy: 0.058594 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 018 | Total loss: 4.305 | Reg loss: 0.033 | Tree loss: 4.305 | Accuracy: 0.064453 | 0.23 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 018 | Total loss: 4.300 | Reg loss: 0.033 | Tree loss: 4.300 | Accuracy: 0.047619 | 0.23 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 64 | Batch: 000 / 018 | Total loss: 4.462 | Reg loss: 0.032 | Tree loss: 4.462 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 018 | Total loss: 4.465 | Reg loss: 0.032 | Tree loss: 4.465 | Accuracy: 0.062500 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 018 | Total loss: 4.391 | Reg loss: 0.032 | Tree loss: 4.391 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 018 | Total loss: 4.440 | Reg loss: 0.032 | Tree loss: 4.440 | Accuracy: 0.068359 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 018 | Total loss: 4.382 | Reg loss: 0.032 | Tree loss: 4.382 | Accuracy: 0.082031 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 018 | Total loss: 4.352 | Reg loss: 0.032 | Tree loss: 4.352 | Accuracy: 0.078125 | 0.23 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 018 | Total loss: 4.407 | Reg loss: 0.032 | Tree loss: 4.407 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 018 | Total loss: 4.354 | Reg loss: 0.032 | Tree loss: 4.354 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 018 | Total loss: 4.363 | Reg loss: 0.032 | Tree loss: 4.363 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 018 | Total loss: 4.392 | Reg loss: 0.033 | Tree loss: 4.392 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 018 | Total loss: 4.295 | Reg loss: 0.033 | Tree loss: 4.295 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 018 | Total loss: 4.372 | Reg loss: 0.033 | Tree loss: 4.372 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 018 | Total loss: 4.331 | Reg loss: 0.033 | Tree loss: 4.331 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 018 | Total loss: 4.321 | Reg loss: 0.033 | Tree loss: 4.321 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 018 | Total loss: 4.300 | Reg loss: 0.033 | Tree loss: 4.300 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 018 | Total loss: 4.260 | Reg loss: 0.033 | Tree loss: 4.260 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 018 | Total loss: 4.293 | Reg loss: 0.033 | Tree loss: 4.293 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 018 | Total loss: 4.366 | Reg loss: 0.033 | Tree loss: 4.366 | Accuracy: 0.053830 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 65 | Batch: 000 / 018 | Total loss: 4.387 | Reg loss: 0.032 | Tree loss: 4.387 | Accuracy: 0.091797 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 018 | Total loss: 4.469 | Reg loss: 0.032 | Tree loss: 4.469 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 018 | Total loss: 4.393 | Reg loss: 0.032 | Tree loss: 4.393 | Accuracy: 0.093750 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 018 | Total loss: 4.429 | Reg loss: 0.033 | Tree loss: 4.429 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 018 | Total loss: 4.402 | Reg loss: 0.033 | Tree loss: 4.402 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 018 | Total loss: 4.419 | Reg loss: 0.033 | Tree loss: 4.419 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 018 | Total loss: 4.353 | Reg loss: 0.033 | Tree loss: 4.353 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 018 | Total loss: 4.355 | Reg loss: 0.033 | Tree loss: 4.355 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 018 | Total loss: 4.390 | Reg loss: 0.033 | Tree loss: 4.390 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 018 | Total loss: 4.328 | Reg loss: 0.033 | Tree loss: 4.328 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 018 | Total loss: 4.326 | Reg loss: 0.033 | Tree loss: 4.326 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 018 | Total loss: 4.289 | Reg loss: 0.033 | Tree loss: 4.289 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 018 | Total loss: 4.297 | Reg loss: 0.033 | Tree loss: 4.297 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 018 | Total loss: 4.252 | Reg loss: 0.033 | Tree loss: 4.252 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 018 | Total loss: 4.312 | Reg loss: 0.033 | Tree loss: 4.312 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 018 | Total loss: 4.314 | Reg loss: 0.033 | Tree loss: 4.314 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 018 | Total loss: 4.231 | Reg loss: 0.033 | Tree loss: 4.231 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 018 | Total loss: 4.305 | Reg loss: 0.034 | Tree loss: 4.305 | Accuracy: 0.055901 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 66 | Batch: 000 / 018 | Total loss: 4.431 | Reg loss: 0.033 | Tree loss: 4.431 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 018 | Total loss: 4.395 | Reg loss: 0.033 | Tree loss: 4.395 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 018 | Total loss: 4.461 | Reg loss: 0.033 | Tree loss: 4.461 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 018 | Total loss: 4.414 | Reg loss: 0.033 | Tree loss: 4.414 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 018 | Total loss: 4.362 | Reg loss: 0.033 | Tree loss: 4.362 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 018 | Total loss: 4.414 | Reg loss: 0.033 | Tree loss: 4.414 | Accuracy: 0.042969 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 018 | Total loss: 4.381 | Reg loss: 0.033 | Tree loss: 4.381 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 018 | Total loss: 4.302 | Reg loss: 0.033 | Tree loss: 4.302 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 018 | Total loss: 4.379 | Reg loss: 0.033 | Tree loss: 4.379 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 018 | Total loss: 4.320 | Reg loss: 0.033 | Tree loss: 4.320 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 018 | Total loss: 4.352 | Reg loss: 0.033 | Tree loss: 4.352 | Accuracy: 0.050781 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 018 | Total loss: 4.259 | Reg loss: 0.033 | Tree loss: 4.259 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 018 | Total loss: 4.262 | Reg loss: 0.033 | Tree loss: 4.262 | Accuracy: 0.099609 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 018 | Total loss: 4.317 | Reg loss: 0.033 | Tree loss: 4.317 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 018 | Total loss: 4.256 | Reg loss: 0.034 | Tree loss: 4.256 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 018 | Total loss: 4.206 | Reg loss: 0.034 | Tree loss: 4.206 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 018 | Total loss: 4.256 | Reg loss: 0.034 | Tree loss: 4.256 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 018 | Total loss: 4.207 | Reg loss: 0.034 | Tree loss: 4.207 | Accuracy: 0.070393 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 67 | Batch: 000 / 018 | Total loss: 4.374 | Reg loss: 0.033 | Tree loss: 4.374 | Accuracy: 0.068359 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 001 / 018 | Total loss: 4.393 | Reg loss: 0.033 | Tree loss: 4.393 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 018 | Total loss: 4.397 | Reg loss: 0.033 | Tree loss: 4.397 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 018 | Total loss: 4.386 | Reg loss: 0.033 | Tree loss: 4.386 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 018 | Total loss: 4.310 | Reg loss: 0.033 | Tree loss: 4.310 | Accuracy: 0.091797 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 018 | Total loss: 4.340 | Reg loss: 0.033 | Tree loss: 4.340 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 018 | Total loss: 4.403 | Reg loss: 0.033 | Tree loss: 4.403 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 018 | Total loss: 4.304 | Reg loss: 0.033 | Tree loss: 4.304 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 018 | Total loss: 4.347 | Reg loss: 0.033 | Tree loss: 4.347 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 018 | Total loss: 4.274 | Reg loss: 0.033 | Tree loss: 4.274 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 018 | Total loss: 4.316 | Reg loss: 0.034 | Tree loss: 4.316 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 018 | Total loss: 4.307 | Reg loss: 0.034 | Tree loss: 4.307 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 018 | Total loss: 4.194 | Reg loss: 0.034 | Tree loss: 4.194 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 018 | Total loss: 4.252 | Reg loss: 0.034 | Tree loss: 4.252 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 018 | Total loss: 4.301 | Reg loss: 0.034 | Tree loss: 4.301 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 018 | Total loss: 4.329 | Reg loss: 0.034 | Tree loss: 4.329 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 018 | Total loss: 4.260 | Reg loss: 0.034 | Tree loss: 4.260 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 018 | Total loss: 4.243 | Reg loss: 0.034 | Tree loss: 4.243 | Accuracy: 0.066253 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 68 | Batch: 000 / 018 | Total loss: 4.424 | Reg loss: 0.033 | Tree loss: 4.424 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 018 | Total loss: 4.379 | Reg loss: 0.033 | Tree loss: 4.379 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 018 | Total loss: 4.364 | Reg loss: 0.033 | Tree loss: 4.364 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 018 | Total loss: 4.381 | Reg loss: 0.033 | Tree loss: 4.381 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 018 | Total loss: 4.369 | Reg loss: 0.033 | Tree loss: 4.369 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 018 | Total loss: 4.368 | Reg loss: 0.033 | Tree loss: 4.368 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 018 | Total loss: 4.402 | Reg loss: 0.034 | Tree loss: 4.402 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 018 | Total loss: 4.351 | Reg loss: 0.034 | Tree loss: 4.351 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 018 | Total loss: 4.265 | Reg loss: 0.034 | Tree loss: 4.265 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 018 | Total loss: 4.239 | Reg loss: 0.034 | Tree loss: 4.239 | Accuracy: 0.101562 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 018 | Total loss: 4.251 | Reg loss: 0.034 | Tree loss: 4.251 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 018 | Total loss: 4.245 | Reg loss: 0.034 | Tree loss: 4.245 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 018 | Total loss: 4.266 | Reg loss: 0.034 | Tree loss: 4.266 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 018 | Total loss: 4.275 | Reg loss: 0.034 | Tree loss: 4.275 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 018 | Total loss: 4.274 | Reg loss: 0.034 | Tree loss: 4.274 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 018 | Total loss: 4.173 | Reg loss: 0.034 | Tree loss: 4.173 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 018 | Total loss: 4.242 | Reg loss: 0.034 | Tree loss: 4.242 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 018 | Total loss: 4.232 | Reg loss: 0.034 | Tree loss: 4.232 | Accuracy: 0.084886 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 69 | Batch: 000 / 018 | Total loss: 4.362 | Reg loss: 0.034 | Tree loss: 4.362 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 018 | Total loss: 4.400 | Reg loss: 0.034 | Tree loss: 4.400 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 018 | Total loss: 4.362 | Reg loss: 0.034 | Tree loss: 4.362 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 018 | Total loss: 4.369 | Reg loss: 0.034 | Tree loss: 4.369 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 018 | Total loss: 4.371 | Reg loss: 0.034 | Tree loss: 4.371 | Accuracy: 0.050781 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 018 | Total loss: 4.353 | Reg loss: 0.034 | Tree loss: 4.353 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 018 | Total loss: 4.311 | Reg loss: 0.034 | Tree loss: 4.311 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 018 | Total loss: 4.296 | Reg loss: 0.034 | Tree loss: 4.296 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 018 | Total loss: 4.286 | Reg loss: 0.034 | Tree loss: 4.286 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 018 | Total loss: 4.235 | Reg loss: 0.034 | Tree loss: 4.235 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 018 | Total loss: 4.280 | Reg loss: 0.034 | Tree loss: 4.280 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 018 | Total loss: 4.290 | Reg loss: 0.034 | Tree loss: 4.290 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 018 | Total loss: 4.220 | Reg loss: 0.034 | Tree loss: 4.220 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 018 | Total loss: 4.275 | Reg loss: 0.034 | Tree loss: 4.275 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 018 | Total loss: 4.239 | Reg loss: 0.034 | Tree loss: 4.239 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 018 | Total loss: 4.219 | Reg loss: 0.034 | Tree loss: 4.219 | Accuracy: 0.095703 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 018 | Total loss: 4.176 | Reg loss: 0.034 | Tree loss: 4.176 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 018 | Total loss: 4.245 | Reg loss: 0.035 | Tree loss: 4.245 | Accuracy: 0.062112 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 70 | Batch: 000 / 018 | Total loss: 4.318 | Reg loss: 0.034 | Tree loss: 4.318 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 018 | Total loss: 4.439 | Reg loss: 0.034 | Tree loss: 4.439 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 018 | Total loss: 4.342 | Reg loss: 0.034 | Tree loss: 4.342 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 018 | Total loss: 4.343 | Reg loss: 0.034 | Tree loss: 4.343 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 018 | Total loss: 4.346 | Reg loss: 0.034 | Tree loss: 4.346 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 018 | Total loss: 4.371 | Reg loss: 0.034 | Tree loss: 4.371 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 018 | Total loss: 4.317 | Reg loss: 0.034 | Tree loss: 4.317 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 018 | Total loss: 4.356 | Reg loss: 0.034 | Tree loss: 4.356 | Accuracy: 0.054688 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 008 / 018 | Total loss: 4.294 | Reg loss: 0.034 | Tree loss: 4.294 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 018 | Total loss: 4.285 | Reg loss: 0.034 | Tree loss: 4.285 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 018 | Total loss: 4.255 | Reg loss: 0.034 | Tree loss: 4.255 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 018 | Total loss: 4.230 | Reg loss: 0.034 | Tree loss: 4.230 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 018 | Total loss: 4.274 | Reg loss: 0.034 | Tree loss: 4.274 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 018 | Total loss: 4.195 | Reg loss: 0.034 | Tree loss: 4.195 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 018 | Total loss: 4.154 | Reg loss: 0.035 | Tree loss: 4.154 | Accuracy: 0.095703 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 018 | Total loss: 4.232 | Reg loss: 0.035 | Tree loss: 4.232 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 018 | Total loss: 4.144 | Reg loss: 0.035 | Tree loss: 4.144 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 018 | Total loss: 4.199 | Reg loss: 0.035 | Tree loss: 4.199 | Accuracy: 0.053830 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 71 | Batch: 000 / 018 | Total loss: 4.385 | Reg loss: 0.034 | Tree loss: 4.385 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 018 | Total loss: 4.382 | Reg loss: 0.034 | Tree loss: 4.382 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 018 | Total loss: 4.360 | Reg loss: 0.034 | Tree loss: 4.360 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 018 | Total loss: 4.341 | Reg loss: 0.034 | Tree loss: 4.341 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 018 | Total loss: 4.400 | Reg loss: 0.034 | Tree loss: 4.400 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 018 | Total loss: 4.332 | Reg loss: 0.034 | Tree loss: 4.332 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 018 | Total loss: 4.337 | Reg loss: 0.034 | Tree loss: 4.337 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 018 | Total loss: 4.260 | Reg loss: 0.034 | Tree loss: 4.260 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 018 | Total loss: 4.228 | Reg loss: 0.034 | Tree loss: 4.228 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 018 | Total loss: 4.271 | Reg loss: 0.034 | Tree loss: 4.271 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 018 | Total loss: 4.242 | Reg loss: 0.034 | Tree loss: 4.242 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 018 | Total loss: 4.241 | Reg loss: 0.034 | Tree loss: 4.241 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 018 | Total loss: 4.165 | Reg loss: 0.035 | Tree loss: 4.165 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 018 | Total loss: 4.303 | Reg loss: 0.035 | Tree loss: 4.303 | Accuracy: 0.046875 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 018 | Total loss: 4.161 | Reg loss: 0.035 | Tree loss: 4.161 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 018 | Total loss: 4.195 | Reg loss: 0.035 | Tree loss: 4.195 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 018 | Total loss: 4.157 | Reg loss: 0.035 | Tree loss: 4.157 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 018 | Total loss: 4.151 | Reg loss: 0.035 | Tree loss: 4.151 | Accuracy: 0.082816 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 72 | Batch: 000 / 018 | Total loss: 4.344 | Reg loss: 0.034 | Tree loss: 4.344 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 018 | Total loss: 4.326 | Reg loss: 0.034 | Tree loss: 4.326 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 018 | Total loss: 4.323 | Reg loss: 0.034 | Tree loss: 4.323 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 018 | Total loss: 4.348 | Reg loss: 0.034 | Tree loss: 4.348 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 018 | Total loss: 4.368 | Reg loss: 0.034 | Tree loss: 4.368 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 018 | Total loss: 4.355 | Reg loss: 0.034 | Tree loss: 4.355 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 018 | Total loss: 4.345 | Reg loss: 0.034 | Tree loss: 4.345 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 018 | Total loss: 4.255 | Reg loss: 0.034 | Tree loss: 4.255 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 018 | Total loss: 4.286 | Reg loss: 0.034 | Tree loss: 4.286 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 018 | Total loss: 4.239 | Reg loss: 0.035 | Tree loss: 4.239 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 018 | Total loss: 4.289 | Reg loss: 0.035 | Tree loss: 4.289 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 018 | Total loss: 4.210 | Reg loss: 0.035 | Tree loss: 4.210 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 018 | Total loss: 4.209 | Reg loss: 0.035 | Tree loss: 4.209 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 018 | Total loss: 4.212 | Reg loss: 0.035 | Tree loss: 4.212 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 018 | Total loss: 4.195 | Reg loss: 0.035 | Tree loss: 4.195 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 018 | Total loss: 4.161 | Reg loss: 0.035 | Tree loss: 4.161 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 018 | Total loss: 4.126 | Reg loss: 0.035 | Tree loss: 4.126 | Accuracy: 0.097656 | 0.231 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 018 | Total loss: 4.157 | Reg loss: 0.035 | Tree loss: 4.157 | Accuracy: 0.057971 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 73 | Batch: 000 / 018 | Total loss: 4.308 | Reg loss: 0.034 | Tree loss: 4.308 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 018 | Total loss: 4.338 | Reg loss: 0.034 | Tree loss: 4.338 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 018 | Total loss: 4.324 | Reg loss: 0.034 | Tree loss: 4.324 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 018 | Total loss: 4.248 | Reg loss: 0.034 | Tree loss: 4.248 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 018 | Total loss: 4.317 | Reg loss: 0.035 | Tree loss: 4.317 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 018 | Total loss: 4.234 | Reg loss: 0.035 | Tree loss: 4.234 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 018 | Total loss: 4.322 | Reg loss: 0.035 | Tree loss: 4.322 | Accuracy: 0.044922 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 018 | Total loss: 4.258 | Reg loss: 0.035 | Tree loss: 4.258 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 018 | Total loss: 4.219 | Reg loss: 0.035 | Tree loss: 4.219 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 018 | Total loss: 4.274 | Reg loss: 0.035 | Tree loss: 4.274 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 018 | Total loss: 4.226 | Reg loss: 0.035 | Tree loss: 4.226 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 018 | Total loss: 4.285 | Reg loss: 0.035 | Tree loss: 4.285 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 018 | Total loss: 4.205 | Reg loss: 0.035 | Tree loss: 4.205 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 018 | Total loss: 4.237 | Reg loss: 0.035 | Tree loss: 4.237 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 018 | Total loss: 4.228 | Reg loss: 0.035 | Tree loss: 4.228 | Accuracy: 0.058594 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 015 / 018 | Total loss: 4.134 | Reg loss: 0.035 | Tree loss: 4.134 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 018 | Total loss: 4.217 | Reg loss: 0.035 | Tree loss: 4.217 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 018 | Total loss: 4.230 | Reg loss: 0.035 | Tree loss: 4.230 | Accuracy: 0.051760 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 74 | Batch: 000 / 018 | Total loss: 4.330 | Reg loss: 0.035 | Tree loss: 4.330 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 018 | Total loss: 4.298 | Reg loss: 0.035 | Tree loss: 4.298 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 018 | Total loss: 4.323 | Reg loss: 0.035 | Tree loss: 4.323 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 018 | Total loss: 4.250 | Reg loss: 0.035 | Tree loss: 4.250 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 018 | Total loss: 4.260 | Reg loss: 0.035 | Tree loss: 4.260 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 018 | Total loss: 4.363 | Reg loss: 0.035 | Tree loss: 4.363 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 018 | Total loss: 4.290 | Reg loss: 0.035 | Tree loss: 4.290 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 018 | Total loss: 4.321 | Reg loss: 0.035 | Tree loss: 4.321 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 018 | Total loss: 4.220 | Reg loss: 0.035 | Tree loss: 4.220 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 018 | Total loss: 4.215 | Reg loss: 0.035 | Tree loss: 4.215 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 018 | Total loss: 4.242 | Reg loss: 0.035 | Tree loss: 4.242 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 018 | Total loss: 4.245 | Reg loss: 0.035 | Tree loss: 4.245 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 018 | Total loss: 4.242 | Reg loss: 0.035 | Tree loss: 4.242 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 018 | Total loss: 4.180 | Reg loss: 0.035 | Tree loss: 4.180 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 018 | Total loss: 4.176 | Reg loss: 0.035 | Tree loss: 4.176 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 018 | Total loss: 4.180 | Reg loss: 0.035 | Tree loss: 4.180 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 018 | Total loss: 4.159 | Reg loss: 0.035 | Tree loss: 4.159 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 018 | Total loss: 4.161 | Reg loss: 0.035 | Tree loss: 4.161 | Accuracy: 0.068323 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 75 | Batch: 000 / 018 | Total loss: 4.388 | Reg loss: 0.035 | Tree loss: 4.388 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 018 | Total loss: 4.303 | Reg loss: 0.035 | Tree loss: 4.303 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 018 | Total loss: 4.293 | Reg loss: 0.035 | Tree loss: 4.293 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 018 | Total loss: 4.326 | Reg loss: 0.035 | Tree loss: 4.326 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 018 | Total loss: 4.301 | Reg loss: 0.035 | Tree loss: 4.301 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 018 | Total loss: 4.299 | Reg loss: 0.035 | Tree loss: 4.299 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 018 | Total loss: 4.263 | Reg loss: 0.035 | Tree loss: 4.263 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 018 | Total loss: 4.250 | Reg loss: 0.035 | Tree loss: 4.250 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 018 | Total loss: 4.261 | Reg loss: 0.035 | Tree loss: 4.261 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 018 | Total loss: 4.198 | Reg loss: 0.035 | Tree loss: 4.198 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 018 | Total loss: 4.177 | Reg loss: 0.035 | Tree loss: 4.177 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 018 | Total loss: 4.249 | Reg loss: 0.035 | Tree loss: 4.249 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 018 | Total loss: 4.189 | Reg loss: 0.035 | Tree loss: 4.189 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 018 | Total loss: 4.190 | Reg loss: 0.035 | Tree loss: 4.190 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 018 | Total loss: 4.113 | Reg loss: 0.035 | Tree loss: 4.113 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 018 | Total loss: 4.211 | Reg loss: 0.035 | Tree loss: 4.211 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 018 | Total loss: 4.150 | Reg loss: 0.036 | Tree loss: 4.150 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 018 | Total loss: 4.159 | Reg loss: 0.036 | Tree loss: 4.159 | Accuracy: 0.060041 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 76 | Batch: 000 / 018 | Total loss: 4.357 | Reg loss: 0.035 | Tree loss: 4.357 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 018 | Total loss: 4.340 | Reg loss: 0.035 | Tree loss: 4.340 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 018 | Total loss: 4.265 | Reg loss: 0.035 | Tree loss: 4.265 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 018 | Total loss: 4.306 | Reg loss: 0.035 | Tree loss: 4.306 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 018 | Total loss: 4.315 | Reg loss: 0.035 | Tree loss: 4.315 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 018 | Total loss: 4.245 | Reg loss: 0.035 | Tree loss: 4.245 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 018 | Total loss: 4.268 | Reg loss: 0.035 | Tree loss: 4.268 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 018 | Total loss: 4.275 | Reg loss: 0.035 | Tree loss: 4.275 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 018 | Total loss: 4.217 | Reg loss: 0.035 | Tree loss: 4.217 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 018 | Total loss: 4.244 | Reg loss: 0.035 | Tree loss: 4.244 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 018 | Total loss: 4.157 | Reg loss: 0.035 | Tree loss: 4.157 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 018 | Total loss: 4.217 | Reg loss: 0.035 | Tree loss: 4.217 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 018 | Total loss: 4.156 | Reg loss: 0.035 | Tree loss: 4.156 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 018 | Total loss: 4.148 | Reg loss: 0.035 | Tree loss: 4.148 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 018 | Total loss: 4.181 | Reg loss: 0.036 | Tree loss: 4.181 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 018 | Total loss: 4.153 | Reg loss: 0.036 | Tree loss: 4.153 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 018 | Total loss: 4.158 | Reg loss: 0.036 | Tree loss: 4.158 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 018 | Total loss: 4.194 | Reg loss: 0.036 | Tree loss: 4.194 | Accuracy: 0.066253 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 77 | Batch: 000 / 018 | Total loss: 4.331 | Reg loss: 0.035 | Tree loss: 4.331 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 018 | Total loss: 4.303 | Reg loss: 0.035 | Tree loss: 4.303 | Accuracy: 0.082031 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 002 / 018 | Total loss: 4.349 | Reg loss: 0.035 | Tree loss: 4.349 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 018 | Total loss: 4.279 | Reg loss: 0.035 | Tree loss: 4.279 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 018 | Total loss: 4.300 | Reg loss: 0.035 | Tree loss: 4.300 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 018 | Total loss: 4.195 | Reg loss: 0.035 | Tree loss: 4.195 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 018 | Total loss: 4.224 | Reg loss: 0.035 | Tree loss: 4.224 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 018 | Total loss: 4.218 | Reg loss: 0.035 | Tree loss: 4.218 | Accuracy: 0.070312 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 018 | Total loss: 4.215 | Reg loss: 0.035 | Tree loss: 4.215 | Accuracy: 0.064453 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 018 | Total loss: 4.187 | Reg loss: 0.035 | Tree loss: 4.187 | Accuracy: 0.070312 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 018 | Total loss: 4.270 | Reg loss: 0.035 | Tree loss: 4.270 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 018 | Total loss: 4.250 | Reg loss: 0.035 | Tree loss: 4.250 | Accuracy: 0.060547 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 018 | Total loss: 4.215 | Reg loss: 0.036 | Tree loss: 4.215 | Accuracy: 0.058594 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 018 | Total loss: 4.215 | Reg loss: 0.036 | Tree loss: 4.215 | Accuracy: 0.058594 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 018 | Total loss: 4.171 | Reg loss: 0.036 | Tree loss: 4.171 | Accuracy: 0.072266 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 018 | Total loss: 4.129 | Reg loss: 0.036 | Tree loss: 4.129 | Accuracy: 0.082031 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 018 | Total loss: 4.092 | Reg loss: 0.036 | Tree loss: 4.092 | Accuracy: 0.078125 | 0.232 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 018 | Total loss: 4.140 | Reg loss: 0.036 | Tree loss: 4.140 | Accuracy: 0.084886 | 0.232 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 78 | Batch: 000 / 018 | Total loss: 4.342 | Reg loss: 0.035 | Tree loss: 4.342 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 018 | Total loss: 4.327 | Reg loss: 0.035 | Tree loss: 4.327 | Accuracy: 0.064453 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 018 | Total loss: 4.260 | Reg loss: 0.035 | Tree loss: 4.260 | Accuracy: 0.070312 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 018 | Total loss: 4.217 | Reg loss: 0.035 | Tree loss: 4.217 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 018 | Total loss: 4.273 | Reg loss: 0.035 | Tree loss: 4.273 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 018 | Total loss: 4.258 | Reg loss: 0.035 | Tree loss: 4.258 | Accuracy: 0.060547 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 018 | Total loss: 4.291 | Reg loss: 0.035 | Tree loss: 4.291 | Accuracy: 0.101562 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 018 | Total loss: 4.188 | Reg loss: 0.035 | Tree loss: 4.188 | Accuracy: 0.085938 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 018 | Total loss: 4.205 | Reg loss: 0.035 | Tree loss: 4.205 | Accuracy: 0.058594 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 018 | Total loss: 4.216 | Reg loss: 0.035 | Tree loss: 4.216 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 018 | Total loss: 4.164 | Reg loss: 0.036 | Tree loss: 4.164 | Accuracy: 0.080078 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 018 | Total loss: 4.243 | Reg loss: 0.036 | Tree loss: 4.243 | Accuracy: 0.076172 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 018 | Total loss: 4.179 | Reg loss: 0.036 | Tree loss: 4.179 | Accuracy: 0.046875 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 018 | Total loss: 4.166 | Reg loss: 0.036 | Tree loss: 4.166 | Accuracy: 0.091797 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 018 | Total loss: 4.231 | Reg loss: 0.036 | Tree loss: 4.231 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 018 | Total loss: 4.145 | Reg loss: 0.036 | Tree loss: 4.145 | Accuracy: 0.062500 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 018 | Total loss: 4.142 | Reg loss: 0.036 | Tree loss: 4.142 | Accuracy: 0.062500 | 0.232 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 018 | Total loss: 4.138 | Reg loss: 0.036 | Tree loss: 4.138 | Accuracy: 0.064182 | 0.232 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 79 | Batch: 000 / 018 | Total loss: 4.329 | Reg loss: 0.035 | Tree loss: 4.329 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 018 | Total loss: 4.269 | Reg loss: 0.035 | Tree loss: 4.269 | Accuracy: 0.074219 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 018 | Total loss: 4.254 | Reg loss: 0.035 | Tree loss: 4.254 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 018 | Total loss: 4.214 | Reg loss: 0.035 | Tree loss: 4.214 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 018 | Total loss: 4.293 | Reg loss: 0.035 | Tree loss: 4.293 | Accuracy: 0.060547 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 018 | Total loss: 4.303 | Reg loss: 0.035 | Tree loss: 4.303 | Accuracy: 0.080078 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 018 | Total loss: 4.210 | Reg loss: 0.035 | Tree loss: 4.210 | Accuracy: 0.062500 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 018 | Total loss: 4.224 | Reg loss: 0.036 | Tree loss: 4.224 | Accuracy: 0.064453 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 018 | Total loss: 4.187 | Reg loss: 0.036 | Tree loss: 4.187 | Accuracy: 0.089844 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 018 | Total loss: 4.229 | Reg loss: 0.036 | Tree loss: 4.229 | Accuracy: 0.064453 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 018 | Total loss: 4.236 | Reg loss: 0.036 | Tree loss: 4.236 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 018 | Total loss: 4.170 | Reg loss: 0.036 | Tree loss: 4.170 | Accuracy: 0.082031 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 018 | Total loss: 4.145 | Reg loss: 0.036 | Tree loss: 4.145 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 018 | Total loss: 4.193 | Reg loss: 0.036 | Tree loss: 4.193 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 018 | Total loss: 4.212 | Reg loss: 0.036 | Tree loss: 4.212 | Accuracy: 0.062500 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 018 | Total loss: 4.131 | Reg loss: 0.036 | Tree loss: 4.131 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 018 | Total loss: 4.114 | Reg loss: 0.036 | Tree loss: 4.114 | Accuracy: 0.082031 | 0.232 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 018 | Total loss: 4.180 | Reg loss: 0.036 | Tree loss: 4.180 | Accuracy: 0.066253 | 0.232 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 80 | Batch: 000 / 018 | Total loss: 4.222 | Reg loss: 0.035 | Tree loss: 4.222 | Accuracy: 0.076172 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 018 | Total loss: 4.357 | Reg loss: 0.035 | Tree loss: 4.357 | Accuracy: 0.048828 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 018 | Total loss: 4.231 | Reg loss: 0.035 | Tree loss: 4.231 | Accuracy: 0.062500 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 018 | Total loss: 4.279 | Reg loss: 0.036 | Tree loss: 4.279 | Accuracy: 0.056641 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 018 | Total loss: 4.283 | Reg loss: 0.036 | Tree loss: 4.283 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 018 | Total loss: 4.259 | Reg loss: 0.036 | Tree loss: 4.259 | Accuracy: 0.056641 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 018 | Total loss: 4.215 | Reg loss: 0.036 | Tree loss: 4.215 | Accuracy: 0.089844 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 018 | Total loss: 4.241 | Reg loss: 0.036 | Tree loss: 4.241 | Accuracy: 0.066406 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 018 | Total loss: 4.226 | Reg loss: 0.036 | Tree loss: 4.226 | Accuracy: 0.076172 | 0.232 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 009 / 018 | Total loss: 4.169 | Reg loss: 0.036 | Tree loss: 4.169 | Accuracy: 0.080078 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 018 | Total loss: 4.230 | Reg loss: 0.036 | Tree loss: 4.230 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 018 | Total loss: 4.235 | Reg loss: 0.036 | Tree loss: 4.235 | Accuracy: 0.068359 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 018 | Total loss: 4.169 | Reg loss: 0.036 | Tree loss: 4.169 | Accuracy: 0.070312 | 0.232 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 018 | Total loss: 4.130 | Reg loss: 0.036 | Tree loss: 4.130 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 018 | Total loss: 4.118 | Reg loss: 0.036 | Tree loss: 4.118 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 018 | Total loss: 4.116 | Reg loss: 0.036 | Tree loss: 4.116 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 018 | Total loss: 4.131 | Reg loss: 0.036 | Tree loss: 4.131 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 018 | Total loss: 4.196 | Reg loss: 0.036 | Tree loss: 4.196 | Accuracy: 0.060041 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 81 | Batch: 000 / 018 | Total loss: 4.312 | Reg loss: 0.036 | Tree loss: 4.312 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 018 | Total loss: 4.291 | Reg loss: 0.036 | Tree loss: 4.291 | Accuracy: 0.046875 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 018 | Total loss: 4.231 | Reg loss: 0.036 | Tree loss: 4.231 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 018 | Total loss: 4.199 | Reg loss: 0.036 | Tree loss: 4.199 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 018 | Total loss: 4.234 | Reg loss: 0.036 | Tree loss: 4.234 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 018 | Total loss: 4.247 | Reg loss: 0.036 | Tree loss: 4.247 | Accuracy: 0.046875 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 018 | Total loss: 4.243 | Reg loss: 0.036 | Tree loss: 4.243 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 018 | Total loss: 4.228 | Reg loss: 0.036 | Tree loss: 4.228 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 018 | Total loss: 4.249 | Reg loss: 0.036 | Tree loss: 4.249 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 018 | Total loss: 4.123 | Reg loss: 0.036 | Tree loss: 4.123 | Accuracy: 0.093750 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 018 | Total loss: 4.195 | Reg loss: 0.036 | Tree loss: 4.195 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 018 | Total loss: 4.229 | Reg loss: 0.036 | Tree loss: 4.229 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 018 | Total loss: 4.118 | Reg loss: 0.036 | Tree loss: 4.118 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 018 | Total loss: 4.126 | Reg loss: 0.036 | Tree loss: 4.126 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 018 | Total loss: 4.153 | Reg loss: 0.036 | Tree loss: 4.153 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 018 | Total loss: 4.194 | Reg loss: 0.036 | Tree loss: 4.194 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 018 | Total loss: 4.192 | Reg loss: 0.036 | Tree loss: 4.192 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 018 | Total loss: 4.163 | Reg loss: 0.036 | Tree loss: 4.163 | Accuracy: 0.074534 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 82 | Batch: 000 / 018 | Total loss: 4.336 | Reg loss: 0.036 | Tree loss: 4.336 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 018 | Total loss: 4.309 | Reg loss: 0.036 | Tree loss: 4.309 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 018 | Total loss: 4.208 | Reg loss: 0.036 | Tree loss: 4.208 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 018 | Total loss: 4.237 | Reg loss: 0.036 | Tree loss: 4.237 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 018 | Total loss: 4.256 | Reg loss: 0.036 | Tree loss: 4.256 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 018 | Total loss: 4.286 | Reg loss: 0.036 | Tree loss: 4.286 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 018 | Total loss: 4.196 | Reg loss: 0.036 | Tree loss: 4.196 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 018 | Total loss: 4.226 | Reg loss: 0.036 | Tree loss: 4.226 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 018 | Total loss: 4.220 | Reg loss: 0.036 | Tree loss: 4.220 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 018 | Total loss: 4.175 | Reg loss: 0.036 | Tree loss: 4.175 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 018 | Total loss: 4.168 | Reg loss: 0.036 | Tree loss: 4.168 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 018 | Total loss: 4.170 | Reg loss: 0.036 | Tree loss: 4.170 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 018 | Total loss: 4.138 | Reg loss: 0.036 | Tree loss: 4.138 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 018 | Total loss: 4.173 | Reg loss: 0.036 | Tree loss: 4.173 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 018 | Total loss: 4.140 | Reg loss: 0.036 | Tree loss: 4.140 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 018 | Total loss: 4.129 | Reg loss: 0.036 | Tree loss: 4.129 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 018 | Total loss: 4.184 | Reg loss: 0.036 | Tree loss: 4.184 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 018 | Total loss: 4.087 | Reg loss: 0.036 | Tree loss: 4.087 | Accuracy: 0.070393 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 83 | Batch: 000 / 018 | Total loss: 4.382 | Reg loss: 0.036 | Tree loss: 4.382 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 018 | Total loss: 4.259 | Reg loss: 0.036 | Tree loss: 4.259 | Accuracy: 0.093750 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 018 | Total loss: 4.342 | Reg loss: 0.036 | Tree loss: 4.342 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 018 | Total loss: 4.291 | Reg loss: 0.036 | Tree loss: 4.291 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 018 | Total loss: 4.220 | Reg loss: 0.036 | Tree loss: 4.220 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 018 | Total loss: 4.188 | Reg loss: 0.036 | Tree loss: 4.188 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 018 | Total loss: 4.207 | Reg loss: 0.036 | Tree loss: 4.207 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 018 | Total loss: 4.173 | Reg loss: 0.036 | Tree loss: 4.173 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 018 | Total loss: 4.224 | Reg loss: 0.036 | Tree loss: 4.224 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 018 | Total loss: 4.199 | Reg loss: 0.036 | Tree loss: 4.199 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 018 | Total loss: 4.166 | Reg loss: 0.036 | Tree loss: 4.166 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 018 | Total loss: 4.211 | Reg loss: 0.036 | Tree loss: 4.211 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 018 | Total loss: 4.172 | Reg loss: 0.036 | Tree loss: 4.172 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 018 | Total loss: 4.115 | Reg loss: 0.036 | Tree loss: 4.115 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 018 | Total loss: 4.119 | Reg loss: 0.036 | Tree loss: 4.119 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 018 | Total loss: 4.131 | Reg loss: 0.036 | Tree loss: 4.131 | Accuracy: 0.068359 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 016 / 018 | Total loss: 4.060 | Reg loss: 0.036 | Tree loss: 4.060 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 018 | Total loss: 4.107 | Reg loss: 0.037 | Tree loss: 4.107 | Accuracy: 0.053830 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 84 | Batch: 000 / 018 | Total loss: 4.288 | Reg loss: 0.036 | Tree loss: 4.288 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 018 | Total loss: 4.231 | Reg loss: 0.036 | Tree loss: 4.231 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 018 | Total loss: 4.289 | Reg loss: 0.036 | Tree loss: 4.289 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 018 | Total loss: 4.318 | Reg loss: 0.036 | Tree loss: 4.318 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 018 | Total loss: 4.196 | Reg loss: 0.036 | Tree loss: 4.196 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 018 | Total loss: 4.283 | Reg loss: 0.036 | Tree loss: 4.283 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 018 | Total loss: 4.286 | Reg loss: 0.036 | Tree loss: 4.286 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 018 | Total loss: 4.263 | Reg loss: 0.036 | Tree loss: 4.263 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 018 | Total loss: 4.147 | Reg loss: 0.036 | Tree loss: 4.147 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 018 | Total loss: 4.184 | Reg loss: 0.036 | Tree loss: 4.184 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 018 | Total loss: 4.241 | Reg loss: 0.036 | Tree loss: 4.241 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 018 | Total loss: 4.160 | Reg loss: 0.036 | Tree loss: 4.160 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 018 | Total loss: 4.099 | Reg loss: 0.036 | Tree loss: 4.099 | Accuracy: 0.099609 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 018 | Total loss: 4.154 | Reg loss: 0.036 | Tree loss: 4.154 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 018 | Total loss: 4.121 | Reg loss: 0.036 | Tree loss: 4.121 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 018 | Total loss: 4.050 | Reg loss: 0.036 | Tree loss: 4.050 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 018 | Total loss: 4.128 | Reg loss: 0.037 | Tree loss: 4.128 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 018 | Total loss: 4.061 | Reg loss: 0.037 | Tree loss: 4.061 | Accuracy: 0.057971 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 85 | Batch: 000 / 018 | Total loss: 4.255 | Reg loss: 0.036 | Tree loss: 4.255 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 018 | Total loss: 4.219 | Reg loss: 0.036 | Tree loss: 4.219 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 018 | Total loss: 4.245 | Reg loss: 0.036 | Tree loss: 4.245 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 018 | Total loss: 4.276 | Reg loss: 0.036 | Tree loss: 4.276 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 018 | Total loss: 4.312 | Reg loss: 0.036 | Tree loss: 4.312 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 018 | Total loss: 4.178 | Reg loss: 0.036 | Tree loss: 4.178 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 018 | Total loss: 4.199 | Reg loss: 0.036 | Tree loss: 4.199 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 018 | Total loss: 4.165 | Reg loss: 0.036 | Tree loss: 4.165 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 018 | Total loss: 4.293 | Reg loss: 0.036 | Tree loss: 4.293 | Accuracy: 0.044922 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 018 | Total loss: 4.187 | Reg loss: 0.036 | Tree loss: 4.187 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 018 | Total loss: 4.192 | Reg loss: 0.036 | Tree loss: 4.192 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 018 | Total loss: 4.191 | Reg loss: 0.036 | Tree loss: 4.191 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 018 | Total loss: 4.176 | Reg loss: 0.036 | Tree loss: 4.176 | Accuracy: 0.044922 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 018 | Total loss: 4.121 | Reg loss: 0.036 | Tree loss: 4.121 | Accuracy: 0.093750 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 018 | Total loss: 4.085 | Reg loss: 0.036 | Tree loss: 4.085 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 018 | Total loss: 4.120 | Reg loss: 0.037 | Tree loss: 4.120 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 018 | Total loss: 4.085 | Reg loss: 0.037 | Tree loss: 4.085 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 018 | Total loss: 4.145 | Reg loss: 0.037 | Tree loss: 4.145 | Accuracy: 0.066253 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 86 | Batch: 000 / 018 | Total loss: 4.222 | Reg loss: 0.036 | Tree loss: 4.222 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 018 | Total loss: 4.317 | Reg loss: 0.036 | Tree loss: 4.317 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 018 | Total loss: 4.201 | Reg loss: 0.036 | Tree loss: 4.201 | Accuracy: 0.101562 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 018 | Total loss: 4.191 | Reg loss: 0.036 | Tree loss: 4.191 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 018 | Total loss: 4.244 | Reg loss: 0.036 | Tree loss: 4.244 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 018 | Total loss: 4.267 | Reg loss: 0.036 | Tree loss: 4.267 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 018 | Total loss: 4.257 | Reg loss: 0.036 | Tree loss: 4.257 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 018 | Total loss: 4.199 | Reg loss: 0.036 | Tree loss: 4.199 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 018 | Total loss: 4.175 | Reg loss: 0.036 | Tree loss: 4.175 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 018 | Total loss: 4.151 | Reg loss: 0.036 | Tree loss: 4.151 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 018 | Total loss: 4.165 | Reg loss: 0.036 | Tree loss: 4.165 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 018 | Total loss: 4.175 | Reg loss: 0.036 | Tree loss: 4.175 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 018 | Total loss: 4.196 | Reg loss: 0.036 | Tree loss: 4.196 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 018 | Total loss: 4.147 | Reg loss: 0.036 | Tree loss: 4.147 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 018 | Total loss: 4.199 | Reg loss: 0.037 | Tree loss: 4.199 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 018 | Total loss: 4.143 | Reg loss: 0.037 | Tree loss: 4.143 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 018 | Total loss: 4.094 | Reg loss: 0.037 | Tree loss: 4.094 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 018 | Total loss: 4.040 | Reg loss: 0.037 | Tree loss: 4.040 | Accuracy: 0.074534 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 87 | Batch: 000 / 018 | Total loss: 4.270 | Reg loss: 0.036 | Tree loss: 4.270 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 018 | Total loss: 4.242 | Reg loss: 0.036 | Tree loss: 4.242 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 018 | Total loss: 4.232 | Reg loss: 0.036 | Tree loss: 4.232 | Accuracy: 0.068359 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 003 / 018 | Total loss: 4.294 | Reg loss: 0.036 | Tree loss: 4.294 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 018 | Total loss: 4.205 | Reg loss: 0.036 | Tree loss: 4.205 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 018 | Total loss: 4.177 | Reg loss: 0.036 | Tree loss: 4.177 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 018 | Total loss: 4.235 | Reg loss: 0.036 | Tree loss: 4.235 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 018 | Total loss: 4.256 | Reg loss: 0.036 | Tree loss: 4.256 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 018 | Total loss: 4.224 | Reg loss: 0.036 | Tree loss: 4.224 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 018 | Total loss: 4.160 | Reg loss: 0.036 | Tree loss: 4.160 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 018 | Total loss: 4.127 | Reg loss: 0.036 | Tree loss: 4.127 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 018 | Total loss: 4.143 | Reg loss: 0.036 | Tree loss: 4.143 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 018 | Total loss: 4.141 | Reg loss: 0.037 | Tree loss: 4.141 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 018 | Total loss: 4.077 | Reg loss: 0.037 | Tree loss: 4.077 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 018 | Total loss: 4.111 | Reg loss: 0.037 | Tree loss: 4.111 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 018 | Total loss: 4.174 | Reg loss: 0.037 | Tree loss: 4.174 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 018 | Total loss: 4.155 | Reg loss: 0.037 | Tree loss: 4.155 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 018 | Total loss: 4.105 | Reg loss: 0.037 | Tree loss: 4.105 | Accuracy: 0.072464 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 88 | Batch: 000 / 018 | Total loss: 4.313 | Reg loss: 0.036 | Tree loss: 4.313 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 018 | Total loss: 4.292 | Reg loss: 0.036 | Tree loss: 4.292 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 018 | Total loss: 4.255 | Reg loss: 0.036 | Tree loss: 4.255 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 018 | Total loss: 4.279 | Reg loss: 0.036 | Tree loss: 4.279 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 018 | Total loss: 4.245 | Reg loss: 0.036 | Tree loss: 4.245 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 018 | Total loss: 4.190 | Reg loss: 0.036 | Tree loss: 4.190 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 018 | Total loss: 4.230 | Reg loss: 0.036 | Tree loss: 4.230 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 018 | Total loss: 4.244 | Reg loss: 0.036 | Tree loss: 4.244 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 018 | Total loss: 4.155 | Reg loss: 0.036 | Tree loss: 4.155 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 018 | Total loss: 4.221 | Reg loss: 0.036 | Tree loss: 4.221 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 018 | Total loss: 4.153 | Reg loss: 0.036 | Tree loss: 4.153 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 018 | Total loss: 4.142 | Reg loss: 0.037 | Tree loss: 4.142 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 018 | Total loss: 4.105 | Reg loss: 0.037 | Tree loss: 4.105 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 018 | Total loss: 4.107 | Reg loss: 0.037 | Tree loss: 4.107 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 018 | Total loss: 4.113 | Reg loss: 0.037 | Tree loss: 4.113 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 018 | Total loss: 4.104 | Reg loss: 0.037 | Tree loss: 4.104 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 018 | Total loss: 4.051 | Reg loss: 0.037 | Tree loss: 4.051 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 018 | Total loss: 4.077 | Reg loss: 0.037 | Tree loss: 4.077 | Accuracy: 0.084886 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 89 | Batch: 000 / 018 | Total loss: 4.285 | Reg loss: 0.036 | Tree loss: 4.285 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 018 | Total loss: 4.375 | Reg loss: 0.036 | Tree loss: 4.375 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 018 | Total loss: 4.249 | Reg loss: 0.036 | Tree loss: 4.249 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 018 | Total loss: 4.193 | Reg loss: 0.036 | Tree loss: 4.193 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 018 | Total loss: 4.226 | Reg loss: 0.036 | Tree loss: 4.226 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 018 | Total loss: 4.151 | Reg loss: 0.036 | Tree loss: 4.151 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 018 | Total loss: 4.243 | Reg loss: 0.036 | Tree loss: 4.243 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 018 | Total loss: 4.191 | Reg loss: 0.036 | Tree loss: 4.191 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 018 | Total loss: 4.198 | Reg loss: 0.036 | Tree loss: 4.198 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 018 | Total loss: 4.153 | Reg loss: 0.037 | Tree loss: 4.153 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 018 | Total loss: 4.194 | Reg loss: 0.037 | Tree loss: 4.194 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 018 | Total loss: 4.104 | Reg loss: 0.037 | Tree loss: 4.104 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 018 | Total loss: 4.138 | Reg loss: 0.037 | Tree loss: 4.138 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 018 | Total loss: 4.106 | Reg loss: 0.037 | Tree loss: 4.106 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 018 | Total loss: 4.134 | Reg loss: 0.037 | Tree loss: 4.134 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 018 | Total loss: 4.070 | Reg loss: 0.037 | Tree loss: 4.070 | Accuracy: 0.101562 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 018 | Total loss: 4.133 | Reg loss: 0.037 | Tree loss: 4.133 | Accuracy: 0.044922 | 0.231 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 018 | Total loss: 4.089 | Reg loss: 0.037 | Tree loss: 4.089 | Accuracy: 0.078675 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 90 | Batch: 000 / 018 | Total loss: 4.293 | Reg loss: 0.036 | Tree loss: 4.293 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 018 | Total loss: 4.280 | Reg loss: 0.036 | Tree loss: 4.280 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 018 | Total loss: 4.157 | Reg loss: 0.036 | Tree loss: 4.157 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 018 | Total loss: 4.261 | Reg loss: 0.036 | Tree loss: 4.261 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 018 | Total loss: 4.243 | Reg loss: 0.036 | Tree loss: 4.243 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 018 | Total loss: 4.245 | Reg loss: 0.036 | Tree loss: 4.245 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 018 | Total loss: 4.197 | Reg loss: 0.036 | Tree loss: 4.197 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 018 | Total loss: 4.294 | Reg loss: 0.036 | Tree loss: 4.294 | Accuracy: 0.050781 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 018 | Total loss: 4.162 | Reg loss: 0.037 | Tree loss: 4.162 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 018 | Total loss: 4.127 | Reg loss: 0.037 | Tree loss: 4.127 | Accuracy: 0.072266 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 010 / 018 | Total loss: 4.171 | Reg loss: 0.037 | Tree loss: 4.171 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 018 | Total loss: 4.160 | Reg loss: 0.037 | Tree loss: 4.160 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 018 | Total loss: 4.093 | Reg loss: 0.037 | Tree loss: 4.093 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 018 | Total loss: 4.119 | Reg loss: 0.037 | Tree loss: 4.119 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 018 | Total loss: 4.118 | Reg loss: 0.037 | Tree loss: 4.118 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 018 | Total loss: 4.140 | Reg loss: 0.037 | Tree loss: 4.140 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 018 | Total loss: 4.071 | Reg loss: 0.037 | Tree loss: 4.071 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 018 | Total loss: 4.052 | Reg loss: 0.037 | Tree loss: 4.052 | Accuracy: 0.078675 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 91 | Batch: 000 / 018 | Total loss: 4.271 | Reg loss: 0.036 | Tree loss: 4.271 | Accuracy: 0.091797 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 018 | Total loss: 4.204 | Reg loss: 0.036 | Tree loss: 4.204 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 018 | Total loss: 4.269 | Reg loss: 0.036 | Tree loss: 4.269 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 018 | Total loss: 4.280 | Reg loss: 0.036 | Tree loss: 4.280 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 018 | Total loss: 4.245 | Reg loss: 0.036 | Tree loss: 4.245 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 018 | Total loss: 4.180 | Reg loss: 0.036 | Tree loss: 4.180 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 018 | Total loss: 4.222 | Reg loss: 0.037 | Tree loss: 4.222 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 018 | Total loss: 4.262 | Reg loss: 0.037 | Tree loss: 4.262 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 018 | Total loss: 4.113 | Reg loss: 0.037 | Tree loss: 4.113 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 018 | Total loss: 4.164 | Reg loss: 0.037 | Tree loss: 4.164 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 018 | Total loss: 4.179 | Reg loss: 0.037 | Tree loss: 4.179 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 018 | Total loss: 4.110 | Reg loss: 0.037 | Tree loss: 4.110 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 018 | Total loss: 4.126 | Reg loss: 0.037 | Tree loss: 4.126 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 018 | Total loss: 4.143 | Reg loss: 0.037 | Tree loss: 4.143 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 018 | Total loss: 4.052 | Reg loss: 0.037 | Tree loss: 4.052 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 018 | Total loss: 4.110 | Reg loss: 0.037 | Tree loss: 4.110 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 018 | Total loss: 4.088 | Reg loss: 0.037 | Tree loss: 4.088 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 018 | Total loss: 4.127 | Reg loss: 0.037 | Tree loss: 4.127 | Accuracy: 0.070393 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 92 | Batch: 000 / 018 | Total loss: 4.273 | Reg loss: 0.036 | Tree loss: 4.273 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 018 | Total loss: 4.215 | Reg loss: 0.036 | Tree loss: 4.215 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 018 | Total loss: 4.236 | Reg loss: 0.036 | Tree loss: 4.236 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 018 | Total loss: 4.240 | Reg loss: 0.037 | Tree loss: 4.240 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 018 | Total loss: 4.212 | Reg loss: 0.037 | Tree loss: 4.212 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 018 | Total loss: 4.126 | Reg loss: 0.037 | Tree loss: 4.126 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 018 | Total loss: 4.201 | Reg loss: 0.037 | Tree loss: 4.201 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 018 | Total loss: 4.266 | Reg loss: 0.037 | Tree loss: 4.266 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 018 | Total loss: 4.197 | Reg loss: 0.037 | Tree loss: 4.197 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 018 | Total loss: 4.186 | Reg loss: 0.037 | Tree loss: 4.186 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 018 | Total loss: 4.201 | Reg loss: 0.037 | Tree loss: 4.201 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 018 | Total loss: 4.120 | Reg loss: 0.037 | Tree loss: 4.120 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 018 | Total loss: 4.129 | Reg loss: 0.037 | Tree loss: 4.129 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 018 | Total loss: 4.149 | Reg loss: 0.037 | Tree loss: 4.149 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 018 | Total loss: 4.112 | Reg loss: 0.037 | Tree loss: 4.112 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 018 | Total loss: 4.027 | Reg loss: 0.037 | Tree loss: 4.027 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 018 | Total loss: 4.118 | Reg loss: 0.037 | Tree loss: 4.118 | Accuracy: 0.046875 | 0.231 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 018 | Total loss: 4.104 | Reg loss: 0.037 | Tree loss: 4.104 | Accuracy: 0.064182 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 93 | Batch: 000 / 018 | Total loss: 4.386 | Reg loss: 0.037 | Tree loss: 4.386 | Accuracy: 0.048828 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 018 | Total loss: 4.336 | Reg loss: 0.037 | Tree loss: 4.336 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 018 | Total loss: 4.212 | Reg loss: 0.037 | Tree loss: 4.212 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 018 | Total loss: 4.240 | Reg loss: 0.037 | Tree loss: 4.240 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 018 | Total loss: 4.214 | Reg loss: 0.037 | Tree loss: 4.214 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 018 | Total loss: 4.219 | Reg loss: 0.037 | Tree loss: 4.219 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 018 | Total loss: 4.233 | Reg loss: 0.037 | Tree loss: 4.233 | Accuracy: 0.050781 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 018 | Total loss: 4.226 | Reg loss: 0.037 | Tree loss: 4.226 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 018 | Total loss: 4.186 | Reg loss: 0.037 | Tree loss: 4.186 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 018 | Total loss: 4.081 | Reg loss: 0.037 | Tree loss: 4.081 | Accuracy: 0.101562 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 018 | Total loss: 4.183 | Reg loss: 0.037 | Tree loss: 4.183 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 018 | Total loss: 4.232 | Reg loss: 0.037 | Tree loss: 4.232 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 018 | Total loss: 4.100 | Reg loss: 0.037 | Tree loss: 4.100 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 018 | Total loss: 4.061 | Reg loss: 0.037 | Tree loss: 4.061 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 018 | Total loss: 4.045 | Reg loss: 0.037 | Tree loss: 4.045 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 018 | Total loss: 4.039 | Reg loss: 0.037 | Tree loss: 4.039 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 018 | Total loss: 4.055 | Reg loss: 0.037 | Tree loss: 4.055 | Accuracy: 0.076172 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 017 / 018 | Total loss: 4.018 | Reg loss: 0.037 | Tree loss: 4.018 | Accuracy: 0.086957 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 94 | Batch: 000 / 018 | Total loss: 4.304 | Reg loss: 0.037 | Tree loss: 4.304 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 018 | Total loss: 4.269 | Reg loss: 0.037 | Tree loss: 4.269 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 018 | Total loss: 4.254 | Reg loss: 0.037 | Tree loss: 4.254 | Accuracy: 0.072266 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 018 | Total loss: 4.203 | Reg loss: 0.037 | Tree loss: 4.203 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 018 | Total loss: 4.224 | Reg loss: 0.037 | Tree loss: 4.224 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 018 | Total loss: 4.185 | Reg loss: 0.037 | Tree loss: 4.185 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 018 | Total loss: 4.115 | Reg loss: 0.037 | Tree loss: 4.115 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 018 | Total loss: 4.204 | Reg loss: 0.037 | Tree loss: 4.204 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 018 | Total loss: 4.226 | Reg loss: 0.037 | Tree loss: 4.226 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 018 | Total loss: 4.120 | Reg loss: 0.037 | Tree loss: 4.120 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 018 | Total loss: 4.184 | Reg loss: 0.037 | Tree loss: 4.184 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 018 | Total loss: 4.152 | Reg loss: 0.037 | Tree loss: 4.152 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 018 | Total loss: 4.109 | Reg loss: 0.037 | Tree loss: 4.109 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 018 | Total loss: 4.166 | Reg loss: 0.037 | Tree loss: 4.166 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 018 | Total loss: 4.099 | Reg loss: 0.037 | Tree loss: 4.099 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 018 | Total loss: 4.149 | Reg loss: 0.037 | Tree loss: 4.149 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 018 | Total loss: 4.058 | Reg loss: 0.037 | Tree loss: 4.058 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 018 | Total loss: 4.010 | Reg loss: 0.037 | Tree loss: 4.010 | Accuracy: 0.080745 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 95 | Batch: 000 / 018 | Total loss: 4.306 | Reg loss: 0.037 | Tree loss: 4.306 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 018 | Total loss: 4.267 | Reg loss: 0.037 | Tree loss: 4.267 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 018 | Total loss: 4.284 | Reg loss: 0.037 | Tree loss: 4.284 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 018 | Total loss: 4.263 | Reg loss: 0.037 | Tree loss: 4.263 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 018 | Total loss: 4.212 | Reg loss: 0.037 | Tree loss: 4.212 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 018 | Total loss: 4.275 | Reg loss: 0.037 | Tree loss: 4.275 | Accuracy: 0.050781 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 018 | Total loss: 4.135 | Reg loss: 0.037 | Tree loss: 4.135 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 018 | Total loss: 4.139 | Reg loss: 0.037 | Tree loss: 4.139 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 018 | Total loss: 4.117 | Reg loss: 0.037 | Tree loss: 4.117 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 018 | Total loss: 4.166 | Reg loss: 0.037 | Tree loss: 4.166 | Accuracy: 0.042969 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 018 | Total loss: 4.097 | Reg loss: 0.037 | Tree loss: 4.097 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 018 | Total loss: 4.128 | Reg loss: 0.037 | Tree loss: 4.128 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 018 | Total loss: 4.065 | Reg loss: 0.037 | Tree loss: 4.065 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 018 | Total loss: 4.192 | Reg loss: 0.037 | Tree loss: 4.192 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 018 | Total loss: 4.090 | Reg loss: 0.037 | Tree loss: 4.090 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 018 | Total loss: 4.082 | Reg loss: 0.037 | Tree loss: 4.082 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 018 | Total loss: 4.140 | Reg loss: 0.037 | Tree loss: 4.140 | Accuracy: 0.082031 | 0.231 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 018 | Total loss: 4.049 | Reg loss: 0.037 | Tree loss: 4.049 | Accuracy: 0.074534 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 96 | Batch: 000 / 018 | Total loss: 4.311 | Reg loss: 0.037 | Tree loss: 4.311 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 018 | Total loss: 4.248 | Reg loss: 0.037 | Tree loss: 4.248 | Accuracy: 0.089844 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 018 | Total loss: 4.231 | Reg loss: 0.037 | Tree loss: 4.231 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 018 | Total loss: 4.214 | Reg loss: 0.037 | Tree loss: 4.214 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 018 | Total loss: 4.161 | Reg loss: 0.037 | Tree loss: 4.161 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 018 | Total loss: 4.141 | Reg loss: 0.037 | Tree loss: 4.141 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 018 | Total loss: 4.230 | Reg loss: 0.037 | Tree loss: 4.230 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 018 | Total loss: 4.139 | Reg loss: 0.037 | Tree loss: 4.139 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 018 | Total loss: 4.150 | Reg loss: 0.037 | Tree loss: 4.150 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 018 | Total loss: 4.233 | Reg loss: 0.037 | Tree loss: 4.233 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 018 | Total loss: 4.129 | Reg loss: 0.037 | Tree loss: 4.129 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 018 | Total loss: 4.100 | Reg loss: 0.037 | Tree loss: 4.100 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 018 | Total loss: 4.135 | Reg loss: 0.037 | Tree loss: 4.135 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 018 | Total loss: 4.152 | Reg loss: 0.037 | Tree loss: 4.152 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 018 | Total loss: 4.065 | Reg loss: 0.037 | Tree loss: 4.065 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 018 | Total loss: 4.115 | Reg loss: 0.037 | Tree loss: 4.115 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 018 | Total loss: 4.143 | Reg loss: 0.037 | Tree loss: 4.143 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 018 | Total loss: 4.087 | Reg loss: 0.037 | Tree loss: 4.087 | Accuracy: 0.051760 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 97 | Batch: 000 / 018 | Total loss: 4.271 | Reg loss: 0.037 | Tree loss: 4.271 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 018 | Total loss: 4.270 | Reg loss: 0.037 | Tree loss: 4.270 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 018 | Total loss: 4.218 | Reg loss: 0.037 | Tree loss: 4.218 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 018 | Total loss: 4.235 | Reg loss: 0.037 | Tree loss: 4.235 | Accuracy: 0.064453 | 0.231 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 004 / 018 | Total loss: 4.211 | Reg loss: 0.037 | Tree loss: 4.211 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 018 | Total loss: 4.241 | Reg loss: 0.037 | Tree loss: 4.241 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 018 | Total loss: 4.253 | Reg loss: 0.037 | Tree loss: 4.253 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 018 | Total loss: 4.167 | Reg loss: 0.037 | Tree loss: 4.167 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 018 | Total loss: 4.138 | Reg loss: 0.037 | Tree loss: 4.138 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 018 | Total loss: 4.107 | Reg loss: 0.037 | Tree loss: 4.107 | Accuracy: 0.087891 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 018 | Total loss: 4.153 | Reg loss: 0.037 | Tree loss: 4.153 | Accuracy: 0.068359 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 018 | Total loss: 4.116 | Reg loss: 0.037 | Tree loss: 4.116 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 018 | Total loss: 4.128 | Reg loss: 0.037 | Tree loss: 4.128 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 018 | Total loss: 4.129 | Reg loss: 0.037 | Tree loss: 4.129 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 018 | Total loss: 4.124 | Reg loss: 0.037 | Tree loss: 4.124 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 018 | Total loss: 4.131 | Reg loss: 0.037 | Tree loss: 4.131 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 018 | Total loss: 4.013 | Reg loss: 0.037 | Tree loss: 4.013 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 018 | Total loss: 4.046 | Reg loss: 0.037 | Tree loss: 4.046 | Accuracy: 0.064182 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 98 | Batch: 000 / 018 | Total loss: 4.230 | Reg loss: 0.037 | Tree loss: 4.230 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 018 | Total loss: 4.242 | Reg loss: 0.037 | Tree loss: 4.242 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 018 | Total loss: 4.222 | Reg loss: 0.037 | Tree loss: 4.222 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 018 | Total loss: 4.331 | Reg loss: 0.037 | Tree loss: 4.331 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 018 | Total loss: 4.197 | Reg loss: 0.037 | Tree loss: 4.197 | Accuracy: 0.093750 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 018 | Total loss: 4.219 | Reg loss: 0.037 | Tree loss: 4.219 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 018 | Total loss: 4.255 | Reg loss: 0.037 | Tree loss: 4.255 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 018 | Total loss: 4.190 | Reg loss: 0.037 | Tree loss: 4.190 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 018 | Total loss: 4.079 | Reg loss: 0.037 | Tree loss: 4.079 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 018 | Total loss: 4.073 | Reg loss: 0.037 | Tree loss: 4.073 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 018 | Total loss: 4.128 | Reg loss: 0.037 | Tree loss: 4.128 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 018 | Total loss: 4.175 | Reg loss: 0.037 | Tree loss: 4.175 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 018 | Total loss: 4.123 | Reg loss: 0.037 | Tree loss: 4.123 | Accuracy: 0.054688 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 018 | Total loss: 4.128 | Reg loss: 0.037 | Tree loss: 4.128 | Accuracy: 0.070312 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 018 | Total loss: 4.095 | Reg loss: 0.037 | Tree loss: 4.095 | Accuracy: 0.080078 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 018 | Total loss: 4.104 | Reg loss: 0.037 | Tree loss: 4.104 | Accuracy: 0.058594 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 018 | Total loss: 4.069 | Reg loss: 0.037 | Tree loss: 4.069 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 018 | Total loss: 4.060 | Reg loss: 0.037 | Tree loss: 4.060 | Accuracy: 0.082816 | 0.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 99 | Batch: 000 / 018 | Total loss: 4.242 | Reg loss: 0.037 | Tree loss: 4.242 | Accuracy: 0.062500 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 018 | Total loss: 4.273 | Reg loss: 0.037 | Tree loss: 4.273 | Accuracy: 0.066406 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 018 | Total loss: 4.225 | Reg loss: 0.037 | Tree loss: 4.225 | Accuracy: 0.078125 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 018 | Total loss: 4.240 | Reg loss: 0.037 | Tree loss: 4.240 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 018 | Total loss: 4.177 | Reg loss: 0.037 | Tree loss: 4.177 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 018 | Total loss: 4.219 | Reg loss: 0.037 | Tree loss: 4.219 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 018 | Total loss: 4.168 | Reg loss: 0.037 | Tree loss: 4.168 | Accuracy: 0.074219 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 018 | Total loss: 4.150 | Reg loss: 0.037 | Tree loss: 4.150 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 018 | Total loss: 4.144 | Reg loss: 0.037 | Tree loss: 4.144 | Accuracy: 0.056641 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 018 | Total loss: 4.099 | Reg loss: 0.037 | Tree loss: 4.099 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 018 | Total loss: 4.190 | Reg loss: 0.037 | Tree loss: 4.190 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 018 | Total loss: 4.166 | Reg loss: 0.037 | Tree loss: 4.166 | Accuracy: 0.060547 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 018 | Total loss: 4.136 | Reg loss: 0.037 | Tree loss: 4.136 | Accuracy: 0.064453 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 018 | Total loss: 4.106 | Reg loss: 0.037 | Tree loss: 4.106 | Accuracy: 0.083984 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 018 | Total loss: 4.147 | Reg loss: 0.037 | Tree loss: 4.147 | Accuracy: 0.052734 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 018 | Total loss: 4.127 | Reg loss: 0.037 | Tree loss: 4.127 | Accuracy: 0.076172 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 018 | Total loss: 4.047 | Reg loss: 0.037 | Tree loss: 4.047 | Accuracy: 0.085938 | 0.231 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 018 | Total loss: 4.039 | Reg loss: 0.037 | Tree loss: 4.039 | Accuracy: 0.066253 | 0.231 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b58740a5fe84bbf901dd8554cea3efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092fab001dfb4b329c40347507b6df83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c2d3d8623941278cecd64b37174f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5891b694083545809ec1fdb7d0dd2822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.996078431372549\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 255\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "6307\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "1696\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "Average comprehensibility: 36.58039215686274\n",
      "std comprehensibility: 1.6355151023865735\n",
      "var comprehensibility: 2.674909650134564\n",
      "minimum comprehensibility: 32\n",
      "maximum comprehensibility: 42\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "#     print(conds)\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
