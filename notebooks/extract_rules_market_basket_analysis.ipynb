{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=r\"C:\\Users\\eitan\\PycharmProjects\\EntangledExplainableClustering\\data\\Groceries_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.115936279296875 | KNN Loss: 6.224985599517822 | BCE Loss: 1.8909504413604736\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.152900695800781 | KNN Loss: 6.225427150726318 | BCE Loss: 1.927473783493042\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.18317985534668 | KNN Loss: 6.225057125091553 | BCE Loss: 1.9581222534179688\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.14844799041748 | KNN Loss: 6.224520683288574 | BCE Loss: 1.9239273071289062\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.107247352600098 | KNN Loss: 6.224483489990234 | BCE Loss: 1.8827638626098633\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.133842468261719 | KNN Loss: 6.224823474884033 | BCE Loss: 1.9090194702148438\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.103509902954102 | KNN Loss: 6.223139762878418 | BCE Loss: 1.880369782447815\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.12332534790039 | KNN Loss: 6.223299980163574 | BCE Loss: 1.9000256061553955\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.118815422058105 | KNN Loss: 6.222807884216309 | BCE Loss: 1.8960076570510864\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.111641883850098 | KNN Loss: 6.2223639488220215 | BCE Loss: 1.8892781734466553\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.156816482543945 | KNN Loss: 6.221984386444092 | BCE Loss: 1.9348316192626953\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.086442947387695 | KNN Loss: 6.221210479736328 | BCE Loss: 1.8652324676513672\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.082956314086914 | KNN Loss: 6.220312595367432 | BCE Loss: 1.8626432418823242\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.068056106567383 | KNN Loss: 6.219665050506592 | BCE Loss: 1.848390817642212\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.066285133361816 | KNN Loss: 6.219250202178955 | BCE Loss: 1.8470349311828613\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.010263442993164 | KNN Loss: 6.219445705413818 | BCE Loss: 1.7908178567886353\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.051345825195312 | KNN Loss: 6.218111515045166 | BCE Loss: 1.8332340717315674\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.019478797912598 | KNN Loss: 6.217012882232666 | BCE Loss: 1.802465558052063\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.045452117919922 | KNN Loss: 6.21455192565918 | BCE Loss: 1.8308998346328735\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 7.980456352233887 | KNN Loss: 6.213927268981934 | BCE Loss: 1.7665289640426636\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 7.964757442474365 | KNN Loss: 6.213200092315674 | BCE Loss: 1.7515573501586914\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 7.996463775634766 | KNN Loss: 6.213253974914551 | BCE Loss: 1.783210039138794\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.965793609619141 | KNN Loss: 6.210472106933594 | BCE Loss: 1.7553215026855469\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.977207183837891 | KNN Loss: 6.2106218338012695 | BCE Loss: 1.766585350036621\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.938922882080078 | KNN Loss: 6.207284927368164 | BCE Loss: 1.7316381931304932\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.955955505371094 | KNN Loss: 6.209962844848633 | BCE Loss: 1.7459925413131714\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.904912948608398 | KNN Loss: 6.204305648803711 | BCE Loss: 1.7006075382232666\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.926054000854492 | KNN Loss: 6.201968193054199 | BCE Loss: 1.7240855693817139\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.908480644226074 | KNN Loss: 6.198767185211182 | BCE Loss: 1.709713339805603\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.913335800170898 | KNN Loss: 6.194787502288818 | BCE Loss: 1.7185484170913696\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.838774681091309 | KNN Loss: 6.191043376922607 | BCE Loss: 1.6477315425872803\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.838516712188721 | KNN Loss: 6.192523956298828 | BCE Loss: 1.6459927558898926\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.837007522583008 | KNN Loss: 6.186947822570801 | BCE Loss: 1.6500599384307861\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.8260064125061035 | KNN Loss: 6.182964324951172 | BCE Loss: 1.643041968345642\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.757015228271484 | KNN Loss: 6.173817157745361 | BCE Loss: 1.583198070526123\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.728144645690918 | KNN Loss: 6.1664347648620605 | BCE Loss: 1.5617096424102783\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.688027858734131 | KNN Loss: 6.163174629211426 | BCE Loss: 1.524853229522705\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.695457458496094 | KNN Loss: 6.159769535064697 | BCE Loss: 1.5356876850128174\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.672951698303223 | KNN Loss: 6.141851902008057 | BCE Loss: 1.5310996770858765\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.629746437072754 | KNN Loss: 6.129019737243652 | BCE Loss: 1.5007268190383911\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.61430549621582 | KNN Loss: 6.122003078460693 | BCE Loss: 1.4923022985458374\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.550785541534424 | KNN Loss: 6.109372138977051 | BCE Loss: 1.441413402557373\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.527536392211914 | KNN Loss: 6.091037750244141 | BCE Loss: 1.4364985227584839\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.466531753540039 | KNN Loss: 6.065274238586426 | BCE Loss: 1.4012577533721924\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.425432205200195 | KNN Loss: 6.039851188659668 | BCE Loss: 1.3855810165405273\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.322551727294922 | KNN Loss: 6.002011299133301 | BCE Loss: 1.320540189743042\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.298880577087402 | KNN Loss: 5.955647945404053 | BCE Loss: 1.3432328701019287\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.224209308624268 | KNN Loss: 5.933961868286133 | BCE Loss: 1.2902475595474243\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.177003383636475 | KNN Loss: 5.892027378082275 | BCE Loss: 1.2849758863449097\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.1095991134643555 | KNN Loss: 5.846946716308594 | BCE Loss: 1.2626525163650513\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.013984203338623 | KNN Loss: 5.798239231109619 | BCE Loss: 1.2157450914382935\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 6.9286088943481445 | KNN Loss: 5.726679801940918 | BCE Loss: 1.2019288539886475\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 6.827755451202393 | KNN Loss: 5.664098739624023 | BCE Loss: 1.1636567115783691\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 6.748506546020508 | KNN Loss: 5.598303318023682 | BCE Loss: 1.150202989578247\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 6.681797981262207 | KNN Loss: 5.5498738288879395 | BCE Loss: 1.1319241523742676\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 6.601619243621826 | KNN Loss: 5.449265003204346 | BCE Loss: 1.15235435962677\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.48692512512207 | KNN Loss: 5.367168426513672 | BCE Loss: 1.1197566986083984\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.393436431884766 | KNN Loss: 5.273923873901367 | BCE Loss: 1.1195127964019775\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.317251205444336 | KNN Loss: 5.187800407409668 | BCE Loss: 1.1294509172439575\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.205546855926514 | KNN Loss: 5.109135627746582 | BCE Loss: 1.096411108970642\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.1283793449401855 | KNN Loss: 5.010702133178711 | BCE Loss: 1.1176772117614746\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.052799224853516 | KNN Loss: 4.933817386627197 | BCE Loss: 1.1189818382263184\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 5.958436489105225 | KNN Loss: 4.863497734069824 | BCE Loss: 1.09493887424469\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 5.85948371887207 | KNN Loss: 4.766700744628906 | BCE Loss: 1.092782735824585\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 5.774790287017822 | KNN Loss: 4.66457986831665 | BCE Loss: 1.1102102994918823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.691650390625 | KNN Loss: 4.585964202880859 | BCE Loss: 1.1056861877441406\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.657222747802734 | KNN Loss: 4.540197849273682 | BCE Loss: 1.1170251369476318\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.559140682220459 | KNN Loss: 4.475207328796387 | BCE Loss: 1.0839334726333618\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.476250648498535 | KNN Loss: 4.389369010925293 | BCE Loss: 1.0868817567825317\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.420239448547363 | KNN Loss: 4.333858013153076 | BCE Loss: 1.0863815546035767\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.372244834899902 | KNN Loss: 4.272647857666016 | BCE Loss: 1.0995970964431763\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.269759178161621 | KNN Loss: 4.20658540725708 | BCE Loss: 1.0631736516952515\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.299194812774658 | KNN Loss: 4.192719459533691 | BCE Loss: 1.1064753532409668\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.197336196899414 | KNN Loss: 4.121513843536377 | BCE Loss: 1.075822353363037\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.186418056488037 | KNN Loss: 4.098254680633545 | BCE Loss: 1.0881634950637817\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.137148380279541 | KNN Loss: 4.055042743682861 | BCE Loss: 1.0821055173873901\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.105648040771484 | KNN Loss: 4.054168224334717 | BCE Loss: 1.0514800548553467\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.0370941162109375 | KNN Loss: 3.973562240600586 | BCE Loss: 1.0635318756103516\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.032104015350342 | KNN Loss: 3.979001760482788 | BCE Loss: 1.0531023740768433\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.069199562072754 | KNN Loss: 3.9779815673828125 | BCE Loss: 1.0912179946899414\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.0399298667907715 | KNN Loss: 3.985790967941284 | BCE Loss: 1.0541387796401978\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.058533191680908 | KNN Loss: 3.958552598953247 | BCE Loss: 1.0999807119369507\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.007971286773682 | KNN Loss: 3.93630313873291 | BCE Loss: 1.071668028831482\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.963761329650879 | KNN Loss: 3.9115548133850098 | BCE Loss: 1.0522065162658691\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.9664411544799805 | KNN Loss: 3.902418375015259 | BCE Loss: 1.0640225410461426\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.9596147537231445 | KNN Loss: 3.8994405269622803 | BCE Loss: 1.0601741075515747\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.939509868621826 | KNN Loss: 3.897366762161255 | BCE Loss: 1.0421429872512817\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.978942394256592 | KNN Loss: 3.9193406105041504 | BCE Loss: 1.0596017837524414\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.910637855529785 | KNN Loss: 3.8690826892852783 | BCE Loss: 1.0415551662445068\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.968855381011963 | KNN Loss: 3.8886570930480957 | BCE Loss: 1.0801982879638672\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.0116963386535645 | KNN Loss: 3.9303267002105713 | BCE Loss: 1.0813696384429932\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.999906539916992 | KNN Loss: 3.9279396533966064 | BCE Loss: 1.0719667673110962\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.919960021972656 | KNN Loss: 3.8638694286346436 | BCE Loss: 1.0560903549194336\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.954962253570557 | KNN Loss: 3.8755574226379395 | BCE Loss: 1.0794049501419067\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.939044952392578 | KNN Loss: 3.8650870323181152 | BCE Loss: 1.073958158493042\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.9181904792785645 | KNN Loss: 3.863250970840454 | BCE Loss: 1.0549396276474\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.918655872344971 | KNN Loss: 3.877490282058716 | BCE Loss: 1.0411654710769653\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.894773960113525 | KNN Loss: 3.8419418334960938 | BCE Loss: 1.052832007408142\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.894496917724609 | KNN Loss: 3.8360562324523926 | BCE Loss: 1.0584405660629272\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.929489612579346 | KNN Loss: 3.881413459777832 | BCE Loss: 1.0480761528015137\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.89919376373291 | KNN Loss: 3.8579962253570557 | BCE Loss: 1.0411977767944336\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.8649678230285645 | KNN Loss: 3.794537305831909 | BCE Loss: 1.0704305171966553\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.930271148681641 | KNN Loss: 3.8718416690826416 | BCE Loss: 1.0584293603897095\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.919505596160889 | KNN Loss: 3.8473074436187744 | BCE Loss: 1.0721980333328247\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.880075454711914 | KNN Loss: 3.844961643218994 | BCE Loss: 1.0351136922836304\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.9206862449646 | KNN Loss: 3.859891653060913 | BCE Loss: 1.060794472694397\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.925500392913818 | KNN Loss: 3.853691577911377 | BCE Loss: 1.0718086957931519\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.880335807800293 | KNN Loss: 3.8328635692596436 | BCE Loss: 1.047472357749939\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.979155540466309 | KNN Loss: 3.8956475257873535 | BCE Loss: 1.0835078954696655\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 5.019376277923584 | KNN Loss: 3.933745861053467 | BCE Loss: 1.0856304168701172\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.917842864990234 | KNN Loss: 3.859320878982544 | BCE Loss: 1.05852210521698\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.896018028259277 | KNN Loss: 3.8561782836914062 | BCE Loss: 1.0398396253585815\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.891861438751221 | KNN Loss: 3.842099666595459 | BCE Loss: 1.0497617721557617\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.871280193328857 | KNN Loss: 3.823935031890869 | BCE Loss: 1.0473451614379883\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.882452487945557 | KNN Loss: 3.8156421184539795 | BCE Loss: 1.0668104887008667\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.920417308807373 | KNN Loss: 3.8762919902801514 | BCE Loss: 1.0441253185272217\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.887322425842285 | KNN Loss: 3.842405319213867 | BCE Loss: 1.0449172258377075\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.869805335998535 | KNN Loss: 3.8122024536132812 | BCE Loss: 1.0576027631759644\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.8972907066345215 | KNN Loss: 3.840648889541626 | BCE Loss: 1.0566418170928955\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.846352577209473 | KNN Loss: 3.7975871562957764 | BCE Loss: 1.0487651824951172\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.910853385925293 | KNN Loss: 3.860513210296631 | BCE Loss: 1.0503400564193726\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.897679328918457 | KNN Loss: 3.859224557876587 | BCE Loss: 1.0384548902511597\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.882957458496094 | KNN Loss: 3.8237340450286865 | BCE Loss: 1.0592231750488281\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.884946823120117 | KNN Loss: 3.826937437057495 | BCE Loss: 1.0580096244812012\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.87785530090332 | KNN Loss: 3.834595203399658 | BCE Loss: 1.0432603359222412\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.833442211151123 | KNN Loss: 3.804460287094116 | BCE Loss: 1.0289819240570068\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.896710395812988 | KNN Loss: 3.8376948833465576 | BCE Loss: 1.0590155124664307\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.846894264221191 | KNN Loss: 3.8111536502838135 | BCE Loss: 1.035740852355957\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.891094207763672 | KNN Loss: 3.8538568019866943 | BCE Loss: 1.0372376441955566\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.835287094116211 | KNN Loss: 3.7933340072631836 | BCE Loss: 1.0419530868530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.917761325836182 | KNN Loss: 3.854158878326416 | BCE Loss: 1.0636024475097656\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.8506550788879395 | KNN Loss: 3.80037522315979 | BCE Loss: 1.0502798557281494\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.865542888641357 | KNN Loss: 3.8176093101501465 | BCE Loss: 1.0479336977005005\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.878241539001465 | KNN Loss: 3.829946279525757 | BCE Loss: 1.0482953786849976\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.888167858123779 | KNN Loss: 3.831289768218994 | BCE Loss: 1.0568782091140747\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.8718719482421875 | KNN Loss: 3.817835569381714 | BCE Loss: 1.0540366172790527\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.849645614624023 | KNN Loss: 3.808359146118164 | BCE Loss: 1.0412867069244385\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.8606719970703125 | KNN Loss: 3.8155460357666016 | BCE Loss: 1.0451260805130005\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.8504462242126465 | KNN Loss: 3.8201894760131836 | BCE Loss: 1.030256748199463\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.858545780181885 | KNN Loss: 3.8253276348114014 | BCE Loss: 1.0332180261611938\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.870031356811523 | KNN Loss: 3.819659471511841 | BCE Loss: 1.0503718852996826\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.8228759765625 | KNN Loss: 3.787081003189087 | BCE Loss: 1.035794734954834\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.895461082458496 | KNN Loss: 3.8241779804229736 | BCE Loss: 1.071283221244812\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.870235443115234 | KNN Loss: 3.8074209690093994 | BCE Loss: 1.0628142356872559\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.844300270080566 | KNN Loss: 3.8035178184509277 | BCE Loss: 1.0407822132110596\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.842310905456543 | KNN Loss: 3.803661346435547 | BCE Loss: 1.038649320602417\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.893364429473877 | KNN Loss: 3.825517177581787 | BCE Loss: 1.0678472518920898\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.82956075668335 | KNN Loss: 3.773531913757324 | BCE Loss: 1.056028962135315\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.874772548675537 | KNN Loss: 3.846355438232422 | BCE Loss: 1.0284172296524048\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.83728551864624 | KNN Loss: 3.8049356937408447 | BCE Loss: 1.032349705696106\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.7768707275390625 | KNN Loss: 3.7529478073120117 | BCE Loss: 1.0239231586456299\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.888867378234863 | KNN Loss: 3.8172004222869873 | BCE Loss: 1.0716670751571655\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.849733352661133 | KNN Loss: 3.806755542755127 | BCE Loss: 1.0429778099060059\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.905518531799316 | KNN Loss: 3.833355665206909 | BCE Loss: 1.0721631050109863\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.84365177154541 | KNN Loss: 3.8097035884857178 | BCE Loss: 1.0339479446411133\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.8450188636779785 | KNN Loss: 3.803062677383423 | BCE Loss: 1.0419563055038452\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.830404281616211 | KNN Loss: 3.7905826568603516 | BCE Loss: 1.0398218631744385\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.833523273468018 | KNN Loss: 3.780932903289795 | BCE Loss: 1.0525903701782227\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.854964733123779 | KNN Loss: 3.810704469680786 | BCE Loss: 1.0442603826522827\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.864604949951172 | KNN Loss: 3.82051420211792 | BCE Loss: 1.044090747833252\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.838624954223633 | KNN Loss: 3.7950563430786133 | BCE Loss: 1.043568730354309\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.820056915283203 | KNN Loss: 3.782837390899658 | BCE Loss: 1.0372196435928345\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.846331596374512 | KNN Loss: 3.8116824626922607 | BCE Loss: 1.03464937210083\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.927326679229736 | KNN Loss: 3.832906723022461 | BCE Loss: 1.094420075416565\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.8046488761901855 | KNN Loss: 3.7733733654022217 | BCE Loss: 1.0312756299972534\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.8853020668029785 | KNN Loss: 3.8179209232330322 | BCE Loss: 1.0673811435699463\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.862087249755859 | KNN Loss: 3.8092453479766846 | BCE Loss: 1.0528420209884644\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.809197425842285 | KNN Loss: 3.7557544708251953 | BCE Loss: 1.0534427165985107\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.851742744445801 | KNN Loss: 3.7884764671325684 | BCE Loss: 1.0632662773132324\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.84782600402832 | KNN Loss: 3.813204765319824 | BCE Loss: 1.034621238708496\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.851468086242676 | KNN Loss: 3.792882204055786 | BCE Loss: 1.0585860013961792\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.8417558670043945 | KNN Loss: 3.786832332611084 | BCE Loss: 1.0549236536026\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.800337314605713 | KNN Loss: 3.7711095809936523 | BCE Loss: 1.029227614402771\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.873685836791992 | KNN Loss: 3.7984938621520996 | BCE Loss: 1.0751922130584717\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.873219966888428 | KNN Loss: 3.8198680877685547 | BCE Loss: 1.0533517599105835\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.836216926574707 | KNN Loss: 3.766035318374634 | BCE Loss: 1.0701813697814941\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.784998416900635 | KNN Loss: 3.7662031650543213 | BCE Loss: 1.018795371055603\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.833704948425293 | KNN Loss: 3.7748703956604004 | BCE Loss: 1.0588343143463135\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.888734817504883 | KNN Loss: 3.839708089828491 | BCE Loss: 1.0490269660949707\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.81178092956543 | KNN Loss: 3.793562173843384 | BCE Loss: 1.0182185173034668\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.857364654541016 | KNN Loss: 3.810473680496216 | BCE Loss: 1.0468909740447998\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.821753025054932 | KNN Loss: 3.7668986320495605 | BCE Loss: 1.054854393005371\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.820087432861328 | KNN Loss: 3.7609310150146484 | BCE Loss: 1.0591566562652588\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.819520950317383 | KNN Loss: 3.793459415435791 | BCE Loss: 1.0260616540908813\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.8339433670043945 | KNN Loss: 3.7936346530914307 | BCE Loss: 1.0403088331222534\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.831717014312744 | KNN Loss: 3.7901718616485596 | BCE Loss: 1.0415451526641846\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.857956886291504 | KNN Loss: 3.8216938972473145 | BCE Loss: 1.0362632274627686\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.834714412689209 | KNN Loss: 3.78369140625 | BCE Loss: 1.0510228872299194\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.8426642417907715 | KNN Loss: 3.7963640689849854 | BCE Loss: 1.0463000535964966\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.816743850708008 | KNN Loss: 3.7634823322296143 | BCE Loss: 1.0532617568969727\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.816437244415283 | KNN Loss: 3.779479742050171 | BCE Loss: 1.0369573831558228\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.8125410079956055 | KNN Loss: 3.7664730548858643 | BCE Loss: 1.046067714691162\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.8716912269592285 | KNN Loss: 3.817727565765381 | BCE Loss: 1.053963541984558\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.790469169616699 | KNN Loss: 3.761789560317993 | BCE Loss: 1.0286797285079956\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.8181471824646 | KNN Loss: 3.770289421081543 | BCE Loss: 1.0478578805923462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.803418159484863 | KNN Loss: 3.7790985107421875 | BCE Loss: 1.0243196487426758\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.8621320724487305 | KNN Loss: 3.814321994781494 | BCE Loss: 1.0478098392486572\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.788928031921387 | KNN Loss: 3.738187551498413 | BCE Loss: 1.0507407188415527\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.772955894470215 | KNN Loss: 3.7529213428497314 | BCE Loss: 1.020034670829773\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.833101749420166 | KNN Loss: 3.7818121910095215 | BCE Loss: 1.051289439201355\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.817929267883301 | KNN Loss: 3.795948028564453 | BCE Loss: 1.0219810009002686\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.77934455871582 | KNN Loss: 3.7568111419677734 | BCE Loss: 1.0225335359573364\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.878941535949707 | KNN Loss: 3.80889630317688 | BCE Loss: 1.0700452327728271\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.8171844482421875 | KNN Loss: 3.783874034881592 | BCE Loss: 1.0333106517791748\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.8375773429870605 | KNN Loss: 3.790834426879883 | BCE Loss: 1.0467430353164673\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.8629913330078125 | KNN Loss: 3.8013434410095215 | BCE Loss: 1.0616481304168701\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.8390913009643555 | KNN Loss: 3.786355972290039 | BCE Loss: 1.0527350902557373\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.858208656311035 | KNN Loss: 3.799520969390869 | BCE Loss: 1.0586879253387451\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.8507981300354 | KNN Loss: 3.7613837718963623 | BCE Loss: 1.0894144773483276\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.802165985107422 | KNN Loss: 3.769000291824341 | BCE Loss: 1.033165454864502\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.819235801696777 | KNN Loss: 3.7606663703918457 | BCE Loss: 1.0585691928863525\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.807150363922119 | KNN Loss: 3.7745542526245117 | BCE Loss: 1.0325959920883179\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.805821418762207 | KNN Loss: 3.759979248046875 | BCE Loss: 1.0458424091339111\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.81585693359375 | KNN Loss: 3.756901979446411 | BCE Loss: 1.0589547157287598\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.838007926940918 | KNN Loss: 3.7887465953826904 | BCE Loss: 1.0492610931396484\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.789420127868652 | KNN Loss: 3.78334903717041 | BCE Loss: 1.006070852279663\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.879385948181152 | KNN Loss: 3.794644832611084 | BCE Loss: 1.0847413539886475\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.8529863357543945 | KNN Loss: 3.779310703277588 | BCE Loss: 1.0736753940582275\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.8357343673706055 | KNN Loss: 3.8122398853302 | BCE Loss: 1.0234946012496948\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.774176597595215 | KNN Loss: 3.7722325325012207 | BCE Loss: 1.001943826675415\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.821576118469238 | KNN Loss: 3.7680704593658447 | BCE Loss: 1.0535056591033936\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.794416427612305 | KNN Loss: 3.7629876136779785 | BCE Loss: 1.0314289331436157\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.784060955047607 | KNN Loss: 3.747023820877075 | BCE Loss: 1.0370371341705322\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.772485733032227 | KNN Loss: 3.738511562347412 | BCE Loss: 1.033974289894104\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.776801109313965 | KNN Loss: 3.7426204681396484 | BCE Loss: 1.0341804027557373\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.818538188934326 | KNN Loss: 3.7822275161743164 | BCE Loss: 1.0363105535507202\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.81346321105957 | KNN Loss: 3.7738053798675537 | BCE Loss: 1.0396580696105957\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.821755409240723 | KNN Loss: 3.787240743637085 | BCE Loss: 1.0345149040222168\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.7870283126831055 | KNN Loss: 3.7596993446350098 | BCE Loss: 1.0273288488388062\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.776311874389648 | KNN Loss: 3.7593979835510254 | BCE Loss: 1.0169137716293335\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.793506145477295 | KNN Loss: 3.7634501457214355 | BCE Loss: 1.0300558805465698\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.795870780944824 | KNN Loss: 3.754180431365967 | BCE Loss: 1.0416905879974365\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.803170204162598 | KNN Loss: 3.727914333343506 | BCE Loss: 1.0752556324005127\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.792601585388184 | KNN Loss: 3.760768175125122 | BCE Loss: 1.0318331718444824\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.780154228210449 | KNN Loss: 3.741149425506592 | BCE Loss: 1.0390050411224365\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.837843894958496 | KNN Loss: 3.8188226222991943 | BCE Loss: 1.0190210342407227\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.789028167724609 | KNN Loss: 3.7721164226531982 | BCE Loss: 1.0169117450714111\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.762228965759277 | KNN Loss: 3.7477645874023438 | BCE Loss: 1.0144644975662231\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.800999164581299 | KNN Loss: 3.7684578895568848 | BCE Loss: 1.032541275024414\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.840929985046387 | KNN Loss: 3.7839250564575195 | BCE Loss: 1.0570050477981567\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.786861896514893 | KNN Loss: 3.7454278469085693 | BCE Loss: 1.0414341688156128\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.7666144371032715 | KNN Loss: 3.76009464263916 | BCE Loss: 1.0065197944641113\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.789043426513672 | KNN Loss: 3.774331569671631 | BCE Loss: 1.0147119760513306\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.80946683883667 | KNN Loss: 3.764765501022339 | BCE Loss: 1.044701337814331\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.820381164550781 | KNN Loss: 3.764427900314331 | BCE Loss: 1.0559533834457397\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.7937493324279785 | KNN Loss: 3.7321178913116455 | BCE Loss: 1.0616313219070435\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.760385036468506 | KNN Loss: 3.74023175239563 | BCE Loss: 1.020153284072876\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.775339126586914 | KNN Loss: 3.731187582015991 | BCE Loss: 1.044151782989502\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.783353805541992 | KNN Loss: 3.7491049766540527 | BCE Loss: 1.034248948097229\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.751350402832031 | KNN Loss: 3.744527816772461 | BCE Loss: 1.0068223476409912\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.76889705657959 | KNN Loss: 3.740426778793335 | BCE Loss: 1.0284702777862549\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.831787586212158 | KNN Loss: 3.790576219558716 | BCE Loss: 1.0412113666534424\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.746931076049805 | KNN Loss: 3.716818332672119 | BCE Loss: 1.0301129817962646\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.80747127532959 | KNN Loss: 3.7425127029418945 | BCE Loss: 1.0649585723876953\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.817635536193848 | KNN Loss: 3.7878782749176025 | BCE Loss: 1.0297571420669556\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.777749061584473 | KNN Loss: 3.7603580951690674 | BCE Loss: 1.0173912048339844\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.807374954223633 | KNN Loss: 3.7691142559051514 | BCE Loss: 1.038260579109192\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.754380226135254 | KNN Loss: 3.719881057739258 | BCE Loss: 1.0344990491867065\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.772678852081299 | KNN Loss: 3.7380056381225586 | BCE Loss: 1.0346732139587402\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.782198905944824 | KNN Loss: 3.765327215194702 | BCE Loss: 1.0168715715408325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.8286662101745605 | KNN Loss: 3.7770891189575195 | BCE Loss: 1.0515769720077515\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.798503398895264 | KNN Loss: 3.756279706954956 | BCE Loss: 1.042223572731018\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.760849952697754 | KNN Loss: 3.7435646057128906 | BCE Loss: 1.0172855854034424\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.74405574798584 | KNN Loss: 3.736921548843384 | BCE Loss: 1.007134199142456\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.801478862762451 | KNN Loss: 3.750027894973755 | BCE Loss: 1.0514508485794067\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.815277099609375 | KNN Loss: 3.7805542945861816 | BCE Loss: 1.0347228050231934\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.7980265617370605 | KNN Loss: 3.7462282180786133 | BCE Loss: 1.0517983436584473\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.757270336151123 | KNN Loss: 3.745525360107422 | BCE Loss: 1.0117448568344116\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.783082008361816 | KNN Loss: 3.7313077449798584 | BCE Loss: 1.0517743825912476\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.788994789123535 | KNN Loss: 3.7594146728515625 | BCE Loss: 1.0295803546905518\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.791933059692383 | KNN Loss: 3.743610382080078 | BCE Loss: 1.0483226776123047\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.75660514831543 | KNN Loss: 3.7311508655548096 | BCE Loss: 1.0254544019699097\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.764926910400391 | KNN Loss: 3.726417303085327 | BCE Loss: 1.038509726524353\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.714925289154053 | KNN Loss: 3.7080228328704834 | BCE Loss: 1.0069023370742798\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.744228839874268 | KNN Loss: 3.7173478603363037 | BCE Loss: 1.0268808603286743\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.812707901000977 | KNN Loss: 3.7670881748199463 | BCE Loss: 1.0456199645996094\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.793641567230225 | KNN Loss: 3.75447678565979 | BCE Loss: 1.0391649007797241\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.765227794647217 | KNN Loss: 3.746635913848877 | BCE Loss: 1.0185918807983398\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.751530170440674 | KNN Loss: 3.7421891689300537 | BCE Loss: 1.0093410015106201\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.747289180755615 | KNN Loss: 3.729679584503174 | BCE Loss: 1.017609715461731\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.784478187561035 | KNN Loss: 3.7320094108581543 | BCE Loss: 1.0524686574935913\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.791911602020264 | KNN Loss: 3.744889259338379 | BCE Loss: 1.0470223426818848\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.782662391662598 | KNN Loss: 3.74515438079834 | BCE Loss: 1.0375080108642578\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.788761615753174 | KNN Loss: 3.7587358951568604 | BCE Loss: 1.0300257205963135\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.773242950439453 | KNN Loss: 3.7528903484344482 | BCE Loss: 1.0203524827957153\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.78874397277832 | KNN Loss: 3.746317148208618 | BCE Loss: 1.0424267053604126\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.821115016937256 | KNN Loss: 3.7821569442749023 | BCE Loss: 1.038957953453064\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.8088836669921875 | KNN Loss: 3.7665278911590576 | BCE Loss: 1.0423558950424194\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.786224365234375 | KNN Loss: 3.7301454544067383 | BCE Loss: 1.0560786724090576\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.7736382484436035 | KNN Loss: 3.73209285736084 | BCE Loss: 1.0415453910827637\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.7714033126831055 | KNN Loss: 3.762024164199829 | BCE Loss: 1.0093791484832764\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.764226913452148 | KNN Loss: 3.7383780479431152 | BCE Loss: 1.0258489847183228\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.770057678222656 | KNN Loss: 3.7328436374664307 | BCE Loss: 1.037213921546936\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.798707962036133 | KNN Loss: 3.754913330078125 | BCE Loss: 1.043794870376587\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.799149513244629 | KNN Loss: 3.747645616531372 | BCE Loss: 1.0515040159225464\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.800959587097168 | KNN Loss: 3.743793249130249 | BCE Loss: 1.0571664571762085\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.777366638183594 | KNN Loss: 3.740118980407715 | BCE Loss: 1.037247657775879\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.780803680419922 | KNN Loss: 3.742302417755127 | BCE Loss: 1.038501501083374\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.812838554382324 | KNN Loss: 3.761387825012207 | BCE Loss: 1.0514506101608276\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.751165390014648 | KNN Loss: 3.7129135131835938 | BCE Loss: 1.0382519960403442\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.750798225402832 | KNN Loss: 3.732558012008667 | BCE Loss: 1.018239974975586\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.796782970428467 | KNN Loss: 3.740969657897949 | BCE Loss: 1.055813193321228\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.785699367523193 | KNN Loss: 3.7296156883239746 | BCE Loss: 1.0560837984085083\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.772167205810547 | KNN Loss: 3.7458934783935547 | BCE Loss: 1.0262736082077026\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.789144515991211 | KNN Loss: 3.7222068309783936 | BCE Loss: 1.0669374465942383\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.797765731811523 | KNN Loss: 3.763716697692871 | BCE Loss: 1.0340492725372314\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.756521224975586 | KNN Loss: 3.7378485202789307 | BCE Loss: 1.0186724662780762\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.774748802185059 | KNN Loss: 3.7458064556121826 | BCE Loss: 1.0289421081542969\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.767777919769287 | KNN Loss: 3.737933874130249 | BCE Loss: 1.029844045639038\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.8028106689453125 | KNN Loss: 3.733314275741577 | BCE Loss: 1.0694962739944458\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.7365617752075195 | KNN Loss: 3.7162420749664307 | BCE Loss: 1.020319938659668\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.74197244644165 | KNN Loss: 3.730323553085327 | BCE Loss: 1.0116490125656128\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.7418928146362305 | KNN Loss: 3.729050636291504 | BCE Loss: 1.0128419399261475\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.773576736450195 | KNN Loss: 3.756593942642212 | BCE Loss: 1.016982913017273\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.811981201171875 | KNN Loss: 3.786088705062866 | BCE Loss: 1.0258922576904297\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.807906627655029 | KNN Loss: 3.748760938644409 | BCE Loss: 1.0591455698013306\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.7783684730529785 | KNN Loss: 3.7498676776885986 | BCE Loss: 1.0285007953643799\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.741763114929199 | KNN Loss: 3.7201485633850098 | BCE Loss: 1.021614670753479\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.831907749176025 | KNN Loss: 3.776434898376465 | BCE Loss: 1.0554728507995605\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.762882709503174 | KNN Loss: 3.7463529109954834 | BCE Loss: 1.0165297985076904\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.731729984283447 | KNN Loss: 3.713693141937256 | BCE Loss: 1.0180368423461914\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.82174015045166 | KNN Loss: 3.7715399265289307 | BCE Loss: 1.0502004623413086\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.795421123504639 | KNN Loss: 3.7564053535461426 | BCE Loss: 1.039015769958496\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.745713233947754 | KNN Loss: 3.7049269676208496 | BCE Loss: 1.0407863855361938\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.7656073570251465 | KNN Loss: 3.7194533348083496 | BCE Loss: 1.0461540222167969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.752335548400879 | KNN Loss: 3.7268314361572266 | BCE Loss: 1.025504231452942\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.776770114898682 | KNN Loss: 3.730642318725586 | BCE Loss: 1.0461277961730957\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.769738674163818 | KNN Loss: 3.7460007667541504 | BCE Loss: 1.0237380266189575\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.758370399475098 | KNN Loss: 3.729994535446167 | BCE Loss: 1.0283759832382202\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.734597206115723 | KNN Loss: 3.7082557678222656 | BCE Loss: 1.026341438293457\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.761995315551758 | KNN Loss: 3.7163920402526855 | BCE Loss: 1.0456032752990723\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.832346439361572 | KNN Loss: 3.7850418090820312 | BCE Loss: 1.047304630279541\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.786372661590576 | KNN Loss: 3.743522882461548 | BCE Loss: 1.0428497791290283\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.751349925994873 | KNN Loss: 3.710028886795044 | BCE Loss: 1.041321039199829\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.75486946105957 | KNN Loss: 3.707840919494629 | BCE Loss: 1.0470285415649414\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.764658451080322 | KNN Loss: 3.7492008209228516 | BCE Loss: 1.0154576301574707\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.779656410217285 | KNN Loss: 3.74686336517334 | BCE Loss: 1.0327932834625244\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.757445335388184 | KNN Loss: 3.7209632396698 | BCE Loss: 1.0364822149276733\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.773361682891846 | KNN Loss: 3.738239288330078 | BCE Loss: 1.0351225137710571\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.753526210784912 | KNN Loss: 3.7266621589660645 | BCE Loss: 1.0268641710281372\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.750124931335449 | KNN Loss: 3.7119040489196777 | BCE Loss: 1.038220763206482\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.744012355804443 | KNN Loss: 3.7366321086883545 | BCE Loss: 1.0073802471160889\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.7585320472717285 | KNN Loss: 3.725046396255493 | BCE Loss: 1.033485770225525\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.782922267913818 | KNN Loss: 3.7357239723205566 | BCE Loss: 1.0471981763839722\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.731032371520996 | KNN Loss: 3.721615791320801 | BCE Loss: 1.0094165802001953\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.776095867156982 | KNN Loss: 3.742779493331909 | BCE Loss: 1.0333163738250732\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.724783897399902 | KNN Loss: 3.7267696857452393 | BCE Loss: 0.9980141520500183\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.746257781982422 | KNN Loss: 3.726526975631714 | BCE Loss: 1.019730806350708\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.747872352600098 | KNN Loss: 3.7220499515533447 | BCE Loss: 1.0258225202560425\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.737378120422363 | KNN Loss: 3.7375612258911133 | BCE Loss: 0.9998170137405396\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.7931694984436035 | KNN Loss: 3.742866039276123 | BCE Loss: 1.050303339958191\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.711557388305664 | KNN Loss: 3.690124750137329 | BCE Loss: 1.021432876586914\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.729362964630127 | KNN Loss: 3.695305824279785 | BCE Loss: 1.0340572595596313\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.735718727111816 | KNN Loss: 3.7093560695648193 | BCE Loss: 1.0263627767562866\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.752382278442383 | KNN Loss: 3.7046964168548584 | BCE Loss: 1.0476856231689453\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.73419713973999 | KNN Loss: 3.721675157546997 | BCE Loss: 1.0125218629837036\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.7336320877075195 | KNN Loss: 3.7229950428009033 | BCE Loss: 1.0106370449066162\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.725729465484619 | KNN Loss: 3.699808359146118 | BCE Loss: 1.025921106338501\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.824514865875244 | KNN Loss: 3.7273740768432617 | BCE Loss: 1.0971406698226929\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.74051570892334 | KNN Loss: 3.69553804397583 | BCE Loss: 1.0449779033660889\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.73888635635376 | KNN Loss: 3.7133395671844482 | BCE Loss: 1.0255467891693115\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.770868301391602 | KNN Loss: 3.7380120754241943 | BCE Loss: 1.0328561067581177\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.786962509155273 | KNN Loss: 3.7716588973999023 | BCE Loss: 1.015303611755371\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.7742509841918945 | KNN Loss: 3.7351720333099365 | BCE Loss: 1.039078712463379\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.774914741516113 | KNN Loss: 3.721414089202881 | BCE Loss: 1.0535006523132324\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.720772743225098 | KNN Loss: 3.6889119148254395 | BCE Loss: 1.0318609476089478\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.741596698760986 | KNN Loss: 3.715425729751587 | BCE Loss: 1.026171088218689\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.760024070739746 | KNN Loss: 3.7419545650482178 | BCE Loss: 1.0180696249008179\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.7606892585754395 | KNN Loss: 3.7386155128479004 | BCE Loss: 1.022073745727539\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.762081146240234 | KNN Loss: 3.7289774417877197 | BCE Loss: 1.033103585243225\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.757002353668213 | KNN Loss: 3.7348155975341797 | BCE Loss: 1.0221868753433228\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.770990371704102 | KNN Loss: 3.7122321128845215 | BCE Loss: 1.058758020401001\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.768766403198242 | KNN Loss: 3.74191951751709 | BCE Loss: 1.0268466472625732\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.771012306213379 | KNN Loss: 3.7479541301727295 | BCE Loss: 1.0230580568313599\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.764355659484863 | KNN Loss: 3.741420269012451 | BCE Loss: 1.0229352712631226\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.755542755126953 | KNN Loss: 3.7254481315612793 | BCE Loss: 1.0300947427749634\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.749588489532471 | KNN Loss: 3.7108614444732666 | BCE Loss: 1.038727045059204\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.759438514709473 | KNN Loss: 3.7339508533477783 | BCE Loss: 1.0254875421524048\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.7725114822387695 | KNN Loss: 3.742467164993286 | BCE Loss: 1.0300441980361938\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.774890422821045 | KNN Loss: 3.7412872314453125 | BCE Loss: 1.0336030721664429\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.750327110290527 | KNN Loss: 3.737337112426758 | BCE Loss: 1.01298987865448\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.723557472229004 | KNN Loss: 3.6941399574279785 | BCE Loss: 1.0294177532196045\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.740976333618164 | KNN Loss: 3.704500436782837 | BCE Loss: 1.0364760160446167\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.705178737640381 | KNN Loss: 3.702970266342163 | BCE Loss: 1.0022084712982178\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.817981243133545 | KNN Loss: 3.7627458572387695 | BCE Loss: 1.0552353858947754\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.748455047607422 | KNN Loss: 3.7399837970733643 | BCE Loss: 1.0084714889526367\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.70035457611084 | KNN Loss: 3.7061331272125244 | BCE Loss: 0.9942216277122498\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.760919570922852 | KNN Loss: 3.724735975265503 | BCE Loss: 1.0361838340759277\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.7367095947265625 | KNN Loss: 3.697532892227173 | BCE Loss: 1.0391767024993896\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.755146503448486 | KNN Loss: 3.6889336109161377 | BCE Loss: 1.0662128925323486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.734915733337402 | KNN Loss: 3.7143349647521973 | BCE Loss: 1.0205806493759155\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.7693681716918945 | KNN Loss: 3.749528646469116 | BCE Loss: 1.0198394060134888\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.743477821350098 | KNN Loss: 3.7292909622192383 | BCE Loss: 1.0141867399215698\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.7527570724487305 | KNN Loss: 3.714845657348633 | BCE Loss: 1.037911295890808\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.719400882720947 | KNN Loss: 3.6981663703918457 | BCE Loss: 1.0212345123291016\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.689562797546387 | KNN Loss: 3.6784324645996094 | BCE Loss: 1.0111305713653564\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.686245441436768 | KNN Loss: 3.6960532665252686 | BCE Loss: 0.9901921153068542\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.756433963775635 | KNN Loss: 3.7147462368011475 | BCE Loss: 1.0416877269744873\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.733283996582031 | KNN Loss: 3.71224308013916 | BCE Loss: 1.021040916442871\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.722937107086182 | KNN Loss: 3.699272871017456 | BCE Loss: 1.0236643552780151\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.707117080688477 | KNN Loss: 3.6826083660125732 | BCE Loss: 1.0245088338851929\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.7319746017456055 | KNN Loss: 3.704519033432007 | BCE Loss: 1.027455449104309\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.743585109710693 | KNN Loss: 3.7384467124938965 | BCE Loss: 1.0051383972167969\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.754188060760498 | KNN Loss: 3.7349393367767334 | BCE Loss: 1.0192488431930542\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.726556777954102 | KNN Loss: 3.727524995803833 | BCE Loss: 0.999031662940979\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.734583377838135 | KNN Loss: 3.7091739177703857 | BCE Loss: 1.0254093408584595\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.700748443603516 | KNN Loss: 3.6930317878723145 | BCE Loss: 1.0077166557312012\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.734149932861328 | KNN Loss: 3.707618474960327 | BCE Loss: 1.026531457901001\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.7961273193359375 | KNN Loss: 3.731029510498047 | BCE Loss: 1.0650975704193115\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.73037052154541 | KNN Loss: 3.7290306091308594 | BCE Loss: 1.0013399124145508\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.7716851234436035 | KNN Loss: 3.722903251647949 | BCE Loss: 1.0487819910049438\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.745009422302246 | KNN Loss: 3.75455904006958 | BCE Loss: 0.9904503226280212\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.760013103485107 | KNN Loss: 3.725787401199341 | BCE Loss: 1.0342258214950562\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.775907516479492 | KNN Loss: 3.7489476203918457 | BCE Loss: 1.026960015296936\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.76115608215332 | KNN Loss: 3.7387759685516357 | BCE Loss: 1.0223801136016846\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.74201774597168 | KNN Loss: 3.708242893218994 | BCE Loss: 1.0337750911712646\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.727590560913086 | KNN Loss: 3.717799425125122 | BCE Loss: 1.0097911357879639\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.777099609375 | KNN Loss: 3.7307353019714355 | BCE Loss: 1.0463640689849854\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.697754383087158 | KNN Loss: 3.6874828338623047 | BCE Loss: 1.010271430015564\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.734371185302734 | KNN Loss: 3.6943514347076416 | BCE Loss: 1.0400199890136719\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.720081329345703 | KNN Loss: 3.7072978019714355 | BCE Loss: 1.0127835273742676\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.789059638977051 | KNN Loss: 3.729949474334717 | BCE Loss: 1.059110164642334\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.760064125061035 | KNN Loss: 3.7224576473236084 | BCE Loss: 1.0376067161560059\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.755939483642578 | KNN Loss: 3.723924398422241 | BCE Loss: 1.032015323638916\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.724621772766113 | KNN Loss: 3.7078936100006104 | BCE Loss: 1.016728401184082\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.693892478942871 | KNN Loss: 3.675004482269287 | BCE Loss: 1.0188878774642944\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.735030651092529 | KNN Loss: 3.707852840423584 | BCE Loss: 1.0271776914596558\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.732562065124512 | KNN Loss: 3.7194132804870605 | BCE Loss: 1.0131487846374512\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.714112758636475 | KNN Loss: 3.6883721351623535 | BCE Loss: 1.0257407426834106\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.7760725021362305 | KNN Loss: 3.7296509742736816 | BCE Loss: 1.0464215278625488\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.748006820678711 | KNN Loss: 3.711578369140625 | BCE Loss: 1.036428451538086\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.726905345916748 | KNN Loss: 3.691469669342041 | BCE Loss: 1.035435676574707\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.694390773773193 | KNN Loss: 3.6764724254608154 | BCE Loss: 1.017918348312378\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.761745929718018 | KNN Loss: 3.7195627689361572 | BCE Loss: 1.04218327999115\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.7094573974609375 | KNN Loss: 3.7068357467651367 | BCE Loss: 1.0026217699050903\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.776031017303467 | KNN Loss: 3.725754499435425 | BCE Loss: 1.050276517868042\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.767397880554199 | KNN Loss: 3.7160089015960693 | BCE Loss: 1.0513887405395508\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.708259105682373 | KNN Loss: 3.695488452911377 | BCE Loss: 1.0127705335617065\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.738192081451416 | KNN Loss: 3.732879638671875 | BCE Loss: 1.005312442779541\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.729185104370117 | KNN Loss: 3.711134672164917 | BCE Loss: 1.0180506706237793\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.746879577636719 | KNN Loss: 3.7261104583740234 | BCE Loss: 1.0207688808441162\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.756278038024902 | KNN Loss: 3.695657253265381 | BCE Loss: 1.0606210231781006\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.709866523742676 | KNN Loss: 3.711122751235962 | BCE Loss: 0.9987438917160034\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.7169036865234375 | KNN Loss: 3.6934430599212646 | BCE Loss: 1.0234605073928833\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.722899913787842 | KNN Loss: 3.7143001556396484 | BCE Loss: 1.0085996389389038\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.7642130851745605 | KNN Loss: 3.739844560623169 | BCE Loss: 1.0243685245513916\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.7332763671875 | KNN Loss: 3.7212467193603516 | BCE Loss: 1.012029767036438\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.74925422668457 | KNN Loss: 3.7338995933532715 | BCE Loss: 1.0153546333312988\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.757593154907227 | KNN Loss: 3.6984877586364746 | BCE Loss: 1.0591052770614624\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.7017316818237305 | KNN Loss: 3.6740190982818604 | BCE Loss: 1.0277125835418701\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.761188983917236 | KNN Loss: 3.726724863052368 | BCE Loss: 1.0344642400741577\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.779026508331299 | KNN Loss: 3.7412447929382324 | BCE Loss: 1.0377815961837769\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.722900390625 | KNN Loss: 3.7068004608154297 | BCE Loss: 1.0161000490188599\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.721230983734131 | KNN Loss: 3.701530933380127 | BCE Loss: 1.0196999311447144\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.813143253326416 | KNN Loss: 3.7658252716064453 | BCE Loss: 1.0473179817199707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.747470855712891 | KNN Loss: 3.7174339294433594 | BCE Loss: 1.0300371646881104\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.704168319702148 | KNN Loss: 3.698162317276001 | BCE Loss: 1.006005883216858\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.730522155761719 | KNN Loss: 3.7140674591064453 | BCE Loss: 1.0164546966552734\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.772841930389404 | KNN Loss: 3.7469029426574707 | BCE Loss: 1.025938868522644\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.81037712097168 | KNN Loss: 3.77256441116333 | BCE Loss: 1.0378127098083496\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.759209632873535 | KNN Loss: 3.7385241985321045 | BCE Loss: 1.0206855535507202\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.692880630493164 | KNN Loss: 3.676941156387329 | BCE Loss: 1.0159395933151245\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.740056037902832 | KNN Loss: 3.727343797683716 | BCE Loss: 1.0127122402191162\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.716925621032715 | KNN Loss: 3.7057912349700928 | BCE Loss: 1.011134386062622\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.7007598876953125 | KNN Loss: 3.673832893371582 | BCE Loss: 1.026926875114441\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.724133014678955 | KNN Loss: 3.6991426944732666 | BCE Loss: 1.0249903202056885\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.7016754150390625 | KNN Loss: 3.6884889602661133 | BCE Loss: 1.0131865739822388\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.729636192321777 | KNN Loss: 3.6942977905273438 | BCE Loss: 1.0353381633758545\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.776445388793945 | KNN Loss: 3.7424721717834473 | BCE Loss: 1.033972978591919\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.708658218383789 | KNN Loss: 3.7164194583892822 | BCE Loss: 0.9922387599945068\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.763507843017578 | KNN Loss: 3.7311127185821533 | BCE Loss: 1.0323952436447144\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.752927780151367 | KNN Loss: 3.718747615814209 | BCE Loss: 1.0341802835464478\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.720288276672363 | KNN Loss: 3.711611270904541 | BCE Loss: 1.0086767673492432\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.70661735534668 | KNN Loss: 3.6801741123199463 | BCE Loss: 1.0264430046081543\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.7530436515808105 | KNN Loss: 3.704643964767456 | BCE Loss: 1.0483996868133545\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.73905611038208 | KNN Loss: 3.7071897983551025 | BCE Loss: 1.031866431236267\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.749795913696289 | KNN Loss: 3.7099366188049316 | BCE Loss: 1.0398591756820679\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.727262020111084 | KNN Loss: 3.695112705230713 | BCE Loss: 1.0321494340896606\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.774274826049805 | KNN Loss: 3.699270725250244 | BCE Loss: 1.0750041007995605\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.737600326538086 | KNN Loss: 3.71663761138916 | BCE Loss: 1.0209629535675049\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.7361159324646 | KNN Loss: 3.6979150772094727 | BCE Loss: 1.038200855255127\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.723214149475098 | KNN Loss: 3.7071521282196045 | BCE Loss: 1.0160620212554932\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.774806976318359 | KNN Loss: 3.7361042499542236 | BCE Loss: 1.0387028455734253\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.784366607666016 | KNN Loss: 3.754274606704712 | BCE Loss: 1.0300917625427246\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.752382278442383 | KNN Loss: 3.7061703205108643 | BCE Loss: 1.0462121963500977\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.751437187194824 | KNN Loss: 3.7335052490234375 | BCE Loss: 1.0179319381713867\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.7552385330200195 | KNN Loss: 3.715683698654175 | BCE Loss: 1.0395550727844238\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.704231262207031 | KNN Loss: 3.700324535369873 | BCE Loss: 1.0039069652557373\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.725804328918457 | KNN Loss: 3.7132208347320557 | BCE Loss: 1.0125833749771118\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.766912937164307 | KNN Loss: 3.7507383823394775 | BCE Loss: 1.0161744356155396\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.7296552658081055 | KNN Loss: 3.685152292251587 | BCE Loss: 1.0445029735565186\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.707723617553711 | KNN Loss: 3.6738545894622803 | BCE Loss: 1.0338692665100098\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.748263835906982 | KNN Loss: 3.7019526958465576 | BCE Loss: 1.0463112592697144\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.740517616271973 | KNN Loss: 3.7117972373962402 | BCE Loss: 1.0287202596664429\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.72707462310791 | KNN Loss: 3.7099239826202393 | BCE Loss: 1.017150640487671\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.695160388946533 | KNN Loss: 3.689816474914551 | BCE Loss: 1.0053439140319824\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.784355163574219 | KNN Loss: 3.731675624847412 | BCE Loss: 1.0526795387268066\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.723689079284668 | KNN Loss: 3.6846139430999756 | BCE Loss: 1.0390753746032715\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.733863353729248 | KNN Loss: 3.7219395637512207 | BCE Loss: 1.011923909187317\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.741001129150391 | KNN Loss: 3.702413320541382 | BCE Loss: 1.0385879278182983\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.753562927246094 | KNN Loss: 3.7066102027893066 | BCE Loss: 1.0469526052474976\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.795035362243652 | KNN Loss: 3.7586069107055664 | BCE Loss: 1.0364285707473755\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.751162528991699 | KNN Loss: 3.7009119987487793 | BCE Loss: 1.050250768661499\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.715630054473877 | KNN Loss: 3.702852487564087 | BCE Loss: 1.01277756690979\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.739648818969727 | KNN Loss: 3.720130681991577 | BCE Loss: 1.019518256187439\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.755170822143555 | KNN Loss: 3.708498477935791 | BCE Loss: 1.0466724634170532\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.697027683258057 | KNN Loss: 3.70304799079895 | BCE Loss: 0.9939796924591064\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.733220100402832 | KNN Loss: 3.698502779006958 | BCE Loss: 1.034717082977295\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.699416637420654 | KNN Loss: 3.6931350231170654 | BCE Loss: 1.0062816143035889\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.730991840362549 | KNN Loss: 3.7100491523742676 | BCE Loss: 1.0209425687789917\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.74910831451416 | KNN Loss: 3.7162628173828125 | BCE Loss: 1.0328457355499268\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.745051383972168 | KNN Loss: 3.715151071548462 | BCE Loss: 1.029900312423706\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.734696388244629 | KNN Loss: 3.7148594856262207 | BCE Loss: 1.019836664199829\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.767061233520508 | KNN Loss: 3.739457845687866 | BCE Loss: 1.027603268623352\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.719264030456543 | KNN Loss: 3.7069876194000244 | BCE Loss: 1.0122761726379395\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.731378555297852 | KNN Loss: 3.7033705711364746 | BCE Loss: 1.0280077457427979\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.713901519775391 | KNN Loss: 3.6828932762145996 | BCE Loss: 1.031008005142212\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.767550945281982 | KNN Loss: 3.735820770263672 | BCE Loss: 1.031730055809021\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.740255355834961 | KNN Loss: 3.713514804840088 | BCE Loss: 1.026740312576294\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.749633312225342 | KNN Loss: 3.719909429550171 | BCE Loss: 1.029723882675171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.7309064865112305 | KNN Loss: 3.705573081970215 | BCE Loss: 1.0253334045410156\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.747256278991699 | KNN Loss: 3.710092306137085 | BCE Loss: 1.0371637344360352\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.806870460510254 | KNN Loss: 3.7454051971435547 | BCE Loss: 1.0614655017852783\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.725842475891113 | KNN Loss: 3.720979690551758 | BCE Loss: 1.004862904548645\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.717747688293457 | KNN Loss: 3.6982581615448 | BCE Loss: 1.0194897651672363\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.732027053833008 | KNN Loss: 3.6981732845306396 | BCE Loss: 1.0338537693023682\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.715744972229004 | KNN Loss: 3.692932367324829 | BCE Loss: 1.0228126049041748\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.781083583831787 | KNN Loss: 3.7257699966430664 | BCE Loss: 1.0553135871887207\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.723294258117676 | KNN Loss: 3.6937742233276367 | BCE Loss: 1.0295201539993286\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.780874252319336 | KNN Loss: 3.736989974975586 | BCE Loss: 1.0438843965530396\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.751512050628662 | KNN Loss: 3.709149122238159 | BCE Loss: 1.0423630475997925\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.709352493286133 | KNN Loss: 3.7036213874816895 | BCE Loss: 1.005731225013733\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.705948829650879 | KNN Loss: 3.6724863052368164 | BCE Loss: 1.0334622859954834\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.756258487701416 | KNN Loss: 3.720339298248291 | BCE Loss: 1.0359190702438354\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.818784713745117 | KNN Loss: 3.780461072921753 | BCE Loss: 1.0383234024047852\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.7463226318359375 | KNN Loss: 3.721722364425659 | BCE Loss: 1.0246000289916992\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.773662567138672 | KNN Loss: 3.7297823429107666 | BCE Loss: 1.0438802242279053\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.743082523345947 | KNN Loss: 3.7049362659454346 | BCE Loss: 1.0381462574005127\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.8126678466796875 | KNN Loss: 3.7541794776916504 | BCE Loss: 1.0584882497787476\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.716955661773682 | KNN Loss: 3.6749751567840576 | BCE Loss: 1.0419803857803345\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.733041763305664 | KNN Loss: 3.6946158409118652 | BCE Loss: 1.0384256839752197\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.738061428070068 | KNN Loss: 3.7286124229431152 | BCE Loss: 1.0094490051269531\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.746946334838867 | KNN Loss: 3.7260992527008057 | BCE Loss: 1.0208473205566406\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.714735984802246 | KNN Loss: 3.6886556148529053 | BCE Loss: 1.02608060836792\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.746697425842285 | KNN Loss: 3.7046196460723877 | BCE Loss: 1.0420780181884766\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.687386512756348 | KNN Loss: 3.679274559020996 | BCE Loss: 1.0081117153167725\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.732761859893799 | KNN Loss: 3.696441411972046 | BCE Loss: 1.036320447921753\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.712520599365234 | KNN Loss: 3.7007834911346436 | BCE Loss: 1.0117369890213013\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.761025905609131 | KNN Loss: 3.7407517433166504 | BCE Loss: 1.0202741622924805\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.751918315887451 | KNN Loss: 3.7195396423339844 | BCE Loss: 1.0323787927627563\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.726433277130127 | KNN Loss: 3.6985673904418945 | BCE Loss: 1.0278658866882324\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.727175712585449 | KNN Loss: 3.705099582672119 | BCE Loss: 1.022075891494751\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.718177318572998 | KNN Loss: 3.6830062866210938 | BCE Loss: 1.0351711511611938\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.74069881439209 | KNN Loss: 3.725877285003662 | BCE Loss: 1.0148215293884277\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.731650352478027 | KNN Loss: 3.699526309967041 | BCE Loss: 1.0321239233016968\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.729340553283691 | KNN Loss: 3.678842306137085 | BCE Loss: 1.050498366355896\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.733911991119385 | KNN Loss: 3.690598964691162 | BCE Loss: 1.0433131456375122\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.7398681640625 | KNN Loss: 3.6941583156585693 | BCE Loss: 1.0457100868225098\n",
      "Epoch    93: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.726120948791504 | KNN Loss: 3.694675922393799 | BCE Loss: 1.031444787979126\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.753667831420898 | KNN Loss: 3.7046968936920166 | BCE Loss: 1.0489708185195923\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.738585472106934 | KNN Loss: 3.7176761627197266 | BCE Loss: 1.020909070968628\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.744163513183594 | KNN Loss: 3.720767021179199 | BCE Loss: 1.0233964920043945\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.738906383514404 | KNN Loss: 3.711304187774658 | BCE Loss: 1.0276020765304565\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.724945545196533 | KNN Loss: 3.691037654876709 | BCE Loss: 1.0339078903198242\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.719315528869629 | KNN Loss: 3.678846597671509 | BCE Loss: 1.0404691696166992\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.749954700469971 | KNN Loss: 3.728858470916748 | BCE Loss: 1.0210963487625122\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.718547821044922 | KNN Loss: 3.713894844055176 | BCE Loss: 1.004652738571167\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.727826118469238 | KNN Loss: 3.707720994949341 | BCE Loss: 1.020105242729187\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.735043525695801 | KNN Loss: 3.705984115600586 | BCE Loss: 1.0290592908859253\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.715747356414795 | KNN Loss: 3.6864211559295654 | BCE Loss: 1.02932608127594\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.720922470092773 | KNN Loss: 3.6941006183624268 | BCE Loss: 1.0268216133117676\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.755393028259277 | KNN Loss: 3.7290992736816406 | BCE Loss: 1.0262938737869263\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.791147232055664 | KNN Loss: 3.716104507446289 | BCE Loss: 1.0750428438186646\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.669293403625488 | KNN Loss: 3.680156946182251 | BCE Loss: 0.9891363382339478\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.689111232757568 | KNN Loss: 3.6852829456329346 | BCE Loss: 1.0038282871246338\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.722521781921387 | KNN Loss: 3.7056963443756104 | BCE Loss: 1.0168253183364868\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.742291450500488 | KNN Loss: 3.7010676860809326 | BCE Loss: 1.0412238836288452\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.728431701660156 | KNN Loss: 3.6948060989379883 | BCE Loss: 1.033625841140747\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.702983379364014 | KNN Loss: 3.690291404724121 | BCE Loss: 1.0126920938491821\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.748913764953613 | KNN Loss: 3.7053065299987793 | BCE Loss: 1.0436069965362549\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.715576171875 | KNN Loss: 3.675628423690796 | BCE Loss: 1.039947509765625\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.7799391746521 | KNN Loss: 3.741389513015747 | BCE Loss: 1.038549542427063\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.706124305725098 | KNN Loss: 3.7001543045043945 | BCE Loss: 1.0059702396392822\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.767877578735352 | KNN Loss: 3.7320704460144043 | BCE Loss: 1.0358073711395264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.726659774780273 | KNN Loss: 3.697092294692993 | BCE Loss: 1.0295674800872803\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.79210090637207 | KNN Loss: 3.75777268409729 | BCE Loss: 1.0343281030654907\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.7152581214904785 | KNN Loss: 3.710667133331299 | BCE Loss: 1.0045909881591797\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.714756965637207 | KNN Loss: 3.689267158508301 | BCE Loss: 1.0254895687103271\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.711281776428223 | KNN Loss: 3.6811327934265137 | BCE Loss: 1.0301487445831299\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.758164882659912 | KNN Loss: 3.739976644515991 | BCE Loss: 1.0181881189346313\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.711976051330566 | KNN Loss: 3.7014362812042236 | BCE Loss: 1.0105395317077637\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.69168758392334 | KNN Loss: 3.7096893787384033 | BCE Loss: 0.9819982051849365\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.717148780822754 | KNN Loss: 3.6842641830444336 | BCE Loss: 1.0328843593597412\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.703196048736572 | KNN Loss: 3.693401336669922 | BCE Loss: 1.0097945928573608\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.703283309936523 | KNN Loss: 3.679171562194824 | BCE Loss: 1.0241117477416992\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.737823963165283 | KNN Loss: 3.70346999168396 | BCE Loss: 1.0343540906906128\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.723541259765625 | KNN Loss: 3.684593439102173 | BCE Loss: 1.0389480590820312\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.735058307647705 | KNN Loss: 3.7286465167999268 | BCE Loss: 1.0064117908477783\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.696019649505615 | KNN Loss: 3.6810476779937744 | BCE Loss: 1.0149719715118408\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.6984710693359375 | KNN Loss: 3.681044578552246 | BCE Loss: 1.017426609992981\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.708815574645996 | KNN Loss: 3.6808621883392334 | BCE Loss: 1.0279533863067627\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.751922607421875 | KNN Loss: 3.689443588256836 | BCE Loss: 1.062479019165039\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.746448040008545 | KNN Loss: 3.717226028442383 | BCE Loss: 1.029222011566162\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.720458984375 | KNN Loss: 3.687480926513672 | BCE Loss: 1.032977819442749\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.710608959197998 | KNN Loss: 3.688626289367676 | BCE Loss: 1.0219825506210327\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.718433380126953 | KNN Loss: 3.715564727783203 | BCE Loss: 1.002868890762329\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.703670501708984 | KNN Loss: 3.6851611137390137 | BCE Loss: 1.0185095071792603\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.791635513305664 | KNN Loss: 3.729468822479248 | BCE Loss: 1.0621668100357056\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.756427764892578 | KNN Loss: 3.7144923210144043 | BCE Loss: 1.041935682296753\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.7348737716674805 | KNN Loss: 3.7279999256134033 | BCE Loss: 1.0068737268447876\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.735284805297852 | KNN Loss: 3.721806764602661 | BCE Loss: 1.0134779214859009\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.713029861450195 | KNN Loss: 3.69504451751709 | BCE Loss: 1.0179853439331055\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.733590602874756 | KNN Loss: 3.701777696609497 | BCE Loss: 1.0318130254745483\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.719996929168701 | KNN Loss: 3.6863508224487305 | BCE Loss: 1.0336462259292603\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.73303747177124 | KNN Loss: 3.689073324203491 | BCE Loss: 1.0439640283584595\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.714974403381348 | KNN Loss: 3.6773078441619873 | BCE Loss: 1.0376663208007812\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.711267471313477 | KNN Loss: 3.6839089393615723 | BCE Loss: 1.0273587703704834\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.6978840827941895 | KNN Loss: 3.6779534816741943 | BCE Loss: 1.0199307203292847\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.728191375732422 | KNN Loss: 3.712639808654785 | BCE Loss: 1.0155515670776367\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.759751319885254 | KNN Loss: 3.7316598892211914 | BCE Loss: 1.0280914306640625\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.755217552185059 | KNN Loss: 3.7321860790252686 | BCE Loss: 1.0230317115783691\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.690837860107422 | KNN Loss: 3.680694580078125 | BCE Loss: 1.0101433992385864\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.723659515380859 | KNN Loss: 3.6950671672821045 | BCE Loss: 1.0285924673080444\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.7259416580200195 | KNN Loss: 3.7181928157806396 | BCE Loss: 1.007749080657959\n",
      "Epoch   104: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.750395774841309 | KNN Loss: 3.713359832763672 | BCE Loss: 1.0370358228683472\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.741053581237793 | KNN Loss: 3.715406894683838 | BCE Loss: 1.025646448135376\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.727721214294434 | KNN Loss: 3.697843074798584 | BCE Loss: 1.02987802028656\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.697653293609619 | KNN Loss: 3.6847386360168457 | BCE Loss: 1.0129145383834839\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.715822696685791 | KNN Loss: 3.6789565086364746 | BCE Loss: 1.0368660688400269\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.6949238777160645 | KNN Loss: 3.6879899501800537 | BCE Loss: 1.0069339275360107\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.729062080383301 | KNN Loss: 3.704387664794922 | BCE Loss: 1.0246741771697998\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.708606719970703 | KNN Loss: 3.691124677658081 | BCE Loss: 1.0174822807312012\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.737748146057129 | KNN Loss: 3.72064208984375 | BCE Loss: 1.0171058177947998\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.70068359375 | KNN Loss: 3.6869614124298096 | BCE Loss: 1.0137220621109009\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.774691581726074 | KNN Loss: 3.728846549987793 | BCE Loss: 1.0458452701568604\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.672205924987793 | KNN Loss: 3.6754915714263916 | BCE Loss: 0.9967142939567566\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.700051307678223 | KNN Loss: 3.6593852043151855 | BCE Loss: 1.0406663417816162\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.699817180633545 | KNN Loss: 3.6915388107299805 | BCE Loss: 1.0082783699035645\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.73548698425293 | KNN Loss: 3.690753698348999 | BCE Loss: 1.0447332859039307\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.747494697570801 | KNN Loss: 3.7102174758911133 | BCE Loss: 1.037277340888977\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.697534084320068 | KNN Loss: 3.7009024620056152 | BCE Loss: 0.9966317415237427\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.704263687133789 | KNN Loss: 3.7071924209594727 | BCE Loss: 0.9970712065696716\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.72560453414917 | KNN Loss: 3.7016966342926025 | BCE Loss: 1.023908019065857\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.7096638679504395 | KNN Loss: 3.6829636096954346 | BCE Loss: 1.0267002582550049\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.735116958618164 | KNN Loss: 3.6918747425079346 | BCE Loss: 1.0432422161102295\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.710495471954346 | KNN Loss: 3.7020885944366455 | BCE Loss: 1.0084067583084106\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.748234748840332 | KNN Loss: 3.717986822128296 | BCE Loss: 1.0302478075027466\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.701288223266602 | KNN Loss: 3.6606438159942627 | BCE Loss: 1.0406442880630493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.672432899475098 | KNN Loss: 3.656273365020752 | BCE Loss: 1.0161592960357666\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.719854354858398 | KNN Loss: 3.708113193511963 | BCE Loss: 1.0117411613464355\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.710195064544678 | KNN Loss: 3.682309865951538 | BCE Loss: 1.0278851985931396\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.722254753112793 | KNN Loss: 3.703908920288086 | BCE Loss: 1.018345594406128\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.760400772094727 | KNN Loss: 3.734800338745117 | BCE Loss: 1.0256006717681885\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.755642414093018 | KNN Loss: 3.6941463947296143 | BCE Loss: 1.0614960193634033\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.730020523071289 | KNN Loss: 3.6985361576080322 | BCE Loss: 1.0314841270446777\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.752190589904785 | KNN Loss: 3.7291274070739746 | BCE Loss: 1.023063063621521\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.764691352844238 | KNN Loss: 3.738739252090454 | BCE Loss: 1.025951862335205\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.72418737411499 | KNN Loss: 3.7195088863372803 | BCE Loss: 1.0046786069869995\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.706304550170898 | KNN Loss: 3.687044858932495 | BCE Loss: 1.0192599296569824\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.678864002227783 | KNN Loss: 3.6550824642181396 | BCE Loss: 1.023781418800354\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.693568706512451 | KNN Loss: 3.675706624984741 | BCE Loss: 1.0178619623184204\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.750089168548584 | KNN Loss: 3.7032806873321533 | BCE Loss: 1.0468086004257202\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.743631362915039 | KNN Loss: 3.733003854751587 | BCE Loss: 1.0106276273727417\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.742978572845459 | KNN Loss: 3.6844146251678467 | BCE Loss: 1.0585640668869019\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.731820106506348 | KNN Loss: 3.6889050006866455 | BCE Loss: 1.0429151058197021\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.711737632751465 | KNN Loss: 3.6771175861358643 | BCE Loss: 1.0346201658248901\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.751379489898682 | KNN Loss: 3.7086989879608154 | BCE Loss: 1.0426805019378662\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.744245529174805 | KNN Loss: 3.7090256214141846 | BCE Loss: 1.0352199077606201\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.732519149780273 | KNN Loss: 3.6922993659973145 | BCE Loss: 1.0402195453643799\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.754532337188721 | KNN Loss: 3.696892499923706 | BCE Loss: 1.0576398372650146\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.717649459838867 | KNN Loss: 3.703042984008789 | BCE Loss: 1.014606237411499\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.751829147338867 | KNN Loss: 3.6970643997192383 | BCE Loss: 1.054764986038208\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.744159698486328 | KNN Loss: 3.70988130569458 | BCE Loss: 1.034278154373169\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.699672222137451 | KNN Loss: 3.6804122924804688 | BCE Loss: 1.0192598104476929\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.7363715171813965 | KNN Loss: 3.6954751014709473 | BCE Loss: 1.0408964157104492\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.719951629638672 | KNN Loss: 3.6853227615356445 | BCE Loss: 1.0346287488937378\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.7322869300842285 | KNN Loss: 3.724008321762085 | BCE Loss: 1.0082786083221436\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.722194194793701 | KNN Loss: 3.6757872104644775 | BCE Loss: 1.0464071035385132\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.749128818511963 | KNN Loss: 3.7122554779052734 | BCE Loss: 1.036873459815979\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.735356330871582 | KNN Loss: 3.7287168502807617 | BCE Loss: 1.0066394805908203\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.756149768829346 | KNN Loss: 3.717806577682495 | BCE Loss: 1.0383433103561401\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.76170539855957 | KNN Loss: 3.7227072715759277 | BCE Loss: 1.0389981269836426\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.694037437438965 | KNN Loss: 3.678525686264038 | BCE Loss: 1.0155118703842163\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.69564962387085 | KNN Loss: 3.6650445461273193 | BCE Loss: 1.0306050777435303\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.798830986022949 | KNN Loss: 3.7469418048858643 | BCE Loss: 1.051889419555664\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.764375686645508 | KNN Loss: 3.7381739616394043 | BCE Loss: 1.0262014865875244\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.692635536193848 | KNN Loss: 3.683837413787842 | BCE Loss: 1.0087980031967163\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.735317230224609 | KNN Loss: 3.7093303203582764 | BCE Loss: 1.0259867906570435\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.717394828796387 | KNN Loss: 3.681867837905884 | BCE Loss: 1.035526990890503\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.732985973358154 | KNN Loss: 3.708656072616577 | BCE Loss: 1.0243299007415771\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.719700813293457 | KNN Loss: 3.6812751293182373 | BCE Loss: 1.0384256839752197\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.710268497467041 | KNN Loss: 3.681417465209961 | BCE Loss: 1.0288511514663696\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.744997024536133 | KNN Loss: 3.7106385231018066 | BCE Loss: 1.034358263015747\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.721017837524414 | KNN Loss: 3.6765995025634766 | BCE Loss: 1.0444180965423584\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.72369384765625 | KNN Loss: 3.707733392715454 | BCE Loss: 1.0159603357315063\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.7248854637146 | KNN Loss: 3.7124829292297363 | BCE Loss: 1.0124026536941528\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.703866958618164 | KNN Loss: 3.6962502002716064 | BCE Loss: 1.0076168775558472\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.759013652801514 | KNN Loss: 3.721952199935913 | BCE Loss: 1.037061333656311\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.741240978240967 | KNN Loss: 3.7093262672424316 | BCE Loss: 1.0319147109985352\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.705514907836914 | KNN Loss: 3.7021119594573975 | BCE Loss: 1.0034027099609375\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.680296897888184 | KNN Loss: 3.674016237258911 | BCE Loss: 1.0062808990478516\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.7543158531188965 | KNN Loss: 3.6853020191192627 | BCE Loss: 1.0690138339996338\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.740184783935547 | KNN Loss: 3.7172892093658447 | BCE Loss: 1.0228954553604126\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.708096504211426 | KNN Loss: 3.70306134223938 | BCE Loss: 1.005035400390625\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.7317705154418945 | KNN Loss: 3.6862542629241943 | BCE Loss: 1.0455164909362793\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.757899761199951 | KNN Loss: 3.6856188774108887 | BCE Loss: 1.072280764579773\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.771018028259277 | KNN Loss: 3.711527109146118 | BCE Loss: 1.0594911575317383\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.725595474243164 | KNN Loss: 3.687174081802368 | BCE Loss: 1.0384212732315063\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.7222676277160645 | KNN Loss: 3.7051730155944824 | BCE Loss: 1.0170944929122925\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.739514350891113 | KNN Loss: 3.7086594104766846 | BCE Loss: 1.0308548212051392\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.735088348388672 | KNN Loss: 3.6969563961029053 | BCE Loss: 1.0381317138671875\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.738339900970459 | KNN Loss: 3.715153694152832 | BCE Loss: 1.0231863260269165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.7846903800964355 | KNN Loss: 3.715196132659912 | BCE Loss: 1.0694941282272339\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.709635257720947 | KNN Loss: 3.712170124053955 | BCE Loss: 0.9974650740623474\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.7561163902282715 | KNN Loss: 3.711711883544922 | BCE Loss: 1.0444045066833496\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.736172676086426 | KNN Loss: 3.684737205505371 | BCE Loss: 1.0514353513717651\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.700291633605957 | KNN Loss: 3.7006659507751465 | BCE Loss: 0.9996259212493896\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.763790130615234 | KNN Loss: 3.7218568325042725 | BCE Loss: 1.0419334173202515\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.69743537902832 | KNN Loss: 3.6858415603637695 | BCE Loss: 1.0115939378738403\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.726835250854492 | KNN Loss: 3.6979422569274902 | BCE Loss: 1.028893232345581\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.693896293640137 | KNN Loss: 3.6879873275756836 | BCE Loss: 1.0059092044830322\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.691277503967285 | KNN Loss: 3.6895954608917236 | BCE Loss: 1.0016818046569824\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.724958419799805 | KNN Loss: 3.70235276222229 | BCE Loss: 1.0226056575775146\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.760251998901367 | KNN Loss: 3.737776756286621 | BCE Loss: 1.0224751234054565\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.759424209594727 | KNN Loss: 3.7117385864257812 | BCE Loss: 1.0476853847503662\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.68353796005249 | KNN Loss: 3.660048484802246 | BCE Loss: 1.0234894752502441\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.705953598022461 | KNN Loss: 3.6897833347320557 | BCE Loss: 1.0161701440811157\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.75333833694458 | KNN Loss: 3.74869441986084 | BCE Loss: 1.0046439170837402\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.683727264404297 | KNN Loss: 3.677513837814331 | BCE Loss: 1.0062134265899658\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.7254228591918945 | KNN Loss: 3.6726744174957275 | BCE Loss: 1.052748680114746\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.853278160095215 | KNN Loss: 3.783590316772461 | BCE Loss: 1.069688081741333\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.687865257263184 | KNN Loss: 3.6745917797088623 | BCE Loss: 1.0132732391357422\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.737557411193848 | KNN Loss: 3.692298173904419 | BCE Loss: 1.0452593564987183\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.756110191345215 | KNN Loss: 3.7211573123931885 | BCE Loss: 1.0349527597427368\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.743607044219971 | KNN Loss: 3.6928200721740723 | BCE Loss: 1.0507868528366089\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.730421543121338 | KNN Loss: 3.7091896533966064 | BCE Loss: 1.0212318897247314\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.684993743896484 | KNN Loss: 3.6737427711486816 | BCE Loss: 1.0112512111663818\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.727174282073975 | KNN Loss: 3.6864206790924072 | BCE Loss: 1.0407536029815674\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.728943347930908 | KNN Loss: 3.699826240539551 | BCE Loss: 1.0291171073913574\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.723998069763184 | KNN Loss: 3.6919238567352295 | BCE Loss: 1.0320744514465332\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.695978164672852 | KNN Loss: 3.669351100921631 | BCE Loss: 1.0266273021697998\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.749734401702881 | KNN Loss: 3.7038631439208984 | BCE Loss: 1.0458712577819824\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.72579288482666 | KNN Loss: 3.6844866275787354 | BCE Loss: 1.0413063764572144\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.736227989196777 | KNN Loss: 3.6944591999053955 | BCE Loss: 1.041769027709961\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.710142135620117 | KNN Loss: 3.69256591796875 | BCE Loss: 1.0175762176513672\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.730472564697266 | KNN Loss: 3.715646266937256 | BCE Loss: 1.0148264169692993\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.709051609039307 | KNN Loss: 3.6978464126586914 | BCE Loss: 1.0112053155899048\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.732459545135498 | KNN Loss: 3.6944785118103027 | BCE Loss: 1.0379809141159058\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.772579669952393 | KNN Loss: 3.7222042083740234 | BCE Loss: 1.0503755807876587\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.709650993347168 | KNN Loss: 3.674825668334961 | BCE Loss: 1.034825086593628\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.753812789916992 | KNN Loss: 3.7275614738464355 | BCE Loss: 1.0262510776519775\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.690426826477051 | KNN Loss: 3.6894149780273438 | BCE Loss: 1.001011610031128\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.714910507202148 | KNN Loss: 3.6942098140716553 | BCE Loss: 1.0207006931304932\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.718864440917969 | KNN Loss: 3.6902527809143066 | BCE Loss: 1.028611421585083\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.719578742980957 | KNN Loss: 3.7012109756469727 | BCE Loss: 1.0183677673339844\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.702620506286621 | KNN Loss: 3.675086736679077 | BCE Loss: 1.027533769607544\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.710888862609863 | KNN Loss: 3.681981086730957 | BCE Loss: 1.0289077758789062\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.717245578765869 | KNN Loss: 3.7082901000976562 | BCE Loss: 1.0089555978775024\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.713648319244385 | KNN Loss: 3.6954245567321777 | BCE Loss: 1.0182238817214966\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.747707366943359 | KNN Loss: 3.736506223678589 | BCE Loss: 1.01120126247406\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.702724456787109 | KNN Loss: 3.703310966491699 | BCE Loss: 0.9994134306907654\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.668163299560547 | KNN Loss: 3.6544041633605957 | BCE Loss: 1.0137593746185303\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.730101108551025 | KNN Loss: 3.708622932434082 | BCE Loss: 1.021478295326233\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.748684883117676 | KNN Loss: 3.7258188724517822 | BCE Loss: 1.0228660106658936\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.724833965301514 | KNN Loss: 3.6977739334106445 | BCE Loss: 1.0270599126815796\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.679693222045898 | KNN Loss: 3.675593137741089 | BCE Loss: 1.00409996509552\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.7569379806518555 | KNN Loss: 3.7370171546936035 | BCE Loss: 1.0199205875396729\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.7237420082092285 | KNN Loss: 3.707487106323242 | BCE Loss: 1.0162549018859863\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.672259330749512 | KNN Loss: 3.6619551181793213 | BCE Loss: 1.0103044509887695\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.7245965003967285 | KNN Loss: 3.702951431274414 | BCE Loss: 1.021645188331604\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.743812561035156 | KNN Loss: 3.6945507526397705 | BCE Loss: 1.0492616891860962\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.712491035461426 | KNN Loss: 3.683055877685547 | BCE Loss: 1.0294349193572998\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.723308563232422 | KNN Loss: 3.698723316192627 | BCE Loss: 1.0245851278305054\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.709624290466309 | KNN Loss: 3.6782400608062744 | BCE Loss: 1.0313844680786133\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.683907508850098 | KNN Loss: 3.6665728092193604 | BCE Loss: 1.0173349380493164\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.685437202453613 | KNN Loss: 3.6843104362487793 | BCE Loss: 1.001127004623413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.721139907836914 | KNN Loss: 3.6969926357269287 | BCE Loss: 1.0241472721099854\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.726146221160889 | KNN Loss: 3.702768564224243 | BCE Loss: 1.023377776145935\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.717582702636719 | KNN Loss: 3.6911728382110596 | BCE Loss: 1.02640962600708\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.732463359832764 | KNN Loss: 3.716783046722412 | BCE Loss: 1.0156803131103516\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.725879192352295 | KNN Loss: 3.704484224319458 | BCE Loss: 1.0213948488235474\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.7041449546813965 | KNN Loss: 3.6880252361297607 | BCE Loss: 1.0161197185516357\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.7561445236206055 | KNN Loss: 3.70953369140625 | BCE Loss: 1.0466105937957764\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.744792461395264 | KNN Loss: 3.698913335800171 | BCE Loss: 1.0458790063858032\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.742499828338623 | KNN Loss: 3.707050085067749 | BCE Loss: 1.0354498624801636\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.729765892028809 | KNN Loss: 3.6912858486175537 | BCE Loss: 1.038480281829834\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.758561611175537 | KNN Loss: 3.738927125930786 | BCE Loss: 1.0196346044540405\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.725475311279297 | KNN Loss: 3.6883065700531006 | BCE Loss: 1.0371688604354858\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.700404167175293 | KNN Loss: 3.678727149963379 | BCE Loss: 1.021676778793335\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.745011329650879 | KNN Loss: 3.678093671798706 | BCE Loss: 1.066917896270752\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.696099281311035 | KNN Loss: 3.6822004318237305 | BCE Loss: 1.0138989686965942\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.698322296142578 | KNN Loss: 3.6774728298187256 | BCE Loss: 1.0208497047424316\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.722361087799072 | KNN Loss: 3.710432291030884 | BCE Loss: 1.0119287967681885\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.747133255004883 | KNN Loss: 3.7158055305480957 | BCE Loss: 1.0313278436660767\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.717754364013672 | KNN Loss: 3.6993966102600098 | BCE Loss: 1.0183576345443726\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.712543487548828 | KNN Loss: 3.692457437515259 | BCE Loss: 1.0200858116149902\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.721039772033691 | KNN Loss: 3.6875267028808594 | BCE Loss: 1.0335133075714111\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.741901874542236 | KNN Loss: 3.707777500152588 | BCE Loss: 1.0341242551803589\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.698738098144531 | KNN Loss: 3.669309616088867 | BCE Loss: 1.029428482055664\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.68473482131958 | KNN Loss: 3.6593899726867676 | BCE Loss: 1.025344967842102\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.755550861358643 | KNN Loss: 3.7232143878936768 | BCE Loss: 1.0323364734649658\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.750575542449951 | KNN Loss: 3.7334377765655518 | BCE Loss: 1.0171376466751099\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.6797566413879395 | KNN Loss: 3.664346218109131 | BCE Loss: 1.0154104232788086\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.73496150970459 | KNN Loss: 3.7180254459381104 | BCE Loss: 1.01693594455719\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.7125959396362305 | KNN Loss: 3.68475341796875 | BCE Loss: 1.02784264087677\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.713983535766602 | KNN Loss: 3.6800410747528076 | BCE Loss: 1.0339423418045044\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.746855735778809 | KNN Loss: 3.707411527633667 | BCE Loss: 1.0394443273544312\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.699960231781006 | KNN Loss: 3.6916775703430176 | BCE Loss: 1.0082826614379883\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.752651214599609 | KNN Loss: 3.7301387786865234 | BCE Loss: 1.0225123167037964\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.732914447784424 | KNN Loss: 3.7114226818084717 | BCE Loss: 1.0214918851852417\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.682836532592773 | KNN Loss: 3.6714141368865967 | BCE Loss: 1.0114226341247559\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.707475662231445 | KNN Loss: 3.692873001098633 | BCE Loss: 1.0146024227142334\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.760504245758057 | KNN Loss: 3.722712278366089 | BCE Loss: 1.0377919673919678\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.71234130859375 | KNN Loss: 3.7038609981536865 | BCE Loss: 1.0084803104400635\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.696593761444092 | KNN Loss: 3.685307025909424 | BCE Loss: 1.0112868547439575\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.7368574142456055 | KNN Loss: 3.6785526275634766 | BCE Loss: 1.058305025100708\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.734001159667969 | KNN Loss: 3.7089240550994873 | BCE Loss: 1.0250773429870605\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.76670503616333 | KNN Loss: 3.7204482555389404 | BCE Loss: 1.0462567806243896\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.721427917480469 | KNN Loss: 3.702569007873535 | BCE Loss: 1.018858790397644\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.690134525299072 | KNN Loss: 3.6694672107696533 | BCE Loss: 1.020667314529419\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.693332195281982 | KNN Loss: 3.6627886295318604 | BCE Loss: 1.030543565750122\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.665567874908447 | KNN Loss: 3.6733341217041016 | BCE Loss: 0.9922336935997009\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.725990295410156 | KNN Loss: 3.682027816772461 | BCE Loss: 1.0439625978469849\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.7570953369140625 | KNN Loss: 3.718968391418457 | BCE Loss: 1.038127064704895\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.693115234375 | KNN Loss: 3.6720130443573 | BCE Loss: 1.0211024284362793\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.750494003295898 | KNN Loss: 3.7313315868377686 | BCE Loss: 1.0191622972488403\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.751397609710693 | KNN Loss: 3.7194840908050537 | BCE Loss: 1.03191339969635\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.758927822113037 | KNN Loss: 3.7195334434509277 | BCE Loss: 1.0393942594528198\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.709015369415283 | KNN Loss: 3.6864116191864014 | BCE Loss: 1.0226037502288818\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.714503288269043 | KNN Loss: 3.7038137912750244 | BCE Loss: 1.0106892585754395\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.775025367736816 | KNN Loss: 3.759772777557373 | BCE Loss: 1.0152524709701538\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.7317633628845215 | KNN Loss: 3.7013232707977295 | BCE Loss: 1.030440092086792\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.717992305755615 | KNN Loss: 3.6903209686279297 | BCE Loss: 1.0276713371276855\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.706098556518555 | KNN Loss: 3.6829841136932373 | BCE Loss: 1.0231142044067383\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.7018609046936035 | KNN Loss: 3.6841378211975098 | BCE Loss: 1.0177232027053833\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.689592361450195 | KNN Loss: 3.687713861465454 | BCE Loss: 1.0018784999847412\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.69120979309082 | KNN Loss: 3.6828761100769043 | BCE Loss: 1.008333444595337\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.69630241394043 | KNN Loss: 3.6974663734436035 | BCE Loss: 0.9988360404968262\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.759498119354248 | KNN Loss: 3.7136785984039307 | BCE Loss: 1.0458195209503174\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.735116481781006 | KNN Loss: 3.6913681030273438 | BCE Loss: 1.0437482595443726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.706297397613525 | KNN Loss: 3.695859909057617 | BCE Loss: 1.0104373693466187\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.726670742034912 | KNN Loss: 3.6705219745635986 | BCE Loss: 1.0561487674713135\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.730268478393555 | KNN Loss: 3.7021918296813965 | BCE Loss: 1.0280765295028687\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.704137325286865 | KNN Loss: 3.700000047683716 | BCE Loss: 1.0041372776031494\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.699563503265381 | KNN Loss: 3.6826281547546387 | BCE Loss: 1.0169353485107422\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.741819381713867 | KNN Loss: 3.6940155029296875 | BCE Loss: 1.0478041172027588\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.686394691467285 | KNN Loss: 3.6744132041931152 | BCE Loss: 1.011981725692749\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.7060394287109375 | KNN Loss: 3.6863229274749756 | BCE Loss: 1.019716501235962\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.783398628234863 | KNN Loss: 3.7207396030426025 | BCE Loss: 1.0626589059829712\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.7199201583862305 | KNN Loss: 3.693584442138672 | BCE Loss: 1.0263357162475586\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.7110276222229 | KNN Loss: 3.7021865844726562 | BCE Loss: 1.0088409185409546\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.664717674255371 | KNN Loss: 3.6680960655212402 | BCE Loss: 0.99662184715271\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.6827521324157715 | KNN Loss: 3.6612682342529297 | BCE Loss: 1.0214838981628418\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.704632759094238 | KNN Loss: 3.6685962677001953 | BCE Loss: 1.036036729812622\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.71562385559082 | KNN Loss: 3.7114486694335938 | BCE Loss: 1.0041751861572266\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.746695518493652 | KNN Loss: 3.715599775314331 | BCE Loss: 1.0310956239700317\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.686330795288086 | KNN Loss: 3.6813957691192627 | BCE Loss: 1.0049350261688232\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.742640495300293 | KNN Loss: 3.692903757095337 | BCE Loss: 1.0497368574142456\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.742265701293945 | KNN Loss: 3.689720630645752 | BCE Loss: 1.0525448322296143\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.7472968101501465 | KNN Loss: 3.7221875190734863 | BCE Loss: 1.0251092910766602\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.76922607421875 | KNN Loss: 3.725909471511841 | BCE Loss: 1.0433168411254883\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.706526756286621 | KNN Loss: 3.6979010105133057 | BCE Loss: 1.0086257457733154\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.764967918395996 | KNN Loss: 3.704047918319702 | BCE Loss: 1.060920238494873\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.734922409057617 | KNN Loss: 3.687450408935547 | BCE Loss: 1.0474722385406494\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.691766262054443 | KNN Loss: 3.682857036590576 | BCE Loss: 1.0089091062545776\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.708719253540039 | KNN Loss: 3.6636576652526855 | BCE Loss: 1.0450613498687744\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.765069484710693 | KNN Loss: 3.727128505706787 | BCE Loss: 1.0379409790039062\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.69595193862915 | KNN Loss: 3.667210578918457 | BCE Loss: 1.0287413597106934\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.738891124725342 | KNN Loss: 3.694101333618164 | BCE Loss: 1.0447897911071777\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.7305779457092285 | KNN Loss: 3.6990277767181396 | BCE Loss: 1.0315500497817993\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.719748497009277 | KNN Loss: 3.7051990032196045 | BCE Loss: 1.0145496129989624\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.759641647338867 | KNN Loss: 3.724017381668091 | BCE Loss: 1.0356242656707764\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.710514068603516 | KNN Loss: 3.6810526847839355 | BCE Loss: 1.0294615030288696\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.676535129547119 | KNN Loss: 3.659726142883301 | BCE Loss: 1.0168088674545288\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.746814250946045 | KNN Loss: 3.695507526397705 | BCE Loss: 1.0513068437576294\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.699963569641113 | KNN Loss: 3.6600914001464844 | BCE Loss: 1.0398719310760498\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.6802077293396 | KNN Loss: 3.677804708480835 | BCE Loss: 1.002402901649475\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.701896667480469 | KNN Loss: 3.693939447402954 | BCE Loss: 1.007957100868225\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.7303466796875 | KNN Loss: 3.719649076461792 | BCE Loss: 1.010697603225708\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.74621057510376 | KNN Loss: 3.6990981101989746 | BCE Loss: 1.0471123456954956\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.736720561981201 | KNN Loss: 3.6895294189453125 | BCE Loss: 1.0471910238265991\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.711205005645752 | KNN Loss: 3.683504581451416 | BCE Loss: 1.027700424194336\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.69943904876709 | KNN Loss: 3.6933016777038574 | BCE Loss: 1.0061373710632324\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.708110332489014 | KNN Loss: 3.707324981689453 | BCE Loss: 1.0007853507995605\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.773619174957275 | KNN Loss: 3.7496225833892822 | BCE Loss: 1.0239964723587036\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.7202959060668945 | KNN Loss: 3.6961402893066406 | BCE Loss: 1.024155855178833\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.729623317718506 | KNN Loss: 3.7287800312042236 | BCE Loss: 1.0008431673049927\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.707306861877441 | KNN Loss: 3.700115442276001 | BCE Loss: 1.0071911811828613\n",
      "Epoch   148: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.7183332443237305 | KNN Loss: 3.7107667922973633 | BCE Loss: 1.0075664520263672\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.754371643066406 | KNN Loss: 3.7122089862823486 | BCE Loss: 1.042162537574768\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.759037017822266 | KNN Loss: 3.7133212089538574 | BCE Loss: 1.045715570449829\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.698846817016602 | KNN Loss: 3.689765691757202 | BCE Loss: 1.009081244468689\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.732642650604248 | KNN Loss: 3.7222397327423096 | BCE Loss: 1.010402798652649\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.728084564208984 | KNN Loss: 3.713961601257324 | BCE Loss: 1.0141229629516602\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.755232334136963 | KNN Loss: 3.697474241256714 | BCE Loss: 1.057758092880249\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.700440406799316 | KNN Loss: 3.6953465938568115 | BCE Loss: 1.0050939321517944\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.727114677429199 | KNN Loss: 3.707409143447876 | BCE Loss: 1.0197056531906128\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.757983684539795 | KNN Loss: 3.7172083854675293 | BCE Loss: 1.0407752990722656\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.67711877822876 | KNN Loss: 3.6716866493225098 | BCE Loss: 1.0054322481155396\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.715512752532959 | KNN Loss: 3.6841800212860107 | BCE Loss: 1.0313327312469482\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.6887054443359375 | KNN Loss: 3.6804492473602295 | BCE Loss: 1.0082560777664185\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.711607933044434 | KNN Loss: 3.698216199874878 | BCE Loss: 1.0133918523788452\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.716146469116211 | KNN Loss: 3.693284749984741 | BCE Loss: 1.0228619575500488\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.725556373596191 | KNN Loss: 3.6931159496307373 | BCE Loss: 1.032440185546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.758878707885742 | KNN Loss: 3.700343132019043 | BCE Loss: 1.0585356950759888\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.734784126281738 | KNN Loss: 3.7115416526794434 | BCE Loss: 1.0232425928115845\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.756502628326416 | KNN Loss: 3.7077901363372803 | BCE Loss: 1.0487124919891357\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.788705825805664 | KNN Loss: 3.769190549850464 | BCE Loss: 1.0195152759552002\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.681026458740234 | KNN Loss: 3.6699345111846924 | BCE Loss: 1.011091709136963\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.7496538162231445 | KNN Loss: 3.7344610691070557 | BCE Loss: 1.0151928663253784\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.742631435394287 | KNN Loss: 3.6847493648529053 | BCE Loss: 1.0578820705413818\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.704981803894043 | KNN Loss: 3.7180047035217285 | BCE Loss: 0.986977219581604\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.721263885498047 | KNN Loss: 3.6830718517303467 | BCE Loss: 1.0381922721862793\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.752260208129883 | KNN Loss: 3.7168002128601074 | BCE Loss: 1.0354599952697754\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.730883598327637 | KNN Loss: 3.7121787071228027 | BCE Loss: 1.0187046527862549\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.674131870269775 | KNN Loss: 3.6601223945617676 | BCE Loss: 1.0140094757080078\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.7120280265808105 | KNN Loss: 3.6955955028533936 | BCE Loss: 1.016432523727417\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.729108810424805 | KNN Loss: 3.7253458499908447 | BCE Loss: 1.00376296043396\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.737424850463867 | KNN Loss: 3.715985059738159 | BCE Loss: 1.021439790725708\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.703413009643555 | KNN Loss: 3.701605796813965 | BCE Loss: 1.0018070936203003\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.7218918800354 | KNN Loss: 3.6952452659606934 | BCE Loss: 1.0266464948654175\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.685281276702881 | KNN Loss: 3.660938262939453 | BCE Loss: 1.0243428945541382\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.764031410217285 | KNN Loss: 3.704885244369507 | BCE Loss: 1.0591459274291992\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.722347259521484 | KNN Loss: 3.69252872467041 | BCE Loss: 1.0298185348510742\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.717912673950195 | KNN Loss: 3.7013602256774902 | BCE Loss: 1.0165523290634155\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.7291364669799805 | KNN Loss: 3.6982645988464355 | BCE Loss: 1.0308717489242554\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.700356960296631 | KNN Loss: 3.6889865398406982 | BCE Loss: 1.0113705396652222\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.701167583465576 | KNN Loss: 3.6682865619659424 | BCE Loss: 1.0328810214996338\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.674781322479248 | KNN Loss: 3.659175395965576 | BCE Loss: 1.0156059265136719\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.675168037414551 | KNN Loss: 3.6722607612609863 | BCE Loss: 1.002907156944275\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.720568656921387 | KNN Loss: 3.7164018154144287 | BCE Loss: 1.004167079925537\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.689758777618408 | KNN Loss: 3.675147771835327 | BCE Loss: 1.0146108865737915\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.724045276641846 | KNN Loss: 3.6824958324432373 | BCE Loss: 1.041549563407898\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.763578414916992 | KNN Loss: 3.7421875 | BCE Loss: 1.0213911533355713\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.725962162017822 | KNN Loss: 3.7109768390655518 | BCE Loss: 1.014985203742981\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.750219345092773 | KNN Loss: 3.7251434326171875 | BCE Loss: 1.025076150894165\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.710579872131348 | KNN Loss: 3.676466464996338 | BCE Loss: 1.0341135263442993\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.724842548370361 | KNN Loss: 3.704988956451416 | BCE Loss: 1.0198535919189453\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.709841251373291 | KNN Loss: 3.685559034347534 | BCE Loss: 1.0242822170257568\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.708481311798096 | KNN Loss: 3.6659629344940186 | BCE Loss: 1.0425184965133667\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.733342170715332 | KNN Loss: 3.6836774349212646 | BCE Loss: 1.0496644973754883\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.721560955047607 | KNN Loss: 3.6998441219329834 | BCE Loss: 1.0217169523239136\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.730853080749512 | KNN Loss: 3.7038683891296387 | BCE Loss: 1.0269849300384521\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.730642318725586 | KNN Loss: 3.71583890914917 | BCE Loss: 1.0148036479949951\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.711427688598633 | KNN Loss: 3.6836509704589844 | BCE Loss: 1.0277765989303589\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.700369834899902 | KNN Loss: 3.6813440322875977 | BCE Loss: 1.0190258026123047\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.737480163574219 | KNN Loss: 3.7088637351989746 | BCE Loss: 1.0286163091659546\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.689554214477539 | KNN Loss: 3.6752841472625732 | BCE Loss: 1.014270305633545\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.754827976226807 | KNN Loss: 3.7258877754211426 | BCE Loss: 1.028940200805664\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.735829830169678 | KNN Loss: 3.711984872817993 | BCE Loss: 1.0238450765609741\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.736144542694092 | KNN Loss: 3.69937801361084 | BCE Loss: 1.036766529083252\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.716076850891113 | KNN Loss: 3.7029664516448975 | BCE Loss: 1.0131101608276367\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.710064888000488 | KNN Loss: 3.6869606971740723 | BCE Loss: 1.0231044292449951\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.70177698135376 | KNN Loss: 3.692448854446411 | BCE Loss: 1.009328007698059\n",
      "Epoch   159: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.758484840393066 | KNN Loss: 3.7184879779815674 | BCE Loss: 1.03999662399292\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.702681541442871 | KNN Loss: 3.6913535594940186 | BCE Loss: 1.0113279819488525\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.737541198730469 | KNN Loss: 3.7177045345306396 | BCE Loss: 1.0198367834091187\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.697842121124268 | KNN Loss: 3.6955795288085938 | BCE Loss: 1.0022627115249634\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.72122859954834 | KNN Loss: 3.686372995376587 | BCE Loss: 1.034855604171753\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.6641845703125 | KNN Loss: 3.65641713142395 | BCE Loss: 1.0077672004699707\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.759829998016357 | KNN Loss: 3.727778911590576 | BCE Loss: 1.0320510864257812\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.686196327209473 | KNN Loss: 3.6533761024475098 | BCE Loss: 1.0328199863433838\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.6994123458862305 | KNN Loss: 3.690487861633301 | BCE Loss: 1.0089246034622192\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.6615705490112305 | KNN Loss: 3.66912579536438 | BCE Loss: 0.9924449920654297\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.739710330963135 | KNN Loss: 3.71736478805542 | BCE Loss: 1.0223456621170044\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.6951165199279785 | KNN Loss: 3.6854193210601807 | BCE Loss: 1.0096971988677979\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.696120262145996 | KNN Loss: 3.6813580989837646 | BCE Loss: 1.014762282371521\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.722097396850586 | KNN Loss: 3.677786111831665 | BCE Loss: 1.044311285018921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.737645149230957 | KNN Loss: 3.7296502590179443 | BCE Loss: 1.0079951286315918\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.715563774108887 | KNN Loss: 3.686504364013672 | BCE Loss: 1.0290594100952148\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.7935471534729 | KNN Loss: 3.741656541824341 | BCE Loss: 1.0518906116485596\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.704719066619873 | KNN Loss: 3.703517198562622 | BCE Loss: 1.001201868057251\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.721606731414795 | KNN Loss: 3.7056291103363037 | BCE Loss: 1.0159775018692017\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.732273101806641 | KNN Loss: 3.7069590091705322 | BCE Loss: 1.0253138542175293\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.764775276184082 | KNN Loss: 3.7357966899871826 | BCE Loss: 1.0289788246154785\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.785272121429443 | KNN Loss: 3.7388341426849365 | BCE Loss: 1.0464378595352173\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.664843559265137 | KNN Loss: 3.6572797298431396 | BCE Loss: 1.0075637102127075\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.715308666229248 | KNN Loss: 3.711171865463257 | BCE Loss: 1.0041366815567017\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.690164089202881 | KNN Loss: 3.662717580795288 | BCE Loss: 1.0274463891983032\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.743602275848389 | KNN Loss: 3.7140512466430664 | BCE Loss: 1.0295511484146118\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.7219133377075195 | KNN Loss: 3.707994222640991 | BCE Loss: 1.0139191150665283\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.720462799072266 | KNN Loss: 3.692202568054199 | BCE Loss: 1.0282602310180664\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.687684059143066 | KNN Loss: 3.6829702854156494 | BCE Loss: 1.004714012145996\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.719108581542969 | KNN Loss: 3.677656650543213 | BCE Loss: 1.041452169418335\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.687582969665527 | KNN Loss: 3.672759771347046 | BCE Loss: 1.0148229598999023\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.715450286865234 | KNN Loss: 3.6810312271118164 | BCE Loss: 1.034419059753418\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.732229709625244 | KNN Loss: 3.688279867172241 | BCE Loss: 1.0439499616622925\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.731799602508545 | KNN Loss: 3.7251675128936768 | BCE Loss: 1.0066319704055786\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.805515289306641 | KNN Loss: 3.769900321960449 | BCE Loss: 1.035615086555481\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.753138542175293 | KNN Loss: 3.715266466140747 | BCE Loss: 1.037872076034546\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.77874755859375 | KNN Loss: 3.7200443744659424 | BCE Loss: 1.0587029457092285\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.7426910400390625 | KNN Loss: 3.6913444995880127 | BCE Loss: 1.0513463020324707\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.715886116027832 | KNN Loss: 3.6923980712890625 | BCE Loss: 1.0234878063201904\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.7225847244262695 | KNN Loss: 3.727762460708618 | BCE Loss: 0.994822084903717\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.684092044830322 | KNN Loss: 3.6760661602020264 | BCE Loss: 1.0080257654190063\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.728968620300293 | KNN Loss: 3.716999053955078 | BCE Loss: 1.0119693279266357\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.722722053527832 | KNN Loss: 3.6823174953460693 | BCE Loss: 1.0404043197631836\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.67124080657959 | KNN Loss: 3.6833958625793457 | BCE Loss: 0.9878448247909546\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.728592872619629 | KNN Loss: 3.6877517700195312 | BCE Loss: 1.0408411026000977\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.69156551361084 | KNN Loss: 3.66607403755188 | BCE Loss: 1.0254912376403809\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.706720352172852 | KNN Loss: 3.6732683181762695 | BCE Loss: 1.033451795578003\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.731762409210205 | KNN Loss: 3.7142438888549805 | BCE Loss: 1.0175186395645142\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.717973232269287 | KNN Loss: 3.6645491123199463 | BCE Loss: 1.0534242391586304\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.7255425453186035 | KNN Loss: 3.6930124759674072 | BCE Loss: 1.0325300693511963\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.712385177612305 | KNN Loss: 3.6881089210510254 | BCE Loss: 1.0242760181427002\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.682706356048584 | KNN Loss: 3.685298442840576 | BCE Loss: 0.9974080920219421\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.706520080566406 | KNN Loss: 3.687532424926758 | BCE Loss: 1.0189876556396484\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.7571635246276855 | KNN Loss: 3.717841863632202 | BCE Loss: 1.0393215417861938\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.712041854858398 | KNN Loss: 3.6803903579711914 | BCE Loss: 1.031651496887207\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.681888580322266 | KNN Loss: 3.6724486351013184 | BCE Loss: 1.0094397068023682\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.73028564453125 | KNN Loss: 3.6815521717071533 | BCE Loss: 1.0487335920333862\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.72103214263916 | KNN Loss: 3.6982219219207764 | BCE Loss: 1.0228099822998047\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.7684783935546875 | KNN Loss: 3.7424447536468506 | BCE Loss: 1.0260334014892578\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.718930721282959 | KNN Loss: 3.7031257152557373 | BCE Loss: 1.0158051252365112\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.697971343994141 | KNN Loss: 3.695129632949829 | BCE Loss: 1.0028419494628906\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.742902755737305 | KNN Loss: 3.7115838527679443 | BCE Loss: 1.03131902217865\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.685113430023193 | KNN Loss: 3.6835339069366455 | BCE Loss: 1.0015795230865479\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.710989952087402 | KNN Loss: 3.699679374694824 | BCE Loss: 1.0113104581832886\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.700957298278809 | KNN Loss: 3.657400608062744 | BCE Loss: 1.043556809425354\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.755711078643799 | KNN Loss: 3.709160327911377 | BCE Loss: 1.0465508699417114\n",
      "Epoch   170: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.745582580566406 | KNN Loss: 3.694164276123047 | BCE Loss: 1.0514185428619385\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.712441444396973 | KNN Loss: 3.6928229331970215 | BCE Loss: 1.0196185111999512\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.747213363647461 | KNN Loss: 3.7133584022521973 | BCE Loss: 1.0338550806045532\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.7733540534973145 | KNN Loss: 3.7150142192840576 | BCE Loss: 1.0583399534225464\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.744198799133301 | KNN Loss: 3.7289774417877197 | BCE Loss: 1.0152215957641602\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.720624923706055 | KNN Loss: 3.6790754795074463 | BCE Loss: 1.0415494441986084\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.72921085357666 | KNN Loss: 3.7031118869781494 | BCE Loss: 1.0260990858078003\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.72263240814209 | KNN Loss: 3.6905574798583984 | BCE Loss: 1.0320751667022705\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.69357967376709 | KNN Loss: 3.6561086177825928 | BCE Loss: 1.037470817565918\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.676846027374268 | KNN Loss: 3.6617941856384277 | BCE Loss: 1.0150517225265503\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.727679252624512 | KNN Loss: 3.7130484580993652 | BCE Loss: 1.0146310329437256\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.719510078430176 | KNN Loss: 3.708113670349121 | BCE Loss: 1.0113965272903442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.663305282592773 | KNN Loss: 3.6540133953094482 | BCE Loss: 1.009291648864746\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.718652725219727 | KNN Loss: 3.712099075317383 | BCE Loss: 1.0065534114837646\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.698805809020996 | KNN Loss: 3.6795685291290283 | BCE Loss: 1.0192371606826782\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.670957565307617 | KNN Loss: 3.667003631591797 | BCE Loss: 1.0039538145065308\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.6957526206970215 | KNN Loss: 3.670919179916382 | BCE Loss: 1.02483332157135\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.711999893188477 | KNN Loss: 3.702652931213379 | BCE Loss: 1.0093467235565186\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.7630767822265625 | KNN Loss: 3.7098870277404785 | BCE Loss: 1.0531895160675049\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.769441604614258 | KNN Loss: 3.734678268432617 | BCE Loss: 1.034763216972351\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.814406871795654 | KNN Loss: 3.746748208999634 | BCE Loss: 1.0676586627960205\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.778345108032227 | KNN Loss: 3.7202069759368896 | BCE Loss: 1.0581382513046265\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.76911735534668 | KNN Loss: 3.7255163192749023 | BCE Loss: 1.0436010360717773\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.777643203735352 | KNN Loss: 3.7217423915863037 | BCE Loss: 1.0559008121490479\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.78042459487915 | KNN Loss: 3.7337377071380615 | BCE Loss: 1.0466867685317993\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.671267032623291 | KNN Loss: 3.6680150032043457 | BCE Loss: 1.0032519102096558\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.682287216186523 | KNN Loss: 3.6750032901763916 | BCE Loss: 1.0072836875915527\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.648780345916748 | KNN Loss: 3.6527011394500732 | BCE Loss: 0.9960792064666748\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.726426124572754 | KNN Loss: 3.6997203826904297 | BCE Loss: 1.0267058610916138\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.699535369873047 | KNN Loss: 3.6669111251831055 | BCE Loss: 1.032624363899231\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.690475940704346 | KNN Loss: 3.6809275150299072 | BCE Loss: 1.0095484256744385\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.735960006713867 | KNN Loss: 3.684134006500244 | BCE Loss: 1.0518261194229126\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.669200897216797 | KNN Loss: 3.6699302196502686 | BCE Loss: 0.9992707967758179\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.71933650970459 | KNN Loss: 3.7271411418914795 | BCE Loss: 0.9921954870223999\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.670846939086914 | KNN Loss: 3.6686899662017822 | BCE Loss: 1.0021570920944214\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.701794624328613 | KNN Loss: 3.6789727210998535 | BCE Loss: 1.0228221416473389\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.735560894012451 | KNN Loss: 3.7078256607055664 | BCE Loss: 1.0277352333068848\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.721017837524414 | KNN Loss: 3.697402000427246 | BCE Loss: 1.023615837097168\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.706326961517334 | KNN Loss: 3.6744866371154785 | BCE Loss: 1.0318403244018555\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.691123008728027 | KNN Loss: 3.6831119060516357 | BCE Loss: 1.008010983467102\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.716675281524658 | KNN Loss: 3.6878302097320557 | BCE Loss: 1.0288450717926025\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.741758346557617 | KNN Loss: 3.7259938716888428 | BCE Loss: 1.0157644748687744\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.7435760498046875 | KNN Loss: 3.7179384231567383 | BCE Loss: 1.0256375074386597\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.734981536865234 | KNN Loss: 3.7199671268463135 | BCE Loss: 1.0150146484375\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.829492568969727 | KNN Loss: 3.743711233139038 | BCE Loss: 1.0857815742492676\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.747467994689941 | KNN Loss: 3.6994025707244873 | BCE Loss: 1.0480653047561646\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.675528049468994 | KNN Loss: 3.663651943206787 | BCE Loss: 1.011876106262207\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.697679042816162 | KNN Loss: 3.6770968437194824 | BCE Loss: 1.0205821990966797\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.711794853210449 | KNN Loss: 3.6758804321289062 | BCE Loss: 1.0359145402908325\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.751525402069092 | KNN Loss: 3.71773362159729 | BCE Loss: 1.0337917804718018\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.6892924308776855 | KNN Loss: 3.7012224197387695 | BCE Loss: 0.9880701303482056\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.753872871398926 | KNN Loss: 3.727076530456543 | BCE Loss: 1.026796579360962\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.764326095581055 | KNN Loss: 3.7260818481445312 | BCE Loss: 1.038244366645813\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.722619533538818 | KNN Loss: 3.707702159881592 | BCE Loss: 1.0149173736572266\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.756904125213623 | KNN Loss: 3.7512640953063965 | BCE Loss: 1.005639910697937\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.752389907836914 | KNN Loss: 3.718501329421997 | BCE Loss: 1.0338886976242065\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.710171699523926 | KNN Loss: 3.6687870025634766 | BCE Loss: 1.0413846969604492\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.715632915496826 | KNN Loss: 3.6951680183410645 | BCE Loss: 1.0204650163650513\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.723638534545898 | KNN Loss: 3.711137056350708 | BCE Loss: 1.0125014781951904\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.7349419593811035 | KNN Loss: 3.7217495441436768 | BCE Loss: 1.0131925344467163\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.66475772857666 | KNN Loss: 3.6546719074249268 | BCE Loss: 1.0100858211517334\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.670808792114258 | KNN Loss: 3.671827793121338 | BCE Loss: 0.998981237411499\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.6781907081604 | KNN Loss: 3.698349952697754 | BCE Loss: 0.9798406958580017\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.742379665374756 | KNN Loss: 3.7252001762390137 | BCE Loss: 1.0171796083450317\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.762331008911133 | KNN Loss: 3.7250289916992188 | BCE Loss: 1.0373018980026245\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.721610069274902 | KNN Loss: 3.6930270195007324 | BCE Loss: 1.0285828113555908\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.713253498077393 | KNN Loss: 3.704482078552246 | BCE Loss: 1.0087714195251465\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.702991485595703 | KNN Loss: 3.6915652751922607 | BCE Loss: 1.0114264488220215\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.742631435394287 | KNN Loss: 3.7102391719818115 | BCE Loss: 1.0323922634124756\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.684869766235352 | KNN Loss: 3.6855382919311523 | BCE Loss: 0.9993314743041992\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.735742568969727 | KNN Loss: 3.7007718086242676 | BCE Loss: 1.034970998764038\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.738685607910156 | KNN Loss: 3.6943860054016113 | BCE Loss: 1.044299840927124\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.710081100463867 | KNN Loss: 3.679692268371582 | BCE Loss: 1.0303888320922852\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.720060348510742 | KNN Loss: 3.702366828918457 | BCE Loss: 1.0176937580108643\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.707382678985596 | KNN Loss: 3.6851806640625 | BCE Loss: 1.0222021341323853\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.775477886199951 | KNN Loss: 3.732699155807495 | BCE Loss: 1.0427788496017456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.740086078643799 | KNN Loss: 3.6989693641662598 | BCE Loss: 1.0411168336868286\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.734223365783691 | KNN Loss: 3.696307420730591 | BCE Loss: 1.0379161834716797\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.7114152908325195 | KNN Loss: 3.6851110458374023 | BCE Loss: 1.0263042449951172\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.679931163787842 | KNN Loss: 3.699855327606201 | BCE Loss: 0.980076014995575\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.713754177093506 | KNN Loss: 3.6949334144592285 | BCE Loss: 1.018820881843567\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.775022506713867 | KNN Loss: 3.764246940612793 | BCE Loss: 1.0107755661010742\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.732752799987793 | KNN Loss: 3.715127468109131 | BCE Loss: 1.017625093460083\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.701748371124268 | KNN Loss: 3.6657233238220215 | BCE Loss: 1.0360249280929565\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.7474775314331055 | KNN Loss: 3.7062015533447266 | BCE Loss: 1.0412757396697998\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.699270248413086 | KNN Loss: 3.6889426708221436 | BCE Loss: 1.010327696800232\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.724040985107422 | KNN Loss: 3.7019476890563965 | BCE Loss: 1.0220935344696045\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.736617088317871 | KNN Loss: 3.6774890422821045 | BCE Loss: 1.0591278076171875\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.787768840789795 | KNN Loss: 3.7278616428375244 | BCE Loss: 1.0599071979522705\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.702372074127197 | KNN Loss: 3.6756644248962402 | BCE Loss: 1.0267075300216675\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.761050701141357 | KNN Loss: 3.763341188430786 | BCE Loss: 0.9977095127105713\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.734250068664551 | KNN Loss: 3.700998544692993 | BCE Loss: 1.0332512855529785\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.735212326049805 | KNN Loss: 3.7088687419891357 | BCE Loss: 1.0263433456420898\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.736795425415039 | KNN Loss: 3.6867117881774902 | BCE Loss: 1.0500835180282593\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.683621406555176 | KNN Loss: 3.683926582336426 | BCE Loss: 0.9996949434280396\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.719367027282715 | KNN Loss: 3.698789119720459 | BCE Loss: 1.0205776691436768\n",
      "Epoch   186: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.718585968017578 | KNN Loss: 3.677006483078003 | BCE Loss: 1.0415797233581543\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.654776096343994 | KNN Loss: 3.665667772293091 | BCE Loss: 0.9891084432601929\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.721279144287109 | KNN Loss: 3.6893861293792725 | BCE Loss: 1.0318927764892578\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.6986541748046875 | KNN Loss: 3.671330451965332 | BCE Loss: 1.027323603630066\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.7822465896606445 | KNN Loss: 3.7385027408599854 | BCE Loss: 1.0437439680099487\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.726197719573975 | KNN Loss: 3.716623544692993 | BCE Loss: 1.009574055671692\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.791237831115723 | KNN Loss: 3.717989206314087 | BCE Loss: 1.0732488632202148\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.679630756378174 | KNN Loss: 3.661989688873291 | BCE Loss: 1.0176409482955933\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.6832594871521 | KNN Loss: 3.667968273162842 | BCE Loss: 1.0152912139892578\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.725628852844238 | KNN Loss: 3.691721200942993 | BCE Loss: 1.033907413482666\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.786711692810059 | KNN Loss: 3.7433035373687744 | BCE Loss: 1.0434083938598633\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.75620698928833 | KNN Loss: 3.7167112827301025 | BCE Loss: 1.039495587348938\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.742869853973389 | KNN Loss: 3.7010021209716797 | BCE Loss: 1.041867733001709\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.67796516418457 | KNN Loss: 3.6691155433654785 | BCE Loss: 1.0088497400283813\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.7185258865356445 | KNN Loss: 3.6998143196105957 | BCE Loss: 1.0187113285064697\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.725011825561523 | KNN Loss: 3.6678569316864014 | BCE Loss: 1.0571551322937012\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.685380935668945 | KNN Loss: 3.66795015335083 | BCE Loss: 1.0174305438995361\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.758275985717773 | KNN Loss: 3.7245683670043945 | BCE Loss: 1.033707618713379\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.737698554992676 | KNN Loss: 3.712862253189087 | BCE Loss: 1.0248364210128784\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.702010631561279 | KNN Loss: 3.700035333633423 | BCE Loss: 1.001975178718567\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.786722183227539 | KNN Loss: 3.7329607009887695 | BCE Loss: 1.0537612438201904\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.714383602142334 | KNN Loss: 3.6940743923187256 | BCE Loss: 1.0203090906143188\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.693840980529785 | KNN Loss: 3.6813950538635254 | BCE Loss: 1.0124456882476807\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.7120466232299805 | KNN Loss: 3.6778194904327393 | BCE Loss: 1.0342270135879517\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.71622371673584 | KNN Loss: 3.6734390258789062 | BCE Loss: 1.042784571647644\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.658783912658691 | KNN Loss: 3.663818597793579 | BCE Loss: 0.9949650764465332\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.684422016143799 | KNN Loss: 3.686675786972046 | BCE Loss: 0.9977462291717529\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.721030235290527 | KNN Loss: 3.6803581714630127 | BCE Loss: 1.040671944618225\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.735668659210205 | KNN Loss: 3.6887669563293457 | BCE Loss: 1.0469017028808594\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.732672691345215 | KNN Loss: 3.68575119972229 | BCE Loss: 1.0469212532043457\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.6585798263549805 | KNN Loss: 3.6494951248168945 | BCE Loss: 1.0090844631195068\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.696293830871582 | KNN Loss: 3.6889841556549072 | BCE Loss: 1.0073097944259644\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.693351745605469 | KNN Loss: 3.6663811206817627 | BCE Loss: 1.0269708633422852\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.713906288146973 | KNN Loss: 3.7101452350616455 | BCE Loss: 1.0037610530853271\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.733351707458496 | KNN Loss: 3.6981582641601562 | BCE Loss: 1.035193681716919\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.734498500823975 | KNN Loss: 3.7146449089050293 | BCE Loss: 1.0198535919189453\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.7421417236328125 | KNN Loss: 3.7059807777404785 | BCE Loss: 1.0361610651016235\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.712054252624512 | KNN Loss: 3.681575059890747 | BCE Loss: 1.0304791927337646\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.675482749938965 | KNN Loss: 3.6793479919433594 | BCE Loss: 0.9961345195770264\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.754776477813721 | KNN Loss: 3.724332571029663 | BCE Loss: 1.0304440259933472\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.742284297943115 | KNN Loss: 3.6901533603668213 | BCE Loss: 1.052130937576294\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.7487030029296875 | KNN Loss: 3.71393084526062 | BCE Loss: 1.034772276878357\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.721169948577881 | KNN Loss: 3.700265884399414 | BCE Loss: 1.0209039449691772\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.706412315368652 | KNN Loss: 3.6977479457855225 | BCE Loss: 1.008664608001709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.723447322845459 | KNN Loss: 3.6960062980651855 | BCE Loss: 1.0274409055709839\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.733638286590576 | KNN Loss: 3.676910161972046 | BCE Loss: 1.0567281246185303\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.7095489501953125 | KNN Loss: 3.6756317615509033 | BCE Loss: 1.0339170694351196\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.704442501068115 | KNN Loss: 3.692896604537964 | BCE Loss: 1.0115458965301514\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.6921162605285645 | KNN Loss: 3.688992738723755 | BCE Loss: 1.0031235218048096\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.690792083740234 | KNN Loss: 3.66967511177063 | BCE Loss: 1.0211167335510254\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.671422004699707 | KNN Loss: 3.666356086730957 | BCE Loss: 1.005065679550171\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.724226474761963 | KNN Loss: 3.686429262161255 | BCE Loss: 1.037797212600708\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.687007904052734 | KNN Loss: 3.6784119606018066 | BCE Loss: 1.0085960626602173\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.730821132659912 | KNN Loss: 3.677542209625244 | BCE Loss: 1.0532788038253784\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.754700660705566 | KNN Loss: 3.7269012928009033 | BCE Loss: 1.0277994871139526\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.74859619140625 | KNN Loss: 3.730794668197632 | BCE Loss: 1.017801284790039\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.726659774780273 | KNN Loss: 3.6952686309814453 | BCE Loss: 1.0313911437988281\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.685081481933594 | KNN Loss: 3.675255537033081 | BCE Loss: 1.0098257064819336\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.718522548675537 | KNN Loss: 3.6971633434295654 | BCE Loss: 1.0213592052459717\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.738265037536621 | KNN Loss: 3.6946189403533936 | BCE Loss: 1.043645977973938\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.692680358886719 | KNN Loss: 3.6569530963897705 | BCE Loss: 1.0357275009155273\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.648894786834717 | KNN Loss: 3.6415932178497314 | BCE Loss: 1.0073014497756958\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.743539333343506 | KNN Loss: 3.7037651538848877 | BCE Loss: 1.0397742986679077\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.732413291931152 | KNN Loss: 3.6932859420776367 | BCE Loss: 1.0391271114349365\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.685194492340088 | KNN Loss: 3.656691789627075 | BCE Loss: 1.0285027027130127\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.702168941497803 | KNN Loss: 3.6717286109924316 | BCE Loss: 1.030440330505371\n",
      "Epoch   197: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.706011772155762 | KNN Loss: 3.687054395675659 | BCE Loss: 1.018957495689392\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.648804187774658 | KNN Loss: 3.65084171295166 | BCE Loss: 0.9979623556137085\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.690207004547119 | KNN Loss: 3.683000087738037 | BCE Loss: 1.0072070360183716\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.693751335144043 | KNN Loss: 3.66494083404541 | BCE Loss: 1.0288106203079224\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.732497692108154 | KNN Loss: 3.696554660797119 | BCE Loss: 1.0359429121017456\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.761319637298584 | KNN Loss: 3.7108535766601562 | BCE Loss: 1.0504660606384277\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.720350742340088 | KNN Loss: 3.721982002258301 | BCE Loss: 0.9983686804771423\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.684216499328613 | KNN Loss: 3.6809024810791016 | BCE Loss: 1.0033140182495117\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.734278678894043 | KNN Loss: 3.7059459686279297 | BCE Loss: 1.0283328294754028\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.6983771324157715 | KNN Loss: 3.683405876159668 | BCE Loss: 1.014971375465393\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.721795082092285 | KNN Loss: 3.7121596336364746 | BCE Loss: 1.009635329246521\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.698248386383057 | KNN Loss: 3.70106840133667 | BCE Loss: 0.997180163860321\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.720818996429443 | KNN Loss: 3.6732685565948486 | BCE Loss: 1.0475504398345947\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.736185073852539 | KNN Loss: 3.7112820148468018 | BCE Loss: 1.0249029397964478\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.714972019195557 | KNN Loss: 3.679636240005493 | BCE Loss: 1.0353357791900635\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.732213020324707 | KNN Loss: 3.713635206222534 | BCE Loss: 1.0185776948928833\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.742780685424805 | KNN Loss: 3.6860055923461914 | BCE Loss: 1.0567749738693237\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.684891700744629 | KNN Loss: 3.6781413555145264 | BCE Loss: 1.0067501068115234\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.709596633911133 | KNN Loss: 3.6640472412109375 | BCE Loss: 1.0455496311187744\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.729374885559082 | KNN Loss: 3.7223105430603027 | BCE Loss: 1.0070643424987793\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.750235557556152 | KNN Loss: 3.723135232925415 | BCE Loss: 1.0271002054214478\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.690513610839844 | KNN Loss: 3.6760494709014893 | BCE Loss: 1.0144643783569336\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.691417694091797 | KNN Loss: 3.683330535888672 | BCE Loss: 1.008087158203125\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.714683532714844 | KNN Loss: 3.6660213470458984 | BCE Loss: 1.0486621856689453\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.74449348449707 | KNN Loss: 3.726041793823242 | BCE Loss: 1.0184518098831177\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.714622497558594 | KNN Loss: 3.6810965538024902 | BCE Loss: 1.0335261821746826\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.775769233703613 | KNN Loss: 3.7289862632751465 | BCE Loss: 1.0467827320098877\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.690685272216797 | KNN Loss: 3.6968226432800293 | BCE Loss: 0.9938627481460571\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.6783342361450195 | KNN Loss: 3.6637041568756104 | BCE Loss: 1.0146300792694092\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.695713043212891 | KNN Loss: 3.663952589035034 | BCE Loss: 1.0317602157592773\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.712586402893066 | KNN Loss: 3.7058639526367188 | BCE Loss: 1.006722331047058\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.71351432800293 | KNN Loss: 3.6841447353363037 | BCE Loss: 1.0293697118759155\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.688459873199463 | KNN Loss: 3.6683943271636963 | BCE Loss: 1.0200656652450562\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.7362961769104 | KNN Loss: 3.7137386798858643 | BCE Loss: 1.0225576162338257\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.7371826171875 | KNN Loss: 3.7066352367401123 | BCE Loss: 1.0305473804473877\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.709030628204346 | KNN Loss: 3.6819100379943848 | BCE Loss: 1.0271207094192505\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.66816520690918 | KNN Loss: 3.673532247543335 | BCE Loss: 0.9946331977844238\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.689756870269775 | KNN Loss: 3.6842594146728516 | BCE Loss: 1.0054974555969238\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.686748504638672 | KNN Loss: 3.6883625984191895 | BCE Loss: 0.9983859062194824\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.701351642608643 | KNN Loss: 3.6805055141448975 | BCE Loss: 1.0208462476730347\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.743651390075684 | KNN Loss: 3.7092933654785156 | BCE Loss: 1.0343579053878784\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.751147270202637 | KNN Loss: 3.7109591960906982 | BCE Loss: 1.0401883125305176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.7221245765686035 | KNN Loss: 3.692314386367798 | BCE Loss: 1.0298100709915161\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.7668938636779785 | KNN Loss: 3.7012853622436523 | BCE Loss: 1.0656083822250366\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.678687572479248 | KNN Loss: 3.671180009841919 | BCE Loss: 1.0075074434280396\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.723668098449707 | KNN Loss: 3.697073459625244 | BCE Loss: 1.0265947580337524\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.741018295288086 | KNN Loss: 3.7199437618255615 | BCE Loss: 1.0210745334625244\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.709545135498047 | KNN Loss: 3.688100814819336 | BCE Loss: 1.0214444398880005\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.7265825271606445 | KNN Loss: 3.7112069129943848 | BCE Loss: 1.0153757333755493\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.644457817077637 | KNN Loss: 3.671842575073242 | BCE Loss: 0.9726152420043945\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.738784313201904 | KNN Loss: 3.7033286094665527 | BCE Loss: 1.0354557037353516\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.747230052947998 | KNN Loss: 3.7393412590026855 | BCE Loss: 1.0078887939453125\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.711274147033691 | KNN Loss: 3.68709659576416 | BCE Loss: 1.0241775512695312\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.721653461456299 | KNN Loss: 3.684027671813965 | BCE Loss: 1.0376256704330444\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.786130905151367 | KNN Loss: 3.746974468231201 | BCE Loss: 1.0391565561294556\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.740894794464111 | KNN Loss: 3.680638551712036 | BCE Loss: 1.0602563619613647\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.749855041503906 | KNN Loss: 3.723811388015747 | BCE Loss: 1.0260438919067383\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.679121971130371 | KNN Loss: 3.675467014312744 | BCE Loss: 1.0036548376083374\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.797802925109863 | KNN Loss: 3.7479615211486816 | BCE Loss: 1.0498414039611816\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.7214813232421875 | KNN Loss: 3.6871068477630615 | BCE Loss: 1.0343742370605469\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.730922222137451 | KNN Loss: 3.685145616531372 | BCE Loss: 1.045776605606079\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.752758979797363 | KNN Loss: 3.721313953399658 | BCE Loss: 1.0314452648162842\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.699415683746338 | KNN Loss: 3.6747686862945557 | BCE Loss: 1.0246471166610718\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.692202091217041 | KNN Loss: 3.6698622703552246 | BCE Loss: 1.0223397016525269\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.717648029327393 | KNN Loss: 3.696220874786377 | BCE Loss: 1.021427035331726\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.676791191101074 | KNN Loss: 3.6571545600891113 | BCE Loss: 1.0196367502212524\n",
      "Epoch   208: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.708476543426514 | KNN Loss: 3.680772542953491 | BCE Loss: 1.027704119682312\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.707807540893555 | KNN Loss: 3.6811699867248535 | BCE Loss: 1.0266377925872803\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.728378772735596 | KNN Loss: 3.7036798000335693 | BCE Loss: 1.0246988534927368\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.709699630737305 | KNN Loss: 3.6916046142578125 | BCE Loss: 1.0180950164794922\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.730116844177246 | KNN Loss: 3.708172559738159 | BCE Loss: 1.021944522857666\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.700556755065918 | KNN Loss: 3.6749703884124756 | BCE Loss: 1.0255866050720215\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.7092156410217285 | KNN Loss: 3.690401554107666 | BCE Loss: 1.0188140869140625\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.70659065246582 | KNN Loss: 3.7022151947021484 | BCE Loss: 1.0043754577636719\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.723865985870361 | KNN Loss: 3.681997299194336 | BCE Loss: 1.0418686866760254\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.665891647338867 | KNN Loss: 3.662652015686035 | BCE Loss: 1.0032398700714111\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.7259650230407715 | KNN Loss: 3.6962978839874268 | BCE Loss: 1.0296671390533447\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.704342365264893 | KNN Loss: 3.666372537612915 | BCE Loss: 1.0379698276519775\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.712215900421143 | KNN Loss: 3.6938986778259277 | BCE Loss: 1.0183171033859253\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.730588436126709 | KNN Loss: 3.7021687030792236 | BCE Loss: 1.0284196138381958\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.758549690246582 | KNN Loss: 3.718830108642578 | BCE Loss: 1.0397193431854248\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.714015960693359 | KNN Loss: 3.7042863368988037 | BCE Loss: 1.0097298622131348\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.743948936462402 | KNN Loss: 3.709886074066162 | BCE Loss: 1.0340628623962402\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.705758094787598 | KNN Loss: 3.6993064880371094 | BCE Loss: 1.0064514875411987\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.722907543182373 | KNN Loss: 3.69132924079895 | BCE Loss: 1.0315783023834229\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.720862865447998 | KNN Loss: 3.684998035430908 | BCE Loss: 1.0358649492263794\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.7219109535217285 | KNN Loss: 3.6938695907592773 | BCE Loss: 1.0280412435531616\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.6264119148254395 | KNN Loss: 3.655716896057129 | BCE Loss: 0.970694899559021\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.709259510040283 | KNN Loss: 3.6906509399414062 | BCE Loss: 1.018608570098877\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.703912734985352 | KNN Loss: 3.6587436199188232 | BCE Loss: 1.0451693534851074\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.729544639587402 | KNN Loss: 3.6827266216278076 | BCE Loss: 1.0468180179595947\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.738664150238037 | KNN Loss: 3.7144293785095215 | BCE Loss: 1.0242348909378052\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.761293888092041 | KNN Loss: 3.70676326751709 | BCE Loss: 1.0545307397842407\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.745800018310547 | KNN Loss: 3.70198917388916 | BCE Loss: 1.0438110828399658\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.738703727722168 | KNN Loss: 3.708456039428711 | BCE Loss: 1.0302479267120361\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.750974655151367 | KNN Loss: 3.7285919189453125 | BCE Loss: 1.0223824977874756\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.733825206756592 | KNN Loss: 3.7125296592712402 | BCE Loss: 1.021295428276062\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.775783538818359 | KNN Loss: 3.7445945739746094 | BCE Loss: 1.0311888456344604\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.715508460998535 | KNN Loss: 3.672787666320801 | BCE Loss: 1.0427207946777344\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.767249584197998 | KNN Loss: 3.7026171684265137 | BCE Loss: 1.0646322965621948\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.706948280334473 | KNN Loss: 3.6715352535247803 | BCE Loss: 1.0354127883911133\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.717710494995117 | KNN Loss: 3.6878535747528076 | BCE Loss: 1.0298566818237305\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.726354122161865 | KNN Loss: 3.7023134231567383 | BCE Loss: 1.0240405797958374\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.7588372230529785 | KNN Loss: 3.7146856784820557 | BCE Loss: 1.0441515445709229\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.7495527267456055 | KNN Loss: 3.706667184829712 | BCE Loss: 1.042885661125183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.727537155151367 | KNN Loss: 3.6818385124206543 | BCE Loss: 1.0456985235214233\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.741756439208984 | KNN Loss: 3.6893279552459717 | BCE Loss: 1.0524282455444336\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.702582836151123 | KNN Loss: 3.695657730102539 | BCE Loss: 1.0069252252578735\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.744598865509033 | KNN Loss: 3.693526029586792 | BCE Loss: 1.0510729551315308\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.73369026184082 | KNN Loss: 3.6861016750335693 | BCE Loss: 1.047588586807251\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.729886054992676 | KNN Loss: 3.6959750652313232 | BCE Loss: 1.0339107513427734\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.771734714508057 | KNN Loss: 3.701991319656372 | BCE Loss: 1.0697435140609741\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.730463981628418 | KNN Loss: 3.701320171356201 | BCE Loss: 1.029144048690796\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.682779312133789 | KNN Loss: 3.6700658798217773 | BCE Loss: 1.0127136707305908\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.685502529144287 | KNN Loss: 3.6643941402435303 | BCE Loss: 1.0211083889007568\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.725317001342773 | KNN Loss: 3.7143361568450928 | BCE Loss: 1.0109810829162598\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.681698322296143 | KNN Loss: 3.661907196044922 | BCE Loss: 1.0197911262512207\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.726861953735352 | KNN Loss: 3.6902711391448975 | BCE Loss: 1.0365910530090332\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.715569019317627 | KNN Loss: 3.6860392093658447 | BCE Loss: 1.0295296907424927\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.671333312988281 | KNN Loss: 3.667891025543213 | BCE Loss: 1.0034420490264893\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.722883224487305 | KNN Loss: 3.7056522369384766 | BCE Loss: 1.017230749130249\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.703598499298096 | KNN Loss: 3.6719863414764404 | BCE Loss: 1.0316120386123657\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.724231719970703 | KNN Loss: 3.68407940864563 | BCE Loss: 1.0401523113250732\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.703258991241455 | KNN Loss: 3.6736629009246826 | BCE Loss: 1.029596209526062\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.699349880218506 | KNN Loss: 3.680224895477295 | BCE Loss: 1.0191251039505005\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.717885971069336 | KNN Loss: 3.691997766494751 | BCE Loss: 1.025888204574585\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.767054080963135 | KNN Loss: 3.711562156677246 | BCE Loss: 1.0554919242858887\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.669058799743652 | KNN Loss: 3.642094612121582 | BCE Loss: 1.0269641876220703\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.796205520629883 | KNN Loss: 3.73123836517334 | BCE Loss: 1.0649670362472534\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.694504737854004 | KNN Loss: 3.6722512245178223 | BCE Loss: 1.0222532749176025\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.747891426086426 | KNN Loss: 3.7108089923858643 | BCE Loss: 1.037082314491272\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.658865451812744 | KNN Loss: 3.674027919769287 | BCE Loss: 0.9848376512527466\n",
      "Epoch   219: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.739770889282227 | KNN Loss: 3.701195001602173 | BCE Loss: 1.0385761260986328\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.741314888000488 | KNN Loss: 3.7008860111236572 | BCE Loss: 1.040428876876831\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.747693061828613 | KNN Loss: 3.7062864303588867 | BCE Loss: 1.0414068698883057\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.737127780914307 | KNN Loss: 3.6994099617004395 | BCE Loss: 1.0377179384231567\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.672450065612793 | KNN Loss: 3.6554510593414307 | BCE Loss: 1.0169988870620728\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.699502468109131 | KNN Loss: 3.675365686416626 | BCE Loss: 1.0241369009017944\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.7374749183654785 | KNN Loss: 3.721766471862793 | BCE Loss: 1.015708327293396\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.7204437255859375 | KNN Loss: 3.699342966079712 | BCE Loss: 1.0211007595062256\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.696625709533691 | KNN Loss: 3.6708853244781494 | BCE Loss: 1.025740385055542\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.695204734802246 | KNN Loss: 3.6980671882629395 | BCE Loss: 0.9971376657485962\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.714858055114746 | KNN Loss: 3.703439950942993 | BCE Loss: 1.0114182233810425\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.726368427276611 | KNN Loss: 3.69205904006958 | BCE Loss: 1.0343093872070312\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.695468425750732 | KNN Loss: 3.701234817504883 | BCE Loss: 0.9942337274551392\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.701262474060059 | KNN Loss: 3.674297332763672 | BCE Loss: 1.0269649028778076\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.727879524230957 | KNN Loss: 3.6963961124420166 | BCE Loss: 1.0314832925796509\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.729428291320801 | KNN Loss: 3.6920971870422363 | BCE Loss: 1.0373308658599854\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.703593730926514 | KNN Loss: 3.680011510848999 | BCE Loss: 1.023582100868225\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.726966857910156 | KNN Loss: 3.700620412826538 | BCE Loss: 1.0263466835021973\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.748551845550537 | KNN Loss: 3.7115354537963867 | BCE Loss: 1.0370162725448608\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.713648796081543 | KNN Loss: 3.662079095840454 | BCE Loss: 1.0515697002410889\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.773757457733154 | KNN Loss: 3.7438387870788574 | BCE Loss: 1.0299187898635864\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.744911193847656 | KNN Loss: 3.7259697914123535 | BCE Loss: 1.0189414024353027\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.754066467285156 | KNN Loss: 3.7323341369628906 | BCE Loss: 1.0217320919036865\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.748324394226074 | KNN Loss: 3.7001631259918213 | BCE Loss: 1.048161268234253\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.7419939041137695 | KNN Loss: 3.702148199081421 | BCE Loss: 1.0398454666137695\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.728371620178223 | KNN Loss: 3.72145938873291 | BCE Loss: 1.0069124698638916\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.729581356048584 | KNN Loss: 3.7034807205200195 | BCE Loss: 1.0261006355285645\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.740962028503418 | KNN Loss: 3.6898415088653564 | BCE Loss: 1.051120400428772\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.707212448120117 | KNN Loss: 3.703500270843506 | BCE Loss: 1.0037122964859009\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.721229076385498 | KNN Loss: 3.6912243366241455 | BCE Loss: 1.030004858970642\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.7181172370910645 | KNN Loss: 3.6836390495300293 | BCE Loss: 1.0344783067703247\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.652956962585449 | KNN Loss: 3.6610257625579834 | BCE Loss: 0.9919309616088867\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.777393341064453 | KNN Loss: 3.7062201499938965 | BCE Loss: 1.071173071861267\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.802128791809082 | KNN Loss: 3.7350544929504395 | BCE Loss: 1.0670744180679321\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.723513126373291 | KNN Loss: 3.713271141052246 | BCE Loss: 1.0102418661117554\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.717793941497803 | KNN Loss: 3.694086790084839 | BCE Loss: 1.0237072706222534\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.6722822189331055 | KNN Loss: 3.676706314086914 | BCE Loss: 0.9955756664276123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.728729724884033 | KNN Loss: 3.6937434673309326 | BCE Loss: 1.0349862575531006\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.686501502990723 | KNN Loss: 3.6707472801208496 | BCE Loss: 1.0157541036605835\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.7180562019348145 | KNN Loss: 3.710627794265747 | BCE Loss: 1.0074282884597778\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.73882532119751 | KNN Loss: 3.7127084732055664 | BCE Loss: 1.0261167287826538\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.724706172943115 | KNN Loss: 3.697829008102417 | BCE Loss: 1.0268771648406982\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.711949348449707 | KNN Loss: 3.706040382385254 | BCE Loss: 1.0059088468551636\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.693180084228516 | KNN Loss: 3.6841881275177 | BCE Loss: 1.0089917182922363\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.749368667602539 | KNN Loss: 3.71199893951416 | BCE Loss: 1.037369728088379\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.659275054931641 | KNN Loss: 3.663018226623535 | BCE Loss: 0.9962570071220398\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.748766899108887 | KNN Loss: 3.6865549087524414 | BCE Loss: 1.0622121095657349\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.686696529388428 | KNN Loss: 3.6736228466033936 | BCE Loss: 1.0130738019943237\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.739325046539307 | KNN Loss: 3.6955323219299316 | BCE Loss: 1.0437926054000854\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.752891540527344 | KNN Loss: 3.7041239738464355 | BCE Loss: 1.0487674474716187\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.761056900024414 | KNN Loss: 3.7228610515594482 | BCE Loss: 1.038196086883545\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.735934257507324 | KNN Loss: 3.678248405456543 | BCE Loss: 1.0576860904693604\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.711102485656738 | KNN Loss: 3.6698708534240723 | BCE Loss: 1.0412315130233765\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.753251075744629 | KNN Loss: 3.678861141204834 | BCE Loss: 1.0743898153305054\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.674500942230225 | KNN Loss: 3.6807384490966797 | BCE Loss: 0.9937626123428345\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.701310157775879 | KNN Loss: 3.6675758361816406 | BCE Loss: 1.0337344408035278\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.7423295974731445 | KNN Loss: 3.6979410648345947 | BCE Loss: 1.0443885326385498\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.7147932052612305 | KNN Loss: 3.685960531234741 | BCE Loss: 1.0288325548171997\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.735875129699707 | KNN Loss: 3.7066876888275146 | BCE Loss: 1.0291876792907715\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.752992153167725 | KNN Loss: 3.7192866802215576 | BCE Loss: 1.0337055921554565\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.682567119598389 | KNN Loss: 3.6868953704833984 | BCE Loss: 0.9956715703010559\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.738399505615234 | KNN Loss: 3.7439258098602295 | BCE Loss: 0.9944734573364258\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.688225269317627 | KNN Loss: 3.66817307472229 | BCE Loss: 1.0200523138046265\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.762453079223633 | KNN Loss: 3.7361197471618652 | BCE Loss: 1.0263334512710571\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.662957191467285 | KNN Loss: 3.6574578285217285 | BCE Loss: 1.0054993629455566\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.754909038543701 | KNN Loss: 3.7040300369262695 | BCE Loss: 1.0508790016174316\n",
      "Epoch   230: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.748242378234863 | KNN Loss: 3.729074001312256 | BCE Loss: 1.0191682577133179\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.683658599853516 | KNN Loss: 3.674938201904297 | BCE Loss: 1.0087205171585083\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.697617053985596 | KNN Loss: 3.687044143676758 | BCE Loss: 1.010572910308838\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.726853847503662 | KNN Loss: 3.6747779846191406 | BCE Loss: 1.052075982093811\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.675570487976074 | KNN Loss: 3.6751773357391357 | BCE Loss: 1.0003929138183594\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.680481910705566 | KNN Loss: 3.652891159057617 | BCE Loss: 1.0275906324386597\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.6835408210754395 | KNN Loss: 3.6633102893829346 | BCE Loss: 1.0202304124832153\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.690187454223633 | KNN Loss: 3.664456844329834 | BCE Loss: 1.0257303714752197\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.716646671295166 | KNN Loss: 3.6895840167999268 | BCE Loss: 1.0270625352859497\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.7282514572143555 | KNN Loss: 3.701341152191162 | BCE Loss: 1.0269101858139038\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.698405742645264 | KNN Loss: 3.689377784729004 | BCE Loss: 1.0090280771255493\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.7759904861450195 | KNN Loss: 3.7113895416259766 | BCE Loss: 1.0646008253097534\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.743391513824463 | KNN Loss: 3.696730613708496 | BCE Loss: 1.0466607809066772\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.757832050323486 | KNN Loss: 3.7280948162078857 | BCE Loss: 1.0297373533248901\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.732397079467773 | KNN Loss: 3.699350118637085 | BCE Loss: 1.0330469608306885\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.690976142883301 | KNN Loss: 3.6686630249023438 | BCE Loss: 1.022312879562378\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.718467712402344 | KNN Loss: 3.6822926998138428 | BCE Loss: 1.0361747741699219\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.712767601013184 | KNN Loss: 3.687819480895996 | BCE Loss: 1.0249478816986084\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.683147430419922 | KNN Loss: 3.6863229274749756 | BCE Loss: 0.9968243837356567\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.755650043487549 | KNN Loss: 3.72159481048584 | BCE Loss: 1.0340553522109985\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.6936869621276855 | KNN Loss: 3.679859161376953 | BCE Loss: 1.013827919960022\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.736621856689453 | KNN Loss: 3.696171283721924 | BCE Loss: 1.0404508113861084\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.746522903442383 | KNN Loss: 3.703460931777954 | BCE Loss: 1.0430619716644287\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.742148399353027 | KNN Loss: 3.7133641242980957 | BCE Loss: 1.0287843942642212\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.701268672943115 | KNN Loss: 3.675936222076416 | BCE Loss: 1.0253324508666992\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.740412712097168 | KNN Loss: 3.734553813934326 | BCE Loss: 1.0058587789535522\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.710672855377197 | KNN Loss: 3.6953768730163574 | BCE Loss: 1.0152961015701294\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.719310760498047 | KNN Loss: 3.7042336463928223 | BCE Loss: 1.0150771141052246\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.732336521148682 | KNN Loss: 3.7136874198913574 | BCE Loss: 1.0186492204666138\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.651223659515381 | KNN Loss: 3.659817934036255 | BCE Loss: 0.9914056062698364\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.752145767211914 | KNN Loss: 3.7375645637512207 | BCE Loss: 1.0145809650421143\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.765913009643555 | KNN Loss: 3.7160091400146484 | BCE Loss: 1.0499038696289062\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.734848976135254 | KNN Loss: 3.682595729827881 | BCE Loss: 1.052253246307373\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.707746505737305 | KNN Loss: 3.6733200550079346 | BCE Loss: 1.0344266891479492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.700353145599365 | KNN Loss: 3.6845409870147705 | BCE Loss: 1.0158121585845947\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.663250923156738 | KNN Loss: 3.6644036769866943 | BCE Loss: 0.998847484588623\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.671493053436279 | KNN Loss: 3.647700548171997 | BCE Loss: 1.0237923860549927\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.7289652824401855 | KNN Loss: 3.6981823444366455 | BCE Loss: 1.03078293800354\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.646731853485107 | KNN Loss: 3.6616616249084473 | BCE Loss: 0.9850702285766602\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.747117042541504 | KNN Loss: 3.7046167850494385 | BCE Loss: 1.0425004959106445\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.716935157775879 | KNN Loss: 3.7105023860931396 | BCE Loss: 1.0064327716827393\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.762528419494629 | KNN Loss: 3.7039144039154053 | BCE Loss: 1.058613896369934\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.765542030334473 | KNN Loss: 3.7109954357147217 | BCE Loss: 1.05454683303833\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.691320419311523 | KNN Loss: 3.6941096782684326 | BCE Loss: 0.9972105026245117\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.739830017089844 | KNN Loss: 3.7219111919403076 | BCE Loss: 1.017918586730957\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.724196434020996 | KNN Loss: 3.6861886978149414 | BCE Loss: 1.0380074977874756\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.7166290283203125 | KNN Loss: 3.7228267192840576 | BCE Loss: 0.9938022494316101\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.718123912811279 | KNN Loss: 3.6816680431365967 | BCE Loss: 1.036455750465393\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.70989465713501 | KNN Loss: 3.68759822845459 | BCE Loss: 1.02229642868042\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.700060844421387 | KNN Loss: 3.6835851669311523 | BCE Loss: 1.0164756774902344\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.7269487380981445 | KNN Loss: 3.704254388809204 | BCE Loss: 1.02269446849823\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.712882995605469 | KNN Loss: 3.6698520183563232 | BCE Loss: 1.0430307388305664\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.704270362854004 | KNN Loss: 3.6768314838409424 | BCE Loss: 1.0274388790130615\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.705583572387695 | KNN Loss: 3.6833696365356445 | BCE Loss: 1.0222136974334717\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.702003479003906 | KNN Loss: 3.6640641689300537 | BCE Loss: 1.0379393100738525\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.724298477172852 | KNN Loss: 3.7027125358581543 | BCE Loss: 1.0215860605239868\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.733983993530273 | KNN Loss: 3.7075676918029785 | BCE Loss: 1.026416301727295\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.728987693786621 | KNN Loss: 3.701765537261963 | BCE Loss: 1.0272221565246582\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.722707748413086 | KNN Loss: 3.678617238998413 | BCE Loss: 1.0440902709960938\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.685433864593506 | KNN Loss: 3.6766602993011475 | BCE Loss: 1.008773684501648\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.678522109985352 | KNN Loss: 3.6689553260803223 | BCE Loss: 1.0095666646957397\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.722733020782471 | KNN Loss: 3.6913020610809326 | BCE Loss: 1.0314310789108276\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.7489094734191895 | KNN Loss: 3.7051162719726562 | BCE Loss: 1.0437932014465332\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.714071750640869 | KNN Loss: 3.7006239891052246 | BCE Loss: 1.013447642326355\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.719804763793945 | KNN Loss: 3.7007384300231934 | BCE Loss: 1.0190660953521729\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.688503742218018 | KNN Loss: 3.677283525466919 | BCE Loss: 1.0112203359603882\n",
      "Epoch   241: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.691563606262207 | KNN Loss: 3.677055835723877 | BCE Loss: 1.0145078897476196\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.7044453620910645 | KNN Loss: 3.689824342727661 | BCE Loss: 1.0146210193634033\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.710164546966553 | KNN Loss: 3.7090024948120117 | BCE Loss: 1.0011619329452515\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.716330528259277 | KNN Loss: 3.678710699081421 | BCE Loss: 1.037619709968567\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.703465938568115 | KNN Loss: 3.7147343158721924 | BCE Loss: 0.9887316823005676\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.752464771270752 | KNN Loss: 3.7007827758789062 | BCE Loss: 1.0516821146011353\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.724379062652588 | KNN Loss: 3.680630922317505 | BCE Loss: 1.043748140335083\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.717715740203857 | KNN Loss: 3.6805477142333984 | BCE Loss: 1.0371679067611694\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.700177192687988 | KNN Loss: 3.6858367919921875 | BCE Loss: 1.0143404006958008\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.751523017883301 | KNN Loss: 3.702702045440674 | BCE Loss: 1.0488208532333374\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.678089618682861 | KNN Loss: 3.6912810802459717 | BCE Loss: 0.9868085384368896\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.725268363952637 | KNN Loss: 3.679767608642578 | BCE Loss: 1.0455009937286377\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.6718645095825195 | KNN Loss: 3.6664416790008545 | BCE Loss: 1.005422830581665\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.729266166687012 | KNN Loss: 3.6987287998199463 | BCE Loss: 1.0305371284484863\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.729893684387207 | KNN Loss: 3.6865007877349854 | BCE Loss: 1.0433928966522217\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.685950756072998 | KNN Loss: 3.69293212890625 | BCE Loss: 0.9930188059806824\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.6931304931640625 | KNN Loss: 3.6763739585876465 | BCE Loss: 1.0167567729949951\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.692168235778809 | KNN Loss: 3.674586296081543 | BCE Loss: 1.0175821781158447\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.701648712158203 | KNN Loss: 3.683411121368408 | BCE Loss: 1.0182377099990845\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.742089748382568 | KNN Loss: 3.688981533050537 | BCE Loss: 1.0531082153320312\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.759642601013184 | KNN Loss: 3.7230472564697266 | BCE Loss: 1.036595106124878\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.71213960647583 | KNN Loss: 3.677253484725952 | BCE Loss: 1.0348862409591675\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.713149547576904 | KNN Loss: 3.671445846557617 | BCE Loss: 1.0417038202285767\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.710488319396973 | KNN Loss: 3.673112154006958 | BCE Loss: 1.0373764038085938\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.701969146728516 | KNN Loss: 3.684649705886841 | BCE Loss: 1.0173192024230957\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.6642069816589355 | KNN Loss: 3.6709394454956055 | BCE Loss: 0.9932674765586853\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.6759867668151855 | KNN Loss: 3.6549534797668457 | BCE Loss: 1.0210332870483398\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.71364688873291 | KNN Loss: 3.6843743324279785 | BCE Loss: 1.029272437095642\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.725380897521973 | KNN Loss: 3.7061028480529785 | BCE Loss: 1.019277811050415\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.7687788009643555 | KNN Loss: 3.7319185733795166 | BCE Loss: 1.0368599891662598\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.67389440536499 | KNN Loss: 3.675880193710327 | BCE Loss: 0.9980140924453735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.705700397491455 | KNN Loss: 3.6888933181762695 | BCE Loss: 1.0168070793151855\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.714965343475342 | KNN Loss: 3.6797373294830322 | BCE Loss: 1.0352281332015991\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.742198467254639 | KNN Loss: 3.706108808517456 | BCE Loss: 1.0360896587371826\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.704590320587158 | KNN Loss: 3.693385362625122 | BCE Loss: 1.0112050771713257\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.738190174102783 | KNN Loss: 3.6975948810577393 | BCE Loss: 1.0405954122543335\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.692266464233398 | KNN Loss: 3.6873929500579834 | BCE Loss: 1.004873514175415\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.699589729309082 | KNN Loss: 3.678147792816162 | BCE Loss: 1.02144193649292\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.678483486175537 | KNN Loss: 3.663785457611084 | BCE Loss: 1.0146980285644531\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.727493762969971 | KNN Loss: 3.6730194091796875 | BCE Loss: 1.0544742345809937\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.702079772949219 | KNN Loss: 3.691335916519165 | BCE Loss: 1.0107440948486328\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.714536190032959 | KNN Loss: 3.6752991676330566 | BCE Loss: 1.039237141609192\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.680028915405273 | KNN Loss: 3.692396640777588 | BCE Loss: 0.987632155418396\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.702420234680176 | KNN Loss: 3.6793127059936523 | BCE Loss: 1.0231072902679443\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.697788715362549 | KNN Loss: 3.692065954208374 | BCE Loss: 1.0057226419448853\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.693431854248047 | KNN Loss: 3.6889147758483887 | BCE Loss: 1.0045173168182373\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.680887222290039 | KNN Loss: 3.6580958366394043 | BCE Loss: 1.0227912664413452\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.723597526550293 | KNN Loss: 3.703101873397827 | BCE Loss: 1.020495891571045\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.708737850189209 | KNN Loss: 3.693371534347534 | BCE Loss: 1.0153661966323853\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.702793121337891 | KNN Loss: 3.6868555545806885 | BCE Loss: 1.0159376859664917\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.698603630065918 | KNN Loss: 3.6717844009399414 | BCE Loss: 1.0268189907073975\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.703440189361572 | KNN Loss: 3.6892435550689697 | BCE Loss: 1.0141966342926025\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.745155334472656 | KNN Loss: 3.731825113296509 | BCE Loss: 1.0133302211761475\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.670494079589844 | KNN Loss: 3.663860321044922 | BCE Loss: 1.006633996963501\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.794810771942139 | KNN Loss: 3.7513513565063477 | BCE Loss: 1.043459415435791\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.689656734466553 | KNN Loss: 3.7020201683044434 | BCE Loss: 0.987636387348175\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.721701622009277 | KNN Loss: 3.674283027648926 | BCE Loss: 1.0474188327789307\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.738389492034912 | KNN Loss: 3.669325590133667 | BCE Loss: 1.0690640211105347\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.731158256530762 | KNN Loss: 3.678295850753784 | BCE Loss: 1.0528621673583984\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.677548885345459 | KNN Loss: 3.688809871673584 | BCE Loss: 0.9887389540672302\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.704736709594727 | KNN Loss: 3.689870595932007 | BCE Loss: 1.0148659944534302\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.688555717468262 | KNN Loss: 3.6667749881744385 | BCE Loss: 1.0217804908752441\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.712066650390625 | KNN Loss: 3.685292959213257 | BCE Loss: 1.0267738103866577\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.74008846282959 | KNN Loss: 3.6814839839935303 | BCE Loss: 1.05860435962677\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.69052267074585 | KNN Loss: 3.6685426235198975 | BCE Loss: 1.0219801664352417\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.769274711608887 | KNN Loss: 3.7159435749053955 | BCE Loss: 1.0533313751220703\n",
      "Epoch   252: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.687546730041504 | KNN Loss: 3.6862688064575195 | BCE Loss: 1.0012776851654053\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.77910041809082 | KNN Loss: 3.724851608276367 | BCE Loss: 1.0542490482330322\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.71725606918335 | KNN Loss: 3.6876299381256104 | BCE Loss: 1.0296261310577393\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.715592861175537 | KNN Loss: 3.680126667022705 | BCE Loss: 1.035466194152832\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.763422012329102 | KNN Loss: 3.7467873096466064 | BCE Loss: 1.0166347026824951\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.698105812072754 | KNN Loss: 3.6769919395446777 | BCE Loss: 1.021113634109497\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.698457717895508 | KNN Loss: 3.6768298149108887 | BCE Loss: 1.0216277837753296\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.741607666015625 | KNN Loss: 3.696472644805908 | BCE Loss: 1.0451350212097168\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.684892654418945 | KNN Loss: 3.699564218521118 | BCE Loss: 0.9853286743164062\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.786055564880371 | KNN Loss: 3.7480709552764893 | BCE Loss: 1.0379846096038818\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.703969955444336 | KNN Loss: 3.6702322959899902 | BCE Loss: 1.0337378978729248\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.704183578491211 | KNN Loss: 3.6509573459625244 | BCE Loss: 1.0532264709472656\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.70832633972168 | KNN Loss: 3.6797115802764893 | BCE Loss: 1.02861487865448\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.726847171783447 | KNN Loss: 3.692758083343506 | BCE Loss: 1.0340890884399414\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.722542762756348 | KNN Loss: 3.7182016372680664 | BCE Loss: 1.0043413639068604\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.661642074584961 | KNN Loss: 3.670318365097046 | BCE Loss: 0.9913235306739807\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.691911697387695 | KNN Loss: 3.6690142154693604 | BCE Loss: 1.0228976011276245\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.7308268547058105 | KNN Loss: 3.710258722305298 | BCE Loss: 1.0205682516098022\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.686412811279297 | KNN Loss: 3.6725921630859375 | BCE Loss: 1.0138206481933594\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.699524879455566 | KNN Loss: 3.698317527770996 | BCE Loss: 1.0012073516845703\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.732751369476318 | KNN Loss: 3.7057697772979736 | BCE Loss: 1.0269817113876343\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.703226089477539 | KNN Loss: 3.6889891624450684 | BCE Loss: 1.0142366886138916\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.692840576171875 | KNN Loss: 3.684898614883423 | BCE Loss: 1.0079419612884521\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.714920997619629 | KNN Loss: 3.6884303092956543 | BCE Loss: 1.0264906883239746\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.690674781799316 | KNN Loss: 3.680119037628174 | BCE Loss: 1.0105557441711426\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.712162017822266 | KNN Loss: 3.6994149684906006 | BCE Loss: 1.012746810913086\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.781759738922119 | KNN Loss: 3.7392208576202393 | BCE Loss: 1.0425387620925903\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.666991233825684 | KNN Loss: 3.660980224609375 | BCE Loss: 1.0060107707977295\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.722107887268066 | KNN Loss: 3.6907174587249756 | BCE Loss: 1.0313901901245117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.698558330535889 | KNN Loss: 3.668684720993042 | BCE Loss: 1.0298734903335571\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.717868328094482 | KNN Loss: 3.6943488121032715 | BCE Loss: 1.0235196352005005\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.696666717529297 | KNN Loss: 3.6820380687713623 | BCE Loss: 1.0146284103393555\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.7254862785339355 | KNN Loss: 3.7186121940612793 | BCE Loss: 1.0068742036819458\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.715353012084961 | KNN Loss: 3.6962459087371826 | BCE Loss: 1.0191069841384888\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.715731143951416 | KNN Loss: 3.6618754863739014 | BCE Loss: 1.0538557767868042\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.718195915222168 | KNN Loss: 3.6890907287597656 | BCE Loss: 1.0291049480438232\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.715391159057617 | KNN Loss: 3.7137813568115234 | BCE Loss: 1.0016098022460938\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.689886569976807 | KNN Loss: 3.669670820236206 | BCE Loss: 1.020215630531311\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.695341110229492 | KNN Loss: 3.66562819480896 | BCE Loss: 1.0297131538391113\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.6825714111328125 | KNN Loss: 3.6700797080993652 | BCE Loss: 1.0124917030334473\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.693672180175781 | KNN Loss: 3.664714813232422 | BCE Loss: 1.0289572477340698\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.732222080230713 | KNN Loss: 3.7223191261291504 | BCE Loss: 1.009903073310852\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.6827921867370605 | KNN Loss: 3.668707847595215 | BCE Loss: 1.0140842199325562\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.706143379211426 | KNN Loss: 3.701070785522461 | BCE Loss: 1.0050725936889648\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.720716953277588 | KNN Loss: 3.68369722366333 | BCE Loss: 1.0370197296142578\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.708296775817871 | KNN Loss: 3.661163568496704 | BCE Loss: 1.0471330881118774\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.704578399658203 | KNN Loss: 3.6917643547058105 | BCE Loss: 1.012813925743103\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.768919944763184 | KNN Loss: 3.7203288078308105 | BCE Loss: 1.0485913753509521\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.6770429611206055 | KNN Loss: 3.663557291030884 | BCE Loss: 1.0134856700897217\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.706801891326904 | KNN Loss: 3.678173303604126 | BCE Loss: 1.0286284685134888\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.729254722595215 | KNN Loss: 3.7113659381866455 | BCE Loss: 1.0178890228271484\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.72525691986084 | KNN Loss: 3.690157413482666 | BCE Loss: 1.0350995063781738\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.707460403442383 | KNN Loss: 3.673053026199341 | BCE Loss: 1.034407615661621\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.665202617645264 | KNN Loss: 3.668513298034668 | BCE Loss: 0.9966891407966614\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.659468650817871 | KNN Loss: 3.6539463996887207 | BCE Loss: 1.0055222511291504\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.743913173675537 | KNN Loss: 3.714848279953003 | BCE Loss: 1.0290650129318237\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.728093147277832 | KNN Loss: 3.6725525856018066 | BCE Loss: 1.055540680885315\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.689569473266602 | KNN Loss: 3.698873281478882 | BCE Loss: 0.9906962513923645\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.698139667510986 | KNN Loss: 3.7211475372314453 | BCE Loss: 0.9769922494888306\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.657072067260742 | KNN Loss: 3.6656975746154785 | BCE Loss: 0.9913744330406189\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.6808671951293945 | KNN Loss: 3.6597626209259033 | BCE Loss: 1.0211046934127808\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.719810485839844 | KNN Loss: 3.698411703109741 | BCE Loss: 1.0213990211486816\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.714224815368652 | KNN Loss: 3.7036988735198975 | BCE Loss: 1.0105260610580444\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.777254104614258 | KNN Loss: 3.738158702850342 | BCE Loss: 1.039095401763916\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.770847797393799 | KNN Loss: 3.749457836151123 | BCE Loss: 1.0213899612426758\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.765158176422119 | KNN Loss: 3.7056801319122314 | BCE Loss: 1.0594779253005981\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.720934867858887 | KNN Loss: 3.679370164871216 | BCE Loss: 1.0415644645690918\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.679973602294922 | KNN Loss: 3.663536310195923 | BCE Loss: 1.0164375305175781\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.692433834075928 | KNN Loss: 3.6668601036071777 | BCE Loss: 1.02557373046875\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.705573081970215 | KNN Loss: 3.6834776401519775 | BCE Loss: 1.0220953226089478\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.708549976348877 | KNN Loss: 3.6922543048858643 | BCE Loss: 1.0162955522537231\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.708899021148682 | KNN Loss: 3.7069718837738037 | BCE Loss: 1.0019272565841675\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.692835807800293 | KNN Loss: 3.6643824577331543 | BCE Loss: 1.0284533500671387\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.777889251708984 | KNN Loss: 3.7532942295074463 | BCE Loss: 1.0245952606201172\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.718999862670898 | KNN Loss: 3.6912496089935303 | BCE Loss: 1.0277501344680786\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.7266364097595215 | KNN Loss: 3.683903932571411 | BCE Loss: 1.0427323579788208\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.684759140014648 | KNN Loss: 3.664853096008301 | BCE Loss: 1.0199060440063477\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.708968639373779 | KNN Loss: 3.692089319229126 | BCE Loss: 1.0168794393539429\n",
      "Epoch   265: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.717498302459717 | KNN Loss: 3.6970133781433105 | BCE Loss: 1.0204849243164062\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.704411506652832 | KNN Loss: 3.660163164138794 | BCE Loss: 1.0442485809326172\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.702044486999512 | KNN Loss: 3.691283941268921 | BCE Loss: 1.0107606649398804\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.6914191246032715 | KNN Loss: 3.6746737957000732 | BCE Loss: 1.0167452096939087\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.683840751647949 | KNN Loss: 3.6712331771850586 | BCE Loss: 1.0126073360443115\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.754827499389648 | KNN Loss: 3.724440813064575 | BCE Loss: 1.0303864479064941\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.679909706115723 | KNN Loss: 3.65948748588562 | BCE Loss: 1.0204219818115234\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.744181156158447 | KNN Loss: 3.7221083641052246 | BCE Loss: 1.0220727920532227\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.710271835327148 | KNN Loss: 3.6991631984710693 | BCE Loss: 1.0111083984375\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.681845664978027 | KNN Loss: 3.667388677597046 | BCE Loss: 1.0144572257995605\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.729184150695801 | KNN Loss: 3.697714328765869 | BCE Loss: 1.0314695835113525\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.749263763427734 | KNN Loss: 3.6925063133239746 | BCE Loss: 1.0567574501037598\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.727369785308838 | KNN Loss: 3.729264259338379 | BCE Loss: 0.9981053471565247\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.701300144195557 | KNN Loss: 3.679342746734619 | BCE Loss: 1.021957516670227\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.721242904663086 | KNN Loss: 3.6700639724731445 | BCE Loss: 1.0511786937713623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.707636833190918 | KNN Loss: 3.7067086696624756 | BCE Loss: 1.0009281635284424\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.678200721740723 | KNN Loss: 3.6721320152282715 | BCE Loss: 1.0060688257217407\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.6675591468811035 | KNN Loss: 3.654594898223877 | BCE Loss: 1.0129642486572266\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.7280120849609375 | KNN Loss: 3.7046127319335938 | BCE Loss: 1.0233993530273438\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.694061279296875 | KNN Loss: 3.703223466873169 | BCE Loss: 0.990837574005127\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.730989456176758 | KNN Loss: 3.7316179275512695 | BCE Loss: 0.9993715882301331\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.689515113830566 | KNN Loss: 3.674952745437622 | BCE Loss: 1.0145623683929443\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.720033645629883 | KNN Loss: 3.677161693572998 | BCE Loss: 1.0428718328475952\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.752353668212891 | KNN Loss: 3.7040836811065674 | BCE Loss: 1.0482697486877441\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.775784015655518 | KNN Loss: 3.7479031085968018 | BCE Loss: 1.0278810262680054\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.669062614440918 | KNN Loss: 3.6563141345977783 | BCE Loss: 1.0127484798431396\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.752890110015869 | KNN Loss: 3.7215757369995117 | BCE Loss: 1.031314492225647\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.731398105621338 | KNN Loss: 3.7076380252838135 | BCE Loss: 1.0237600803375244\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.685229301452637 | KNN Loss: 3.6722710132598877 | BCE Loss: 1.012958288192749\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.7155351638793945 | KNN Loss: 3.6852433681488037 | BCE Loss: 1.03029203414917\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.747220039367676 | KNN Loss: 3.7262845039367676 | BCE Loss: 1.0209357738494873\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.691841125488281 | KNN Loss: 3.674344778060913 | BCE Loss: 1.017496109008789\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.763123512268066 | KNN Loss: 3.7266321182250977 | BCE Loss: 1.0364916324615479\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.692223072052002 | KNN Loss: 3.6620678901672363 | BCE Loss: 1.0301551818847656\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.713259220123291 | KNN Loss: 3.6898953914642334 | BCE Loss: 1.0233639478683472\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.763608932495117 | KNN Loss: 3.723086357116699 | BCE Loss: 1.040522813796997\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.70180606842041 | KNN Loss: 3.68444561958313 | BCE Loss: 1.0173602104187012\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.728149890899658 | KNN Loss: 3.6747541427612305 | BCE Loss: 1.0533957481384277\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.705410480499268 | KNN Loss: 3.679363489151001 | BCE Loss: 1.0260469913482666\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.716334819793701 | KNN Loss: 3.696049213409424 | BCE Loss: 1.0202856063842773\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.684502601623535 | KNN Loss: 3.6647636890411377 | BCE Loss: 1.0197386741638184\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.715131759643555 | KNN Loss: 3.6932129859924316 | BCE Loss: 1.0219190120697021\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.780939102172852 | KNN Loss: 3.7554028034210205 | BCE Loss: 1.025536060333252\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.72777795791626 | KNN Loss: 3.6817333698272705 | BCE Loss: 1.0460445880889893\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.75864839553833 | KNN Loss: 3.718810796737671 | BCE Loss: 1.0398374795913696\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.694554805755615 | KNN Loss: 3.6891260147094727 | BCE Loss: 1.005428671836853\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.650233268737793 | KNN Loss: 3.657170534133911 | BCE Loss: 0.9930626153945923\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.741171360015869 | KNN Loss: 3.7022106647491455 | BCE Loss: 1.0389608144760132\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.73808479309082 | KNN Loss: 3.7112069129943848 | BCE Loss: 1.0268778800964355\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.726217746734619 | KNN Loss: 3.7219715118408203 | BCE Loss: 1.0042461156845093\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.761442184448242 | KNN Loss: 3.7014827728271484 | BCE Loss: 1.0599596500396729\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.7162628173828125 | KNN Loss: 3.6919162273406982 | BCE Loss: 1.0243467092514038\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.718990802764893 | KNN Loss: 3.705822706222534 | BCE Loss: 1.0131680965423584\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.71435022354126 | KNN Loss: 3.7012834548950195 | BCE Loss: 1.0130666494369507\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.723621368408203 | KNN Loss: 3.686540365219116 | BCE Loss: 1.0370811223983765\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.712696552276611 | KNN Loss: 3.668945789337158 | BCE Loss: 1.0437508821487427\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.73079776763916 | KNN Loss: 3.6916749477386475 | BCE Loss: 1.0391230583190918\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.723489761352539 | KNN Loss: 3.673877239227295 | BCE Loss: 1.0496125221252441\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.64143180847168 | KNN Loss: 3.6577298641204834 | BCE Loss: 0.9837020039558411\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.787569046020508 | KNN Loss: 3.7181224822998047 | BCE Loss: 1.069446325302124\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.656871795654297 | KNN Loss: 3.6641364097595215 | BCE Loss: 0.9927353858947754\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.72011137008667 | KNN Loss: 3.684629201889038 | BCE Loss: 1.0354822874069214\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.8251519203186035 | KNN Loss: 3.806121349334717 | BCE Loss: 1.0190305709838867\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.671145915985107 | KNN Loss: 3.6573336124420166 | BCE Loss: 1.0138121843338013\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.7203145027160645 | KNN Loss: 3.689155101776123 | BCE Loss: 1.0311594009399414\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.711670875549316 | KNN Loss: 3.675690174102783 | BCE Loss: 1.0359807014465332\n",
      "Epoch   276: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.713362693786621 | KNN Loss: 3.6766679286956787 | BCE Loss: 1.0366946458816528\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.732062816619873 | KNN Loss: 3.71225905418396 | BCE Loss: 1.019803762435913\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.726846218109131 | KNN Loss: 3.692718267440796 | BCE Loss: 1.0341278314590454\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.728720664978027 | KNN Loss: 3.7012064456939697 | BCE Loss: 1.027514100074768\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.7279510498046875 | KNN Loss: 3.699538230895996 | BCE Loss: 1.0284128189086914\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.726220607757568 | KNN Loss: 3.707746982574463 | BCE Loss: 1.018473505973816\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.769960403442383 | KNN Loss: 3.7539141178131104 | BCE Loss: 1.0160460472106934\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.767517566680908 | KNN Loss: 3.7214131355285645 | BCE Loss: 1.0461043119430542\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.6996846199035645 | KNN Loss: 3.6614882946014404 | BCE Loss: 1.0381964445114136\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.752791404724121 | KNN Loss: 3.703380823135376 | BCE Loss: 1.0494105815887451\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.754795074462891 | KNN Loss: 3.717439889907837 | BCE Loss: 1.0373549461364746\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.6991376876831055 | KNN Loss: 3.689779043197632 | BCE Loss: 1.0093588829040527\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.718637943267822 | KNN Loss: 3.6929800510406494 | BCE Loss: 1.0256580114364624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.730372905731201 | KNN Loss: 3.706259250640869 | BCE Loss: 1.0241137742996216\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.7361836433410645 | KNN Loss: 3.7037582397460938 | BCE Loss: 1.0324252843856812\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.695908546447754 | KNN Loss: 3.6684470176696777 | BCE Loss: 1.0274617671966553\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.7384209632873535 | KNN Loss: 3.727468967437744 | BCE Loss: 1.0109518766403198\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.697651386260986 | KNN Loss: 3.6647002696990967 | BCE Loss: 1.0329509973526\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.68834924697876 | KNN Loss: 3.6803154945373535 | BCE Loss: 1.0080338716506958\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.721467018127441 | KNN Loss: 3.700702667236328 | BCE Loss: 1.0207645893096924\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.708103179931641 | KNN Loss: 3.6669704914093018 | BCE Loss: 1.041132926940918\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.680335998535156 | KNN Loss: 3.694422721862793 | BCE Loss: 0.9859131574630737\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.709865570068359 | KNN Loss: 3.7090721130371094 | BCE Loss: 1.0007933378219604\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.733060836791992 | KNN Loss: 3.7139320373535156 | BCE Loss: 1.0191290378570557\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.690262794494629 | KNN Loss: 3.678093671798706 | BCE Loss: 1.0121691226959229\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.763998031616211 | KNN Loss: 3.738267660140991 | BCE Loss: 1.0257306098937988\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.676011562347412 | KNN Loss: 3.6809568405151367 | BCE Loss: 0.9950545430183411\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.702718257904053 | KNN Loss: 3.6590116024017334 | BCE Loss: 1.0437067747116089\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.708156108856201 | KNN Loss: 3.7023086547851562 | BCE Loss: 1.0058473348617554\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.715001106262207 | KNN Loss: 3.6749637126922607 | BCE Loss: 1.0400373935699463\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.740992546081543 | KNN Loss: 3.7235586643218994 | BCE Loss: 1.017433762550354\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.643118858337402 | KNN Loss: 3.66031551361084 | BCE Loss: 0.9828031659126282\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.721879005432129 | KNN Loss: 3.688647747039795 | BCE Loss: 1.0332313776016235\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.700775623321533 | KNN Loss: 3.671459197998047 | BCE Loss: 1.0293164253234863\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.730747222900391 | KNN Loss: 3.682171106338501 | BCE Loss: 1.0485763549804688\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.707754135131836 | KNN Loss: 3.67232084274292 | BCE Loss: 1.0354334115982056\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.73668098449707 | KNN Loss: 3.7109649181365967 | BCE Loss: 1.0257158279418945\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.684140682220459 | KNN Loss: 3.6627299785614014 | BCE Loss: 1.021410584449768\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.705277919769287 | KNN Loss: 3.682046413421631 | BCE Loss: 1.0232313871383667\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.713186264038086 | KNN Loss: 3.6746597290039062 | BCE Loss: 1.0385262966156006\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.723367214202881 | KNN Loss: 3.6894001960754395 | BCE Loss: 1.0339670181274414\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.727367401123047 | KNN Loss: 3.692744016647339 | BCE Loss: 1.034623622894287\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.746214389801025 | KNN Loss: 3.714340925216675 | BCE Loss: 1.0318735837936401\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.681738376617432 | KNN Loss: 3.660853624343872 | BCE Loss: 1.0208847522735596\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.670888900756836 | KNN Loss: 3.6627724170684814 | BCE Loss: 1.008116364479065\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.732379913330078 | KNN Loss: 3.711017608642578 | BCE Loss: 1.0213624238967896\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.739324569702148 | KNN Loss: 3.6947340965270996 | BCE Loss: 1.0445904731750488\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.764706611633301 | KNN Loss: 3.7313029766082764 | BCE Loss: 1.033403754234314\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.726398944854736 | KNN Loss: 3.686398506164551 | BCE Loss: 1.040000557899475\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.748271465301514 | KNN Loss: 3.7330126762390137 | BCE Loss: 1.0152589082717896\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.631472110748291 | KNN Loss: 3.6426517963409424 | BCE Loss: 0.9888203144073486\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.685135841369629 | KNN Loss: 3.667073965072632 | BCE Loss: 1.0180619955062866\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.792064666748047 | KNN Loss: 3.7689104080200195 | BCE Loss: 1.0231540203094482\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.693365097045898 | KNN Loss: 3.68742036819458 | BCE Loss: 1.0059447288513184\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.694128036499023 | KNN Loss: 3.664111852645874 | BCE Loss: 1.0300159454345703\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.6637468338012695 | KNN Loss: 3.6567423343658447 | BCE Loss: 1.0070044994354248\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.695351600646973 | KNN Loss: 3.6630611419677734 | BCE Loss: 1.0322906970977783\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.684305667877197 | KNN Loss: 3.667041063308716 | BCE Loss: 1.0172646045684814\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.7490386962890625 | KNN Loss: 3.696803569793701 | BCE Loss: 1.0522352457046509\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.711606979370117 | KNN Loss: 3.6744887828826904 | BCE Loss: 1.0371179580688477\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.74287223815918 | KNN Loss: 3.710768699645996 | BCE Loss: 1.0321033000946045\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.700578212738037 | KNN Loss: 3.66642427444458 | BCE Loss: 1.0341538190841675\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.678369522094727 | KNN Loss: 3.6598727703094482 | BCE Loss: 1.0184966325759888\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.706614971160889 | KNN Loss: 3.69291353225708 | BCE Loss: 1.013701319694519\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.725225448608398 | KNN Loss: 3.7060234546661377 | BCE Loss: 1.0192021131515503\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.716089248657227 | KNN Loss: 3.689955711364746 | BCE Loss: 1.0261332988739014\n",
      "Epoch   287: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.772350311279297 | KNN Loss: 3.734189510345459 | BCE Loss: 1.0381606817245483\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.658260345458984 | KNN Loss: 3.6542723178863525 | BCE Loss: 1.0039880275726318\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.737268924713135 | KNN Loss: 3.70326566696167 | BCE Loss: 1.0340032577514648\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.732774257659912 | KNN Loss: 3.6904499530792236 | BCE Loss: 1.0423243045806885\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.73653507232666 | KNN Loss: 3.682142496109009 | BCE Loss: 1.0543924570083618\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.753335952758789 | KNN Loss: 3.703568696975708 | BCE Loss: 1.049767255783081\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.776400566101074 | KNN Loss: 3.7266860008239746 | BCE Loss: 1.0497146844863892\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.698256015777588 | KNN Loss: 3.6727395057678223 | BCE Loss: 1.025516390800476\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.696557998657227 | KNN Loss: 3.651221513748169 | BCE Loss: 1.0453367233276367\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.720338821411133 | KNN Loss: 3.6900429725646973 | BCE Loss: 1.0302958488464355\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.72524356842041 | KNN Loss: 3.6927108764648438 | BCE Loss: 1.0325324535369873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.699319839477539 | KNN Loss: 3.6849000453948975 | BCE Loss: 1.0144195556640625\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.678578853607178 | KNN Loss: 3.657196521759033 | BCE Loss: 1.0213823318481445\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.734014987945557 | KNN Loss: 3.701911211013794 | BCE Loss: 1.0321038961410522\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.737849235534668 | KNN Loss: 3.722419023513794 | BCE Loss: 1.0154300928115845\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.739881992340088 | KNN Loss: 3.7007198333740234 | BCE Loss: 1.039162278175354\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.7268171310424805 | KNN Loss: 3.7190210819244385 | BCE Loss: 1.007796287536621\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.680582046508789 | KNN Loss: 3.669222354888916 | BCE Loss: 1.0113599300384521\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.692950248718262 | KNN Loss: 3.6754825115203857 | BCE Loss: 1.017467975616455\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.6843791007995605 | KNN Loss: 3.6537458896636963 | BCE Loss: 1.0306330919265747\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.735203742980957 | KNN Loss: 3.6970529556274414 | BCE Loss: 1.0381510257720947\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.735334873199463 | KNN Loss: 3.7208311557769775 | BCE Loss: 1.0145037174224854\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.72031307220459 | KNN Loss: 3.7026748657226562 | BCE Loss: 1.0176383256912231\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.676982402801514 | KNN Loss: 3.667747735977173 | BCE Loss: 1.0092345476150513\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.709579944610596 | KNN Loss: 3.6905720233917236 | BCE Loss: 1.019007921218872\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.691108703613281 | KNN Loss: 3.673330307006836 | BCE Loss: 1.0177782773971558\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.743072509765625 | KNN Loss: 3.714689254760742 | BCE Loss: 1.0283832550048828\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.679563045501709 | KNN Loss: 3.6622018814086914 | BCE Loss: 1.0173611640930176\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.722749710083008 | KNN Loss: 3.681042194366455 | BCE Loss: 1.0417076349258423\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.711731910705566 | KNN Loss: 3.680368423461914 | BCE Loss: 1.031363606452942\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.7276201248168945 | KNN Loss: 3.7247893810272217 | BCE Loss: 1.002830982208252\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.69085693359375 | KNN Loss: 3.6507225036621094 | BCE Loss: 1.0401341915130615\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.712435245513916 | KNN Loss: 3.701482057571411 | BCE Loss: 1.0109533071517944\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.7206196784973145 | KNN Loss: 3.702923059463501 | BCE Loss: 1.017696738243103\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.716233253479004 | KNN Loss: 3.695394277572632 | BCE Loss: 1.0208392143249512\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.718167781829834 | KNN Loss: 3.703514337539673 | BCE Loss: 1.0146535634994507\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.719184398651123 | KNN Loss: 3.7089765071868896 | BCE Loss: 1.010208010673523\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.7351765632629395 | KNN Loss: 3.715466022491455 | BCE Loss: 1.0197105407714844\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.709274768829346 | KNN Loss: 3.6872525215148926 | BCE Loss: 1.0220222473144531\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.706110954284668 | KNN Loss: 3.704810619354248 | BCE Loss: 1.00130033493042\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.6640472412109375 | KNN Loss: 3.6828014850616455 | BCE Loss: 0.9812455177307129\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.746963977813721 | KNN Loss: 3.7183127403259277 | BCE Loss: 1.0286511182785034\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.7237629890441895 | KNN Loss: 3.692369222640991 | BCE Loss: 1.0313937664031982\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.683553695678711 | KNN Loss: 3.657712459564209 | BCE Loss: 1.0258413553237915\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.695255756378174 | KNN Loss: 3.6736209392547607 | BCE Loss: 1.0216349363327026\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.697029113769531 | KNN Loss: 3.682894468307495 | BCE Loss: 1.0141345262527466\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.721173286437988 | KNN Loss: 3.720250368118286 | BCE Loss: 1.0009230375289917\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.718510627746582 | KNN Loss: 3.7016515731811523 | BCE Loss: 1.0168592929840088\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.702872276306152 | KNN Loss: 3.684217691421509 | BCE Loss: 1.018654465675354\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.733592987060547 | KNN Loss: 3.6852660179138184 | BCE Loss: 1.0483272075653076\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.714930534362793 | KNN Loss: 3.6769416332244873 | BCE Loss: 1.0379890203475952\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.696192741394043 | KNN Loss: 3.6871373653411865 | BCE Loss: 1.0090556144714355\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.691816806793213 | KNN Loss: 3.6819612979888916 | BCE Loss: 1.0098553895950317\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.699195861816406 | KNN Loss: 3.6965622901916504 | BCE Loss: 1.002633810043335\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.656322002410889 | KNN Loss: 3.6533610820770264 | BCE Loss: 1.0029609203338623\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.709099292755127 | KNN Loss: 3.694760799407959 | BCE Loss: 1.014338493347168\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.690008163452148 | KNN Loss: 3.6854145526885986 | BCE Loss: 1.004593849182129\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.746819972991943 | KNN Loss: 3.7019500732421875 | BCE Loss: 1.0448697805404663\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.707813262939453 | KNN Loss: 3.6739890575408936 | BCE Loss: 1.0338244438171387\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.714428424835205 | KNN Loss: 3.6890876293182373 | BCE Loss: 1.0253406763076782\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.739185333251953 | KNN Loss: 3.7062995433807373 | BCE Loss: 1.0328859090805054\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.712388515472412 | KNN Loss: 3.6733672618865967 | BCE Loss: 1.0390212535858154\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.695061683654785 | KNN Loss: 3.6741249561309814 | BCE Loss: 1.0209368467330933\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.705792427062988 | KNN Loss: 3.696575164794922 | BCE Loss: 1.0092172622680664\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.676630020141602 | KNN Loss: 3.673837661743164 | BCE Loss: 1.0027925968170166\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.680725574493408 | KNN Loss: 3.680596113204956 | BCE Loss: 1.0001294612884521\n",
      "Epoch   298: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.759636878967285 | KNN Loss: 3.7389533519744873 | BCE Loss: 1.0206832885742188\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.69970178604126 | KNN Loss: 3.674750566482544 | BCE Loss: 1.0249513387680054\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.696933746337891 | KNN Loss: 3.684819459915161 | BCE Loss: 1.0121142864227295\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.721105575561523 | KNN Loss: 3.7036619186401367 | BCE Loss: 1.0174435377120972\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.709047317504883 | KNN Loss: 3.702082633972168 | BCE Loss: 1.0069648027420044\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.727113723754883 | KNN Loss: 3.7149429321289062 | BCE Loss: 1.012170672416687\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.68497896194458 | KNN Loss: 3.659179210662842 | BCE Loss: 1.0257996320724487\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.706508636474609 | KNN Loss: 3.6682145595550537 | BCE Loss: 1.0382938385009766\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.720271110534668 | KNN Loss: 3.724334239959717 | BCE Loss: 0.9959366321563721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.6643805503845215 | KNN Loss: 3.660395383834839 | BCE Loss: 1.003985047340393\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.664844512939453 | KNN Loss: 3.6575748920440674 | BCE Loss: 1.0072696208953857\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.73767614364624 | KNN Loss: 3.7242987155914307 | BCE Loss: 1.0133775472640991\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.740182876586914 | KNN Loss: 3.6962735652923584 | BCE Loss: 1.0439090728759766\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.70028829574585 | KNN Loss: 3.647413492202759 | BCE Loss: 1.0528748035430908\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.725006103515625 | KNN Loss: 3.697453498840332 | BCE Loss: 1.0275527238845825\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.720986843109131 | KNN Loss: 3.67313814163208 | BCE Loss: 1.0478485822677612\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.742677688598633 | KNN Loss: 3.7246346473693848 | BCE Loss: 1.018042802810669\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.712244033813477 | KNN Loss: 3.6720454692840576 | BCE Loss: 1.040198802947998\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.715773582458496 | KNN Loss: 3.6993861198425293 | BCE Loss: 1.0163874626159668\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.752923488616943 | KNN Loss: 3.698409080505371 | BCE Loss: 1.0545144081115723\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.676143646240234 | KNN Loss: 3.6652088165283203 | BCE Loss: 1.0109350681304932\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.740970611572266 | KNN Loss: 3.7097253799438477 | BCE Loss: 1.0312449932098389\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.710103988647461 | KNN Loss: 3.6856725215911865 | BCE Loss: 1.0244314670562744\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.716851234436035 | KNN Loss: 3.6945197582244873 | BCE Loss: 1.022331714630127\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.711040496826172 | KNN Loss: 3.6982791423797607 | BCE Loss: 1.0127613544464111\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.758014678955078 | KNN Loss: 3.7016944885253906 | BCE Loss: 1.0563199520111084\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.702394485473633 | KNN Loss: 3.6918556690216064 | BCE Loss: 1.0105385780334473\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.6708245277404785 | KNN Loss: 3.6542811393737793 | BCE Loss: 1.0165433883666992\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.68617582321167 | KNN Loss: 3.6746013164520264 | BCE Loss: 1.011574625968933\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.738812446594238 | KNN Loss: 3.7212772369384766 | BCE Loss: 1.0175353288650513\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.712685585021973 | KNN Loss: 3.6858248710632324 | BCE Loss: 1.0268607139587402\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.653742790222168 | KNN Loss: 3.6554598808288574 | BCE Loss: 0.9982827305793762\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.650284290313721 | KNN Loss: 3.6635425090789795 | BCE Loss: 0.9867419600486755\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.739551544189453 | KNN Loss: 3.6983401775360107 | BCE Loss: 1.0412113666534424\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.708196640014648 | KNN Loss: 3.6595427989959717 | BCE Loss: 1.0486540794372559\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.744699954986572 | KNN Loss: 3.721128225326538 | BCE Loss: 1.0235716104507446\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.753588676452637 | KNN Loss: 3.7081851959228516 | BCE Loss: 1.0454037189483643\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.718567848205566 | KNN Loss: 3.7044734954833984 | BCE Loss: 1.0140942335128784\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.690343856811523 | KNN Loss: 3.6818675994873047 | BCE Loss: 1.0084764957427979\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.690404891967773 | KNN Loss: 3.672874689102173 | BCE Loss: 1.0175302028656006\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.761897563934326 | KNN Loss: 3.739698886871338 | BCE Loss: 1.0221985578536987\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.719069480895996 | KNN Loss: 3.6623291969299316 | BCE Loss: 1.0567402839660645\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.740325450897217 | KNN Loss: 3.688828706741333 | BCE Loss: 1.0514966249465942\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.748531818389893 | KNN Loss: 3.7082412242889404 | BCE Loss: 1.0402905941009521\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.703000068664551 | KNN Loss: 3.6822640895843506 | BCE Loss: 1.0207362174987793\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.717258453369141 | KNN Loss: 3.6632962226867676 | BCE Loss: 1.053962230682373\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.6789374351501465 | KNN Loss: 3.6847314834594727 | BCE Loss: 0.994205892086029\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.675630569458008 | KNN Loss: 3.6483640670776367 | BCE Loss: 1.0272667407989502\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.69499397277832 | KNN Loss: 3.668253183364868 | BCE Loss: 1.0267406702041626\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.712703227996826 | KNN Loss: 3.683443307876587 | BCE Loss: 1.0292598009109497\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.667440414428711 | KNN Loss: 3.679409980773926 | BCE Loss: 0.9880306124687195\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.698147773742676 | KNN Loss: 3.67714262008667 | BCE Loss: 1.0210049152374268\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.663122177124023 | KNN Loss: 3.6731719970703125 | BCE Loss: 0.9899501800537109\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.697589874267578 | KNN Loss: 3.6681020259857178 | BCE Loss: 1.02948796749115\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.703967094421387 | KNN Loss: 3.685242176055908 | BCE Loss: 1.0187246799468994\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.773128032684326 | KNN Loss: 3.702862024307251 | BCE Loss: 1.0702660083770752\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.724976539611816 | KNN Loss: 3.6977477073669434 | BCE Loss: 1.0272287130355835\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.743108749389648 | KNN Loss: 3.689554452896118 | BCE Loss: 1.0535541772842407\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.719939708709717 | KNN Loss: 3.6808323860168457 | BCE Loss: 1.0391074419021606\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.7262678146362305 | KNN Loss: 3.712179183959961 | BCE Loss: 1.014088749885559\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.681394100189209 | KNN Loss: 3.6832525730133057 | BCE Loss: 0.9981417059898376\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.712459087371826 | KNN Loss: 3.6786856651306152 | BCE Loss: 1.0337733030319214\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.707716941833496 | KNN Loss: 3.6497926712036133 | BCE Loss: 1.0579240322113037\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.761487007141113 | KNN Loss: 3.7366459369659424 | BCE Loss: 1.024841070175171\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.69642972946167 | KNN Loss: 3.6833150386810303 | BCE Loss: 1.0131146907806396\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.763979911804199 | KNN Loss: 3.737995147705078 | BCE Loss: 1.025984525680542\n",
      "Epoch   309: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.723507881164551 | KNN Loss: 3.7356674671173096 | BCE Loss: 0.9878406524658203\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.696381568908691 | KNN Loss: 3.671698808670044 | BCE Loss: 1.0246829986572266\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.701175689697266 | KNN Loss: 3.6839168071746826 | BCE Loss: 1.017258882522583\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.658379554748535 | KNN Loss: 3.6522367000579834 | BCE Loss: 1.0061426162719727\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.722314834594727 | KNN Loss: 3.693702459335327 | BCE Loss: 1.0286126136779785\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.703560829162598 | KNN Loss: 3.672029733657837 | BCE Loss: 1.0315312147140503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.707760810852051 | KNN Loss: 3.672421932220459 | BCE Loss: 1.0353388786315918\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.724827289581299 | KNN Loss: 3.6814639568328857 | BCE Loss: 1.0433634519577026\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.732338905334473 | KNN Loss: 3.6983423233032227 | BCE Loss: 1.0339964628219604\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.681587219238281 | KNN Loss: 3.673604965209961 | BCE Loss: 1.0079824924468994\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.736474990844727 | KNN Loss: 3.693772077560425 | BCE Loss: 1.0427027940750122\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.702083587646484 | KNN Loss: 3.690814971923828 | BCE Loss: 1.0112687349319458\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.727140426635742 | KNN Loss: 3.704580545425415 | BCE Loss: 1.0225598812103271\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.6939311027526855 | KNN Loss: 3.6808786392211914 | BCE Loss: 1.0130525827407837\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.723066329956055 | KNN Loss: 3.6638641357421875 | BCE Loss: 1.0592024326324463\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.696418285369873 | KNN Loss: 3.673839807510376 | BCE Loss: 1.022578477859497\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.7571187019348145 | KNN Loss: 3.697230100631714 | BCE Loss: 1.0598887205123901\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.706730842590332 | KNN Loss: 3.6798932552337646 | BCE Loss: 1.0268373489379883\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.678717136383057 | KNN Loss: 3.656127691268921 | BCE Loss: 1.0225895643234253\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.682852745056152 | KNN Loss: 3.6592016220092773 | BCE Loss: 1.0236512422561646\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.725167274475098 | KNN Loss: 3.6856577396392822 | BCE Loss: 1.0395097732543945\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.718001842498779 | KNN Loss: 3.673940658569336 | BCE Loss: 1.0440611839294434\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.652048587799072 | KNN Loss: 3.650730609893799 | BCE Loss: 1.001318097114563\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.711618900299072 | KNN Loss: 3.701449155807495 | BCE Loss: 1.0101697444915771\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.759861469268799 | KNN Loss: 3.705012798309326 | BCE Loss: 1.0548487901687622\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.746440887451172 | KNN Loss: 3.6872365474700928 | BCE Loss: 1.0592045783996582\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.732972621917725 | KNN Loss: 3.712627410888672 | BCE Loss: 1.0203452110290527\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.693361282348633 | KNN Loss: 3.6571738719940186 | BCE Loss: 1.0361874103546143\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.743305206298828 | KNN Loss: 3.6793644428253174 | BCE Loss: 1.0639405250549316\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.671443462371826 | KNN Loss: 3.6726291179656982 | BCE Loss: 0.9988141655921936\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.777072906494141 | KNN Loss: 3.7524213790893555 | BCE Loss: 1.0246515274047852\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.724611759185791 | KNN Loss: 3.69450306892395 | BCE Loss: 1.0301085710525513\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.702734470367432 | KNN Loss: 3.678860902786255 | BCE Loss: 1.0238734483718872\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.717529773712158 | KNN Loss: 3.7061238288879395 | BCE Loss: 1.0114059448242188\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.679686069488525 | KNN Loss: 3.6773641109466553 | BCE Loss: 1.0023219585418701\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.733185291290283 | KNN Loss: 3.700038194656372 | BCE Loss: 1.0331470966339111\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.74815034866333 | KNN Loss: 3.735774278640747 | BCE Loss: 1.0123759508132935\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.70613956451416 | KNN Loss: 3.6996848583221436 | BCE Loss: 1.0064547061920166\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.735877990722656 | KNN Loss: 3.6969661712646484 | BCE Loss: 1.0389117002487183\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.775886058807373 | KNN Loss: 3.7243711948394775 | BCE Loss: 1.051514983177185\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.725318431854248 | KNN Loss: 3.682421922683716 | BCE Loss: 1.0428966283798218\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.694956302642822 | KNN Loss: 3.690091848373413 | BCE Loss: 1.0048645734786987\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.687685966491699 | KNN Loss: 3.677544593811035 | BCE Loss: 1.0101412534713745\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.802080154418945 | KNN Loss: 3.734694004058838 | BCE Loss: 1.0673859119415283\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.699895858764648 | KNN Loss: 3.6831257343292236 | BCE Loss: 1.0167701244354248\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.712928771972656 | KNN Loss: 3.7049872875213623 | BCE Loss: 1.0079412460327148\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.684857368469238 | KNN Loss: 3.6629796028137207 | BCE Loss: 1.0218780040740967\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.721301078796387 | KNN Loss: 3.7191338539123535 | BCE Loss: 1.0021673440933228\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.765289306640625 | KNN Loss: 3.7220559120178223 | BCE Loss: 1.0432336330413818\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.736104488372803 | KNN Loss: 3.691983222961426 | BCE Loss: 1.044121265411377\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.740530014038086 | KNN Loss: 3.7031137943267822 | BCE Loss: 1.0374163389205933\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.701416015625 | KNN Loss: 3.67166805267334 | BCE Loss: 1.0297479629516602\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.712616443634033 | KNN Loss: 3.7000560760498047 | BCE Loss: 1.0125603675842285\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.708765029907227 | KNN Loss: 3.6688480377197266 | BCE Loss: 1.039917230606079\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.7004547119140625 | KNN Loss: 3.6838204860687256 | BCE Loss: 1.0166343450546265\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.762252330780029 | KNN Loss: 3.742244243621826 | BCE Loss: 1.0200079679489136\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.674718856811523 | KNN Loss: 3.655012369155884 | BCE Loss: 1.0197064876556396\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.719101905822754 | KNN Loss: 3.694711446762085 | BCE Loss: 1.0243903398513794\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.68996000289917 | KNN Loss: 3.692065954208374 | BCE Loss: 0.9978941679000854\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.797167778015137 | KNN Loss: 3.748373031616211 | BCE Loss: 1.0487948656082153\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.692775726318359 | KNN Loss: 3.7166011333465576 | BCE Loss: 0.9761747121810913\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.7032294273376465 | KNN Loss: 3.6697864532470703 | BCE Loss: 1.0334430932998657\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.715304851531982 | KNN Loss: 3.6651721000671387 | BCE Loss: 1.0501327514648438\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.7312822341918945 | KNN Loss: 3.6896910667419434 | BCE Loss: 1.0415910482406616\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.682457447052002 | KNN Loss: 3.6678645610809326 | BCE Loss: 1.0145927667617798\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.709095478057861 | KNN Loss: 3.698040008544922 | BCE Loss: 1.01105535030365\n",
      "Epoch   320: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.720024108886719 | KNN Loss: 3.6878950595855713 | BCE Loss: 1.0321292877197266\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.76507043838501 | KNN Loss: 3.726594924926758 | BCE Loss: 1.038475513458252\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.724883079528809 | KNN Loss: 3.705792188644409 | BCE Loss: 1.0190911293029785\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.704516410827637 | KNN Loss: 3.6728243827819824 | BCE Loss: 1.0316922664642334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.713349342346191 | KNN Loss: 3.694822072982788 | BCE Loss: 1.0185270309448242\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.733984470367432 | KNN Loss: 3.710693836212158 | BCE Loss: 1.0232906341552734\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.720214366912842 | KNN Loss: 3.684551477432251 | BCE Loss: 1.0356627702713013\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.770184516906738 | KNN Loss: 3.7196998596191406 | BCE Loss: 1.0504846572875977\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.754359722137451 | KNN Loss: 3.7077207565307617 | BCE Loss: 1.0466389656066895\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.6976494789123535 | KNN Loss: 3.7114882469177246 | BCE Loss: 0.9861613512039185\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.7822370529174805 | KNN Loss: 3.7580761909484863 | BCE Loss: 1.0241608619689941\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.731814384460449 | KNN Loss: 3.696969747543335 | BCE Loss: 1.0348443984985352\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.674909591674805 | KNN Loss: 3.6566591262817383 | BCE Loss: 1.0182503461837769\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.688640594482422 | KNN Loss: 3.6680192947387695 | BCE Loss: 1.0206212997436523\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.692487716674805 | KNN Loss: 3.6987311840057373 | BCE Loss: 0.9937565922737122\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.695427894592285 | KNN Loss: 3.675753116607666 | BCE Loss: 1.0196747779846191\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.675312042236328 | KNN Loss: 3.6700332164764404 | BCE Loss: 1.0052788257598877\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.723744869232178 | KNN Loss: 3.7009036540985107 | BCE Loss: 1.0228413343429565\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.743463039398193 | KNN Loss: 3.723041296005249 | BCE Loss: 1.0204217433929443\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.793049335479736 | KNN Loss: 3.784547805786133 | BCE Loss: 1.008501648902893\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.70366096496582 | KNN Loss: 3.68076491355896 | BCE Loss: 1.0228960514068604\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.711803913116455 | KNN Loss: 3.6674983501434326 | BCE Loss: 1.0443055629730225\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.680992603302002 | KNN Loss: 3.6681628227233887 | BCE Loss: 1.0128298997879028\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.727647304534912 | KNN Loss: 3.7247557640075684 | BCE Loss: 1.0028914213180542\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.738772392272949 | KNN Loss: 3.6999473571777344 | BCE Loss: 1.0388251543045044\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.685520172119141 | KNN Loss: 3.6667137145996094 | BCE Loss: 1.0188063383102417\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.7377028465271 | KNN Loss: 3.7145907878875732 | BCE Loss: 1.0231120586395264\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.727695465087891 | KNN Loss: 3.692788600921631 | BCE Loss: 1.0349068641662598\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.759279251098633 | KNN Loss: 3.707914352416992 | BCE Loss: 1.0513651371002197\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.759100914001465 | KNN Loss: 3.7323246002197266 | BCE Loss: 1.0267761945724487\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.6887030601501465 | KNN Loss: 3.687685012817383 | BCE Loss: 1.0010179281234741\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.716475009918213 | KNN Loss: 3.686457872390747 | BCE Loss: 1.0300170183181763\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.747953414916992 | KNN Loss: 3.695667028427124 | BCE Loss: 1.0522863864898682\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.770416259765625 | KNN Loss: 3.738978385925293 | BCE Loss: 1.0314381122589111\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.69951057434082 | KNN Loss: 3.6881728172302246 | BCE Loss: 1.0113378763198853\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.67083740234375 | KNN Loss: 3.6786463260650635 | BCE Loss: 0.9921908974647522\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.711493015289307 | KNN Loss: 3.6609091758728027 | BCE Loss: 1.050583839416504\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.712732315063477 | KNN Loss: 3.671402931213379 | BCE Loss: 1.0413293838500977\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.666859149932861 | KNN Loss: 3.6588165760040283 | BCE Loss: 1.0080424547195435\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.731919765472412 | KNN Loss: 3.714677572250366 | BCE Loss: 1.017242193222046\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.681549072265625 | KNN Loss: 3.6651105880737305 | BCE Loss: 1.016438603401184\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.6989922523498535 | KNN Loss: 3.6601011753082275 | BCE Loss: 1.0388911962509155\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.7622833251953125 | KNN Loss: 3.718388319015503 | BCE Loss: 1.0438951253890991\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.724013805389404 | KNN Loss: 3.695021629333496 | BCE Loss: 1.0289922952651978\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.7092084884643555 | KNN Loss: 3.6759965419769287 | BCE Loss: 1.0332119464874268\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.6941328048706055 | KNN Loss: 3.655832052230835 | BCE Loss: 1.038300633430481\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.683467864990234 | KNN Loss: 3.678602457046509 | BCE Loss: 1.0048651695251465\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.699538707733154 | KNN Loss: 3.6884522438049316 | BCE Loss: 1.011086344718933\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.730587482452393 | KNN Loss: 3.6832785606384277 | BCE Loss: 1.0473089218139648\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.71054744720459 | KNN Loss: 3.6864218711853027 | BCE Loss: 1.0241258144378662\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.701920509338379 | KNN Loss: 3.659419298171997 | BCE Loss: 1.0425009727478027\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.673112869262695 | KNN Loss: 3.670905351638794 | BCE Loss: 1.0022077560424805\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.750653266906738 | KNN Loss: 3.7025203704833984 | BCE Loss: 1.0481328964233398\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.635195732116699 | KNN Loss: 3.6522724628448486 | BCE Loss: 0.9829232096672058\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.733297348022461 | KNN Loss: 3.7133264541625977 | BCE Loss: 1.0199706554412842\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.7440667152404785 | KNN Loss: 3.7094852924346924 | BCE Loss: 1.0345814228057861\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.700019359588623 | KNN Loss: 3.6976537704467773 | BCE Loss: 1.0023657083511353\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.679226875305176 | KNN Loss: 3.680021047592163 | BCE Loss: 0.999206006526947\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.698602199554443 | KNN Loss: 3.6655526161193848 | BCE Loss: 1.0330495834350586\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.7471113204956055 | KNN Loss: 3.7004129886627197 | BCE Loss: 1.0466982126235962\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.7282819747924805 | KNN Loss: 3.691957473754883 | BCE Loss: 1.0363245010375977\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.728241920471191 | KNN Loss: 3.71297550201416 | BCE Loss: 1.0152661800384521\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.685704708099365 | KNN Loss: 3.663743257522583 | BCE Loss: 1.0219615697860718\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.75237512588501 | KNN Loss: 3.7481870651245117 | BCE Loss: 1.0041881799697876\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.737476348876953 | KNN Loss: 3.710677146911621 | BCE Loss: 1.0267990827560425\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.776645183563232 | KNN Loss: 3.7248430252075195 | BCE Loss: 1.051802158355713\n",
      "Epoch   331: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.7776899337768555 | KNN Loss: 3.7614498138427734 | BCE Loss: 1.0162400007247925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.767207145690918 | KNN Loss: 3.699498414993286 | BCE Loss: 1.0677084922790527\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.7347187995910645 | KNN Loss: 3.696225166320801 | BCE Loss: 1.0384935140609741\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.773586273193359 | KNN Loss: 3.732598304748535 | BCE Loss: 1.0409879684448242\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.709778308868408 | KNN Loss: 3.681497097015381 | BCE Loss: 1.0282812118530273\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.703167915344238 | KNN Loss: 3.6793127059936523 | BCE Loss: 1.023855447769165\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.744091510772705 | KNN Loss: 3.7277071475982666 | BCE Loss: 1.016384243965149\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.726428031921387 | KNN Loss: 3.6827516555786133 | BCE Loss: 1.0436763763427734\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.6956562995910645 | KNN Loss: 3.6531338691711426 | BCE Loss: 1.0425224304199219\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.7231340408325195 | KNN Loss: 3.678886890411377 | BCE Loss: 1.0442472696304321\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.729511260986328 | KNN Loss: 3.69618558883667 | BCE Loss: 1.033325433731079\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.675095081329346 | KNN Loss: 3.6675174236297607 | BCE Loss: 1.007577657699585\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.689435958862305 | KNN Loss: 3.6680819988250732 | BCE Loss: 1.0213537216186523\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.703795433044434 | KNN Loss: 3.6803929805755615 | BCE Loss: 1.023402214050293\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.764594554901123 | KNN Loss: 3.7267072200775146 | BCE Loss: 1.037887454032898\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.726476669311523 | KNN Loss: 3.700925350189209 | BCE Loss: 1.0255510807037354\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.6632184982299805 | KNN Loss: 3.6735575199127197 | BCE Loss: 0.9896607398986816\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.66215705871582 | KNN Loss: 3.663724422454834 | BCE Loss: 0.9984328150749207\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.751656532287598 | KNN Loss: 3.7140796184539795 | BCE Loss: 1.037576675415039\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.708273887634277 | KNN Loss: 3.6759629249572754 | BCE Loss: 1.032310962677002\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.680558681488037 | KNN Loss: 3.6613283157348633 | BCE Loss: 1.0192304849624634\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.710615634918213 | KNN Loss: 3.681652307510376 | BCE Loss: 1.0289632081985474\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.714120864868164 | KNN Loss: 3.6729726791381836 | BCE Loss: 1.0411484241485596\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.707948684692383 | KNN Loss: 3.6765811443328857 | BCE Loss: 1.0313674211502075\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.695126533508301 | KNN Loss: 3.669275999069214 | BCE Loss: 1.025850534439087\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.714399337768555 | KNN Loss: 3.689500570297241 | BCE Loss: 1.024898886680603\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.688593864440918 | KNN Loss: 3.667919874191284 | BCE Loss: 1.0206737518310547\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.726916790008545 | KNN Loss: 3.703312873840332 | BCE Loss: 1.023603916168213\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.713968276977539 | KNN Loss: 3.6599345207214355 | BCE Loss: 1.0540337562561035\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.694588661193848 | KNN Loss: 3.667617082595825 | BCE Loss: 1.0269715785980225\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.720669269561768 | KNN Loss: 3.682115316390991 | BCE Loss: 1.0385539531707764\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.699891090393066 | KNN Loss: 3.6821484565734863 | BCE Loss: 1.01774263381958\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.734847068786621 | KNN Loss: 3.708420991897583 | BCE Loss: 1.0264261960983276\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.733468055725098 | KNN Loss: 3.691927671432495 | BCE Loss: 1.041540503501892\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.698883056640625 | KNN Loss: 3.6769938468933105 | BCE Loss: 1.0218894481658936\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.750401496887207 | KNN Loss: 3.720219612121582 | BCE Loss: 1.030181884765625\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.6935224533081055 | KNN Loss: 3.6754887104034424 | BCE Loss: 1.0180336236953735\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.7639546394348145 | KNN Loss: 3.7422149181365967 | BCE Loss: 1.0217397212982178\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.7063798904418945 | KNN Loss: 3.6817219257354736 | BCE Loss: 1.0246577262878418\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.702801704406738 | KNN Loss: 3.698117971420288 | BCE Loss: 1.004683494567871\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.688880443572998 | KNN Loss: 3.6571879386901855 | BCE Loss: 1.0316925048828125\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.696585655212402 | KNN Loss: 3.669517755508423 | BCE Loss: 1.0270678997039795\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.748064994812012 | KNN Loss: 3.717758893966675 | BCE Loss: 1.030306339263916\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.77459716796875 | KNN Loss: 3.7103512287139893 | BCE Loss: 1.0642461776733398\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.6941657066345215 | KNN Loss: 3.6969804763793945 | BCE Loss: 0.9971852898597717\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.67795991897583 | KNN Loss: 3.6632018089294434 | BCE Loss: 1.0147581100463867\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.707669258117676 | KNN Loss: 3.6863481998443604 | BCE Loss: 1.0213212966918945\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.692749977111816 | KNN Loss: 3.674943685531616 | BCE Loss: 1.0178061723709106\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.67813777923584 | KNN Loss: 3.6640331745147705 | BCE Loss: 1.0141048431396484\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.7096099853515625 | KNN Loss: 3.660815954208374 | BCE Loss: 1.0487942695617676\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.682196617126465 | KNN Loss: 3.6803810596466064 | BCE Loss: 1.0018155574798584\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.71320104598999 | KNN Loss: 3.6954855918884277 | BCE Loss: 1.0177154541015625\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.69023323059082 | KNN Loss: 3.681126356124878 | BCE Loss: 1.009106993675232\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.735410213470459 | KNN Loss: 3.6879117488861084 | BCE Loss: 1.047498345375061\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.703773498535156 | KNN Loss: 3.674461841583252 | BCE Loss: 1.0293115377426147\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.710849285125732 | KNN Loss: 3.6719696521759033 | BCE Loss: 1.0388797521591187\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.714837074279785 | KNN Loss: 3.7234973907470703 | BCE Loss: 0.9913396239280701\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.695695400238037 | KNN Loss: 3.6653401851654053 | BCE Loss: 1.0303553342819214\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.71388053894043 | KNN Loss: 3.6959314346313477 | BCE Loss: 1.0179493427276611\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.683918476104736 | KNN Loss: 3.6535556316375732 | BCE Loss: 1.0303627252578735\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.729451656341553 | KNN Loss: 3.676758289337158 | BCE Loss: 1.052693247795105\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.72042179107666 | KNN Loss: 3.687633991241455 | BCE Loss: 1.032787561416626\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.724280834197998 | KNN Loss: 3.7218868732452393 | BCE Loss: 1.0023938417434692\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.69223165512085 | KNN Loss: 3.661855936050415 | BCE Loss: 1.0303757190704346\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.685721397399902 | KNN Loss: 3.676471471786499 | BCE Loss: 1.0092499256134033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.64122200012207 | KNN Loss: 3.65218448638916 | BCE Loss: 0.9890373349189758\n",
      "Epoch   342: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.728679656982422 | KNN Loss: 3.7024009227752686 | BCE Loss: 1.0262789726257324\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.704317092895508 | KNN Loss: 3.6968178749084473 | BCE Loss: 1.0074992179870605\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.727871894836426 | KNN Loss: 3.670689105987549 | BCE Loss: 1.057182788848877\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.725987434387207 | KNN Loss: 3.6991994380950928 | BCE Loss: 1.0267881155014038\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.7078447341918945 | KNN Loss: 3.688105821609497 | BCE Loss: 1.0197389125823975\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.7216315269470215 | KNN Loss: 3.702838659286499 | BCE Loss: 1.018792748451233\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.726799488067627 | KNN Loss: 3.684319496154785 | BCE Loss: 1.0424799919128418\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.700933456420898 | KNN Loss: 3.6853208541870117 | BCE Loss: 1.0156127214431763\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.7433552742004395 | KNN Loss: 3.6852407455444336 | BCE Loss: 1.0581144094467163\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.710505962371826 | KNN Loss: 3.6680023670196533 | BCE Loss: 1.0425035953521729\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.703653812408447 | KNN Loss: 3.6841049194335938 | BCE Loss: 1.0195488929748535\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.661499977111816 | KNN Loss: 3.651805877685547 | BCE Loss: 1.0096938610076904\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.736690998077393 | KNN Loss: 3.7157177925109863 | BCE Loss: 1.0209733247756958\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.683259963989258 | KNN Loss: 3.6568236351013184 | BCE Loss: 1.02643620967865\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.716365814208984 | KNN Loss: 3.6850531101226807 | BCE Loss: 1.0313128232955933\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.715690612792969 | KNN Loss: 3.707075595855713 | BCE Loss: 1.0086151361465454\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.704810619354248 | KNN Loss: 3.6990599632263184 | BCE Loss: 1.0057505369186401\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.669426918029785 | KNN Loss: 3.6487913131713867 | BCE Loss: 1.0206353664398193\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.711823463439941 | KNN Loss: 3.693434238433838 | BCE Loss: 1.0183889865875244\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.7118940353393555 | KNN Loss: 3.6928040981292725 | BCE Loss: 1.019089698791504\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.745662689208984 | KNN Loss: 3.701866626739502 | BCE Loss: 1.0437960624694824\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.772369384765625 | KNN Loss: 3.714310646057129 | BCE Loss: 1.0580589771270752\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.709561347961426 | KNN Loss: 3.696582555770874 | BCE Loss: 1.0129789113998413\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.718543529510498 | KNN Loss: 3.6864333152770996 | BCE Loss: 1.0321102142333984\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.700770378112793 | KNN Loss: 3.682856798171997 | BCE Loss: 1.0179136991500854\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.681798458099365 | KNN Loss: 3.6568520069122314 | BCE Loss: 1.0249465703964233\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.686417102813721 | KNN Loss: 3.6723971366882324 | BCE Loss: 1.0140198469161987\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.719881057739258 | KNN Loss: 3.6845932006835938 | BCE Loss: 1.0352879762649536\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.725444793701172 | KNN Loss: 3.7078263759613037 | BCE Loss: 1.0176182985305786\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.711163520812988 | KNN Loss: 3.657297134399414 | BCE Loss: 1.0538666248321533\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.688088417053223 | KNN Loss: 3.6822879314422607 | BCE Loss: 1.005800485610962\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.6836981773376465 | KNN Loss: 3.6680612564086914 | BCE Loss: 1.0156368017196655\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.740610599517822 | KNN Loss: 3.7201743125915527 | BCE Loss: 1.020436406135559\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.68311882019043 | KNN Loss: 3.6608293056488037 | BCE Loss: 1.0222896337509155\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.716771602630615 | KNN Loss: 3.693722724914551 | BCE Loss: 1.0230488777160645\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.797891616821289 | KNN Loss: 3.746458053588867 | BCE Loss: 1.0514334440231323\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.6925458908081055 | KNN Loss: 3.6796772480010986 | BCE Loss: 1.0128685235977173\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.706172943115234 | KNN Loss: 3.682645320892334 | BCE Loss: 1.0235278606414795\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.743027687072754 | KNN Loss: 3.6865181922912598 | BCE Loss: 1.056509256362915\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.723365783691406 | KNN Loss: 3.700037717819214 | BCE Loss: 1.0233279466629028\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.677655220031738 | KNN Loss: 3.6579301357269287 | BCE Loss: 1.0197252035140991\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.705423355102539 | KNN Loss: 3.6937170028686523 | BCE Loss: 1.0117064714431763\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.670497894287109 | KNN Loss: 3.6602938175201416 | BCE Loss: 1.0102043151855469\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.719841957092285 | KNN Loss: 3.6804869174957275 | BCE Loss: 1.0393548011779785\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.676243782043457 | KNN Loss: 3.6608567237854004 | BCE Loss: 1.0153871774673462\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.705878257751465 | KNN Loss: 3.665557384490967 | BCE Loss: 1.0403211116790771\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.70512580871582 | KNN Loss: 3.7001383304595947 | BCE Loss: 1.0049875974655151\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.726551055908203 | KNN Loss: 3.6804616451263428 | BCE Loss: 1.0460896492004395\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.753911972045898 | KNN Loss: 3.7034270763397217 | BCE Loss: 1.0504847764968872\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.68048620223999 | KNN Loss: 3.6595540046691895 | BCE Loss: 1.0209323167800903\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.7130560874938965 | KNN Loss: 3.6970527172088623 | BCE Loss: 1.0160032510757446\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.696002960205078 | KNN Loss: 3.69260835647583 | BCE Loss: 1.0033947229385376\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.671657085418701 | KNN Loss: 3.6472623348236084 | BCE Loss: 1.0243946313858032\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.759669303894043 | KNN Loss: 3.70590877532959 | BCE Loss: 1.0537604093551636\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.698754787445068 | KNN Loss: 3.6891229152679443 | BCE Loss: 1.0096317529678345\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.729905605316162 | KNN Loss: 3.6980531215667725 | BCE Loss: 1.0318523645401\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.674625873565674 | KNN Loss: 3.655745267868042 | BCE Loss: 1.0188806056976318\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.714622497558594 | KNN Loss: 3.7126781940460205 | BCE Loss: 1.0019444227218628\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.655949592590332 | KNN Loss: 3.6636879444122314 | BCE Loss: 0.9922616481781006\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.760009765625 | KNN Loss: 3.7137670516967773 | BCE Loss: 1.0462424755096436\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.735074996948242 | KNN Loss: 3.675513505935669 | BCE Loss: 1.0595616102218628\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.755033493041992 | KNN Loss: 3.7100253105163574 | BCE Loss: 1.0450081825256348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.763634204864502 | KNN Loss: 3.70817494392395 | BCE Loss: 1.0554591417312622\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.733028888702393 | KNN Loss: 3.6872875690460205 | BCE Loss: 1.045741319656372\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.73328161239624 | KNN Loss: 3.696939706802368 | BCE Loss: 1.0363420248031616\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.719284534454346 | KNN Loss: 3.6873185634613037 | BCE Loss: 1.031965970993042\n",
      "Epoch   353: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.757143497467041 | KNN Loss: 3.705756187438965 | BCE Loss: 1.0513871908187866\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.666490077972412 | KNN Loss: 3.650517702102661 | BCE Loss: 1.015972375869751\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.712058067321777 | KNN Loss: 3.685976982116699 | BCE Loss: 1.0260813236236572\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.703032970428467 | KNN Loss: 3.672013759613037 | BCE Loss: 1.0310193300247192\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.699103355407715 | KNN Loss: 3.6939339637756348 | BCE Loss: 1.005169153213501\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.705085754394531 | KNN Loss: 3.6715140342712402 | BCE Loss: 1.0335719585418701\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.715612888336182 | KNN Loss: 3.693920373916626 | BCE Loss: 1.0216923952102661\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.720609664916992 | KNN Loss: 3.6928348541259766 | BCE Loss: 1.0277749300003052\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.655768394470215 | KNN Loss: 3.6574442386627197 | BCE Loss: 0.998323917388916\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.731807231903076 | KNN Loss: 3.707789659500122 | BCE Loss: 1.0240176916122437\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.744736671447754 | KNN Loss: 3.6958487033843994 | BCE Loss: 1.0488882064819336\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.775653839111328 | KNN Loss: 3.7087748050689697 | BCE Loss: 1.0668787956237793\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.759779453277588 | KNN Loss: 3.7200348377227783 | BCE Loss: 1.0397447347640991\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.680980205535889 | KNN Loss: 3.6858632564544678 | BCE Loss: 0.9951168298721313\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.75312614440918 | KNN Loss: 3.7203762531280518 | BCE Loss: 1.0327496528625488\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.7174859046936035 | KNN Loss: 3.6748738288879395 | BCE Loss: 1.042612075805664\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.680480480194092 | KNN Loss: 3.669722318649292 | BCE Loss: 1.0107580423355103\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.735020637512207 | KNN Loss: 3.708660125732422 | BCE Loss: 1.0263606309890747\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.704930782318115 | KNN Loss: 3.6940715312957764 | BCE Loss: 1.0108593702316284\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.7403669357299805 | KNN Loss: 3.6880335807800293 | BCE Loss: 1.052333116531372\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.699400424957275 | KNN Loss: 3.674227476119995 | BCE Loss: 1.0251730680465698\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.715494632720947 | KNN Loss: 3.6860287189483643 | BCE Loss: 1.0294660329818726\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.711507797241211 | KNN Loss: 3.6772851943969727 | BCE Loss: 1.0342224836349487\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.702503204345703 | KNN Loss: 3.6684353351593018 | BCE Loss: 1.0340681076049805\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.7005109786987305 | KNN Loss: 3.6653635501861572 | BCE Loss: 1.0351471900939941\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.735740661621094 | KNN Loss: 3.7304654121398926 | BCE Loss: 1.0052754878997803\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.72537088394165 | KNN Loss: 3.6751298904418945 | BCE Loss: 1.0502409934997559\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.68156099319458 | KNN Loss: 3.6879873275756836 | BCE Loss: 0.9935734868049622\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.663395404815674 | KNN Loss: 3.6520793437957764 | BCE Loss: 1.011316180229187\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.715091228485107 | KNN Loss: 3.6892833709716797 | BCE Loss: 1.0258077383041382\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.714921474456787 | KNN Loss: 3.689535140991211 | BCE Loss: 1.0253862142562866\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.7758402824401855 | KNN Loss: 3.7375900745391846 | BCE Loss: 1.0382500886917114\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.672183036804199 | KNN Loss: 3.6683542728424072 | BCE Loss: 1.0038288831710815\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.688990592956543 | KNN Loss: 3.673837900161743 | BCE Loss: 1.0151524543762207\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.696064472198486 | KNN Loss: 3.6715099811553955 | BCE Loss: 1.0245546102523804\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.806697845458984 | KNN Loss: 3.740832567214966 | BCE Loss: 1.0658650398254395\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.705200672149658 | KNN Loss: 3.6844558715820312 | BCE Loss: 1.020744800567627\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.701554298400879 | KNN Loss: 3.658665895462036 | BCE Loss: 1.0428884029388428\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.673079967498779 | KNN Loss: 3.699099063873291 | BCE Loss: 0.9739810228347778\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.694792747497559 | KNN Loss: 3.6825358867645264 | BCE Loss: 1.0122567415237427\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.704248428344727 | KNN Loss: 3.6969106197357178 | BCE Loss: 1.0073375701904297\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.698601245880127 | KNN Loss: 3.688828706741333 | BCE Loss: 1.0097726583480835\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.701053142547607 | KNN Loss: 3.668586492538452 | BCE Loss: 1.0324667692184448\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.699617862701416 | KNN Loss: 3.6967568397521973 | BCE Loss: 1.0028611421585083\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.7410688400268555 | KNN Loss: 3.7107882499694824 | BCE Loss: 1.0302808284759521\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.7481536865234375 | KNN Loss: 3.700428009033203 | BCE Loss: 1.0477255582809448\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.700197696685791 | KNN Loss: 3.6742589473724365 | BCE Loss: 1.025938868522644\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.727276802062988 | KNN Loss: 3.7005207538604736 | BCE Loss: 1.0267561674118042\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.713818550109863 | KNN Loss: 3.686793565750122 | BCE Loss: 1.0270251035690308\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.689009666442871 | KNN Loss: 3.6846225261688232 | BCE Loss: 1.0043870210647583\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.6876068115234375 | KNN Loss: 3.6878106594085693 | BCE Loss: 0.9997959136962891\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.722317218780518 | KNN Loss: 3.693209171295166 | BCE Loss: 1.0291081666946411\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.71623420715332 | KNN Loss: 3.6973605155944824 | BCE Loss: 1.0188738107681274\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.71661901473999 | KNN Loss: 3.7070748805999756 | BCE Loss: 1.0095442533493042\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.698802471160889 | KNN Loss: 3.7013933658599854 | BCE Loss: 0.9974091053009033\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.751133918762207 | KNN Loss: 3.7246553897857666 | BCE Loss: 1.0264782905578613\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.712787628173828 | KNN Loss: 3.7103588581085205 | BCE Loss: 1.002428650856018\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.7516632080078125 | KNN Loss: 3.7390432357788086 | BCE Loss: 1.0126198530197144\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.7295451164245605 | KNN Loss: 3.7109272480010986 | BCE Loss: 1.018617868423462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.698068618774414 | KNN Loss: 3.6694324016571045 | BCE Loss: 1.02863609790802\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.7259016036987305 | KNN Loss: 3.683556079864502 | BCE Loss: 1.0423457622528076\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.723474502563477 | KNN Loss: 3.67948317527771 | BCE Loss: 1.0439913272857666\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.710903167724609 | KNN Loss: 3.6999237537384033 | BCE Loss: 1.0109796524047852\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.6990966796875 | KNN Loss: 3.6848998069763184 | BCE Loss: 1.0141966342926025\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.719333171844482 | KNN Loss: 3.681230306625366 | BCE Loss: 1.0381029844284058\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.683670997619629 | KNN Loss: 3.667605400085449 | BCE Loss: 1.0160653591156006\n",
      "Epoch   364: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.723168849945068 | KNN Loss: 3.6664721965789795 | BCE Loss: 1.0566965341567993\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.752457141876221 | KNN Loss: 3.729086399078369 | BCE Loss: 1.0233708620071411\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.719280242919922 | KNN Loss: 3.6691510677337646 | BCE Loss: 1.0501289367675781\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.7225847244262695 | KNN Loss: 3.702608346939087 | BCE Loss: 1.0199761390686035\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.740335941314697 | KNN Loss: 3.7331438064575195 | BCE Loss: 1.0071922540664673\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.7400736808776855 | KNN Loss: 3.68196439743042 | BCE Loss: 1.058109164237976\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.688133716583252 | KNN Loss: 3.6567347049713135 | BCE Loss: 1.031399130821228\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.745520114898682 | KNN Loss: 3.692087411880493 | BCE Loss: 1.053432583808899\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.736090183258057 | KNN Loss: 3.6855361461639404 | BCE Loss: 1.0505539178848267\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.686122417449951 | KNN Loss: 3.6670570373535156 | BCE Loss: 1.019065499305725\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.7098069190979 | KNN Loss: 3.67919921875 | BCE Loss: 1.0306077003479004\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.706093788146973 | KNN Loss: 3.682917594909668 | BCE Loss: 1.0231759548187256\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.703801155090332 | KNN Loss: 3.677786111831665 | BCE Loss: 1.026015281677246\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.724269866943359 | KNN Loss: 3.7133378982543945 | BCE Loss: 1.0109320878982544\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.704730987548828 | KNN Loss: 3.673938512802124 | BCE Loss: 1.030792236328125\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.747202396392822 | KNN Loss: 3.7045390605926514 | BCE Loss: 1.0426632165908813\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.696529388427734 | KNN Loss: 3.657909393310547 | BCE Loss: 1.0386202335357666\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.700081825256348 | KNN Loss: 3.6643195152282715 | BCE Loss: 1.0357625484466553\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.690238952636719 | KNN Loss: 3.6879947185516357 | BCE Loss: 1.0022443532943726\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.685232639312744 | KNN Loss: 3.659261703491211 | BCE Loss: 1.0259708166122437\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.719759941101074 | KNN Loss: 3.6789631843566895 | BCE Loss: 1.0407965183258057\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.676063060760498 | KNN Loss: 3.6694769859313965 | BCE Loss: 1.006585955619812\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.674379825592041 | KNN Loss: 3.6901795864105225 | BCE Loss: 0.9842001795768738\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.695463180541992 | KNN Loss: 3.666926383972168 | BCE Loss: 1.0285367965698242\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.708617210388184 | KNN Loss: 3.6761419773101807 | BCE Loss: 1.032475471496582\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.688570976257324 | KNN Loss: 3.6740779876708984 | BCE Loss: 1.0144932270050049\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.7075090408325195 | KNN Loss: 3.680102586746216 | BCE Loss: 1.0274062156677246\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.787076950073242 | KNN Loss: 3.7236437797546387 | BCE Loss: 1.0634331703186035\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.710556983947754 | KNN Loss: 3.7065324783325195 | BCE Loss: 1.0040247440338135\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.734292984008789 | KNN Loss: 3.6983964443206787 | BCE Loss: 1.0358964204788208\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.695833683013916 | KNN Loss: 3.6766343116760254 | BCE Loss: 1.0191994905471802\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.709537506103516 | KNN Loss: 3.6736199855804443 | BCE Loss: 1.0359172821044922\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.692801475524902 | KNN Loss: 3.674696922302246 | BCE Loss: 1.0181046724319458\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.703225612640381 | KNN Loss: 3.6720125675201416 | BCE Loss: 1.0312129259109497\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.7052226066589355 | KNN Loss: 3.66731333732605 | BCE Loss: 1.0379091501235962\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.706913471221924 | KNN Loss: 3.7064385414123535 | BCE Loss: 1.0004749298095703\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.6963043212890625 | KNN Loss: 3.671142101287842 | BCE Loss: 1.0251622200012207\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.706862449645996 | KNN Loss: 3.683431625366211 | BCE Loss: 1.0234309434890747\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.734391212463379 | KNN Loss: 3.7000584602355957 | BCE Loss: 1.034332513809204\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.669562339782715 | KNN Loss: 3.658597946166992 | BCE Loss: 1.0109645128250122\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.7424116134643555 | KNN Loss: 3.6923558712005615 | BCE Loss: 1.0500555038452148\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.680915832519531 | KNN Loss: 3.6550097465515137 | BCE Loss: 1.0259062051773071\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.739511013031006 | KNN Loss: 3.6953346729278564 | BCE Loss: 1.044176459312439\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.704246520996094 | KNN Loss: 3.6892597675323486 | BCE Loss: 1.0149868726730347\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.678016662597656 | KNN Loss: 3.654202699661255 | BCE Loss: 1.023814082145691\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.748188018798828 | KNN Loss: 3.730376720428467 | BCE Loss: 1.0178112983703613\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.676599025726318 | KNN Loss: 3.6763198375701904 | BCE Loss: 1.0002790689468384\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.7515387535095215 | KNN Loss: 3.7037830352783203 | BCE Loss: 1.0477558374404907\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.725700378417969 | KNN Loss: 3.6957132816314697 | BCE Loss: 1.02998685836792\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.702341556549072 | KNN Loss: 3.684507131576538 | BCE Loss: 1.0178343057632446\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.68853759765625 | KNN Loss: 3.6732301712036133 | BCE Loss: 1.0153071880340576\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.722845554351807 | KNN Loss: 3.6808252334594727 | BCE Loss: 1.0420204401016235\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.696220874786377 | KNN Loss: 3.690368890762329 | BCE Loss: 1.0058521032333374\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.718342304229736 | KNN Loss: 3.685955286026001 | BCE Loss: 1.0323868989944458\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.667113780975342 | KNN Loss: 3.6714022159576416 | BCE Loss: 0.9957116842269897\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.662737846374512 | KNN Loss: 3.6510233879089355 | BCE Loss: 1.0117143392562866\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.704005718231201 | KNN Loss: 3.7020516395568848 | BCE Loss: 1.001954197883606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.732789039611816 | KNN Loss: 3.7025368213653564 | BCE Loss: 1.03025221824646\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.736299991607666 | KNN Loss: 3.6944735050201416 | BCE Loss: 1.0418264865875244\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.729044437408447 | KNN Loss: 3.6954715251922607 | BCE Loss: 1.033573031425476\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.685482025146484 | KNN Loss: 3.6864864826202393 | BCE Loss: 0.9989956021308899\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.71626091003418 | KNN Loss: 3.683424472808838 | BCE Loss: 1.0328365564346313\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.678765296936035 | KNN Loss: 3.661109685897827 | BCE Loss: 1.017655611038208\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.729079246520996 | KNN Loss: 3.6952106952667236 | BCE Loss: 1.0338685512542725\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.72708797454834 | KNN Loss: 3.6894328594207764 | BCE Loss: 1.037654995918274\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.798182487487793 | KNN Loss: 3.7415997982025146 | BCE Loss: 1.0565828084945679\n",
      "Epoch   375: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.711016654968262 | KNN Loss: 3.6842517852783203 | BCE Loss: 1.0267648696899414\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.741291522979736 | KNN Loss: 3.715441942214966 | BCE Loss: 1.02584969997406\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.719300270080566 | KNN Loss: 3.6735925674438477 | BCE Loss: 1.0457079410552979\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.721273422241211 | KNN Loss: 3.6986916065216064 | BCE Loss: 1.022581696510315\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.722744941711426 | KNN Loss: 3.714320421218872 | BCE Loss: 1.0084246397018433\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.715900421142578 | KNN Loss: 3.691204071044922 | BCE Loss: 1.0246963500976562\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.740316390991211 | KNN Loss: 3.71628475189209 | BCE Loss: 1.024031639099121\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.711578369140625 | KNN Loss: 3.683570384979248 | BCE Loss: 1.0280077457427979\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.690322399139404 | KNN Loss: 3.6903579235076904 | BCE Loss: 0.9999644160270691\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.7239179611206055 | KNN Loss: 3.717597723007202 | BCE Loss: 1.0063202381134033\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.702557563781738 | KNN Loss: 3.679924488067627 | BCE Loss: 1.0226328372955322\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.7479448318481445 | KNN Loss: 3.716984510421753 | BCE Loss: 1.0309604406356812\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.736102104187012 | KNN Loss: 3.7209770679473877 | BCE Loss: 1.0151252746582031\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.7189836502075195 | KNN Loss: 3.6850011348724365 | BCE Loss: 1.033982276916504\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.653106689453125 | KNN Loss: 3.68670654296875 | BCE Loss: 0.966400146484375\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.740617275238037 | KNN Loss: 3.724506139755249 | BCE Loss: 1.016111135482788\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.680644989013672 | KNN Loss: 3.6515302658081055 | BCE Loss: 1.0291147232055664\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.733061790466309 | KNN Loss: 3.6679162979125977 | BCE Loss: 1.0651452541351318\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.683982849121094 | KNN Loss: 3.6612708568573 | BCE Loss: 1.0227117538452148\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.719837188720703 | KNN Loss: 3.689765453338623 | BCE Loss: 1.0300716161727905\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.720368385314941 | KNN Loss: 3.6877453327178955 | BCE Loss: 1.032623052597046\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.7053937911987305 | KNN Loss: 3.679722785949707 | BCE Loss: 1.0256707668304443\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.676455497741699 | KNN Loss: 3.6611053943634033 | BCE Loss: 1.015350341796875\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.734401226043701 | KNN Loss: 3.676429033279419 | BCE Loss: 1.0579721927642822\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.678407669067383 | KNN Loss: 3.674926996231079 | BCE Loss: 1.0034807920455933\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.746109962463379 | KNN Loss: 3.684595823287964 | BCE Loss: 1.061514139175415\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.722148895263672 | KNN Loss: 3.687809944152832 | BCE Loss: 1.0343389511108398\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.795144557952881 | KNN Loss: 3.7553350925445557 | BCE Loss: 1.0398095846176147\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.71115255355835 | KNN Loss: 3.674757242202759 | BCE Loss: 1.0363953113555908\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.719160556793213 | KNN Loss: 3.6980319023132324 | BCE Loss: 1.02112877368927\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.747986316680908 | KNN Loss: 3.721522569656372 | BCE Loss: 1.0264637470245361\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.7536516189575195 | KNN Loss: 3.711785078048706 | BCE Loss: 1.0418665409088135\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.788967132568359 | KNN Loss: 3.7254509925842285 | BCE Loss: 1.0635160207748413\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.755991458892822 | KNN Loss: 3.7170820236206055 | BCE Loss: 1.0389094352722168\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.7903265953063965 | KNN Loss: 3.737149715423584 | BCE Loss: 1.053176760673523\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.724603652954102 | KNN Loss: 3.6909637451171875 | BCE Loss: 1.033639669418335\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.7412872314453125 | KNN Loss: 3.700978994369507 | BCE Loss: 1.0403084754943848\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.718709945678711 | KNN Loss: 3.669867515563965 | BCE Loss: 1.048842430114746\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.692351341247559 | KNN Loss: 3.6749536991119385 | BCE Loss: 1.0173976421356201\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.728579521179199 | KNN Loss: 3.7087719440460205 | BCE Loss: 1.0198076963424683\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.723887920379639 | KNN Loss: 3.691287040710449 | BCE Loss: 1.0326008796691895\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.789982795715332 | KNN Loss: 3.7397708892822266 | BCE Loss: 1.0502119064331055\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.754578113555908 | KNN Loss: 3.700519323348999 | BCE Loss: 1.0540586709976196\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.7071533203125 | KNN Loss: 3.693089723587036 | BCE Loss: 1.0140637159347534\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.673758506774902 | KNN Loss: 3.672300100326538 | BCE Loss: 1.0014585256576538\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.64377498626709 | KNN Loss: 3.651265859603882 | BCE Loss: 0.9925089478492737\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.731184005737305 | KNN Loss: 3.699061155319214 | BCE Loss: 1.0321228504180908\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.655510425567627 | KNN Loss: 3.648223876953125 | BCE Loss: 1.0072864294052124\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.743305206298828 | KNN Loss: 3.7137722969055176 | BCE Loss: 1.0295326709747314\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.727016925811768 | KNN Loss: 3.7001593112945557 | BCE Loss: 1.026857614517212\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.715044021606445 | KNN Loss: 3.715102434158325 | BCE Loss: 0.9999417066574097\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.706425666809082 | KNN Loss: 3.6702523231506348 | BCE Loss: 1.0361733436584473\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.721729755401611 | KNN Loss: 3.676142692565918 | BCE Loss: 1.045587182044983\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.729761123657227 | KNN Loss: 3.6839382648468018 | BCE Loss: 1.045823097229004\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.677981376647949 | KNN Loss: 3.6642138957977295 | BCE Loss: 1.0137677192687988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.66320276260376 | KNN Loss: 3.667816400527954 | BCE Loss: 0.9953862428665161\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.751406192779541 | KNN Loss: 3.7286503314971924 | BCE Loss: 1.0227559804916382\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.720571517944336 | KNN Loss: 3.703176259994507 | BCE Loss: 1.017395257949829\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.756339073181152 | KNN Loss: 3.732741355895996 | BCE Loss: 1.0235974788665771\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.729302406311035 | KNN Loss: 3.689518690109253 | BCE Loss: 1.0397834777832031\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.731909275054932 | KNN Loss: 3.6979174613952637 | BCE Loss: 1.0339919328689575\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.70383358001709 | KNN Loss: 3.6813950538635254 | BCE Loss: 1.0224382877349854\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.7687296867370605 | KNN Loss: 3.719104766845703 | BCE Loss: 1.049625039100647\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.689233303070068 | KNN Loss: 3.6909279823303223 | BCE Loss: 0.9983053207397461\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.718926906585693 | KNN Loss: 3.681267023086548 | BCE Loss: 1.0376598834991455\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.701412677764893 | KNN Loss: 3.678518056869507 | BCE Loss: 1.0228947401046753\n",
      "Epoch   386: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.671339988708496 | KNN Loss: 3.658956527709961 | BCE Loss: 1.0123834609985352\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.668285369873047 | KNN Loss: 3.6786468029022217 | BCE Loss: 0.9896387457847595\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.749828815460205 | KNN Loss: 3.7208445072174072 | BCE Loss: 1.0289844274520874\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.731240749359131 | KNN Loss: 3.7065296173095703 | BCE Loss: 1.0247111320495605\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.698276042938232 | KNN Loss: 3.670720338821411 | BCE Loss: 1.0275557041168213\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.715914249420166 | KNN Loss: 3.6704940795898438 | BCE Loss: 1.0454202890396118\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.731057167053223 | KNN Loss: 3.6906588077545166 | BCE Loss: 1.0403982400894165\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.706418037414551 | KNN Loss: 3.683089017868042 | BCE Loss: 1.023329257965088\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.690902233123779 | KNN Loss: 3.6689274311065674 | BCE Loss: 1.0219749212265015\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.716772556304932 | KNN Loss: 3.70233416557312 | BCE Loss: 1.0144383907318115\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.715889930725098 | KNN Loss: 3.6752076148986816 | BCE Loss: 1.0406824350357056\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.683015823364258 | KNN Loss: 3.6746513843536377 | BCE Loss: 1.0083643198013306\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.734619140625 | KNN Loss: 3.6865382194519043 | BCE Loss: 1.0480811595916748\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.689633846282959 | KNN Loss: 3.6660680770874023 | BCE Loss: 1.0235658884048462\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.704950332641602 | KNN Loss: 3.660299301147461 | BCE Loss: 1.0446507930755615\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.67879581451416 | KNN Loss: 3.6774632930755615 | BCE Loss: 1.0013322830200195\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.720968723297119 | KNN Loss: 3.69132924079895 | BCE Loss: 1.029639482498169\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.741548538208008 | KNN Loss: 3.7370855808258057 | BCE Loss: 1.0044631958007812\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.716974258422852 | KNN Loss: 3.672027349472046 | BCE Loss: 1.0449469089508057\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.755120277404785 | KNN Loss: 3.717761278152466 | BCE Loss: 1.0373587608337402\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.749011993408203 | KNN Loss: 3.6924021244049072 | BCE Loss: 1.056609869003296\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.71785306930542 | KNN Loss: 3.708552837371826 | BCE Loss: 1.0093002319335938\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.691946983337402 | KNN Loss: 3.691673994064331 | BCE Loss: 1.0002729892730713\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.718425750732422 | KNN Loss: 3.692107915878296 | BCE Loss: 1.026317834854126\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.705442428588867 | KNN Loss: 3.6888070106506348 | BCE Loss: 1.0166356563568115\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.718790054321289 | KNN Loss: 3.679858446121216 | BCE Loss: 1.0389317274093628\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.701972961425781 | KNN Loss: 3.6745359897613525 | BCE Loss: 1.0274369716644287\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.7358198165893555 | KNN Loss: 3.728759765625 | BCE Loss: 1.0070602893829346\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.7095136642456055 | KNN Loss: 3.705277681350708 | BCE Loss: 1.0042357444763184\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.744353294372559 | KNN Loss: 3.6971774101257324 | BCE Loss: 1.0471760034561157\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.688121318817139 | KNN Loss: 3.6752777099609375 | BCE Loss: 1.0128437280654907\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.710421085357666 | KNN Loss: 3.6612892150878906 | BCE Loss: 1.049131989479065\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.717870712280273 | KNN Loss: 3.6909003257751465 | BCE Loss: 1.026970624923706\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.7354559898376465 | KNN Loss: 3.706829071044922 | BCE Loss: 1.0286269187927246\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.792572021484375 | KNN Loss: 3.7122995853424072 | BCE Loss: 1.0802724361419678\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.6941680908203125 | KNN Loss: 3.6706764698028564 | BCE Loss: 1.023491382598877\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.688169002532959 | KNN Loss: 3.6667261123657227 | BCE Loss: 1.0214430093765259\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.734317779541016 | KNN Loss: 3.6919453144073486 | BCE Loss: 1.042372226715088\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.696495056152344 | KNN Loss: 3.6830828189849854 | BCE Loss: 1.0134121179580688\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.716210842132568 | KNN Loss: 3.6810200214385986 | BCE Loss: 1.0351909399032593\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.723949432373047 | KNN Loss: 3.717874050140381 | BCE Loss: 1.0060755014419556\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.714936256408691 | KNN Loss: 3.6857926845550537 | BCE Loss: 1.0291438102722168\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.732661724090576 | KNN Loss: 3.6814894676208496 | BCE Loss: 1.0511723756790161\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.721737861633301 | KNN Loss: 3.6765024662017822 | BCE Loss: 1.0452351570129395\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.716891288757324 | KNN Loss: 3.705155849456787 | BCE Loss: 1.0117356777191162\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.709883213043213 | KNN Loss: 3.686880350112915 | BCE Loss: 1.0230029821395874\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.698830604553223 | KNN Loss: 3.699265956878662 | BCE Loss: 0.9995645880699158\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.6957807540893555 | KNN Loss: 3.688220500946045 | BCE Loss: 1.0075602531433105\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.690493583679199 | KNN Loss: 3.6859216690063477 | BCE Loss: 1.004571795463562\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.7432780265808105 | KNN Loss: 3.729180097579956 | BCE Loss: 1.0140979290008545\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.700681209564209 | KNN Loss: 3.6910228729248047 | BCE Loss: 1.0096584558486938\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.744430065155029 | KNN Loss: 3.6824378967285156 | BCE Loss: 1.0619921684265137\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.743099689483643 | KNN Loss: 3.694786548614502 | BCE Loss: 1.048313021659851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.76121711730957 | KNN Loss: 3.7254230976104736 | BCE Loss: 1.0357942581176758\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.70926570892334 | KNN Loss: 3.6817100048065186 | BCE Loss: 1.0275557041168213\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.73371696472168 | KNN Loss: 3.706780433654785 | BCE Loss: 1.0269367694854736\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.725086212158203 | KNN Loss: 3.71771502494812 | BCE Loss: 1.0073710680007935\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.682054042816162 | KNN Loss: 3.680115222930908 | BCE Loss: 1.0019389390945435\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.712334632873535 | KNN Loss: 3.704024314880371 | BCE Loss: 1.0083105564117432\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.697964191436768 | KNN Loss: 3.688457727432251 | BCE Loss: 1.0095064640045166\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.704105377197266 | KNN Loss: 3.6885366439819336 | BCE Loss: 1.0155689716339111\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.774247169494629 | KNN Loss: 3.7057363986968994 | BCE Loss: 1.0685107707977295\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.735964775085449 | KNN Loss: 3.7029004096984863 | BCE Loss: 1.0330641269683838\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.703194618225098 | KNN Loss: 3.6830313205718994 | BCE Loss: 1.0201630592346191\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.731505393981934 | KNN Loss: 3.695077419281006 | BCE Loss: 1.0364277362823486\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.709230422973633 | KNN Loss: 3.680201530456543 | BCE Loss: 1.0290288925170898\n",
      "Epoch   397: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.6798996925354 | KNN Loss: 3.683044910430908 | BCE Loss: 0.9968546628952026\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.748241424560547 | KNN Loss: 3.7416372299194336 | BCE Loss: 1.0066041946411133\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.722012519836426 | KNN Loss: 3.682907819747925 | BCE Loss: 1.0391045808792114\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.7270283699035645 | KNN Loss: 3.6952340602874756 | BCE Loss: 1.0317943096160889\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.740763187408447 | KNN Loss: 3.6933109760284424 | BCE Loss: 1.0474522113800049\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.728287696838379 | KNN Loss: 3.7007298469543457 | BCE Loss: 1.0275579690933228\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.710452079772949 | KNN Loss: 3.6849637031555176 | BCE Loss: 1.0254884958267212\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.712685585021973 | KNN Loss: 3.667288064956665 | BCE Loss: 1.0453972816467285\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.670492172241211 | KNN Loss: 3.663266897201538 | BCE Loss: 1.007225513458252\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.699186325073242 | KNN Loss: 3.690915107727051 | BCE Loss: 1.0082709789276123\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.726040363311768 | KNN Loss: 3.6996257305145264 | BCE Loss: 1.0264145135879517\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.695065498352051 | KNN Loss: 3.6586074829101562 | BCE Loss: 1.0364580154418945\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.71212911605835 | KNN Loss: 3.6640377044677734 | BCE Loss: 1.0480915307998657\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.704596519470215 | KNN Loss: 3.68523907661438 | BCE Loss: 1.019357681274414\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.688709259033203 | KNN Loss: 3.685293436050415 | BCE Loss: 1.003415822982788\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.76181697845459 | KNN Loss: 3.7108242511749268 | BCE Loss: 1.0509929656982422\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.75648832321167 | KNN Loss: 3.7153477668762207 | BCE Loss: 1.0411404371261597\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.750366687774658 | KNN Loss: 3.7335286140441895 | BCE Loss: 1.0168379545211792\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.707096576690674 | KNN Loss: 3.6813974380493164 | BCE Loss: 1.0256990194320679\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.711188316345215 | KNN Loss: 3.6798839569091797 | BCE Loss: 1.0313045978546143\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.697805881500244 | KNN Loss: 3.6888427734375 | BCE Loss: 1.0089631080627441\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.742069721221924 | KNN Loss: 3.6880428791046143 | BCE Loss: 1.0540269613265991\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.749856948852539 | KNN Loss: 3.7069404125213623 | BCE Loss: 1.0429162979125977\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.6996002197265625 | KNN Loss: 3.684264898300171 | BCE Loss: 1.015335202217102\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.689746856689453 | KNN Loss: 3.6638362407684326 | BCE Loss: 1.0259103775024414\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.718013286590576 | KNN Loss: 3.6799654960632324 | BCE Loss: 1.0380477905273438\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.780603885650635 | KNN Loss: 3.72090220451355 | BCE Loss: 1.0597015619277954\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.743561744689941 | KNN Loss: 3.707993268966675 | BCE Loss: 1.0355682373046875\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.755828857421875 | KNN Loss: 3.7280750274658203 | BCE Loss: 1.0277535915374756\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.684378623962402 | KNN Loss: 3.672215223312378 | BCE Loss: 1.0121636390686035\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.690174102783203 | KNN Loss: 3.675497055053711 | BCE Loss: 1.0146769285202026\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.7132344245910645 | KNN Loss: 3.6851706504821777 | BCE Loss: 1.0280638933181763\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.6620259284973145 | KNN Loss: 3.657108783721924 | BCE Loss: 1.0049172639846802\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.706874370574951 | KNN Loss: 3.691941738128662 | BCE Loss: 1.0149327516555786\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.695028305053711 | KNN Loss: 3.666134834289551 | BCE Loss: 1.0288934707641602\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.684211254119873 | KNN Loss: 3.6563618183135986 | BCE Loss: 1.0278493165969849\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.684162139892578 | KNN Loss: 3.6661832332611084 | BCE Loss: 1.0179791450500488\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.745981216430664 | KNN Loss: 3.6970858573913574 | BCE Loss: 1.048895239830017\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.732956886291504 | KNN Loss: 3.6969594955444336 | BCE Loss: 1.0359971523284912\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.730440139770508 | KNN Loss: 3.7033004760742188 | BCE Loss: 1.0271399021148682\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.6857709884643555 | KNN Loss: 3.662562370300293 | BCE Loss: 1.023208498954773\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.717565536499023 | KNN Loss: 3.70786452293396 | BCE Loss: 1.0097010135650635\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.686737060546875 | KNN Loss: 3.6840665340423584 | BCE Loss: 1.0026707649230957\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.731880187988281 | KNN Loss: 3.6861464977264404 | BCE Loss: 1.0457334518432617\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.724032402038574 | KNN Loss: 3.7125933170318604 | BCE Loss: 1.0114390850067139\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.725650310516357 | KNN Loss: 3.676004409790039 | BCE Loss: 1.0496457815170288\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.782577991485596 | KNN Loss: 3.7405571937561035 | BCE Loss: 1.0420207977294922\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.7013092041015625 | KNN Loss: 3.6984076499938965 | BCE Loss: 1.002901315689087\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.796247482299805 | KNN Loss: 3.7591700553894043 | BCE Loss: 1.0370776653289795\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.7233781814575195 | KNN Loss: 3.69846510887146 | BCE Loss: 1.0249133110046387\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.736526012420654 | KNN Loss: 3.7037265300750732 | BCE Loss: 1.0327996015548706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.739022254943848 | KNN Loss: 3.689577341079712 | BCE Loss: 1.0494447946548462\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.695044994354248 | KNN Loss: 3.6684489250183105 | BCE Loss: 1.026595950126648\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.687225818634033 | KNN Loss: 3.671999931335449 | BCE Loss: 1.015225887298584\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.764193058013916 | KNN Loss: 3.709041118621826 | BCE Loss: 1.0551519393920898\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.722105979919434 | KNN Loss: 3.7030696868896484 | BCE Loss: 1.0190365314483643\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.715198516845703 | KNN Loss: 3.6727888584136963 | BCE Loss: 1.0424094200134277\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.762514114379883 | KNN Loss: 3.715512990951538 | BCE Loss: 1.0470011234283447\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.763265609741211 | KNN Loss: 3.7025747299194336 | BCE Loss: 1.0606906414031982\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.678498268127441 | KNN Loss: 3.6679494380950928 | BCE Loss: 1.0105488300323486\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.733404159545898 | KNN Loss: 3.730144500732422 | BCE Loss: 1.0032594203948975\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.704782485961914 | KNN Loss: 3.6973845958709717 | BCE Loss: 1.0073977708816528\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.663766384124756 | KNN Loss: 3.675999164581299 | BCE Loss: 0.987767219543457\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.742593765258789 | KNN Loss: 3.701875686645508 | BCE Loss: 1.0407181978225708\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.689122676849365 | KNN Loss: 3.678554058074951 | BCE Loss: 1.0105687379837036\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.6970109939575195 | KNN Loss: 3.6750733852386475 | BCE Loss: 1.021937608718872\n",
      "Epoch   408: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.744602203369141 | KNN Loss: 3.7061338424682617 | BCE Loss: 1.0384681224822998\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.718810081481934 | KNN Loss: 3.6946635246276855 | BCE Loss: 1.024146318435669\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.647276401519775 | KNN Loss: 3.652958393096924 | BCE Loss: 0.994317889213562\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.674973487854004 | KNN Loss: 3.6703145503997803 | BCE Loss: 1.0046590566635132\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.702737331390381 | KNN Loss: 3.6866111755371094 | BCE Loss: 1.016126275062561\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.759942531585693 | KNN Loss: 3.718716859817505 | BCE Loss: 1.041225552558899\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.704951286315918 | KNN Loss: 3.6634433269500732 | BCE Loss: 1.0415079593658447\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.663403511047363 | KNN Loss: 3.6531219482421875 | BCE Loss: 1.0102816820144653\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.684262275695801 | KNN Loss: 3.666429281234741 | BCE Loss: 1.01783287525177\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.690435886383057 | KNN Loss: 3.685655117034912 | BCE Loss: 1.0047807693481445\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.677619934082031 | KNN Loss: 3.6568453311920166 | BCE Loss: 1.0207746028900146\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.698983192443848 | KNN Loss: 3.697850465774536 | BCE Loss: 1.0011329650878906\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.708710193634033 | KNN Loss: 3.6841983795166016 | BCE Loss: 1.0245119333267212\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.780106544494629 | KNN Loss: 3.7613251209259033 | BCE Loss: 1.018781304359436\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.728404998779297 | KNN Loss: 3.6853482723236084 | BCE Loss: 1.0430567264556885\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.729797840118408 | KNN Loss: 3.6856706142425537 | BCE Loss: 1.0441272258758545\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.704190254211426 | KNN Loss: 3.7045655250549316 | BCE Loss: 0.9996245503425598\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.688997268676758 | KNN Loss: 3.663487672805786 | BCE Loss: 1.0255095958709717\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.731257915496826 | KNN Loss: 3.6885125637054443 | BCE Loss: 1.0427453517913818\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.70743465423584 | KNN Loss: 3.6841325759887695 | BCE Loss: 1.0233021974563599\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.699800491333008 | KNN Loss: 3.691619873046875 | BCE Loss: 1.008180856704712\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.747364521026611 | KNN Loss: 3.703936815261841 | BCE Loss: 1.043427586555481\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.670176982879639 | KNN Loss: 3.6519551277160645 | BCE Loss: 1.0182218551635742\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.718848705291748 | KNN Loss: 3.7036783695220947 | BCE Loss: 1.0151702165603638\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.737123489379883 | KNN Loss: 3.698512554168701 | BCE Loss: 1.0386111736297607\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.701000690460205 | KNN Loss: 3.6700453758239746 | BCE Loss: 1.0309553146362305\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.714980125427246 | KNN Loss: 3.6707048416137695 | BCE Loss: 1.0442755222320557\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.705193519592285 | KNN Loss: 3.6976234912872314 | BCE Loss: 1.0075697898864746\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.688470840454102 | KNN Loss: 3.683964252471924 | BCE Loss: 1.0045064687728882\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.726643085479736 | KNN Loss: 3.710127592086792 | BCE Loss: 1.0165154933929443\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.675244331359863 | KNN Loss: 3.66788387298584 | BCE Loss: 1.0073604583740234\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.755600929260254 | KNN Loss: 3.7131009101867676 | BCE Loss: 1.0425000190734863\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.674887180328369 | KNN Loss: 3.6536545753479004 | BCE Loss: 1.0212327241897583\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.674126625061035 | KNN Loss: 3.67101788520813 | BCE Loss: 1.0031089782714844\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.662283897399902 | KNN Loss: 3.660043478012085 | BCE Loss: 1.0022401809692383\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.73013973236084 | KNN Loss: 3.7222251892089844 | BCE Loss: 1.0079143047332764\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.708605766296387 | KNN Loss: 3.69498872756958 | BCE Loss: 1.0136168003082275\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.681854248046875 | KNN Loss: 3.651583194732666 | BCE Loss: 1.030271053314209\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.747764587402344 | KNN Loss: 3.7105062007904053 | BCE Loss: 1.0372586250305176\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.695889472961426 | KNN Loss: 3.657320499420166 | BCE Loss: 1.0385692119598389\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.697133541107178 | KNN Loss: 3.6705799102783203 | BCE Loss: 1.0265536308288574\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.694438934326172 | KNN Loss: 3.6736278533935547 | BCE Loss: 1.0208110809326172\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.730327129364014 | KNN Loss: 3.6919798851013184 | BCE Loss: 1.0383472442626953\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.715801239013672 | KNN Loss: 3.686414957046509 | BCE Loss: 1.0293865203857422\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.6672844886779785 | KNN Loss: 3.6628355979919434 | BCE Loss: 1.0044488906860352\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.6891632080078125 | KNN Loss: 3.683964967727661 | BCE Loss: 1.0051982402801514\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.703883647918701 | KNN Loss: 3.683072566986084 | BCE Loss: 1.0208109617233276\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.764749526977539 | KNN Loss: 3.7033045291900635 | BCE Loss: 1.0614452362060547\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.710442066192627 | KNN Loss: 3.653696298599243 | BCE Loss: 1.0567457675933838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.718160152435303 | KNN Loss: 3.6901931762695312 | BCE Loss: 1.0279669761657715\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.725253105163574 | KNN Loss: 3.699852705001831 | BCE Loss: 1.025400161743164\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.72184944152832 | KNN Loss: 3.6811981201171875 | BCE Loss: 1.0406512022018433\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.7003397941589355 | KNN Loss: 3.670062303543091 | BCE Loss: 1.0302776098251343\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.666968822479248 | KNN Loss: 3.6545517444610596 | BCE Loss: 1.0124170780181885\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.67454195022583 | KNN Loss: 3.664839744567871 | BCE Loss: 1.009702205657959\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.732693672180176 | KNN Loss: 3.7023942470550537 | BCE Loss: 1.0302995443344116\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.762372970581055 | KNN Loss: 3.717921018600464 | BCE Loss: 1.0444519519805908\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.709102153778076 | KNN Loss: 3.6882100105285645 | BCE Loss: 1.0208920240402222\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.714987277984619 | KNN Loss: 3.680162191390991 | BCE Loss: 1.034825086593628\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.682168006896973 | KNN Loss: 3.6786513328552246 | BCE Loss: 1.0035169124603271\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.690760612487793 | KNN Loss: 3.668790817260742 | BCE Loss: 1.0219700336456299\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.668414115905762 | KNN Loss: 3.6699657440185547 | BCE Loss: 0.998448371887207\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.690330505371094 | KNN Loss: 3.6723721027374268 | BCE Loss: 1.017958164215088\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.6897807121276855 | KNN Loss: 3.701479434967041 | BCE Loss: 0.9883014559745789\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.676234722137451 | KNN Loss: 3.6736700534820557 | BCE Loss: 1.0025646686553955\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.71967887878418 | KNN Loss: 3.6592249870300293 | BCE Loss: 1.06045401096344\n",
      "Epoch   419: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.713343143463135 | KNN Loss: 3.6701369285583496 | BCE Loss: 1.0432060956954956\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.731592178344727 | KNN Loss: 3.703583240509033 | BCE Loss: 1.0280089378356934\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.728850364685059 | KNN Loss: 3.6972806453704834 | BCE Loss: 1.0315697193145752\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.708436489105225 | KNN Loss: 3.672675848007202 | BCE Loss: 1.0357606410980225\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.694009780883789 | KNN Loss: 3.7002642154693604 | BCE Loss: 0.9937455654144287\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.6957688331604 | KNN Loss: 3.6756229400634766 | BCE Loss: 1.0201458930969238\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.744112491607666 | KNN Loss: 3.7155256271362305 | BCE Loss: 1.0285868644714355\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.679256439208984 | KNN Loss: 3.682189464569092 | BCE Loss: 0.9970672130584717\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.754458427429199 | KNN Loss: 3.7402102947235107 | BCE Loss: 1.0142483711242676\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.731147766113281 | KNN Loss: 3.695862293243408 | BCE Loss: 1.0352855920791626\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.706622123718262 | KNN Loss: 3.6673614978790283 | BCE Loss: 1.039260745048523\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.7148261070251465 | KNN Loss: 3.68683123588562 | BCE Loss: 1.027994990348816\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.697723865509033 | KNN Loss: 3.6797797679901123 | BCE Loss: 1.017944097518921\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.678581237792969 | KNN Loss: 3.6628684997558594 | BCE Loss: 1.015712857246399\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.7158002853393555 | KNN Loss: 3.6807985305786133 | BCE Loss: 1.035001516342163\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.693083763122559 | KNN Loss: 3.68683123588562 | BCE Loss: 1.006252646446228\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.684013843536377 | KNN Loss: 3.668138265609741 | BCE Loss: 1.0158756971359253\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.708477973937988 | KNN Loss: 3.669769763946533 | BCE Loss: 1.0387084484100342\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.694466590881348 | KNN Loss: 3.6615610122680664 | BCE Loss: 1.0329056978225708\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.672173500061035 | KNN Loss: 3.66972017288208 | BCE Loss: 1.0024532079696655\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.755378246307373 | KNN Loss: 3.7210280895233154 | BCE Loss: 1.0343501567840576\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.741029739379883 | KNN Loss: 3.71688175201416 | BCE Loss: 1.0241477489471436\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.717301845550537 | KNN Loss: 3.7184112071990967 | BCE Loss: 0.9988908171653748\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.717801570892334 | KNN Loss: 3.6991372108459473 | BCE Loss: 1.0186643600463867\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.7316436767578125 | KNN Loss: 3.710339307785034 | BCE Loss: 1.0213041305541992\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.679244518280029 | KNN Loss: 3.67014479637146 | BCE Loss: 1.0090997219085693\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.757290840148926 | KNN Loss: 3.7188639640808105 | BCE Loss: 1.0384266376495361\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.709083080291748 | KNN Loss: 3.6934549808502197 | BCE Loss: 1.0156279802322388\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.723879337310791 | KNN Loss: 3.707716703414917 | BCE Loss: 1.016162633895874\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.765170097351074 | KNN Loss: 3.69231915473938 | BCE Loss: 1.0728507041931152\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.722100257873535 | KNN Loss: 3.6936495304107666 | BCE Loss: 1.028450608253479\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.694953918457031 | KNN Loss: 3.6927220821380615 | BCE Loss: 1.0022318363189697\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.70863151550293 | KNN Loss: 3.6543684005737305 | BCE Loss: 1.0542628765106201\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.7223920822143555 | KNN Loss: 3.6944689750671387 | BCE Loss: 1.0279229879379272\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.738306045532227 | KNN Loss: 3.689558267593384 | BCE Loss: 1.0487477779388428\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.719799041748047 | KNN Loss: 3.698399066925049 | BCE Loss: 1.021399736404419\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.702727317810059 | KNN Loss: 3.6703758239746094 | BCE Loss: 1.0323517322540283\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.696188926696777 | KNN Loss: 3.6960220336914062 | BCE Loss: 1.0001671314239502\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.727869510650635 | KNN Loss: 3.7035114765167236 | BCE Loss: 1.0243579149246216\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.691910743713379 | KNN Loss: 3.663153648376465 | BCE Loss: 1.0287569761276245\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.6668701171875 | KNN Loss: 3.660767078399658 | BCE Loss: 1.006103277206421\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.71408224105835 | KNN Loss: 3.6915597915649414 | BCE Loss: 1.0225225687026978\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.71033239364624 | KNN Loss: 3.689296007156372 | BCE Loss: 1.0210363864898682\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.7529683113098145 | KNN Loss: 3.6911404132843018 | BCE Loss: 1.0618280172348022\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.741179466247559 | KNN Loss: 3.7072999477386475 | BCE Loss: 1.0338797569274902\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.640386581420898 | KNN Loss: 3.6533749103546143 | BCE Loss: 0.9870117902755737\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.720067024230957 | KNN Loss: 3.70643949508667 | BCE Loss: 1.0136277675628662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.708044528961182 | KNN Loss: 3.6804416179656982 | BCE Loss: 1.0276027917861938\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.7303924560546875 | KNN Loss: 3.7076668739318848 | BCE Loss: 1.0227257013320923\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.747120380401611 | KNN Loss: 3.725834608078003 | BCE Loss: 1.0212856531143188\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.731727123260498 | KNN Loss: 3.687772512435913 | BCE Loss: 1.0439544916152954\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.66817569732666 | KNN Loss: 3.695002794265747 | BCE Loss: 0.9731730818748474\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.719873905181885 | KNN Loss: 3.692668914794922 | BCE Loss: 1.027204990386963\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.693912982940674 | KNN Loss: 3.65382981300354 | BCE Loss: 1.0400830507278442\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.770133018493652 | KNN Loss: 3.726199150085449 | BCE Loss: 1.0439337491989136\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.7568254470825195 | KNN Loss: 3.705576181411743 | BCE Loss: 1.0512490272521973\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.668703079223633 | KNN Loss: 3.659372329711914 | BCE Loss: 1.0093307495117188\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.720489501953125 | KNN Loss: 3.682431221008301 | BCE Loss: 1.0380580425262451\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.705801963806152 | KNN Loss: 3.6828250885009766 | BCE Loss: 1.0229767560958862\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.67444372177124 | KNN Loss: 3.6564791202545166 | BCE Loss: 1.0179647207260132\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.705386638641357 | KNN Loss: 3.667114496231079 | BCE Loss: 1.0382721424102783\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.703646659851074 | KNN Loss: 3.6883437633514404 | BCE Loss: 1.015303134918213\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.672651290893555 | KNN Loss: 3.667238712310791 | BCE Loss: 1.0054126977920532\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.73480749130249 | KNN Loss: 3.6557300090789795 | BCE Loss: 1.0790774822235107\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.722571849822998 | KNN Loss: 3.710304021835327 | BCE Loss: 1.0122677087783813\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.747807502746582 | KNN Loss: 3.7164902687072754 | BCE Loss: 1.0313173532485962\n",
      "Epoch   430: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.69854736328125 | KNN Loss: 3.6741533279418945 | BCE Loss: 1.0243940353393555\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.704916477203369 | KNN Loss: 3.6593801975250244 | BCE Loss: 1.0455363988876343\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.714682102203369 | KNN Loss: 3.6785097122192383 | BCE Loss: 1.0361725091934204\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.660139083862305 | KNN Loss: 3.651702404022217 | BCE Loss: 1.008436918258667\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.757569313049316 | KNN Loss: 3.725773572921753 | BCE Loss: 1.0317955017089844\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.706573486328125 | KNN Loss: 3.683431625366211 | BCE Loss: 1.0231417417526245\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.722538948059082 | KNN Loss: 3.691997766494751 | BCE Loss: 1.0305413007736206\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.705716609954834 | KNN Loss: 3.669935941696167 | BCE Loss: 1.0357805490493774\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.73116397857666 | KNN Loss: 3.6966800689697266 | BCE Loss: 1.0344840288162231\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.705714702606201 | KNN Loss: 3.6813557147979736 | BCE Loss: 1.0243589878082275\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.756684303283691 | KNN Loss: 3.714841842651367 | BCE Loss: 1.0418426990509033\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.703124046325684 | KNN Loss: 3.680206537246704 | BCE Loss: 1.0229172706604004\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.707589626312256 | KNN Loss: 3.686972141265869 | BCE Loss: 1.0206173658370972\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.789422035217285 | KNN Loss: 3.744392156600952 | BCE Loss: 1.0450299978256226\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.750178813934326 | KNN Loss: 3.70151686668396 | BCE Loss: 1.0486618280410767\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.732583045959473 | KNN Loss: 3.71779465675354 | BCE Loss: 1.014788269996643\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.724740982055664 | KNN Loss: 3.7024083137512207 | BCE Loss: 1.022332787513733\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.704405784606934 | KNN Loss: 3.682905435562134 | BCE Loss: 1.021500587463379\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.702606201171875 | KNN Loss: 3.6865339279174805 | BCE Loss: 1.016072154045105\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.666424751281738 | KNN Loss: 3.6644415855407715 | BCE Loss: 1.0019832849502563\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.790126800537109 | KNN Loss: 3.7578656673431396 | BCE Loss: 1.0322613716125488\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.707665920257568 | KNN Loss: 3.667879581451416 | BCE Loss: 1.039786458015442\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.705661773681641 | KNN Loss: 3.708132266998291 | BCE Loss: 0.9975296258926392\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.706185340881348 | KNN Loss: 3.6718313694000244 | BCE Loss: 1.0343539714813232\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.736678600311279 | KNN Loss: 3.7212469577789307 | BCE Loss: 1.0154317617416382\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.699219703674316 | KNN Loss: 3.689000129699707 | BCE Loss: 1.010219693183899\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.712323188781738 | KNN Loss: 3.6940386295318604 | BCE Loss: 1.0182844400405884\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.7235307693481445 | KNN Loss: 3.7164995670318604 | BCE Loss: 1.0070314407348633\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.675366401672363 | KNN Loss: 3.670809030532837 | BCE Loss: 1.0045573711395264\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.684727191925049 | KNN Loss: 3.670301675796509 | BCE Loss: 1.0144253969192505\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.695473670959473 | KNN Loss: 3.673384189605713 | BCE Loss: 1.0220894813537598\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.746913909912109 | KNN Loss: 3.69437837600708 | BCE Loss: 1.0525356531143188\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.70117712020874 | KNN Loss: 3.6796183586120605 | BCE Loss: 1.0215587615966797\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.6866936683654785 | KNN Loss: 3.669182777404785 | BCE Loss: 1.017511010169983\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.727272987365723 | KNN Loss: 3.6843504905700684 | BCE Loss: 1.0429224967956543\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.7265305519104 | KNN Loss: 3.684027910232544 | BCE Loss: 1.042502760887146\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.694756984710693 | KNN Loss: 3.6685831546783447 | BCE Loss: 1.0261739492416382\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.740375518798828 | KNN Loss: 3.69233775138855 | BCE Loss: 1.0480380058288574\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.7056779861450195 | KNN Loss: 3.6809680461883545 | BCE Loss: 1.0247098207473755\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.738239765167236 | KNN Loss: 3.7081639766693115 | BCE Loss: 1.0300759077072144\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.717080116271973 | KNN Loss: 3.706054210662842 | BCE Loss: 1.0110257863998413\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.6990580558776855 | KNN Loss: 3.687530994415283 | BCE Loss: 1.0115270614624023\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.742262363433838 | KNN Loss: 3.7161309719085693 | BCE Loss: 1.026131510734558\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.736245155334473 | KNN Loss: 3.732858419418335 | BCE Loss: 1.0033868551254272\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.711507797241211 | KNN Loss: 3.6939525604248047 | BCE Loss: 1.0175554752349854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.734422206878662 | KNN Loss: 3.696061134338379 | BCE Loss: 1.0383610725402832\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.743606090545654 | KNN Loss: 3.7316668033599854 | BCE Loss: 1.0119394063949585\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.706932067871094 | KNN Loss: 3.681630849838257 | BCE Loss: 1.0253009796142578\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.6917924880981445 | KNN Loss: 3.6965365409851074 | BCE Loss: 0.9952561855316162\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.6929192543029785 | KNN Loss: 3.7015035152435303 | BCE Loss: 0.9914158582687378\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.736515522003174 | KNN Loss: 3.6732630729675293 | BCE Loss: 1.063252568244934\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.750938415527344 | KNN Loss: 3.715184450149536 | BCE Loss: 1.0357542037963867\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.672362804412842 | KNN Loss: 3.6810216903686523 | BCE Loss: 0.9913411140441895\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.668759346008301 | KNN Loss: 3.674021005630493 | BCE Loss: 0.9947382211685181\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.714944839477539 | KNN Loss: 3.680807590484619 | BCE Loss: 1.0341371297836304\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.729569435119629 | KNN Loss: 3.6932830810546875 | BCE Loss: 1.0362861156463623\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.6862359046936035 | KNN Loss: 3.6827454566955566 | BCE Loss: 1.0034903287887573\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.679579734802246 | KNN Loss: 3.658189296722412 | BCE Loss: 1.0213905572891235\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.674941539764404 | KNN Loss: 3.6736862659454346 | BCE Loss: 1.0012553930282593\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.728903293609619 | KNN Loss: 3.678006649017334 | BCE Loss: 1.0508967638015747\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.739540100097656 | KNN Loss: 3.6796560287475586 | BCE Loss: 1.0598841905593872\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.667565822601318 | KNN Loss: 3.6708264350891113 | BCE Loss: 0.9967395067214966\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.668178081512451 | KNN Loss: 3.6565537452697754 | BCE Loss: 1.0116243362426758\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.737961769104004 | KNN Loss: 3.7151784896850586 | BCE Loss: 1.0227830410003662\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.699233055114746 | KNN Loss: 3.666496992111206 | BCE Loss: 1.0327363014221191\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.73759651184082 | KNN Loss: 3.729004383087158 | BCE Loss: 1.0085923671722412\n",
      "Epoch   441: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.753688335418701 | KNN Loss: 3.7135910987854004 | BCE Loss: 1.0400972366333008\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.723535060882568 | KNN Loss: 3.697460412979126 | BCE Loss: 1.0260746479034424\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.710291862487793 | KNN Loss: 3.6841320991516113 | BCE Loss: 1.0261595249176025\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.706104278564453 | KNN Loss: 3.6785643100738525 | BCE Loss: 1.0275402069091797\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.712541103363037 | KNN Loss: 3.6904988288879395 | BCE Loss: 1.0220422744750977\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.7148542404174805 | KNN Loss: 3.678101062774658 | BCE Loss: 1.0367529392242432\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.695433139801025 | KNN Loss: 3.6738126277923584 | BCE Loss: 1.0216206312179565\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.693680286407471 | KNN Loss: 3.6901910305023193 | BCE Loss: 1.0034891366958618\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.72200870513916 | KNN Loss: 3.6893815994262695 | BCE Loss: 1.0326271057128906\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.691329002380371 | KNN Loss: 3.6727232933044434 | BCE Loss: 1.0186059474945068\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.688614845275879 | KNN Loss: 3.6565935611724854 | BCE Loss: 1.0320210456848145\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.704059600830078 | KNN Loss: 3.695241689682007 | BCE Loss: 1.0088181495666504\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.709835529327393 | KNN Loss: 3.6945719718933105 | BCE Loss: 1.015263557434082\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.6817450523376465 | KNN Loss: 3.6774840354919434 | BCE Loss: 1.0042608976364136\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.713108539581299 | KNN Loss: 3.679388999938965 | BCE Loss: 1.0337194204330444\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.671677589416504 | KNN Loss: 3.676412582397461 | BCE Loss: 0.9952651858329773\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.704307556152344 | KNN Loss: 3.675006151199341 | BCE Loss: 1.0293011665344238\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.680945873260498 | KNN Loss: 3.6678731441497803 | BCE Loss: 1.0130727291107178\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.72127103805542 | KNN Loss: 3.705524206161499 | BCE Loss: 1.0157469511032104\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.741374969482422 | KNN Loss: 3.676950693130493 | BCE Loss: 1.0644242763519287\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.732300758361816 | KNN Loss: 3.6983306407928467 | BCE Loss: 1.0339698791503906\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.675515174865723 | KNN Loss: 3.678666114807129 | BCE Loss: 0.9968488216400146\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.70350456237793 | KNN Loss: 3.6829652786254883 | BCE Loss: 1.020539402961731\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.738813400268555 | KNN Loss: 3.6928842067718506 | BCE Loss: 1.045929193496704\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.697709083557129 | KNN Loss: 3.709052801132202 | BCE Loss: 0.9886560440063477\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.6947855949401855 | KNN Loss: 3.6695351600646973 | BCE Loss: 1.0252505540847778\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.712536334991455 | KNN Loss: 3.7019598484039307 | BCE Loss: 1.0105764865875244\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.7362260818481445 | KNN Loss: 3.678621292114258 | BCE Loss: 1.0576047897338867\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.700989246368408 | KNN Loss: 3.655055522918701 | BCE Loss: 1.0459338426589966\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.711915016174316 | KNN Loss: 3.6834287643432617 | BCE Loss: 1.0284864902496338\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.771247863769531 | KNN Loss: 3.7126593589782715 | BCE Loss: 1.0585883855819702\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.723875045776367 | KNN Loss: 3.7199883460998535 | BCE Loss: 1.0038868188858032\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.753920555114746 | KNN Loss: 3.7134268283843994 | BCE Loss: 1.0404934883117676\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.691420555114746 | KNN Loss: 3.6783447265625 | BCE Loss: 1.013075828552246\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.743223667144775 | KNN Loss: 3.6785032749176025 | BCE Loss: 1.0647203922271729\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.732335090637207 | KNN Loss: 3.707214832305908 | BCE Loss: 1.0251200199127197\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.670031547546387 | KNN Loss: 3.6746418476104736 | BCE Loss: 0.9953898191452026\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.718084335327148 | KNN Loss: 3.6953065395355225 | BCE Loss: 1.0227776765823364\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.734082221984863 | KNN Loss: 3.7089033126831055 | BCE Loss: 1.0251787900924683\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.726478576660156 | KNN Loss: 3.7048118114471436 | BCE Loss: 1.0216670036315918\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.732612609863281 | KNN Loss: 3.6954283714294434 | BCE Loss: 1.037184238433838\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.742142200469971 | KNN Loss: 3.705765724182129 | BCE Loss: 1.0363763570785522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.732075214385986 | KNN Loss: 3.7153801918029785 | BCE Loss: 1.0166950225830078\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.716768264770508 | KNN Loss: 3.6851818561553955 | BCE Loss: 1.0315861701965332\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.7052998542785645 | KNN Loss: 3.690096616744995 | BCE Loss: 1.0152032375335693\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.742417335510254 | KNN Loss: 3.6937782764434814 | BCE Loss: 1.0486392974853516\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.696242809295654 | KNN Loss: 3.697507619857788 | BCE Loss: 0.9987351298332214\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.72514009475708 | KNN Loss: 3.6809780597686768 | BCE Loss: 1.0441621541976929\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.748376846313477 | KNN Loss: 3.7178196907043457 | BCE Loss: 1.0305571556091309\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.729743480682373 | KNN Loss: 3.7027416229248047 | BCE Loss: 1.027001976966858\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.684095859527588 | KNN Loss: 3.6587467193603516 | BCE Loss: 1.0253492593765259\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.675909519195557 | KNN Loss: 3.660935401916504 | BCE Loss: 1.0149739980697632\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.732907295227051 | KNN Loss: 3.67763614654541 | BCE Loss: 1.0552712678909302\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.69381046295166 | KNN Loss: 3.670055866241455 | BCE Loss: 1.0237548351287842\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.740007400512695 | KNN Loss: 3.685115337371826 | BCE Loss: 1.0548923015594482\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.702896595001221 | KNN Loss: 3.6832451820373535 | BCE Loss: 1.0196515321731567\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.733691215515137 | KNN Loss: 3.6720776557922363 | BCE Loss: 1.0616133213043213\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.685001373291016 | KNN Loss: 3.6675643920898438 | BCE Loss: 1.0174368619918823\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.731179237365723 | KNN Loss: 3.6854453086853027 | BCE Loss: 1.0457340478897095\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.696138381958008 | KNN Loss: 3.665764570236206 | BCE Loss: 1.0303736925125122\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.659412860870361 | KNN Loss: 3.6451079845428467 | BCE Loss: 1.0143049955368042\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.7155632972717285 | KNN Loss: 3.6720921993255615 | BCE Loss: 1.0434712171554565\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.698333740234375 | KNN Loss: 3.6841347217559814 | BCE Loss: 1.014199137687683\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.7383317947387695 | KNN Loss: 3.686253786087036 | BCE Loss: 1.052078127861023\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.7569708824157715 | KNN Loss: 3.704453468322754 | BCE Loss: 1.0525174140930176\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.771152496337891 | KNN Loss: 3.73039174079895 | BCE Loss: 1.0407607555389404\n",
      "Epoch   452: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.679012775421143 | KNN Loss: 3.6692054271698 | BCE Loss: 1.0098073482513428\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.712925910949707 | KNN Loss: 3.691189765930176 | BCE Loss: 1.0217361450195312\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.754522800445557 | KNN Loss: 3.7137320041656494 | BCE Loss: 1.0407907962799072\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.716616153717041 | KNN Loss: 3.6785755157470703 | BCE Loss: 1.0380406379699707\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.688080787658691 | KNN Loss: 3.669100761413574 | BCE Loss: 1.0189800262451172\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.713045120239258 | KNN Loss: 3.6733038425445557 | BCE Loss: 1.0397413969039917\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.756349563598633 | KNN Loss: 3.726398468017578 | BCE Loss: 1.0299508571624756\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.73921537399292 | KNN Loss: 3.6979787349700928 | BCE Loss: 1.0412367582321167\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.68449592590332 | KNN Loss: 3.6898014545440674 | BCE Loss: 0.9946945905685425\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.6836090087890625 | KNN Loss: 3.6623947620391846 | BCE Loss: 1.0212140083312988\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.718508720397949 | KNN Loss: 3.672498941421509 | BCE Loss: 1.0460100173950195\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.707546234130859 | KNN Loss: 3.6756858825683594 | BCE Loss: 1.031860589981079\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.6986894607543945 | KNN Loss: 3.6869683265686035 | BCE Loss: 1.011721134185791\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.729712009429932 | KNN Loss: 3.6846044063568115 | BCE Loss: 1.0451074838638306\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.698184967041016 | KNN Loss: 3.662740468978882 | BCE Loss: 1.0354446172714233\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.733460426330566 | KNN Loss: 3.6872851848602295 | BCE Loss: 1.046175241470337\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.710947036743164 | KNN Loss: 3.713541030883789 | BCE Loss: 0.9974058270454407\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.686807632446289 | KNN Loss: 3.6711068153381348 | BCE Loss: 1.0157009363174438\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.7192792892456055 | KNN Loss: 3.687856435775757 | BCE Loss: 1.0314228534698486\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.712364673614502 | KNN Loss: 3.695197105407715 | BCE Loss: 1.0171676874160767\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.7241621017456055 | KNN Loss: 3.69103741645813 | BCE Loss: 1.0331249237060547\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.713146209716797 | KNN Loss: 3.6983447074890137 | BCE Loss: 1.0148015022277832\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.700498104095459 | KNN Loss: 3.661994457244873 | BCE Loss: 1.0385035276412964\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.690052032470703 | KNN Loss: 3.684473752975464 | BCE Loss: 1.0055785179138184\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.6967363357543945 | KNN Loss: 3.6586573123931885 | BCE Loss: 1.038078784942627\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.702276229858398 | KNN Loss: 3.6905670166015625 | BCE Loss: 1.011709451675415\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.762087821960449 | KNN Loss: 3.7241287231445312 | BCE Loss: 1.0379589796066284\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.691096782684326 | KNN Loss: 3.684307098388672 | BCE Loss: 1.0067898035049438\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.7031331062316895 | KNN Loss: 3.678095817565918 | BCE Loss: 1.0250372886657715\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.742913246154785 | KNN Loss: 3.7024614810943604 | BCE Loss: 1.0404518842697144\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.648590087890625 | KNN Loss: 3.658581018447876 | BCE Loss: 0.9900088906288147\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.738418102264404 | KNN Loss: 3.7197017669677734 | BCE Loss: 1.0187163352966309\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.745855331420898 | KNN Loss: 3.7104976177215576 | BCE Loss: 1.0353577136993408\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.691123962402344 | KNN Loss: 3.678429126739502 | BCE Loss: 1.0126947164535522\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.726823806762695 | KNN Loss: 3.6916065216064453 | BCE Loss: 1.03521728515625\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.751221656799316 | KNN Loss: 3.697006940841675 | BCE Loss: 1.0542147159576416\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.715015411376953 | KNN Loss: 3.6766114234924316 | BCE Loss: 1.0384039878845215\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.722043991088867 | KNN Loss: 3.6832664012908936 | BCE Loss: 1.038777470588684\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.696680068969727 | KNN Loss: 3.6816866397857666 | BCE Loss: 1.01499342918396\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.7520294189453125 | KNN Loss: 3.7194042205810547 | BCE Loss: 1.0326249599456787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.7442216873168945 | KNN Loss: 3.72982120513916 | BCE Loss: 1.014400601387024\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.720094680786133 | KNN Loss: 3.694822311401367 | BCE Loss: 1.025272250175476\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.7125420570373535 | KNN Loss: 3.689485788345337 | BCE Loss: 1.0230562686920166\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.674315929412842 | KNN Loss: 3.6623287200927734 | BCE Loss: 1.011987328529358\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.732183456420898 | KNN Loss: 3.7102231979370117 | BCE Loss: 1.0219604969024658\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.6692352294921875 | KNN Loss: 3.6817195415496826 | BCE Loss: 0.987515926361084\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.71513557434082 | KNN Loss: 3.6937007904052734 | BCE Loss: 1.021435022354126\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.742697238922119 | KNN Loss: 3.6982126235961914 | BCE Loss: 1.0444844961166382\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.662234783172607 | KNN Loss: 3.66709566116333 | BCE Loss: 0.9951390624046326\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.708520889282227 | KNN Loss: 3.6859729290008545 | BCE Loss: 1.022547960281372\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.688690662384033 | KNN Loss: 3.6626298427581787 | BCE Loss: 1.026060700416565\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.704814434051514 | KNN Loss: 3.689150333404541 | BCE Loss: 1.0156641006469727\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.692470550537109 | KNN Loss: 3.6694324016571045 | BCE Loss: 1.0230379104614258\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.690902233123779 | KNN Loss: 3.6766862869262695 | BCE Loss: 1.0142159461975098\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.678620338439941 | KNN Loss: 3.6612436771392822 | BCE Loss: 1.01737642288208\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.6684112548828125 | KNN Loss: 3.6619675159454346 | BCE Loss: 1.006443977355957\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.716601371765137 | KNN Loss: 3.6764490604400635 | BCE Loss: 1.0401524305343628\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.734073162078857 | KNN Loss: 3.7151176929473877 | BCE Loss: 1.0189553499221802\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.682812213897705 | KNN Loss: 3.6693949699401855 | BCE Loss: 1.01341712474823\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.684627532958984 | KNN Loss: 3.6735262870788574 | BCE Loss: 1.011101484298706\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.713624954223633 | KNN Loss: 3.671382188796997 | BCE Loss: 1.0422427654266357\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.744481086730957 | KNN Loss: 3.7253501415252686 | BCE Loss: 1.019131064414978\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.724715709686279 | KNN Loss: 3.6835203170776367 | BCE Loss: 1.0411953926086426\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.699146270751953 | KNN Loss: 3.66536021232605 | BCE Loss: 1.0337860584259033\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.661679267883301 | KNN Loss: 3.6622815132141113 | BCE Loss: 0.9993978142738342\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.720462322235107 | KNN Loss: 3.6926231384277344 | BCE Loss: 1.0278393030166626\n",
      "Epoch   463: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.672049522399902 | KNN Loss: 3.663337230682373 | BCE Loss: 1.0087125301361084\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.6972126960754395 | KNN Loss: 3.6891517639160156 | BCE Loss: 1.0080609321594238\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.649238109588623 | KNN Loss: 3.6612863540649414 | BCE Loss: 0.9879518747329712\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.727352142333984 | KNN Loss: 3.701230049133301 | BCE Loss: 1.0261222124099731\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.751636505126953 | KNN Loss: 3.730635404586792 | BCE Loss: 1.0210009813308716\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.692054748535156 | KNN Loss: 3.6723690032958984 | BCE Loss: 1.0196858644485474\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.6673078536987305 | KNN Loss: 3.658533811569214 | BCE Loss: 1.0087742805480957\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.732958793640137 | KNN Loss: 3.697821617126465 | BCE Loss: 1.0351371765136719\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.750239849090576 | KNN Loss: 3.6853814125061035 | BCE Loss: 1.0648584365844727\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.72157621383667 | KNN Loss: 3.6973330974578857 | BCE Loss: 1.0242431163787842\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.692960262298584 | KNN Loss: 3.6829538345336914 | BCE Loss: 1.0100064277648926\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.712568283081055 | KNN Loss: 3.68875789642334 | BCE Loss: 1.0238101482391357\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.6547980308532715 | KNN Loss: 3.6667356491088867 | BCE Loss: 0.9880624413490295\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.74636173248291 | KNN Loss: 3.684133768081665 | BCE Loss: 1.0622279644012451\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.7120866775512695 | KNN Loss: 3.693756341934204 | BCE Loss: 1.0183305740356445\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.7497172355651855 | KNN Loss: 3.697296619415283 | BCE Loss: 1.0524204969406128\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.670847415924072 | KNN Loss: 3.659620523452759 | BCE Loss: 1.0112268924713135\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.729706764221191 | KNN Loss: 3.711714506149292 | BCE Loss: 1.0179924964904785\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.7281293869018555 | KNN Loss: 3.716749429702759 | BCE Loss: 1.0113798379898071\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.736540794372559 | KNN Loss: 3.7258083820343018 | BCE Loss: 1.010732650756836\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.695855617523193 | KNN Loss: 3.691434144973755 | BCE Loss: 1.004421353340149\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.6902666091918945 | KNN Loss: 3.662306308746338 | BCE Loss: 1.027960181236267\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.678523063659668 | KNN Loss: 3.6800551414489746 | BCE Loss: 0.9984678030014038\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.753819465637207 | KNN Loss: 3.716282606124878 | BCE Loss: 1.037536859512329\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.751738548278809 | KNN Loss: 3.7161338329315186 | BCE Loss: 1.0356045961380005\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.701493263244629 | KNN Loss: 3.664714813232422 | BCE Loss: 1.0367786884307861\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.70309591293335 | KNN Loss: 3.690385580062866 | BCE Loss: 1.0127102136611938\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.734781742095947 | KNN Loss: 3.709514617919922 | BCE Loss: 1.025267243385315\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.686270713806152 | KNN Loss: 3.6803879737854004 | BCE Loss: 1.0058826208114624\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.7334442138671875 | KNN Loss: 3.686281442642212 | BCE Loss: 1.0471630096435547\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.71561861038208 | KNN Loss: 3.6965744495391846 | BCE Loss: 1.019044280052185\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.698659896850586 | KNN Loss: 3.684133529663086 | BCE Loss: 1.014526128768921\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.706065654754639 | KNN Loss: 3.66090989112854 | BCE Loss: 1.0451558828353882\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.719708442687988 | KNN Loss: 3.690211534500122 | BCE Loss: 1.0294970273971558\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.691465377807617 | KNN Loss: 3.6608822345733643 | BCE Loss: 1.0305832624435425\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.7889909744262695 | KNN Loss: 3.746983528137207 | BCE Loss: 1.0420072078704834\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.722493648529053 | KNN Loss: 3.674389362335205 | BCE Loss: 1.048104166984558\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.7466278076171875 | KNN Loss: 3.726325511932373 | BCE Loss: 1.0203025341033936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.695771217346191 | KNN Loss: 3.6765801906585693 | BCE Loss: 1.019190788269043\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.709070205688477 | KNN Loss: 3.6821446418762207 | BCE Loss: 1.0269253253936768\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.658243656158447 | KNN Loss: 3.664552927017212 | BCE Loss: 0.9936908483505249\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.726993560791016 | KNN Loss: 3.6653761863708496 | BCE Loss: 1.0616176128387451\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.704527378082275 | KNN Loss: 3.685822010040283 | BCE Loss: 1.0187053680419922\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.696495056152344 | KNN Loss: 3.66705584526062 | BCE Loss: 1.0294393301010132\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.683419704437256 | KNN Loss: 3.684457540512085 | BCE Loss: 0.9989620447158813\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.679872035980225 | KNN Loss: 3.6961405277252197 | BCE Loss: 0.9837313294410706\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.725654602050781 | KNN Loss: 3.686800003051758 | BCE Loss: 1.0388543605804443\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.741228103637695 | KNN Loss: 3.7046613693237305 | BCE Loss: 1.0365668535232544\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.70550012588501 | KNN Loss: 3.6803934574127197 | BCE Loss: 1.02510666847229\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.697479248046875 | KNN Loss: 3.6660189628601074 | BCE Loss: 1.031460165977478\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.695713996887207 | KNN Loss: 3.6784586906433105 | BCE Loss: 1.0172555446624756\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.69119930267334 | KNN Loss: 3.7003800868988037 | BCE Loss: 0.9908193945884705\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.704434394836426 | KNN Loss: 3.673929214477539 | BCE Loss: 1.0305049419403076\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.673941135406494 | KNN Loss: 3.6831324100494385 | BCE Loss: 0.9908086657524109\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.7214460372924805 | KNN Loss: 3.699512243270874 | BCE Loss: 1.021933674812317\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.684126377105713 | KNN Loss: 3.6794650554656982 | BCE Loss: 1.004661202430725\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.666660308837891 | KNN Loss: 3.654559850692749 | BCE Loss: 1.0121002197265625\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.710987567901611 | KNN Loss: 3.671647548675537 | BCE Loss: 1.0393401384353638\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.742048263549805 | KNN Loss: 3.6927552223205566 | BCE Loss: 1.049293041229248\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.726415634155273 | KNN Loss: 3.6874377727508545 | BCE Loss: 1.0389776229858398\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.691315650939941 | KNN Loss: 3.68898868560791 | BCE Loss: 1.0023272037506104\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.746862411499023 | KNN Loss: 3.7347476482391357 | BCE Loss: 1.0121146440505981\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.673488140106201 | KNN Loss: 3.676515579223633 | BCE Loss: 0.996972382068634\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.717724800109863 | KNN Loss: 3.71122145652771 | BCE Loss: 1.0065033435821533\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.745743274688721 | KNN Loss: 3.7068076133728027 | BCE Loss: 1.0389357805252075\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.718459129333496 | KNN Loss: 3.6881582736968994 | BCE Loss: 1.0303009748458862\n",
      "Epoch   474: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.716973781585693 | KNN Loss: 3.7002453804016113 | BCE Loss: 1.0167285203933716\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.727508068084717 | KNN Loss: 3.6857285499572754 | BCE Loss: 1.0417795181274414\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.721693515777588 | KNN Loss: 3.6859967708587646 | BCE Loss: 1.0356967449188232\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.733829498291016 | KNN Loss: 3.6907570362091064 | BCE Loss: 1.0430724620819092\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.677638053894043 | KNN Loss: 3.6708595752716064 | BCE Loss: 1.006778359413147\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.673099040985107 | KNN Loss: 3.6680960655212402 | BCE Loss: 1.0050029754638672\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.78452730178833 | KNN Loss: 3.753279209136963 | BCE Loss: 1.0312480926513672\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.719686031341553 | KNN Loss: 3.689180612564087 | BCE Loss: 1.0305054187774658\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.740575790405273 | KNN Loss: 3.711097478866577 | BCE Loss: 1.0294781923294067\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.702913284301758 | KNN Loss: 3.687713146209717 | BCE Loss: 1.015200138092041\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.695333480834961 | KNN Loss: 3.695094347000122 | BCE Loss: 1.0002388954162598\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.7909979820251465 | KNN Loss: 3.745814561843872 | BCE Loss: 1.0451834201812744\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.780659198760986 | KNN Loss: 3.7272543907165527 | BCE Loss: 1.0534049272537231\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.727156639099121 | KNN Loss: 3.707423210144043 | BCE Loss: 1.0197334289550781\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.753177642822266 | KNN Loss: 3.715291976928711 | BCE Loss: 1.0378856658935547\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.753518581390381 | KNN Loss: 3.7305591106414795 | BCE Loss: 1.022959589958191\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.764964580535889 | KNN Loss: 3.7197537422180176 | BCE Loss: 1.045210838317871\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.737906455993652 | KNN Loss: 3.7095792293548584 | BCE Loss: 1.028327465057373\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.686513900756836 | KNN Loss: 3.674457311630249 | BCE Loss: 1.012056589126587\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.7270588874816895 | KNN Loss: 3.704887866973877 | BCE Loss: 1.022171139717102\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.630975723266602 | KNN Loss: 3.6486876010894775 | BCE Loss: 0.9822878837585449\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.757082939147949 | KNN Loss: 3.719703435897827 | BCE Loss: 1.037379503250122\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.669103622436523 | KNN Loss: 3.6524972915649414 | BCE Loss: 1.0166064500808716\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.702288627624512 | KNN Loss: 3.695371150970459 | BCE Loss: 1.0069173574447632\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.686264991760254 | KNN Loss: 3.676647901535034 | BCE Loss: 1.0096169710159302\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.708061218261719 | KNN Loss: 3.689382553100586 | BCE Loss: 1.0186786651611328\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.779720306396484 | KNN Loss: 3.7188079357147217 | BCE Loss: 1.0609123706817627\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.657902240753174 | KNN Loss: 3.6767208576202393 | BCE Loss: 0.9811813831329346\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.698614120483398 | KNN Loss: 3.705040693283081 | BCE Loss: 0.9935732483863831\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.7014360427856445 | KNN Loss: 3.70786190032959 | BCE Loss: 0.9935739040374756\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.727346420288086 | KNN Loss: 3.702375650405884 | BCE Loss: 1.024970531463623\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.736291885375977 | KNN Loss: 3.7103404998779297 | BCE Loss: 1.0259511470794678\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.722224235534668 | KNN Loss: 3.698087215423584 | BCE Loss: 1.024137020111084\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.715684413909912 | KNN Loss: 3.6949360370635986 | BCE Loss: 1.0207483768463135\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.734054088592529 | KNN Loss: 3.679619789123535 | BCE Loss: 1.0544344186782837\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.695070266723633 | KNN Loss: 3.679891586303711 | BCE Loss: 1.0151787996292114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.680174350738525 | KNN Loss: 3.695300579071045 | BCE Loss: 0.9848736524581909\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.6911163330078125 | KNN Loss: 3.6690118312835693 | BCE Loss: 1.0221046209335327\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.690283298492432 | KNN Loss: 3.699307680130005 | BCE Loss: 0.9909757971763611\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.677189826965332 | KNN Loss: 3.6597557067871094 | BCE Loss: 1.0174338817596436\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.69784688949585 | KNN Loss: 3.669581890106201 | BCE Loss: 1.0282648801803589\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.691289901733398 | KNN Loss: 3.673945188522339 | BCE Loss: 1.0173447132110596\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.698472023010254 | KNN Loss: 3.668053150177002 | BCE Loss: 1.030419111251831\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.7260236740112305 | KNN Loss: 3.7319328784942627 | BCE Loss: 0.9940908551216125\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.716144561767578 | KNN Loss: 3.6961679458618164 | BCE Loss: 1.0199763774871826\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.690944194793701 | KNN Loss: 3.6710150241851807 | BCE Loss: 1.0199291706085205\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.756763458251953 | KNN Loss: 3.741772174835205 | BCE Loss: 1.014991283416748\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.759967803955078 | KNN Loss: 3.69500732421875 | BCE Loss: 1.0649603605270386\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.760778427124023 | KNN Loss: 3.7008306980133057 | BCE Loss: 1.0599476099014282\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.708890914916992 | KNN Loss: 3.683135986328125 | BCE Loss: 1.025754690170288\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.650592803955078 | KNN Loss: 3.6704111099243164 | BCE Loss: 0.9801819324493408\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.725255489349365 | KNN Loss: 3.696963310241699 | BCE Loss: 1.028292179107666\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.735332489013672 | KNN Loss: 3.698362350463867 | BCE Loss: 1.0369703769683838\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.745835304260254 | KNN Loss: 3.7172772884368896 | BCE Loss: 1.0285582542419434\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.70460319519043 | KNN Loss: 3.6762337684631348 | BCE Loss: 1.0283691883087158\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.691766738891602 | KNN Loss: 3.6662237644195557 | BCE Loss: 1.025543212890625\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.7001190185546875 | KNN Loss: 3.682079315185547 | BCE Loss: 1.0180397033691406\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.708902835845947 | KNN Loss: 3.671827793121338 | BCE Loss: 1.037075161933899\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.6567182540893555 | KNN Loss: 3.6364681720733643 | BCE Loss: 1.0202500820159912\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.750502586364746 | KNN Loss: 3.7270689010620117 | BCE Loss: 1.0234336853027344\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.683035850524902 | KNN Loss: 3.667659044265747 | BCE Loss: 1.0153769254684448\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.676510810852051 | KNN Loss: 3.6767961978912354 | BCE Loss: 0.9997148513793945\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.685428619384766 | KNN Loss: 3.6804885864257812 | BCE Loss: 1.0049400329589844\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.711766242980957 | KNN Loss: 3.6724348068237305 | BCE Loss: 1.0393316745758057\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.709481239318848 | KNN Loss: 3.671926498413086 | BCE Loss: 1.0375546216964722\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.767975330352783 | KNN Loss: 3.7254979610443115 | BCE Loss: 1.0424772500991821\n",
      "Epoch   485: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.717142581939697 | KNN Loss: 3.685643434524536 | BCE Loss: 1.0314990282058716\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.762073516845703 | KNN Loss: 3.692883253097534 | BCE Loss: 1.0691903829574585\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.696901321411133 | KNN Loss: 3.6600189208984375 | BCE Loss: 1.0368821620941162\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.727387428283691 | KNN Loss: 3.7069168090820312 | BCE Loss: 1.0204706192016602\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.70379638671875 | KNN Loss: 3.6994786262512207 | BCE Loss: 1.0043177604675293\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.699060440063477 | KNN Loss: 3.7044448852539062 | BCE Loss: 0.994615375995636\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.695642471313477 | KNN Loss: 3.6722660064697266 | BCE Loss: 1.0233765840530396\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.79263162612915 | KNN Loss: 3.7675650119781494 | BCE Loss: 1.0250667333602905\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.694626808166504 | KNN Loss: 3.662665367126465 | BCE Loss: 1.031961441040039\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.716471195220947 | KNN Loss: 3.7023441791534424 | BCE Loss: 1.0141268968582153\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.7382049560546875 | KNN Loss: 3.7203006744384766 | BCE Loss: 1.0179041624069214\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.677614688873291 | KNN Loss: 3.657562732696533 | BCE Loss: 1.0200520753860474\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.735801696777344 | KNN Loss: 3.7187137603759766 | BCE Loss: 1.017087697982788\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.710537910461426 | KNN Loss: 3.6738851070404053 | BCE Loss: 1.0366528034210205\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.70754337310791 | KNN Loss: 3.6751177310943604 | BCE Loss: 1.0324255228042603\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.684288024902344 | KNN Loss: 3.6741321086883545 | BCE Loss: 1.0101561546325684\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.742671966552734 | KNN Loss: 3.707977533340454 | BCE Loss: 1.0346944332122803\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.667531967163086 | KNN Loss: 3.666232109069824 | BCE Loss: 1.0012996196746826\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.6995134353637695 | KNN Loss: 3.693183183670044 | BCE Loss: 1.0063303709030151\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.717449188232422 | KNN Loss: 3.711003065109253 | BCE Loss: 1.0064458847045898\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.744926452636719 | KNN Loss: 3.703838586807251 | BCE Loss: 1.0410881042480469\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.698296546936035 | KNN Loss: 3.680370330810547 | BCE Loss: 1.0179264545440674\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.747152328491211 | KNN Loss: 3.6985909938812256 | BCE Loss: 1.048561453819275\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.75979471206665 | KNN Loss: 3.718754768371582 | BCE Loss: 1.0410399436950684\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.726737022399902 | KNN Loss: 3.707815647125244 | BCE Loss: 1.0189216136932373\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.748105049133301 | KNN Loss: 3.7053616046905518 | BCE Loss: 1.042743444442749\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.808737277984619 | KNN Loss: 3.763944625854492 | BCE Loss: 1.044792652130127\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.677728652954102 | KNN Loss: 3.662775754928589 | BCE Loss: 1.0149528980255127\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.697206974029541 | KNN Loss: 3.666412830352783 | BCE Loss: 1.0307942628860474\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.704223155975342 | KNN Loss: 3.664862871170044 | BCE Loss: 1.0393602848052979\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.690677165985107 | KNN Loss: 3.6986849308013916 | BCE Loss: 0.9919924139976501\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.700606822967529 | KNN Loss: 3.6752405166625977 | BCE Loss: 1.0253664255142212\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.678746223449707 | KNN Loss: 3.6671760082244873 | BCE Loss: 1.0115702152252197\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.7454633712768555 | KNN Loss: 3.698307752609253 | BCE Loss: 1.0471556186676025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.658114910125732 | KNN Loss: 3.671670913696289 | BCE Loss: 0.9864441752433777\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.715171813964844 | KNN Loss: 3.6964666843414307 | BCE Loss: 1.018704891204834\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.735581398010254 | KNN Loss: 3.679997682571411 | BCE Loss: 1.0555838346481323\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.728246212005615 | KNN Loss: 3.735576629638672 | BCE Loss: 0.9926697015762329\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.7432451248168945 | KNN Loss: 3.7044808864593506 | BCE Loss: 1.038764238357544\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.702518463134766 | KNN Loss: 3.6970551013946533 | BCE Loss: 1.0054631233215332\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.74367094039917 | KNN Loss: 3.6656463146209717 | BCE Loss: 1.0780247449874878\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.733044624328613 | KNN Loss: 3.6849606037139893 | BCE Loss: 1.0480841398239136\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.708831310272217 | KNN Loss: 3.6627185344696045 | BCE Loss: 1.0461126565933228\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.736892223358154 | KNN Loss: 3.7134830951690674 | BCE Loss: 1.023409128189087\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.71035099029541 | KNN Loss: 3.6650390625 | BCE Loss: 1.0453121662139893\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.734722137451172 | KNN Loss: 3.7153737545013428 | BCE Loss: 1.0193486213684082\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.688777923583984 | KNN Loss: 3.669430732727051 | BCE Loss: 1.0193471908569336\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.685361862182617 | KNN Loss: 3.6688454151153564 | BCE Loss: 1.0165166854858398\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.730962753295898 | KNN Loss: 3.723127841949463 | BCE Loss: 1.0078346729278564\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.726277828216553 | KNN Loss: 3.6900360584259033 | BCE Loss: 1.0362417697906494\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.6666364669799805 | KNN Loss: 3.6838269233703613 | BCE Loss: 0.9828093647956848\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.757559299468994 | KNN Loss: 3.692126512527466 | BCE Loss: 1.0654326677322388\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.665599346160889 | KNN Loss: 3.673966884613037 | BCE Loss: 0.9916325807571411\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.767116546630859 | KNN Loss: 3.7460739612579346 | BCE Loss: 1.0210423469543457\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.6670427322387695 | KNN Loss: 3.6794533729553223 | BCE Loss: 0.9875892400741577\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.746245861053467 | KNN Loss: 3.697171211242676 | BCE Loss: 1.0490747690200806\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.719645023345947 | KNN Loss: 3.697840929031372 | BCE Loss: 1.0218039751052856\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.656189918518066 | KNN Loss: 3.6765213012695312 | BCE Loss: 0.9796687364578247\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.759720802307129 | KNN Loss: 3.7140393257141113 | BCE Loss: 1.0456812381744385\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.693077564239502 | KNN Loss: 3.6812708377838135 | BCE Loss: 1.0118067264556885\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.698978424072266 | KNN Loss: 3.6732654571533203 | BCE Loss: 1.0257129669189453\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.70531702041626 | KNN Loss: 3.6666064262390137 | BCE Loss: 1.0387107133865356\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.700249195098877 | KNN Loss: 3.6886038780212402 | BCE Loss: 1.0116453170776367\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.737048149108887 | KNN Loss: 3.6890485286712646 | BCE Loss: 1.047999620437622\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.697873115539551 | KNN Loss: 3.6841115951538086 | BCE Loss: 1.0137616395950317\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.736327648162842 | KNN Loss: 3.6720998287200928 | BCE Loss: 1.064227819442749\n",
      "Epoch   496: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.732113361358643 | KNN Loss: 3.6851723194122314 | BCE Loss: 1.0469411611557007\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.7313690185546875 | KNN Loss: 3.68371844291687 | BCE Loss: 1.0476508140563965\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.713492393493652 | KNN Loss: 3.6845884323120117 | BCE Loss: 1.0289041996002197\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.707123756408691 | KNN Loss: 3.6677725315093994 | BCE Loss: 1.039351224899292\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.680638790130615 | KNN Loss: 3.651495933532715 | BCE Loss: 1.02914297580719\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.708497524261475 | KNN Loss: 3.6958136558532715 | BCE Loss: 1.0126839876174927\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.681030750274658 | KNN Loss: 3.673677682876587 | BCE Loss: 1.0073529481887817\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.766182899475098 | KNN Loss: 3.6881155967712402 | BCE Loss: 1.0780673027038574\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.72548246383667 | KNN Loss: 3.6924381256103516 | BCE Loss: 1.0330442190170288\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.7473907470703125 | KNN Loss: 3.680445432662964 | BCE Loss: 1.0669453144073486\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.706645488739014 | KNN Loss: 3.698103189468384 | BCE Loss: 1.0085422992706299\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.704307556152344 | KNN Loss: 3.6787545680999756 | BCE Loss: 1.0255531072616577\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.742191791534424 | KNN Loss: 3.684037923812866 | BCE Loss: 1.0581539869308472\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.673022270202637 | KNN Loss: 3.654813766479492 | BCE Loss: 1.0182082653045654\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.721916198730469 | KNN Loss: 3.691561460494995 | BCE Loss: 1.030354619026184\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.681061744689941 | KNN Loss: 3.66464900970459 | BCE Loss: 1.0164124965667725\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.752327919006348 | KNN Loss: 3.701631546020508 | BCE Loss: 1.0506964921951294\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.736249923706055 | KNN Loss: 3.723670721054077 | BCE Loss: 1.0125792026519775\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.693230152130127 | KNN Loss: 3.689305067062378 | BCE Loss: 1.0039252042770386\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.709011077880859 | KNN Loss: 3.6757991313934326 | BCE Loss: 1.0332119464874268\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.75002908706665 | KNN Loss: 3.7234346866607666 | BCE Loss: 1.0265942811965942\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.703921318054199 | KNN Loss: 3.6738131046295166 | BCE Loss: 1.0301084518432617\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.700593948364258 | KNN Loss: 3.677093505859375 | BCE Loss: 1.0235004425048828\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.7181172370910645 | KNN Loss: 3.690917491912842 | BCE Loss: 1.0271997451782227\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=32)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        knn_loss = knn_crt(iterm)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7604,  4.4755,  3.0771,  3.9097,  4.0269,  0.5695,  1.7009,  2.7212,\n",
      "          1.9785,  1.8334,  2.6973,  2.0479,  0.5888,  2.0219,  1.4895,  0.5669,\n",
      "          1.7798,  2.0688,  3.1418,  1.2685,  2.0295,  1.9587,  1.8234,  2.0338,\n",
      "          1.7594,  1.3992,  2.6015,  0.9626,  1.8034,  0.1201, -0.0493,  0.8887,\n",
      "          0.1603,  0.8327,  1.9717,  1.0101,  0.7896,  1.8603,  0.6187,  1.4763,\n",
      "          0.8109, -0.5784, -0.3982,  2.8505,  1.4647,  0.7623, -0.0900,  0.2955,\n",
      "          1.6988,  2.8949,  2.0963,  0.3493,  1.5421,  0.9165, -0.3905,  0.9805,\n",
      "          1.5791,  1.6771,  1.2341,  1.6186,  0.4137,  0.5113, -0.0765,  1.7783,\n",
      "          0.6330,  1.9503, -1.7280,  0.0204,  1.8930,  2.5013,  2.9008, -0.0242,\n",
      "          1.5629,  2.4864,  2.0565,  1.5185,  0.1033,  0.7187, -0.1259,  1.9349,\n",
      "          0.0165, -0.0965,  2.1189, -0.2565,  0.5036, -1.0598, -2.2981, -0.0859,\n",
      "          0.8270, -1.8429,  0.0118, -0.0174, -0.5732, -0.6276,  0.3815,  1.0661,\n",
      "         -0.6287, -0.8592,  0.3710,  1.1645,  0.6001, -1.3395,  0.6647,  1.4397,\n",
      "         -1.0174, -1.0876, -0.1294, -0.1851, -0.7667, -1.5447, -0.5711, -2.6890,\n",
      "         -0.6028,  1.2484,  1.7918, -0.1909, -0.3010, -0.0398,  1.5034, -2.5400,\n",
      "         -0.1591, -0.0844,  0.3797, -0.5878,  0.3438, -0.8890, -0.8700,  1.1088,\n",
      "          0.1263, -0.7104,  0.5387, -0.5933, -1.2953, -0.4043, -0.2480,  0.5647,\n",
      "         -0.5502,  0.3524, -2.4642, -0.6873, -1.5209,  0.9125, -2.2741, -0.8956,\n",
      "         -0.9325, -0.8715, -1.3131, -0.9653, -2.6662, -1.0803, -1.4651, -0.2896,\n",
      "         -1.7786,  0.4477, -1.4179, -0.7448, -3.4290,  0.4212,  0.1063, -0.9087,\n",
      "         -2.1239, -1.6710, -1.2817, -1.4180, -2.2022, -2.1471, -3.2908]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor(-3.4290, grad_fn=<MinBackward1>)\n",
      "tensor(4.4755, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42982a843e4474087a74e77d90cec72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7230dec0596143a890dca0cfbaebb784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65e5b8a127c4f3a993d88526ab7cca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079c9e93199341f89230950b2ec3369a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22159491884262528\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb5f5a6823c4160a93fb9c9192c51de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "for min_samples in range(1,50, 1):\n",
    "    clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "    clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "    scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(list(range(1,50, 1)), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "for rule in rules:\n",
    "    n_pos = 0\n",
    "    for c,p,v in rule['rule']:\n",
    "        if p == '>':\n",
    "            n_pos += 1\n",
    "    rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509d70bb7db44f4cb328ab375efb1e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "probs = [r['proba'] for r in rules]\n",
    "plt.hist(probs, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "rules = sorted(rules, key=lambda x:x['pos'])\n",
    "rules = [r for r in rules if r['proba'] > 50]\n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- rule 0 length 4 -------------\n",
      "{'target': ' then class: 3 (proba: 55.56%) | based on 9 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '>', 0.5), ('other vegetables', '>', 0.5)], 'proba': 55.56, 'pos': 2}\n",
      "------------- rule 1 length 4 -------------\n",
      "{'target': ' then class: 5 (proba: 57.14%) | based on 7 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '>', 0.5), ('hard cheese', '>', 0.5)], 'proba': 57.14, 'pos': 2}\n",
      "------------- rule 2 length 4 -------------\n",
      "{'target': ' then class: 1 (proba: 57.14%) | based on 7 samples', 'rule': [('oil', '>', 0.5), ('whole milk', '<=', 0.5), ('sausage', '>', 0.5)], 'proba': 57.14, 'pos': 2}\n",
      "------------- rule 3 length 4 -------------\n",
      "{'target': ' then class: 0 (proba: 66.67%) | based on 6 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '<=', 0.5), ('canned fish', '<=', 0.5), ('beverages', '<=', 0.5), ('kitchen towels', '<=', 0.5), ('UHT-milk', '<=', 0.5), ('cake bar', '<=', 0.5), ('skin care', '<=', 0.5), ('liver loaf', '<=', 0.5), ('fruit/vegetable juice', '<=', 0.5), ('prosecco', '<=', 0.5), ('seasonal products', '<=', 0.5), ('specialty fat', '<=', 0.5), ('nut snack', '<=', 0.5), ('whole milk', '<=', 0.5), ('pork', '<=', 0.5), ('herbs', '<=', 0.5), ('detergent', '<=', 0.5), ('light bulbs', '<=', 0.5), ('flower (seeds)', '<=', 0.5), ('vinegar', '<=', 0.5), ('cocoa drinks', '<=', 0.5), ('Instant food products', '<=', 0.5), ('frozen vegetables', '<=', 0.5), ('canned beer', '<=', 0.5), ('ham', '<=', 0.5), ('dishes', '<=', 0.5), ('turkey', '<=', 0.5), ('semi-finished bread', '<=', 0.5), ('syrup', '<=', 0.5), ('liquor', '<=', 0.5), ('red/blush wine', '<=', 0.5), ('whipped/sour cream', '<=', 0.5), ('sweet spreads', '<=', 0.5), ('frozen potato products', '<=', 0.5), ('liquor (appetizer)', '<=', 0.5), ('frozen fish', '<=', 0.5), ('chocolate marshmallow', '<=', 0.5), ('flour', '<=', 0.5), ('spices', '<=', 0.5), ('curd', '<=', 0.5), ('dessert', '<=', 0.5), ('brown bread', '<=', 0.5), ('butter', '<=', 0.5), ('bottled water', '<=', 0.5), ('hygiene articles', '<=', 0.5), ('white wine', '<=', 0.5), ('meat spreads', '<=', 0.5), ('specialty bar', '<=', 0.5), ('pickled vegetables', '<=', 0.5), ('newspapers', '<=', 0.5), ('chicken', '<=', 0.5), ('curd cheese', '<=', 0.5), ('female sanitary products', '<=', 0.5), ('napkins', '<=', 0.5), ('soft cheese', '<=', 0.5), ('rice', '<=', 0.5), ('margarine', '>', 0.5), ('beef', '>', 0.5)], 'proba': 66.67, 'pos': 2}\n",
      "------------- rule 4 length 4 -------------\n",
      "{'target': ' then class: 0 (proba: 66.67%) | based on 6 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '>', 0.5), ('hard cheese', '<=', 0.5), ('chicken', '>', 0.5)], 'proba': 66.67, 'pos': 2}\n",
      "------------- rule 5 length 4 -------------\n",
      "{'target': ' then class: 1 (proba: 66.67%) | based on 6 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '>', 0.5), ('butter', '>', 0.5)], 'proba': 66.67, 'pos': 2}\n",
      "------------- rule 6 length 4 -------------\n",
      "{'target': ' then class: 5 (proba: 66.67%) | based on 6 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '>', 0.5), ('chocolate', '>', 0.5)], 'proba': 66.67, 'pos': 2}\n",
      "------------- rule 7 length 4 -------------\n",
      "{'target': ' then class: 3 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '<=', 0.5), ('canned fish', '<=', 0.5), ('beverages', '<=', 0.5), ('kitchen towels', '<=', 0.5), ('UHT-milk', '<=', 0.5), ('cake bar', '<=', 0.5), ('skin care', '<=', 0.5), ('liver loaf', '<=', 0.5), ('fruit/vegetable juice', '<=', 0.5), ('prosecco', '<=', 0.5), ('seasonal products', '<=', 0.5), ('specialty fat', '<=', 0.5), ('nut snack', '<=', 0.5), ('whole milk', '<=', 0.5), ('pork', '<=', 0.5), ('herbs', '<=', 0.5), ('detergent', '<=', 0.5), ('light bulbs', '<=', 0.5), ('flower (seeds)', '<=', 0.5), ('vinegar', '<=', 0.5), ('cocoa drinks', '<=', 0.5), ('Instant food products', '<=', 0.5), ('frozen vegetables', '<=', 0.5), ('canned beer', '<=', 0.5), ('ham', '<=', 0.5), ('dishes', '<=', 0.5), ('turkey', '<=', 0.5), ('semi-finished bread', '<=', 0.5), ('syrup', '<=', 0.5), ('liquor', '<=', 0.5), ('red/blush wine', '<=', 0.5), ('whipped/sour cream', '<=', 0.5), ('sweet spreads', '<=', 0.5), ('frozen potato products', '<=', 0.5), ('liquor (appetizer)', '<=', 0.5), ('frozen fish', '<=', 0.5), ('chocolate marshmallow', '<=', 0.5), ('flour', '<=', 0.5), ('spices', '<=', 0.5), ('curd', '<=', 0.5), ('dessert', '<=', 0.5), ('brown bread', '<=', 0.5), ('butter', '<=', 0.5), ('bottled water', '<=', 0.5), ('hygiene articles', '<=', 0.5), ('white wine', '<=', 0.5), ('meat spreads', '<=', 0.5), ('specialty bar', '<=', 0.5), ('pickled vegetables', '>', 0.5), ('other vegetables', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 8 length 4 -------------\n",
      "{'target': ' then class: 1 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '>', 0.5), ('whole milk', '<=', 0.5), ('sausage', '<=', 0.5), ('canned beer', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 9 length 4 -------------\n",
      "{'target': ' then class: 3 (proba: 80.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '>', 0.5), ('white bread', '>', 0.5)], 'proba': 80.0, 'pos': 2}\n",
      "------------- rule 10 length 4 -------------\n",
      "{'target': ' then class: 0 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '<=', 0.5), ('canned fish', '<=', 0.5), ('beverages', '<=', 0.5), ('kitchen towels', '<=', 0.5), ('UHT-milk', '<=', 0.5), ('cake bar', '<=', 0.5), ('skin care', '<=', 0.5), ('liver loaf', '<=', 0.5), ('fruit/vegetable juice', '<=', 0.5), ('prosecco', '<=', 0.5), ('seasonal products', '<=', 0.5), ('specialty fat', '<=', 0.5), ('nut snack', '<=', 0.5), ('whole milk', '<=', 0.5), ('pork', '<=', 0.5), ('herbs', '<=', 0.5), ('detergent', '<=', 0.5), ('light bulbs', '<=', 0.5), ('flower (seeds)', '<=', 0.5), ('vinegar', '<=', 0.5), ('cocoa drinks', '<=', 0.5), ('Instant food products', '<=', 0.5), ('frozen vegetables', '<=', 0.5), ('canned beer', '<=', 0.5), ('ham', '<=', 0.5), ('dishes', '<=', 0.5), ('turkey', '<=', 0.5), ('semi-finished bread', '<=', 0.5), ('syrup', '<=', 0.5), ('liquor', '<=', 0.5), ('red/blush wine', '<=', 0.5), ('whipped/sour cream', '<=', 0.5), ('sweet spreads', '<=', 0.5), ('frozen potato products', '<=', 0.5), ('liquor (appetizer)', '<=', 0.5), ('frozen fish', '<=', 0.5), ('chocolate marshmallow', '<=', 0.5), ('flour', '<=', 0.5), ('spices', '<=', 0.5), ('curd', '<=', 0.5), ('dessert', '<=', 0.5), ('brown bread', '<=', 0.5), ('butter', '<=', 0.5), ('bottled water', '<=', 0.5), ('hygiene articles', '<=', 0.5), ('white wine', '>', 0.5), ('root vegetables', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 11 length 4 -------------\n",
      "{'target': ' then class: 0 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '>', 0.5), ('whole milk', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 12 length 4 -------------\n",
      "{'target': ' then class: 14 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '<=', 0.5), ('canned fish', '<=', 0.5), ('beverages', '<=', 0.5), ('kitchen towels', '<=', 0.5), ('UHT-milk', '<=', 0.5), ('cake bar', '<=', 0.5), ('skin care', '<=', 0.5), ('liver loaf', '<=', 0.5), ('fruit/vegetable juice', '<=', 0.5), ('prosecco', '<=', 0.5), ('seasonal products', '<=', 0.5), ('specialty fat', '<=', 0.5), ('nut snack', '<=', 0.5), ('whole milk', '<=', 0.5), ('pork', '<=', 0.5), ('herbs', '<=', 0.5), ('detergent', '<=', 0.5), ('light bulbs', '<=', 0.5), ('flower (seeds)', '<=', 0.5), ('vinegar', '<=', 0.5), ('cocoa drinks', '<=', 0.5), ('Instant food products', '<=', 0.5), ('frozen vegetables', '<=', 0.5), ('canned beer', '<=', 0.5), ('ham', '<=', 0.5), ('dishes', '<=', 0.5), ('turkey', '<=', 0.5), ('semi-finished bread', '<=', 0.5), ('syrup', '<=', 0.5), ('liquor', '<=', 0.5), ('red/blush wine', '<=', 0.5), ('whipped/sour cream', '<=', 0.5), ('sweet spreads', '<=', 0.5), ('frozen potato products', '<=', 0.5), ('liquor (appetizer)', '<=', 0.5), ('frozen fish', '<=', 0.5), ('chocolate marshmallow', '<=', 0.5), ('flour', '<=', 0.5), ('spices', '<=', 0.5), ('curd', '<=', 0.5), ('dessert', '<=', 0.5), ('brown bread', '<=', 0.5), ('butter', '<=', 0.5), ('bottled water', '<=', 0.5), ('hygiene articles', '>', 0.5), ('soda', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 13 length 4 -------------\n",
      "{'target': ' then class: 3 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '<=', 0.5), ('artif. sweetener', '<=', 0.5), ('sugar', '<=', 0.5), ('butter milk', '<=', 0.5), ('house keeping products', '<=', 0.5), ('salty snack', '<=', 0.5), ('pet care', '<=', 0.5), ('pudding powder', '<=', 0.5), ('frozen meals', '<=', 0.5), ('cream cheese ', '<=', 0.5), ('packaged fruit/vegetables', '<=', 0.5), ('specialty cheese', '<=', 0.5), ('tea', '<=', 0.5), ('cat food', '<=', 0.5), ('tidbits', '<=', 0.5), ('brandy', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('dog food', '<=', 0.5), ('male cosmetics', '<=', 0.5), ('grapes', '<=', 0.5), ('roll products ', '<=', 0.5), ('meat', '<=', 0.5), ('citrus fruit', '<=', 0.5), ('canned fish', '<=', 0.5), ('beverages', '<=', 0.5), ('kitchen towels', '<=', 0.5), ('UHT-milk', '<=', 0.5), ('cake bar', '<=', 0.5), ('skin care', '<=', 0.5), ('liver loaf', '<=', 0.5), ('fruit/vegetable juice', '<=', 0.5), ('prosecco', '<=', 0.5), ('seasonal products', '<=', 0.5), ('specialty fat', '<=', 0.5), ('nut snack', '<=', 0.5), ('whole milk', '<=', 0.5), ('pork', '<=', 0.5), ('herbs', '<=', 0.5), ('detergent', '<=', 0.5), ('light bulbs', '<=', 0.5), ('flower (seeds)', '<=', 0.5), ('vinegar', '<=', 0.5), ('cocoa drinks', '<=', 0.5), ('Instant food products', '<=', 0.5), ('frozen vegetables', '>', 0.5), ('whipped/sour cream', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 14 length 4 -------------\n",
      "{'target': ' then class: 3 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '<=', 0.5), ('mayonnaise', '<=', 0.5), ('processed cheese', '<=', 0.5), ('fish', '<=', 0.5), ('berries', '<=', 0.5), ('soap', '<=', 0.5), ('hamburger meat', '<=', 0.5), ('onions', '<=', 0.5), ('rum', '<=', 0.5), ('frozen dessert', '<=', 0.5), ('mustard', '<=', 0.5), ('baking powder', '<=', 0.5), ('rolls/buns', '<=', 0.5), ('white bread', '<=', 0.5), ('condensed milk', '<=', 0.5), ('flower soil/fertilizer', '<=', 0.5), ('decalcifier', '<=', 0.5), ('coffee', '<=', 0.5), ('misc. beverages', '>', 0.5), ('brown bread', '>', 0.5)], 'proba': 60.0, 'pos': 2}\n",
      "------------- rule 15 length 4 -------------\n",
      "{'target': ' then class: 0 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '<=', 0.5), ('pastry', '<=', 0.5), ('pasta', '<=', 0.5), ('liqueur', '<=', 0.5), ('frozen fruits', '<=', 0.5), ('yogurt', '>', 0.5), ('ham', '<=', 0.5), ('hard cheese', '<=', 0.5), ('whole milk', '>', 0.5), ('canned beer', '>', 0.5)], 'proba': 60.0, 'pos': 3}\n",
      "------------- rule 16 length 4 -------------\n",
      "{'target': ' then class: 1 (proba: 60.0%) | based on 5 samples', 'rule': [('oil', '<=', 0.5), ('ketchup', '<=', 0.5), ('sliced cheese', '<=', 0.5), ('dish cleaner', '<=', 0.5), ('sparkling wine', '<=', 0.5), ('dental care', '<=', 0.5), ('specialty chocolate', '<=', 0.5), ('bottled beer', '>', 0.5), ('coffee', '<=', 0.5), ('frankfurter', '<=', 0.5), ('frozen vegetables', '<=', 0.5), ('white bread', '<=', 0.5), ('brown bread', '<=', 0.5), ('cat food', '<=', 0.5), ('tropical fruit', '<=', 0.5), ('pork', '<=', 0.5), ('butter', '<=', 0.5), ('other vegetables', '>', 0.5), ('whole milk', '>', 0.5)], 'proba': 60.0, 'pos': 3}\n"
     ]
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    r_i = rules[i]\n",
    "    print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "    print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10\n",
    "device = 'cpu'\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 025 | Total loss: 9.621 | Reg loss: 0.012 | Tree loss: 9.621 | Accuracy: 0.000000 | 6.428 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 025 | Total loss: 9.618 | Reg loss: 0.011 | Tree loss: 9.618 | Accuracy: 0.000000 | 6.431 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 025 | Total loss: 9.613 | Reg loss: 0.010 | Tree loss: 9.613 | Accuracy: 0.000000 | 6.465 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 025 | Total loss: 9.611 | Reg loss: 0.009 | Tree loss: 9.611 | Accuracy: 0.000000 | 6.474 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 025 | Total loss: 9.610 | Reg loss: 0.009 | Tree loss: 9.610 | Accuracy: 0.000000 | 6.494 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 025 | Total loss: 9.603 | Reg loss: 0.008 | Tree loss: 9.603 | Accuracy: 0.000000 | 6.525 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 025 | Total loss: 9.601 | Reg loss: 0.008 | Tree loss: 9.601 | Accuracy: 0.000000 | 6.486 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 025 | Total loss: 9.597 | Reg loss: 0.007 | Tree loss: 9.597 | Accuracy: 0.000000 | 6.461 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 025 | Total loss: 9.596 | Reg loss: 0.007 | Tree loss: 9.596 | Accuracy: 0.000000 | 6.458 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 025 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.000000 | 6.461 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 025 | Total loss: 9.595 | Reg loss: 0.006 | Tree loss: 9.595 | Accuracy: 0.000000 | 6.449 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 025 | Total loss: 9.589 | Reg loss: 0.006 | Tree loss: 9.589 | Accuracy: 0.000000 | 6.436 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 025 | Total loss: 9.587 | Reg loss: 0.006 | Tree loss: 9.587 | Accuracy: 0.000000 | 6.425 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 025 | Total loss: 9.585 | Reg loss: 0.006 | Tree loss: 9.585 | Accuracy: 0.000000 | 6.415 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 025 | Total loss: 9.583 | Reg loss: 0.006 | Tree loss: 9.583 | Accuracy: 0.000000 | 6.406 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 025 | Total loss: 9.582 | Reg loss: 0.006 | Tree loss: 9.582 | Accuracy: 0.000000 | 6.4 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 025 | Total loss: 9.580 | Reg loss: 0.006 | Tree loss: 9.580 | Accuracy: 0.000000 | 6.398 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 025 | Total loss: 9.575 | Reg loss: 0.006 | Tree loss: 9.575 | Accuracy: 0.000000 | 6.391 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 025 | Total loss: 9.576 | Reg loss: 0.006 | Tree loss: 9.576 | Accuracy: 0.000000 | 6.406 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 025 | Total loss: 9.572 | Reg loss: 0.006 | Tree loss: 9.572 | Accuracy: 0.000000 | 6.453 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 025 | Total loss: 9.571 | Reg loss: 0.006 | Tree loss: 9.571 | Accuracy: 0.005859 | 6.505 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 025 | Total loss: 9.569 | Reg loss: 0.006 | Tree loss: 9.569 | Accuracy: 0.003906 | 6.509 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 025 | Total loss: 9.568 | Reg loss: 0.006 | Tree loss: 9.568 | Accuracy: 0.003906 | 6.524 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 025 | Total loss: 9.565 | Reg loss: 0.007 | Tree loss: 9.565 | Accuracy: 0.005859 | 6.581 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 025 | Total loss: 9.564 | Reg loss: 0.007 | Tree loss: 9.564 | Accuracy: 0.019355 | 6.519 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 025 | Total loss: 9.578 | Reg loss: 0.003 | Tree loss: 9.578 | Accuracy: 0.000000 | 6.571 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 025 | Total loss: 9.580 | Reg loss: 0.003 | Tree loss: 9.580 | Accuracy: 0.000000 | 6.627 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 025 | Total loss: 9.578 | Reg loss: 0.004 | Tree loss: 9.578 | Accuracy: 0.003906 | 6.69 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 025 | Total loss: 9.575 | Reg loss: 0.004 | Tree loss: 9.575 | Accuracy: 0.000000 | 6.681 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 025 | Total loss: 9.570 | Reg loss: 0.004 | Tree loss: 9.570 | Accuracy: 0.003906 | 6.673 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 025 | Total loss: 9.570 | Reg loss: 0.004 | Tree loss: 9.570 | Accuracy: 0.007812 | 6.662 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 025 | Total loss: 9.568 | Reg loss: 0.004 | Tree loss: 9.568 | Accuracy: 0.011719 | 6.72 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 025 | Total loss: 9.565 | Reg loss: 0.004 | Tree loss: 9.565 | Accuracy: 0.023438 | 6.762 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 025 | Total loss: 9.564 | Reg loss: 0.005 | Tree loss: 9.564 | Accuracy: 0.023438 | 6.784 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 025 | Total loss: 9.559 | Reg loss: 0.005 | Tree loss: 9.559 | Accuracy: 0.044922 | 6.804 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 025 | Total loss: 9.559 | Reg loss: 0.005 | Tree loss: 9.559 | Accuracy: 0.035156 | 6.812 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 025 | Total loss: 9.557 | Reg loss: 0.005 | Tree loss: 9.557 | Accuracy: 0.056641 | 6.819 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 025 | Total loss: 9.556 | Reg loss: 0.005 | Tree loss: 9.556 | Accuracy: 0.060547 | 6.808 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 025 | Total loss: 9.555 | Reg loss: 0.006 | Tree loss: 9.555 | Accuracy: 0.060547 | 6.8 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 025 | Total loss: 9.553 | Reg loss: 0.006 | Tree loss: 9.553 | Accuracy: 0.052734 | 6.847 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 025 | Total loss: 9.549 | Reg loss: 0.006 | Tree loss: 9.549 | Accuracy: 0.076172 | 6.885 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 025 | Total loss: 9.544 | Reg loss: 0.006 | Tree loss: 9.544 | Accuracy: 0.078125 | 6.88 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 025 | Total loss: 9.542 | Reg loss: 0.006 | Tree loss: 9.542 | Accuracy: 0.082031 | 6.895 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 025 | Total loss: 9.543 | Reg loss: 0.007 | Tree loss: 9.543 | Accuracy: 0.066406 | 6.918 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 025 | Total loss: 9.541 | Reg loss: 0.007 | Tree loss: 9.541 | Accuracy: 0.064453 | 6.928 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 025 | Total loss: 9.539 | Reg loss: 0.007 | Tree loss: 9.539 | Accuracy: 0.074219 | 6.938 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 025 | Total loss: 9.537 | Reg loss: 0.007 | Tree loss: 9.537 | Accuracy: 0.066406 | 6.944 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 025 | Total loss: 9.538 | Reg loss: 0.007 | Tree loss: 9.538 | Accuracy: 0.044922 | 6.953 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 025 | Total loss: 9.534 | Reg loss: 0.008 | Tree loss: 9.534 | Accuracy: 0.070312 | 6.977 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 025 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.062366 | 6.935 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 025 | Total loss: 9.550 | Reg loss: 0.005 | Tree loss: 9.550 | Accuracy: 0.080078 | 6.946 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 025 | Total loss: 9.550 | Reg loss: 0.005 | Tree loss: 9.550 | Accuracy: 0.068359 | 6.938 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 025 | Total loss: 9.547 | Reg loss: 0.005 | Tree loss: 9.547 | Accuracy: 0.064453 | 6.931 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 025 | Total loss: 9.545 | Reg loss: 0.005 | Tree loss: 9.545 | Accuracy: 0.074219 | 6.936 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 025 | Total loss: 9.547 | Reg loss: 0.006 | Tree loss: 9.547 | Accuracy: 0.068359 | 6.947 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 025 | Total loss: 9.541 | Reg loss: 0.006 | Tree loss: 9.541 | Accuracy: 0.068359 | 6.986 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 025 | Total loss: 9.541 | Reg loss: 0.006 | Tree loss: 9.541 | Accuracy: 0.058594 | 6.994 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 025 | Total loss: 9.535 | Reg loss: 0.006 | Tree loss: 9.535 | Accuracy: 0.076172 | 6.988 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 025 | Total loss: 9.535 | Reg loss: 0.006 | Tree loss: 9.535 | Accuracy: 0.074219 | 6.997 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 025 | Total loss: 9.533 | Reg loss: 0.007 | Tree loss: 9.533 | Accuracy: 0.080078 | 6.994 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 025 | Total loss: 9.533 | Reg loss: 0.007 | Tree loss: 9.533 | Accuracy: 0.072266 | 7.0 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 011 / 025 | Total loss: 9.527 | Reg loss: 0.007 | Tree loss: 9.527 | Accuracy: 0.085938 | 7.031 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 025 | Total loss: 9.527 | Reg loss: 0.007 | Tree loss: 9.527 | Accuracy: 0.082031 | 7.071 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 025 | Total loss: 9.523 | Reg loss: 0.007 | Tree loss: 9.523 | Accuracy: 0.072266 | 7.104 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 025 | Total loss: 9.521 | Reg loss: 0.008 | Tree loss: 9.521 | Accuracy: 0.076172 | 7.106 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 025 | Total loss: 9.520 | Reg loss: 0.008 | Tree loss: 9.520 | Accuracy: 0.064453 | 7.106 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 025 | Total loss: 9.518 | Reg loss: 0.008 | Tree loss: 9.518 | Accuracy: 0.085938 | 7.101 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 025 | Total loss: 9.517 | Reg loss: 0.009 | Tree loss: 9.517 | Accuracy: 0.042969 | 7.093 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 025 | Total loss: 9.510 | Reg loss: 0.009 | Tree loss: 9.510 | Accuracy: 0.060547 | 7.089 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 025 | Total loss: 9.510 | Reg loss: 0.009 | Tree loss: 9.510 | Accuracy: 0.072266 | 7.154 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 025 | Total loss: 9.504 | Reg loss: 0.009 | Tree loss: 9.504 | Accuracy: 0.085938 | 7.212 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 025 | Total loss: 9.505 | Reg loss: 0.010 | Tree loss: 9.505 | Accuracy: 0.058594 | 7.241 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 025 | Total loss: 9.502 | Reg loss: 0.010 | Tree loss: 9.502 | Accuracy: 0.087891 | 7.262 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 025 | Total loss: 9.498 | Reg loss: 0.010 | Tree loss: 9.498 | Accuracy: 0.085938 | 7.26 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 025 | Total loss: 9.499 | Reg loss: 0.011 | Tree loss: 9.499 | Accuracy: 0.073118 | 7.231 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 025 | Total loss: 9.525 | Reg loss: 0.007 | Tree loss: 9.525 | Accuracy: 0.082031 | 7.225 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 025 | Total loss: 9.519 | Reg loss: 0.007 | Tree loss: 9.519 | Accuracy: 0.095703 | 7.213 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 025 | Total loss: 9.515 | Reg loss: 0.007 | Tree loss: 9.515 | Accuracy: 0.082031 | 7.202 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 025 | Total loss: 9.514 | Reg loss: 0.008 | Tree loss: 9.514 | Accuracy: 0.078125 | 7.19 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 025 | Total loss: 9.516 | Reg loss: 0.008 | Tree loss: 9.516 | Accuracy: 0.068359 | 7.178 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 025 | Total loss: 9.512 | Reg loss: 0.008 | Tree loss: 9.512 | Accuracy: 0.066406 | 7.167 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 025 | Total loss: 9.510 | Reg loss: 0.008 | Tree loss: 9.510 | Accuracy: 0.080078 | 7.16 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 025 | Total loss: 9.502 | Reg loss: 0.008 | Tree loss: 9.502 | Accuracy: 0.107422 | 7.149 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 025 | Total loss: 9.503 | Reg loss: 0.009 | Tree loss: 9.503 | Accuracy: 0.082031 | 7.145 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 025 | Total loss: 9.500 | Reg loss: 0.009 | Tree loss: 9.500 | Accuracy: 0.105469 | 7.139 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 025 | Total loss: 9.496 | Reg loss: 0.009 | Tree loss: 9.496 | Accuracy: 0.068359 | 7.131 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 025 | Total loss: 9.490 | Reg loss: 0.010 | Tree loss: 9.490 | Accuracy: 0.097656 | 7.147 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 025 | Total loss: 9.488 | Reg loss: 0.010 | Tree loss: 9.488 | Accuracy: 0.080078 | 7.147 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 025 | Total loss: 9.485 | Reg loss: 0.010 | Tree loss: 9.485 | Accuracy: 0.087891 | 7.152 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 025 | Total loss: 9.477 | Reg loss: 0.011 | Tree loss: 9.477 | Accuracy: 0.089844 | 7.153 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 025 | Total loss: 9.473 | Reg loss: 0.011 | Tree loss: 9.473 | Accuracy: 0.083984 | 7.164 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 025 | Total loss: 9.470 | Reg loss: 0.011 | Tree loss: 9.470 | Accuracy: 0.087891 | 7.171 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 025 | Total loss: 9.467 | Reg loss: 0.012 | Tree loss: 9.467 | Accuracy: 0.089844 | 7.173 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 025 | Total loss: 9.469 | Reg loss: 0.012 | Tree loss: 9.469 | Accuracy: 0.068359 | 7.172 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 025 | Total loss: 9.459 | Reg loss: 0.012 | Tree loss: 9.459 | Accuracy: 0.082031 | 7.166 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 025 | Total loss: 9.455 | Reg loss: 0.013 | Tree loss: 9.455 | Accuracy: 0.082031 | 7.158 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 025 | Total loss: 9.449 | Reg loss: 0.013 | Tree loss: 9.449 | Accuracy: 0.078125 | 7.149 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 025 | Total loss: 9.448 | Reg loss: 0.014 | Tree loss: 9.448 | Accuracy: 0.058594 | 7.139 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 025 | Total loss: 9.442 | Reg loss: 0.014 | Tree loss: 9.442 | Accuracy: 0.064453 | 7.129 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 025 | Total loss: 9.438 | Reg loss: 0.014 | Tree loss: 9.438 | Accuracy: 0.068817 | 7.108 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 025 | Total loss: 9.485 | Reg loss: 0.010 | Tree loss: 9.485 | Accuracy: 0.097656 | 7.103 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 025 | Total loss: 9.480 | Reg loss: 0.010 | Tree loss: 9.480 | Accuracy: 0.087891 | 7.095 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 025 | Total loss: 9.477 | Reg loss: 0.010 | Tree loss: 9.477 | Accuracy: 0.085938 | 7.089 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 025 | Total loss: 9.475 | Reg loss: 0.010 | Tree loss: 9.475 | Accuracy: 0.076172 | 7.084 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 025 | Total loss: 9.469 | Reg loss: 0.010 | Tree loss: 9.469 | Accuracy: 0.064453 | 7.077 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 025 | Total loss: 9.461 | Reg loss: 0.010 | Tree loss: 9.461 | Accuracy: 0.074219 | 7.07 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 025 | Total loss: 9.458 | Reg loss: 0.011 | Tree loss: 9.458 | Accuracy: 0.082031 | 7.064 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 025 | Total loss: 9.453 | Reg loss: 0.011 | Tree loss: 9.453 | Accuracy: 0.082031 | 7.06 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 025 | Total loss: 9.445 | Reg loss: 0.011 | Tree loss: 9.445 | Accuracy: 0.099609 | 7.054 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 025 | Total loss: 9.441 | Reg loss: 0.012 | Tree loss: 9.441 | Accuracy: 0.078125 | 7.049 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 025 | Total loss: 9.442 | Reg loss: 0.012 | Tree loss: 9.442 | Accuracy: 0.060547 | 7.042 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 025 | Total loss: 9.431 | Reg loss: 0.012 | Tree loss: 9.431 | Accuracy: 0.076172 | 7.035 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 025 | Total loss: 9.417 | Reg loss: 0.013 | Tree loss: 9.417 | Accuracy: 0.085938 | 7.029 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 025 | Total loss: 9.417 | Reg loss: 0.013 | Tree loss: 9.417 | Accuracy: 0.078125 | 7.025 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 025 | Total loss: 9.409 | Reg loss: 0.014 | Tree loss: 9.409 | Accuracy: 0.103516 | 7.022 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 025 | Total loss: 9.398 | Reg loss: 0.014 | Tree loss: 9.398 | Accuracy: 0.091797 | 7.016 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 025 | Total loss: 9.393 | Reg loss: 0.014 | Tree loss: 9.393 | Accuracy: 0.091797 | 7.011 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 025 | Total loss: 9.389 | Reg loss: 0.015 | Tree loss: 9.389 | Accuracy: 0.087891 | 7.007 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 025 | Total loss: 9.380 | Reg loss: 0.015 | Tree loss: 9.380 | Accuracy: 0.068359 | 7.003 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 025 | Total loss: 9.374 | Reg loss: 0.016 | Tree loss: 9.374 | Accuracy: 0.066406 | 6.997 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 025 | Total loss: 9.358 | Reg loss: 0.016 | Tree loss: 9.358 | Accuracy: 0.080078 | 6.992 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 025 | Total loss: 9.361 | Reg loss: 0.017 | Tree loss: 9.361 | Accuracy: 0.072266 | 6.986 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 025 | Total loss: 9.344 | Reg loss: 0.017 | Tree loss: 9.344 | Accuracy: 0.080078 | 6.982 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 023 / 025 | Total loss: 9.343 | Reg loss: 0.017 | Tree loss: 9.343 | Accuracy: 0.070312 | 6.977 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 025 | Total loss: 9.333 | Reg loss: 0.018 | Tree loss: 9.333 | Accuracy: 0.068817 | 6.964 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 025 | Total loss: 9.423 | Reg loss: 0.012 | Tree loss: 9.423 | Accuracy: 0.078125 | 6.963 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 025 | Total loss: 9.420 | Reg loss: 0.012 | Tree loss: 9.420 | Accuracy: 0.089844 | 6.959 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 025 | Total loss: 9.409 | Reg loss: 0.012 | Tree loss: 9.409 | Accuracy: 0.076172 | 6.956 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 025 | Total loss: 9.396 | Reg loss: 0.012 | Tree loss: 9.396 | Accuracy: 0.089844 | 6.951 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 025 | Total loss: 9.401 | Reg loss: 0.013 | Tree loss: 9.401 | Accuracy: 0.083984 | 6.947 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 025 | Total loss: 9.391 | Reg loss: 0.013 | Tree loss: 9.391 | Accuracy: 0.074219 | 6.944 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 025 | Total loss: 9.381 | Reg loss: 0.013 | Tree loss: 9.381 | Accuracy: 0.076172 | 6.94 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 025 | Total loss: 9.362 | Reg loss: 0.013 | Tree loss: 9.362 | Accuracy: 0.076172 | 6.94 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 025 | Total loss: 9.359 | Reg loss: 0.014 | Tree loss: 9.359 | Accuracy: 0.091797 | 6.936 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 025 | Total loss: 9.351 | Reg loss: 0.014 | Tree loss: 9.351 | Accuracy: 0.087891 | 6.932 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 025 | Total loss: 9.335 | Reg loss: 0.014 | Tree loss: 9.335 | Accuracy: 0.087891 | 6.929 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 025 | Total loss: 9.332 | Reg loss: 0.015 | Tree loss: 9.332 | Accuracy: 0.070312 | 6.925 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 025 | Total loss: 9.309 | Reg loss: 0.015 | Tree loss: 9.309 | Accuracy: 0.085938 | 6.922 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 025 | Total loss: 9.311 | Reg loss: 0.016 | Tree loss: 9.311 | Accuracy: 0.072266 | 6.919 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 025 | Total loss: 9.294 | Reg loss: 0.016 | Tree loss: 9.294 | Accuracy: 0.068359 | 6.915 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 025 | Total loss: 9.301 | Reg loss: 0.017 | Tree loss: 9.301 | Accuracy: 0.056641 | 6.912 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 025 | Total loss: 9.272 | Reg loss: 0.017 | Tree loss: 9.272 | Accuracy: 0.082031 | 6.909 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 025 | Total loss: 9.257 | Reg loss: 0.018 | Tree loss: 9.257 | Accuracy: 0.074219 | 6.909 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 025 | Total loss: 9.243 | Reg loss: 0.018 | Tree loss: 9.243 | Accuracy: 0.058594 | 6.907 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 025 | Total loss: 9.232 | Reg loss: 0.018 | Tree loss: 9.232 | Accuracy: 0.074219 | 6.903 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 025 | Total loss: 9.219 | Reg loss: 0.019 | Tree loss: 9.219 | Accuracy: 0.087891 | 6.901 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 025 | Total loss: 9.196 | Reg loss: 0.019 | Tree loss: 9.196 | Accuracy: 0.078125 | 6.898 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 025 | Total loss: 9.213 | Reg loss: 0.020 | Tree loss: 9.213 | Accuracy: 0.054688 | 6.895 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 025 | Total loss: 9.172 | Reg loss: 0.020 | Tree loss: 9.172 | Accuracy: 0.064453 | 6.893 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 025 | Total loss: 9.168 | Reg loss: 0.021 | Tree loss: 9.168 | Accuracy: 0.081720 | 6.879 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 025 | Total loss: 9.316 | Reg loss: 0.015 | Tree loss: 9.316 | Accuracy: 0.091797 | 6.878 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 025 | Total loss: 9.318 | Reg loss: 0.015 | Tree loss: 9.318 | Accuracy: 0.083984 | 6.874 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 025 | Total loss: 9.307 | Reg loss: 0.015 | Tree loss: 9.307 | Accuracy: 0.087891 | 6.87 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 025 | Total loss: 9.298 | Reg loss: 0.015 | Tree loss: 9.298 | Accuracy: 0.082031 | 6.867 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 025 | Total loss: 9.280 | Reg loss: 0.015 | Tree loss: 9.280 | Accuracy: 0.087891 | 6.863 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 025 | Total loss: 9.281 | Reg loss: 0.015 | Tree loss: 9.281 | Accuracy: 0.091797 | 6.86 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 025 | Total loss: 9.257 | Reg loss: 0.016 | Tree loss: 9.257 | Accuracy: 0.105469 | 6.858 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 025 | Total loss: 9.239 | Reg loss: 0.016 | Tree loss: 9.239 | Accuracy: 0.074219 | 6.854 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 025 | Total loss: 9.232 | Reg loss: 0.016 | Tree loss: 9.232 | Accuracy: 0.091797 | 6.87 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 025 | Total loss: 9.218 | Reg loss: 0.017 | Tree loss: 9.218 | Accuracy: 0.064453 | 6.882 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 025 | Total loss: 9.204 | Reg loss: 0.017 | Tree loss: 9.204 | Accuracy: 0.076172 | 6.892 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 025 | Total loss: 9.185 | Reg loss: 0.017 | Tree loss: 9.185 | Accuracy: 0.062500 | 6.894 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 025 | Total loss: 9.171 | Reg loss: 0.018 | Tree loss: 9.171 | Accuracy: 0.093750 | 6.906 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 025 | Total loss: 9.140 | Reg loss: 0.018 | Tree loss: 9.140 | Accuracy: 0.105469 | 6.909 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 025 | Total loss: 9.137 | Reg loss: 0.019 | Tree loss: 9.137 | Accuracy: 0.080078 | 6.918 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 025 | Total loss: 9.114 | Reg loss: 0.019 | Tree loss: 9.114 | Accuracy: 0.072266 | 6.918 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 025 | Total loss: 9.094 | Reg loss: 0.020 | Tree loss: 9.094 | Accuracy: 0.074219 | 6.917 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 025 | Total loss: 9.088 | Reg loss: 0.020 | Tree loss: 9.088 | Accuracy: 0.080078 | 6.914 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 025 | Total loss: 9.079 | Reg loss: 0.020 | Tree loss: 9.079 | Accuracy: 0.103516 | 6.911 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 025 | Total loss: 9.046 | Reg loss: 0.021 | Tree loss: 9.046 | Accuracy: 0.076172 | 6.907 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 025 | Total loss: 9.020 | Reg loss: 0.021 | Tree loss: 9.020 | Accuracy: 0.070312 | 6.904 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 025 | Total loss: 9.020 | Reg loss: 0.022 | Tree loss: 9.020 | Accuracy: 0.060547 | 6.9 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 025 | Total loss: 8.989 | Reg loss: 0.022 | Tree loss: 8.989 | Accuracy: 0.083984 | 6.898 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 025 | Total loss: 8.971 | Reg loss: 0.023 | Tree loss: 8.971 | Accuracy: 0.091797 | 6.896 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 025 | Total loss: 8.968 | Reg loss: 0.023 | Tree loss: 8.968 | Accuracy: 0.066667 | 6.882 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 025 | Total loss: 9.206 | Reg loss: 0.017 | Tree loss: 9.206 | Accuracy: 0.095703 | 6.879 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 025 | Total loss: 9.192 | Reg loss: 0.017 | Tree loss: 9.192 | Accuracy: 0.097656 | 6.877 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 025 | Total loss: 9.166 | Reg loss: 0.017 | Tree loss: 9.166 | Accuracy: 0.080078 | 6.873 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 025 | Total loss: 9.135 | Reg loss: 0.017 | Tree loss: 9.135 | Accuracy: 0.089844 | 6.871 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 025 | Total loss: 9.130 | Reg loss: 0.017 | Tree loss: 9.130 | Accuracy: 0.083984 | 6.869 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 025 | Total loss: 9.119 | Reg loss: 0.018 | Tree loss: 9.119 | Accuracy: 0.083984 | 6.868 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 025 | Total loss: 9.101 | Reg loss: 0.018 | Tree loss: 9.101 | Accuracy: 0.085938 | 6.866 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 025 | Total loss: 9.073 | Reg loss: 0.018 | Tree loss: 9.073 | Accuracy: 0.091797 | 6.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 008 / 025 | Total loss: 9.049 | Reg loss: 0.019 | Tree loss: 9.049 | Accuracy: 0.058594 | 6.861 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 025 | Total loss: 9.045 | Reg loss: 0.019 | Tree loss: 9.045 | Accuracy: 0.091797 | 6.858 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 025 | Total loss: 9.014 | Reg loss: 0.019 | Tree loss: 9.014 | Accuracy: 0.099609 | 6.855 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 025 | Total loss: 8.995 | Reg loss: 0.020 | Tree loss: 8.995 | Accuracy: 0.091797 | 6.853 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 025 | Total loss: 8.980 | Reg loss: 0.020 | Tree loss: 8.980 | Accuracy: 0.076172 | 6.849 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 025 | Total loss: 8.960 | Reg loss: 0.020 | Tree loss: 8.960 | Accuracy: 0.085938 | 6.846 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 025 | Total loss: 8.937 | Reg loss: 0.021 | Tree loss: 8.937 | Accuracy: 0.076172 | 6.843 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 025 | Total loss: 8.921 | Reg loss: 0.021 | Tree loss: 8.921 | Accuracy: 0.078125 | 6.84 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 025 | Total loss: 8.902 | Reg loss: 0.022 | Tree loss: 8.902 | Accuracy: 0.099609 | 6.837 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 025 | Total loss: 8.879 | Reg loss: 0.022 | Tree loss: 8.879 | Accuracy: 0.095703 | 6.846 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 025 | Total loss: 8.846 | Reg loss: 0.022 | Tree loss: 8.846 | Accuracy: 0.089844 | 6.846 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 025 | Total loss: 8.835 | Reg loss: 0.023 | Tree loss: 8.835 | Accuracy: 0.087891 | 6.849 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 025 | Total loss: 8.807 | Reg loss: 0.023 | Tree loss: 8.807 | Accuracy: 0.083984 | 6.848 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 025 | Total loss: 8.798 | Reg loss: 0.024 | Tree loss: 8.798 | Accuracy: 0.080078 | 6.846 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 025 | Total loss: 8.768 | Reg loss: 0.024 | Tree loss: 8.768 | Accuracy: 0.060547 | 6.845 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 025 | Total loss: 8.744 | Reg loss: 0.024 | Tree loss: 8.744 | Accuracy: 0.068359 | 6.844 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 025 | Total loss: 8.724 | Reg loss: 0.025 | Tree loss: 8.724 | Accuracy: 0.081720 | 6.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 025 | Total loss: 9.026 | Reg loss: 0.019 | Tree loss: 9.026 | Accuracy: 0.089844 | 6.837 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 025 | Total loss: 8.994 | Reg loss: 0.019 | Tree loss: 8.994 | Accuracy: 0.078125 | 6.835 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 025 | Total loss: 8.987 | Reg loss: 0.019 | Tree loss: 8.987 | Accuracy: 0.080078 | 6.843 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 025 | Total loss: 8.949 | Reg loss: 0.019 | Tree loss: 8.949 | Accuracy: 0.093750 | 6.852 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 025 | Total loss: 8.954 | Reg loss: 0.020 | Tree loss: 8.954 | Accuracy: 0.101562 | 6.85 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 025 | Total loss: 8.920 | Reg loss: 0.020 | Tree loss: 8.920 | Accuracy: 0.101562 | 6.848 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 025 | Total loss: 8.909 | Reg loss: 0.020 | Tree loss: 8.909 | Accuracy: 0.089844 | 6.855 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 025 | Total loss: 8.860 | Reg loss: 0.020 | Tree loss: 8.860 | Accuracy: 0.080078 | 6.863 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 025 | Total loss: 8.830 | Reg loss: 0.021 | Tree loss: 8.830 | Accuracy: 0.091797 | 6.865 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 025 | Total loss: 8.829 | Reg loss: 0.021 | Tree loss: 8.829 | Accuracy: 0.085938 | 6.864 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 025 | Total loss: 8.793 | Reg loss: 0.021 | Tree loss: 8.793 | Accuracy: 0.064453 | 6.863 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 025 | Total loss: 8.775 | Reg loss: 0.022 | Tree loss: 8.775 | Accuracy: 0.097656 | 6.863 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 025 | Total loss: 8.760 | Reg loss: 0.022 | Tree loss: 8.760 | Accuracy: 0.089844 | 6.86 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 025 | Total loss: 8.740 | Reg loss: 0.022 | Tree loss: 8.740 | Accuracy: 0.101562 | 6.859 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 025 | Total loss: 8.707 | Reg loss: 0.023 | Tree loss: 8.707 | Accuracy: 0.072266 | 6.857 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 025 | Total loss: 8.702 | Reg loss: 0.023 | Tree loss: 8.702 | Accuracy: 0.082031 | 6.855 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 025 | Total loss: 8.668 | Reg loss: 0.023 | Tree loss: 8.668 | Accuracy: 0.085938 | 6.853 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 025 | Total loss: 8.661 | Reg loss: 0.024 | Tree loss: 8.661 | Accuracy: 0.080078 | 6.85 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 025 | Total loss: 8.646 | Reg loss: 0.024 | Tree loss: 8.646 | Accuracy: 0.101562 | 6.856 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 025 | Total loss: 8.631 | Reg loss: 0.025 | Tree loss: 8.631 | Accuracy: 0.085938 | 6.855 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 025 | Total loss: 8.569 | Reg loss: 0.025 | Tree loss: 8.569 | Accuracy: 0.082031 | 6.854 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 025 | Total loss: 8.584 | Reg loss: 0.025 | Tree loss: 8.584 | Accuracy: 0.064453 | 6.852 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 025 | Total loss: 8.514 | Reg loss: 0.026 | Tree loss: 8.514 | Accuracy: 0.099609 | 6.851 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 025 | Total loss: 8.531 | Reg loss: 0.026 | Tree loss: 8.531 | Accuracy: 0.101562 | 6.849 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 025 | Total loss: 8.484 | Reg loss: 0.027 | Tree loss: 8.484 | Accuracy: 0.096774 | 6.846 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 025 | Total loss: 8.817 | Reg loss: 0.021 | Tree loss: 8.817 | Accuracy: 0.087891 | 6.853 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 025 | Total loss: 8.786 | Reg loss: 0.021 | Tree loss: 8.786 | Accuracy: 0.082031 | 6.852 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 025 | Total loss: 8.767 | Reg loss: 0.021 | Tree loss: 8.767 | Accuracy: 0.089844 | 6.851 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 025 | Total loss: 8.739 | Reg loss: 0.021 | Tree loss: 8.739 | Accuracy: 0.083984 | 6.849 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 025 | Total loss: 8.711 | Reg loss: 0.022 | Tree loss: 8.711 | Accuracy: 0.109375 | 6.847 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 025 | Total loss: 8.722 | Reg loss: 0.022 | Tree loss: 8.722 | Accuracy: 0.085938 | 6.845 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 025 | Total loss: 8.670 | Reg loss: 0.022 | Tree loss: 8.670 | Accuracy: 0.074219 | 6.844 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 025 | Total loss: 8.652 | Reg loss: 0.022 | Tree loss: 8.652 | Accuracy: 0.097656 | 6.844 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 025 | Total loss: 8.618 | Reg loss: 0.022 | Tree loss: 8.618 | Accuracy: 0.072266 | 6.843 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 025 | Total loss: 8.576 | Reg loss: 0.023 | Tree loss: 8.576 | Accuracy: 0.107422 | 6.845 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 025 | Total loss: 8.588 | Reg loss: 0.023 | Tree loss: 8.588 | Accuracy: 0.095703 | 6.85 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 025 | Total loss: 8.569 | Reg loss: 0.023 | Tree loss: 8.569 | Accuracy: 0.070312 | 6.849 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 025 | Total loss: 8.547 | Reg loss: 0.024 | Tree loss: 8.547 | Accuracy: 0.058594 | 6.848 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 025 | Total loss: 8.497 | Reg loss: 0.024 | Tree loss: 8.497 | Accuracy: 0.089844 | 6.846 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 025 | Total loss: 8.467 | Reg loss: 0.024 | Tree loss: 8.467 | Accuracy: 0.095703 | 6.845 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 025 | Total loss: 8.445 | Reg loss: 0.025 | Tree loss: 8.445 | Accuracy: 0.093750 | 6.847 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 025 | Total loss: 8.432 | Reg loss: 0.025 | Tree loss: 8.432 | Accuracy: 0.093750 | 6.854 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 025 | Total loss: 8.438 | Reg loss: 0.026 | Tree loss: 8.438 | Accuracy: 0.082031 | 6.854 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 025 | Total loss: 8.406 | Reg loss: 0.026 | Tree loss: 8.406 | Accuracy: 0.109375 | 6.853 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 025 | Total loss: 8.345 | Reg loss: 0.026 | Tree loss: 8.345 | Accuracy: 0.076172 | 6.852 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 020 / 025 | Total loss: 8.342 | Reg loss: 0.027 | Tree loss: 8.342 | Accuracy: 0.095703 | 6.851 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 025 | Total loss: 8.291 | Reg loss: 0.027 | Tree loss: 8.291 | Accuracy: 0.105469 | 6.85 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 025 | Total loss: 8.311 | Reg loss: 0.027 | Tree loss: 8.311 | Accuracy: 0.070312 | 6.853 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 025 | Total loss: 8.267 | Reg loss: 0.028 | Tree loss: 8.267 | Accuracy: 0.074219 | 6.851 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 025 | Total loss: 8.271 | Reg loss: 0.028 | Tree loss: 8.271 | Accuracy: 0.094624 | 6.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 025 | Total loss: 8.588 | Reg loss: 0.023 | Tree loss: 8.588 | Accuracy: 0.093750 | 6.846 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 025 | Total loss: 8.564 | Reg loss: 0.023 | Tree loss: 8.564 | Accuracy: 0.093750 | 6.845 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 025 | Total loss: 8.540 | Reg loss: 0.023 | Tree loss: 8.540 | Accuracy: 0.103516 | 6.844 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 025 | Total loss: 8.518 | Reg loss: 0.023 | Tree loss: 8.518 | Accuracy: 0.123047 | 6.842 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 025 | Total loss: 8.456 | Reg loss: 0.023 | Tree loss: 8.456 | Accuracy: 0.099609 | 6.84 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 025 | Total loss: 8.470 | Reg loss: 0.024 | Tree loss: 8.470 | Accuracy: 0.064453 | 6.838 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 025 | Total loss: 8.456 | Reg loss: 0.024 | Tree loss: 8.456 | Accuracy: 0.089844 | 6.837 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 025 | Total loss: 8.405 | Reg loss: 0.024 | Tree loss: 8.405 | Accuracy: 0.072266 | 6.835 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 025 | Total loss: 8.401 | Reg loss: 0.024 | Tree loss: 8.401 | Accuracy: 0.095703 | 6.833 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 025 | Total loss: 8.395 | Reg loss: 0.024 | Tree loss: 8.395 | Accuracy: 0.111328 | 6.831 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 025 | Total loss: 8.326 | Reg loss: 0.025 | Tree loss: 8.326 | Accuracy: 0.085938 | 6.83 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 025 | Total loss: 8.341 | Reg loss: 0.025 | Tree loss: 8.341 | Accuracy: 0.070312 | 6.829 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 025 | Total loss: 8.300 | Reg loss: 0.025 | Tree loss: 8.300 | Accuracy: 0.085938 | 6.828 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 025 | Total loss: 8.264 | Reg loss: 0.026 | Tree loss: 8.264 | Accuracy: 0.078125 | 6.826 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 025 | Total loss: 8.256 | Reg loss: 0.026 | Tree loss: 8.256 | Accuracy: 0.072266 | 6.825 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 025 | Total loss: 8.222 | Reg loss: 0.026 | Tree loss: 8.222 | Accuracy: 0.093750 | 6.824 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 025 | Total loss: 8.191 | Reg loss: 0.027 | Tree loss: 8.191 | Accuracy: 0.076172 | 6.822 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 025 | Total loss: 8.161 | Reg loss: 0.027 | Tree loss: 8.161 | Accuracy: 0.082031 | 6.823 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 025 | Total loss: 8.148 | Reg loss: 0.027 | Tree loss: 8.148 | Accuracy: 0.109375 | 6.826 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 025 | Total loss: 8.153 | Reg loss: 0.027 | Tree loss: 8.153 | Accuracy: 0.070312 | 6.826 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 025 | Total loss: 8.063 | Reg loss: 0.028 | Tree loss: 8.063 | Accuracy: 0.080078 | 6.825 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 025 | Total loss: 8.085 | Reg loss: 0.028 | Tree loss: 8.085 | Accuracy: 0.089844 | 6.823 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 025 | Total loss: 8.039 | Reg loss: 0.028 | Tree loss: 8.039 | Accuracy: 0.093750 | 6.822 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 025 | Total loss: 8.094 | Reg loss: 0.029 | Tree loss: 8.094 | Accuracy: 0.056641 | 6.821 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 025 | Total loss: 7.972 | Reg loss: 0.029 | Tree loss: 7.972 | Accuracy: 0.083871 | 6.814 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 025 | Total loss: 8.320 | Reg loss: 0.025 | Tree loss: 8.320 | Accuracy: 0.109375 | 6.813 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 025 | Total loss: 8.336 | Reg loss: 0.025 | Tree loss: 8.336 | Accuracy: 0.101562 | 6.812 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 025 | Total loss: 8.322 | Reg loss: 0.025 | Tree loss: 8.322 | Accuracy: 0.091797 | 6.81 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 025 | Total loss: 8.281 | Reg loss: 0.025 | Tree loss: 8.281 | Accuracy: 0.085938 | 6.809 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 025 | Total loss: 8.227 | Reg loss: 0.025 | Tree loss: 8.227 | Accuracy: 0.082031 | 6.807 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 025 | Total loss: 8.230 | Reg loss: 0.025 | Tree loss: 8.230 | Accuracy: 0.109375 | 6.805 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 025 | Total loss: 8.227 | Reg loss: 0.025 | Tree loss: 8.227 | Accuracy: 0.074219 | 6.804 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 025 | Total loss: 8.186 | Reg loss: 0.025 | Tree loss: 8.186 | Accuracy: 0.093750 | 6.803 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 025 | Total loss: 8.145 | Reg loss: 0.026 | Tree loss: 8.145 | Accuracy: 0.083984 | 6.802 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 025 | Total loss: 8.112 | Reg loss: 0.026 | Tree loss: 8.112 | Accuracy: 0.087891 | 6.8 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 025 | Total loss: 8.120 | Reg loss: 0.026 | Tree loss: 8.120 | Accuracy: 0.060547 | 6.799 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 025 | Total loss: 8.105 | Reg loss: 0.026 | Tree loss: 8.105 | Accuracy: 0.089844 | 6.797 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 025 | Total loss: 8.044 | Reg loss: 0.026 | Tree loss: 8.044 | Accuracy: 0.078125 | 6.796 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 025 | Total loss: 8.030 | Reg loss: 0.027 | Tree loss: 8.030 | Accuracy: 0.080078 | 6.794 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 025 | Total loss: 8.022 | Reg loss: 0.027 | Tree loss: 8.022 | Accuracy: 0.095703 | 6.792 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 025 | Total loss: 8.004 | Reg loss: 0.027 | Tree loss: 8.004 | Accuracy: 0.068359 | 6.79 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 025 | Total loss: 7.939 | Reg loss: 0.028 | Tree loss: 7.939 | Accuracy: 0.115234 | 6.789 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 025 | Total loss: 7.945 | Reg loss: 0.028 | Tree loss: 7.945 | Accuracy: 0.087891 | 6.789 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 025 | Total loss: 7.900 | Reg loss: 0.028 | Tree loss: 7.900 | Accuracy: 0.091797 | 6.789 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 025 | Total loss: 7.859 | Reg loss: 0.028 | Tree loss: 7.859 | Accuracy: 0.103516 | 6.789 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 025 | Total loss: 7.863 | Reg loss: 0.029 | Tree loss: 7.863 | Accuracy: 0.089844 | 6.788 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 025 | Total loss: 7.881 | Reg loss: 0.029 | Tree loss: 7.881 | Accuracy: 0.068359 | 6.787 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 025 | Total loss: 7.855 | Reg loss: 0.029 | Tree loss: 7.855 | Accuracy: 0.087891 | 6.786 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 025 | Total loss: 7.816 | Reg loss: 0.030 | Tree loss: 7.816 | Accuracy: 0.080078 | 6.785 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 025 | Total loss: 7.804 | Reg loss: 0.030 | Tree loss: 7.804 | Accuracy: 0.088172 | 6.779 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 025 | Total loss: 8.141 | Reg loss: 0.026 | Tree loss: 8.141 | Accuracy: 0.074219 | 6.78 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 025 | Total loss: 8.071 | Reg loss: 0.026 | Tree loss: 8.071 | Accuracy: 0.101562 | 6.782 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 025 | Total loss: 8.079 | Reg loss: 0.026 | Tree loss: 8.079 | Accuracy: 0.115234 | 6.78 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 025 | Total loss: 8.046 | Reg loss: 0.026 | Tree loss: 8.046 | Accuracy: 0.082031 | 6.782 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 025 | Total loss: 8.055 | Reg loss: 0.026 | Tree loss: 8.055 | Accuracy: 0.091797 | 6.786 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 005 / 025 | Total loss: 8.002 | Reg loss: 0.026 | Tree loss: 8.002 | Accuracy: 0.080078 | 6.788 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 025 | Total loss: 7.970 | Reg loss: 0.026 | Tree loss: 7.970 | Accuracy: 0.080078 | 6.791 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 025 | Total loss: 7.996 | Reg loss: 0.027 | Tree loss: 7.996 | Accuracy: 0.074219 | 6.797 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 025 | Total loss: 7.919 | Reg loss: 0.027 | Tree loss: 7.919 | Accuracy: 0.097656 | 6.797 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 025 | Total loss: 7.902 | Reg loss: 0.027 | Tree loss: 7.902 | Accuracy: 0.103516 | 6.797 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 025 | Total loss: 7.893 | Reg loss: 0.027 | Tree loss: 7.893 | Accuracy: 0.062500 | 6.796 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 025 | Total loss: 7.833 | Reg loss: 0.027 | Tree loss: 7.833 | Accuracy: 0.107422 | 6.795 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 025 | Total loss: 7.842 | Reg loss: 0.028 | Tree loss: 7.842 | Accuracy: 0.082031 | 6.794 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 025 | Total loss: 7.766 | Reg loss: 0.028 | Tree loss: 7.766 | Accuracy: 0.093750 | 6.793 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 025 | Total loss: 7.755 | Reg loss: 0.028 | Tree loss: 7.755 | Accuracy: 0.082031 | 6.793 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 025 | Total loss: 7.758 | Reg loss: 0.028 | Tree loss: 7.758 | Accuracy: 0.074219 | 6.792 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 025 | Total loss: 7.730 | Reg loss: 0.029 | Tree loss: 7.730 | Accuracy: 0.080078 | 6.791 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 025 | Total loss: 7.706 | Reg loss: 0.029 | Tree loss: 7.706 | Accuracy: 0.080078 | 6.79 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 025 | Total loss: 7.694 | Reg loss: 0.029 | Tree loss: 7.694 | Accuracy: 0.085938 | 6.788 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 025 | Total loss: 7.659 | Reg loss: 0.029 | Tree loss: 7.659 | Accuracy: 0.095703 | 6.787 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 025 | Total loss: 7.621 | Reg loss: 0.029 | Tree loss: 7.621 | Accuracy: 0.078125 | 6.786 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 025 | Total loss: 7.628 | Reg loss: 0.030 | Tree loss: 7.628 | Accuracy: 0.085938 | 6.784 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 025 | Total loss: 7.598 | Reg loss: 0.030 | Tree loss: 7.598 | Accuracy: 0.091797 | 6.784 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 025 | Total loss: 7.595 | Reg loss: 0.030 | Tree loss: 7.595 | Accuracy: 0.087891 | 6.782 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 025 | Total loss: 7.551 | Reg loss: 0.030 | Tree loss: 7.551 | Accuracy: 0.122581 | 6.775 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 025 | Total loss: 7.896 | Reg loss: 0.027 | Tree loss: 7.896 | Accuracy: 0.089844 | 6.774 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 025 | Total loss: 7.885 | Reg loss: 0.027 | Tree loss: 7.885 | Accuracy: 0.101562 | 6.773 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 025 | Total loss: 7.828 | Reg loss: 0.027 | Tree loss: 7.828 | Accuracy: 0.085938 | 6.771 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 025 | Total loss: 7.795 | Reg loss: 0.027 | Tree loss: 7.795 | Accuracy: 0.121094 | 6.769 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 025 | Total loss: 7.828 | Reg loss: 0.027 | Tree loss: 7.828 | Accuracy: 0.072266 | 6.768 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 025 | Total loss: 7.799 | Reg loss: 0.027 | Tree loss: 7.799 | Accuracy: 0.080078 | 6.766 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 025 | Total loss: 7.759 | Reg loss: 0.028 | Tree loss: 7.759 | Accuracy: 0.085938 | 6.764 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 025 | Total loss: 7.695 | Reg loss: 0.028 | Tree loss: 7.695 | Accuracy: 0.107422 | 6.764 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 025 | Total loss: 7.694 | Reg loss: 0.028 | Tree loss: 7.694 | Accuracy: 0.052734 | 6.763 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 025 | Total loss: 7.699 | Reg loss: 0.028 | Tree loss: 7.699 | Accuracy: 0.072266 | 6.762 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 025 | Total loss: 7.626 | Reg loss: 0.028 | Tree loss: 7.626 | Accuracy: 0.082031 | 6.762 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 025 | Total loss: 7.616 | Reg loss: 0.028 | Tree loss: 7.616 | Accuracy: 0.101562 | 6.761 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 025 | Total loss: 7.575 | Reg loss: 0.029 | Tree loss: 7.575 | Accuracy: 0.109375 | 6.76 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 025 | Total loss: 7.561 | Reg loss: 0.029 | Tree loss: 7.561 | Accuracy: 0.082031 | 6.758 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 025 | Total loss: 7.559 | Reg loss: 0.029 | Tree loss: 7.559 | Accuracy: 0.085938 | 6.757 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 025 | Total loss: 7.536 | Reg loss: 0.029 | Tree loss: 7.536 | Accuracy: 0.099609 | 6.756 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 025 | Total loss: 7.525 | Reg loss: 0.029 | Tree loss: 7.525 | Accuracy: 0.095703 | 6.755 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 025 | Total loss: 7.484 | Reg loss: 0.030 | Tree loss: 7.484 | Accuracy: 0.072266 | 6.755 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 025 | Total loss: 7.442 | Reg loss: 0.030 | Tree loss: 7.442 | Accuracy: 0.097656 | 6.756 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 025 | Total loss: 7.413 | Reg loss: 0.030 | Tree loss: 7.413 | Accuracy: 0.095703 | 6.755 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 025 | Total loss: 7.447 | Reg loss: 0.030 | Tree loss: 7.447 | Accuracy: 0.101562 | 6.754 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 025 | Total loss: 7.372 | Reg loss: 0.030 | Tree loss: 7.372 | Accuracy: 0.082031 | 6.752 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 025 | Total loss: 7.377 | Reg loss: 0.031 | Tree loss: 7.377 | Accuracy: 0.070312 | 6.752 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 025 | Total loss: 7.397 | Reg loss: 0.031 | Tree loss: 7.397 | Accuracy: 0.089844 | 6.75 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 025 | Total loss: 7.377 | Reg loss: 0.031 | Tree loss: 7.377 | Accuracy: 0.070968 | 6.744 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 025 | Total loss: 7.693 | Reg loss: 0.028 | Tree loss: 7.693 | Accuracy: 0.082031 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 025 | Total loss: 7.628 | Reg loss: 0.028 | Tree loss: 7.628 | Accuracy: 0.097656 | 6.742 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 025 | Total loss: 7.649 | Reg loss: 0.028 | Tree loss: 7.649 | Accuracy: 0.095703 | 6.742 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 025 | Total loss: 7.610 | Reg loss: 0.028 | Tree loss: 7.610 | Accuracy: 0.076172 | 6.74 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 025 | Total loss: 7.550 | Reg loss: 0.028 | Tree loss: 7.550 | Accuracy: 0.109375 | 6.739 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 025 | Total loss: 7.566 | Reg loss: 0.028 | Tree loss: 7.566 | Accuracy: 0.062500 | 6.739 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 025 | Total loss: 7.548 | Reg loss: 0.029 | Tree loss: 7.548 | Accuracy: 0.082031 | 6.737 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 025 | Total loss: 7.494 | Reg loss: 0.029 | Tree loss: 7.494 | Accuracy: 0.091797 | 6.736 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 025 | Total loss: 7.477 | Reg loss: 0.029 | Tree loss: 7.477 | Accuracy: 0.082031 | 6.735 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 025 | Total loss: 7.449 | Reg loss: 0.029 | Tree loss: 7.449 | Accuracy: 0.107422 | 6.734 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 025 | Total loss: 7.420 | Reg loss: 0.029 | Tree loss: 7.420 | Accuracy: 0.121094 | 6.737 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 025 | Total loss: 7.420 | Reg loss: 0.029 | Tree loss: 7.420 | Accuracy: 0.082031 | 6.737 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 025 | Total loss: 7.395 | Reg loss: 0.029 | Tree loss: 7.395 | Accuracy: 0.087891 | 6.736 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 025 | Total loss: 7.343 | Reg loss: 0.030 | Tree loss: 7.343 | Accuracy: 0.083984 | 6.738 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 025 | Total loss: 7.341 | Reg loss: 0.030 | Tree loss: 7.341 | Accuracy: 0.074219 | 6.739 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 025 | Total loss: 7.318 | Reg loss: 0.030 | Tree loss: 7.318 | Accuracy: 0.078125 | 6.742 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 025 | Total loss: 7.285 | Reg loss: 0.030 | Tree loss: 7.285 | Accuracy: 0.087891 | 6.742 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 017 / 025 | Total loss: 7.260 | Reg loss: 0.030 | Tree loss: 7.260 | Accuracy: 0.083984 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 025 | Total loss: 7.265 | Reg loss: 0.030 | Tree loss: 7.265 | Accuracy: 0.074219 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 025 | Total loss: 7.199 | Reg loss: 0.031 | Tree loss: 7.199 | Accuracy: 0.093750 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 025 | Total loss: 7.161 | Reg loss: 0.031 | Tree loss: 7.161 | Accuracy: 0.099609 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 025 | Total loss: 7.172 | Reg loss: 0.031 | Tree loss: 7.172 | Accuracy: 0.091797 | 6.744 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 025 | Total loss: 7.173 | Reg loss: 0.031 | Tree loss: 7.173 | Accuracy: 0.082031 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 025 | Total loss: 7.148 | Reg loss: 0.031 | Tree loss: 7.148 | Accuracy: 0.085938 | 6.743 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 025 | Total loss: 7.126 | Reg loss: 0.032 | Tree loss: 7.126 | Accuracy: 0.079570 | 6.738 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 025 | Total loss: 7.459 | Reg loss: 0.029 | Tree loss: 7.459 | Accuracy: 0.070312 | 6.741 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 025 | Total loss: 7.412 | Reg loss: 0.029 | Tree loss: 7.412 | Accuracy: 0.105469 | 6.742 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 025 | Total loss: 7.414 | Reg loss: 0.029 | Tree loss: 7.414 | Accuracy: 0.082031 | 6.742 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 025 | Total loss: 7.320 | Reg loss: 0.029 | Tree loss: 7.320 | Accuracy: 0.093750 | 6.741 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 025 | Total loss: 7.352 | Reg loss: 0.029 | Tree loss: 7.352 | Accuracy: 0.068359 | 6.74 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 025 | Total loss: 7.308 | Reg loss: 0.029 | Tree loss: 7.308 | Accuracy: 0.083984 | 6.739 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 025 | Total loss: 7.311 | Reg loss: 0.029 | Tree loss: 7.311 | Accuracy: 0.080078 | 6.739 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 025 | Total loss: 7.257 | Reg loss: 0.030 | Tree loss: 7.257 | Accuracy: 0.076172 | 6.738 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 025 | Total loss: 7.288 | Reg loss: 0.030 | Tree loss: 7.288 | Accuracy: 0.074219 | 6.738 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 025 | Total loss: 7.240 | Reg loss: 0.030 | Tree loss: 7.240 | Accuracy: 0.085938 | 6.75 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 025 | Total loss: 7.247 | Reg loss: 0.030 | Tree loss: 7.247 | Accuracy: 0.085938 | 6.751 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 025 | Total loss: 7.187 | Reg loss: 0.030 | Tree loss: 7.187 | Accuracy: 0.091797 | 6.752 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 025 | Total loss: 7.114 | Reg loss: 0.030 | Tree loss: 7.114 | Accuracy: 0.085938 | 6.751 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 025 | Total loss: 7.155 | Reg loss: 0.030 | Tree loss: 7.155 | Accuracy: 0.078125 | 6.75 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 025 | Total loss: 7.105 | Reg loss: 0.031 | Tree loss: 7.105 | Accuracy: 0.089844 | 6.75 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 025 | Total loss: 7.115 | Reg loss: 0.031 | Tree loss: 7.115 | Accuracy: 0.091797 | 6.748 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 025 | Total loss: 7.096 | Reg loss: 0.031 | Tree loss: 7.096 | Accuracy: 0.099609 | 6.747 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 025 | Total loss: 7.064 | Reg loss: 0.031 | Tree loss: 7.064 | Accuracy: 0.074219 | 6.746 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 025 | Total loss: 7.048 | Reg loss: 0.031 | Tree loss: 7.048 | Accuracy: 0.087891 | 6.748 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 025 | Total loss: 7.044 | Reg loss: 0.031 | Tree loss: 7.044 | Accuracy: 0.087891 | 6.751 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 025 | Total loss: 7.007 | Reg loss: 0.032 | Tree loss: 7.007 | Accuracy: 0.095703 | 6.751 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 025 | Total loss: 6.968 | Reg loss: 0.032 | Tree loss: 6.968 | Accuracy: 0.117188 | 6.75 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 025 | Total loss: 6.987 | Reg loss: 0.032 | Tree loss: 6.987 | Accuracy: 0.085938 | 6.75 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 025 | Total loss: 6.958 | Reg loss: 0.032 | Tree loss: 6.958 | Accuracy: 0.082031 | 6.749 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 025 | Total loss: 6.915 | Reg loss: 0.032 | Tree loss: 6.915 | Accuracy: 0.124731 | 6.744 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 025 | Total loss: 7.210 | Reg loss: 0.030 | Tree loss: 7.210 | Accuracy: 0.083984 | 6.744 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 025 | Total loss: 7.194 | Reg loss: 0.030 | Tree loss: 7.194 | Accuracy: 0.093750 | 6.743 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 025 | Total loss: 7.188 | Reg loss: 0.030 | Tree loss: 7.188 | Accuracy: 0.066406 | 6.743 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 025 | Total loss: 7.163 | Reg loss: 0.030 | Tree loss: 7.163 | Accuracy: 0.083984 | 6.742 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 025 | Total loss: 7.150 | Reg loss: 0.030 | Tree loss: 7.150 | Accuracy: 0.095703 | 6.741 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 025 | Total loss: 7.161 | Reg loss: 0.030 | Tree loss: 7.161 | Accuracy: 0.078125 | 6.741 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 025 | Total loss: 7.076 | Reg loss: 0.030 | Tree loss: 7.076 | Accuracy: 0.080078 | 6.74 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 025 | Total loss: 7.076 | Reg loss: 0.030 | Tree loss: 7.076 | Accuracy: 0.078125 | 6.742 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 025 | Total loss: 7.133 | Reg loss: 0.030 | Tree loss: 7.133 | Accuracy: 0.085938 | 6.746 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 025 | Total loss: 7.038 | Reg loss: 0.031 | Tree loss: 7.038 | Accuracy: 0.082031 | 6.749 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 025 | Total loss: 6.991 | Reg loss: 0.031 | Tree loss: 6.991 | Accuracy: 0.089844 | 6.749 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 025 | Total loss: 6.958 | Reg loss: 0.031 | Tree loss: 6.958 | Accuracy: 0.111328 | 6.749 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 025 | Total loss: 6.959 | Reg loss: 0.031 | Tree loss: 6.959 | Accuracy: 0.072266 | 6.756 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 025 | Total loss: 6.916 | Reg loss: 0.031 | Tree loss: 6.916 | Accuracy: 0.113281 | 6.758 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 025 | Total loss: 6.892 | Reg loss: 0.031 | Tree loss: 6.892 | Accuracy: 0.109375 | 6.758 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 025 | Total loss: 6.895 | Reg loss: 0.031 | Tree loss: 6.895 | Accuracy: 0.091797 | 6.758 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 025 | Total loss: 6.869 | Reg loss: 0.032 | Tree loss: 6.869 | Accuracy: 0.091797 | 6.757 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 025 | Total loss: 6.848 | Reg loss: 0.032 | Tree loss: 6.848 | Accuracy: 0.103516 | 6.757 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 025 | Total loss: 6.863 | Reg loss: 0.032 | Tree loss: 6.863 | Accuracy: 0.076172 | 6.757 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 025 | Total loss: 6.865 | Reg loss: 0.032 | Tree loss: 6.865 | Accuracy: 0.068359 | 6.758 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 025 | Total loss: 6.803 | Reg loss: 0.032 | Tree loss: 6.803 | Accuracy: 0.097656 | 6.759 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 025 | Total loss: 6.794 | Reg loss: 0.032 | Tree loss: 6.794 | Accuracy: 0.080078 | 6.76 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 025 | Total loss: 6.749 | Reg loss: 0.032 | Tree loss: 6.749 | Accuracy: 0.074219 | 6.762 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 025 | Total loss: 6.742 | Reg loss: 0.033 | Tree loss: 6.742 | Accuracy: 0.083984 | 6.763 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 025 | Total loss: 6.713 | Reg loss: 0.033 | Tree loss: 6.713 | Accuracy: 0.103226 | 6.758 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 025 | Total loss: 7.008 | Reg loss: 0.031 | Tree loss: 7.008 | Accuracy: 0.072266 | 6.759 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 025 | Total loss: 7.004 | Reg loss: 0.031 | Tree loss: 7.004 | Accuracy: 0.087891 | 6.76 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 002 / 025 | Total loss: 6.983 | Reg loss: 0.031 | Tree loss: 6.983 | Accuracy: 0.080078 | 6.761 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 025 | Total loss: 6.950 | Reg loss: 0.031 | Tree loss: 6.950 | Accuracy: 0.082031 | 6.765 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 025 | Total loss: 6.914 | Reg loss: 0.031 | Tree loss: 6.914 | Accuracy: 0.097656 | 6.769 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 025 | Total loss: 6.898 | Reg loss: 0.031 | Tree loss: 6.898 | Accuracy: 0.103516 | 6.775 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 025 | Total loss: 6.913 | Reg loss: 0.031 | Tree loss: 6.913 | Accuracy: 0.089844 | 6.777 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 025 | Total loss: 6.888 | Reg loss: 0.031 | Tree loss: 6.888 | Accuracy: 0.085938 | 6.78 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 025 | Total loss: 6.861 | Reg loss: 0.031 | Tree loss: 6.861 | Accuracy: 0.072266 | 6.78 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 025 | Total loss: 6.861 | Reg loss: 0.031 | Tree loss: 6.861 | Accuracy: 0.080078 | 6.779 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 025 | Total loss: 6.788 | Reg loss: 0.031 | Tree loss: 6.788 | Accuracy: 0.083984 | 6.778 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 025 | Total loss: 6.765 | Reg loss: 0.031 | Tree loss: 6.765 | Accuracy: 0.099609 | 6.777 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 025 | Total loss: 6.755 | Reg loss: 0.032 | Tree loss: 6.755 | Accuracy: 0.111328 | 6.776 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 025 | Total loss: 6.726 | Reg loss: 0.032 | Tree loss: 6.726 | Accuracy: 0.095703 | 6.775 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 025 | Total loss: 6.750 | Reg loss: 0.032 | Tree loss: 6.750 | Accuracy: 0.080078 | 6.774 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 025 | Total loss: 6.699 | Reg loss: 0.032 | Tree loss: 6.699 | Accuracy: 0.099609 | 6.774 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 025 | Total loss: 6.694 | Reg loss: 0.032 | Tree loss: 6.694 | Accuracy: 0.076172 | 6.773 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 025 | Total loss: 6.651 | Reg loss: 0.032 | Tree loss: 6.651 | Accuracy: 0.091797 | 6.772 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 025 | Total loss: 6.641 | Reg loss: 0.032 | Tree loss: 6.641 | Accuracy: 0.082031 | 6.771 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 025 | Total loss: 6.654 | Reg loss: 0.033 | Tree loss: 6.654 | Accuracy: 0.080078 | 6.77 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 025 | Total loss: 6.612 | Reg loss: 0.033 | Tree loss: 6.612 | Accuracy: 0.099609 | 6.769 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 025 | Total loss: 6.606 | Reg loss: 0.033 | Tree loss: 6.606 | Accuracy: 0.080078 | 6.768 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 025 | Total loss: 6.573 | Reg loss: 0.033 | Tree loss: 6.573 | Accuracy: 0.085938 | 6.767 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 025 | Total loss: 6.544 | Reg loss: 0.033 | Tree loss: 6.544 | Accuracy: 0.091797 | 6.766 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 025 | Total loss: 6.547 | Reg loss: 0.033 | Tree loss: 6.547 | Accuracy: 0.090323 | 6.761 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 025 | Total loss: 6.850 | Reg loss: 0.031 | Tree loss: 6.850 | Accuracy: 0.080078 | 6.76 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 025 | Total loss: 6.744 | Reg loss: 0.031 | Tree loss: 6.744 | Accuracy: 0.101562 | 6.763 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 025 | Total loss: 6.743 | Reg loss: 0.031 | Tree loss: 6.743 | Accuracy: 0.074219 | 6.768 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 025 | Total loss: 6.736 | Reg loss: 0.032 | Tree loss: 6.736 | Accuracy: 0.093750 | 6.77 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 025 | Total loss: 6.775 | Reg loss: 0.032 | Tree loss: 6.775 | Accuracy: 0.087891 | 6.77 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 025 | Total loss: 6.758 | Reg loss: 0.032 | Tree loss: 6.758 | Accuracy: 0.083984 | 6.769 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 025 | Total loss: 6.666 | Reg loss: 0.032 | Tree loss: 6.666 | Accuracy: 0.105469 | 6.768 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 025 | Total loss: 6.719 | Reg loss: 0.032 | Tree loss: 6.719 | Accuracy: 0.089844 | 6.767 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 025 | Total loss: 6.657 | Reg loss: 0.032 | Tree loss: 6.657 | Accuracy: 0.078125 | 6.767 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 025 | Total loss: 6.646 | Reg loss: 0.032 | Tree loss: 6.646 | Accuracy: 0.087891 | 6.766 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 025 | Total loss: 6.619 | Reg loss: 0.032 | Tree loss: 6.619 | Accuracy: 0.064453 | 6.765 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 025 | Total loss: 6.585 | Reg loss: 0.032 | Tree loss: 6.585 | Accuracy: 0.070312 | 6.764 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 025 | Total loss: 6.582 | Reg loss: 0.032 | Tree loss: 6.582 | Accuracy: 0.099609 | 6.763 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 025 | Total loss: 6.552 | Reg loss: 0.032 | Tree loss: 6.552 | Accuracy: 0.087891 | 6.762 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 025 | Total loss: 6.548 | Reg loss: 0.032 | Tree loss: 6.548 | Accuracy: 0.107422 | 6.761 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 025 | Total loss: 6.476 | Reg loss: 0.033 | Tree loss: 6.476 | Accuracy: 0.103516 | 6.76 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 025 | Total loss: 6.460 | Reg loss: 0.033 | Tree loss: 6.460 | Accuracy: 0.087891 | 6.76 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 025 | Total loss: 6.453 | Reg loss: 0.033 | Tree loss: 6.453 | Accuracy: 0.099609 | 6.759 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 025 | Total loss: 6.526 | Reg loss: 0.033 | Tree loss: 6.526 | Accuracy: 0.078125 | 6.758 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 025 | Total loss: 6.440 | Reg loss: 0.033 | Tree loss: 6.440 | Accuracy: 0.070312 | 6.757 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 025 | Total loss: 6.428 | Reg loss: 0.033 | Tree loss: 6.428 | Accuracy: 0.105469 | 6.757 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 025 | Total loss: 6.402 | Reg loss: 0.033 | Tree loss: 6.402 | Accuracy: 0.087891 | 6.756 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 025 | Total loss: 6.399 | Reg loss: 0.033 | Tree loss: 6.399 | Accuracy: 0.085938 | 6.755 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 025 | Total loss: 6.371 | Reg loss: 0.034 | Tree loss: 6.371 | Accuracy: 0.095703 | 6.763 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 025 | Total loss: 6.352 | Reg loss: 0.034 | Tree loss: 6.352 | Accuracy: 0.083871 | 6.76 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 025 | Total loss: 6.640 | Reg loss: 0.032 | Tree loss: 6.640 | Accuracy: 0.087891 | 6.76 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 025 | Total loss: 6.620 | Reg loss: 0.032 | Tree loss: 6.620 | Accuracy: 0.107422 | 6.76 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 025 | Total loss: 6.599 | Reg loss: 0.032 | Tree loss: 6.599 | Accuracy: 0.082031 | 6.759 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 025 | Total loss: 6.548 | Reg loss: 0.032 | Tree loss: 6.548 | Accuracy: 0.085938 | 6.759 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 025 | Total loss: 6.621 | Reg loss: 0.032 | Tree loss: 6.621 | Accuracy: 0.068359 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 025 | Total loss: 6.490 | Reg loss: 0.032 | Tree loss: 6.490 | Accuracy: 0.105469 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 025 | Total loss: 6.504 | Reg loss: 0.032 | Tree loss: 6.504 | Accuracy: 0.074219 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 025 | Total loss: 6.501 | Reg loss: 0.032 | Tree loss: 6.501 | Accuracy: 0.074219 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 025 | Total loss: 6.479 | Reg loss: 0.032 | Tree loss: 6.479 | Accuracy: 0.103516 | 6.763 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 025 | Total loss: 6.465 | Reg loss: 0.033 | Tree loss: 6.465 | Accuracy: 0.074219 | 6.764 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 025 | Total loss: 6.377 | Reg loss: 0.033 | Tree loss: 6.377 | Accuracy: 0.082031 | 6.766 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 025 | Total loss: 6.394 | Reg loss: 0.033 | Tree loss: 6.394 | Accuracy: 0.093750 | 6.766 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 025 | Total loss: 6.406 | Reg loss: 0.033 | Tree loss: 6.406 | Accuracy: 0.078125 | 6.765 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 025 | Total loss: 6.384 | Reg loss: 0.033 | Tree loss: 6.384 | Accuracy: 0.089844 | 6.765 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 014 / 025 | Total loss: 6.345 | Reg loss: 0.033 | Tree loss: 6.345 | Accuracy: 0.068359 | 6.765 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 025 | Total loss: 6.321 | Reg loss: 0.033 | Tree loss: 6.321 | Accuracy: 0.093750 | 6.764 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 025 | Total loss: 6.321 | Reg loss: 0.033 | Tree loss: 6.321 | Accuracy: 0.078125 | 6.764 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 025 | Total loss: 6.333 | Reg loss: 0.033 | Tree loss: 6.333 | Accuracy: 0.082031 | 6.763 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 025 | Total loss: 6.290 | Reg loss: 0.033 | Tree loss: 6.290 | Accuracy: 0.089844 | 6.763 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 025 | Total loss: 6.274 | Reg loss: 0.034 | Tree loss: 6.274 | Accuracy: 0.074219 | 6.762 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 025 | Total loss: 6.217 | Reg loss: 0.034 | Tree loss: 6.217 | Accuracy: 0.103516 | 6.762 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 025 | Total loss: 6.226 | Reg loss: 0.034 | Tree loss: 6.226 | Accuracy: 0.085938 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 025 | Total loss: 6.166 | Reg loss: 0.034 | Tree loss: 6.166 | Accuracy: 0.095703 | 6.761 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 025 | Total loss: 6.178 | Reg loss: 0.034 | Tree loss: 6.178 | Accuracy: 0.113281 | 6.76 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 025 | Total loss: 6.171 | Reg loss: 0.034 | Tree loss: 6.171 | Accuracy: 0.107527 | 6.755 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 025 | Total loss: 6.440 | Reg loss: 0.033 | Tree loss: 6.440 | Accuracy: 0.101562 | 6.754 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 025 | Total loss: 6.453 | Reg loss: 0.033 | Tree loss: 6.453 | Accuracy: 0.076172 | 6.754 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 025 | Total loss: 6.416 | Reg loss: 0.033 | Tree loss: 6.416 | Accuracy: 0.111328 | 6.753 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 025 | Total loss: 6.388 | Reg loss: 0.033 | Tree loss: 6.388 | Accuracy: 0.085938 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 025 | Total loss: 6.339 | Reg loss: 0.033 | Tree loss: 6.339 | Accuracy: 0.093750 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 025 | Total loss: 6.319 | Reg loss: 0.033 | Tree loss: 6.319 | Accuracy: 0.074219 | 6.751 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 025 | Total loss: 6.308 | Reg loss: 0.033 | Tree loss: 6.308 | Accuracy: 0.074219 | 6.751 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 025 | Total loss: 6.314 | Reg loss: 0.033 | Tree loss: 6.314 | Accuracy: 0.091797 | 6.75 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 025 | Total loss: 6.277 | Reg loss: 0.033 | Tree loss: 6.277 | Accuracy: 0.082031 | 6.75 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 025 | Total loss: 6.252 | Reg loss: 0.033 | Tree loss: 6.252 | Accuracy: 0.105469 | 6.749 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 025 | Total loss: 6.270 | Reg loss: 0.033 | Tree loss: 6.270 | Accuracy: 0.082031 | 6.749 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 025 | Total loss: 6.211 | Reg loss: 0.033 | Tree loss: 6.211 | Accuracy: 0.083984 | 6.75 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 025 | Total loss: 6.175 | Reg loss: 0.033 | Tree loss: 6.175 | Accuracy: 0.103516 | 6.749 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 025 | Total loss: 6.201 | Reg loss: 0.033 | Tree loss: 6.201 | Accuracy: 0.083984 | 6.749 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 025 | Total loss: 6.193 | Reg loss: 0.034 | Tree loss: 6.193 | Accuracy: 0.083984 | 6.748 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 025 | Total loss: 6.173 | Reg loss: 0.034 | Tree loss: 6.173 | Accuracy: 0.093750 | 6.753 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 025 | Total loss: 6.126 | Reg loss: 0.034 | Tree loss: 6.126 | Accuracy: 0.095703 | 6.753 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 025 | Total loss: 6.125 | Reg loss: 0.034 | Tree loss: 6.125 | Accuracy: 0.105469 | 6.753 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 025 | Total loss: 6.096 | Reg loss: 0.034 | Tree loss: 6.096 | Accuracy: 0.082031 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 025 | Total loss: 6.129 | Reg loss: 0.034 | Tree loss: 6.129 | Accuracy: 0.078125 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 025 | Total loss: 6.075 | Reg loss: 0.034 | Tree loss: 6.075 | Accuracy: 0.068359 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 025 | Total loss: 6.015 | Reg loss: 0.034 | Tree loss: 6.015 | Accuracy: 0.085938 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 025 | Total loss: 6.050 | Reg loss: 0.034 | Tree loss: 6.050 | Accuracy: 0.095703 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 025 | Total loss: 6.040 | Reg loss: 0.034 | Tree loss: 6.040 | Accuracy: 0.080078 | 6.752 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 025 | Total loss: 6.055 | Reg loss: 0.035 | Tree loss: 6.055 | Accuracy: 0.073118 | 6.748 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 025 | Total loss: 6.237 | Reg loss: 0.033 | Tree loss: 6.237 | Accuracy: 0.115234 | 6.748 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 025 | Total loss: 6.262 | Reg loss: 0.033 | Tree loss: 6.262 | Accuracy: 0.089844 | 6.748 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 025 | Total loss: 6.243 | Reg loss: 0.033 | Tree loss: 6.243 | Accuracy: 0.087891 | 6.748 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 025 | Total loss: 6.234 | Reg loss: 0.033 | Tree loss: 6.234 | Accuracy: 0.091797 | 6.748 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 025 | Total loss: 6.139 | Reg loss: 0.033 | Tree loss: 6.139 | Accuracy: 0.093750 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 025 | Total loss: 6.152 | Reg loss: 0.033 | Tree loss: 6.152 | Accuracy: 0.083984 | 6.752 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 025 | Total loss: 6.181 | Reg loss: 0.033 | Tree loss: 6.181 | Accuracy: 0.089844 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 025 | Total loss: 6.166 | Reg loss: 0.033 | Tree loss: 6.166 | Accuracy: 0.080078 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 025 | Total loss: 6.162 | Reg loss: 0.034 | Tree loss: 6.162 | Accuracy: 0.082031 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 025 | Total loss: 6.090 | Reg loss: 0.034 | Tree loss: 6.090 | Accuracy: 0.089844 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 025 | Total loss: 6.041 | Reg loss: 0.034 | Tree loss: 6.041 | Accuracy: 0.083984 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 025 | Total loss: 6.015 | Reg loss: 0.034 | Tree loss: 6.015 | Accuracy: 0.080078 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 025 | Total loss: 6.007 | Reg loss: 0.034 | Tree loss: 6.007 | Accuracy: 0.093750 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 025 | Total loss: 6.034 | Reg loss: 0.034 | Tree loss: 6.034 | Accuracy: 0.080078 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 025 | Total loss: 6.029 | Reg loss: 0.034 | Tree loss: 6.029 | Accuracy: 0.113281 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 025 | Total loss: 5.960 | Reg loss: 0.034 | Tree loss: 5.960 | Accuracy: 0.087891 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 025 | Total loss: 6.011 | Reg loss: 0.034 | Tree loss: 6.011 | Accuracy: 0.068359 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 025 | Total loss: 5.950 | Reg loss: 0.034 | Tree loss: 5.950 | Accuracy: 0.089844 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 025 | Total loss: 5.927 | Reg loss: 0.034 | Tree loss: 5.927 | Accuracy: 0.089844 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 025 | Total loss: 5.928 | Reg loss: 0.034 | Tree loss: 5.928 | Accuracy: 0.074219 | 6.751 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 025 | Total loss: 5.914 | Reg loss: 0.035 | Tree loss: 5.914 | Accuracy: 0.095703 | 6.75 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 025 | Total loss: 5.910 | Reg loss: 0.035 | Tree loss: 5.910 | Accuracy: 0.087891 | 6.749 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 025 | Total loss: 5.888 | Reg loss: 0.035 | Tree loss: 5.888 | Accuracy: 0.085938 | 6.749 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 025 | Total loss: 5.847 | Reg loss: 0.035 | Tree loss: 5.847 | Accuracy: 0.082031 | 6.749 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 025 | Total loss: 5.852 | Reg loss: 0.035 | Tree loss: 5.852 | Accuracy: 0.083871 | 6.745 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 000 / 025 | Total loss: 6.123 | Reg loss: 0.034 | Tree loss: 6.123 | Accuracy: 0.078125 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 025 | Total loss: 6.047 | Reg loss: 0.034 | Tree loss: 6.047 | Accuracy: 0.056641 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 025 | Total loss: 6.058 | Reg loss: 0.034 | Tree loss: 6.058 | Accuracy: 0.087891 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 025 | Total loss: 6.060 | Reg loss: 0.034 | Tree loss: 6.060 | Accuracy: 0.111328 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 025 | Total loss: 5.983 | Reg loss: 0.034 | Tree loss: 5.983 | Accuracy: 0.109375 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 025 | Total loss: 5.996 | Reg loss: 0.034 | Tree loss: 5.996 | Accuracy: 0.089844 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 025 | Total loss: 5.995 | Reg loss: 0.034 | Tree loss: 5.995 | Accuracy: 0.078125 | 6.748 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 025 | Total loss: 5.935 | Reg loss: 0.034 | Tree loss: 5.935 | Accuracy: 0.107422 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 025 | Total loss: 5.950 | Reg loss: 0.034 | Tree loss: 5.950 | Accuracy: 0.113281 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 025 | Total loss: 5.933 | Reg loss: 0.034 | Tree loss: 5.933 | Accuracy: 0.095703 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 025 | Total loss: 5.920 | Reg loss: 0.034 | Tree loss: 5.920 | Accuracy: 0.070312 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 025 | Total loss: 5.867 | Reg loss: 0.034 | Tree loss: 5.867 | Accuracy: 0.083984 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 025 | Total loss: 5.896 | Reg loss: 0.034 | Tree loss: 5.896 | Accuracy: 0.101562 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 025 | Total loss: 5.849 | Reg loss: 0.034 | Tree loss: 5.849 | Accuracy: 0.080078 | 6.747 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 025 | Total loss: 5.851 | Reg loss: 0.034 | Tree loss: 5.851 | Accuracy: 0.072266 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 025 | Total loss: 5.766 | Reg loss: 0.034 | Tree loss: 5.766 | Accuracy: 0.089844 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 025 | Total loss: 5.794 | Reg loss: 0.035 | Tree loss: 5.794 | Accuracy: 0.085938 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 025 | Total loss: 5.773 | Reg loss: 0.035 | Tree loss: 5.773 | Accuracy: 0.123047 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 025 | Total loss: 5.775 | Reg loss: 0.035 | Tree loss: 5.775 | Accuracy: 0.076172 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 025 | Total loss: 5.756 | Reg loss: 0.035 | Tree loss: 5.756 | Accuracy: 0.083984 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 025 | Total loss: 5.779 | Reg loss: 0.035 | Tree loss: 5.779 | Accuracy: 0.089844 | 6.746 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 025 | Total loss: 5.770 | Reg loss: 0.035 | Tree loss: 5.770 | Accuracy: 0.068359 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 025 | Total loss: 5.754 | Reg loss: 0.035 | Tree loss: 5.754 | Accuracy: 0.085938 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 025 | Total loss: 5.738 | Reg loss: 0.035 | Tree loss: 5.738 | Accuracy: 0.082031 | 6.745 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 025 | Total loss: 5.700 | Reg loss: 0.035 | Tree loss: 5.700 | Accuracy: 0.068817 | 6.742 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 025 | Total loss: 5.947 | Reg loss: 0.034 | Tree loss: 5.947 | Accuracy: 0.105469 | 6.742 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 025 | Total loss: 5.943 | Reg loss: 0.034 | Tree loss: 5.943 | Accuracy: 0.072266 | 6.742 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 025 | Total loss: 5.895 | Reg loss: 0.034 | Tree loss: 5.895 | Accuracy: 0.074219 | 6.741 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 025 | Total loss: 5.885 | Reg loss: 0.034 | Tree loss: 5.885 | Accuracy: 0.089844 | 6.741 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 025 | Total loss: 5.889 | Reg loss: 0.034 | Tree loss: 5.889 | Accuracy: 0.097656 | 6.741 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 025 | Total loss: 5.863 | Reg loss: 0.034 | Tree loss: 5.863 | Accuracy: 0.078125 | 6.741 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 025 | Total loss: 5.825 | Reg loss: 0.034 | Tree loss: 5.825 | Accuracy: 0.099609 | 6.741 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 025 | Total loss: 5.763 | Reg loss: 0.034 | Tree loss: 5.763 | Accuracy: 0.101562 | 6.744 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 025 | Total loss: 5.832 | Reg loss: 0.034 | Tree loss: 5.832 | Accuracy: 0.072266 | 6.747 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 025 | Total loss: 5.752 | Reg loss: 0.034 | Tree loss: 5.752 | Accuracy: 0.089844 | 6.747 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 025 | Total loss: 5.725 | Reg loss: 0.034 | Tree loss: 5.725 | Accuracy: 0.099609 | 6.747 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 025 | Total loss: 5.705 | Reg loss: 0.035 | Tree loss: 5.705 | Accuracy: 0.070312 | 6.747 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 025 | Total loss: 5.716 | Reg loss: 0.035 | Tree loss: 5.716 | Accuracy: 0.076172 | 6.747 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 025 | Total loss: 5.697 | Reg loss: 0.035 | Tree loss: 5.697 | Accuracy: 0.103516 | 6.749 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 025 | Total loss: 5.687 | Reg loss: 0.035 | Tree loss: 5.687 | Accuracy: 0.091797 | 6.751 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 025 | Total loss: 5.653 | Reg loss: 0.035 | Tree loss: 5.653 | Accuracy: 0.083984 | 6.755 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 025 | Total loss: 5.669 | Reg loss: 0.035 | Tree loss: 5.669 | Accuracy: 0.107422 | 6.757 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 025 | Total loss: 5.618 | Reg loss: 0.035 | Tree loss: 5.618 | Accuracy: 0.082031 | 6.759 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 025 | Total loss: 5.635 | Reg loss: 0.035 | Tree loss: 5.635 | Accuracy: 0.083984 | 6.761 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 025 | Total loss: 5.625 | Reg loss: 0.035 | Tree loss: 5.625 | Accuracy: 0.101562 | 6.762 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 025 | Total loss: 5.568 | Reg loss: 0.035 | Tree loss: 5.568 | Accuracy: 0.091797 | 6.765 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 025 | Total loss: 5.593 | Reg loss: 0.035 | Tree loss: 5.593 | Accuracy: 0.068359 | 6.767 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 025 | Total loss: 5.551 | Reg loss: 0.035 | Tree loss: 5.551 | Accuracy: 0.087891 | 6.771 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 025 | Total loss: 5.546 | Reg loss: 0.036 | Tree loss: 5.546 | Accuracy: 0.072266 | 6.773 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 025 | Total loss: 5.563 | Reg loss: 0.036 | Tree loss: 5.563 | Accuracy: 0.107527 | 6.772 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 025 | Total loss: 5.764 | Reg loss: 0.034 | Tree loss: 5.764 | Accuracy: 0.101562 | 6.773 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 025 | Total loss: 5.745 | Reg loss: 0.034 | Tree loss: 5.745 | Accuracy: 0.103516 | 6.773 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 025 | Total loss: 5.764 | Reg loss: 0.035 | Tree loss: 5.764 | Accuracy: 0.087891 | 6.775 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 025 | Total loss: 5.723 | Reg loss: 0.035 | Tree loss: 5.723 | Accuracy: 0.091797 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 025 | Total loss: 5.707 | Reg loss: 0.035 | Tree loss: 5.707 | Accuracy: 0.091797 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 025 | Total loss: 5.666 | Reg loss: 0.035 | Tree loss: 5.666 | Accuracy: 0.091797 | 6.776 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 025 | Total loss: 5.677 | Reg loss: 0.035 | Tree loss: 5.677 | Accuracy: 0.089844 | 6.776 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 025 | Total loss: 5.636 | Reg loss: 0.035 | Tree loss: 5.636 | Accuracy: 0.085938 | 6.776 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 025 | Total loss: 5.672 | Reg loss: 0.035 | Tree loss: 5.672 | Accuracy: 0.085938 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 025 | Total loss: 5.611 | Reg loss: 0.035 | Tree loss: 5.611 | Accuracy: 0.087891 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 025 | Total loss: 5.627 | Reg loss: 0.035 | Tree loss: 5.627 | Accuracy: 0.078125 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 025 | Total loss: 5.597 | Reg loss: 0.035 | Tree loss: 5.597 | Accuracy: 0.085938 | 6.779 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 012 / 025 | Total loss: 5.571 | Reg loss: 0.035 | Tree loss: 5.571 | Accuracy: 0.093750 | 6.779 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 025 | Total loss: 5.564 | Reg loss: 0.035 | Tree loss: 5.564 | Accuracy: 0.095703 | 6.779 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 025 | Total loss: 5.538 | Reg loss: 0.035 | Tree loss: 5.538 | Accuracy: 0.074219 | 6.778 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 025 | Total loss: 5.485 | Reg loss: 0.035 | Tree loss: 5.485 | Accuracy: 0.099609 | 6.778 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 025 | Total loss: 5.533 | Reg loss: 0.035 | Tree loss: 5.533 | Accuracy: 0.078125 | 6.778 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 025 | Total loss: 5.468 | Reg loss: 0.035 | Tree loss: 5.468 | Accuracy: 0.087891 | 6.778 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 025 | Total loss: 5.467 | Reg loss: 0.035 | Tree loss: 5.467 | Accuracy: 0.076172 | 6.778 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 025 | Total loss: 5.436 | Reg loss: 0.035 | Tree loss: 5.436 | Accuracy: 0.089844 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 025 | Total loss: 5.444 | Reg loss: 0.036 | Tree loss: 5.444 | Accuracy: 0.103516 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 025 | Total loss: 5.419 | Reg loss: 0.036 | Tree loss: 5.419 | Accuracy: 0.097656 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 025 | Total loss: 5.405 | Reg loss: 0.036 | Tree loss: 5.405 | Accuracy: 0.089844 | 6.777 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 025 | Total loss: 5.448 | Reg loss: 0.036 | Tree loss: 5.448 | Accuracy: 0.060547 | 6.776 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 025 | Total loss: 5.424 | Reg loss: 0.036 | Tree loss: 5.424 | Accuracy: 0.062366 | 6.773 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 025 | Total loss: 5.681 | Reg loss: 0.035 | Tree loss: 5.681 | Accuracy: 0.076172 | 6.773 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 025 | Total loss: 5.604 | Reg loss: 0.035 | Tree loss: 5.604 | Accuracy: 0.085938 | 6.773 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 025 | Total loss: 5.608 | Reg loss: 0.035 | Tree loss: 5.608 | Accuracy: 0.093750 | 6.772 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 025 | Total loss: 5.570 | Reg loss: 0.035 | Tree loss: 5.570 | Accuracy: 0.085938 | 6.772 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 025 | Total loss: 5.606 | Reg loss: 0.035 | Tree loss: 5.606 | Accuracy: 0.082031 | 6.772 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 025 | Total loss: 5.575 | Reg loss: 0.035 | Tree loss: 5.575 | Accuracy: 0.085938 | 6.772 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 025 | Total loss: 5.498 | Reg loss: 0.035 | Tree loss: 5.498 | Accuracy: 0.111328 | 6.772 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 025 | Total loss: 5.480 | Reg loss: 0.035 | Tree loss: 5.480 | Accuracy: 0.089844 | 6.771 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 025 | Total loss: 5.476 | Reg loss: 0.035 | Tree loss: 5.476 | Accuracy: 0.083984 | 6.771 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 025 | Total loss: 5.481 | Reg loss: 0.035 | Tree loss: 5.481 | Accuracy: 0.082031 | 6.771 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 025 | Total loss: 5.446 | Reg loss: 0.035 | Tree loss: 5.446 | Accuracy: 0.085938 | 6.77 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 025 | Total loss: 5.441 | Reg loss: 0.035 | Tree loss: 5.441 | Accuracy: 0.087891 | 6.771 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 025 | Total loss: 5.394 | Reg loss: 0.035 | Tree loss: 5.394 | Accuracy: 0.103516 | 6.77 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 025 | Total loss: 5.427 | Reg loss: 0.035 | Tree loss: 5.427 | Accuracy: 0.082031 | 6.77 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 025 | Total loss: 5.406 | Reg loss: 0.035 | Tree loss: 5.406 | Accuracy: 0.091797 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 025 | Total loss: 5.382 | Reg loss: 0.035 | Tree loss: 5.382 | Accuracy: 0.070312 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 025 | Total loss: 5.333 | Reg loss: 0.036 | Tree loss: 5.333 | Accuracy: 0.089844 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 025 | Total loss: 5.366 | Reg loss: 0.036 | Tree loss: 5.366 | Accuracy: 0.097656 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 025 | Total loss: 5.354 | Reg loss: 0.036 | Tree loss: 5.354 | Accuracy: 0.080078 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 025 | Total loss: 5.305 | Reg loss: 0.036 | Tree loss: 5.305 | Accuracy: 0.078125 | 6.769 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 025 | Total loss: 5.305 | Reg loss: 0.036 | Tree loss: 5.305 | Accuracy: 0.074219 | 6.768 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 025 | Total loss: 5.270 | Reg loss: 0.036 | Tree loss: 5.270 | Accuracy: 0.109375 | 6.768 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 025 | Total loss: 5.288 | Reg loss: 0.036 | Tree loss: 5.288 | Accuracy: 0.095703 | 6.767 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 025 | Total loss: 5.254 | Reg loss: 0.036 | Tree loss: 5.254 | Accuracy: 0.083984 | 6.767 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 025 | Total loss: 5.232 | Reg loss: 0.036 | Tree loss: 5.232 | Accuracy: 0.113978 | 6.763 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 025 | Total loss: 5.465 | Reg loss: 0.035 | Tree loss: 5.465 | Accuracy: 0.091797 | 6.763 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 025 | Total loss: 5.520 | Reg loss: 0.035 | Tree loss: 5.520 | Accuracy: 0.076172 | 6.763 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 025 | Total loss: 5.531 | Reg loss: 0.035 | Tree loss: 5.531 | Accuracy: 0.082031 | 6.763 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 025 | Total loss: 5.433 | Reg loss: 0.035 | Tree loss: 5.433 | Accuracy: 0.093750 | 6.763 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 025 | Total loss: 5.427 | Reg loss: 0.035 | Tree loss: 5.427 | Accuracy: 0.085938 | 6.763 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 025 | Total loss: 5.380 | Reg loss: 0.035 | Tree loss: 5.380 | Accuracy: 0.103516 | 6.765 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 025 | Total loss: 5.371 | Reg loss: 0.035 | Tree loss: 5.371 | Accuracy: 0.087891 | 6.765 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 025 | Total loss: 5.324 | Reg loss: 0.035 | Tree loss: 5.324 | Accuracy: 0.083984 | 6.767 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 025 | Total loss: 5.323 | Reg loss: 0.035 | Tree loss: 5.323 | Accuracy: 0.095703 | 6.768 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 025 | Total loss: 5.322 | Reg loss: 0.035 | Tree loss: 5.322 | Accuracy: 0.117188 | 6.77 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 025 | Total loss: 5.321 | Reg loss: 0.035 | Tree loss: 5.321 | Accuracy: 0.080078 | 6.772 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 025 | Total loss: 5.291 | Reg loss: 0.035 | Tree loss: 5.291 | Accuracy: 0.078125 | 6.772 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 025 | Total loss: 5.301 | Reg loss: 0.036 | Tree loss: 5.301 | Accuracy: 0.091797 | 6.773 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 025 | Total loss: 5.309 | Reg loss: 0.036 | Tree loss: 5.309 | Accuracy: 0.095703 | 6.773 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 025 | Total loss: 5.290 | Reg loss: 0.036 | Tree loss: 5.290 | Accuracy: 0.076172 | 6.774 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 025 | Total loss: 5.261 | Reg loss: 0.036 | Tree loss: 5.261 | Accuracy: 0.087891 | 6.776 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 025 | Total loss: 5.218 | Reg loss: 0.036 | Tree loss: 5.218 | Accuracy: 0.087891 | 6.776 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 025 | Total loss: 5.246 | Reg loss: 0.036 | Tree loss: 5.246 | Accuracy: 0.085938 | 6.775 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 025 | Total loss: 5.169 | Reg loss: 0.036 | Tree loss: 5.169 | Accuracy: 0.083984 | 6.775 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 025 | Total loss: 5.147 | Reg loss: 0.036 | Tree loss: 5.147 | Accuracy: 0.101562 | 6.774 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 025 | Total loss: 5.161 | Reg loss: 0.036 | Tree loss: 5.161 | Accuracy: 0.080078 | 6.774 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 025 | Total loss: 5.129 | Reg loss: 0.036 | Tree loss: 5.129 | Accuracy: 0.091797 | 6.773 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 025 | Total loss: 5.196 | Reg loss: 0.036 | Tree loss: 5.196 | Accuracy: 0.083984 | 6.772 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 025 | Total loss: 5.174 | Reg loss: 0.036 | Tree loss: 5.174 | Accuracy: 0.072266 | 6.772 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 024 / 025 | Total loss: 5.026 | Reg loss: 0.036 | Tree loss: 5.026 | Accuracy: 0.101075 | 6.773 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 025 | Total loss: 5.367 | Reg loss: 0.035 | Tree loss: 5.367 | Accuracy: 0.082031 | 6.78 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 025 | Total loss: 5.332 | Reg loss: 0.035 | Tree loss: 5.332 | Accuracy: 0.078125 | 6.782 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 025 | Total loss: 5.308 | Reg loss: 0.035 | Tree loss: 5.308 | Accuracy: 0.082031 | 6.782 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 025 | Total loss: 5.297 | Reg loss: 0.035 | Tree loss: 5.297 | Accuracy: 0.099609 | 6.785 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 025 | Total loss: 5.281 | Reg loss: 0.035 | Tree loss: 5.281 | Accuracy: 0.083984 | 6.787 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 025 | Total loss: 5.299 | Reg loss: 0.035 | Tree loss: 5.299 | Accuracy: 0.083984 | 6.789 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 025 | Total loss: 5.276 | Reg loss: 0.036 | Tree loss: 5.276 | Accuracy: 0.082031 | 6.791 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 025 | Total loss: 5.237 | Reg loss: 0.036 | Tree loss: 5.237 | Accuracy: 0.089844 | 6.795 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 025 | Total loss: 5.253 | Reg loss: 0.036 | Tree loss: 5.253 | Accuracy: 0.074219 | 6.797 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 025 | Total loss: 5.266 | Reg loss: 0.036 | Tree loss: 5.266 | Accuracy: 0.068359 | 6.796 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 025 | Total loss: 5.189 | Reg loss: 0.036 | Tree loss: 5.189 | Accuracy: 0.082031 | 6.796 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 025 | Total loss: 5.163 | Reg loss: 0.036 | Tree loss: 5.163 | Accuracy: 0.083984 | 6.795 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 025 | Total loss: 5.110 | Reg loss: 0.036 | Tree loss: 5.110 | Accuracy: 0.093750 | 6.795 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 025 | Total loss: 5.138 | Reg loss: 0.036 | Tree loss: 5.138 | Accuracy: 0.091797 | 6.795 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 025 | Total loss: 5.095 | Reg loss: 0.036 | Tree loss: 5.095 | Accuracy: 0.085938 | 6.796 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 025 | Total loss: 5.075 | Reg loss: 0.036 | Tree loss: 5.075 | Accuracy: 0.093750 | 6.796 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 025 | Total loss: 5.097 | Reg loss: 0.036 | Tree loss: 5.097 | Accuracy: 0.087891 | 6.797 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 025 | Total loss: 5.093 | Reg loss: 0.036 | Tree loss: 5.093 | Accuracy: 0.089844 | 6.799 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 025 | Total loss: 5.049 | Reg loss: 0.036 | Tree loss: 5.049 | Accuracy: 0.093750 | 6.799 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 025 | Total loss: 5.064 | Reg loss: 0.036 | Tree loss: 5.064 | Accuracy: 0.105469 | 6.801 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 025 | Total loss: 5.077 | Reg loss: 0.036 | Tree loss: 5.077 | Accuracy: 0.087891 | 6.801 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 025 | Total loss: 5.067 | Reg loss: 0.036 | Tree loss: 5.067 | Accuracy: 0.072266 | 6.801 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 025 | Total loss: 4.990 | Reg loss: 0.036 | Tree loss: 4.990 | Accuracy: 0.123047 | 6.804 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 025 | Total loss: 4.966 | Reg loss: 0.036 | Tree loss: 4.966 | Accuracy: 0.083984 | 6.803 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 025 | Total loss: 4.985 | Reg loss: 0.037 | Tree loss: 4.985 | Accuracy: 0.098925 | 6.8 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 025 | Total loss: 5.202 | Reg loss: 0.036 | Tree loss: 5.202 | Accuracy: 0.097656 | 6.802 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 025 | Total loss: 5.236 | Reg loss: 0.036 | Tree loss: 5.236 | Accuracy: 0.078125 | 6.802 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 025 | Total loss: 5.235 | Reg loss: 0.036 | Tree loss: 5.235 | Accuracy: 0.097656 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 025 | Total loss: 5.174 | Reg loss: 0.036 | Tree loss: 5.174 | Accuracy: 0.107422 | 6.803 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 025 | Total loss: 5.145 | Reg loss: 0.036 | Tree loss: 5.145 | Accuracy: 0.101562 | 6.802 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 025 | Total loss: 5.192 | Reg loss: 0.036 | Tree loss: 5.192 | Accuracy: 0.076172 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 025 | Total loss: 5.099 | Reg loss: 0.036 | Tree loss: 5.099 | Accuracy: 0.107422 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 025 | Total loss: 5.133 | Reg loss: 0.036 | Tree loss: 5.133 | Accuracy: 0.076172 | 6.8 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 025 | Total loss: 5.104 | Reg loss: 0.036 | Tree loss: 5.104 | Accuracy: 0.076172 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 025 | Total loss: 5.024 | Reg loss: 0.036 | Tree loss: 5.024 | Accuracy: 0.103516 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 025 | Total loss: 5.061 | Reg loss: 0.036 | Tree loss: 5.061 | Accuracy: 0.095703 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 025 | Total loss: 5.034 | Reg loss: 0.036 | Tree loss: 5.034 | Accuracy: 0.082031 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 025 | Total loss: 5.056 | Reg loss: 0.036 | Tree loss: 5.056 | Accuracy: 0.078125 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 025 | Total loss: 5.042 | Reg loss: 0.036 | Tree loss: 5.042 | Accuracy: 0.085938 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 025 | Total loss: 5.025 | Reg loss: 0.036 | Tree loss: 5.025 | Accuracy: 0.080078 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 025 | Total loss: 4.970 | Reg loss: 0.036 | Tree loss: 4.970 | Accuracy: 0.103516 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 025 | Total loss: 4.948 | Reg loss: 0.036 | Tree loss: 4.948 | Accuracy: 0.083984 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 025 | Total loss: 4.944 | Reg loss: 0.036 | Tree loss: 4.944 | Accuracy: 0.083984 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 025 | Total loss: 4.957 | Reg loss: 0.036 | Tree loss: 4.957 | Accuracy: 0.072266 | 6.8 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 025 | Total loss: 4.926 | Reg loss: 0.036 | Tree loss: 4.926 | Accuracy: 0.085938 | 6.802 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 025 | Total loss: 4.900 | Reg loss: 0.036 | Tree loss: 4.900 | Accuracy: 0.095703 | 6.801 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 025 | Total loss: 4.925 | Reg loss: 0.037 | Tree loss: 4.925 | Accuracy: 0.082031 | 6.803 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 025 | Total loss: 4.871 | Reg loss: 0.037 | Tree loss: 4.871 | Accuracy: 0.078125 | 6.803 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 025 | Total loss: 4.911 | Reg loss: 0.037 | Tree loss: 4.911 | Accuracy: 0.083984 | 6.805 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 025 | Total loss: 4.863 | Reg loss: 0.037 | Tree loss: 4.863 | Accuracy: 0.096774 | 6.802 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 025 | Total loss: 5.062 | Reg loss: 0.036 | Tree loss: 5.062 | Accuracy: 0.103516 | 6.801 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 025 | Total loss: 5.173 | Reg loss: 0.036 | Tree loss: 5.173 | Accuracy: 0.080078 | 6.801 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 025 | Total loss: 5.096 | Reg loss: 0.036 | Tree loss: 5.096 | Accuracy: 0.099609 | 6.8 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 025 | Total loss: 5.096 | Reg loss: 0.036 | Tree loss: 5.096 | Accuracy: 0.068359 | 6.8 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 025 | Total loss: 5.035 | Reg loss: 0.036 | Tree loss: 5.035 | Accuracy: 0.099609 | 6.799 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 025 | Total loss: 5.134 | Reg loss: 0.036 | Tree loss: 5.134 | Accuracy: 0.072266 | 6.799 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 025 | Total loss: 4.996 | Reg loss: 0.036 | Tree loss: 4.996 | Accuracy: 0.082031 | 6.798 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 025 | Total loss: 5.004 | Reg loss: 0.036 | Tree loss: 5.004 | Accuracy: 0.091797 | 6.797 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 025 | Total loss: 4.984 | Reg loss: 0.036 | Tree loss: 4.984 | Accuracy: 0.076172 | 6.799 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 009 / 025 | Total loss: 4.955 | Reg loss: 0.036 | Tree loss: 4.955 | Accuracy: 0.119141 | 6.801 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 025 | Total loss: 4.915 | Reg loss: 0.036 | Tree loss: 4.915 | Accuracy: 0.080078 | 6.802 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 025 | Total loss: 4.915 | Reg loss: 0.036 | Tree loss: 4.915 | Accuracy: 0.091797 | 6.803 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 025 | Total loss: 4.939 | Reg loss: 0.036 | Tree loss: 4.939 | Accuracy: 0.076172 | 6.803 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 025 | Total loss: 4.925 | Reg loss: 0.036 | Tree loss: 4.925 | Accuracy: 0.083984 | 6.803 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 025 | Total loss: 4.856 | Reg loss: 0.036 | Tree loss: 4.856 | Accuracy: 0.085938 | 6.804 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 025 | Total loss: 4.812 | Reg loss: 0.036 | Tree loss: 4.812 | Accuracy: 0.105469 | 6.806 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 025 | Total loss: 4.903 | Reg loss: 0.036 | Tree loss: 4.903 | Accuracy: 0.066406 | 6.806 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 025 | Total loss: 4.868 | Reg loss: 0.036 | Tree loss: 4.868 | Accuracy: 0.087891 | 6.806 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 025 | Total loss: 4.847 | Reg loss: 0.036 | Tree loss: 4.847 | Accuracy: 0.078125 | 6.806 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 025 | Total loss: 4.737 | Reg loss: 0.036 | Tree loss: 4.737 | Accuracy: 0.089844 | 6.805 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 025 | Total loss: 4.753 | Reg loss: 0.037 | Tree loss: 4.753 | Accuracy: 0.089844 | 6.805 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 025 | Total loss: 4.778 | Reg loss: 0.037 | Tree loss: 4.778 | Accuracy: 0.097656 | 6.805 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 025 | Total loss: 4.759 | Reg loss: 0.037 | Tree loss: 4.759 | Accuracy: 0.085938 | 6.804 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 025 | Total loss: 4.753 | Reg loss: 0.037 | Tree loss: 4.753 | Accuracy: 0.091797 | 6.804 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 025 | Total loss: 4.729 | Reg loss: 0.037 | Tree loss: 4.729 | Accuracy: 0.103226 | 6.801 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 025 | Total loss: 4.996 | Reg loss: 0.036 | Tree loss: 4.996 | Accuracy: 0.078125 | 6.801 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 025 | Total loss: 4.968 | Reg loss: 0.036 | Tree loss: 4.968 | Accuracy: 0.078125 | 6.8 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 025 | Total loss: 4.899 | Reg loss: 0.036 | Tree loss: 4.899 | Accuracy: 0.117188 | 6.8 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 025 | Total loss: 5.000 | Reg loss: 0.036 | Tree loss: 5.000 | Accuracy: 0.089844 | 6.799 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 025 | Total loss: 4.951 | Reg loss: 0.036 | Tree loss: 4.951 | Accuracy: 0.101562 | 6.799 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 025 | Total loss: 4.933 | Reg loss: 0.036 | Tree loss: 4.933 | Accuracy: 0.082031 | 6.798 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 025 | Total loss: 4.884 | Reg loss: 0.036 | Tree loss: 4.884 | Accuracy: 0.076172 | 6.797 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 025 | Total loss: 4.868 | Reg loss: 0.036 | Tree loss: 4.868 | Accuracy: 0.109375 | 6.797 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 025 | Total loss: 4.846 | Reg loss: 0.036 | Tree loss: 4.846 | Accuracy: 0.099609 | 6.796 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 025 | Total loss: 4.846 | Reg loss: 0.036 | Tree loss: 4.846 | Accuracy: 0.101562 | 6.795 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 025 | Total loss: 4.830 | Reg loss: 0.036 | Tree loss: 4.830 | Accuracy: 0.068359 | 6.795 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 025 | Total loss: 4.788 | Reg loss: 0.036 | Tree loss: 4.788 | Accuracy: 0.076172 | 6.794 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 025 | Total loss: 4.807 | Reg loss: 0.036 | Tree loss: 4.807 | Accuracy: 0.093750 | 6.794 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 025 | Total loss: 4.758 | Reg loss: 0.036 | Tree loss: 4.758 | Accuracy: 0.089844 | 6.796 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 025 | Total loss: 4.793 | Reg loss: 0.036 | Tree loss: 4.793 | Accuracy: 0.082031 | 6.795 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 025 | Total loss: 4.751 | Reg loss: 0.036 | Tree loss: 4.751 | Accuracy: 0.083984 | 6.795 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 025 | Total loss: 4.736 | Reg loss: 0.036 | Tree loss: 4.736 | Accuracy: 0.101562 | 6.794 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 025 | Total loss: 4.770 | Reg loss: 0.037 | Tree loss: 4.770 | Accuracy: 0.089844 | 6.794 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 025 | Total loss: 4.724 | Reg loss: 0.037 | Tree loss: 4.724 | Accuracy: 0.080078 | 6.793 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 025 | Total loss: 4.721 | Reg loss: 0.037 | Tree loss: 4.721 | Accuracy: 0.082031 | 6.793 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 025 | Total loss: 4.674 | Reg loss: 0.037 | Tree loss: 4.674 | Accuracy: 0.087891 | 6.792 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 025 | Total loss: 4.692 | Reg loss: 0.037 | Tree loss: 4.692 | Accuracy: 0.060547 | 6.792 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 025 | Total loss: 4.669 | Reg loss: 0.037 | Tree loss: 4.669 | Accuracy: 0.087891 | 6.792 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 025 | Total loss: 4.678 | Reg loss: 0.037 | Tree loss: 4.678 | Accuracy: 0.085938 | 6.792 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 025 | Total loss: 4.636 | Reg loss: 0.037 | Tree loss: 4.636 | Accuracy: 0.098925 | 6.79 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 025 | Total loss: 4.933 | Reg loss: 0.036 | Tree loss: 4.933 | Accuracy: 0.087891 | 6.789 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 025 | Total loss: 4.875 | Reg loss: 0.036 | Tree loss: 4.875 | Accuracy: 0.085938 | 6.788 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 025 | Total loss: 4.870 | Reg loss: 0.036 | Tree loss: 4.870 | Accuracy: 0.099609 | 6.788 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 025 | Total loss: 4.807 | Reg loss: 0.036 | Tree loss: 4.807 | Accuracy: 0.091797 | 6.787 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 025 | Total loss: 4.855 | Reg loss: 0.036 | Tree loss: 4.855 | Accuracy: 0.056641 | 6.787 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 025 | Total loss: 4.766 | Reg loss: 0.036 | Tree loss: 4.766 | Accuracy: 0.085938 | 6.787 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 025 | Total loss: 4.753 | Reg loss: 0.036 | Tree loss: 4.753 | Accuracy: 0.111328 | 6.786 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 025 | Total loss: 4.759 | Reg loss: 0.036 | Tree loss: 4.759 | Accuracy: 0.113281 | 6.786 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 025 | Total loss: 4.749 | Reg loss: 0.036 | Tree loss: 4.749 | Accuracy: 0.091797 | 6.786 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 025 | Total loss: 4.697 | Reg loss: 0.036 | Tree loss: 4.697 | Accuracy: 0.093750 | 6.785 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 025 | Total loss: 4.700 | Reg loss: 0.036 | Tree loss: 4.700 | Accuracy: 0.107422 | 6.785 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 025 | Total loss: 4.744 | Reg loss: 0.036 | Tree loss: 4.744 | Accuracy: 0.093750 | 6.784 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 025 | Total loss: 4.671 | Reg loss: 0.036 | Tree loss: 4.671 | Accuracy: 0.099609 | 6.784 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 025 | Total loss: 4.654 | Reg loss: 0.036 | Tree loss: 4.654 | Accuracy: 0.095703 | 6.784 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 025 | Total loss: 4.674 | Reg loss: 0.036 | Tree loss: 4.674 | Accuracy: 0.101562 | 6.783 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 025 | Total loss: 4.668 | Reg loss: 0.036 | Tree loss: 4.668 | Accuracy: 0.062500 | 6.783 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 025 | Total loss: 4.678 | Reg loss: 0.037 | Tree loss: 4.678 | Accuracy: 0.070312 | 6.782 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 025 | Total loss: 4.639 | Reg loss: 0.037 | Tree loss: 4.639 | Accuracy: 0.087891 | 6.782 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 025 | Total loss: 4.631 | Reg loss: 0.037 | Tree loss: 4.631 | Accuracy: 0.074219 | 6.781 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 025 | Total loss: 4.629 | Reg loss: 0.037 | Tree loss: 4.629 | Accuracy: 0.078125 | 6.78 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 025 | Total loss: 4.634 | Reg loss: 0.037 | Tree loss: 4.634 | Accuracy: 0.072266 | 6.78 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 021 / 025 | Total loss: 4.553 | Reg loss: 0.037 | Tree loss: 4.553 | Accuracy: 0.109375 | 6.779 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 025 | Total loss: 4.582 | Reg loss: 0.037 | Tree loss: 4.582 | Accuracy: 0.085938 | 6.779 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 025 | Total loss: 4.516 | Reg loss: 0.037 | Tree loss: 4.516 | Accuracy: 0.082031 | 6.778 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 025 | Total loss: 4.553 | Reg loss: 0.037 | Tree loss: 4.553 | Accuracy: 0.079570 | 6.775 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 025 | Total loss: 4.770 | Reg loss: 0.036 | Tree loss: 4.770 | Accuracy: 0.093750 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 025 | Total loss: 4.754 | Reg loss: 0.036 | Tree loss: 4.754 | Accuracy: 0.076172 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 025 | Total loss: 4.713 | Reg loss: 0.036 | Tree loss: 4.713 | Accuracy: 0.105469 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 025 | Total loss: 4.741 | Reg loss: 0.036 | Tree loss: 4.741 | Accuracy: 0.078125 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 025 | Total loss: 4.687 | Reg loss: 0.036 | Tree loss: 4.687 | Accuracy: 0.089844 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 025 | Total loss: 4.680 | Reg loss: 0.036 | Tree loss: 4.680 | Accuracy: 0.115234 | 6.775 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 025 | Total loss: 4.677 | Reg loss: 0.036 | Tree loss: 4.677 | Accuracy: 0.080078 | 6.776 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 025 | Total loss: 4.686 | Reg loss: 0.036 | Tree loss: 4.686 | Accuracy: 0.101562 | 6.777 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 025 | Total loss: 4.695 | Reg loss: 0.036 | Tree loss: 4.695 | Accuracy: 0.083984 | 6.777 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 025 | Total loss: 4.647 | Reg loss: 0.036 | Tree loss: 4.647 | Accuracy: 0.093750 | 6.777 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 025 | Total loss: 4.617 | Reg loss: 0.036 | Tree loss: 4.617 | Accuracy: 0.078125 | 6.778 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 025 | Total loss: 4.599 | Reg loss: 0.036 | Tree loss: 4.599 | Accuracy: 0.128906 | 6.779 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 025 | Total loss: 4.568 | Reg loss: 0.036 | Tree loss: 4.568 | Accuracy: 0.089844 | 6.778 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 025 | Total loss: 4.574 | Reg loss: 0.036 | Tree loss: 4.574 | Accuracy: 0.107422 | 6.778 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 025 | Total loss: 4.526 | Reg loss: 0.036 | Tree loss: 4.526 | Accuracy: 0.085938 | 6.777 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 025 | Total loss: 4.589 | Reg loss: 0.037 | Tree loss: 4.589 | Accuracy: 0.078125 | 6.777 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 025 | Total loss: 4.594 | Reg loss: 0.037 | Tree loss: 4.594 | Accuracy: 0.091797 | 6.776 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 025 | Total loss: 4.571 | Reg loss: 0.037 | Tree loss: 4.571 | Accuracy: 0.089844 | 6.776 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 025 | Total loss: 4.487 | Reg loss: 0.037 | Tree loss: 4.487 | Accuracy: 0.099609 | 6.775 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 025 | Total loss: 4.523 | Reg loss: 0.037 | Tree loss: 4.523 | Accuracy: 0.089844 | 6.775 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 025 | Total loss: 4.497 | Reg loss: 0.037 | Tree loss: 4.497 | Accuracy: 0.076172 | 6.775 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 025 | Total loss: 4.474 | Reg loss: 0.037 | Tree loss: 4.474 | Accuracy: 0.066406 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 025 | Total loss: 4.498 | Reg loss: 0.037 | Tree loss: 4.498 | Accuracy: 0.070312 | 6.774 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 025 | Total loss: 4.502 | Reg loss: 0.037 | Tree loss: 4.502 | Accuracy: 0.062500 | 6.773 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 025 | Total loss: 4.418 | Reg loss: 0.037 | Tree loss: 4.418 | Accuracy: 0.077419 | 6.771 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 025 | Total loss: 4.676 | Reg loss: 0.036 | Tree loss: 4.676 | Accuracy: 0.101562 | 6.771 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 025 | Total loss: 4.644 | Reg loss: 0.036 | Tree loss: 4.644 | Accuracy: 0.080078 | 6.77 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 025 | Total loss: 4.622 | Reg loss: 0.036 | Tree loss: 4.622 | Accuracy: 0.109375 | 6.77 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 025 | Total loss: 4.666 | Reg loss: 0.036 | Tree loss: 4.666 | Accuracy: 0.080078 | 6.769 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 025 | Total loss: 4.631 | Reg loss: 0.036 | Tree loss: 4.631 | Accuracy: 0.070312 | 6.768 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 025 | Total loss: 4.610 | Reg loss: 0.036 | Tree loss: 4.610 | Accuracy: 0.097656 | 6.768 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 025 | Total loss: 4.624 | Reg loss: 0.036 | Tree loss: 4.624 | Accuracy: 0.072266 | 6.768 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 025 | Total loss: 4.661 | Reg loss: 0.036 | Tree loss: 4.661 | Accuracy: 0.066406 | 6.767 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 025 | Total loss: 4.590 | Reg loss: 0.036 | Tree loss: 4.590 | Accuracy: 0.082031 | 6.767 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 025 | Total loss: 4.503 | Reg loss: 0.036 | Tree loss: 4.503 | Accuracy: 0.117188 | 6.766 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 025 | Total loss: 4.496 | Reg loss: 0.036 | Tree loss: 4.496 | Accuracy: 0.080078 | 6.766 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 025 | Total loss: 4.493 | Reg loss: 0.036 | Tree loss: 4.493 | Accuracy: 0.089844 | 6.765 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 025 | Total loss: 4.508 | Reg loss: 0.036 | Tree loss: 4.508 | Accuracy: 0.082031 | 6.764 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 025 | Total loss: 4.483 | Reg loss: 0.036 | Tree loss: 4.483 | Accuracy: 0.068359 | 6.764 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 025 | Total loss: 4.484 | Reg loss: 0.036 | Tree loss: 4.484 | Accuracy: 0.085938 | 6.764 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 025 | Total loss: 4.459 | Reg loss: 0.037 | Tree loss: 4.459 | Accuracy: 0.103516 | 6.763 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 025 | Total loss: 4.446 | Reg loss: 0.037 | Tree loss: 4.446 | Accuracy: 0.095703 | 6.763 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 025 | Total loss: 4.410 | Reg loss: 0.037 | Tree loss: 4.410 | Accuracy: 0.105469 | 6.762 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 025 | Total loss: 4.476 | Reg loss: 0.037 | Tree loss: 4.476 | Accuracy: 0.085938 | 6.761 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 025 | Total loss: 4.395 | Reg loss: 0.037 | Tree loss: 4.395 | Accuracy: 0.117188 | 6.761 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 025 | Total loss: 4.424 | Reg loss: 0.037 | Tree loss: 4.424 | Accuracy: 0.074219 | 6.76 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 025 | Total loss: 4.370 | Reg loss: 0.037 | Tree loss: 4.370 | Accuracy: 0.085938 | 6.76 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 025 | Total loss: 4.362 | Reg loss: 0.037 | Tree loss: 4.362 | Accuracy: 0.082031 | 6.759 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 025 | Total loss: 4.370 | Reg loss: 0.037 | Tree loss: 4.370 | Accuracy: 0.089844 | 6.759 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 025 | Total loss: 4.318 | Reg loss: 0.037 | Tree loss: 4.318 | Accuracy: 0.107527 | 6.756 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 025 | Total loss: 4.596 | Reg loss: 0.036 | Tree loss: 4.596 | Accuracy: 0.105469 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 025 | Total loss: 4.595 | Reg loss: 0.036 | Tree loss: 4.595 | Accuracy: 0.093750 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 025 | Total loss: 4.550 | Reg loss: 0.036 | Tree loss: 4.550 | Accuracy: 0.097656 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 025 | Total loss: 4.534 | Reg loss: 0.036 | Tree loss: 4.534 | Accuracy: 0.103516 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 025 | Total loss: 4.556 | Reg loss: 0.036 | Tree loss: 4.556 | Accuracy: 0.076172 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 025 | Total loss: 4.574 | Reg loss: 0.036 | Tree loss: 4.574 | Accuracy: 0.072266 | 6.756 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 006 / 025 | Total loss: 4.560 | Reg loss: 0.036 | Tree loss: 4.560 | Accuracy: 0.083984 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 025 | Total loss: 4.483 | Reg loss: 0.036 | Tree loss: 4.483 | Accuracy: 0.099609 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 025 | Total loss: 4.477 | Reg loss: 0.036 | Tree loss: 4.477 | Accuracy: 0.078125 | 6.756 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 025 | Total loss: 4.432 | Reg loss: 0.036 | Tree loss: 4.432 | Accuracy: 0.091797 | 6.755 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 025 | Total loss: 4.443 | Reg loss: 0.036 | Tree loss: 4.443 | Accuracy: 0.093750 | 6.755 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 025 | Total loss: 4.394 | Reg loss: 0.036 | Tree loss: 4.394 | Accuracy: 0.095703 | 6.755 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 025 | Total loss: 4.410 | Reg loss: 0.036 | Tree loss: 4.410 | Accuracy: 0.083984 | 6.755 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 025 | Total loss: 4.382 | Reg loss: 0.036 | Tree loss: 4.382 | Accuracy: 0.103516 | 6.755 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 025 | Total loss: 4.360 | Reg loss: 0.036 | Tree loss: 4.360 | Accuracy: 0.101562 | 6.754 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 025 | Total loss: 4.370 | Reg loss: 0.037 | Tree loss: 4.370 | Accuracy: 0.089844 | 6.754 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 025 | Total loss: 4.357 | Reg loss: 0.037 | Tree loss: 4.357 | Accuracy: 0.082031 | 6.754 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 025 | Total loss: 4.342 | Reg loss: 0.037 | Tree loss: 4.342 | Accuracy: 0.078125 | 6.754 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 025 | Total loss: 4.343 | Reg loss: 0.037 | Tree loss: 4.343 | Accuracy: 0.076172 | 6.753 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 025 | Total loss: 4.333 | Reg loss: 0.037 | Tree loss: 4.333 | Accuracy: 0.089844 | 6.753 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 025 | Total loss: 4.311 | Reg loss: 0.037 | Tree loss: 4.311 | Accuracy: 0.105469 | 6.753 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 025 | Total loss: 4.316 | Reg loss: 0.037 | Tree loss: 4.316 | Accuracy: 0.080078 | 6.753 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 025 | Total loss: 4.317 | Reg loss: 0.037 | Tree loss: 4.317 | Accuracy: 0.083984 | 6.752 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 025 | Total loss: 4.240 | Reg loss: 0.037 | Tree loss: 4.240 | Accuracy: 0.103516 | 6.752 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 025 | Total loss: 4.261 | Reg loss: 0.037 | Tree loss: 4.261 | Accuracy: 0.060215 | 6.75 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 025 | Total loss: 4.540 | Reg loss: 0.036 | Tree loss: 4.540 | Accuracy: 0.085938 | 6.75 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 025 | Total loss: 4.547 | Reg loss: 0.036 | Tree loss: 4.547 | Accuracy: 0.068359 | 6.749 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 025 | Total loss: 4.476 | Reg loss: 0.036 | Tree loss: 4.476 | Accuracy: 0.068359 | 6.749 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 025 | Total loss: 4.523 | Reg loss: 0.036 | Tree loss: 4.523 | Accuracy: 0.064453 | 6.749 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 025 | Total loss: 4.439 | Reg loss: 0.036 | Tree loss: 4.439 | Accuracy: 0.083984 | 6.749 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 025 | Total loss: 4.413 | Reg loss: 0.036 | Tree loss: 4.413 | Accuracy: 0.109375 | 6.749 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 025 | Total loss: 4.431 | Reg loss: 0.036 | Tree loss: 4.431 | Accuracy: 0.095703 | 6.748 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 025 | Total loss: 4.394 | Reg loss: 0.036 | Tree loss: 4.394 | Accuracy: 0.091797 | 6.748 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 025 | Total loss: 4.428 | Reg loss: 0.036 | Tree loss: 4.428 | Accuracy: 0.078125 | 6.748 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 025 | Total loss: 4.324 | Reg loss: 0.036 | Tree loss: 4.324 | Accuracy: 0.117188 | 6.747 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 025 | Total loss: 4.341 | Reg loss: 0.036 | Tree loss: 4.341 | Accuracy: 0.121094 | 6.747 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 025 | Total loss: 4.355 | Reg loss: 0.036 | Tree loss: 4.355 | Accuracy: 0.083984 | 6.747 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 025 | Total loss: 4.355 | Reg loss: 0.036 | Tree loss: 4.355 | Accuracy: 0.093750 | 6.746 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 025 | Total loss: 4.306 | Reg loss: 0.036 | Tree loss: 4.306 | Accuracy: 0.097656 | 6.746 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 025 | Total loss: 4.281 | Reg loss: 0.036 | Tree loss: 4.281 | Accuracy: 0.103516 | 6.746 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 025 | Total loss: 4.338 | Reg loss: 0.036 | Tree loss: 4.338 | Accuracy: 0.070312 | 6.746 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 025 | Total loss: 4.270 | Reg loss: 0.037 | Tree loss: 4.270 | Accuracy: 0.072266 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 025 | Total loss: 4.292 | Reg loss: 0.037 | Tree loss: 4.292 | Accuracy: 0.083984 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 025 | Total loss: 4.240 | Reg loss: 0.037 | Tree loss: 4.240 | Accuracy: 0.103516 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 025 | Total loss: 4.195 | Reg loss: 0.037 | Tree loss: 4.195 | Accuracy: 0.070312 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 025 | Total loss: 4.208 | Reg loss: 0.037 | Tree loss: 4.208 | Accuracy: 0.093750 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 025 | Total loss: 4.236 | Reg loss: 0.037 | Tree loss: 4.236 | Accuracy: 0.085938 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 025 | Total loss: 4.217 | Reg loss: 0.037 | Tree loss: 4.217 | Accuracy: 0.082031 | 6.745 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 025 | Total loss: 4.145 | Reg loss: 0.037 | Tree loss: 4.145 | Accuracy: 0.103516 | 6.746 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 025 | Total loss: 4.188 | Reg loss: 0.037 | Tree loss: 4.188 | Accuracy: 0.098925 | 6.743 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 025 | Total loss: 4.441 | Reg loss: 0.036 | Tree loss: 4.441 | Accuracy: 0.097656 | 6.744 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 025 | Total loss: 4.464 | Reg loss: 0.036 | Tree loss: 4.464 | Accuracy: 0.082031 | 6.744 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 025 | Total loss: 4.350 | Reg loss: 0.036 | Tree loss: 4.350 | Accuracy: 0.115234 | 6.743 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 025 | Total loss: 4.384 | Reg loss: 0.036 | Tree loss: 4.384 | Accuracy: 0.109375 | 6.743 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 025 | Total loss: 4.390 | Reg loss: 0.036 | Tree loss: 4.390 | Accuracy: 0.097656 | 6.743 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 025 | Total loss: 4.341 | Reg loss: 0.036 | Tree loss: 4.341 | Accuracy: 0.083984 | 6.742 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 025 | Total loss: 4.368 | Reg loss: 0.036 | Tree loss: 4.368 | Accuracy: 0.093750 | 6.742 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 025 | Total loss: 4.295 | Reg loss: 0.036 | Tree loss: 4.295 | Accuracy: 0.091797 | 6.742 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 025 | Total loss: 4.346 | Reg loss: 0.036 | Tree loss: 4.346 | Accuracy: 0.101562 | 6.742 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 025 | Total loss: 4.300 | Reg loss: 0.036 | Tree loss: 4.300 | Accuracy: 0.083984 | 6.741 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 025 | Total loss: 4.244 | Reg loss: 0.036 | Tree loss: 4.244 | Accuracy: 0.091797 | 6.742 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 025 | Total loss: 4.260 | Reg loss: 0.036 | Tree loss: 4.260 | Accuracy: 0.083984 | 6.741 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 025 | Total loss: 4.303 | Reg loss: 0.036 | Tree loss: 4.303 | Accuracy: 0.056641 | 6.741 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 025 | Total loss: 4.206 | Reg loss: 0.036 | Tree loss: 4.206 | Accuracy: 0.095703 | 6.741 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 025 | Total loss: 4.248 | Reg loss: 0.036 | Tree loss: 4.248 | Accuracy: 0.068359 | 6.74 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 025 | Total loss: 4.261 | Reg loss: 0.036 | Tree loss: 4.261 | Accuracy: 0.076172 | 6.74 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 025 | Total loss: 4.187 | Reg loss: 0.036 | Tree loss: 4.187 | Accuracy: 0.089844 | 6.74 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 025 | Total loss: 4.155 | Reg loss: 0.036 | Tree loss: 4.155 | Accuracy: 0.091797 | 6.74 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 018 / 025 | Total loss: 4.179 | Reg loss: 0.037 | Tree loss: 4.179 | Accuracy: 0.083984 | 6.739 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 025 | Total loss: 4.171 | Reg loss: 0.037 | Tree loss: 4.171 | Accuracy: 0.107422 | 6.739 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 025 | Total loss: 4.154 | Reg loss: 0.037 | Tree loss: 4.154 | Accuracy: 0.078125 | 6.739 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 025 | Total loss: 4.163 | Reg loss: 0.037 | Tree loss: 4.163 | Accuracy: 0.087891 | 6.739 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 025 | Total loss: 4.147 | Reg loss: 0.037 | Tree loss: 4.147 | Accuracy: 0.091797 | 6.738 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 025 | Total loss: 4.080 | Reg loss: 0.037 | Tree loss: 4.080 | Accuracy: 0.115234 | 6.738 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 025 | Total loss: 4.094 | Reg loss: 0.037 | Tree loss: 4.094 | Accuracy: 0.068817 | 6.736 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 025 | Total loss: 4.326 | Reg loss: 0.036 | Tree loss: 4.326 | Accuracy: 0.091797 | 6.736 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 025 | Total loss: 4.373 | Reg loss: 0.036 | Tree loss: 4.373 | Accuracy: 0.068359 | 6.736 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 025 | Total loss: 4.358 | Reg loss: 0.036 | Tree loss: 4.358 | Accuracy: 0.080078 | 6.736 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 025 | Total loss: 4.249 | Reg loss: 0.036 | Tree loss: 4.249 | Accuracy: 0.099609 | 6.735 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 025 | Total loss: 4.353 | Reg loss: 0.036 | Tree loss: 4.353 | Accuracy: 0.087891 | 6.735 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 025 | Total loss: 4.295 | Reg loss: 0.036 | Tree loss: 4.295 | Accuracy: 0.099609 | 6.735 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 025 | Total loss: 4.295 | Reg loss: 0.036 | Tree loss: 4.295 | Accuracy: 0.078125 | 6.734 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 025 | Total loss: 4.221 | Reg loss: 0.036 | Tree loss: 4.221 | Accuracy: 0.083984 | 6.734 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 025 | Total loss: 4.254 | Reg loss: 0.036 | Tree loss: 4.254 | Accuracy: 0.093750 | 6.734 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 025 | Total loss: 4.205 | Reg loss: 0.036 | Tree loss: 4.205 | Accuracy: 0.093750 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 025 | Total loss: 4.242 | Reg loss: 0.036 | Tree loss: 4.242 | Accuracy: 0.082031 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 025 | Total loss: 4.210 | Reg loss: 0.036 | Tree loss: 4.210 | Accuracy: 0.105469 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 025 | Total loss: 4.197 | Reg loss: 0.036 | Tree loss: 4.197 | Accuracy: 0.078125 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 025 | Total loss: 4.198 | Reg loss: 0.036 | Tree loss: 4.198 | Accuracy: 0.095703 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 025 | Total loss: 4.110 | Reg loss: 0.036 | Tree loss: 4.110 | Accuracy: 0.105469 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 025 | Total loss: 4.129 | Reg loss: 0.036 | Tree loss: 4.129 | Accuracy: 0.099609 | 6.733 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 025 | Total loss: 4.091 | Reg loss: 0.036 | Tree loss: 4.091 | Accuracy: 0.078125 | 6.732 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 025 | Total loss: 4.089 | Reg loss: 0.036 | Tree loss: 4.089 | Accuracy: 0.099609 | 6.732 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 025 | Total loss: 4.148 | Reg loss: 0.036 | Tree loss: 4.148 | Accuracy: 0.078125 | 6.732 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 025 | Total loss: 4.127 | Reg loss: 0.036 | Tree loss: 4.127 | Accuracy: 0.095703 | 6.732 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 025 | Total loss: 4.098 | Reg loss: 0.037 | Tree loss: 4.098 | Accuracy: 0.087891 | 6.731 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 025 | Total loss: 4.018 | Reg loss: 0.037 | Tree loss: 4.018 | Accuracy: 0.103516 | 6.731 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 025 | Total loss: 4.078 | Reg loss: 0.037 | Tree loss: 4.078 | Accuracy: 0.083984 | 6.731 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 025 | Total loss: 4.038 | Reg loss: 0.037 | Tree loss: 4.038 | Accuracy: 0.087891 | 6.731 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 025 | Total loss: 4.040 | Reg loss: 0.037 | Tree loss: 4.040 | Accuracy: 0.094624 | 6.73 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 025 | Total loss: 4.320 | Reg loss: 0.036 | Tree loss: 4.320 | Accuracy: 0.089844 | 6.729 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 025 | Total loss: 4.332 | Reg loss: 0.036 | Tree loss: 4.332 | Accuracy: 0.091797 | 6.729 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 025 | Total loss: 4.239 | Reg loss: 0.036 | Tree loss: 4.239 | Accuracy: 0.078125 | 6.729 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 025 | Total loss: 4.260 | Reg loss: 0.036 | Tree loss: 4.260 | Accuracy: 0.099609 | 6.728 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 025 | Total loss: 4.185 | Reg loss: 0.036 | Tree loss: 4.185 | Accuracy: 0.085938 | 6.728 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 025 | Total loss: 4.234 | Reg loss: 0.036 | Tree loss: 4.234 | Accuracy: 0.089844 | 6.728 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 025 | Total loss: 4.200 | Reg loss: 0.036 | Tree loss: 4.200 | Accuracy: 0.068359 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 025 | Total loss: 4.141 | Reg loss: 0.036 | Tree loss: 4.141 | Accuracy: 0.082031 | 6.728 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 025 | Total loss: 4.129 | Reg loss: 0.036 | Tree loss: 4.129 | Accuracy: 0.109375 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 025 | Total loss: 4.194 | Reg loss: 0.036 | Tree loss: 4.194 | Accuracy: 0.089844 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 025 | Total loss: 4.181 | Reg loss: 0.036 | Tree loss: 4.181 | Accuracy: 0.093750 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 025 | Total loss: 4.085 | Reg loss: 0.036 | Tree loss: 4.085 | Accuracy: 0.099609 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 025 | Total loss: 4.104 | Reg loss: 0.036 | Tree loss: 4.104 | Accuracy: 0.109375 | 6.727 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 025 | Total loss: 4.151 | Reg loss: 0.036 | Tree loss: 4.151 | Accuracy: 0.087891 | 6.726 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 025 | Total loss: 4.065 | Reg loss: 0.036 | Tree loss: 4.065 | Accuracy: 0.089844 | 6.726 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 025 | Total loss: 4.070 | Reg loss: 0.036 | Tree loss: 4.070 | Accuracy: 0.085938 | 6.726 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 025 | Total loss: 4.063 | Reg loss: 0.036 | Tree loss: 4.063 | Accuracy: 0.097656 | 6.726 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 025 | Total loss: 4.010 | Reg loss: 0.036 | Tree loss: 4.010 | Accuracy: 0.101562 | 6.726 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 025 | Total loss: 4.072 | Reg loss: 0.036 | Tree loss: 4.072 | Accuracy: 0.087891 | 6.725 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 025 | Total loss: 4.038 | Reg loss: 0.036 | Tree loss: 4.038 | Accuracy: 0.083984 | 6.725 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 025 | Total loss: 4.026 | Reg loss: 0.036 | Tree loss: 4.026 | Accuracy: 0.078125 | 6.725 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 025 | Total loss: 4.050 | Reg loss: 0.036 | Tree loss: 4.050 | Accuracy: 0.087891 | 6.725 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 025 | Total loss: 3.984 | Reg loss: 0.036 | Tree loss: 3.984 | Accuracy: 0.093750 | 6.725 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 025 | Total loss: 3.982 | Reg loss: 0.037 | Tree loss: 3.982 | Accuracy: 0.093750 | 6.724 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 025 | Total loss: 3.931 | Reg loss: 0.037 | Tree loss: 3.931 | Accuracy: 0.081720 | 6.722 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 025 | Total loss: 4.247 | Reg loss: 0.036 | Tree loss: 4.247 | Accuracy: 0.097656 | 6.722 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 025 | Total loss: 4.206 | Reg loss: 0.036 | Tree loss: 4.206 | Accuracy: 0.087891 | 6.722 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 025 | Total loss: 4.151 | Reg loss: 0.036 | Tree loss: 4.151 | Accuracy: 0.134766 | 6.722 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 003 / 025 | Total loss: 4.208 | Reg loss: 0.036 | Tree loss: 4.208 | Accuracy: 0.091797 | 6.722 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 025 | Total loss: 4.163 | Reg loss: 0.036 | Tree loss: 4.163 | Accuracy: 0.083984 | 6.721 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 025 | Total loss: 4.176 | Reg loss: 0.036 | Tree loss: 4.176 | Accuracy: 0.101562 | 6.721 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 025 | Total loss: 4.125 | Reg loss: 0.036 | Tree loss: 4.125 | Accuracy: 0.089844 | 6.721 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 025 | Total loss: 4.113 | Reg loss: 0.036 | Tree loss: 4.113 | Accuracy: 0.093750 | 6.721 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 025 | Total loss: 4.104 | Reg loss: 0.036 | Tree loss: 4.104 | Accuracy: 0.089844 | 6.72 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 025 | Total loss: 4.069 | Reg loss: 0.036 | Tree loss: 4.069 | Accuracy: 0.097656 | 6.72 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 025 | Total loss: 4.090 | Reg loss: 0.036 | Tree loss: 4.090 | Accuracy: 0.080078 | 6.72 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 025 | Total loss: 4.043 | Reg loss: 0.036 | Tree loss: 4.043 | Accuracy: 0.082031 | 6.72 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 025 | Total loss: 4.055 | Reg loss: 0.036 | Tree loss: 4.055 | Accuracy: 0.074219 | 6.72 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 025 | Total loss: 4.011 | Reg loss: 0.036 | Tree loss: 4.011 | Accuracy: 0.095703 | 6.719 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 025 | Total loss: 4.016 | Reg loss: 0.036 | Tree loss: 4.016 | Accuracy: 0.083984 | 6.719 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 025 | Total loss: 4.044 | Reg loss: 0.036 | Tree loss: 4.044 | Accuracy: 0.091797 | 6.719 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 025 | Total loss: 4.049 | Reg loss: 0.036 | Tree loss: 4.049 | Accuracy: 0.091797 | 6.719 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 025 | Total loss: 4.003 | Reg loss: 0.036 | Tree loss: 4.003 | Accuracy: 0.085938 | 6.719 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 025 | Total loss: 3.884 | Reg loss: 0.036 | Tree loss: 3.884 | Accuracy: 0.085938 | 6.718 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 025 | Total loss: 4.005 | Reg loss: 0.036 | Tree loss: 4.005 | Accuracy: 0.083984 | 6.718 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 025 | Total loss: 3.933 | Reg loss: 0.036 | Tree loss: 3.933 | Accuracy: 0.103516 | 6.718 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 025 | Total loss: 3.903 | Reg loss: 0.036 | Tree loss: 3.903 | Accuracy: 0.097656 | 6.718 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 025 | Total loss: 3.983 | Reg loss: 0.036 | Tree loss: 3.983 | Accuracy: 0.072266 | 6.718 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 025 | Total loss: 3.958 | Reg loss: 0.036 | Tree loss: 3.958 | Accuracy: 0.089844 | 6.717 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 025 | Total loss: 3.924 | Reg loss: 0.036 | Tree loss: 3.924 | Accuracy: 0.066667 | 6.715 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 025 | Total loss: 4.191 | Reg loss: 0.036 | Tree loss: 4.191 | Accuracy: 0.058594 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 025 | Total loss: 4.106 | Reg loss: 0.036 | Tree loss: 4.106 | Accuracy: 0.115234 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 025 | Total loss: 4.150 | Reg loss: 0.036 | Tree loss: 4.150 | Accuracy: 0.093750 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 025 | Total loss: 4.135 | Reg loss: 0.036 | Tree loss: 4.135 | Accuracy: 0.085938 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 025 | Total loss: 4.061 | Reg loss: 0.036 | Tree loss: 4.061 | Accuracy: 0.093750 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 025 | Total loss: 4.062 | Reg loss: 0.036 | Tree loss: 4.062 | Accuracy: 0.103516 | 6.715 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 025 | Total loss: 4.059 | Reg loss: 0.036 | Tree loss: 4.059 | Accuracy: 0.080078 | 6.714 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 025 | Total loss: 4.066 | Reg loss: 0.036 | Tree loss: 4.066 | Accuracy: 0.101562 | 6.714 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 025 | Total loss: 4.079 | Reg loss: 0.036 | Tree loss: 4.079 | Accuracy: 0.050781 | 6.714 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 025 | Total loss: 4.009 | Reg loss: 0.036 | Tree loss: 4.009 | Accuracy: 0.105469 | 6.714 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 025 | Total loss: 4.010 | Reg loss: 0.036 | Tree loss: 4.010 | Accuracy: 0.113281 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 025 | Total loss: 4.022 | Reg loss: 0.036 | Tree loss: 4.022 | Accuracy: 0.103516 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 025 | Total loss: 4.027 | Reg loss: 0.036 | Tree loss: 4.027 | Accuracy: 0.076172 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 025 | Total loss: 3.948 | Reg loss: 0.036 | Tree loss: 3.948 | Accuracy: 0.097656 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 025 | Total loss: 3.959 | Reg loss: 0.036 | Tree loss: 3.959 | Accuracy: 0.105469 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 025 | Total loss: 3.953 | Reg loss: 0.036 | Tree loss: 3.953 | Accuracy: 0.101562 | 6.713 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 025 | Total loss: 3.949 | Reg loss: 0.036 | Tree loss: 3.949 | Accuracy: 0.103516 | 6.712 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 025 | Total loss: 3.939 | Reg loss: 0.036 | Tree loss: 3.939 | Accuracy: 0.087891 | 6.712 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 025 | Total loss: 3.894 | Reg loss: 0.036 | Tree loss: 3.894 | Accuracy: 0.087891 | 6.712 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 025 | Total loss: 3.958 | Reg loss: 0.036 | Tree loss: 3.958 | Accuracy: 0.072266 | 6.712 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 025 | Total loss: 3.851 | Reg loss: 0.036 | Tree loss: 3.851 | Accuracy: 0.101562 | 6.711 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 025 | Total loss: 3.970 | Reg loss: 0.036 | Tree loss: 3.970 | Accuracy: 0.082031 | 6.711 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 025 | Total loss: 3.916 | Reg loss: 0.036 | Tree loss: 3.916 | Accuracy: 0.070312 | 6.711 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 025 | Total loss: 3.806 | Reg loss: 0.036 | Tree loss: 3.806 | Accuracy: 0.080078 | 6.711 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 025 | Total loss: 3.848 | Reg loss: 0.036 | Tree loss: 3.848 | Accuracy: 0.086022 | 6.709 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 025 | Total loss: 4.080 | Reg loss: 0.035 | Tree loss: 4.080 | Accuracy: 0.095703 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 025 | Total loss: 4.045 | Reg loss: 0.035 | Tree loss: 4.045 | Accuracy: 0.082031 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 025 | Total loss: 4.126 | Reg loss: 0.035 | Tree loss: 4.126 | Accuracy: 0.082031 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 025 | Total loss: 4.066 | Reg loss: 0.035 | Tree loss: 4.066 | Accuracy: 0.083984 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 025 | Total loss: 4.074 | Reg loss: 0.035 | Tree loss: 4.074 | Accuracy: 0.105469 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 025 | Total loss: 3.986 | Reg loss: 0.035 | Tree loss: 3.986 | Accuracy: 0.091797 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 025 | Total loss: 4.033 | Reg loss: 0.035 | Tree loss: 4.033 | Accuracy: 0.076172 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 025 | Total loss: 4.065 | Reg loss: 0.035 | Tree loss: 4.065 | Accuracy: 0.103516 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 025 | Total loss: 4.001 | Reg loss: 0.035 | Tree loss: 4.001 | Accuracy: 0.095703 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 025 | Total loss: 3.998 | Reg loss: 0.036 | Tree loss: 3.998 | Accuracy: 0.083984 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 025 | Total loss: 3.982 | Reg loss: 0.036 | Tree loss: 3.982 | Accuracy: 0.089844 | 6.708 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 025 | Total loss: 3.959 | Reg loss: 0.036 | Tree loss: 3.959 | Accuracy: 0.085938 | 6.707 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 025 | Total loss: 3.912 | Reg loss: 0.036 | Tree loss: 3.912 | Accuracy: 0.095703 | 6.707 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 025 | Total loss: 3.894 | Reg loss: 0.036 | Tree loss: 3.894 | Accuracy: 0.093750 | 6.707 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 025 | Total loss: 3.895 | Reg loss: 0.036 | Tree loss: 3.895 | Accuracy: 0.093750 | 6.707 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 015 / 025 | Total loss: 3.880 | Reg loss: 0.036 | Tree loss: 3.880 | Accuracy: 0.109375 | 6.706 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 025 | Total loss: 3.899 | Reg loss: 0.036 | Tree loss: 3.899 | Accuracy: 0.097656 | 6.706 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 025 | Total loss: 3.830 | Reg loss: 0.036 | Tree loss: 3.830 | Accuracy: 0.087891 | 6.706 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 025 | Total loss: 3.859 | Reg loss: 0.036 | Tree loss: 3.859 | Accuracy: 0.091797 | 6.706 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 025 | Total loss: 3.898 | Reg loss: 0.036 | Tree loss: 3.898 | Accuracy: 0.119141 | 6.706 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 025 | Total loss: 3.857 | Reg loss: 0.036 | Tree loss: 3.857 | Accuracy: 0.076172 | 6.705 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 025 | Total loss: 3.809 | Reg loss: 0.036 | Tree loss: 3.809 | Accuracy: 0.091797 | 6.705 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 025 | Total loss: 3.830 | Reg loss: 0.036 | Tree loss: 3.830 | Accuracy: 0.083984 | 6.705 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 025 | Total loss: 3.779 | Reg loss: 0.036 | Tree loss: 3.779 | Accuracy: 0.064453 | 6.705 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 025 | Total loss: 3.828 | Reg loss: 0.036 | Tree loss: 3.828 | Accuracy: 0.083871 | 6.702 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 025 | Total loss: 4.020 | Reg loss: 0.035 | Tree loss: 4.020 | Accuracy: 0.103516 | 6.702 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 025 | Total loss: 3.994 | Reg loss: 0.035 | Tree loss: 3.994 | Accuracy: 0.099609 | 6.702 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 025 | Total loss: 4.061 | Reg loss: 0.035 | Tree loss: 4.061 | Accuracy: 0.097656 | 6.702 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 025 | Total loss: 4.050 | Reg loss: 0.035 | Tree loss: 4.050 | Accuracy: 0.095703 | 6.701 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 025 | Total loss: 4.004 | Reg loss: 0.035 | Tree loss: 4.004 | Accuracy: 0.087891 | 6.701 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 025 | Total loss: 3.927 | Reg loss: 0.035 | Tree loss: 3.927 | Accuracy: 0.085938 | 6.701 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 025 | Total loss: 3.932 | Reg loss: 0.035 | Tree loss: 3.932 | Accuracy: 0.097656 | 6.701 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 025 | Total loss: 3.940 | Reg loss: 0.035 | Tree loss: 3.940 | Accuracy: 0.076172 | 6.7 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 025 | Total loss: 3.914 | Reg loss: 0.035 | Tree loss: 3.914 | Accuracy: 0.089844 | 6.7 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 025 | Total loss: 3.920 | Reg loss: 0.035 | Tree loss: 3.920 | Accuracy: 0.097656 | 6.7 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 025 | Total loss: 3.915 | Reg loss: 0.035 | Tree loss: 3.915 | Accuracy: 0.099609 | 6.7 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 025 | Total loss: 3.906 | Reg loss: 0.035 | Tree loss: 3.906 | Accuracy: 0.085938 | 6.7 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 025 | Total loss: 3.882 | Reg loss: 0.035 | Tree loss: 3.882 | Accuracy: 0.076172 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 025 | Total loss: 3.876 | Reg loss: 0.035 | Tree loss: 3.876 | Accuracy: 0.083984 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 025 | Total loss: 3.897 | Reg loss: 0.035 | Tree loss: 3.897 | Accuracy: 0.087891 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 025 | Total loss: 3.885 | Reg loss: 0.036 | Tree loss: 3.885 | Accuracy: 0.101562 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 025 | Total loss: 3.849 | Reg loss: 0.036 | Tree loss: 3.849 | Accuracy: 0.087891 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 025 | Total loss: 3.851 | Reg loss: 0.036 | Tree loss: 3.851 | Accuracy: 0.083984 | 6.699 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 025 | Total loss: 3.790 | Reg loss: 0.036 | Tree loss: 3.790 | Accuracy: 0.080078 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 025 | Total loss: 3.824 | Reg loss: 0.036 | Tree loss: 3.824 | Accuracy: 0.105469 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 025 | Total loss: 3.758 | Reg loss: 0.036 | Tree loss: 3.758 | Accuracy: 0.099609 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 025 | Total loss: 3.782 | Reg loss: 0.036 | Tree loss: 3.782 | Accuracy: 0.074219 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 025 | Total loss: 3.740 | Reg loss: 0.036 | Tree loss: 3.740 | Accuracy: 0.087891 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 025 | Total loss: 3.774 | Reg loss: 0.036 | Tree loss: 3.774 | Accuracy: 0.093750 | 6.698 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 025 | Total loss: 3.786 | Reg loss: 0.036 | Tree loss: 3.786 | Accuracy: 0.079570 | 6.696 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 025 | Total loss: 4.007 | Reg loss: 0.035 | Tree loss: 4.007 | Accuracy: 0.076172 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 025 | Total loss: 3.993 | Reg loss: 0.035 | Tree loss: 3.993 | Accuracy: 0.089844 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 025 | Total loss: 3.985 | Reg loss: 0.035 | Tree loss: 3.985 | Accuracy: 0.085938 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 025 | Total loss: 3.923 | Reg loss: 0.035 | Tree loss: 3.923 | Accuracy: 0.085938 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 025 | Total loss: 3.929 | Reg loss: 0.035 | Tree loss: 3.929 | Accuracy: 0.097656 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 025 | Total loss: 3.931 | Reg loss: 0.035 | Tree loss: 3.931 | Accuracy: 0.085938 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 025 | Total loss: 3.968 | Reg loss: 0.035 | Tree loss: 3.968 | Accuracy: 0.089844 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 025 | Total loss: 3.958 | Reg loss: 0.035 | Tree loss: 3.958 | Accuracy: 0.097656 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 025 | Total loss: 3.863 | Reg loss: 0.035 | Tree loss: 3.863 | Accuracy: 0.099609 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 025 | Total loss: 3.902 | Reg loss: 0.035 | Tree loss: 3.902 | Accuracy: 0.089844 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 025 | Total loss: 3.807 | Reg loss: 0.035 | Tree loss: 3.807 | Accuracy: 0.117188 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 025 | Total loss: 3.890 | Reg loss: 0.035 | Tree loss: 3.890 | Accuracy: 0.082031 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 025 | Total loss: 3.787 | Reg loss: 0.035 | Tree loss: 3.787 | Accuracy: 0.087891 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 025 | Total loss: 3.756 | Reg loss: 0.035 | Tree loss: 3.756 | Accuracy: 0.097656 | 6.693 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 025 | Total loss: 3.793 | Reg loss: 0.035 | Tree loss: 3.793 | Accuracy: 0.093750 | 6.694 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 025 | Total loss: 3.834 | Reg loss: 0.035 | Tree loss: 3.834 | Accuracy: 0.099609 | 6.699 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 025 | Total loss: 3.847 | Reg loss: 0.035 | Tree loss: 3.847 | Accuracy: 0.078125 | 6.699 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 025 | Total loss: 3.771 | Reg loss: 0.035 | Tree loss: 3.771 | Accuracy: 0.085938 | 6.698 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 025 | Total loss: 3.767 | Reg loss: 0.035 | Tree loss: 3.767 | Accuracy: 0.091797 | 6.698 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 025 | Total loss: 3.724 | Reg loss: 0.036 | Tree loss: 3.724 | Accuracy: 0.072266 | 6.697 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 025 | Total loss: 3.727 | Reg loss: 0.036 | Tree loss: 3.727 | Accuracy: 0.074219 | 6.696 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 025 | Total loss: 3.722 | Reg loss: 0.036 | Tree loss: 3.722 | Accuracy: 0.091797 | 6.696 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 025 | Total loss: 3.691 | Reg loss: 0.036 | Tree loss: 3.691 | Accuracy: 0.089844 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 025 | Total loss: 3.748 | Reg loss: 0.036 | Tree loss: 3.748 | Accuracy: 0.087891 | 6.695 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 025 | Total loss: 3.710 | Reg loss: 0.036 | Tree loss: 3.710 | Accuracy: 0.113978 | 6.693 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 000 / 025 | Total loss: 3.960 | Reg loss: 0.035 | Tree loss: 3.960 | Accuracy: 0.097656 | 6.693 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 025 | Total loss: 3.957 | Reg loss: 0.035 | Tree loss: 3.957 | Accuracy: 0.083984 | 6.692 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 025 | Total loss: 4.018 | Reg loss: 0.035 | Tree loss: 4.018 | Accuracy: 0.078125 | 6.692 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 025 | Total loss: 3.939 | Reg loss: 0.035 | Tree loss: 3.939 | Accuracy: 0.085938 | 6.691 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 025 | Total loss: 3.887 | Reg loss: 0.035 | Tree loss: 3.887 | Accuracy: 0.078125 | 6.69 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 025 | Total loss: 3.903 | Reg loss: 0.035 | Tree loss: 3.903 | Accuracy: 0.097656 | 6.69 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 025 | Total loss: 3.794 | Reg loss: 0.035 | Tree loss: 3.794 | Accuracy: 0.107422 | 6.69 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 025 | Total loss: 3.880 | Reg loss: 0.035 | Tree loss: 3.880 | Accuracy: 0.091797 | 6.689 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 025 | Total loss: 3.883 | Reg loss: 0.035 | Tree loss: 3.883 | Accuracy: 0.068359 | 6.688 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 025 | Total loss: 3.765 | Reg loss: 0.035 | Tree loss: 3.765 | Accuracy: 0.097656 | 6.688 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 025 | Total loss: 3.719 | Reg loss: 0.035 | Tree loss: 3.719 | Accuracy: 0.105469 | 6.688 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 025 | Total loss: 3.804 | Reg loss: 0.035 | Tree loss: 3.804 | Accuracy: 0.097656 | 6.687 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 025 | Total loss: 3.791 | Reg loss: 0.035 | Tree loss: 3.791 | Accuracy: 0.080078 | 6.687 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 025 | Total loss: 3.780 | Reg loss: 0.035 | Tree loss: 3.780 | Accuracy: 0.093750 | 6.686 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 025 | Total loss: 3.774 | Reg loss: 0.035 | Tree loss: 3.774 | Accuracy: 0.080078 | 6.685 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 025 | Total loss: 3.741 | Reg loss: 0.035 | Tree loss: 3.741 | Accuracy: 0.083984 | 6.685 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 025 | Total loss: 3.763 | Reg loss: 0.035 | Tree loss: 3.763 | Accuracy: 0.082031 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 025 | Total loss: 3.645 | Reg loss: 0.035 | Tree loss: 3.645 | Accuracy: 0.107422 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 025 | Total loss: 3.764 | Reg loss: 0.035 | Tree loss: 3.764 | Accuracy: 0.097656 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 025 | Total loss: 3.720 | Reg loss: 0.035 | Tree loss: 3.720 | Accuracy: 0.078125 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 025 | Total loss: 3.702 | Reg loss: 0.035 | Tree loss: 3.702 | Accuracy: 0.085938 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 025 | Total loss: 3.664 | Reg loss: 0.035 | Tree loss: 3.664 | Accuracy: 0.103516 | 6.684 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 025 | Total loss: 3.669 | Reg loss: 0.036 | Tree loss: 3.669 | Accuracy: 0.080078 | 6.683 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 025 | Total loss: 3.667 | Reg loss: 0.036 | Tree loss: 3.667 | Accuracy: 0.103516 | 6.683 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 025 | Total loss: 3.600 | Reg loss: 0.036 | Tree loss: 3.600 | Accuracy: 0.094624 | 6.681 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 025 | Total loss: 3.940 | Reg loss: 0.035 | Tree loss: 3.940 | Accuracy: 0.095703 | 6.68 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 025 | Total loss: 3.920 | Reg loss: 0.035 | Tree loss: 3.920 | Accuracy: 0.103516 | 6.68 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 025 | Total loss: 3.824 | Reg loss: 0.035 | Tree loss: 3.824 | Accuracy: 0.095703 | 6.679 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 025 | Total loss: 3.890 | Reg loss: 0.035 | Tree loss: 3.890 | Accuracy: 0.097656 | 6.679 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 025 | Total loss: 3.862 | Reg loss: 0.035 | Tree loss: 3.862 | Accuracy: 0.070312 | 6.679 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 025 | Total loss: 3.862 | Reg loss: 0.035 | Tree loss: 3.862 | Accuracy: 0.087891 | 6.678 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 025 | Total loss: 3.852 | Reg loss: 0.035 | Tree loss: 3.852 | Accuracy: 0.091797 | 6.678 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 025 | Total loss: 3.809 | Reg loss: 0.035 | Tree loss: 3.809 | Accuracy: 0.097656 | 6.677 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 025 | Total loss: 3.739 | Reg loss: 0.035 | Tree loss: 3.739 | Accuracy: 0.103516 | 6.677 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 025 | Total loss: 3.800 | Reg loss: 0.035 | Tree loss: 3.800 | Accuracy: 0.099609 | 6.676 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 025 | Total loss: 3.811 | Reg loss: 0.035 | Tree loss: 3.811 | Accuracy: 0.070312 | 6.676 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 025 | Total loss: 3.759 | Reg loss: 0.035 | Tree loss: 3.759 | Accuracy: 0.087891 | 6.675 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 025 | Total loss: 3.735 | Reg loss: 0.035 | Tree loss: 3.735 | Accuracy: 0.087891 | 6.675 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 025 | Total loss: 3.790 | Reg loss: 0.035 | Tree loss: 3.790 | Accuracy: 0.080078 | 6.675 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 025 | Total loss: 3.700 | Reg loss: 0.035 | Tree loss: 3.700 | Accuracy: 0.099609 | 6.674 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 025 | Total loss: 3.697 | Reg loss: 0.035 | Tree loss: 3.697 | Accuracy: 0.095703 | 6.674 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 025 | Total loss: 3.633 | Reg loss: 0.035 | Tree loss: 3.633 | Accuracy: 0.085938 | 6.673 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 025 | Total loss: 3.633 | Reg loss: 0.035 | Tree loss: 3.633 | Accuracy: 0.097656 | 6.673 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 025 | Total loss: 3.669 | Reg loss: 0.035 | Tree loss: 3.669 | Accuracy: 0.099609 | 6.672 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 025 | Total loss: 3.666 | Reg loss: 0.035 | Tree loss: 3.666 | Accuracy: 0.097656 | 6.672 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 025 | Total loss: 3.632 | Reg loss: 0.035 | Tree loss: 3.632 | Accuracy: 0.064453 | 6.671 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 025 | Total loss: 3.649 | Reg loss: 0.035 | Tree loss: 3.649 | Accuracy: 0.080078 | 6.671 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 025 | Total loss: 3.574 | Reg loss: 0.035 | Tree loss: 3.574 | Accuracy: 0.076172 | 6.67 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 025 | Total loss: 3.574 | Reg loss: 0.035 | Tree loss: 3.574 | Accuracy: 0.105469 | 6.67 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 025 | Total loss: 3.709 | Reg loss: 0.035 | Tree loss: 3.709 | Accuracy: 0.092473 | 6.668 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 025 | Total loss: 3.841 | Reg loss: 0.035 | Tree loss: 3.841 | Accuracy: 0.082031 | 6.668 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 025 | Total loss: 3.912 | Reg loss: 0.035 | Tree loss: 3.912 | Accuracy: 0.085938 | 6.667 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 025 | Total loss: 3.841 | Reg loss: 0.035 | Tree loss: 3.841 | Accuracy: 0.078125 | 6.667 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 025 | Total loss: 3.813 | Reg loss: 0.035 | Tree loss: 3.813 | Accuracy: 0.103516 | 6.666 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 025 | Total loss: 3.795 | Reg loss: 0.035 | Tree loss: 3.795 | Accuracy: 0.095703 | 6.666 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 025 | Total loss: 3.798 | Reg loss: 0.035 | Tree loss: 3.798 | Accuracy: 0.080078 | 6.665 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 025 | Total loss: 3.740 | Reg loss: 0.035 | Tree loss: 3.740 | Accuracy: 0.074219 | 6.665 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 025 | Total loss: 3.795 | Reg loss: 0.035 | Tree loss: 3.795 | Accuracy: 0.083984 | 6.664 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 025 | Total loss: 3.753 | Reg loss: 0.035 | Tree loss: 3.753 | Accuracy: 0.087891 | 6.664 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 025 | Total loss: 3.706 | Reg loss: 0.035 | Tree loss: 3.706 | Accuracy: 0.087891 | 6.663 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 025 | Total loss: 3.785 | Reg loss: 0.035 | Tree loss: 3.785 | Accuracy: 0.058594 | 6.663 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 025 | Total loss: 3.703 | Reg loss: 0.035 | Tree loss: 3.703 | Accuracy: 0.093750 | 6.662 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 012 / 025 | Total loss: 3.693 | Reg loss: 0.035 | Tree loss: 3.693 | Accuracy: 0.095703 | 6.662 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 025 | Total loss: 3.709 | Reg loss: 0.035 | Tree loss: 3.709 | Accuracy: 0.097656 | 6.661 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 025 | Total loss: 3.705 | Reg loss: 0.035 | Tree loss: 3.705 | Accuracy: 0.101562 | 6.661 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 025 | Total loss: 3.685 | Reg loss: 0.035 | Tree loss: 3.685 | Accuracy: 0.076172 | 6.661 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 025 | Total loss: 3.690 | Reg loss: 0.035 | Tree loss: 3.690 | Accuracy: 0.107422 | 6.66 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 025 | Total loss: 3.663 | Reg loss: 0.035 | Tree loss: 3.663 | Accuracy: 0.085938 | 6.66 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 025 | Total loss: 3.638 | Reg loss: 0.035 | Tree loss: 3.638 | Accuracy: 0.093750 | 6.659 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 025 | Total loss: 3.620 | Reg loss: 0.035 | Tree loss: 3.620 | Accuracy: 0.091797 | 6.659 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 025 | Total loss: 3.593 | Reg loss: 0.035 | Tree loss: 3.593 | Accuracy: 0.105469 | 6.658 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 025 | Total loss: 3.598 | Reg loss: 0.035 | Tree loss: 3.598 | Accuracy: 0.103516 | 6.658 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 025 | Total loss: 3.529 | Reg loss: 0.035 | Tree loss: 3.529 | Accuracy: 0.113281 | 6.657 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 025 | Total loss: 3.581 | Reg loss: 0.035 | Tree loss: 3.581 | Accuracy: 0.085938 | 6.656 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 025 | Total loss: 3.534 | Reg loss: 0.035 | Tree loss: 3.534 | Accuracy: 0.116129 | 6.654 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 025 | Total loss: 3.898 | Reg loss: 0.034 | Tree loss: 3.898 | Accuracy: 0.085938 | 6.654 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 025 | Total loss: 3.771 | Reg loss: 0.034 | Tree loss: 3.771 | Accuracy: 0.076172 | 6.653 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 025 | Total loss: 3.801 | Reg loss: 0.034 | Tree loss: 3.801 | Accuracy: 0.085938 | 6.653 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 025 | Total loss: 3.801 | Reg loss: 0.034 | Tree loss: 3.801 | Accuracy: 0.087891 | 6.652 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 025 | Total loss: 3.784 | Reg loss: 0.034 | Tree loss: 3.784 | Accuracy: 0.107422 | 6.652 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 025 | Total loss: 3.811 | Reg loss: 0.034 | Tree loss: 3.811 | Accuracy: 0.089844 | 6.652 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 025 | Total loss: 3.803 | Reg loss: 0.034 | Tree loss: 3.803 | Accuracy: 0.093750 | 6.651 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 025 | Total loss: 3.722 | Reg loss: 0.034 | Tree loss: 3.722 | Accuracy: 0.087891 | 6.651 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 025 | Total loss: 3.692 | Reg loss: 0.034 | Tree loss: 3.692 | Accuracy: 0.107422 | 6.65 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 025 | Total loss: 3.729 | Reg loss: 0.034 | Tree loss: 3.729 | Accuracy: 0.099609 | 6.65 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 025 | Total loss: 3.673 | Reg loss: 0.034 | Tree loss: 3.673 | Accuracy: 0.085938 | 6.649 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 025 | Total loss: 3.722 | Reg loss: 0.035 | Tree loss: 3.722 | Accuracy: 0.070312 | 6.649 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 025 | Total loss: 3.663 | Reg loss: 0.035 | Tree loss: 3.663 | Accuracy: 0.087891 | 6.649 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 025 | Total loss: 3.577 | Reg loss: 0.035 | Tree loss: 3.577 | Accuracy: 0.089844 | 6.648 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 025 | Total loss: 3.635 | Reg loss: 0.035 | Tree loss: 3.635 | Accuracy: 0.101562 | 6.648 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 025 | Total loss: 3.668 | Reg loss: 0.035 | Tree loss: 3.668 | Accuracy: 0.082031 | 6.647 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 025 | Total loss: 3.608 | Reg loss: 0.035 | Tree loss: 3.608 | Accuracy: 0.103516 | 6.647 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 025 | Total loss: 3.588 | Reg loss: 0.035 | Tree loss: 3.588 | Accuracy: 0.093750 | 6.646 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 025 | Total loss: 3.547 | Reg loss: 0.035 | Tree loss: 3.547 | Accuracy: 0.083984 | 6.646 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 025 | Total loss: 3.576 | Reg loss: 0.035 | Tree loss: 3.576 | Accuracy: 0.099609 | 6.645 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 025 | Total loss: 3.558 | Reg loss: 0.035 | Tree loss: 3.558 | Accuracy: 0.097656 | 6.645 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 025 | Total loss: 3.614 | Reg loss: 0.035 | Tree loss: 3.614 | Accuracy: 0.103516 | 6.645 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 025 | Total loss: 3.475 | Reg loss: 0.035 | Tree loss: 3.475 | Accuracy: 0.097656 | 6.644 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 025 | Total loss: 3.534 | Reg loss: 0.035 | Tree loss: 3.534 | Accuracy: 0.074219 | 6.644 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 025 | Total loss: 3.529 | Reg loss: 0.035 | Tree loss: 3.529 | Accuracy: 0.096774 | 6.642 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 025 | Total loss: 3.799 | Reg loss: 0.034 | Tree loss: 3.799 | Accuracy: 0.107422 | 6.642 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 025 | Total loss: 3.757 | Reg loss: 0.034 | Tree loss: 3.757 | Accuracy: 0.087891 | 6.642 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 025 | Total loss: 3.817 | Reg loss: 0.034 | Tree loss: 3.817 | Accuracy: 0.091797 | 6.641 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 025 | Total loss: 3.734 | Reg loss: 0.034 | Tree loss: 3.734 | Accuracy: 0.095703 | 6.641 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 025 | Total loss: 3.726 | Reg loss: 0.034 | Tree loss: 3.726 | Accuracy: 0.083984 | 6.641 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 025 | Total loss: 3.725 | Reg loss: 0.034 | Tree loss: 3.725 | Accuracy: 0.072266 | 6.641 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 025 | Total loss: 3.738 | Reg loss: 0.034 | Tree loss: 3.738 | Accuracy: 0.105469 | 6.64 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 025 | Total loss: 3.734 | Reg loss: 0.034 | Tree loss: 3.734 | Accuracy: 0.083984 | 6.64 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 025 | Total loss: 3.694 | Reg loss: 0.034 | Tree loss: 3.694 | Accuracy: 0.080078 | 6.639 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 025 | Total loss: 3.685 | Reg loss: 0.034 | Tree loss: 3.685 | Accuracy: 0.083984 | 6.639 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 025 | Total loss: 3.662 | Reg loss: 0.034 | Tree loss: 3.662 | Accuracy: 0.078125 | 6.638 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 025 | Total loss: 3.639 | Reg loss: 0.034 | Tree loss: 3.639 | Accuracy: 0.105469 | 6.638 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 025 | Total loss: 3.634 | Reg loss: 0.034 | Tree loss: 3.634 | Accuracy: 0.103516 | 6.637 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 025 | Total loss: 3.634 | Reg loss: 0.034 | Tree loss: 3.634 | Accuracy: 0.083984 | 6.637 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 025 | Total loss: 3.640 | Reg loss: 0.034 | Tree loss: 3.640 | Accuracy: 0.095703 | 6.637 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 025 | Total loss: 3.594 | Reg loss: 0.035 | Tree loss: 3.594 | Accuracy: 0.087891 | 6.636 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 025 | Total loss: 3.614 | Reg loss: 0.035 | Tree loss: 3.614 | Accuracy: 0.085938 | 6.636 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 025 | Total loss: 3.589 | Reg loss: 0.035 | Tree loss: 3.589 | Accuracy: 0.113281 | 6.635 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 025 | Total loss: 3.543 | Reg loss: 0.035 | Tree loss: 3.543 | Accuracy: 0.107422 | 6.635 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 025 | Total loss: 3.531 | Reg loss: 0.035 | Tree loss: 3.531 | Accuracy: 0.083984 | 6.635 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 025 | Total loss: 3.479 | Reg loss: 0.035 | Tree loss: 3.479 | Accuracy: 0.097656 | 6.635 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 025 | Total loss: 3.510 | Reg loss: 0.035 | Tree loss: 3.510 | Accuracy: 0.072266 | 6.634 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 025 | Total loss: 3.510 | Reg loss: 0.035 | Tree loss: 3.510 | Accuracy: 0.087891 | 6.634 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 025 | Total loss: 3.458 | Reg loss: 0.035 | Tree loss: 3.458 | Accuracy: 0.101562 | 6.634 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 024 / 025 | Total loss: 3.466 | Reg loss: 0.035 | Tree loss: 3.466 | Accuracy: 0.105376 | 6.632 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 025 | Total loss: 3.733 | Reg loss: 0.034 | Tree loss: 3.733 | Accuracy: 0.105469 | 6.632 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 025 | Total loss: 3.742 | Reg loss: 0.034 | Tree loss: 3.742 | Accuracy: 0.087891 | 6.631 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 025 | Total loss: 3.783 | Reg loss: 0.034 | Tree loss: 3.783 | Accuracy: 0.091797 | 6.631 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 025 | Total loss: 3.713 | Reg loss: 0.034 | Tree loss: 3.713 | Accuracy: 0.105469 | 6.631 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 025 | Total loss: 3.712 | Reg loss: 0.034 | Tree loss: 3.712 | Accuracy: 0.105469 | 6.63 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 025 | Total loss: 3.697 | Reg loss: 0.034 | Tree loss: 3.697 | Accuracy: 0.072266 | 6.63 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 025 | Total loss: 3.700 | Reg loss: 0.034 | Tree loss: 3.700 | Accuracy: 0.083984 | 6.63 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 025 | Total loss: 3.738 | Reg loss: 0.034 | Tree loss: 3.738 | Accuracy: 0.085938 | 6.629 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 025 | Total loss: 3.683 | Reg loss: 0.034 | Tree loss: 3.683 | Accuracy: 0.091797 | 6.629 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 025 | Total loss: 3.619 | Reg loss: 0.034 | Tree loss: 3.619 | Accuracy: 0.099609 | 6.628 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 025 | Total loss: 3.585 | Reg loss: 0.034 | Tree loss: 3.585 | Accuracy: 0.093750 | 6.628 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 025 | Total loss: 3.583 | Reg loss: 0.034 | Tree loss: 3.583 | Accuracy: 0.091797 | 6.629 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 025 | Total loss: 3.533 | Reg loss: 0.034 | Tree loss: 3.533 | Accuracy: 0.117188 | 6.628 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 025 | Total loss: 3.558 | Reg loss: 0.034 | Tree loss: 3.558 | Accuracy: 0.109375 | 6.628 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 025 | Total loss: 3.499 | Reg loss: 0.034 | Tree loss: 3.499 | Accuracy: 0.083984 | 6.627 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 025 | Total loss: 3.501 | Reg loss: 0.034 | Tree loss: 3.501 | Accuracy: 0.099609 | 6.627 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 025 | Total loss: 3.546 | Reg loss: 0.034 | Tree loss: 3.546 | Accuracy: 0.083984 | 6.626 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 025 | Total loss: 3.515 | Reg loss: 0.034 | Tree loss: 3.515 | Accuracy: 0.113281 | 6.626 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 025 | Total loss: 3.524 | Reg loss: 0.034 | Tree loss: 3.524 | Accuracy: 0.107422 | 6.626 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 025 | Total loss: 3.620 | Reg loss: 0.034 | Tree loss: 3.620 | Accuracy: 0.068359 | 6.625 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 025 | Total loss: 3.570 | Reg loss: 0.035 | Tree loss: 3.570 | Accuracy: 0.074219 | 6.625 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 025 | Total loss: 3.521 | Reg loss: 0.035 | Tree loss: 3.521 | Accuracy: 0.101562 | 6.625 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 025 | Total loss: 3.497 | Reg loss: 0.035 | Tree loss: 3.497 | Accuracy: 0.097656 | 6.624 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 025 | Total loss: 3.467 | Reg loss: 0.035 | Tree loss: 3.467 | Accuracy: 0.093750 | 6.624 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 025 | Total loss: 3.492 | Reg loss: 0.035 | Tree loss: 3.492 | Accuracy: 0.101075 | 6.622 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 025 | Total loss: 3.701 | Reg loss: 0.034 | Tree loss: 3.701 | Accuracy: 0.115234 | 6.621 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 025 | Total loss: 3.707 | Reg loss: 0.034 | Tree loss: 3.707 | Accuracy: 0.091797 | 6.621 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 025 | Total loss: 3.777 | Reg loss: 0.034 | Tree loss: 3.777 | Accuracy: 0.093750 | 6.621 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 025 | Total loss: 3.785 | Reg loss: 0.034 | Tree loss: 3.785 | Accuracy: 0.085938 | 6.62 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 025 | Total loss: 3.670 | Reg loss: 0.034 | Tree loss: 3.670 | Accuracy: 0.115234 | 6.62 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 025 | Total loss: 3.674 | Reg loss: 0.034 | Tree loss: 3.674 | Accuracy: 0.085938 | 6.619 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 025 | Total loss: 3.703 | Reg loss: 0.034 | Tree loss: 3.703 | Accuracy: 0.082031 | 6.619 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 025 | Total loss: 3.638 | Reg loss: 0.034 | Tree loss: 3.638 | Accuracy: 0.089844 | 6.619 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 025 | Total loss: 3.596 | Reg loss: 0.034 | Tree loss: 3.596 | Accuracy: 0.091797 | 6.619 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 025 | Total loss: 3.647 | Reg loss: 0.034 | Tree loss: 3.647 | Accuracy: 0.087891 | 6.618 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 025 | Total loss: 3.593 | Reg loss: 0.034 | Tree loss: 3.593 | Accuracy: 0.083984 | 6.618 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 025 | Total loss: 3.582 | Reg loss: 0.034 | Tree loss: 3.582 | Accuracy: 0.095703 | 6.618 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 025 | Total loss: 3.513 | Reg loss: 0.034 | Tree loss: 3.513 | Accuracy: 0.076172 | 6.617 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 025 | Total loss: 3.588 | Reg loss: 0.034 | Tree loss: 3.588 | Accuracy: 0.115234 | 6.617 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 025 | Total loss: 3.576 | Reg loss: 0.034 | Tree loss: 3.576 | Accuracy: 0.070312 | 6.616 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 025 | Total loss: 3.509 | Reg loss: 0.034 | Tree loss: 3.509 | Accuracy: 0.097656 | 6.616 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 025 | Total loss: 3.498 | Reg loss: 0.034 | Tree loss: 3.498 | Accuracy: 0.085938 | 6.616 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 025 | Total loss: 3.519 | Reg loss: 0.034 | Tree loss: 3.519 | Accuracy: 0.076172 | 6.615 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 025 | Total loss: 3.526 | Reg loss: 0.034 | Tree loss: 3.526 | Accuracy: 0.103516 | 6.615 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 025 | Total loss: 3.516 | Reg loss: 0.034 | Tree loss: 3.516 | Accuracy: 0.109375 | 6.614 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 025 | Total loss: 3.491 | Reg loss: 0.034 | Tree loss: 3.491 | Accuracy: 0.082031 | 6.614 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 025 | Total loss: 3.376 | Reg loss: 0.034 | Tree loss: 3.376 | Accuracy: 0.099609 | 6.614 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 025 | Total loss: 3.453 | Reg loss: 0.034 | Tree loss: 3.453 | Accuracy: 0.103516 | 6.613 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 025 | Total loss: 3.371 | Reg loss: 0.034 | Tree loss: 3.371 | Accuracy: 0.132812 | 6.613 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 025 | Total loss: 3.364 | Reg loss: 0.035 | Tree loss: 3.364 | Accuracy: 0.111828 | 6.611 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 025 | Total loss: 3.634 | Reg loss: 0.034 | Tree loss: 3.634 | Accuracy: 0.103516 | 6.611 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 025 | Total loss: 3.671 | Reg loss: 0.034 | Tree loss: 3.671 | Accuracy: 0.103516 | 6.61 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 025 | Total loss: 3.614 | Reg loss: 0.034 | Tree loss: 3.614 | Accuracy: 0.103516 | 6.61 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 025 | Total loss: 3.637 | Reg loss: 0.034 | Tree loss: 3.637 | Accuracy: 0.093750 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 025 | Total loss: 3.626 | Reg loss: 0.034 | Tree loss: 3.626 | Accuracy: 0.091797 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 025 | Total loss: 3.609 | Reg loss: 0.034 | Tree loss: 3.609 | Accuracy: 0.097656 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 025 | Total loss: 3.571 | Reg loss: 0.034 | Tree loss: 3.571 | Accuracy: 0.087891 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 025 | Total loss: 3.639 | Reg loss: 0.034 | Tree loss: 3.639 | Accuracy: 0.087891 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 025 | Total loss: 3.580 | Reg loss: 0.034 | Tree loss: 3.580 | Accuracy: 0.093750 | 6.607 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 009 / 025 | Total loss: 3.556 | Reg loss: 0.034 | Tree loss: 3.556 | Accuracy: 0.085938 | 6.607 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 025 | Total loss: 3.643 | Reg loss: 0.034 | Tree loss: 3.643 | Accuracy: 0.082031 | 6.607 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 025 | Total loss: 3.555 | Reg loss: 0.034 | Tree loss: 3.555 | Accuracy: 0.093750 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 025 | Total loss: 3.554 | Reg loss: 0.034 | Tree loss: 3.554 | Accuracy: 0.085938 | 6.607 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 025 | Total loss: 3.520 | Reg loss: 0.034 | Tree loss: 3.520 | Accuracy: 0.107422 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 025 | Total loss: 3.481 | Reg loss: 0.034 | Tree loss: 3.481 | Accuracy: 0.109375 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 025 | Total loss: 3.507 | Reg loss: 0.034 | Tree loss: 3.507 | Accuracy: 0.117188 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 025 | Total loss: 3.452 | Reg loss: 0.034 | Tree loss: 3.452 | Accuracy: 0.099609 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 025 | Total loss: 3.569 | Reg loss: 0.034 | Tree loss: 3.569 | Accuracy: 0.109375 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 025 | Total loss: 3.533 | Reg loss: 0.034 | Tree loss: 3.533 | Accuracy: 0.091797 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 025 | Total loss: 3.501 | Reg loss: 0.034 | Tree loss: 3.501 | Accuracy: 0.103516 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 025 | Total loss: 3.495 | Reg loss: 0.034 | Tree loss: 3.495 | Accuracy: 0.101562 | 6.608 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 025 | Total loss: 3.437 | Reg loss: 0.034 | Tree loss: 3.437 | Accuracy: 0.117188 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 025 | Total loss: 3.449 | Reg loss: 0.034 | Tree loss: 3.449 | Accuracy: 0.076172 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 025 | Total loss: 3.369 | Reg loss: 0.034 | Tree loss: 3.369 | Accuracy: 0.080078 | 6.609 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 025 | Total loss: 3.472 | Reg loss: 0.034 | Tree loss: 3.472 | Accuracy: 0.088172 | 6.608 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 025 | Total loss: 3.653 | Reg loss: 0.033 | Tree loss: 3.653 | Accuracy: 0.095703 | 6.608 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 025 | Total loss: 3.669 | Reg loss: 0.033 | Tree loss: 3.669 | Accuracy: 0.091797 | 6.608 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 025 | Total loss: 3.624 | Reg loss: 0.033 | Tree loss: 3.624 | Accuracy: 0.105469 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 025 | Total loss: 3.677 | Reg loss: 0.033 | Tree loss: 3.677 | Accuracy: 0.080078 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 025 | Total loss: 3.596 | Reg loss: 0.033 | Tree loss: 3.596 | Accuracy: 0.099609 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 025 | Total loss: 3.587 | Reg loss: 0.033 | Tree loss: 3.587 | Accuracy: 0.097656 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 025 | Total loss: 3.564 | Reg loss: 0.033 | Tree loss: 3.564 | Accuracy: 0.103516 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 025 | Total loss: 3.624 | Reg loss: 0.033 | Tree loss: 3.624 | Accuracy: 0.083984 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 025 | Total loss: 3.536 | Reg loss: 0.033 | Tree loss: 3.536 | Accuracy: 0.082031 | 6.611 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 025 | Total loss: 3.540 | Reg loss: 0.033 | Tree loss: 3.540 | Accuracy: 0.103516 | 6.611 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 025 | Total loss: 3.513 | Reg loss: 0.033 | Tree loss: 3.513 | Accuracy: 0.089844 | 6.611 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 025 | Total loss: 3.580 | Reg loss: 0.033 | Tree loss: 3.580 | Accuracy: 0.109375 | 6.612 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 025 | Total loss: 3.517 | Reg loss: 0.034 | Tree loss: 3.517 | Accuracy: 0.107422 | 6.611 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 025 | Total loss: 3.506 | Reg loss: 0.034 | Tree loss: 3.506 | Accuracy: 0.093750 | 6.611 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 025 | Total loss: 3.494 | Reg loss: 0.034 | Tree loss: 3.494 | Accuracy: 0.097656 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 025 | Total loss: 3.450 | Reg loss: 0.034 | Tree loss: 3.450 | Accuracy: 0.107422 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 025 | Total loss: 3.507 | Reg loss: 0.034 | Tree loss: 3.507 | Accuracy: 0.103516 | 6.61 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 025 | Total loss: 3.530 | Reg loss: 0.034 | Tree loss: 3.530 | Accuracy: 0.076172 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 025 | Total loss: 3.442 | Reg loss: 0.034 | Tree loss: 3.442 | Accuracy: 0.113281 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 025 | Total loss: 3.433 | Reg loss: 0.034 | Tree loss: 3.433 | Accuracy: 0.105469 | 6.609 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 025 | Total loss: 3.381 | Reg loss: 0.034 | Tree loss: 3.381 | Accuracy: 0.089844 | 6.608 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 025 | Total loss: 3.411 | Reg loss: 0.034 | Tree loss: 3.411 | Accuracy: 0.113281 | 6.608 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 025 | Total loss: 3.409 | Reg loss: 0.034 | Tree loss: 3.409 | Accuracy: 0.093750 | 6.608 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 025 | Total loss: 3.400 | Reg loss: 0.034 | Tree loss: 3.400 | Accuracy: 0.076172 | 6.607 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 025 | Total loss: 3.365 | Reg loss: 0.034 | Tree loss: 3.365 | Accuracy: 0.096774 | 6.605 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 025 | Total loss: 3.604 | Reg loss: 0.033 | Tree loss: 3.604 | Accuracy: 0.097656 | 6.605 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 025 | Total loss: 3.676 | Reg loss: 0.033 | Tree loss: 3.676 | Accuracy: 0.109375 | 6.605 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 025 | Total loss: 3.596 | Reg loss: 0.033 | Tree loss: 3.596 | Accuracy: 0.091797 | 6.604 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 025 | Total loss: 3.621 | Reg loss: 0.033 | Tree loss: 3.621 | Accuracy: 0.113281 | 6.604 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 025 | Total loss: 3.562 | Reg loss: 0.033 | Tree loss: 3.562 | Accuracy: 0.085938 | 6.604 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 025 | Total loss: 3.588 | Reg loss: 0.033 | Tree loss: 3.588 | Accuracy: 0.087891 | 6.603 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 025 | Total loss: 3.523 | Reg loss: 0.033 | Tree loss: 3.523 | Accuracy: 0.130859 | 6.603 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 025 | Total loss: 3.584 | Reg loss: 0.033 | Tree loss: 3.584 | Accuracy: 0.087891 | 6.603 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 025 | Total loss: 3.527 | Reg loss: 0.033 | Tree loss: 3.527 | Accuracy: 0.103516 | 6.602 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 025 | Total loss: 3.466 | Reg loss: 0.033 | Tree loss: 3.466 | Accuracy: 0.111328 | 6.602 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 025 | Total loss: 3.616 | Reg loss: 0.033 | Tree loss: 3.616 | Accuracy: 0.091797 | 6.602 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 025 | Total loss: 3.503 | Reg loss: 0.033 | Tree loss: 3.503 | Accuracy: 0.091797 | 6.602 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 025 | Total loss: 3.520 | Reg loss: 0.033 | Tree loss: 3.520 | Accuracy: 0.083984 | 6.601 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 025 | Total loss: 3.447 | Reg loss: 0.033 | Tree loss: 3.447 | Accuracy: 0.093750 | 6.601 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 025 | Total loss: 3.549 | Reg loss: 0.033 | Tree loss: 3.549 | Accuracy: 0.082031 | 6.601 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 025 | Total loss: 3.425 | Reg loss: 0.033 | Tree loss: 3.425 | Accuracy: 0.087891 | 6.601 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 025 | Total loss: 3.388 | Reg loss: 0.034 | Tree loss: 3.388 | Accuracy: 0.113281 | 6.6 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 025 | Total loss: 3.392 | Reg loss: 0.034 | Tree loss: 3.392 | Accuracy: 0.089844 | 6.6 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 025 | Total loss: 3.401 | Reg loss: 0.034 | Tree loss: 3.401 | Accuracy: 0.078125 | 6.6 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 025 | Total loss: 3.391 | Reg loss: 0.034 | Tree loss: 3.391 | Accuracy: 0.080078 | 6.599 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 025 | Total loss: 3.434 | Reg loss: 0.034 | Tree loss: 3.434 | Accuracy: 0.109375 | 6.599 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 021 / 025 | Total loss: 3.418 | Reg loss: 0.034 | Tree loss: 3.418 | Accuracy: 0.091797 | 6.599 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 025 | Total loss: 3.378 | Reg loss: 0.034 | Tree loss: 3.378 | Accuracy: 0.085938 | 6.598 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 025 | Total loss: 3.378 | Reg loss: 0.034 | Tree loss: 3.378 | Accuracy: 0.107422 | 6.598 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 025 | Total loss: 3.425 | Reg loss: 0.034 | Tree loss: 3.425 | Accuracy: 0.083871 | 6.596 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 025 | Total loss: 3.695 | Reg loss: 0.033 | Tree loss: 3.695 | Accuracy: 0.089844 | 6.596 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 025 | Total loss: 3.581 | Reg loss: 0.033 | Tree loss: 3.581 | Accuracy: 0.095703 | 6.596 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 025 | Total loss: 3.583 | Reg loss: 0.033 | Tree loss: 3.583 | Accuracy: 0.107422 | 6.595 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 025 | Total loss: 3.633 | Reg loss: 0.033 | Tree loss: 3.633 | Accuracy: 0.091797 | 6.595 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 025 | Total loss: 3.607 | Reg loss: 0.033 | Tree loss: 3.607 | Accuracy: 0.119141 | 6.595 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 025 | Total loss: 3.545 | Reg loss: 0.033 | Tree loss: 3.545 | Accuracy: 0.115234 | 6.594 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 025 | Total loss: 3.547 | Reg loss: 0.033 | Tree loss: 3.547 | Accuracy: 0.105469 | 6.594 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 025 | Total loss: 3.517 | Reg loss: 0.033 | Tree loss: 3.517 | Accuracy: 0.109375 | 6.594 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 025 | Total loss: 3.559 | Reg loss: 0.033 | Tree loss: 3.559 | Accuracy: 0.082031 | 6.593 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 025 | Total loss: 3.488 | Reg loss: 0.033 | Tree loss: 3.488 | Accuracy: 0.093750 | 6.593 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 025 | Total loss: 3.516 | Reg loss: 0.033 | Tree loss: 3.516 | Accuracy: 0.078125 | 6.593 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 025 | Total loss: 3.468 | Reg loss: 0.033 | Tree loss: 3.468 | Accuracy: 0.087891 | 6.592 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 025 | Total loss: 3.469 | Reg loss: 0.033 | Tree loss: 3.469 | Accuracy: 0.113281 | 6.592 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 025 | Total loss: 3.421 | Reg loss: 0.033 | Tree loss: 3.421 | Accuracy: 0.082031 | 6.591 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 025 | Total loss: 3.484 | Reg loss: 0.033 | Tree loss: 3.484 | Accuracy: 0.103516 | 6.591 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 025 | Total loss: 3.450 | Reg loss: 0.033 | Tree loss: 3.450 | Accuracy: 0.089844 | 6.591 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 025 | Total loss: 3.408 | Reg loss: 0.033 | Tree loss: 3.408 | Accuracy: 0.099609 | 6.59 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 025 | Total loss: 3.390 | Reg loss: 0.033 | Tree loss: 3.390 | Accuracy: 0.105469 | 6.59 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 025 | Total loss: 3.356 | Reg loss: 0.033 | Tree loss: 3.356 | Accuracy: 0.095703 | 6.59 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 025 | Total loss: 3.392 | Reg loss: 0.033 | Tree loss: 3.392 | Accuracy: 0.109375 | 6.589 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 025 | Total loss: 3.353 | Reg loss: 0.034 | Tree loss: 3.353 | Accuracy: 0.080078 | 6.589 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 025 | Total loss: 3.382 | Reg loss: 0.034 | Tree loss: 3.382 | Accuracy: 0.101562 | 6.589 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 025 | Total loss: 3.345 | Reg loss: 0.034 | Tree loss: 3.345 | Accuracy: 0.095703 | 6.588 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 025 | Total loss: 3.325 | Reg loss: 0.034 | Tree loss: 3.325 | Accuracy: 0.080078 | 6.588 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 025 | Total loss: 3.307 | Reg loss: 0.034 | Tree loss: 3.307 | Accuracy: 0.109677 | 6.586 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 025 | Total loss: 3.662 | Reg loss: 0.033 | Tree loss: 3.662 | Accuracy: 0.087891 | 6.586 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 025 | Total loss: 3.581 | Reg loss: 0.033 | Tree loss: 3.581 | Accuracy: 0.097656 | 6.586 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 025 | Total loss: 3.584 | Reg loss: 0.033 | Tree loss: 3.584 | Accuracy: 0.087891 | 6.586 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 025 | Total loss: 3.592 | Reg loss: 0.033 | Tree loss: 3.592 | Accuracy: 0.095703 | 6.585 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 025 | Total loss: 3.552 | Reg loss: 0.033 | Tree loss: 3.552 | Accuracy: 0.091797 | 6.585 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 025 | Total loss: 3.543 | Reg loss: 0.033 | Tree loss: 3.543 | Accuracy: 0.097656 | 6.585 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 025 | Total loss: 3.525 | Reg loss: 0.033 | Tree loss: 3.525 | Accuracy: 0.126953 | 6.584 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 025 | Total loss: 3.571 | Reg loss: 0.033 | Tree loss: 3.571 | Accuracy: 0.064453 | 6.584 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 025 | Total loss: 3.466 | Reg loss: 0.033 | Tree loss: 3.466 | Accuracy: 0.093750 | 6.584 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 025 | Total loss: 3.498 | Reg loss: 0.033 | Tree loss: 3.498 | Accuracy: 0.097656 | 6.584 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 025 | Total loss: 3.433 | Reg loss: 0.033 | Tree loss: 3.433 | Accuracy: 0.082031 | 6.583 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 025 | Total loss: 3.426 | Reg loss: 0.033 | Tree loss: 3.426 | Accuracy: 0.107422 | 6.583 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 025 | Total loss: 3.393 | Reg loss: 0.033 | Tree loss: 3.393 | Accuracy: 0.087891 | 6.582 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 025 | Total loss: 3.469 | Reg loss: 0.033 | Tree loss: 3.469 | Accuracy: 0.119141 | 6.582 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 025 | Total loss: 3.410 | Reg loss: 0.033 | Tree loss: 3.410 | Accuracy: 0.093750 | 6.582 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 025 | Total loss: 3.343 | Reg loss: 0.033 | Tree loss: 3.343 | Accuracy: 0.103516 | 6.582 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 025 | Total loss: 3.335 | Reg loss: 0.033 | Tree loss: 3.335 | Accuracy: 0.089844 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 025 | Total loss: 3.418 | Reg loss: 0.033 | Tree loss: 3.418 | Accuracy: 0.095703 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 025 | Total loss: 3.384 | Reg loss: 0.033 | Tree loss: 3.384 | Accuracy: 0.078125 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 025 | Total loss: 3.327 | Reg loss: 0.033 | Tree loss: 3.327 | Accuracy: 0.109375 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 025 | Total loss: 3.345 | Reg loss: 0.033 | Tree loss: 3.345 | Accuracy: 0.101562 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 025 | Total loss: 3.328 | Reg loss: 0.033 | Tree loss: 3.328 | Accuracy: 0.082031 | 6.581 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 025 | Total loss: 3.339 | Reg loss: 0.034 | Tree loss: 3.339 | Accuracy: 0.125000 | 6.58 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 025 | Total loss: 3.340 | Reg loss: 0.034 | Tree loss: 3.340 | Accuracy: 0.091797 | 6.58 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 025 | Total loss: 3.333 | Reg loss: 0.034 | Tree loss: 3.333 | Accuracy: 0.101075 | 6.578 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 025 | Total loss: 3.559 | Reg loss: 0.033 | Tree loss: 3.559 | Accuracy: 0.087891 | 6.578 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 025 | Total loss: 3.501 | Reg loss: 0.033 | Tree loss: 3.501 | Accuracy: 0.091797 | 6.578 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 025 | Total loss: 3.594 | Reg loss: 0.033 | Tree loss: 3.594 | Accuracy: 0.091797 | 6.577 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 025 | Total loss: 3.555 | Reg loss: 0.033 | Tree loss: 3.555 | Accuracy: 0.101562 | 6.577 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 025 | Total loss: 3.584 | Reg loss: 0.033 | Tree loss: 3.584 | Accuracy: 0.060547 | 6.577 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 025 | Total loss: 3.441 | Reg loss: 0.033 | Tree loss: 3.441 | Accuracy: 0.111328 | 6.576 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 006 / 025 | Total loss: 3.476 | Reg loss: 0.033 | Tree loss: 3.476 | Accuracy: 0.117188 | 6.576 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 025 | Total loss: 3.476 | Reg loss: 0.033 | Tree loss: 3.476 | Accuracy: 0.113281 | 6.576 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 025 | Total loss: 3.496 | Reg loss: 0.033 | Tree loss: 3.496 | Accuracy: 0.093750 | 6.576 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 025 | Total loss: 3.448 | Reg loss: 0.033 | Tree loss: 3.448 | Accuracy: 0.074219 | 6.575 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 025 | Total loss: 3.405 | Reg loss: 0.033 | Tree loss: 3.405 | Accuracy: 0.123047 | 6.575 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 025 | Total loss: 3.431 | Reg loss: 0.033 | Tree loss: 3.431 | Accuracy: 0.091797 | 6.575 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 025 | Total loss: 3.456 | Reg loss: 0.033 | Tree loss: 3.456 | Accuracy: 0.097656 | 6.574 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 025 | Total loss: 3.391 | Reg loss: 0.033 | Tree loss: 3.391 | Accuracy: 0.093750 | 6.574 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 025 | Total loss: 3.373 | Reg loss: 0.033 | Tree loss: 3.373 | Accuracy: 0.089844 | 6.574 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 025 | Total loss: 3.411 | Reg loss: 0.033 | Tree loss: 3.411 | Accuracy: 0.099609 | 6.574 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 025 | Total loss: 3.359 | Reg loss: 0.033 | Tree loss: 3.359 | Accuracy: 0.085938 | 6.573 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 025 | Total loss: 3.305 | Reg loss: 0.033 | Tree loss: 3.305 | Accuracy: 0.095703 | 6.573 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 025 | Total loss: 3.425 | Reg loss: 0.033 | Tree loss: 3.425 | Accuracy: 0.085938 | 6.573 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 025 | Total loss: 3.365 | Reg loss: 0.033 | Tree loss: 3.365 | Accuracy: 0.103516 | 6.572 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 025 | Total loss: 3.349 | Reg loss: 0.033 | Tree loss: 3.349 | Accuracy: 0.103516 | 6.572 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 025 | Total loss: 3.288 | Reg loss: 0.033 | Tree loss: 3.288 | Accuracy: 0.101562 | 6.572 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 025 | Total loss: 3.309 | Reg loss: 0.033 | Tree loss: 3.309 | Accuracy: 0.121094 | 6.572 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 025 | Total loss: 3.322 | Reg loss: 0.033 | Tree loss: 3.322 | Accuracy: 0.107422 | 6.571 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 025 | Total loss: 3.332 | Reg loss: 0.034 | Tree loss: 3.332 | Accuracy: 0.077419 | 6.57 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 025 | Total loss: 3.566 | Reg loss: 0.033 | Tree loss: 3.566 | Accuracy: 0.093750 | 6.57 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 025 | Total loss: 3.565 | Reg loss: 0.033 | Tree loss: 3.565 | Accuracy: 0.089844 | 6.57 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 025 | Total loss: 3.511 | Reg loss: 0.033 | Tree loss: 3.511 | Accuracy: 0.087891 | 6.569 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 025 | Total loss: 3.530 | Reg loss: 0.033 | Tree loss: 3.530 | Accuracy: 0.107422 | 6.569 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 025 | Total loss: 3.534 | Reg loss: 0.033 | Tree loss: 3.534 | Accuracy: 0.099609 | 6.569 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 025 | Total loss: 3.462 | Reg loss: 0.033 | Tree loss: 3.462 | Accuracy: 0.097656 | 6.568 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 025 | Total loss: 3.419 | Reg loss: 0.033 | Tree loss: 3.419 | Accuracy: 0.105469 | 6.568 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 025 | Total loss: 3.430 | Reg loss: 0.033 | Tree loss: 3.430 | Accuracy: 0.101562 | 6.568 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 025 | Total loss: 3.482 | Reg loss: 0.033 | Tree loss: 3.482 | Accuracy: 0.099609 | 6.567 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 025 | Total loss: 3.415 | Reg loss: 0.033 | Tree loss: 3.415 | Accuracy: 0.080078 | 6.567 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 025 | Total loss: 3.429 | Reg loss: 0.033 | Tree loss: 3.429 | Accuracy: 0.083984 | 6.567 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 025 | Total loss: 3.374 | Reg loss: 0.033 | Tree loss: 3.374 | Accuracy: 0.128906 | 6.567 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 025 | Total loss: 3.477 | Reg loss: 0.033 | Tree loss: 3.477 | Accuracy: 0.099609 | 6.566 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 025 | Total loss: 3.377 | Reg loss: 0.033 | Tree loss: 3.377 | Accuracy: 0.089844 | 6.566 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 025 | Total loss: 3.368 | Reg loss: 0.033 | Tree loss: 3.368 | Accuracy: 0.095703 | 6.565 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 025 | Total loss: 3.317 | Reg loss: 0.033 | Tree loss: 3.317 | Accuracy: 0.076172 | 6.565 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 025 | Total loss: 3.307 | Reg loss: 0.033 | Tree loss: 3.307 | Accuracy: 0.117188 | 6.565 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 025 | Total loss: 3.392 | Reg loss: 0.033 | Tree loss: 3.392 | Accuracy: 0.080078 | 6.565 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 025 | Total loss: 3.343 | Reg loss: 0.033 | Tree loss: 3.343 | Accuracy: 0.095703 | 6.564 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 025 | Total loss: 3.316 | Reg loss: 0.033 | Tree loss: 3.316 | Accuracy: 0.093750 | 6.564 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 025 | Total loss: 3.335 | Reg loss: 0.033 | Tree loss: 3.335 | Accuracy: 0.085938 | 6.564 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 025 | Total loss: 3.366 | Reg loss: 0.033 | Tree loss: 3.366 | Accuracy: 0.078125 | 6.563 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 025 | Total loss: 3.275 | Reg loss: 0.033 | Tree loss: 3.275 | Accuracy: 0.087891 | 6.563 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 025 | Total loss: 3.307 | Reg loss: 0.033 | Tree loss: 3.307 | Accuracy: 0.091797 | 6.563 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 025 | Total loss: 3.266 | Reg loss: 0.033 | Tree loss: 3.266 | Accuracy: 0.107527 | 6.561 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 025 | Total loss: 3.547 | Reg loss: 0.032 | Tree loss: 3.547 | Accuracy: 0.089844 | 6.561 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 025 | Total loss: 3.549 | Reg loss: 0.032 | Tree loss: 3.549 | Accuracy: 0.087891 | 6.561 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 025 | Total loss: 3.521 | Reg loss: 0.032 | Tree loss: 3.521 | Accuracy: 0.089844 | 6.56 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 025 | Total loss: 3.528 | Reg loss: 0.032 | Tree loss: 3.528 | Accuracy: 0.076172 | 6.56 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 025 | Total loss: 3.494 | Reg loss: 0.032 | Tree loss: 3.494 | Accuracy: 0.101562 | 6.56 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 025 | Total loss: 3.498 | Reg loss: 0.032 | Tree loss: 3.498 | Accuracy: 0.111328 | 6.56 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 025 | Total loss: 3.477 | Reg loss: 0.032 | Tree loss: 3.477 | Accuracy: 0.082031 | 6.559 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 025 | Total loss: 3.458 | Reg loss: 0.033 | Tree loss: 3.458 | Accuracy: 0.113281 | 6.559 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 025 | Total loss: 3.419 | Reg loss: 0.033 | Tree loss: 3.419 | Accuracy: 0.082031 | 6.559 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 025 | Total loss: 3.448 | Reg loss: 0.033 | Tree loss: 3.448 | Accuracy: 0.089844 | 6.558 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 025 | Total loss: 3.406 | Reg loss: 0.033 | Tree loss: 3.406 | Accuracy: 0.082031 | 6.558 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 025 | Total loss: 3.378 | Reg loss: 0.033 | Tree loss: 3.378 | Accuracy: 0.109375 | 6.558 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 025 | Total loss: 3.332 | Reg loss: 0.033 | Tree loss: 3.332 | Accuracy: 0.105469 | 6.557 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 025 | Total loss: 3.325 | Reg loss: 0.033 | Tree loss: 3.325 | Accuracy: 0.113281 | 6.557 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 025 | Total loss: 3.279 | Reg loss: 0.033 | Tree loss: 3.279 | Accuracy: 0.111328 | 6.557 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 025 | Total loss: 3.324 | Reg loss: 0.033 | Tree loss: 3.324 | Accuracy: 0.082031 | 6.556 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 025 | Total loss: 3.290 | Reg loss: 0.033 | Tree loss: 3.290 | Accuracy: 0.078125 | 6.556 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 025 | Total loss: 3.380 | Reg loss: 0.033 | Tree loss: 3.380 | Accuracy: 0.107422 | 6.556 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 018 / 025 | Total loss: 3.277 | Reg loss: 0.033 | Tree loss: 3.277 | Accuracy: 0.087891 | 6.555 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 025 | Total loss: 3.383 | Reg loss: 0.033 | Tree loss: 3.383 | Accuracy: 0.083984 | 6.555 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 025 | Total loss: 3.269 | Reg loss: 0.033 | Tree loss: 3.269 | Accuracy: 0.105469 | 6.555 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 025 | Total loss: 3.334 | Reg loss: 0.033 | Tree loss: 3.334 | Accuracy: 0.101562 | 6.554 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 025 | Total loss: 3.249 | Reg loss: 0.033 | Tree loss: 3.249 | Accuracy: 0.083984 | 6.554 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 025 | Total loss: 3.256 | Reg loss: 0.033 | Tree loss: 3.256 | Accuracy: 0.107422 | 6.554 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 025 | Total loss: 3.288 | Reg loss: 0.033 | Tree loss: 3.288 | Accuracy: 0.068817 | 6.553 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 025 | Total loss: 3.514 | Reg loss: 0.032 | Tree loss: 3.514 | Accuracy: 0.123047 | 6.553 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 025 | Total loss: 3.508 | Reg loss: 0.032 | Tree loss: 3.508 | Accuracy: 0.099609 | 6.553 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 025 | Total loss: 3.423 | Reg loss: 0.032 | Tree loss: 3.423 | Accuracy: 0.091797 | 6.553 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 025 | Total loss: 3.481 | Reg loss: 0.032 | Tree loss: 3.481 | Accuracy: 0.080078 | 6.552 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 025 | Total loss: 3.453 | Reg loss: 0.032 | Tree loss: 3.453 | Accuracy: 0.087891 | 6.552 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 025 | Total loss: 3.456 | Reg loss: 0.032 | Tree loss: 3.456 | Accuracy: 0.099609 | 6.552 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 025 | Total loss: 3.367 | Reg loss: 0.032 | Tree loss: 3.367 | Accuracy: 0.093750 | 6.551 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 025 | Total loss: 3.408 | Reg loss: 0.032 | Tree loss: 3.408 | Accuracy: 0.078125 | 6.551 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 025 | Total loss: 3.365 | Reg loss: 0.033 | Tree loss: 3.365 | Accuracy: 0.091797 | 6.551 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 025 | Total loss: 3.307 | Reg loss: 0.033 | Tree loss: 3.307 | Accuracy: 0.085938 | 6.551 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 025 | Total loss: 3.424 | Reg loss: 0.033 | Tree loss: 3.424 | Accuracy: 0.105469 | 6.55 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 025 | Total loss: 3.354 | Reg loss: 0.033 | Tree loss: 3.354 | Accuracy: 0.111328 | 6.55 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 025 | Total loss: 3.315 | Reg loss: 0.033 | Tree loss: 3.315 | Accuracy: 0.099609 | 6.55 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 025 | Total loss: 3.359 | Reg loss: 0.033 | Tree loss: 3.359 | Accuracy: 0.103516 | 6.549 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 025 | Total loss: 3.348 | Reg loss: 0.033 | Tree loss: 3.348 | Accuracy: 0.076172 | 6.549 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 025 | Total loss: 3.235 | Reg loss: 0.033 | Tree loss: 3.235 | Accuracy: 0.097656 | 6.549 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 025 | Total loss: 3.283 | Reg loss: 0.033 | Tree loss: 3.283 | Accuracy: 0.097656 | 6.548 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 025 | Total loss: 3.234 | Reg loss: 0.033 | Tree loss: 3.234 | Accuracy: 0.123047 | 6.548 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 025 | Total loss: 3.277 | Reg loss: 0.033 | Tree loss: 3.277 | Accuracy: 0.097656 | 6.548 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 025 | Total loss: 3.310 | Reg loss: 0.033 | Tree loss: 3.310 | Accuracy: 0.097656 | 6.548 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 025 | Total loss: 3.330 | Reg loss: 0.033 | Tree loss: 3.330 | Accuracy: 0.080078 | 6.548 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 025 | Total loss: 3.226 | Reg loss: 0.033 | Tree loss: 3.226 | Accuracy: 0.095703 | 6.547 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 025 | Total loss: 3.234 | Reg loss: 0.033 | Tree loss: 3.234 | Accuracy: 0.103516 | 6.547 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 025 | Total loss: 3.239 | Reg loss: 0.033 | Tree loss: 3.239 | Accuracy: 0.070312 | 6.547 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 025 | Total loss: 3.245 | Reg loss: 0.033 | Tree loss: 3.245 | Accuracy: 0.118280 | 6.545 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 025 | Total loss: 3.461 | Reg loss: 0.032 | Tree loss: 3.461 | Accuracy: 0.074219 | 6.545 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 025 | Total loss: 3.494 | Reg loss: 0.032 | Tree loss: 3.494 | Accuracy: 0.101562 | 6.545 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 025 | Total loss: 3.444 | Reg loss: 0.032 | Tree loss: 3.444 | Accuracy: 0.099609 | 6.544 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 025 | Total loss: 3.436 | Reg loss: 0.032 | Tree loss: 3.436 | Accuracy: 0.111328 | 6.544 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 025 | Total loss: 3.419 | Reg loss: 0.032 | Tree loss: 3.419 | Accuracy: 0.105469 | 6.544 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 025 | Total loss: 3.402 | Reg loss: 0.032 | Tree loss: 3.402 | Accuracy: 0.093750 | 6.544 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 025 | Total loss: 3.341 | Reg loss: 0.032 | Tree loss: 3.341 | Accuracy: 0.103516 | 6.544 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 025 | Total loss: 3.347 | Reg loss: 0.032 | Tree loss: 3.347 | Accuracy: 0.095703 | 6.543 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 025 | Total loss: 3.299 | Reg loss: 0.032 | Tree loss: 3.299 | Accuracy: 0.091797 | 6.543 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 025 | Total loss: 3.305 | Reg loss: 0.032 | Tree loss: 3.305 | Accuracy: 0.099609 | 6.543 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 025 | Total loss: 3.310 | Reg loss: 0.033 | Tree loss: 3.310 | Accuracy: 0.097656 | 6.543 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 025 | Total loss: 3.310 | Reg loss: 0.033 | Tree loss: 3.310 | Accuracy: 0.072266 | 6.542 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 025 | Total loss: 3.283 | Reg loss: 0.033 | Tree loss: 3.283 | Accuracy: 0.089844 | 6.542 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 025 | Total loss: 3.301 | Reg loss: 0.033 | Tree loss: 3.301 | Accuracy: 0.107422 | 6.542 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 025 | Total loss: 3.322 | Reg loss: 0.033 | Tree loss: 3.322 | Accuracy: 0.101562 | 6.542 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 025 | Total loss: 3.270 | Reg loss: 0.033 | Tree loss: 3.270 | Accuracy: 0.097656 | 6.541 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 025 | Total loss: 3.197 | Reg loss: 0.033 | Tree loss: 3.197 | Accuracy: 0.125000 | 6.541 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 025 | Total loss: 3.244 | Reg loss: 0.033 | Tree loss: 3.244 | Accuracy: 0.082031 | 6.541 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 025 | Total loss: 3.249 | Reg loss: 0.033 | Tree loss: 3.249 | Accuracy: 0.091797 | 6.541 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 025 | Total loss: 3.279 | Reg loss: 0.033 | Tree loss: 3.279 | Accuracy: 0.085938 | 6.54 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 025 | Total loss: 3.207 | Reg loss: 0.033 | Tree loss: 3.207 | Accuracy: 0.083984 | 6.54 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 025 | Total loss: 3.211 | Reg loss: 0.033 | Tree loss: 3.211 | Accuracy: 0.103516 | 6.54 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 025 | Total loss: 3.270 | Reg loss: 0.033 | Tree loss: 3.270 | Accuracy: 0.097656 | 6.54 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 025 | Total loss: 3.201 | Reg loss: 0.033 | Tree loss: 3.201 | Accuracy: 0.097656 | 6.539 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 025 | Total loss: 3.238 | Reg loss: 0.033 | Tree loss: 3.238 | Accuracy: 0.083871 | 6.538 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 025 | Total loss: 3.438 | Reg loss: 0.032 | Tree loss: 3.438 | Accuracy: 0.085938 | 6.538 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 025 | Total loss: 3.351 | Reg loss: 0.032 | Tree loss: 3.351 | Accuracy: 0.099609 | 6.537 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 025 | Total loss: 3.374 | Reg loss: 0.032 | Tree loss: 3.374 | Accuracy: 0.087891 | 6.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 003 / 025 | Total loss: 3.376 | Reg loss: 0.032 | Tree loss: 3.376 | Accuracy: 0.111328 | 6.537 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 025 | Total loss: 3.269 | Reg loss: 0.032 | Tree loss: 3.269 | Accuracy: 0.111328 | 6.536 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 025 | Total loss: 3.292 | Reg loss: 0.032 | Tree loss: 3.292 | Accuracy: 0.119141 | 6.536 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 025 | Total loss: 3.317 | Reg loss: 0.032 | Tree loss: 3.317 | Accuracy: 0.109375 | 6.536 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 025 | Total loss: 3.368 | Reg loss: 0.032 | Tree loss: 3.368 | Accuracy: 0.103516 | 6.536 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 025 | Total loss: 3.362 | Reg loss: 0.032 | Tree loss: 3.362 | Accuracy: 0.087891 | 6.535 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 025 | Total loss: 3.318 | Reg loss: 0.032 | Tree loss: 3.318 | Accuracy: 0.099609 | 6.535 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 025 | Total loss: 3.290 | Reg loss: 0.032 | Tree loss: 3.290 | Accuracy: 0.115234 | 6.535 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 025 | Total loss: 3.291 | Reg loss: 0.032 | Tree loss: 3.291 | Accuracy: 0.087891 | 6.535 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 025 | Total loss: 3.237 | Reg loss: 0.032 | Tree loss: 3.237 | Accuracy: 0.097656 | 6.535 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 025 | Total loss: 3.287 | Reg loss: 0.033 | Tree loss: 3.287 | Accuracy: 0.109375 | 6.534 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 025 | Total loss: 3.265 | Reg loss: 0.033 | Tree loss: 3.265 | Accuracy: 0.109375 | 6.534 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 025 | Total loss: 3.191 | Reg loss: 0.033 | Tree loss: 3.191 | Accuracy: 0.119141 | 6.534 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 025 | Total loss: 3.262 | Reg loss: 0.033 | Tree loss: 3.262 | Accuracy: 0.089844 | 6.533 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 025 | Total loss: 3.304 | Reg loss: 0.033 | Tree loss: 3.304 | Accuracy: 0.076172 | 6.533 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 025 | Total loss: 3.315 | Reg loss: 0.033 | Tree loss: 3.315 | Accuracy: 0.082031 | 6.533 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 025 | Total loss: 3.198 | Reg loss: 0.033 | Tree loss: 3.198 | Accuracy: 0.082031 | 6.532 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 025 | Total loss: 3.241 | Reg loss: 0.033 | Tree loss: 3.241 | Accuracy: 0.087891 | 6.532 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 025 | Total loss: 3.167 | Reg loss: 0.033 | Tree loss: 3.167 | Accuracy: 0.078125 | 6.532 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 025 | Total loss: 3.199 | Reg loss: 0.033 | Tree loss: 3.199 | Accuracy: 0.099609 | 6.532 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 025 | Total loss: 3.168 | Reg loss: 0.033 | Tree loss: 3.168 | Accuracy: 0.080078 | 6.531 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 025 | Total loss: 3.163 | Reg loss: 0.033 | Tree loss: 3.163 | Accuracy: 0.094624 | 6.53 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 025 | Total loss: 3.386 | Reg loss: 0.032 | Tree loss: 3.386 | Accuracy: 0.083984 | 6.53 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 025 | Total loss: 3.434 | Reg loss: 0.032 | Tree loss: 3.434 | Accuracy: 0.093750 | 6.53 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 025 | Total loss: 3.372 | Reg loss: 0.032 | Tree loss: 3.372 | Accuracy: 0.087891 | 6.529 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 025 | Total loss: 3.319 | Reg loss: 0.032 | Tree loss: 3.319 | Accuracy: 0.107422 | 6.529 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 025 | Total loss: 3.360 | Reg loss: 0.032 | Tree loss: 3.360 | Accuracy: 0.113281 | 6.529 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 025 | Total loss: 3.306 | Reg loss: 0.032 | Tree loss: 3.306 | Accuracy: 0.115234 | 6.529 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 025 | Total loss: 3.289 | Reg loss: 0.032 | Tree loss: 3.289 | Accuracy: 0.103516 | 6.529 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 025 | Total loss: 3.313 | Reg loss: 0.032 | Tree loss: 3.313 | Accuracy: 0.082031 | 6.528 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 025 | Total loss: 3.304 | Reg loss: 0.032 | Tree loss: 3.304 | Accuracy: 0.111328 | 6.528 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 025 | Total loss: 3.316 | Reg loss: 0.032 | Tree loss: 3.316 | Accuracy: 0.095703 | 6.528 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 025 | Total loss: 3.254 | Reg loss: 0.032 | Tree loss: 3.254 | Accuracy: 0.082031 | 6.527 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 025 | Total loss: 3.220 | Reg loss: 0.032 | Tree loss: 3.220 | Accuracy: 0.097656 | 6.527 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 025 | Total loss: 3.282 | Reg loss: 0.032 | Tree loss: 3.282 | Accuracy: 0.085938 | 6.527 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 025 | Total loss: 3.236 | Reg loss: 0.032 | Tree loss: 3.236 | Accuracy: 0.109375 | 6.527 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 025 | Total loss: 3.180 | Reg loss: 0.032 | Tree loss: 3.180 | Accuracy: 0.093750 | 6.526 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 025 | Total loss: 3.187 | Reg loss: 0.032 | Tree loss: 3.187 | Accuracy: 0.105469 | 6.526 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 025 | Total loss: 3.202 | Reg loss: 0.033 | Tree loss: 3.202 | Accuracy: 0.087891 | 6.526 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 025 | Total loss: 3.224 | Reg loss: 0.033 | Tree loss: 3.224 | Accuracy: 0.097656 | 6.525 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 025 | Total loss: 3.163 | Reg loss: 0.033 | Tree loss: 3.163 | Accuracy: 0.119141 | 6.525 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 025 | Total loss: 3.189 | Reg loss: 0.033 | Tree loss: 3.189 | Accuracy: 0.105469 | 6.525 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 025 | Total loss: 3.176 | Reg loss: 0.033 | Tree loss: 3.176 | Accuracy: 0.091797 | 6.525 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 025 | Total loss: 3.101 | Reg loss: 0.033 | Tree loss: 3.101 | Accuracy: 0.087891 | 6.524 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 025 | Total loss: 3.137 | Reg loss: 0.033 | Tree loss: 3.137 | Accuracy: 0.066406 | 6.524 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 025 | Total loss: 3.127 | Reg loss: 0.033 | Tree loss: 3.127 | Accuracy: 0.103516 | 6.524 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 025 | Total loss: 3.248 | Reg loss: 0.033 | Tree loss: 3.248 | Accuracy: 0.075269 | 6.523 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 025 | Total loss: 3.310 | Reg loss: 0.032 | Tree loss: 3.310 | Accuracy: 0.097656 | 6.523 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 025 | Total loss: 3.235 | Reg loss: 0.032 | Tree loss: 3.235 | Accuracy: 0.121094 | 6.522 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 025 | Total loss: 3.360 | Reg loss: 0.032 | Tree loss: 3.360 | Accuracy: 0.101562 | 6.522 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 025 | Total loss: 3.282 | Reg loss: 0.032 | Tree loss: 3.282 | Accuracy: 0.074219 | 6.522 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 025 | Total loss: 3.351 | Reg loss: 0.032 | Tree loss: 3.351 | Accuracy: 0.105469 | 6.522 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 025 | Total loss: 3.294 | Reg loss: 0.032 | Tree loss: 3.294 | Accuracy: 0.093750 | 6.521 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 025 | Total loss: 3.298 | Reg loss: 0.032 | Tree loss: 3.298 | Accuracy: 0.091797 | 6.521 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 025 | Total loss: 3.252 | Reg loss: 0.032 | Tree loss: 3.252 | Accuracy: 0.105469 | 6.521 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 025 | Total loss: 3.286 | Reg loss: 0.032 | Tree loss: 3.286 | Accuracy: 0.087891 | 6.521 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 025 | Total loss: 3.281 | Reg loss: 0.032 | Tree loss: 3.281 | Accuracy: 0.099609 | 6.521 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 025 | Total loss: 3.268 | Reg loss: 0.032 | Tree loss: 3.268 | Accuracy: 0.093750 | 6.52 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 025 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.107422 | 6.52 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 025 | Total loss: 3.250 | Reg loss: 0.032 | Tree loss: 3.250 | Accuracy: 0.103516 | 6.52 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 025 | Total loss: 3.219 | Reg loss: 0.032 | Tree loss: 3.219 | Accuracy: 0.107422 | 6.52 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 025 | Total loss: 3.235 | Reg loss: 0.032 | Tree loss: 3.235 | Accuracy: 0.093750 | 6.52 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 015 / 025 | Total loss: 3.238 | Reg loss: 0.032 | Tree loss: 3.238 | Accuracy: 0.091797 | 6.519 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 025 | Total loss: 3.175 | Reg loss: 0.032 | Tree loss: 3.175 | Accuracy: 0.113281 | 6.519 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 025 | Total loss: 3.171 | Reg loss: 0.032 | Tree loss: 3.171 | Accuracy: 0.082031 | 6.519 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 025 | Total loss: 3.260 | Reg loss: 0.032 | Tree loss: 3.260 | Accuracy: 0.087891 | 6.519 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 025 | Total loss: 3.151 | Reg loss: 0.032 | Tree loss: 3.151 | Accuracy: 0.087891 | 6.519 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 025 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.113281 | 6.518 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 025 | Total loss: 3.146 | Reg loss: 0.033 | Tree loss: 3.146 | Accuracy: 0.121094 | 6.518 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 025 | Total loss: 3.055 | Reg loss: 0.033 | Tree loss: 3.055 | Accuracy: 0.119141 | 6.518 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 025 | Total loss: 3.152 | Reg loss: 0.033 | Tree loss: 3.152 | Accuracy: 0.089844 | 6.518 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 025 | Total loss: 3.088 | Reg loss: 0.033 | Tree loss: 3.088 | Accuracy: 0.073118 | 6.516 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 025 | Total loss: 3.310 | Reg loss: 0.032 | Tree loss: 3.310 | Accuracy: 0.109375 | 6.516 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 025 | Total loss: 3.281 | Reg loss: 0.032 | Tree loss: 3.281 | Accuracy: 0.117188 | 6.516 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 025 | Total loss: 3.315 | Reg loss: 0.032 | Tree loss: 3.315 | Accuracy: 0.117188 | 6.516 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 025 | Total loss: 3.286 | Reg loss: 0.032 | Tree loss: 3.286 | Accuracy: 0.093750 | 6.515 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 025 | Total loss: 3.277 | Reg loss: 0.032 | Tree loss: 3.277 | Accuracy: 0.103516 | 6.515 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 025 | Total loss: 3.314 | Reg loss: 0.032 | Tree loss: 3.314 | Accuracy: 0.128906 | 6.515 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 025 | Total loss: 3.287 | Reg loss: 0.032 | Tree loss: 3.287 | Accuracy: 0.074219 | 6.515 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 025 | Total loss: 3.253 | Reg loss: 0.032 | Tree loss: 3.253 | Accuracy: 0.115234 | 6.514 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 025 | Total loss: 3.212 | Reg loss: 0.032 | Tree loss: 3.212 | Accuracy: 0.085938 | 6.514 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 025 | Total loss: 3.232 | Reg loss: 0.032 | Tree loss: 3.232 | Accuracy: 0.083984 | 6.514 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 025 | Total loss: 3.175 | Reg loss: 0.032 | Tree loss: 3.175 | Accuracy: 0.113281 | 6.514 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 025 | Total loss: 3.205 | Reg loss: 0.032 | Tree loss: 3.205 | Accuracy: 0.113281 | 6.514 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 025 | Total loss: 3.237 | Reg loss: 0.032 | Tree loss: 3.237 | Accuracy: 0.093750 | 6.513 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 025 | Total loss: 3.243 | Reg loss: 0.032 | Tree loss: 3.243 | Accuracy: 0.093750 | 6.513 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 025 | Total loss: 3.182 | Reg loss: 0.032 | Tree loss: 3.182 | Accuracy: 0.093750 | 6.513 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 025 | Total loss: 3.177 | Reg loss: 0.032 | Tree loss: 3.177 | Accuracy: 0.082031 | 6.513 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 025 | Total loss: 3.135 | Reg loss: 0.032 | Tree loss: 3.135 | Accuracy: 0.089844 | 6.512 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 025 | Total loss: 3.120 | Reg loss: 0.032 | Tree loss: 3.120 | Accuracy: 0.095703 | 6.512 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 025 | Total loss: 3.184 | Reg loss: 0.032 | Tree loss: 3.184 | Accuracy: 0.080078 | 6.512 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 025 | Total loss: 3.100 | Reg loss: 0.032 | Tree loss: 3.100 | Accuracy: 0.123047 | 6.512 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 025 | Total loss: 3.107 | Reg loss: 0.032 | Tree loss: 3.107 | Accuracy: 0.082031 | 6.511 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 025 | Total loss: 3.121 | Reg loss: 0.032 | Tree loss: 3.121 | Accuracy: 0.095703 | 6.511 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 025 | Total loss: 3.091 | Reg loss: 0.032 | Tree loss: 3.091 | Accuracy: 0.091797 | 6.511 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 025 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.101562 | 6.511 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 025 | Total loss: 3.113 | Reg loss: 0.032 | Tree loss: 3.113 | Accuracy: 0.109677 | 6.509 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 025 | Total loss: 3.257 | Reg loss: 0.032 | Tree loss: 3.257 | Accuracy: 0.121094 | 6.509 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 025 | Total loss: 3.249 | Reg loss: 0.032 | Tree loss: 3.249 | Accuracy: 0.132812 | 6.509 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 025 | Total loss: 3.251 | Reg loss: 0.032 | Tree loss: 3.251 | Accuracy: 0.085938 | 6.508 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 025 | Total loss: 3.306 | Reg loss: 0.032 | Tree loss: 3.306 | Accuracy: 0.097656 | 6.508 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 025 | Total loss: 3.270 | Reg loss: 0.032 | Tree loss: 3.270 | Accuracy: 0.087891 | 6.508 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 025 | Total loss: 3.228 | Reg loss: 0.032 | Tree loss: 3.228 | Accuracy: 0.113281 | 6.508 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 025 | Total loss: 3.287 | Reg loss: 0.032 | Tree loss: 3.287 | Accuracy: 0.085938 | 6.508 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 025 | Total loss: 3.249 | Reg loss: 0.032 | Tree loss: 3.249 | Accuracy: 0.083984 | 6.507 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 025 | Total loss: 3.215 | Reg loss: 0.032 | Tree loss: 3.215 | Accuracy: 0.111328 | 6.507 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 025 | Total loss: 3.219 | Reg loss: 0.032 | Tree loss: 3.219 | Accuracy: 0.113281 | 6.507 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 025 | Total loss: 3.157 | Reg loss: 0.032 | Tree loss: 3.157 | Accuracy: 0.103516 | 6.507 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 025 | Total loss: 3.132 | Reg loss: 0.032 | Tree loss: 3.132 | Accuracy: 0.085938 | 6.506 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 025 | Total loss: 3.179 | Reg loss: 0.032 | Tree loss: 3.179 | Accuracy: 0.101562 | 6.506 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 025 | Total loss: 3.106 | Reg loss: 0.032 | Tree loss: 3.106 | Accuracy: 0.126953 | 6.506 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 025 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.113281 | 6.506 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 025 | Total loss: 3.153 | Reg loss: 0.032 | Tree loss: 3.153 | Accuracy: 0.113281 | 6.505 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 025 | Total loss: 3.129 | Reg loss: 0.032 | Tree loss: 3.129 | Accuracy: 0.095703 | 6.505 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 025 | Total loss: 3.137 | Reg loss: 0.032 | Tree loss: 3.137 | Accuracy: 0.089844 | 6.505 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 025 | Total loss: 3.206 | Reg loss: 0.032 | Tree loss: 3.206 | Accuracy: 0.087891 | 6.504 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 025 | Total loss: 3.127 | Reg loss: 0.032 | Tree loss: 3.127 | Accuracy: 0.119141 | 6.504 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 025 | Total loss: 3.109 | Reg loss: 0.032 | Tree loss: 3.109 | Accuracy: 0.080078 | 6.504 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 025 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.095703 | 6.504 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 025 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.103516 | 6.504 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 025 | Total loss: 3.165 | Reg loss: 0.032 | Tree loss: 3.165 | Accuracy: 0.068359 | 6.503 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 025 | Total loss: 3.074 | Reg loss: 0.032 | Tree loss: 3.074 | Accuracy: 0.109677 | 6.502 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 025 | Total loss: 3.305 | Reg loss: 0.032 | Tree loss: 3.305 | Accuracy: 0.097656 | 6.502 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 025 | Total loss: 3.285 | Reg loss: 0.032 | Tree loss: 3.285 | Accuracy: 0.091797 | 6.502 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 025 | Total loss: 3.271 | Reg loss: 0.032 | Tree loss: 3.271 | Accuracy: 0.091797 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 025 | Total loss: 3.179 | Reg loss: 0.032 | Tree loss: 3.179 | Accuracy: 0.113281 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 025 | Total loss: 3.208 | Reg loss: 0.032 | Tree loss: 3.208 | Accuracy: 0.107422 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 025 | Total loss: 3.150 | Reg loss: 0.032 | Tree loss: 3.150 | Accuracy: 0.107422 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 025 | Total loss: 3.124 | Reg loss: 0.032 | Tree loss: 3.124 | Accuracy: 0.087891 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 025 | Total loss: 3.215 | Reg loss: 0.032 | Tree loss: 3.215 | Accuracy: 0.091797 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 025 | Total loss: 3.187 | Reg loss: 0.032 | Tree loss: 3.187 | Accuracy: 0.113281 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 025 | Total loss: 3.213 | Reg loss: 0.032 | Tree loss: 3.213 | Accuracy: 0.095703 | 6.501 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 025 | Total loss: 3.196 | Reg loss: 0.032 | Tree loss: 3.196 | Accuracy: 0.105469 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 025 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.109375 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 025 | Total loss: 3.142 | Reg loss: 0.032 | Tree loss: 3.142 | Accuracy: 0.136719 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 025 | Total loss: 3.095 | Reg loss: 0.032 | Tree loss: 3.095 | Accuracy: 0.125000 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 025 | Total loss: 3.225 | Reg loss: 0.032 | Tree loss: 3.225 | Accuracy: 0.087891 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 025 | Total loss: 3.209 | Reg loss: 0.032 | Tree loss: 3.209 | Accuracy: 0.099609 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 025 | Total loss: 3.116 | Reg loss: 0.032 | Tree loss: 3.116 | Accuracy: 0.087891 | 6.5 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 025 | Total loss: 3.065 | Reg loss: 0.032 | Tree loss: 3.065 | Accuracy: 0.080078 | 6.499 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 025 | Total loss: 3.158 | Reg loss: 0.032 | Tree loss: 3.158 | Accuracy: 0.099609 | 6.499 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 025 | Total loss: 3.175 | Reg loss: 0.032 | Tree loss: 3.175 | Accuracy: 0.091797 | 6.499 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 025 | Total loss: 3.088 | Reg loss: 0.032 | Tree loss: 3.088 | Accuracy: 0.113281 | 6.499 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 025 | Total loss: 3.122 | Reg loss: 0.032 | Tree loss: 3.122 | Accuracy: 0.103516 | 6.498 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 025 | Total loss: 3.073 | Reg loss: 0.032 | Tree loss: 3.073 | Accuracy: 0.074219 | 6.498 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 025 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.087891 | 6.498 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 025 | Total loss: 3.090 | Reg loss: 0.032 | Tree loss: 3.090 | Accuracy: 0.096774 | 6.497 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 025 | Total loss: 3.230 | Reg loss: 0.032 | Tree loss: 3.230 | Accuracy: 0.117188 | 6.497 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 025 | Total loss: 3.259 | Reg loss: 0.032 | Tree loss: 3.259 | Accuracy: 0.099609 | 6.497 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 025 | Total loss: 3.216 | Reg loss: 0.032 | Tree loss: 3.216 | Accuracy: 0.134766 | 6.497 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 025 | Total loss: 3.240 | Reg loss: 0.032 | Tree loss: 3.240 | Accuracy: 0.103516 | 6.497 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 025 | Total loss: 3.149 | Reg loss: 0.032 | Tree loss: 3.149 | Accuracy: 0.097656 | 6.497 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 025 | Total loss: 3.146 | Reg loss: 0.032 | Tree loss: 3.146 | Accuracy: 0.105469 | 6.496 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 025 | Total loss: 3.198 | Reg loss: 0.032 | Tree loss: 3.198 | Accuracy: 0.083984 | 6.496 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 025 | Total loss: 3.214 | Reg loss: 0.032 | Tree loss: 3.214 | Accuracy: 0.099609 | 6.496 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 025 | Total loss: 3.104 | Reg loss: 0.032 | Tree loss: 3.104 | Accuracy: 0.109375 | 6.495 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 025 | Total loss: 3.153 | Reg loss: 0.032 | Tree loss: 3.153 | Accuracy: 0.085938 | 6.495 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 025 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.091797 | 6.495 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 025 | Total loss: 3.203 | Reg loss: 0.032 | Tree loss: 3.203 | Accuracy: 0.097656 | 6.495 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 025 | Total loss: 3.136 | Reg loss: 0.032 | Tree loss: 3.136 | Accuracy: 0.103516 | 6.494 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 025 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.099609 | 6.494 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 025 | Total loss: 3.137 | Reg loss: 0.032 | Tree loss: 3.137 | Accuracy: 0.111328 | 6.494 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 025 | Total loss: 3.128 | Reg loss: 0.032 | Tree loss: 3.128 | Accuracy: 0.111328 | 6.494 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 025 | Total loss: 3.147 | Reg loss: 0.032 | Tree loss: 3.147 | Accuracy: 0.101562 | 6.493 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 025 | Total loss: 3.099 | Reg loss: 0.032 | Tree loss: 3.099 | Accuracy: 0.097656 | 6.493 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 025 | Total loss: 3.070 | Reg loss: 0.032 | Tree loss: 3.070 | Accuracy: 0.101562 | 6.493 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 025 | Total loss: 3.048 | Reg loss: 0.032 | Tree loss: 3.048 | Accuracy: 0.093750 | 6.493 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 025 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.095703 | 6.492 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 025 | Total loss: 3.037 | Reg loss: 0.032 | Tree loss: 3.037 | Accuracy: 0.097656 | 6.492 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 025 | Total loss: 3.119 | Reg loss: 0.032 | Tree loss: 3.119 | Accuracy: 0.093750 | 6.492 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 025 | Total loss: 3.093 | Reg loss: 0.032 | Tree loss: 3.093 | Accuracy: 0.082031 | 6.492 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 025 | Total loss: 3.121 | Reg loss: 0.032 | Tree loss: 3.121 | Accuracy: 0.098925 | 6.491 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 025 | Total loss: 3.273 | Reg loss: 0.031 | Tree loss: 3.273 | Accuracy: 0.103516 | 6.492 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 025 | Total loss: 3.243 | Reg loss: 0.031 | Tree loss: 3.243 | Accuracy: 0.115234 | 6.492 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 025 | Total loss: 3.229 | Reg loss: 0.031 | Tree loss: 3.229 | Accuracy: 0.109375 | 6.492 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 025 | Total loss: 3.198 | Reg loss: 0.031 | Tree loss: 3.198 | Accuracy: 0.087891 | 6.492 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 025 | Total loss: 3.271 | Reg loss: 0.031 | Tree loss: 3.271 | Accuracy: 0.078125 | 6.491 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 025 | Total loss: 3.120 | Reg loss: 0.031 | Tree loss: 3.120 | Accuracy: 0.128906 | 6.491 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 025 | Total loss: 3.199 | Reg loss: 0.031 | Tree loss: 3.199 | Accuracy: 0.111328 | 6.491 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 025 | Total loss: 3.165 | Reg loss: 0.031 | Tree loss: 3.165 | Accuracy: 0.105469 | 6.491 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 025 | Total loss: 3.127 | Reg loss: 0.031 | Tree loss: 3.127 | Accuracy: 0.097656 | 6.49 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 025 | Total loss: 3.169 | Reg loss: 0.031 | Tree loss: 3.169 | Accuracy: 0.113281 | 6.49 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 025 | Total loss: 3.105 | Reg loss: 0.031 | Tree loss: 3.105 | Accuracy: 0.111328 | 6.49 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 025 | Total loss: 3.110 | Reg loss: 0.031 | Tree loss: 3.110 | Accuracy: 0.072266 | 6.489 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 012 / 025 | Total loss: 3.097 | Reg loss: 0.031 | Tree loss: 3.097 | Accuracy: 0.101562 | 6.489 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 025 | Total loss: 3.044 | Reg loss: 0.031 | Tree loss: 3.044 | Accuracy: 0.097656 | 6.489 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 025 | Total loss: 3.121 | Reg loss: 0.031 | Tree loss: 3.121 | Accuracy: 0.097656 | 6.489 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 025 | Total loss: 3.120 | Reg loss: 0.032 | Tree loss: 3.120 | Accuracy: 0.093750 | 6.488 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 025 | Total loss: 3.117 | Reg loss: 0.032 | Tree loss: 3.117 | Accuracy: 0.097656 | 6.488 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 025 | Total loss: 3.126 | Reg loss: 0.032 | Tree loss: 3.126 | Accuracy: 0.097656 | 6.488 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 025 | Total loss: 3.097 | Reg loss: 0.032 | Tree loss: 3.097 | Accuracy: 0.111328 | 6.488 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 025 | Total loss: 3.041 | Reg loss: 0.032 | Tree loss: 3.041 | Accuracy: 0.111328 | 6.488 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 025 | Total loss: 3.052 | Reg loss: 0.032 | Tree loss: 3.052 | Accuracy: 0.097656 | 6.487 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 025 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.113281 | 6.487 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 025 | Total loss: 3.080 | Reg loss: 0.032 | Tree loss: 3.080 | Accuracy: 0.111328 | 6.487 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 025 | Total loss: 3.059 | Reg loss: 0.032 | Tree loss: 3.059 | Accuracy: 0.087891 | 6.487 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 025 | Total loss: 3.098 | Reg loss: 0.032 | Tree loss: 3.098 | Accuracy: 0.083871 | 6.486 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 025 | Total loss: 3.189 | Reg loss: 0.031 | Tree loss: 3.189 | Accuracy: 0.117188 | 6.486 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 025 | Total loss: 3.180 | Reg loss: 0.031 | Tree loss: 3.180 | Accuracy: 0.117188 | 6.485 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 025 | Total loss: 3.246 | Reg loss: 0.031 | Tree loss: 3.246 | Accuracy: 0.083984 | 6.485 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 025 | Total loss: 3.225 | Reg loss: 0.031 | Tree loss: 3.225 | Accuracy: 0.105469 | 6.485 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 025 | Total loss: 3.154 | Reg loss: 0.031 | Tree loss: 3.154 | Accuracy: 0.132812 | 6.485 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 025 | Total loss: 3.121 | Reg loss: 0.031 | Tree loss: 3.121 | Accuracy: 0.091797 | 6.484 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 025 | Total loss: 3.156 | Reg loss: 0.031 | Tree loss: 3.156 | Accuracy: 0.095703 | 6.484 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 025 | Total loss: 3.127 | Reg loss: 0.031 | Tree loss: 3.127 | Accuracy: 0.093750 | 6.484 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 025 | Total loss: 3.168 | Reg loss: 0.031 | Tree loss: 3.168 | Accuracy: 0.089844 | 6.484 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 025 | Total loss: 3.189 | Reg loss: 0.031 | Tree loss: 3.189 | Accuracy: 0.093750 | 6.483 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 025 | Total loss: 3.133 | Reg loss: 0.031 | Tree loss: 3.133 | Accuracy: 0.132812 | 6.483 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 025 | Total loss: 3.151 | Reg loss: 0.031 | Tree loss: 3.151 | Accuracy: 0.087891 | 6.483 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 025 | Total loss: 3.092 | Reg loss: 0.031 | Tree loss: 3.092 | Accuracy: 0.105469 | 6.483 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 025 | Total loss: 3.120 | Reg loss: 0.031 | Tree loss: 3.120 | Accuracy: 0.087891 | 6.483 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 025 | Total loss: 3.075 | Reg loss: 0.031 | Tree loss: 3.075 | Accuracy: 0.097656 | 6.482 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 025 | Total loss: 3.076 | Reg loss: 0.031 | Tree loss: 3.076 | Accuracy: 0.095703 | 6.482 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 025 | Total loss: 3.107 | Reg loss: 0.031 | Tree loss: 3.107 | Accuracy: 0.109375 | 6.482 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 025 | Total loss: 3.048 | Reg loss: 0.031 | Tree loss: 3.048 | Accuracy: 0.115234 | 6.482 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 025 | Total loss: 3.045 | Reg loss: 0.031 | Tree loss: 3.045 | Accuracy: 0.117188 | 6.481 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 025 | Total loss: 3.118 | Reg loss: 0.031 | Tree loss: 3.118 | Accuracy: 0.087891 | 6.481 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 025 | Total loss: 3.027 | Reg loss: 0.031 | Tree loss: 3.027 | Accuracy: 0.097656 | 6.481 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 025 | Total loss: 3.047 | Reg loss: 0.031 | Tree loss: 3.047 | Accuracy: 0.078125 | 6.481 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 025 | Total loss: 3.052 | Reg loss: 0.031 | Tree loss: 3.052 | Accuracy: 0.107422 | 6.481 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 025 | Total loss: 3.029 | Reg loss: 0.031 | Tree loss: 3.029 | Accuracy: 0.099609 | 6.48 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 025 | Total loss: 3.032 | Reg loss: 0.032 | Tree loss: 3.032 | Accuracy: 0.077419 | 6.479 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 025 | Total loss: 3.216 | Reg loss: 0.031 | Tree loss: 3.216 | Accuracy: 0.082031 | 6.479 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 025 | Total loss: 3.269 | Reg loss: 0.031 | Tree loss: 3.269 | Accuracy: 0.087891 | 6.479 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 025 | Total loss: 3.171 | Reg loss: 0.031 | Tree loss: 3.171 | Accuracy: 0.101562 | 6.479 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 025 | Total loss: 3.147 | Reg loss: 0.031 | Tree loss: 3.147 | Accuracy: 0.105469 | 6.479 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 025 | Total loss: 3.176 | Reg loss: 0.031 | Tree loss: 3.176 | Accuracy: 0.099609 | 6.478 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 025 | Total loss: 3.143 | Reg loss: 0.031 | Tree loss: 3.143 | Accuracy: 0.123047 | 6.478 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 025 | Total loss: 3.156 | Reg loss: 0.031 | Tree loss: 3.156 | Accuracy: 0.105469 | 6.478 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 025 | Total loss: 3.111 | Reg loss: 0.031 | Tree loss: 3.111 | Accuracy: 0.105469 | 6.478 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 025 | Total loss: 3.117 | Reg loss: 0.031 | Tree loss: 3.117 | Accuracy: 0.103516 | 6.477 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 025 | Total loss: 3.162 | Reg loss: 0.031 | Tree loss: 3.162 | Accuracy: 0.093750 | 6.477 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 025 | Total loss: 3.104 | Reg loss: 0.031 | Tree loss: 3.104 | Accuracy: 0.097656 | 6.477 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 025 | Total loss: 3.127 | Reg loss: 0.031 | Tree loss: 3.127 | Accuracy: 0.113281 | 6.477 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 025 | Total loss: 3.081 | Reg loss: 0.031 | Tree loss: 3.081 | Accuracy: 0.113281 | 6.477 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 025 | Total loss: 3.114 | Reg loss: 0.031 | Tree loss: 3.114 | Accuracy: 0.117188 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 025 | Total loss: 3.104 | Reg loss: 0.031 | Tree loss: 3.104 | Accuracy: 0.080078 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 025 | Total loss: 3.116 | Reg loss: 0.031 | Tree loss: 3.116 | Accuracy: 0.062500 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 025 | Total loss: 3.107 | Reg loss: 0.031 | Tree loss: 3.107 | Accuracy: 0.091797 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 025 | Total loss: 3.083 | Reg loss: 0.031 | Tree loss: 3.083 | Accuracy: 0.095703 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 025 | Total loss: 3.047 | Reg loss: 0.031 | Tree loss: 3.047 | Accuracy: 0.119141 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 025 | Total loss: 2.968 | Reg loss: 0.031 | Tree loss: 2.968 | Accuracy: 0.117188 | 6.476 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 025 | Total loss: 3.015 | Reg loss: 0.031 | Tree loss: 3.015 | Accuracy: 0.115234 | 6.475 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 025 | Total loss: 3.030 | Reg loss: 0.031 | Tree loss: 3.030 | Accuracy: 0.089844 | 6.475 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 025 | Total loss: 3.006 | Reg loss: 0.031 | Tree loss: 3.006 | Accuracy: 0.123047 | 6.475 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 025 | Total loss: 2.999 | Reg loss: 0.031 | Tree loss: 2.999 | Accuracy: 0.101562 | 6.475 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 024 / 025 | Total loss: 3.049 | Reg loss: 0.031 | Tree loss: 3.049 | Accuracy: 0.092473 | 6.474 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 025 | Total loss: 3.195 | Reg loss: 0.031 | Tree loss: 3.195 | Accuracy: 0.097656 | 6.474 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 025 | Total loss: 3.186 | Reg loss: 0.031 | Tree loss: 3.186 | Accuracy: 0.111328 | 6.474 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 025 | Total loss: 3.173 | Reg loss: 0.031 | Tree loss: 3.173 | Accuracy: 0.097656 | 6.474 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 025 | Total loss: 3.179 | Reg loss: 0.031 | Tree loss: 3.179 | Accuracy: 0.095703 | 6.474 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 025 | Total loss: 3.177 | Reg loss: 0.031 | Tree loss: 3.177 | Accuracy: 0.107422 | 6.473 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 025 | Total loss: 3.117 | Reg loss: 0.031 | Tree loss: 3.117 | Accuracy: 0.105469 | 6.473 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 025 | Total loss: 3.168 | Reg loss: 0.031 | Tree loss: 3.168 | Accuracy: 0.101562 | 6.473 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 025 | Total loss: 3.158 | Reg loss: 0.031 | Tree loss: 3.158 | Accuracy: 0.101562 | 6.473 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 025 | Total loss: 3.101 | Reg loss: 0.031 | Tree loss: 3.101 | Accuracy: 0.087891 | 6.473 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 025 | Total loss: 3.113 | Reg loss: 0.031 | Tree loss: 3.113 | Accuracy: 0.111328 | 6.472 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 025 | Total loss: 3.095 | Reg loss: 0.031 | Tree loss: 3.095 | Accuracy: 0.093750 | 6.472 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 025 | Total loss: 3.078 | Reg loss: 0.031 | Tree loss: 3.078 | Accuracy: 0.103516 | 6.472 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 025 | Total loss: 3.137 | Reg loss: 0.031 | Tree loss: 3.137 | Accuracy: 0.109375 | 6.472 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 025 | Total loss: 3.049 | Reg loss: 0.031 | Tree loss: 3.049 | Accuracy: 0.089844 | 6.472 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 025 | Total loss: 3.024 | Reg loss: 0.031 | Tree loss: 3.024 | Accuracy: 0.113281 | 6.471 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 025 | Total loss: 3.103 | Reg loss: 0.031 | Tree loss: 3.103 | Accuracy: 0.095703 | 6.471 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 025 | Total loss: 3.056 | Reg loss: 0.031 | Tree loss: 3.056 | Accuracy: 0.097656 | 6.471 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 025 | Total loss: 3.031 | Reg loss: 0.031 | Tree loss: 3.031 | Accuracy: 0.095703 | 6.471 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 025 | Total loss: 3.076 | Reg loss: 0.031 | Tree loss: 3.076 | Accuracy: 0.138672 | 6.47 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 025 | Total loss: 3.030 | Reg loss: 0.031 | Tree loss: 3.030 | Accuracy: 0.117188 | 6.47 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 025 | Total loss: 3.070 | Reg loss: 0.031 | Tree loss: 3.070 | Accuracy: 0.093750 | 6.47 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 025 | Total loss: 3.025 | Reg loss: 0.031 | Tree loss: 3.025 | Accuracy: 0.085938 | 6.47 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 025 | Total loss: 3.003 | Reg loss: 0.031 | Tree loss: 3.003 | Accuracy: 0.095703 | 6.469 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 025 | Total loss: 3.009 | Reg loss: 0.031 | Tree loss: 3.009 | Accuracy: 0.087891 | 6.469 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 025 | Total loss: 3.000 | Reg loss: 0.031 | Tree loss: 3.000 | Accuracy: 0.092473 | 6.468 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 025 | Total loss: 3.180 | Reg loss: 0.031 | Tree loss: 3.180 | Accuracy: 0.101562 | 6.468 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 025 | Total loss: 3.195 | Reg loss: 0.031 | Tree loss: 3.195 | Accuracy: 0.083984 | 6.468 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 025 | Total loss: 3.155 | Reg loss: 0.031 | Tree loss: 3.155 | Accuracy: 0.099609 | 6.468 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 025 | Total loss: 3.140 | Reg loss: 0.031 | Tree loss: 3.140 | Accuracy: 0.107422 | 6.468 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 025 | Total loss: 3.132 | Reg loss: 0.031 | Tree loss: 3.132 | Accuracy: 0.105469 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 025 | Total loss: 3.054 | Reg loss: 0.031 | Tree loss: 3.054 | Accuracy: 0.109375 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 025 | Total loss: 3.121 | Reg loss: 0.031 | Tree loss: 3.121 | Accuracy: 0.074219 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 025 | Total loss: 3.144 | Reg loss: 0.031 | Tree loss: 3.144 | Accuracy: 0.101562 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 025 | Total loss: 3.117 | Reg loss: 0.031 | Tree loss: 3.117 | Accuracy: 0.101562 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 025 | Total loss: 3.067 | Reg loss: 0.031 | Tree loss: 3.067 | Accuracy: 0.119141 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 025 | Total loss: 3.064 | Reg loss: 0.031 | Tree loss: 3.064 | Accuracy: 0.123047 | 6.467 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 025 | Total loss: 3.097 | Reg loss: 0.031 | Tree loss: 3.097 | Accuracy: 0.095703 | 6.466 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 025 | Total loss: 3.105 | Reg loss: 0.031 | Tree loss: 3.105 | Accuracy: 0.115234 | 6.466 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 025 | Total loss: 3.047 | Reg loss: 0.031 | Tree loss: 3.047 | Accuracy: 0.083984 | 6.466 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 025 | Total loss: 3.108 | Reg loss: 0.031 | Tree loss: 3.108 | Accuracy: 0.083984 | 6.466 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 025 | Total loss: 3.015 | Reg loss: 0.031 | Tree loss: 3.015 | Accuracy: 0.128906 | 6.466 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 025 | Total loss: 3.056 | Reg loss: 0.031 | Tree loss: 3.056 | Accuracy: 0.093750 | 6.465 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 025 | Total loss: 3.057 | Reg loss: 0.031 | Tree loss: 3.057 | Accuracy: 0.097656 | 6.465 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 025 | Total loss: 3.041 | Reg loss: 0.031 | Tree loss: 3.041 | Accuracy: 0.126953 | 6.465 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 025 | Total loss: 3.024 | Reg loss: 0.031 | Tree loss: 3.024 | Accuracy: 0.087891 | 6.465 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 025 | Total loss: 3.032 | Reg loss: 0.031 | Tree loss: 3.032 | Accuracy: 0.117188 | 6.464 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 025 | Total loss: 3.067 | Reg loss: 0.031 | Tree loss: 3.067 | Accuracy: 0.097656 | 6.464 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 025 | Total loss: 3.061 | Reg loss: 0.031 | Tree loss: 3.061 | Accuracy: 0.083984 | 6.464 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 025 | Total loss: 3.060 | Reg loss: 0.031 | Tree loss: 3.060 | Accuracy: 0.089844 | 6.464 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 025 | Total loss: 2.968 | Reg loss: 0.031 | Tree loss: 2.968 | Accuracy: 0.105376 | 6.462 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 025 | Total loss: 3.176 | Reg loss: 0.031 | Tree loss: 3.176 | Accuracy: 0.078125 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 025 | Total loss: 3.161 | Reg loss: 0.031 | Tree loss: 3.161 | Accuracy: 0.093750 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 025 | Total loss: 3.213 | Reg loss: 0.031 | Tree loss: 3.213 | Accuracy: 0.121094 | 6.463 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 025 | Total loss: 3.105 | Reg loss: 0.031 | Tree loss: 3.105 | Accuracy: 0.101562 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 025 | Total loss: 3.104 | Reg loss: 0.031 | Tree loss: 3.104 | Accuracy: 0.105469 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 025 | Total loss: 3.123 | Reg loss: 0.031 | Tree loss: 3.123 | Accuracy: 0.109375 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 025 | Total loss: 3.183 | Reg loss: 0.031 | Tree loss: 3.183 | Accuracy: 0.097656 | 6.462 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 025 | Total loss: 3.138 | Reg loss: 0.031 | Tree loss: 3.138 | Accuracy: 0.089844 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 025 | Total loss: 3.115 | Reg loss: 0.031 | Tree loss: 3.115 | Accuracy: 0.109375 | 6.461 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 009 / 025 | Total loss: 3.135 | Reg loss: 0.031 | Tree loss: 3.135 | Accuracy: 0.093750 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 025 | Total loss: 3.055 | Reg loss: 0.031 | Tree loss: 3.055 | Accuracy: 0.103516 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 025 | Total loss: 3.070 | Reg loss: 0.031 | Tree loss: 3.070 | Accuracy: 0.099609 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 025 | Total loss: 3.050 | Reg loss: 0.031 | Tree loss: 3.050 | Accuracy: 0.111328 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 025 | Total loss: 3.051 | Reg loss: 0.031 | Tree loss: 3.051 | Accuracy: 0.126953 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 025 | Total loss: 3.016 | Reg loss: 0.031 | Tree loss: 3.016 | Accuracy: 0.103516 | 6.461 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 025 | Total loss: 3.016 | Reg loss: 0.031 | Tree loss: 3.016 | Accuracy: 0.105469 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 025 | Total loss: 3.007 | Reg loss: 0.031 | Tree loss: 3.007 | Accuracy: 0.103516 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 025 | Total loss: 3.052 | Reg loss: 0.031 | Tree loss: 3.052 | Accuracy: 0.099609 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 025 | Total loss: 3.118 | Reg loss: 0.031 | Tree loss: 3.118 | Accuracy: 0.070312 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 025 | Total loss: 3.002 | Reg loss: 0.031 | Tree loss: 3.002 | Accuracy: 0.101562 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 025 | Total loss: 2.999 | Reg loss: 0.031 | Tree loss: 2.999 | Accuracy: 0.107422 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 025 | Total loss: 3.006 | Reg loss: 0.031 | Tree loss: 3.006 | Accuracy: 0.109375 | 6.46 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 025 | Total loss: 3.034 | Reg loss: 0.031 | Tree loss: 3.034 | Accuracy: 0.095703 | 6.459 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 025 | Total loss: 2.946 | Reg loss: 0.031 | Tree loss: 2.946 | Accuracy: 0.101562 | 6.459 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 025 | Total loss: 3.028 | Reg loss: 0.031 | Tree loss: 3.028 | Accuracy: 0.092473 | 6.458 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 025 | Total loss: 3.173 | Reg loss: 0.030 | Tree loss: 3.173 | Accuracy: 0.087891 | 6.458 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 025 | Total loss: 3.175 | Reg loss: 0.030 | Tree loss: 3.175 | Accuracy: 0.123047 | 6.458 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 025 | Total loss: 3.220 | Reg loss: 0.030 | Tree loss: 3.220 | Accuracy: 0.091797 | 6.458 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 025 | Total loss: 3.096 | Reg loss: 0.030 | Tree loss: 3.096 | Accuracy: 0.101562 | 6.458 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 025 | Total loss: 3.134 | Reg loss: 0.030 | Tree loss: 3.134 | Accuracy: 0.091797 | 6.457 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 025 | Total loss: 3.108 | Reg loss: 0.030 | Tree loss: 3.108 | Accuracy: 0.119141 | 6.457 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 025 | Total loss: 3.126 | Reg loss: 0.030 | Tree loss: 3.126 | Accuracy: 0.097656 | 6.457 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 025 | Total loss: 3.089 | Reg loss: 0.030 | Tree loss: 3.089 | Accuracy: 0.119141 | 6.457 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 025 | Total loss: 3.095 | Reg loss: 0.030 | Tree loss: 3.095 | Accuracy: 0.111328 | 6.457 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 025 | Total loss: 3.087 | Reg loss: 0.030 | Tree loss: 3.087 | Accuracy: 0.105469 | 6.456 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 025 | Total loss: 3.108 | Reg loss: 0.031 | Tree loss: 3.108 | Accuracy: 0.095703 | 6.456 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 025 | Total loss: 3.086 | Reg loss: 0.031 | Tree loss: 3.086 | Accuracy: 0.091797 | 6.456 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 025 | Total loss: 3.069 | Reg loss: 0.031 | Tree loss: 3.069 | Accuracy: 0.107422 | 6.456 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 025 | Total loss: 3.011 | Reg loss: 0.031 | Tree loss: 3.011 | Accuracy: 0.111328 | 6.456 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 025 | Total loss: 3.057 | Reg loss: 0.031 | Tree loss: 3.057 | Accuracy: 0.074219 | 6.455 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 025 | Total loss: 2.990 | Reg loss: 0.031 | Tree loss: 2.990 | Accuracy: 0.091797 | 6.455 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 025 | Total loss: 3.045 | Reg loss: 0.031 | Tree loss: 3.045 | Accuracy: 0.121094 | 6.455 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 025 | Total loss: 3.016 | Reg loss: 0.031 | Tree loss: 3.016 | Accuracy: 0.082031 | 6.455 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 025 | Total loss: 3.069 | Reg loss: 0.031 | Tree loss: 3.069 | Accuracy: 0.105469 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 025 | Total loss: 3.037 | Reg loss: 0.031 | Tree loss: 3.037 | Accuracy: 0.107422 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 025 | Total loss: 2.999 | Reg loss: 0.031 | Tree loss: 2.999 | Accuracy: 0.107422 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 025 | Total loss: 2.968 | Reg loss: 0.031 | Tree loss: 2.968 | Accuracy: 0.089844 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 025 | Total loss: 2.969 | Reg loss: 0.031 | Tree loss: 2.969 | Accuracy: 0.093750 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 025 | Total loss: 3.013 | Reg loss: 0.031 | Tree loss: 3.013 | Accuracy: 0.105469 | 6.454 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 025 | Total loss: 2.970 | Reg loss: 0.031 | Tree loss: 2.970 | Accuracy: 0.101075 | 6.453 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 025 | Total loss: 3.176 | Reg loss: 0.030 | Tree loss: 3.176 | Accuracy: 0.064453 | 6.453 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 025 | Total loss: 3.066 | Reg loss: 0.030 | Tree loss: 3.066 | Accuracy: 0.103516 | 6.452 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 025 | Total loss: 3.150 | Reg loss: 0.030 | Tree loss: 3.150 | Accuracy: 0.099609 | 6.452 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 025 | Total loss: 3.221 | Reg loss: 0.030 | Tree loss: 3.221 | Accuracy: 0.099609 | 6.452 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 025 | Total loss: 3.108 | Reg loss: 0.030 | Tree loss: 3.108 | Accuracy: 0.125000 | 6.452 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 025 | Total loss: 3.094 | Reg loss: 0.030 | Tree loss: 3.094 | Accuracy: 0.097656 | 6.452 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 025 | Total loss: 3.141 | Reg loss: 0.030 | Tree loss: 3.141 | Accuracy: 0.099609 | 6.451 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 025 | Total loss: 3.161 | Reg loss: 0.030 | Tree loss: 3.161 | Accuracy: 0.083984 | 6.451 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 025 | Total loss: 3.115 | Reg loss: 0.030 | Tree loss: 3.115 | Accuracy: 0.089844 | 6.451 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 025 | Total loss: 3.065 | Reg loss: 0.030 | Tree loss: 3.065 | Accuracy: 0.107422 | 6.451 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 025 | Total loss: 3.038 | Reg loss: 0.030 | Tree loss: 3.038 | Accuracy: 0.105469 | 6.451 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 025 | Total loss: 3.037 | Reg loss: 0.030 | Tree loss: 3.037 | Accuracy: 0.107422 | 6.45 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 025 | Total loss: 3.019 | Reg loss: 0.030 | Tree loss: 3.019 | Accuracy: 0.105469 | 6.45 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 025 | Total loss: 3.064 | Reg loss: 0.030 | Tree loss: 3.064 | Accuracy: 0.121094 | 6.45 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 025 | Total loss: 3.055 | Reg loss: 0.030 | Tree loss: 3.055 | Accuracy: 0.113281 | 6.45 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 025 | Total loss: 3.001 | Reg loss: 0.030 | Tree loss: 3.001 | Accuracy: 0.099609 | 6.45 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 025 | Total loss: 3.004 | Reg loss: 0.030 | Tree loss: 3.004 | Accuracy: 0.097656 | 6.449 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 025 | Total loss: 3.047 | Reg loss: 0.030 | Tree loss: 3.047 | Accuracy: 0.099609 | 6.449 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 025 | Total loss: 2.997 | Reg loss: 0.030 | Tree loss: 2.997 | Accuracy: 0.093750 | 6.449 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 025 | Total loss: 3.064 | Reg loss: 0.030 | Tree loss: 3.064 | Accuracy: 0.091797 | 6.449 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 025 | Total loss: 3.020 | Reg loss: 0.031 | Tree loss: 3.020 | Accuracy: 0.093750 | 6.449 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 021 / 025 | Total loss: 3.012 | Reg loss: 0.031 | Tree loss: 3.012 | Accuracy: 0.105469 | 6.449 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 025 | Total loss: 3.006 | Reg loss: 0.031 | Tree loss: 3.006 | Accuracy: 0.101562 | 6.448 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 025 | Total loss: 2.957 | Reg loss: 0.031 | Tree loss: 2.957 | Accuracy: 0.117188 | 6.448 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 025 | Total loss: 2.921 | Reg loss: 0.031 | Tree loss: 2.921 | Accuracy: 0.109677 | 6.447 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 025 | Total loss: 3.156 | Reg loss: 0.030 | Tree loss: 3.156 | Accuracy: 0.105469 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 025 | Total loss: 3.169 | Reg loss: 0.030 | Tree loss: 3.169 | Accuracy: 0.093750 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 025 | Total loss: 3.054 | Reg loss: 0.030 | Tree loss: 3.054 | Accuracy: 0.113281 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 025 | Total loss: 3.131 | Reg loss: 0.030 | Tree loss: 3.131 | Accuracy: 0.119141 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 025 | Total loss: 3.167 | Reg loss: 0.030 | Tree loss: 3.167 | Accuracy: 0.101562 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 025 | Total loss: 3.100 | Reg loss: 0.030 | Tree loss: 3.100 | Accuracy: 0.107422 | 6.447 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 025 | Total loss: 3.120 | Reg loss: 0.030 | Tree loss: 3.120 | Accuracy: 0.089844 | 6.446 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 025 | Total loss: 3.053 | Reg loss: 0.030 | Tree loss: 3.053 | Accuracy: 0.119141 | 6.446 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 025 | Total loss: 3.110 | Reg loss: 0.030 | Tree loss: 3.110 | Accuracy: 0.076172 | 6.446 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 025 | Total loss: 3.051 | Reg loss: 0.030 | Tree loss: 3.051 | Accuracy: 0.105469 | 6.446 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 025 | Total loss: 3.019 | Reg loss: 0.030 | Tree loss: 3.019 | Accuracy: 0.105469 | 6.446 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 025 | Total loss: 3.071 | Reg loss: 0.030 | Tree loss: 3.071 | Accuracy: 0.093750 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 025 | Total loss: 3.016 | Reg loss: 0.030 | Tree loss: 3.016 | Accuracy: 0.091797 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 025 | Total loss: 3.020 | Reg loss: 0.030 | Tree loss: 3.020 | Accuracy: 0.087891 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 025 | Total loss: 3.019 | Reg loss: 0.030 | Tree loss: 3.019 | Accuracy: 0.107422 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 025 | Total loss: 3.039 | Reg loss: 0.030 | Tree loss: 3.039 | Accuracy: 0.113281 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 025 | Total loss: 3.038 | Reg loss: 0.030 | Tree loss: 3.038 | Accuracy: 0.091797 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 025 | Total loss: 3.078 | Reg loss: 0.030 | Tree loss: 3.078 | Accuracy: 0.093750 | 6.445 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 025 | Total loss: 3.009 | Reg loss: 0.030 | Tree loss: 3.009 | Accuracy: 0.101562 | 6.444 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 025 | Total loss: 2.989 | Reg loss: 0.030 | Tree loss: 2.989 | Accuracy: 0.091797 | 6.444 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 025 | Total loss: 2.948 | Reg loss: 0.030 | Tree loss: 2.948 | Accuracy: 0.113281 | 6.444 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 025 | Total loss: 3.007 | Reg loss: 0.030 | Tree loss: 3.007 | Accuracy: 0.103516 | 6.444 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 025 | Total loss: 2.994 | Reg loss: 0.030 | Tree loss: 2.994 | Accuracy: 0.097656 | 6.444 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 025 | Total loss: 3.033 | Reg loss: 0.030 | Tree loss: 3.033 | Accuracy: 0.123047 | 6.443 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 025 | Total loss: 2.996 | Reg loss: 0.030 | Tree loss: 2.996 | Accuracy: 0.096774 | 6.442 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 025 | Total loss: 3.168 | Reg loss: 0.030 | Tree loss: 3.168 | Accuracy: 0.087891 | 6.442 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 025 | Total loss: 3.197 | Reg loss: 0.030 | Tree loss: 3.197 | Accuracy: 0.099609 | 6.442 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 025 | Total loss: 3.081 | Reg loss: 0.030 | Tree loss: 3.081 | Accuracy: 0.107422 | 6.442 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 025 | Total loss: 3.108 | Reg loss: 0.030 | Tree loss: 3.108 | Accuracy: 0.107422 | 6.442 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 025 | Total loss: 3.138 | Reg loss: 0.030 | Tree loss: 3.138 | Accuracy: 0.089844 | 6.442 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 025 | Total loss: 3.117 | Reg loss: 0.030 | Tree loss: 3.117 | Accuracy: 0.105469 | 6.441 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 025 | Total loss: 3.069 | Reg loss: 0.030 | Tree loss: 3.069 | Accuracy: 0.078125 | 6.441 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 025 | Total loss: 3.016 | Reg loss: 0.030 | Tree loss: 3.016 | Accuracy: 0.099609 | 6.441 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 025 | Total loss: 3.090 | Reg loss: 0.030 | Tree loss: 3.090 | Accuracy: 0.103516 | 6.441 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 025 | Total loss: 3.089 | Reg loss: 0.030 | Tree loss: 3.089 | Accuracy: 0.101562 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 025 | Total loss: 3.018 | Reg loss: 0.030 | Tree loss: 3.018 | Accuracy: 0.101562 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 025 | Total loss: 3.047 | Reg loss: 0.030 | Tree loss: 3.047 | Accuracy: 0.082031 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 025 | Total loss: 3.064 | Reg loss: 0.030 | Tree loss: 3.064 | Accuracy: 0.103516 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 025 | Total loss: 3.087 | Reg loss: 0.030 | Tree loss: 3.087 | Accuracy: 0.083984 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 025 | Total loss: 3.019 | Reg loss: 0.030 | Tree loss: 3.019 | Accuracy: 0.109375 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 025 | Total loss: 2.939 | Reg loss: 0.030 | Tree loss: 2.939 | Accuracy: 0.113281 | 6.44 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 025 | Total loss: 3.018 | Reg loss: 0.030 | Tree loss: 3.018 | Accuracy: 0.101562 | 6.439 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 025 | Total loss: 3.035 | Reg loss: 0.030 | Tree loss: 3.035 | Accuracy: 0.095703 | 6.439 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 025 | Total loss: 2.988 | Reg loss: 0.030 | Tree loss: 2.988 | Accuracy: 0.140625 | 6.439 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 025 | Total loss: 2.919 | Reg loss: 0.030 | Tree loss: 2.919 | Accuracy: 0.115234 | 6.439 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 025 | Total loss: 3.042 | Reg loss: 0.030 | Tree loss: 3.042 | Accuracy: 0.097656 | 6.439 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 025 | Total loss: 3.039 | Reg loss: 0.030 | Tree loss: 3.039 | Accuracy: 0.111328 | 6.438 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 025 | Total loss: 2.990 | Reg loss: 0.030 | Tree loss: 2.990 | Accuracy: 0.083984 | 6.438 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 025 | Total loss: 2.993 | Reg loss: 0.030 | Tree loss: 2.993 | Accuracy: 0.105469 | 6.438 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 025 | Total loss: 2.967 | Reg loss: 0.030 | Tree loss: 2.967 | Accuracy: 0.113978 | 6.437 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 025 | Total loss: 3.105 | Reg loss: 0.030 | Tree loss: 3.105 | Accuracy: 0.101562 | 6.437 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 025 | Total loss: 3.109 | Reg loss: 0.030 | Tree loss: 3.109 | Accuracy: 0.117188 | 6.437 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 025 | Total loss: 3.101 | Reg loss: 0.030 | Tree loss: 3.101 | Accuracy: 0.089844 | 6.436 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 025 | Total loss: 3.056 | Reg loss: 0.030 | Tree loss: 3.056 | Accuracy: 0.117188 | 6.436 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 025 | Total loss: 3.113 | Reg loss: 0.030 | Tree loss: 3.113 | Accuracy: 0.093750 | 6.436 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 025 | Total loss: 3.067 | Reg loss: 0.030 | Tree loss: 3.067 | Accuracy: 0.121094 | 6.436 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 006 / 025 | Total loss: 3.035 | Reg loss: 0.030 | Tree loss: 3.035 | Accuracy: 0.111328 | 6.436 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 025 | Total loss: 3.114 | Reg loss: 0.030 | Tree loss: 3.114 | Accuracy: 0.083984 | 6.436 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 025 | Total loss: 3.062 | Reg loss: 0.030 | Tree loss: 3.062 | Accuracy: 0.076172 | 6.435 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 025 | Total loss: 3.102 | Reg loss: 0.030 | Tree loss: 3.102 | Accuracy: 0.111328 | 6.435 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 025 | Total loss: 3.025 | Reg loss: 0.030 | Tree loss: 3.025 | Accuracy: 0.107422 | 6.435 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 025 | Total loss: 3.077 | Reg loss: 0.030 | Tree loss: 3.077 | Accuracy: 0.093750 | 6.435 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 025 | Total loss: 3.029 | Reg loss: 0.030 | Tree loss: 3.029 | Accuracy: 0.085938 | 6.435 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 025 | Total loss: 3.027 | Reg loss: 0.030 | Tree loss: 3.027 | Accuracy: 0.105469 | 6.434 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 025 | Total loss: 3.032 | Reg loss: 0.030 | Tree loss: 3.032 | Accuracy: 0.101562 | 6.434 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 025 | Total loss: 3.000 | Reg loss: 0.030 | Tree loss: 3.000 | Accuracy: 0.103516 | 6.434 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 025 | Total loss: 2.964 | Reg loss: 0.030 | Tree loss: 2.964 | Accuracy: 0.105469 | 6.434 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 025 | Total loss: 3.050 | Reg loss: 0.030 | Tree loss: 3.050 | Accuracy: 0.097656 | 6.434 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 025 | Total loss: 3.030 | Reg loss: 0.030 | Tree loss: 3.030 | Accuracy: 0.117188 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 025 | Total loss: 3.036 | Reg loss: 0.030 | Tree loss: 3.036 | Accuracy: 0.093750 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 025 | Total loss: 3.041 | Reg loss: 0.030 | Tree loss: 3.041 | Accuracy: 0.099609 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 025 | Total loss: 3.005 | Reg loss: 0.030 | Tree loss: 3.005 | Accuracy: 0.105469 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 025 | Total loss: 2.985 | Reg loss: 0.030 | Tree loss: 2.985 | Accuracy: 0.093750 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 025 | Total loss: 2.982 | Reg loss: 0.030 | Tree loss: 2.982 | Accuracy: 0.101562 | 6.433 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 025 | Total loss: 2.965 | Reg loss: 0.030 | Tree loss: 2.965 | Accuracy: 0.101075 | 6.432 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 025 | Total loss: 3.137 | Reg loss: 0.030 | Tree loss: 3.137 | Accuracy: 0.091797 | 6.431 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 025 | Total loss: 3.080 | Reg loss: 0.030 | Tree loss: 3.080 | Accuracy: 0.103516 | 6.432 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 025 | Total loss: 3.146 | Reg loss: 0.030 | Tree loss: 3.146 | Accuracy: 0.111328 | 6.431 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 025 | Total loss: 3.087 | Reg loss: 0.030 | Tree loss: 3.087 | Accuracy: 0.105469 | 6.431 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 025 | Total loss: 3.099 | Reg loss: 0.030 | Tree loss: 3.099 | Accuracy: 0.103516 | 6.431 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 025 | Total loss: 3.071 | Reg loss: 0.030 | Tree loss: 3.071 | Accuracy: 0.095703 | 6.431 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 025 | Total loss: 3.138 | Reg loss: 0.030 | Tree loss: 3.138 | Accuracy: 0.083984 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 025 | Total loss: 3.042 | Reg loss: 0.030 | Tree loss: 3.042 | Accuracy: 0.089844 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 025 | Total loss: 3.064 | Reg loss: 0.030 | Tree loss: 3.064 | Accuracy: 0.105469 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 025 | Total loss: 3.077 | Reg loss: 0.030 | Tree loss: 3.077 | Accuracy: 0.085938 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 025 | Total loss: 3.023 | Reg loss: 0.030 | Tree loss: 3.023 | Accuracy: 0.109375 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 025 | Total loss: 3.062 | Reg loss: 0.030 | Tree loss: 3.062 | Accuracy: 0.103516 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 025 | Total loss: 3.013 | Reg loss: 0.030 | Tree loss: 3.013 | Accuracy: 0.130859 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 025 | Total loss: 3.038 | Reg loss: 0.030 | Tree loss: 3.038 | Accuracy: 0.105469 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 025 | Total loss: 2.998 | Reg loss: 0.030 | Tree loss: 2.998 | Accuracy: 0.107422 | 6.43 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 025 | Total loss: 3.017 | Reg loss: 0.030 | Tree loss: 3.017 | Accuracy: 0.103516 | 6.429 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 025 | Total loss: 3.008 | Reg loss: 0.030 | Tree loss: 3.008 | Accuracy: 0.103516 | 6.429 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 025 | Total loss: 2.996 | Reg loss: 0.030 | Tree loss: 2.996 | Accuracy: 0.132812 | 6.429 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 025 | Total loss: 2.978 | Reg loss: 0.030 | Tree loss: 2.978 | Accuracy: 0.085938 | 6.429 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 025 | Total loss: 3.003 | Reg loss: 0.030 | Tree loss: 3.003 | Accuracy: 0.097656 | 6.429 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 025 | Total loss: 2.983 | Reg loss: 0.030 | Tree loss: 2.983 | Accuracy: 0.101562 | 6.428 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 025 | Total loss: 2.997 | Reg loss: 0.030 | Tree loss: 2.997 | Accuracy: 0.101562 | 6.428 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 025 | Total loss: 3.010 | Reg loss: 0.030 | Tree loss: 3.010 | Accuracy: 0.085938 | 6.428 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 025 | Total loss: 2.930 | Reg loss: 0.030 | Tree loss: 2.930 | Accuracy: 0.095703 | 6.428 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 025 | Total loss: 2.993 | Reg loss: 0.030 | Tree loss: 2.993 | Accuracy: 0.096774 | 6.427 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 025 | Total loss: 3.087 | Reg loss: 0.030 | Tree loss: 3.087 | Accuracy: 0.103516 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 025 | Total loss: 3.156 | Reg loss: 0.030 | Tree loss: 3.156 | Accuracy: 0.066406 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 025 | Total loss: 3.085 | Reg loss: 0.030 | Tree loss: 3.085 | Accuracy: 0.105469 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 025 | Total loss: 3.106 | Reg loss: 0.030 | Tree loss: 3.106 | Accuracy: 0.091797 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 025 | Total loss: 3.060 | Reg loss: 0.030 | Tree loss: 3.060 | Accuracy: 0.113281 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 025 | Total loss: 3.044 | Reg loss: 0.030 | Tree loss: 3.044 | Accuracy: 0.091797 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 025 | Total loss: 3.097 | Reg loss: 0.030 | Tree loss: 3.097 | Accuracy: 0.126953 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 025 | Total loss: 3.055 | Reg loss: 0.030 | Tree loss: 3.055 | Accuracy: 0.101562 | 6.426 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 025 | Total loss: 3.061 | Reg loss: 0.030 | Tree loss: 3.061 | Accuracy: 0.107422 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 025 | Total loss: 3.037 | Reg loss: 0.030 | Tree loss: 3.037 | Accuracy: 0.125000 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 025 | Total loss: 3.017 | Reg loss: 0.030 | Tree loss: 3.017 | Accuracy: 0.121094 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 025 | Total loss: 3.120 | Reg loss: 0.030 | Tree loss: 3.120 | Accuracy: 0.085938 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 025 | Total loss: 3.020 | Reg loss: 0.030 | Tree loss: 3.020 | Accuracy: 0.101562 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 025 | Total loss: 3.013 | Reg loss: 0.030 | Tree loss: 3.013 | Accuracy: 0.111328 | 6.425 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 025 | Total loss: 3.088 | Reg loss: 0.030 | Tree loss: 3.088 | Accuracy: 0.089844 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 025 | Total loss: 3.077 | Reg loss: 0.030 | Tree loss: 3.077 | Accuracy: 0.103516 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 025 | Total loss: 3.070 | Reg loss: 0.030 | Tree loss: 3.070 | Accuracy: 0.099609 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 025 | Total loss: 2.999 | Reg loss: 0.030 | Tree loss: 2.999 | Accuracy: 0.101562 | 6.424 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 018 / 025 | Total loss: 2.938 | Reg loss: 0.030 | Tree loss: 2.938 | Accuracy: 0.111328 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 025 | Total loss: 2.940 | Reg loss: 0.030 | Tree loss: 2.940 | Accuracy: 0.107422 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 025 | Total loss: 2.999 | Reg loss: 0.030 | Tree loss: 2.999 | Accuracy: 0.074219 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 025 | Total loss: 2.986 | Reg loss: 0.030 | Tree loss: 2.986 | Accuracy: 0.091797 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 025 | Total loss: 2.973 | Reg loss: 0.030 | Tree loss: 2.973 | Accuracy: 0.089844 | 6.424 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 025 | Total loss: 2.920 | Reg loss: 0.030 | Tree loss: 2.920 | Accuracy: 0.101562 | 6.423 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 025 | Total loss: 2.933 | Reg loss: 0.030 | Tree loss: 2.933 | Accuracy: 0.116129 | 6.422 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 025 | Total loss: 3.116 | Reg loss: 0.030 | Tree loss: 3.116 | Accuracy: 0.103516 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 025 | Total loss: 3.071 | Reg loss: 0.030 | Tree loss: 3.071 | Accuracy: 0.119141 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 025 | Total loss: 3.114 | Reg loss: 0.030 | Tree loss: 3.114 | Accuracy: 0.089844 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 025 | Total loss: 3.073 | Reg loss: 0.030 | Tree loss: 3.073 | Accuracy: 0.123047 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 025 | Total loss: 3.098 | Reg loss: 0.030 | Tree loss: 3.098 | Accuracy: 0.103516 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 025 | Total loss: 3.055 | Reg loss: 0.030 | Tree loss: 3.055 | Accuracy: 0.128906 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 025 | Total loss: 3.054 | Reg loss: 0.030 | Tree loss: 3.054 | Accuracy: 0.091797 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 025 | Total loss: 3.047 | Reg loss: 0.030 | Tree loss: 3.047 | Accuracy: 0.085938 | 6.422 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 025 | Total loss: 3.030 | Reg loss: 0.030 | Tree loss: 3.030 | Accuracy: 0.082031 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 025 | Total loss: 3.055 | Reg loss: 0.030 | Tree loss: 3.055 | Accuracy: 0.093750 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 025 | Total loss: 3.085 | Reg loss: 0.030 | Tree loss: 3.085 | Accuracy: 0.082031 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 025 | Total loss: 3.003 | Reg loss: 0.030 | Tree loss: 3.003 | Accuracy: 0.117188 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 025 | Total loss: 2.991 | Reg loss: 0.030 | Tree loss: 2.991 | Accuracy: 0.101562 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 025 | Total loss: 3.058 | Reg loss: 0.030 | Tree loss: 3.058 | Accuracy: 0.093750 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 025 | Total loss: 3.024 | Reg loss: 0.030 | Tree loss: 3.024 | Accuracy: 0.099609 | 6.421 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 025 | Total loss: 3.023 | Reg loss: 0.030 | Tree loss: 3.023 | Accuracy: 0.115234 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 025 | Total loss: 3.058 | Reg loss: 0.030 | Tree loss: 3.058 | Accuracy: 0.105469 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 025 | Total loss: 2.924 | Reg loss: 0.030 | Tree loss: 2.924 | Accuracy: 0.103516 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 025 | Total loss: 2.942 | Reg loss: 0.030 | Tree loss: 2.942 | Accuracy: 0.097656 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 025 | Total loss: 2.970 | Reg loss: 0.030 | Tree loss: 2.970 | Accuracy: 0.097656 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 025 | Total loss: 3.033 | Reg loss: 0.030 | Tree loss: 3.033 | Accuracy: 0.080078 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 025 | Total loss: 3.009 | Reg loss: 0.030 | Tree loss: 3.009 | Accuracy: 0.105469 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 025 | Total loss: 2.976 | Reg loss: 0.030 | Tree loss: 2.976 | Accuracy: 0.115234 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 025 | Total loss: 2.993 | Reg loss: 0.030 | Tree loss: 2.993 | Accuracy: 0.103516 | 6.42 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 025 | Total loss: 2.979 | Reg loss: 0.030 | Tree loss: 2.979 | Accuracy: 0.109677 | 6.419 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 025 | Total loss: 3.107 | Reg loss: 0.029 | Tree loss: 3.107 | Accuracy: 0.105469 | 6.419 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 025 | Total loss: 3.121 | Reg loss: 0.029 | Tree loss: 3.121 | Accuracy: 0.097656 | 6.419 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 025 | Total loss: 3.138 | Reg loss: 0.029 | Tree loss: 3.138 | Accuracy: 0.087891 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 025 | Total loss: 3.079 | Reg loss: 0.029 | Tree loss: 3.079 | Accuracy: 0.091797 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 025 | Total loss: 3.076 | Reg loss: 0.029 | Tree loss: 3.076 | Accuracy: 0.095703 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 025 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.097656 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 025 | Total loss: 3.067 | Reg loss: 0.029 | Tree loss: 3.067 | Accuracy: 0.119141 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 025 | Total loss: 3.172 | Reg loss: 0.029 | Tree loss: 3.172 | Accuracy: 0.097656 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 025 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.093750 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 025 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.113281 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 025 | Total loss: 3.026 | Reg loss: 0.030 | Tree loss: 3.026 | Accuracy: 0.087891 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 025 | Total loss: 2.966 | Reg loss: 0.030 | Tree loss: 2.966 | Accuracy: 0.115234 | 6.418 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 025 | Total loss: 3.039 | Reg loss: 0.030 | Tree loss: 3.039 | Accuracy: 0.109375 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 025 | Total loss: 3.028 | Reg loss: 0.030 | Tree loss: 3.028 | Accuracy: 0.119141 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 025 | Total loss: 3.015 | Reg loss: 0.030 | Tree loss: 3.015 | Accuracy: 0.083984 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 025 | Total loss: 3.009 | Reg loss: 0.030 | Tree loss: 3.009 | Accuracy: 0.123047 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 025 | Total loss: 2.997 | Reg loss: 0.030 | Tree loss: 2.997 | Accuracy: 0.099609 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 025 | Total loss: 2.967 | Reg loss: 0.030 | Tree loss: 2.967 | Accuracy: 0.085938 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 025 | Total loss: 2.961 | Reg loss: 0.030 | Tree loss: 2.961 | Accuracy: 0.105469 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 025 | Total loss: 2.976 | Reg loss: 0.030 | Tree loss: 2.976 | Accuracy: 0.123047 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 025 | Total loss: 2.977 | Reg loss: 0.030 | Tree loss: 2.977 | Accuracy: 0.083984 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 025 | Total loss: 2.925 | Reg loss: 0.030 | Tree loss: 2.925 | Accuracy: 0.115234 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 025 | Total loss: 3.005 | Reg loss: 0.030 | Tree loss: 3.005 | Accuracy: 0.099609 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 025 | Total loss: 2.957 | Reg loss: 0.030 | Tree loss: 2.957 | Accuracy: 0.103516 | 6.417 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 025 | Total loss: 2.996 | Reg loss: 0.030 | Tree loss: 2.996 | Accuracy: 0.088172 | 6.416 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 025 | Total loss: 3.158 | Reg loss: 0.029 | Tree loss: 3.158 | Accuracy: 0.107422 | 6.416 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 025 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.113281 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 025 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.111328 | 6.415 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 003 / 025 | Total loss: 3.091 | Reg loss: 0.029 | Tree loss: 3.091 | Accuracy: 0.103516 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 025 | Total loss: 3.112 | Reg loss: 0.029 | Tree loss: 3.112 | Accuracy: 0.105469 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 025 | Total loss: 3.107 | Reg loss: 0.029 | Tree loss: 3.107 | Accuracy: 0.105469 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 025 | Total loss: 3.030 | Reg loss: 0.029 | Tree loss: 3.030 | Accuracy: 0.107422 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 025 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.080078 | 6.415 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 025 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.117188 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 025 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.113281 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 025 | Total loss: 3.053 | Reg loss: 0.029 | Tree loss: 3.053 | Accuracy: 0.091797 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 025 | Total loss: 3.077 | Reg loss: 0.029 | Tree loss: 3.077 | Accuracy: 0.091797 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 025 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.076172 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 025 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.087891 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 025 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.119141 | 6.414 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 025 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.101562 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 025 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.087891 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 025 | Total loss: 2.979 | Reg loss: 0.029 | Tree loss: 2.979 | Accuracy: 0.107422 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 025 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.093750 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 025 | Total loss: 2.923 | Reg loss: 0.029 | Tree loss: 2.923 | Accuracy: 0.113281 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 025 | Total loss: 2.925 | Reg loss: 0.030 | Tree loss: 2.925 | Accuracy: 0.146484 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 025 | Total loss: 2.975 | Reg loss: 0.030 | Tree loss: 2.975 | Accuracy: 0.087891 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 025 | Total loss: 3.012 | Reg loss: 0.030 | Tree loss: 3.012 | Accuracy: 0.095703 | 6.413 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 025 | Total loss: 2.934 | Reg loss: 0.030 | Tree loss: 2.934 | Accuracy: 0.097656 | 6.412 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 025 | Total loss: 2.975 | Reg loss: 0.030 | Tree loss: 2.975 | Accuracy: 0.096774 | 6.411 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 025 | Total loss: 3.138 | Reg loss: 0.029 | Tree loss: 3.138 | Accuracy: 0.087891 | 6.411 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 025 | Total loss: 3.092 | Reg loss: 0.029 | Tree loss: 3.092 | Accuracy: 0.105469 | 6.411 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 025 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.097656 | 6.411 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 025 | Total loss: 3.091 | Reg loss: 0.029 | Tree loss: 3.091 | Accuracy: 0.115234 | 6.411 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 025 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.095703 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 025 | Total loss: 3.055 | Reg loss: 0.029 | Tree loss: 3.055 | Accuracy: 0.109375 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 025 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.089844 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 025 | Total loss: 3.073 | Reg loss: 0.029 | Tree loss: 3.073 | Accuracy: 0.101562 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 025 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.109375 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 025 | Total loss: 3.052 | Reg loss: 0.029 | Tree loss: 3.052 | Accuracy: 0.087891 | 6.41 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 025 | Total loss: 3.001 | Reg loss: 0.029 | Tree loss: 3.001 | Accuracy: 0.123047 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 025 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.111328 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 025 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.115234 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 025 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.093750 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 025 | Total loss: 2.958 | Reg loss: 0.029 | Tree loss: 2.958 | Accuracy: 0.115234 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 025 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.097656 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 025 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.089844 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 025 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.105469 | 6.409 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 025 | Total loss: 2.974 | Reg loss: 0.029 | Tree loss: 2.974 | Accuracy: 0.101562 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 025 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.082031 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 025 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.101562 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 025 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.103516 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 025 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.117188 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 025 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.101562 | 6.408 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 025 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.090323 | 6.407 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 025 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.105469 | 6.407 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 025 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.089844 | 6.407 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 025 | Total loss: 3.116 | Reg loss: 0.029 | Tree loss: 3.116 | Accuracy: 0.095703 | 6.407 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 025 | Total loss: 3.126 | Reg loss: 0.029 | Tree loss: 3.126 | Accuracy: 0.101562 | 6.407 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 025 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.107422 | 6.407 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 025 | Total loss: 3.057 | Reg loss: 0.029 | Tree loss: 3.057 | Accuracy: 0.103516 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 025 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.128906 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 025 | Total loss: 3.032 | Reg loss: 0.029 | Tree loss: 3.032 | Accuracy: 0.085938 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 025 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.103516 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 025 | Total loss: 3.018 | Reg loss: 0.029 | Tree loss: 3.018 | Accuracy: 0.101562 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 025 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.109375 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 025 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 0.123047 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 025 | Total loss: 3.046 | Reg loss: 0.029 | Tree loss: 3.046 | Accuracy: 0.083984 | 6.406 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 025 | Total loss: 3.023 | Reg loss: 0.029 | Tree loss: 3.023 | Accuracy: 0.101562 | 6.405 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 025 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.093750 | 6.405 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 015 / 025 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.093750 | 6.405 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.103516 | 6.405 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 025 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.091797 | 6.405 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 025 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.109375 | 6.405 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 025 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 0.121094 | 6.404 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 025 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.097656 | 6.404 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 025 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.089844 | 6.404 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 025 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.103516 | 6.404 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 025 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.080078 | 6.404 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 025 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.126882 | 6.403 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 025 | Total loss: 3.102 | Reg loss: 0.029 | Tree loss: 3.102 | Accuracy: 0.089844 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 025 | Total loss: 3.101 | Reg loss: 0.029 | Tree loss: 3.101 | Accuracy: 0.109375 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 025 | Total loss: 3.093 | Reg loss: 0.029 | Tree loss: 3.093 | Accuracy: 0.109375 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 025 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 0.107422 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 025 | Total loss: 3.070 | Reg loss: 0.029 | Tree loss: 3.070 | Accuracy: 0.091797 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 025 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.099609 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 025 | Total loss: 3.083 | Reg loss: 0.029 | Tree loss: 3.083 | Accuracy: 0.097656 | 6.403 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 025 | Total loss: 3.066 | Reg loss: 0.029 | Tree loss: 3.066 | Accuracy: 0.115234 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 025 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.099609 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 025 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.105469 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 025 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.115234 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 025 | Total loss: 3.071 | Reg loss: 0.029 | Tree loss: 3.071 | Accuracy: 0.082031 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 025 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.105469 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 025 | Total loss: 2.995 | Reg loss: 0.029 | Tree loss: 2.995 | Accuracy: 0.091797 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 025 | Total loss: 2.972 | Reg loss: 0.029 | Tree loss: 2.972 | Accuracy: 0.107422 | 6.402 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 025 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.121094 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 025 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.093750 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 025 | Total loss: 2.959 | Reg loss: 0.029 | Tree loss: 2.959 | Accuracy: 0.087891 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 025 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.095703 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.097656 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 025 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.099609 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 025 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.095703 | 6.401 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 025 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.101562 | 6.4 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 025 | Total loss: 2.980 | Reg loss: 0.029 | Tree loss: 2.980 | Accuracy: 0.113281 | 6.4 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 025 | Total loss: 2.985 | Reg loss: 0.029 | Tree loss: 2.985 | Accuracy: 0.111828 | 6.399 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 025 | Total loss: 3.109 | Reg loss: 0.029 | Tree loss: 3.109 | Accuracy: 0.095703 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 025 | Total loss: 3.058 | Reg loss: 0.029 | Tree loss: 3.058 | Accuracy: 0.072266 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 025 | Total loss: 3.083 | Reg loss: 0.029 | Tree loss: 3.083 | Accuracy: 0.115234 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 025 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.125000 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 025 | Total loss: 3.051 | Reg loss: 0.029 | Tree loss: 3.051 | Accuracy: 0.097656 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 025 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.082031 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 025 | Total loss: 3.052 | Reg loss: 0.029 | Tree loss: 3.052 | Accuracy: 0.103516 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 025 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.111328 | 6.399 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 025 | Total loss: 3.028 | Reg loss: 0.029 | Tree loss: 3.028 | Accuracy: 0.105469 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 025 | Total loss: 3.007 | Reg loss: 0.029 | Tree loss: 3.007 | Accuracy: 0.089844 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 025 | Total loss: 3.019 | Reg loss: 0.029 | Tree loss: 3.019 | Accuracy: 0.101562 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 025 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.097656 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 025 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.117188 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 025 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.107422 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 025 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 0.095703 | 6.398 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 025 | Total loss: 2.980 | Reg loss: 0.029 | Tree loss: 2.980 | Accuracy: 0.117188 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 025 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.105469 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 025 | Total loss: 2.989 | Reg loss: 0.029 | Tree loss: 2.989 | Accuracy: 0.107422 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 025 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.089844 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 025 | Total loss: 3.021 | Reg loss: 0.029 | Tree loss: 3.021 | Accuracy: 0.093750 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 025 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.095703 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 025 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.097656 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 025 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.105469 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 025 | Total loss: 2.964 | Reg loss: 0.029 | Tree loss: 2.964 | Accuracy: 0.103516 | 6.397 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 025 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.113978 | 6.396 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 000 / 025 | Total loss: 3.147 | Reg loss: 0.029 | Tree loss: 3.147 | Accuracy: 0.091797 | 6.396 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 025 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.130859 | 6.396 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 025 | Total loss: 3.073 | Reg loss: 0.029 | Tree loss: 3.073 | Accuracy: 0.095703 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 025 | Total loss: 3.028 | Reg loss: 0.029 | Tree loss: 3.028 | Accuracy: 0.117188 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 025 | Total loss: 3.129 | Reg loss: 0.029 | Tree loss: 3.129 | Accuracy: 0.105469 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 025 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.126953 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 025 | Total loss: 3.059 | Reg loss: 0.029 | Tree loss: 3.059 | Accuracy: 0.089844 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 025 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.099609 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 025 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.105469 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 025 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.103516 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 025 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.085938 | 6.395 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 025 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.091797 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 025 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.085938 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 025 | Total loss: 2.971 | Reg loss: 0.029 | Tree loss: 2.971 | Accuracy: 0.095703 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 025 | Total loss: 3.003 | Reg loss: 0.029 | Tree loss: 3.003 | Accuracy: 0.107422 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 025 | Total loss: 2.949 | Reg loss: 0.029 | Tree loss: 2.949 | Accuracy: 0.091797 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 025 | Total loss: 3.032 | Reg loss: 0.029 | Tree loss: 3.032 | Accuracy: 0.115234 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.103516 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 025 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.121094 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 025 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.101562 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 025 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.078125 | 6.394 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 025 | Total loss: 2.950 | Reg loss: 0.029 | Tree loss: 2.950 | Accuracy: 0.103516 | 6.393 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 025 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.085938 | 6.393 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 025 | Total loss: 2.982 | Reg loss: 0.029 | Tree loss: 2.982 | Accuracy: 0.101562 | 6.393 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 025 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 0.109677 | 6.392 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 025 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.109375 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 025 | Total loss: 3.055 | Reg loss: 0.029 | Tree loss: 3.055 | Accuracy: 0.093750 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 025 | Total loss: 3.042 | Reg loss: 0.029 | Tree loss: 3.042 | Accuracy: 0.066406 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 025 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.117188 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 025 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.097656 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 025 | Total loss: 3.014 | Reg loss: 0.029 | Tree loss: 3.014 | Accuracy: 0.107422 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 025 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.101562 | 6.392 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 025 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.068359 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 025 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.126953 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 025 | Total loss: 3.020 | Reg loss: 0.029 | Tree loss: 3.020 | Accuracy: 0.089844 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 025 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.089844 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 025 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.091797 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 025 | Total loss: 3.057 | Reg loss: 0.029 | Tree loss: 3.057 | Accuracy: 0.105469 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.107422 | 6.391 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 025 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.117188 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 025 | Total loss: 3.015 | Reg loss: 0.029 | Tree loss: 3.015 | Accuracy: 0.105469 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 025 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.121094 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 025 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.076172 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 025 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.121094 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 025 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.097656 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 025 | Total loss: 2.975 | Reg loss: 0.029 | Tree loss: 2.975 | Accuracy: 0.105469 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 025 | Total loss: 2.956 | Reg loss: 0.029 | Tree loss: 2.956 | Accuracy: 0.126953 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 025 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.097656 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 025 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.103516 | 6.39 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 025 | Total loss: 2.916 | Reg loss: 0.029 | Tree loss: 2.916 | Accuracy: 0.101075 | 6.389 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 025 | Total loss: 3.087 | Reg loss: 0.029 | Tree loss: 3.087 | Accuracy: 0.089844 | 6.389 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 025 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.093750 | 6.389 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 025 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.103516 | 6.389 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 025 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.095703 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 025 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.099609 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 025 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.109375 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 025 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.080078 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 025 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.103516 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 025 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.119141 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 025 | Total loss: 3.080 | Reg loss: 0.029 | Tree loss: 3.080 | Accuracy: 0.085938 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 025 | Total loss: 3.029 | Reg loss: 0.029 | Tree loss: 3.029 | Accuracy: 0.115234 | 6.388 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 025 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.083984 | 6.388 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 012 / 025 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.128906 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 025 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.107422 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 025 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.095703 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 025 | Total loss: 3.032 | Reg loss: 0.029 | Tree loss: 3.032 | Accuracy: 0.091797 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 025 | Total loss: 3.002 | Reg loss: 0.029 | Tree loss: 3.002 | Accuracy: 0.115234 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 025 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.125000 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 025 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.097656 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 025 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.105469 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 025 | Total loss: 2.918 | Reg loss: 0.029 | Tree loss: 2.918 | Accuracy: 0.099609 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 025 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.115234 | 6.386 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 025 | Total loss: 2.958 | Reg loss: 0.029 | Tree loss: 2.958 | Accuracy: 0.074219 | 6.386 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 025 | Total loss: 2.916 | Reg loss: 0.029 | Tree loss: 2.916 | Accuracy: 0.115234 | 6.387 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 025 | Total loss: 2.902 | Reg loss: 0.029 | Tree loss: 2.902 | Accuracy: 0.094624 | 6.386 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 025 | Total loss: 3.050 | Reg loss: 0.029 | Tree loss: 3.050 | Accuracy: 0.125000 | 6.386 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 025 | Total loss: 3.051 | Reg loss: 0.029 | Tree loss: 3.051 | Accuracy: 0.107422 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 025 | Total loss: 3.051 | Reg loss: 0.029 | Tree loss: 3.051 | Accuracy: 0.101562 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 025 | Total loss: 3.104 | Reg loss: 0.029 | Tree loss: 3.104 | Accuracy: 0.089844 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 025 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.097656 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 025 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.109375 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 025 | Total loss: 3.054 | Reg loss: 0.029 | Tree loss: 3.054 | Accuracy: 0.119141 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 025 | Total loss: 3.038 | Reg loss: 0.029 | Tree loss: 3.038 | Accuracy: 0.091797 | 6.385 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 025 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.109375 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 025 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.080078 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 025 | Total loss: 3.031 | Reg loss: 0.029 | Tree loss: 3.031 | Accuracy: 0.107422 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 025 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.083984 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 025 | Total loss: 3.036 | Reg loss: 0.029 | Tree loss: 3.036 | Accuracy: 0.103516 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 025 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.109375 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 025 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.113281 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 025 | Total loss: 2.982 | Reg loss: 0.029 | Tree loss: 2.982 | Accuracy: 0.111328 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 025 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.097656 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 025 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.097656 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 025 | Total loss: 2.937 | Reg loss: 0.029 | Tree loss: 2.937 | Accuracy: 0.089844 | 6.384 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 025 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.113281 | 6.383 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 025 | Total loss: 2.968 | Reg loss: 0.029 | Tree loss: 2.968 | Accuracy: 0.105469 | 6.383 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 025 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.093750 | 6.383 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 025 | Total loss: 2.917 | Reg loss: 0.029 | Tree loss: 2.917 | Accuracy: 0.109375 | 6.383 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 025 | Total loss: 2.976 | Reg loss: 0.029 | Tree loss: 2.976 | Accuracy: 0.095703 | 6.383 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 025 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 0.079570 | 6.382 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 025 | Total loss: 3.076 | Reg loss: 0.029 | Tree loss: 3.076 | Accuracy: 0.078125 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 025 | Total loss: 3.124 | Reg loss: 0.029 | Tree loss: 3.124 | Accuracy: 0.105469 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 025 | Total loss: 3.039 | Reg loss: 0.029 | Tree loss: 3.039 | Accuracy: 0.101562 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 025 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.107422 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 025 | Total loss: 3.095 | Reg loss: 0.029 | Tree loss: 3.095 | Accuracy: 0.095703 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 025 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.113281 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 025 | Total loss: 3.044 | Reg loss: 0.029 | Tree loss: 3.044 | Accuracy: 0.097656 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 025 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 0.125000 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 025 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.109375 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 025 | Total loss: 2.952 | Reg loss: 0.029 | Tree loss: 2.952 | Accuracy: 0.097656 | 6.382 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 025 | Total loss: 3.025 | Reg loss: 0.029 | Tree loss: 3.025 | Accuracy: 0.107422 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 025 | Total loss: 3.037 | Reg loss: 0.029 | Tree loss: 3.037 | Accuracy: 0.072266 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 025 | Total loss: 2.970 | Reg loss: 0.029 | Tree loss: 2.970 | Accuracy: 0.099609 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 025 | Total loss: 3.064 | Reg loss: 0.029 | Tree loss: 3.064 | Accuracy: 0.093750 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 025 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.101562 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 025 | Total loss: 2.962 | Reg loss: 0.029 | Tree loss: 2.962 | Accuracy: 0.123047 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 025 | Total loss: 2.942 | Reg loss: 0.029 | Tree loss: 2.942 | Accuracy: 0.119141 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 025 | Total loss: 2.963 | Reg loss: 0.029 | Tree loss: 2.963 | Accuracy: 0.083984 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 025 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.109375 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 025 | Total loss: 3.006 | Reg loss: 0.029 | Tree loss: 3.006 | Accuracy: 0.091797 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 025 | Total loss: 2.933 | Reg loss: 0.029 | Tree loss: 2.933 | Accuracy: 0.095703 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 025 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.109375 | 6.381 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 025 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 0.115234 | 6.38 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 025 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.087891 | 6.38 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 024 / 025 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.105376 | 6.38 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 025 | Total loss: 3.094 | Reg loss: 0.028 | Tree loss: 3.094 | Accuracy: 0.134766 | 6.38 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 025 | Total loss: 3.079 | Reg loss: 0.028 | Tree loss: 3.079 | Accuracy: 0.107422 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 025 | Total loss: 3.107 | Reg loss: 0.028 | Tree loss: 3.107 | Accuracy: 0.089844 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 025 | Total loss: 3.082 | Reg loss: 0.028 | Tree loss: 3.082 | Accuracy: 0.103516 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 025 | Total loss: 3.015 | Reg loss: 0.028 | Tree loss: 3.015 | Accuracy: 0.107422 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 025 | Total loss: 3.038 | Reg loss: 0.028 | Tree loss: 3.038 | Accuracy: 0.083984 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 025 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.089844 | 6.379 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 025 | Total loss: 3.029 | Reg loss: 0.028 | Tree loss: 3.029 | Accuracy: 0.107422 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 025 | Total loss: 2.980 | Reg loss: 0.028 | Tree loss: 2.980 | Accuracy: 0.107422 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 025 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.115234 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 025 | Total loss: 3.019 | Reg loss: 0.028 | Tree loss: 3.019 | Accuracy: 0.103516 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 025 | Total loss: 3.035 | Reg loss: 0.028 | Tree loss: 3.035 | Accuracy: 0.093750 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 025 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 0.091797 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 025 | Total loss: 2.967 | Reg loss: 0.028 | Tree loss: 2.967 | Accuracy: 0.134766 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 025 | Total loss: 2.894 | Reg loss: 0.028 | Tree loss: 2.894 | Accuracy: 0.119141 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 025 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.105469 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 025 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.105469 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 025 | Total loss: 3.011 | Reg loss: 0.029 | Tree loss: 3.011 | Accuracy: 0.099609 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 025 | Total loss: 2.971 | Reg loss: 0.029 | Tree loss: 2.971 | Accuracy: 0.083984 | 6.378 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 025 | Total loss: 2.933 | Reg loss: 0.029 | Tree loss: 2.933 | Accuracy: 0.064453 | 6.377 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 025 | Total loss: 2.984 | Reg loss: 0.029 | Tree loss: 2.984 | Accuracy: 0.095703 | 6.377 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 025 | Total loss: 2.987 | Reg loss: 0.029 | Tree loss: 2.987 | Accuracy: 0.095703 | 6.377 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 025 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.103516 | 6.377 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 025 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.105469 | 6.377 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 025 | Total loss: 2.977 | Reg loss: 0.029 | Tree loss: 2.977 | Accuracy: 0.098925 | 6.376 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 025 | Total loss: 3.077 | Reg loss: 0.028 | Tree loss: 3.077 | Accuracy: 0.082031 | 6.376 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 025 | Total loss: 3.072 | Reg loss: 0.028 | Tree loss: 3.072 | Accuracy: 0.109375 | 6.376 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 025 | Total loss: 3.062 | Reg loss: 0.028 | Tree loss: 3.062 | Accuracy: 0.107422 | 6.376 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 025 | Total loss: 3.084 | Reg loss: 0.028 | Tree loss: 3.084 | Accuracy: 0.128906 | 6.376 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 025 | Total loss: 3.039 | Reg loss: 0.028 | Tree loss: 3.039 | Accuracy: 0.101562 | 6.376 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 025 | Total loss: 3.050 | Reg loss: 0.028 | Tree loss: 3.050 | Accuracy: 0.095703 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 025 | Total loss: 3.006 | Reg loss: 0.028 | Tree loss: 3.006 | Accuracy: 0.097656 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 025 | Total loss: 2.993 | Reg loss: 0.028 | Tree loss: 2.993 | Accuracy: 0.103516 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 025 | Total loss: 3.006 | Reg loss: 0.028 | Tree loss: 3.006 | Accuracy: 0.107422 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 025 | Total loss: 3.052 | Reg loss: 0.028 | Tree loss: 3.052 | Accuracy: 0.087891 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 025 | Total loss: 3.014 | Reg loss: 0.028 | Tree loss: 3.014 | Accuracy: 0.119141 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 025 | Total loss: 2.963 | Reg loss: 0.028 | Tree loss: 2.963 | Accuracy: 0.113281 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 025 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 0.103516 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 025 | Total loss: 2.971 | Reg loss: 0.028 | Tree loss: 2.971 | Accuracy: 0.078125 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 025 | Total loss: 2.984 | Reg loss: 0.028 | Tree loss: 2.984 | Accuracy: 0.097656 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 025 | Total loss: 2.991 | Reg loss: 0.028 | Tree loss: 2.991 | Accuracy: 0.101562 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 025 | Total loss: 2.960 | Reg loss: 0.028 | Tree loss: 2.960 | Accuracy: 0.123047 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 025 | Total loss: 2.959 | Reg loss: 0.028 | Tree loss: 2.959 | Accuracy: 0.109375 | 6.375 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 025 | Total loss: 2.982 | Reg loss: 0.028 | Tree loss: 2.982 | Accuracy: 0.099609 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 025 | Total loss: 2.927 | Reg loss: 0.028 | Tree loss: 2.927 | Accuracy: 0.105469 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 025 | Total loss: 2.946 | Reg loss: 0.028 | Tree loss: 2.946 | Accuracy: 0.097656 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 025 | Total loss: 2.942 | Reg loss: 0.028 | Tree loss: 2.942 | Accuracy: 0.095703 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 025 | Total loss: 2.940 | Reg loss: 0.029 | Tree loss: 2.940 | Accuracy: 0.095703 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 025 | Total loss: 2.929 | Reg loss: 0.029 | Tree loss: 2.929 | Accuracy: 0.093750 | 6.374 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 025 | Total loss: 2.996 | Reg loss: 0.029 | Tree loss: 2.996 | Accuracy: 0.092473 | 6.373 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 025 | Total loss: 3.035 | Reg loss: 0.028 | Tree loss: 3.035 | Accuracy: 0.107422 | 6.373 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 025 | Total loss: 3.037 | Reg loss: 0.028 | Tree loss: 3.037 | Accuracy: 0.115234 | 6.373 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 025 | Total loss: 3.072 | Reg loss: 0.028 | Tree loss: 3.072 | Accuracy: 0.089844 | 6.373 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 025 | Total loss: 3.090 | Reg loss: 0.028 | Tree loss: 3.090 | Accuracy: 0.105469 | 6.373 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 025 | Total loss: 3.045 | Reg loss: 0.028 | Tree loss: 3.045 | Accuracy: 0.117188 | 6.373 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 025 | Total loss: 3.026 | Reg loss: 0.028 | Tree loss: 3.026 | Accuracy: 0.111328 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 025 | Total loss: 3.044 | Reg loss: 0.028 | Tree loss: 3.044 | Accuracy: 0.101562 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 025 | Total loss: 3.010 | Reg loss: 0.028 | Tree loss: 3.010 | Accuracy: 0.093750 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 025 | Total loss: 3.017 | Reg loss: 0.028 | Tree loss: 3.017 | Accuracy: 0.097656 | 6.372 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 009 / 025 | Total loss: 3.056 | Reg loss: 0.028 | Tree loss: 3.056 | Accuracy: 0.093750 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 025 | Total loss: 3.006 | Reg loss: 0.028 | Tree loss: 3.006 | Accuracy: 0.105469 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 025 | Total loss: 2.986 | Reg loss: 0.028 | Tree loss: 2.986 | Accuracy: 0.109375 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 025 | Total loss: 3.008 | Reg loss: 0.028 | Tree loss: 3.008 | Accuracy: 0.072266 | 6.372 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 025 | Total loss: 3.000 | Reg loss: 0.028 | Tree loss: 3.000 | Accuracy: 0.091797 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 025 | Total loss: 3.014 | Reg loss: 0.028 | Tree loss: 3.014 | Accuracy: 0.082031 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 025 | Total loss: 2.980 | Reg loss: 0.028 | Tree loss: 2.980 | Accuracy: 0.132812 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 025 | Total loss: 2.960 | Reg loss: 0.028 | Tree loss: 2.960 | Accuracy: 0.125000 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 025 | Total loss: 2.996 | Reg loss: 0.028 | Tree loss: 2.996 | Accuracy: 0.068359 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 025 | Total loss: 2.921 | Reg loss: 0.028 | Tree loss: 2.921 | Accuracy: 0.097656 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 025 | Total loss: 2.988 | Reg loss: 0.028 | Tree loss: 2.988 | Accuracy: 0.107422 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 025 | Total loss: 2.934 | Reg loss: 0.028 | Tree loss: 2.934 | Accuracy: 0.105469 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 025 | Total loss: 2.917 | Reg loss: 0.028 | Tree loss: 2.917 | Accuracy: 0.117188 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 025 | Total loss: 2.979 | Reg loss: 0.028 | Tree loss: 2.979 | Accuracy: 0.080078 | 6.371 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 025 | Total loss: 2.914 | Reg loss: 0.028 | Tree loss: 2.914 | Accuracy: 0.103516 | 6.37 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 025 | Total loss: 2.908 | Reg loss: 0.028 | Tree loss: 2.908 | Accuracy: 0.118280 | 6.37 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 025 | Total loss: 3.049 | Reg loss: 0.028 | Tree loss: 3.049 | Accuracy: 0.123047 | 6.37 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 025 | Total loss: 3.019 | Reg loss: 0.028 | Tree loss: 3.019 | Accuracy: 0.105469 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 025 | Total loss: 3.028 | Reg loss: 0.028 | Tree loss: 3.028 | Accuracy: 0.121094 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 025 | Total loss: 3.061 | Reg loss: 0.028 | Tree loss: 3.061 | Accuracy: 0.089844 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 025 | Total loss: 3.042 | Reg loss: 0.028 | Tree loss: 3.042 | Accuracy: 0.091797 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 025 | Total loss: 2.997 | Reg loss: 0.028 | Tree loss: 2.997 | Accuracy: 0.109375 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 025 | Total loss: 3.048 | Reg loss: 0.028 | Tree loss: 3.048 | Accuracy: 0.087891 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 025 | Total loss: 3.029 | Reg loss: 0.028 | Tree loss: 3.029 | Accuracy: 0.117188 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 025 | Total loss: 3.060 | Reg loss: 0.028 | Tree loss: 3.060 | Accuracy: 0.101562 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 025 | Total loss: 2.980 | Reg loss: 0.028 | Tree loss: 2.980 | Accuracy: 0.107422 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 025 | Total loss: 2.962 | Reg loss: 0.028 | Tree loss: 2.962 | Accuracy: 0.095703 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 025 | Total loss: 3.021 | Reg loss: 0.028 | Tree loss: 3.021 | Accuracy: 0.115234 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 025 | Total loss: 2.960 | Reg loss: 0.028 | Tree loss: 2.960 | Accuracy: 0.109375 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 025 | Total loss: 2.939 | Reg loss: 0.028 | Tree loss: 2.939 | Accuracy: 0.095703 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 025 | Total loss: 2.968 | Reg loss: 0.028 | Tree loss: 2.968 | Accuracy: 0.089844 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 025 | Total loss: 2.978 | Reg loss: 0.028 | Tree loss: 2.978 | Accuracy: 0.089844 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 025 | Total loss: 2.960 | Reg loss: 0.028 | Tree loss: 2.960 | Accuracy: 0.091797 | 6.369 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 025 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 0.093750 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 025 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.099609 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 025 | Total loss: 3.015 | Reg loss: 0.028 | Tree loss: 3.015 | Accuracy: 0.087891 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 025 | Total loss: 2.970 | Reg loss: 0.028 | Tree loss: 2.970 | Accuracy: 0.109375 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 025 | Total loss: 2.933 | Reg loss: 0.028 | Tree loss: 2.933 | Accuracy: 0.103516 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 025 | Total loss: 2.928 | Reg loss: 0.028 | Tree loss: 2.928 | Accuracy: 0.101562 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 025 | Total loss: 2.976 | Reg loss: 0.028 | Tree loss: 2.976 | Accuracy: 0.091797 | 6.368 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 025 | Total loss: 2.950 | Reg loss: 0.028 | Tree loss: 2.950 | Accuracy: 0.120430 | 6.367 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 025 | Total loss: 3.100 | Reg loss: 0.028 | Tree loss: 3.100 | Accuracy: 0.105469 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 025 | Total loss: 3.029 | Reg loss: 0.028 | Tree loss: 3.029 | Accuracy: 0.111328 | 6.368 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 025 | Total loss: 3.027 | Reg loss: 0.028 | Tree loss: 3.027 | Accuracy: 0.087891 | 6.368 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 025 | Total loss: 3.052 | Reg loss: 0.028 | Tree loss: 3.052 | Accuracy: 0.095703 | 6.368 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 025 | Total loss: 2.968 | Reg loss: 0.028 | Tree loss: 2.968 | Accuracy: 0.105469 | 6.368 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 025 | Total loss: 3.075 | Reg loss: 0.028 | Tree loss: 3.075 | Accuracy: 0.087891 | 6.368 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 025 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.125000 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 025 | Total loss: 3.007 | Reg loss: 0.028 | Tree loss: 3.007 | Accuracy: 0.103516 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 025 | Total loss: 3.001 | Reg loss: 0.028 | Tree loss: 3.001 | Accuracy: 0.101562 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 025 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.099609 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 025 | Total loss: 3.000 | Reg loss: 0.028 | Tree loss: 3.000 | Accuracy: 0.099609 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 025 | Total loss: 3.019 | Reg loss: 0.028 | Tree loss: 3.019 | Accuracy: 0.093750 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 025 | Total loss: 3.034 | Reg loss: 0.028 | Tree loss: 3.034 | Accuracy: 0.085938 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 025 | Total loss: 2.973 | Reg loss: 0.028 | Tree loss: 2.973 | Accuracy: 0.087891 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 025 | Total loss: 2.961 | Reg loss: 0.028 | Tree loss: 2.961 | Accuracy: 0.091797 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 025 | Total loss: 2.996 | Reg loss: 0.028 | Tree loss: 2.996 | Accuracy: 0.119141 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 025 | Total loss: 2.918 | Reg loss: 0.028 | Tree loss: 2.918 | Accuracy: 0.136719 | 6.366 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 025 | Total loss: 2.989 | Reg loss: 0.028 | Tree loss: 2.989 | Accuracy: 0.101562 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 025 | Total loss: 2.931 | Reg loss: 0.028 | Tree loss: 2.931 | Accuracy: 0.101562 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 025 | Total loss: 2.956 | Reg loss: 0.028 | Tree loss: 2.956 | Accuracy: 0.119141 | 6.367 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 025 | Total loss: 2.973 | Reg loss: 0.028 | Tree loss: 2.973 | Accuracy: 0.126953 | 6.367 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 021 / 025 | Total loss: 3.009 | Reg loss: 0.028 | Tree loss: 3.009 | Accuracy: 0.085938 | 6.366 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 025 | Total loss: 2.968 | Reg loss: 0.028 | Tree loss: 2.968 | Accuracy: 0.099609 | 6.366 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 025 | Total loss: 2.915 | Reg loss: 0.028 | Tree loss: 2.915 | Accuracy: 0.082031 | 6.366 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 025 | Total loss: 2.906 | Reg loss: 0.028 | Tree loss: 2.906 | Accuracy: 0.092473 | 6.365 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 025 | Total loss: 3.064 | Reg loss: 0.028 | Tree loss: 3.064 | Accuracy: 0.107422 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 025 | Total loss: 3.066 | Reg loss: 0.028 | Tree loss: 3.066 | Accuracy: 0.107422 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 025 | Total loss: 3.072 | Reg loss: 0.028 | Tree loss: 3.072 | Accuracy: 0.121094 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 025 | Total loss: 3.021 | Reg loss: 0.028 | Tree loss: 3.021 | Accuracy: 0.119141 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 025 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.105469 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 025 | Total loss: 3.011 | Reg loss: 0.028 | Tree loss: 3.011 | Accuracy: 0.111328 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 025 | Total loss: 2.986 | Reg loss: 0.028 | Tree loss: 2.986 | Accuracy: 0.107422 | 6.366 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 025 | Total loss: 2.999 | Reg loss: 0.028 | Tree loss: 2.999 | Accuracy: 0.085938 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 025 | Total loss: 3.027 | Reg loss: 0.028 | Tree loss: 3.027 | Accuracy: 0.093750 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 025 | Total loss: 3.002 | Reg loss: 0.028 | Tree loss: 3.002 | Accuracy: 0.111328 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 025 | Total loss: 3.038 | Reg loss: 0.028 | Tree loss: 3.038 | Accuracy: 0.083984 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 025 | Total loss: 2.991 | Reg loss: 0.028 | Tree loss: 2.991 | Accuracy: 0.109375 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 025 | Total loss: 2.974 | Reg loss: 0.028 | Tree loss: 2.974 | Accuracy: 0.111328 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 025 | Total loss: 2.976 | Reg loss: 0.028 | Tree loss: 2.976 | Accuracy: 0.105469 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 025 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.101562 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 025 | Total loss: 2.991 | Reg loss: 0.028 | Tree loss: 2.991 | Accuracy: 0.105469 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 025 | Total loss: 3.005 | Reg loss: 0.028 | Tree loss: 3.005 | Accuracy: 0.095703 | 6.365 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 025 | Total loss: 2.964 | Reg loss: 0.028 | Tree loss: 2.964 | Accuracy: 0.083984 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 025 | Total loss: 2.930 | Reg loss: 0.028 | Tree loss: 2.930 | Accuracy: 0.107422 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 025 | Total loss: 2.986 | Reg loss: 0.028 | Tree loss: 2.986 | Accuracy: 0.087891 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 025 | Total loss: 2.963 | Reg loss: 0.028 | Tree loss: 2.963 | Accuracy: 0.091797 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 025 | Total loss: 2.935 | Reg loss: 0.028 | Tree loss: 2.935 | Accuracy: 0.095703 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 025 | Total loss: 2.979 | Reg loss: 0.028 | Tree loss: 2.979 | Accuracy: 0.089844 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 025 | Total loss: 2.982 | Reg loss: 0.028 | Tree loss: 2.982 | Accuracy: 0.113281 | 6.364 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 025 | Total loss: 2.896 | Reg loss: 0.028 | Tree loss: 2.896 | Accuracy: 0.094624 | 6.363 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 025 | Total loss: 3.049 | Reg loss: 0.028 | Tree loss: 3.049 | Accuracy: 0.111328 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 025 | Total loss: 3.089 | Reg loss: 0.028 | Tree loss: 3.089 | Accuracy: 0.091797 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 025 | Total loss: 2.986 | Reg loss: 0.028 | Tree loss: 2.986 | Accuracy: 0.109375 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 025 | Total loss: 3.013 | Reg loss: 0.028 | Tree loss: 3.013 | Accuracy: 0.097656 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 025 | Total loss: 3.056 | Reg loss: 0.028 | Tree loss: 3.056 | Accuracy: 0.097656 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 025 | Total loss: 2.983 | Reg loss: 0.028 | Tree loss: 2.983 | Accuracy: 0.105469 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 025 | Total loss: 3.018 | Reg loss: 0.028 | Tree loss: 3.018 | Accuracy: 0.103516 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 025 | Total loss: 3.067 | Reg loss: 0.028 | Tree loss: 3.067 | Accuracy: 0.093750 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 025 | Total loss: 2.975 | Reg loss: 0.028 | Tree loss: 2.975 | Accuracy: 0.087891 | 6.363 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 025 | Total loss: 3.027 | Reg loss: 0.028 | Tree loss: 3.027 | Accuracy: 0.095703 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 025 | Total loss: 2.996 | Reg loss: 0.028 | Tree loss: 2.996 | Accuracy: 0.132812 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 025 | Total loss: 3.013 | Reg loss: 0.028 | Tree loss: 3.013 | Accuracy: 0.107422 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 025 | Total loss: 2.981 | Reg loss: 0.028 | Tree loss: 2.981 | Accuracy: 0.107422 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 025 | Total loss: 3.053 | Reg loss: 0.028 | Tree loss: 3.053 | Accuracy: 0.107422 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 025 | Total loss: 3.043 | Reg loss: 0.028 | Tree loss: 3.043 | Accuracy: 0.105469 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 025 | Total loss: 2.983 | Reg loss: 0.028 | Tree loss: 2.983 | Accuracy: 0.093750 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 025 | Total loss: 2.990 | Reg loss: 0.028 | Tree loss: 2.990 | Accuracy: 0.103516 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 025 | Total loss: 2.928 | Reg loss: 0.028 | Tree loss: 2.928 | Accuracy: 0.101562 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 025 | Total loss: 2.900 | Reg loss: 0.028 | Tree loss: 2.900 | Accuracy: 0.101562 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 025 | Total loss: 2.958 | Reg loss: 0.028 | Tree loss: 2.958 | Accuracy: 0.093750 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 025 | Total loss: 2.911 | Reg loss: 0.028 | Tree loss: 2.911 | Accuracy: 0.078125 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 025 | Total loss: 2.932 | Reg loss: 0.028 | Tree loss: 2.932 | Accuracy: 0.111328 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 025 | Total loss: 2.913 | Reg loss: 0.028 | Tree loss: 2.913 | Accuracy: 0.103516 | 6.362 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 025 | Total loss: 2.991 | Reg loss: 0.028 | Tree loss: 2.991 | Accuracy: 0.097656 | 6.361 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 025 | Total loss: 2.966 | Reg loss: 0.028 | Tree loss: 2.966 | Accuracy: 0.109677 | 6.36 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3105f135c9764e1e8259460deed3a7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5806c39c6147a5ac0dc1bb4624e4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 9.966666666666667\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 990\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12753\n",
      "============== Pattern 46 | comprehensibility: 42 ==============\n",
      "[-0.22315509617328644 * whole milk + -0.1576598435640335 * yogurt >= tensor(5.8347), -0.22315509617328644 * whole milk + -0.1576598435640335 * yogurt <= tensor(5.8356), 0.22214579582214355 * whole milk + 0.15752269327640533 * yogurt >= tensor(0.0029), 0.22214579582214355 * whole milk + 0.15752269327640533 * yogurt <= tensor(0.3817), -0.22095395624637604 * whole milk + -0.15674421191215515 * yogurt >= tensor(5.8491), -0.22095395624637604 * whole milk + -0.15674421191215515 * yogurt <= tensor(5.8500), 0.21899883449077606 * whole milk + 0.42691516876220703 * rolls/buns >= tensor(0.0030), 0.21899883449077606 * whole milk + 0.42691516876220703 * rolls/buns <= tensor(0.6474), -0.15359069406986237 * yogurt + -0.3081378936767578 * soda >= tensor(5.8474), -0.15359069406986237 * yogurt + -0.3081378936767578 * soda <= tensor(5.8485), -0.20576642453670502 * whole milk + -0.14803259074687958 * yogurt >= tensor(5.8460), -0.20576642453670502 * whole milk + -0.14803259074687958 * yogurt <= tensor(5.8468), 0.18908287584781647 * whole milk + 0.42224663496017456 * rolls/buns >= tensor(0.0030), 0.18908287584781647 * whole milk + 0.42224663496017456 * rolls/buns <= tensor(0.6130), 2.7721805572509766 * whole milk + 2.4291470050811768 * yogurt + 1.9198017120361328 * brown bread >= tensor(0.6931), 2.7721805572509766 * whole milk + 2.4291470050811768 * yogurt + 1.9198017120361328 * brown bread <= tensor(7.1219), 1.64939284324646 * rolls/buns + 1.7313936948776245 * soda >= tensor(0.0306), 1.64939284324646 * rolls/buns + 1.7313936948776245 * soda <= tensor(3.3818), 0.056681398302316666 * whole milk + 0.21955011785030365 * canned beer >= tensor(0.0026), 0.056681398302316666 * whole milk + 0.21955011785030365 * canned beer <= tensor(0.2782)]\n",
      "\n",
      "\n",
      "\n",
      "Average comprehensibility: 0.04242424242424243\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    if len(conds) == 0:\n",
    "        continue\n",
    "    \n",
    "    comp = sum([cond.comprehensibility for cond in conds])\n",
    "    print(f\"============== Pattern {pattern_counter + 1} | comprehensibility: {comp} ==============\")\n",
    "    print(conds)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    sum_comprehensibility += comp\n",
    "    \n",
    "print(f\"Average comprehensibility: {sum_comprehensibility / len(leaves)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
