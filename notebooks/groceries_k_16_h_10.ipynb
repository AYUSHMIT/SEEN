{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.220763206481934 | KNN Loss: 6.225732326507568 | BCE Loss: 1.9950306415557861\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.19043254852295 | KNN Loss: 6.225588321685791 | BCE Loss: 1.9648442268371582\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.231579780578613 | KNN Loss: 6.225534439086914 | BCE Loss: 2.006045341491699\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.17219352722168 | KNN Loss: 6.225142955780029 | BCE Loss: 1.947050929069519\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.164810180664062 | KNN Loss: 6.2255425453186035 | BCE Loss: 1.939267635345459\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.082189559936523 | KNN Loss: 6.224427700042725 | BCE Loss: 1.857762098312378\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.12572956085205 | KNN Loss: 6.224025726318359 | BCE Loss: 1.9017040729522705\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.135915756225586 | KNN Loss: 6.2237677574157715 | BCE Loss: 1.9121475219726562\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.112762451171875 | KNN Loss: 6.223908424377441 | BCE Loss: 1.8888545036315918\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.116373062133789 | KNN Loss: 6.223422050476074 | BCE Loss: 1.8929510116577148\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.10087776184082 | KNN Loss: 6.223198890686035 | BCE Loss: 1.8776787519454956\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.16634464263916 | KNN Loss: 6.222334861755371 | BCE Loss: 1.9440100193023682\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.14006233215332 | KNN Loss: 6.221959114074707 | BCE Loss: 1.918102741241455\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.110417366027832 | KNN Loss: 6.221432685852051 | BCE Loss: 1.88898503780365\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.098997116088867 | KNN Loss: 6.221719264984131 | BCE Loss: 1.8772780895233154\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.064451217651367 | KNN Loss: 6.2205810546875 | BCE Loss: 1.843869686126709\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.038871765136719 | KNN Loss: 6.220208168029785 | BCE Loss: 1.8186635971069336\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.06745719909668 | KNN Loss: 6.21963357925415 | BCE Loss: 1.8478240966796875\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 7.9834442138671875 | KNN Loss: 6.2177534103393555 | BCE Loss: 1.765690803527832\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.02696704864502 | KNN Loss: 6.218084335327148 | BCE Loss: 1.8088829517364502\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.017487525939941 | KNN Loss: 6.2174177169799805 | BCE Loss: 1.800069808959961\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.068255424499512 | KNN Loss: 6.216278553009033 | BCE Loss: 1.8519765138626099\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.067914009094238 | KNN Loss: 6.215011119842529 | BCE Loss: 1.8529032468795776\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.003668785095215 | KNN Loss: 6.214148998260498 | BCE Loss: 1.7895199060440063\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.986358642578125 | KNN Loss: 6.214023113250732 | BCE Loss: 1.772335410118103\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.986522197723389 | KNN Loss: 6.210351467132568 | BCE Loss: 1.7761707305908203\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 8.0083589553833 | KNN Loss: 6.209832191467285 | BCE Loss: 1.7985267639160156\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.004103660583496 | KNN Loss: 6.207260608673096 | BCE Loss: 1.7968430519104004\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.990250587463379 | KNN Loss: 6.205795764923096 | BCE Loss: 1.7844550609588623\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.972167491912842 | KNN Loss: 6.203283786773682 | BCE Loss: 1.7688835859298706\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.981219291687012 | KNN Loss: 6.201108455657959 | BCE Loss: 1.7801105976104736\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.94796085357666 | KNN Loss: 6.195968151092529 | BCE Loss: 1.7519924640655518\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.908747673034668 | KNN Loss: 6.195226192474365 | BCE Loss: 1.7135214805603027\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.935989856719971 | KNN Loss: 6.194324970245361 | BCE Loss: 1.7416647672653198\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.916643142700195 | KNN Loss: 6.1900248527526855 | BCE Loss: 1.7266184091567993\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.872467041015625 | KNN Loss: 6.183825492858887 | BCE Loss: 1.6886417865753174\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.865463733673096 | KNN Loss: 6.177277565002441 | BCE Loss: 1.6881862878799438\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.832812309265137 | KNN Loss: 6.167090892791748 | BCE Loss: 1.6657214164733887\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.848292827606201 | KNN Loss: 6.169102668762207 | BCE Loss: 1.6791900396347046\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.852560043334961 | KNN Loss: 6.161231517791748 | BCE Loss: 1.691328763961792\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.789666652679443 | KNN Loss: 6.150573253631592 | BCE Loss: 1.6390933990478516\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.726648330688477 | KNN Loss: 6.133987903594971 | BCE Loss: 1.592660665512085\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.746825218200684 | KNN Loss: 6.1239705085754395 | BCE Loss: 1.6228547096252441\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.721193790435791 | KNN Loss: 6.112710475921631 | BCE Loss: 1.6084831953048706\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.685835361480713 | KNN Loss: 6.1034369468688965 | BCE Loss: 1.5823984146118164\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.618278980255127 | KNN Loss: 6.070693016052246 | BCE Loss: 1.5475860834121704\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.568667411804199 | KNN Loss: 6.055474281311035 | BCE Loss: 1.5131932497024536\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.592131614685059 | KNN Loss: 6.053181171417236 | BCE Loss: 1.5389502048492432\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.545773506164551 | KNN Loss: 6.032601833343506 | BCE Loss: 1.513171672821045\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.553314685821533 | KNN Loss: 6.022616386413574 | BCE Loss: 1.530698299407959\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.492358207702637 | KNN Loss: 5.972041130065918 | BCE Loss: 1.5203173160552979\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.4415059089660645 | KNN Loss: 5.949999809265137 | BCE Loss: 1.4915060997009277\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.333465576171875 | KNN Loss: 5.874408721923828 | BCE Loss: 1.459057092666626\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.2568206787109375 | KNN Loss: 5.840343952178955 | BCE Loss: 1.4164766073226929\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.223727226257324 | KNN Loss: 5.812396049499512 | BCE Loss: 1.4113309383392334\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.155768871307373 | KNN Loss: 5.742559432983398 | BCE Loss: 1.4132095575332642\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.046815872192383 | KNN Loss: 5.683277130126953 | BCE Loss: 1.3635387420654297\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 6.992843151092529 | KNN Loss: 5.628707408905029 | BCE Loss: 1.3641357421875\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 6.841303825378418 | KNN Loss: 5.51348876953125 | BCE Loss: 1.3278148174285889\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 6.702325820922852 | KNN Loss: 5.38037633895874 | BCE Loss: 1.3219497203826904\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 6.575524806976318 | KNN Loss: 5.286498546600342 | BCE Loss: 1.2890263795852661\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.434109687805176 | KNN Loss: 5.178131580352783 | BCE Loss: 1.2559783458709717\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.290237903594971 | KNN Loss: 5.085316181182861 | BCE Loss: 1.2049217224121094\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.13742733001709 | KNN Loss: 4.923263072967529 | BCE Loss: 1.2141640186309814\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.042845249176025 | KNN Loss: 4.830077648162842 | BCE Loss: 1.212767481803894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 5.893028736114502 | KNN Loss: 4.714121341705322 | BCE Loss: 1.1789073944091797\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 5.67885160446167 | KNN Loss: 4.536438465118408 | BCE Loss: 1.1424131393432617\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 5.617745876312256 | KNN Loss: 4.462981224060059 | BCE Loss: 1.1547646522521973\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 5.47523307800293 | KNN Loss: 4.33170223236084 | BCE Loss: 1.1435308456420898\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 5.381700038909912 | KNN Loss: 4.258389949798584 | BCE Loss: 1.1233100891113281\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 5.270651817321777 | KNN Loss: 4.141578674316406 | BCE Loss: 1.129073143005371\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.185419082641602 | KNN Loss: 4.051194667816162 | BCE Loss: 1.13422429561615\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.1508612632751465 | KNN Loss: 4.007626533508301 | BCE Loss: 1.1432347297668457\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.065990447998047 | KNN Loss: 3.933425188064575 | BCE Loss: 1.1325650215148926\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 4.959907531738281 | KNN Loss: 3.860342025756836 | BCE Loss: 1.0995652675628662\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 4.908028602600098 | KNN Loss: 3.7992866039276123 | BCE Loss: 1.1087417602539062\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 4.829752445220947 | KNN Loss: 3.7236626148223877 | BCE Loss: 1.10608971118927\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 4.808707237243652 | KNN Loss: 3.691445827484131 | BCE Loss: 1.1172616481781006\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 4.766530513763428 | KNN Loss: 3.64591121673584 | BCE Loss: 1.120619297027588\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 4.7333173751831055 | KNN Loss: 3.607858180999756 | BCE Loss: 1.1254591941833496\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 4.677825927734375 | KNN Loss: 3.56062388420105 | BCE Loss: 1.117201805114746\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 4.688526630401611 | KNN Loss: 3.580760955810547 | BCE Loss: 1.1077656745910645\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 4.572175979614258 | KNN Loss: 3.514948844909668 | BCE Loss: 1.057227373123169\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.599862098693848 | KNN Loss: 3.5276966094970703 | BCE Loss: 1.0721654891967773\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.598022937774658 | KNN Loss: 3.4895646572113037 | BCE Loss: 1.108458399772644\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.583164215087891 | KNN Loss: 3.502086877822876 | BCE Loss: 1.0810775756835938\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.48114538192749 | KNN Loss: 3.399352550506592 | BCE Loss: 1.0817927122116089\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.49936580657959 | KNN Loss: 3.4190332889556885 | BCE Loss: 1.0803323984146118\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.499886989593506 | KNN Loss: 3.4009008407592773 | BCE Loss: 1.098986268043518\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.460353851318359 | KNN Loss: 3.3686721324920654 | BCE Loss: 1.0916814804077148\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.4285478591918945 | KNN Loss: 3.3677542209625244 | BCE Loss: 1.0607936382293701\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.403433799743652 | KNN Loss: 3.323626756668091 | BCE Loss: 1.0798068046569824\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.476748466491699 | KNN Loss: 3.398576259613037 | BCE Loss: 1.0781723260879517\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.369770050048828 | KNN Loss: 3.296827793121338 | BCE Loss: 1.0729422569274902\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.4384918212890625 | KNN Loss: 3.3682994842529297 | BCE Loss: 1.0701923370361328\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.354221343994141 | KNN Loss: 3.280104160308838 | BCE Loss: 1.0741170644760132\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.404528617858887 | KNN Loss: 3.320389747619629 | BCE Loss: 1.084139108657837\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.373483180999756 | KNN Loss: 3.3054511547088623 | BCE Loss: 1.068031907081604\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.389134407043457 | KNN Loss: 3.3129663467407227 | BCE Loss: 1.0761678218841553\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.366809844970703 | KNN Loss: 3.322087287902832 | BCE Loss: 1.0447224378585815\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.382287502288818 | KNN Loss: 3.297107696533203 | BCE Loss: 1.0851799249649048\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.42507791519165 | KNN Loss: 3.339235544204712 | BCE Loss: 1.0858423709869385\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.346248626708984 | KNN Loss: 3.2911243438720703 | BCE Loss: 1.0551241636276245\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.327643394470215 | KNN Loss: 3.275541067123413 | BCE Loss: 1.0521022081375122\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.3782806396484375 | KNN Loss: 3.280773878097534 | BCE Loss: 1.0975069999694824\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.371992111206055 | KNN Loss: 3.293651819229126 | BCE Loss: 1.0783401727676392\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.333858489990234 | KNN Loss: 3.271371364593506 | BCE Loss: 1.0624873638153076\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.283499240875244 | KNN Loss: 3.248767852783203 | BCE Loss: 1.0347315073013306\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.349672794342041 | KNN Loss: 3.2863919734954834 | BCE Loss: 1.063280701637268\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.308476448059082 | KNN Loss: 3.2594070434570312 | BCE Loss: 1.0490696430206299\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.325284481048584 | KNN Loss: 3.268341064453125 | BCE Loss: 1.056943416595459\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.328065872192383 | KNN Loss: 3.256955862045288 | BCE Loss: 1.0711102485656738\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.324325084686279 | KNN Loss: 3.274299383163452 | BCE Loss: 1.0500257015228271\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.3174896240234375 | KNN Loss: 3.259655714035034 | BCE Loss: 1.0578336715698242\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.259105205535889 | KNN Loss: 3.1895437240600586 | BCE Loss: 1.0695613622665405\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.272631645202637 | KNN Loss: 3.222369909286499 | BCE Loss: 1.0502618551254272\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.337907791137695 | KNN Loss: 3.273270606994629 | BCE Loss: 1.0646371841430664\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.285008430480957 | KNN Loss: 3.2350242137908936 | BCE Loss: 1.0499839782714844\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.342331409454346 | KNN Loss: 3.26613712310791 | BCE Loss: 1.0761942863464355\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.296869277954102 | KNN Loss: 3.235841751098633 | BCE Loss: 1.0610277652740479\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.260255813598633 | KNN Loss: 3.1973583698272705 | BCE Loss: 1.0628973245620728\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.287461280822754 | KNN Loss: 3.227600336074829 | BCE Loss: 1.0598607063293457\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.26182746887207 | KNN Loss: 3.2086915969848633 | BCE Loss: 1.0531359910964966\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.246950149536133 | KNN Loss: 3.204907178878784 | BCE Loss: 1.0420429706573486\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.259076118469238 | KNN Loss: 3.2062201499938965 | BCE Loss: 1.0528558492660522\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.315804958343506 | KNN Loss: 3.2775633335113525 | BCE Loss: 1.0382417440414429\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.261903762817383 | KNN Loss: 3.228285789489746 | BCE Loss: 1.0336182117462158\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.239264011383057 | KNN Loss: 3.2193593978881836 | BCE Loss: 1.019904613494873\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.284551620483398 | KNN Loss: 3.2283291816711426 | BCE Loss: 1.056222677230835\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.296228408813477 | KNN Loss: 3.2228407859802246 | BCE Loss: 1.0733877420425415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.242801666259766 | KNN Loss: 3.2004213333129883 | BCE Loss: 1.0423803329467773\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.2983222007751465 | KNN Loss: 3.217681884765625 | BCE Loss: 1.0806403160095215\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.241363048553467 | KNN Loss: 3.191906452178955 | BCE Loss: 1.0494565963745117\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.24920129776001 | KNN Loss: 3.2198221683502197 | BCE Loss: 1.0293792486190796\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.245300769805908 | KNN Loss: 3.216764211654663 | BCE Loss: 1.0285365581512451\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.269352912902832 | KNN Loss: 3.2253353595733643 | BCE Loss: 1.0440176725387573\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.278356552124023 | KNN Loss: 3.2230496406555176 | BCE Loss: 1.0553069114685059\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.317556381225586 | KNN Loss: 3.2745354175567627 | BCE Loss: 1.0430210828781128\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.3506927490234375 | KNN Loss: 3.260626792907715 | BCE Loss: 1.0900657176971436\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.2653608322143555 | KNN Loss: 3.228203296661377 | BCE Loss: 1.0371575355529785\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.216358184814453 | KNN Loss: 3.1602909564971924 | BCE Loss: 1.0560673475265503\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.233667850494385 | KNN Loss: 3.1912081241607666 | BCE Loss: 1.0424596071243286\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.2943267822265625 | KNN Loss: 3.2325940132141113 | BCE Loss: 1.0617326498031616\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.289803504943848 | KNN Loss: 3.2523553371429443 | BCE Loss: 1.0374484062194824\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.241316318511963 | KNN Loss: 3.2141847610473633 | BCE Loss: 1.0271315574645996\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.228142261505127 | KNN Loss: 3.17671799659729 | BCE Loss: 1.051424264907837\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.258053779602051 | KNN Loss: 3.230433225631714 | BCE Loss: 1.0276203155517578\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.218263626098633 | KNN Loss: 3.1650888919830322 | BCE Loss: 1.0531749725341797\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.177265644073486 | KNN Loss: 3.1525819301605225 | BCE Loss: 1.0246837139129639\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.203168869018555 | KNN Loss: 3.17619252204895 | BCE Loss: 1.0269763469696045\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.224635124206543 | KNN Loss: 3.1937317848205566 | BCE Loss: 1.0309031009674072\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.217949867248535 | KNN Loss: 3.18880295753479 | BCE Loss: 1.029146671295166\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.183140754699707 | KNN Loss: 3.165964365005493 | BCE Loss: 1.017176628112793\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.262264251708984 | KNN Loss: 3.2144222259521484 | BCE Loss: 1.0478417873382568\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.269498825073242 | KNN Loss: 3.2225654125213623 | BCE Loss: 1.0469335317611694\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.225210189819336 | KNN Loss: 3.18415904045105 | BCE Loss: 1.041050910949707\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.204775810241699 | KNN Loss: 3.1488664150238037 | BCE Loss: 1.055909276008606\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.200733184814453 | KNN Loss: 3.1834681034088135 | BCE Loss: 1.0172653198242188\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.24545431137085 | KNN Loss: 3.2122690677642822 | BCE Loss: 1.0331852436065674\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.1839599609375 | KNN Loss: 3.1477766036987305 | BCE Loss: 1.0361835956573486\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.1810503005981445 | KNN Loss: 3.1505091190338135 | BCE Loss: 1.030540943145752\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.200308799743652 | KNN Loss: 3.1790525913238525 | BCE Loss: 1.021256446838379\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.210718154907227 | KNN Loss: 3.155963659286499 | BCE Loss: 1.0547542572021484\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.277887344360352 | KNN Loss: 3.1955432891845703 | BCE Loss: 1.0823438167572021\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.248693943023682 | KNN Loss: 3.186866521835327 | BCE Loss: 1.061827540397644\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.237913608551025 | KNN Loss: 3.1805031299591064 | BCE Loss: 1.057410478591919\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.247013568878174 | KNN Loss: 3.203590154647827 | BCE Loss: 1.0434235334396362\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.206982612609863 | KNN Loss: 3.167806386947632 | BCE Loss: 1.0391762256622314\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.243912220001221 | KNN Loss: 3.2075583934783936 | BCE Loss: 1.0363538265228271\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.247429847717285 | KNN Loss: 3.176593065261841 | BCE Loss: 1.0708367824554443\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.2137908935546875 | KNN Loss: 3.1740224361419678 | BCE Loss: 1.0397684574127197\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.229265213012695 | KNN Loss: 3.184579372406006 | BCE Loss: 1.0446858406066895\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.209099769592285 | KNN Loss: 3.159977912902832 | BCE Loss: 1.0491220951080322\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.210515022277832 | KNN Loss: 3.1885149478912354 | BCE Loss: 1.0220003128051758\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.174896717071533 | KNN Loss: 3.164936065673828 | BCE Loss: 1.009960651397705\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.244731426239014 | KNN Loss: 3.202286720275879 | BCE Loss: 1.0424447059631348\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.253798484802246 | KNN Loss: 3.1782801151275635 | BCE Loss: 1.0755183696746826\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.213637828826904 | KNN Loss: 3.1657721996307373 | BCE Loss: 1.047865629196167\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.2249908447265625 | KNN Loss: 3.1737146377563477 | BCE Loss: 1.0512759685516357\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.178938865661621 | KNN Loss: 3.1533968448638916 | BCE Loss: 1.0255422592163086\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.215322494506836 | KNN Loss: 3.1671559810638428 | BCE Loss: 1.0481667518615723\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.22062873840332 | KNN Loss: 3.164820671081543 | BCE Loss: 1.0558078289031982\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.186944961547852 | KNN Loss: 3.1327874660491943 | BCE Loss: 1.0541574954986572\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.160976886749268 | KNN Loss: 3.1318819522857666 | BCE Loss: 1.0290950536727905\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.236912727355957 | KNN Loss: 3.1793041229248047 | BCE Loss: 1.0576088428497314\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.237058639526367 | KNN Loss: 3.1786587238311768 | BCE Loss: 1.0583997964859009\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.2114386558532715 | KNN Loss: 3.175229549407959 | BCE Loss: 1.0362091064453125\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.203921794891357 | KNN Loss: 3.182399272918701 | BCE Loss: 1.0215225219726562\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.171378135681152 | KNN Loss: 3.15295672416687 | BCE Loss: 1.0184214115142822\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.218768119812012 | KNN Loss: 3.1753485202789307 | BCE Loss: 1.043419599533081\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.182531356811523 | KNN Loss: 3.1466057300567627 | BCE Loss: 1.0359258651733398\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.216718673706055 | KNN Loss: 3.172797203063965 | BCE Loss: 1.0439215898513794\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.170276641845703 | KNN Loss: 3.157628297805786 | BCE Loss: 1.012648582458496\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.236806392669678 | KNN Loss: 3.1831300258636475 | BCE Loss: 1.0536762475967407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.19987678527832 | KNN Loss: 3.17140793800354 | BCE Loss: 1.0284690856933594\n",
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.165718078613281 | KNN Loss: 3.1408562660217285 | BCE Loss: 1.0248615741729736\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.24215030670166 | KNN Loss: 3.179236650466919 | BCE Loss: 1.062913417816162\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.17344856262207 | KNN Loss: 3.1509287357330322 | BCE Loss: 1.022519588470459\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.247107982635498 | KNN Loss: 3.177555561065674 | BCE Loss: 1.0695523023605347\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.196765899658203 | KNN Loss: 3.199018716812134 | BCE Loss: 0.9977471232414246\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.234078407287598 | KNN Loss: 3.1877129077911377 | BCE Loss: 1.0463652610778809\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.192921161651611 | KNN Loss: 3.1578729152679443 | BCE Loss: 1.035048246383667\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.137474060058594 | KNN Loss: 3.1166088581085205 | BCE Loss: 1.0208652019500732\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.194740295410156 | KNN Loss: 3.1656603813171387 | BCE Loss: 1.0290801525115967\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.146975517272949 | KNN Loss: 3.12140154838562 | BCE Loss: 1.0255738496780396\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.204204559326172 | KNN Loss: 3.171462059020996 | BCE Loss: 1.0327422618865967\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.179762363433838 | KNN Loss: 3.1536190509796143 | BCE Loss: 1.026143193244934\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.198089599609375 | KNN Loss: 3.1688849925994873 | BCE Loss: 1.0292047262191772\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.187443256378174 | KNN Loss: 3.165259838104248 | BCE Loss: 1.0221834182739258\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.176595211029053 | KNN Loss: 3.1288955211639404 | BCE Loss: 1.0476995706558228\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.176287651062012 | KNN Loss: 3.148805618286133 | BCE Loss: 1.027482032775879\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.230480194091797 | KNN Loss: 3.204758882522583 | BCE Loss: 1.0257211923599243\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.203787803649902 | KNN Loss: 3.1675002574920654 | BCE Loss: 1.0362873077392578\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.169763565063477 | KNN Loss: 3.1616201400756836 | BCE Loss: 1.008143663406372\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.135376453399658 | KNN Loss: 3.1190013885498047 | BCE Loss: 1.0163750648498535\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.1431474685668945 | KNN Loss: 3.1202220916748047 | BCE Loss: 1.0229251384735107\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.1820526123046875 | KNN Loss: 3.1471331119537354 | BCE Loss: 1.0349196195602417\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.179574966430664 | KNN Loss: 3.1697614192962646 | BCE Loss: 1.0098135471343994\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.192168712615967 | KNN Loss: 3.1676065921783447 | BCE Loss: 1.0245620012283325\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.161907196044922 | KNN Loss: 3.129701614379883 | BCE Loss: 1.0322058200836182\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.18883752822876 | KNN Loss: 3.145763397216797 | BCE Loss: 1.0430740118026733\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.176957607269287 | KNN Loss: 3.1770317554473877 | BCE Loss: 0.9999257326126099\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.170521259307861 | KNN Loss: 3.1662769317626953 | BCE Loss: 1.0042444467544556\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.1522040367126465 | KNN Loss: 3.147183895111084 | BCE Loss: 1.005020260810852\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.170406818389893 | KNN Loss: 3.148664951324463 | BCE Loss: 1.0217418670654297\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.144820690155029 | KNN Loss: 3.0924413204193115 | BCE Loss: 1.0523793697357178\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.221065521240234 | KNN Loss: 3.183316707611084 | BCE Loss: 1.0377490520477295\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.204642295837402 | KNN Loss: 3.160609006881714 | BCE Loss: 1.044033408164978\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.145153522491455 | KNN Loss: 3.1178386211395264 | BCE Loss: 1.0273147821426392\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.191321849822998 | KNN Loss: 3.1799070835113525 | BCE Loss: 1.0114147663116455\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.202935218811035 | KNN Loss: 3.151911497116089 | BCE Loss: 1.0510239601135254\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.211006164550781 | KNN Loss: 3.1344597339630127 | BCE Loss: 1.0765461921691895\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.220720291137695 | KNN Loss: 3.181262969970703 | BCE Loss: 1.0394573211669922\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.187664031982422 | KNN Loss: 3.1302759647369385 | BCE Loss: 1.0573880672454834\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.14802360534668 | KNN Loss: 3.128375291824341 | BCE Loss: 1.019648551940918\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.1680145263671875 | KNN Loss: 3.1359617710113525 | BCE Loss: 1.0320526361465454\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.136627197265625 | KNN Loss: 3.110344171524048 | BCE Loss: 1.0262832641601562\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.168715000152588 | KNN Loss: 3.140289545059204 | BCE Loss: 1.0284255743026733\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.231789588928223 | KNN Loss: 3.1735999584198 | BCE Loss: 1.0581893920898438\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.153750419616699 | KNN Loss: 3.1191084384918213 | BCE Loss: 1.034642219543457\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.151275634765625 | KNN Loss: 3.1241135597229004 | BCE Loss: 1.0271623134613037\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.198460102081299 | KNN Loss: 3.1503710746765137 | BCE Loss: 1.0480890274047852\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.175838470458984 | KNN Loss: 3.153479814529419 | BCE Loss: 1.0223588943481445\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.20513916015625 | KNN Loss: 3.195861339569092 | BCE Loss: 1.009277582168579\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.141134262084961 | KNN Loss: 3.131202220916748 | BCE Loss: 1.009932279586792\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.1606221199035645 | KNN Loss: 3.1283042430877686 | BCE Loss: 1.032317876815796\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.150069236755371 | KNN Loss: 3.1270358562469482 | BCE Loss: 1.0230331420898438\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.1365647315979 | KNN Loss: 3.1272335052490234 | BCE Loss: 1.0093313455581665\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.188380241394043 | KNN Loss: 3.1654038429260254 | BCE Loss: 1.0229766368865967\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.097470760345459 | KNN Loss: 3.1040472984313965 | BCE Loss: 0.9934236407279968\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.141129493713379 | KNN Loss: 3.1016595363616943 | BCE Loss: 1.039469838142395\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.158436298370361 | KNN Loss: 3.1078333854675293 | BCE Loss: 1.050602912902832\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.1336989402771 | KNN Loss: 3.1125941276550293 | BCE Loss: 1.0211048126220703\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.1930999755859375 | KNN Loss: 3.16428542137146 | BCE Loss: 1.0288143157958984\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.114094257354736 | KNN Loss: 3.1143016815185547 | BCE Loss: 0.9997925758361816\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.130457878112793 | KNN Loss: 3.1228015422821045 | BCE Loss: 1.0076560974121094\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.15351676940918 | KNN Loss: 3.159877061843872 | BCE Loss: 0.9936396479606628\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.131670951843262 | KNN Loss: 3.1377007961273193 | BCE Loss: 0.9939700365066528\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.158363342285156 | KNN Loss: 3.1183080673217773 | BCE Loss: 1.040055274963379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.209124565124512 | KNN Loss: 3.176114320755005 | BCE Loss: 1.0330100059509277\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.183102607727051 | KNN Loss: 3.1572511196136475 | BCE Loss: 1.0258512496948242\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.148543834686279 | KNN Loss: 3.1326045989990234 | BCE Loss: 1.0159392356872559\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.208381175994873 | KNN Loss: 3.163536787033081 | BCE Loss: 1.044844388961792\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.18538761138916 | KNN Loss: 3.1472864151000977 | BCE Loss: 1.0381011962890625\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.159329414367676 | KNN Loss: 3.1228044033050537 | BCE Loss: 1.0365251302719116\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.176938056945801 | KNN Loss: 3.125596761703491 | BCE Loss: 1.0513412952423096\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.148064136505127 | KNN Loss: 3.1323349475860596 | BCE Loss: 1.0157290697097778\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.16694450378418 | KNN Loss: 3.1247801780700684 | BCE Loss: 1.0421645641326904\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.18124532699585 | KNN Loss: 3.1517345905303955 | BCE Loss: 1.029510736465454\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.203182697296143 | KNN Loss: 3.158179521560669 | BCE Loss: 1.0450032949447632\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.183414459228516 | KNN Loss: 3.1257379055023193 | BCE Loss: 1.0576766729354858\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.165131568908691 | KNN Loss: 3.138383150100708 | BCE Loss: 1.0267482995986938\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.172459602355957 | KNN Loss: 3.150803327560425 | BCE Loss: 1.0216562747955322\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.172481060028076 | KNN Loss: 3.162005662918091 | BCE Loss: 1.0104752779006958\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.217535018920898 | KNN Loss: 3.1724932193756104 | BCE Loss: 1.045041799545288\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.145298957824707 | KNN Loss: 3.114840269088745 | BCE Loss: 1.030458688735962\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.129907608032227 | KNN Loss: 3.113301992416382 | BCE Loss: 1.0166058540344238\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.16553258895874 | KNN Loss: 3.1416401863098145 | BCE Loss: 1.0238924026489258\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.222479343414307 | KNN Loss: 3.1834053993225098 | BCE Loss: 1.0390739440917969\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.1715168952941895 | KNN Loss: 3.1420516967773438 | BCE Loss: 1.0294653177261353\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.127610206604004 | KNN Loss: 3.1212072372436523 | BCE Loss: 1.0064027309417725\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.162563323974609 | KNN Loss: 3.132709503173828 | BCE Loss: 1.0298535823822021\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.136652946472168 | KNN Loss: 3.133460760116577 | BCE Loss: 1.0031920671463013\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.181405067443848 | KNN Loss: 3.1275463104248047 | BCE Loss: 1.053858995437622\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.221846580505371 | KNN Loss: 3.1666553020477295 | BCE Loss: 1.0551912784576416\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.1407151222229 | KNN Loss: 3.1417176723480225 | BCE Loss: 0.9989975690841675\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.159169673919678 | KNN Loss: 3.138861656188965 | BCE Loss: 1.0203081369400024\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.186860084533691 | KNN Loss: 3.1609840393066406 | BCE Loss: 1.0258760452270508\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.15223503112793 | KNN Loss: 3.145387887954712 | BCE Loss: 1.0068472623825073\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.197766304016113 | KNN Loss: 3.184971570968628 | BCE Loss: 1.0127947330474854\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.176937103271484 | KNN Loss: 3.1358563899993896 | BCE Loss: 1.0410804748535156\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.183030128479004 | KNN Loss: 3.144223213195801 | BCE Loss: 1.0388069152832031\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.183383941650391 | KNN Loss: 3.126333475112915 | BCE Loss: 1.057050347328186\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.165897846221924 | KNN Loss: 3.133350133895874 | BCE Loss: 1.0325475931167603\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.186069488525391 | KNN Loss: 3.146061420440674 | BCE Loss: 1.0400079488754272\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.203154563903809 | KNN Loss: 3.1970410346984863 | BCE Loss: 1.0061132907867432\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.198465347290039 | KNN Loss: 3.180511474609375 | BCE Loss: 1.0179539918899536\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.175771713256836 | KNN Loss: 3.135420083999634 | BCE Loss: 1.0403516292572021\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.120206356048584 | KNN Loss: 3.125173568725586 | BCE Loss: 0.9950329065322876\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.136650085449219 | KNN Loss: 3.130002498626709 | BCE Loss: 1.0066473484039307\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.170949935913086 | KNN Loss: 3.1348583698272705 | BCE Loss: 1.036091685295105\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.1999077796936035 | KNN Loss: 3.1574461460113525 | BCE Loss: 1.042461633682251\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.205058574676514 | KNN Loss: 3.184221029281616 | BCE Loss: 1.020837664604187\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.141026020050049 | KNN Loss: 3.134321689605713 | BCE Loss: 1.006704330444336\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.184086322784424 | KNN Loss: 3.1589457988739014 | BCE Loss: 1.025140404701233\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.166347503662109 | KNN Loss: 3.1332194805145264 | BCE Loss: 1.033127784729004\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.140253067016602 | KNN Loss: 3.1365468502044678 | BCE Loss: 1.0037062168121338\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.157686233520508 | KNN Loss: 3.137117862701416 | BCE Loss: 1.0205683708190918\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.157406330108643 | KNN Loss: 3.1317784786224365 | BCE Loss: 1.025627851486206\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.163766860961914 | KNN Loss: 3.114255905151367 | BCE Loss: 1.0495107173919678\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.1482391357421875 | KNN Loss: 3.1291933059692383 | BCE Loss: 1.0190458297729492\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.132068157196045 | KNN Loss: 3.1347639560699463 | BCE Loss: 0.9973042011260986\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.118128776550293 | KNN Loss: 3.0922067165374756 | BCE Loss: 1.0259218215942383\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.177652359008789 | KNN Loss: 3.1554017066955566 | BCE Loss: 1.0222508907318115\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.196208953857422 | KNN Loss: 3.157743215560913 | BCE Loss: 1.0384657382965088\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.1722540855407715 | KNN Loss: 3.133190155029297 | BCE Loss: 1.0390639305114746\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.169252395629883 | KNN Loss: 3.1455469131469727 | BCE Loss: 1.0237057209014893\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.160904407501221 | KNN Loss: 3.1142289638519287 | BCE Loss: 1.0466753244400024\n",
      "Epoch    53: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.158635139465332 | KNN Loss: 3.124753713607788 | BCE Loss: 1.0338813066482544\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.1678667068481445 | KNN Loss: 3.1355018615722656 | BCE Loss: 1.032365083694458\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.158502578735352 | KNN Loss: 3.12656569480896 | BCE Loss: 1.0319368839263916\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.132411479949951 | KNN Loss: 3.1342108249664307 | BCE Loss: 0.9982007741928101\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.199067115783691 | KNN Loss: 3.129434823989868 | BCE Loss: 1.0696322917938232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.173694610595703 | KNN Loss: 3.1289548873901367 | BCE Loss: 1.0447399616241455\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.117863655090332 | KNN Loss: 3.0925514698028564 | BCE Loss: 1.0253123044967651\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.124027252197266 | KNN Loss: 3.0948495864868164 | BCE Loss: 1.0291776657104492\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.139721870422363 | KNN Loss: 3.1219544410705566 | BCE Loss: 1.0177671909332275\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.1156511306762695 | KNN Loss: 3.0814177989959717 | BCE Loss: 1.0342330932617188\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.114005088806152 | KNN Loss: 3.0988214015960693 | BCE Loss: 1.015183448791504\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.159073829650879 | KNN Loss: 3.1355228424072266 | BCE Loss: 1.0235509872436523\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.180908203125 | KNN Loss: 3.116016387939453 | BCE Loss: 1.0648915767669678\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.188755512237549 | KNN Loss: 3.147925615310669 | BCE Loss: 1.0408300161361694\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.150416851043701 | KNN Loss: 3.110309362411499 | BCE Loss: 1.0401074886322021\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.112631320953369 | KNN Loss: 3.105816125869751 | BCE Loss: 1.0068151950836182\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.149356842041016 | KNN Loss: 3.1221704483032227 | BCE Loss: 1.027186393737793\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.100780487060547 | KNN Loss: 3.0951504707336426 | BCE Loss: 1.0056301355361938\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.11448335647583 | KNN Loss: 3.1150004863739014 | BCE Loss: 0.9994827508926392\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.116987228393555 | KNN Loss: 3.1163418292999268 | BCE Loss: 1.0006455183029175\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.144478797912598 | KNN Loss: 3.1187944412231445 | BCE Loss: 1.0256843566894531\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.185651779174805 | KNN Loss: 3.1629817485809326 | BCE Loss: 1.022670030593872\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.143575191497803 | KNN Loss: 3.105372190475464 | BCE Loss: 1.0382030010223389\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.125713348388672 | KNN Loss: 3.109374523162842 | BCE Loss: 1.01633882522583\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.165090560913086 | KNN Loss: 3.126647472381592 | BCE Loss: 1.0384430885314941\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.1035919189453125 | KNN Loss: 3.1143834590911865 | BCE Loss: 0.989208459854126\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.189419746398926 | KNN Loss: 3.1432130336761475 | BCE Loss: 1.0462064743041992\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.109930992126465 | KNN Loss: 3.0897305011749268 | BCE Loss: 1.0202007293701172\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.138145446777344 | KNN Loss: 3.129094362258911 | BCE Loss: 1.0090513229370117\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.146646976470947 | KNN Loss: 3.1190598011016846 | BCE Loss: 1.0275870561599731\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.119058609008789 | KNN Loss: 3.106609582901001 | BCE Loss: 1.0124491453170776\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.157077789306641 | KNN Loss: 3.121845006942749 | BCE Loss: 1.0352325439453125\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.171648979187012 | KNN Loss: 3.1509408950805664 | BCE Loss: 1.0207083225250244\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.18446159362793 | KNN Loss: 3.1202871799468994 | BCE Loss: 1.0641744136810303\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.147129535675049 | KNN Loss: 3.1280832290649414 | BCE Loss: 1.0190463066101074\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.125357151031494 | KNN Loss: 3.0988106727600098 | BCE Loss: 1.0265464782714844\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.089672088623047 | KNN Loss: 3.0732109546661377 | BCE Loss: 1.0164613723754883\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.113187313079834 | KNN Loss: 3.122990608215332 | BCE Loss: 0.9901966452598572\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.133410453796387 | KNN Loss: 3.1217896938323975 | BCE Loss: 1.0116207599639893\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.135044574737549 | KNN Loss: 3.107818603515625 | BCE Loss: 1.0272259712219238\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.122622489929199 | KNN Loss: 3.088135242462158 | BCE Loss: 1.0344874858856201\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.14102029800415 | KNN Loss: 3.1030805110931396 | BCE Loss: 1.0379397869110107\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.152421951293945 | KNN Loss: 3.129448413848877 | BCE Loss: 1.0229732990264893\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.151642799377441 | KNN Loss: 3.1302101612091064 | BCE Loss: 1.021432638168335\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.167972564697266 | KNN Loss: 3.133082866668701 | BCE Loss: 1.0348894596099854\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.1295318603515625 | KNN Loss: 3.1209559440612793 | BCE Loss: 1.0085761547088623\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.121379852294922 | KNN Loss: 3.0942537784576416 | BCE Loss: 1.0271261930465698\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.126936912536621 | KNN Loss: 3.1110570430755615 | BCE Loss: 1.0158801078796387\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.111597061157227 | KNN Loss: 3.102261543273926 | BCE Loss: 1.0093352794647217\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.146607398986816 | KNN Loss: 3.1126608848571777 | BCE Loss: 1.0339467525482178\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.150124549865723 | KNN Loss: 3.100018262863159 | BCE Loss: 1.050106167793274\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.118892669677734 | KNN Loss: 3.0916616916656494 | BCE Loss: 1.027230978012085\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.113495826721191 | KNN Loss: 3.085853338241577 | BCE Loss: 1.0276422500610352\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.065986633300781 | KNN Loss: 3.0688772201538086 | BCE Loss: 0.9971095323562622\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.092918395996094 | KNN Loss: 3.0801899433135986 | BCE Loss: 1.0127286911010742\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.151147842407227 | KNN Loss: 3.093698263168335 | BCE Loss: 1.0574495792388916\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.109090805053711 | KNN Loss: 3.110975980758667 | BCE Loss: 0.9981145858764648\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.1272783279418945 | KNN Loss: 3.1167447566986084 | BCE Loss: 1.0105335712432861\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.0899128913879395 | KNN Loss: 3.073204755783081 | BCE Loss: 1.0167080163955688\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.144741058349609 | KNN Loss: 3.104463815689087 | BCE Loss: 1.0402772426605225\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.097240924835205 | KNN Loss: 3.070403814315796 | BCE Loss: 1.0268372297286987\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.157157897949219 | KNN Loss: 3.1085734367370605 | BCE Loss: 1.048584222793579\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.151604652404785 | KNN Loss: 3.1240885257720947 | BCE Loss: 1.0275163650512695\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.140510559082031 | KNN Loss: 3.1251416206359863 | BCE Loss: 1.015369176864624\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.075162410736084 | KNN Loss: 3.0674164295196533 | BCE Loss: 1.0077459812164307\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.114441871643066 | KNN Loss: 3.101757526397705 | BCE Loss: 1.0126844644546509\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.061817169189453 | KNN Loss: 3.0745291709899902 | BCE Loss: 0.9872878193855286\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.140355110168457 | KNN Loss: 3.104952335357666 | BCE Loss: 1.035402774810791\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.143964767456055 | KNN Loss: 3.1325249671936035 | BCE Loss: 1.011439561843872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.086202621459961 | KNN Loss: 3.0866317749023438 | BCE Loss: 0.9995708465576172\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.110311508178711 | KNN Loss: 3.068460464477539 | BCE Loss: 1.0418509244918823\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.068864822387695 | KNN Loss: 3.0672857761383057 | BCE Loss: 1.0015792846679688\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.086534023284912 | KNN Loss: 3.079218864440918 | BCE Loss: 1.0073152780532837\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.120290756225586 | KNN Loss: 3.128277540206909 | BCE Loss: 0.992013156414032\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.151926040649414 | KNN Loss: 3.1280758380889893 | BCE Loss: 1.0238502025604248\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.119924545288086 | KNN Loss: 3.104179859161377 | BCE Loss: 1.015744924545288\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.085965156555176 | KNN Loss: 3.086134672164917 | BCE Loss: 0.9998306632041931\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.123246192932129 | KNN Loss: 3.08678936958313 | BCE Loss: 1.0364570617675781\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.158477783203125 | KNN Loss: 3.161961793899536 | BCE Loss: 0.9965157508850098\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.154648780822754 | KNN Loss: 3.126035451889038 | BCE Loss: 1.0286133289337158\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.131861686706543 | KNN Loss: 3.109905481338501 | BCE Loss: 1.021956443786621\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.113389015197754 | KNN Loss: 3.113891363143921 | BCE Loss: 0.9994974136352539\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.123836517333984 | KNN Loss: 3.0991532802581787 | BCE Loss: 1.0246829986572266\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.170630931854248 | KNN Loss: 3.1129684448242188 | BCE Loss: 1.0576623678207397\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.13593864440918 | KNN Loss: 3.0990304946899414 | BCE Loss: 1.0369079113006592\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.0869574546813965 | KNN Loss: 3.096653699874878 | BCE Loss: 0.9903038740158081\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.153438091278076 | KNN Loss: 3.1203267574310303 | BCE Loss: 1.033111333847046\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.1626877784729 | KNN Loss: 3.106419324874878 | BCE Loss: 1.0562684535980225\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.100057125091553 | KNN Loss: 3.0772957801818848 | BCE Loss: 1.0227614641189575\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.093459129333496 | KNN Loss: 3.1034960746765137 | BCE Loss: 0.9899628758430481\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.106378078460693 | KNN Loss: 3.0840260982513428 | BCE Loss: 1.0223520994186401\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.140437126159668 | KNN Loss: 3.115630626678467 | BCE Loss: 1.0248064994812012\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.127802848815918 | KNN Loss: 3.1007156372070312 | BCE Loss: 1.0270874500274658\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.140720844268799 | KNN Loss: 3.105355978012085 | BCE Loss: 1.0353648662567139\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.138028144836426 | KNN Loss: 3.1347742080688477 | BCE Loss: 1.0032541751861572\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.131701469421387 | KNN Loss: 3.1094510555267334 | BCE Loss: 1.0222502946853638\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.129561424255371 | KNN Loss: 3.1028852462768555 | BCE Loss: 1.0266761779785156\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.159412384033203 | KNN Loss: 3.1239402294158936 | BCE Loss: 1.0354721546173096\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.157607555389404 | KNN Loss: 3.136274576187134 | BCE Loss: 1.0213329792022705\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.186452865600586 | KNN Loss: 3.1398766040802 | BCE Loss: 1.0465760231018066\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.128744125366211 | KNN Loss: 3.1039230823516846 | BCE Loss: 1.0248208045959473\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.101850509643555 | KNN Loss: 3.1037299633026123 | BCE Loss: 0.9981206655502319\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.129049777984619 | KNN Loss: 3.1144981384277344 | BCE Loss: 1.0145516395568848\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.169911861419678 | KNN Loss: 3.160224199295044 | BCE Loss: 1.0096875429153442\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.1234307289123535 | KNN Loss: 3.1082327365875244 | BCE Loss: 1.015197992324829\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.102762222290039 | KNN Loss: 3.088531732559204 | BCE Loss: 1.0142303705215454\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.114101886749268 | KNN Loss: 3.106574296951294 | BCE Loss: 1.0075275897979736\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.133385181427002 | KNN Loss: 3.1077070236206055 | BCE Loss: 1.0256781578063965\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.09344482421875 | KNN Loss: 3.0924272537231445 | BCE Loss: 1.0010175704956055\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.170717716217041 | KNN Loss: 3.128588914871216 | BCE Loss: 1.0421289205551147\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.181327819824219 | KNN Loss: 3.1204261779785156 | BCE Loss: 1.0609015226364136\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.149662971496582 | KNN Loss: 3.121670722961426 | BCE Loss: 1.0279923677444458\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.194975852966309 | KNN Loss: 3.136678457260132 | BCE Loss: 1.0582971572875977\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.1445770263671875 | KNN Loss: 3.1211440563201904 | BCE Loss: 1.0234332084655762\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.104767799377441 | KNN Loss: 3.1031885147094727 | BCE Loss: 1.0015792846679688\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.149311542510986 | KNN Loss: 3.1151254177093506 | BCE Loss: 1.0341861248016357\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.166170597076416 | KNN Loss: 3.1357245445251465 | BCE Loss: 1.030446171760559\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.181164741516113 | KNN Loss: 3.130030393600464 | BCE Loss: 1.0511341094970703\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.18201208114624 | KNN Loss: 3.1165854930877686 | BCE Loss: 1.0654265880584717\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.128726482391357 | KNN Loss: 3.109875202178955 | BCE Loss: 1.0188511610031128\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.1110076904296875 | KNN Loss: 3.110265016555786 | BCE Loss: 1.0007429122924805\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.143054962158203 | KNN Loss: 3.1295230388641357 | BCE Loss: 1.0135316848754883\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.145692825317383 | KNN Loss: 3.115647315979004 | BCE Loss: 1.0300452709197998\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.129152297973633 | KNN Loss: 3.110124111175537 | BCE Loss: 1.0190284252166748\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.080791473388672 | KNN Loss: 3.089736223220825 | BCE Loss: 0.9910552501678467\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.1314592361450195 | KNN Loss: 3.0973751544952393 | BCE Loss: 1.0340838432312012\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.1274027824401855 | KNN Loss: 3.1167068481445312 | BCE Loss: 1.0106960535049438\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.1709794998168945 | KNN Loss: 3.1159069538116455 | BCE Loss: 1.0550727844238281\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.152068138122559 | KNN Loss: 3.1021275520324707 | BCE Loss: 1.049940586090088\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.174141883850098 | KNN Loss: 3.1013882160186768 | BCE Loss: 1.072753667831421\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.116974353790283 | KNN Loss: 3.102983236312866 | BCE Loss: 1.0139912366867065\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.100611209869385 | KNN Loss: 3.0797626972198486 | BCE Loss: 1.0208483934402466\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.127813339233398 | KNN Loss: 3.115460157394409 | BCE Loss: 1.0123534202575684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.14432430267334 | KNN Loss: 3.100449800491333 | BCE Loss: 1.0438742637634277\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.107976913452148 | KNN Loss: 3.0922722816467285 | BCE Loss: 1.0157043933868408\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.112934589385986 | KNN Loss: 3.104480504989624 | BCE Loss: 1.0084539651870728\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.130946159362793 | KNN Loss: 3.0945303440093994 | BCE Loss: 1.0364155769348145\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.109489440917969 | KNN Loss: 3.095674753189087 | BCE Loss: 1.013814926147461\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.11566162109375 | KNN Loss: 3.0900044441223145 | BCE Loss: 1.0256569385528564\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.14339017868042 | KNN Loss: 3.1207950115203857 | BCE Loss: 1.0225950479507446\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.103658676147461 | KNN Loss: 3.0873661041259766 | BCE Loss: 1.016292691230774\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.187355041503906 | KNN Loss: 3.1435470581054688 | BCE Loss: 1.0438082218170166\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.099664211273193 | KNN Loss: 3.0858020782470703 | BCE Loss: 1.013862133026123\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.119913578033447 | KNN Loss: 3.087420701980591 | BCE Loss: 1.032492756843567\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.150020599365234 | KNN Loss: 3.112943410873413 | BCE Loss: 1.0370769500732422\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.140214920043945 | KNN Loss: 3.115281820297241 | BCE Loss: 1.0249329805374146\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.1332292556762695 | KNN Loss: 3.1178503036499023 | BCE Loss: 1.0153790712356567\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.054930686950684 | KNN Loss: 3.0597426891326904 | BCE Loss: 0.9951878786087036\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.139047622680664 | KNN Loss: 3.0896387100219727 | BCE Loss: 1.0494089126586914\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.142824649810791 | KNN Loss: 3.0790393352508545 | BCE Loss: 1.063785195350647\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.114728927612305 | KNN Loss: 3.0754573345184326 | BCE Loss: 1.0392718315124512\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.105731964111328 | KNN Loss: 3.1024222373962402 | BCE Loss: 1.003309726715088\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.149070739746094 | KNN Loss: 3.1163041591644287 | BCE Loss: 1.0327668190002441\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.16728401184082 | KNN Loss: 3.1339914798736572 | BCE Loss: 1.0332927703857422\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.161777973175049 | KNN Loss: 3.1023049354553223 | BCE Loss: 1.0594730377197266\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.102840900421143 | KNN Loss: 3.0836708545684814 | BCE Loss: 1.0191699266433716\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.127812385559082 | KNN Loss: 3.10845685005188 | BCE Loss: 1.0193555355072021\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.151630401611328 | KNN Loss: 3.1221766471862793 | BCE Loss: 1.0294535160064697\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.170799255371094 | KNN Loss: 3.162032127380371 | BCE Loss: 1.0087671279907227\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.167047500610352 | KNN Loss: 3.1371750831604004 | BCE Loss: 1.0298722982406616\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.200209617614746 | KNN Loss: 3.1411705017089844 | BCE Loss: 1.0590392351150513\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.095959663391113 | KNN Loss: 3.0945801734924316 | BCE Loss: 1.0013792514801025\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.141110897064209 | KNN Loss: 3.0994350910186768 | BCE Loss: 1.0416758060455322\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.144338130950928 | KNN Loss: 3.1009883880615234 | BCE Loss: 1.0433496236801147\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.1421918869018555 | KNN Loss: 3.1102538108825684 | BCE Loss: 1.0319379568099976\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.133342742919922 | KNN Loss: 3.093244791030884 | BCE Loss: 1.0400978326797485\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.078628063201904 | KNN Loss: 3.090876817703247 | BCE Loss: 0.9877512454986572\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.1069159507751465 | KNN Loss: 3.078760862350464 | BCE Loss: 1.028154969215393\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.130424976348877 | KNN Loss: 3.0945677757263184 | BCE Loss: 1.0358572006225586\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.135636329650879 | KNN Loss: 3.1332457065582275 | BCE Loss: 1.0023903846740723\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.139169216156006 | KNN Loss: 3.0935795307159424 | BCE Loss: 1.0455896854400635\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.130687713623047 | KNN Loss: 3.110171318054199 | BCE Loss: 1.0205166339874268\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.125650882720947 | KNN Loss: 3.1208176612854004 | BCE Loss: 1.0048332214355469\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.110208988189697 | KNN Loss: 3.0840039253234863 | BCE Loss: 1.0262049436569214\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.108675003051758 | KNN Loss: 3.08046817779541 | BCE Loss: 1.028206706047058\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.1362433433532715 | KNN Loss: 3.1265759468078613 | BCE Loss: 1.0096675157546997\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.118466854095459 | KNN Loss: 3.0981881618499756 | BCE Loss: 1.0202785730361938\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.120443344116211 | KNN Loss: 3.086961507797241 | BCE Loss: 1.0334820747375488\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.122644424438477 | KNN Loss: 3.0928292274475098 | BCE Loss: 1.0298149585723877\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.129324436187744 | KNN Loss: 3.102710723876953 | BCE Loss: 1.026613712310791\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.120645046234131 | KNN Loss: 3.0898661613464355 | BCE Loss: 1.0307788848876953\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.132267475128174 | KNN Loss: 3.1073999404907227 | BCE Loss: 1.0248676538467407\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.137405872344971 | KNN Loss: 3.1125693321228027 | BCE Loss: 1.024836540222168\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.152868270874023 | KNN Loss: 3.129652976989746 | BCE Loss: 1.0232152938842773\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.144771099090576 | KNN Loss: 3.1141884326934814 | BCE Loss: 1.0305825471878052\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.133363723754883 | KNN Loss: 3.0959205627441406 | BCE Loss: 1.0374432802200317\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.141237258911133 | KNN Loss: 3.103057861328125 | BCE Loss: 1.038179636001587\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.144487380981445 | KNN Loss: 3.1186745166778564 | BCE Loss: 1.0258128643035889\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.124388217926025 | KNN Loss: 3.1197891235351562 | BCE Loss: 1.0045992136001587\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.11978816986084 | KNN Loss: 3.112729072570801 | BCE Loss: 1.0070592164993286\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.137645244598389 | KNN Loss: 3.1257917881011963 | BCE Loss: 1.0118534564971924\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.092094421386719 | KNN Loss: 3.076580047607422 | BCE Loss: 1.015514612197876\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.108401298522949 | KNN Loss: 3.086418390274048 | BCE Loss: 1.021983027458191\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.178775787353516 | KNN Loss: 3.1277456283569336 | BCE Loss: 1.0510303974151611\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.142518520355225 | KNN Loss: 3.111128807067871 | BCE Loss: 1.0313897132873535\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.0888848304748535 | KNN Loss: 3.0998575687408447 | BCE Loss: 0.9890271425247192\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.131776809692383 | KNN Loss: 3.0950400829315186 | BCE Loss: 1.0367367267608643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.154972553253174 | KNN Loss: 3.1094422340393066 | BCE Loss: 1.0455303192138672\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.079039573669434 | KNN Loss: 3.077627420425415 | BCE Loss: 1.0014121532440186\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.123193264007568 | KNN Loss: 3.1338441371917725 | BCE Loss: 0.9893491864204407\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.125030517578125 | KNN Loss: 3.1264941692352295 | BCE Loss: 0.9985362887382507\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.1130547523498535 | KNN Loss: 3.090869903564453 | BCE Loss: 1.0221848487854004\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.150155067443848 | KNN Loss: 3.087007522583008 | BCE Loss: 1.0631476640701294\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.105134963989258 | KNN Loss: 3.070647954940796 | BCE Loss: 1.034487009048462\n",
      "Epoch    87: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.127814769744873 | KNN Loss: 3.097935438156128 | BCE Loss: 1.0298793315887451\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.143135070800781 | KNN Loss: 3.093183994293213 | BCE Loss: 1.0499510765075684\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.091948986053467 | KNN Loss: 3.091440200805664 | BCE Loss: 1.0005087852478027\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.152286529541016 | KNN Loss: 3.1073431968688965 | BCE Loss: 1.0449435710906982\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.097978591918945 | KNN Loss: 3.0646262168884277 | BCE Loss: 1.033352255821228\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.142368316650391 | KNN Loss: 3.1181275844573975 | BCE Loss: 1.0242407321929932\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.106200218200684 | KNN Loss: 3.099944591522217 | BCE Loss: 1.0062557458877563\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.108841896057129 | KNN Loss: 3.092899799346924 | BCE Loss: 1.0159419775009155\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.077051162719727 | KNN Loss: 3.0752182006835938 | BCE Loss: 1.0018327236175537\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.116250038146973 | KNN Loss: 3.076648473739624 | BCE Loss: 1.0396013259887695\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.117077827453613 | KNN Loss: 3.099461793899536 | BCE Loss: 1.0176160335540771\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.1157941818237305 | KNN Loss: 3.0972301959991455 | BCE Loss: 1.018563985824585\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.0979180335998535 | KNN Loss: 3.0568666458129883 | BCE Loss: 1.0410513877868652\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.128418922424316 | KNN Loss: 3.0881056785583496 | BCE Loss: 1.0403130054473877\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.096631050109863 | KNN Loss: 3.075446128845215 | BCE Loss: 1.0211851596832275\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.122477054595947 | KNN Loss: 3.094409704208374 | BCE Loss: 1.0280673503875732\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.094273567199707 | KNN Loss: 3.0816264152526855 | BCE Loss: 1.0126473903656006\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.127965927124023 | KNN Loss: 3.0831148624420166 | BCE Loss: 1.0448510646820068\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.129002571105957 | KNN Loss: 3.1095993518829346 | BCE Loss: 1.0194034576416016\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.098492622375488 | KNN Loss: 3.0969645977020264 | BCE Loss: 1.0015279054641724\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.114861488342285 | KNN Loss: 3.0663089752197266 | BCE Loss: 1.0485525131225586\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.124216556549072 | KNN Loss: 3.1010255813598633 | BCE Loss: 1.0231910943984985\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.134531497955322 | KNN Loss: 3.0998311042785645 | BCE Loss: 1.0347005128860474\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.122819900512695 | KNN Loss: 3.093472719192505 | BCE Loss: 1.0293471813201904\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.079755783081055 | KNN Loss: 3.068573474884033 | BCE Loss: 1.0111825466156006\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.112638473510742 | KNN Loss: 3.117327928543091 | BCE Loss: 0.9953107237815857\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.115949630737305 | KNN Loss: 3.09857177734375 | BCE Loss: 1.0173776149749756\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.14285945892334 | KNN Loss: 3.121384382247925 | BCE Loss: 1.021475076675415\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.173678398132324 | KNN Loss: 3.0879313945770264 | BCE Loss: 1.0857468843460083\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.071406364440918 | KNN Loss: 3.0682778358459473 | BCE Loss: 1.0031285285949707\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.085657119750977 | KNN Loss: 3.0838942527770996 | BCE Loss: 1.0017627477645874\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.123944282531738 | KNN Loss: 3.096987247467041 | BCE Loss: 1.0269572734832764\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.106317520141602 | KNN Loss: 3.0768027305603027 | BCE Loss: 1.029515027999878\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.113633632659912 | KNN Loss: 3.087272882461548 | BCE Loss: 1.0263608694076538\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.110145568847656 | KNN Loss: 3.0643467903137207 | BCE Loss: 1.0457990169525146\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.07729959487915 | KNN Loss: 3.088421106338501 | BCE Loss: 0.9888784885406494\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.125042915344238 | KNN Loss: 3.0912930965423584 | BCE Loss: 1.0337495803833008\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.161280632019043 | KNN Loss: 3.1308681964874268 | BCE Loss: 1.0304125547409058\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.132073402404785 | KNN Loss: 3.096153974533081 | BCE Loss: 1.035919427871704\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.085772514343262 | KNN Loss: 3.0991179943084717 | BCE Loss: 0.9866544008255005\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.107027053833008 | KNN Loss: 3.0684947967529297 | BCE Loss: 1.0385322570800781\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.09263801574707 | KNN Loss: 3.0840768814086914 | BCE Loss: 1.008561372756958\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.045168876647949 | KNN Loss: 3.054945468902588 | BCE Loss: 0.9902231693267822\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.145035743713379 | KNN Loss: 3.1124370098114014 | BCE Loss: 1.032598853111267\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.113868713378906 | KNN Loss: 3.081144332885742 | BCE Loss: 1.0327244997024536\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.110263824462891 | KNN Loss: 3.1152074337005615 | BCE Loss: 0.9950565099716187\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.105713844299316 | KNN Loss: 3.082989454269409 | BCE Loss: 1.0227246284484863\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.104241371154785 | KNN Loss: 3.0916199684143066 | BCE Loss: 1.0126211643218994\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.132192134857178 | KNN Loss: 3.0804860591888428 | BCE Loss: 1.0517059564590454\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.115152835845947 | KNN Loss: 3.1043498516082764 | BCE Loss: 1.0108031034469604\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.106470584869385 | KNN Loss: 3.0842463970184326 | BCE Loss: 1.0222243070602417\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.071776866912842 | KNN Loss: 3.0742597579956055 | BCE Loss: 0.9975169897079468\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.101953983306885 | KNN Loss: 3.084470748901367 | BCE Loss: 1.0174832344055176\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.08695125579834 | KNN Loss: 3.0884666442871094 | BCE Loss: 0.9984846711158752\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.113466262817383 | KNN Loss: 3.0702109336853027 | BCE Loss: 1.04325532913208\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.1241960525512695 | KNN Loss: 3.0966598987579346 | BCE Loss: 1.0275362730026245\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.16552734375 | KNN Loss: 3.1165363788604736 | BCE Loss: 1.0489912033081055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.105724334716797 | KNN Loss: 3.0808966159820557 | BCE Loss: 1.024827480316162\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.126381874084473 | KNN Loss: 3.100461721420288 | BCE Loss: 1.0259203910827637\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.138910293579102 | KNN Loss: 3.1139800548553467 | BCE Loss: 1.024930477142334\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.097437381744385 | KNN Loss: 3.0835530757904053 | BCE Loss: 1.0138843059539795\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.14650297164917 | KNN Loss: 3.130908489227295 | BCE Loss: 1.015594482421875\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.142204761505127 | KNN Loss: 3.1192705631256104 | BCE Loss: 1.0229341983795166\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.174666404724121 | KNN Loss: 3.1399638652801514 | BCE Loss: 1.0347023010253906\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.112357139587402 | KNN Loss: 3.097900629043579 | BCE Loss: 1.0144566297531128\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.099400520324707 | KNN Loss: 3.094996452331543 | BCE Loss: 1.0044043064117432\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.099589824676514 | KNN Loss: 3.0655741691589355 | BCE Loss: 1.0340156555175781\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.154435634613037 | KNN Loss: 3.1090192794799805 | BCE Loss: 1.0454163551330566\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.098027229309082 | KNN Loss: 3.0866518020629883 | BCE Loss: 1.0113751888275146\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.128145217895508 | KNN Loss: 3.103374481201172 | BCE Loss: 1.024770736694336\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.072940826416016 | KNN Loss: 3.05737042427063 | BCE Loss: 1.0155705213546753\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.13626766204834 | KNN Loss: 3.101383686065674 | BCE Loss: 1.0348842144012451\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.137135982513428 | KNN Loss: 3.1140682697296143 | BCE Loss: 1.0230677127838135\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.120143890380859 | KNN Loss: 3.105987548828125 | BCE Loss: 1.0141561031341553\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.135710716247559 | KNN Loss: 3.1188173294067383 | BCE Loss: 1.0168931484222412\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.100316047668457 | KNN Loss: 3.08166766166687 | BCE Loss: 1.018648624420166\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.136961936950684 | KNN Loss: 3.086463212966919 | BCE Loss: 1.0504984855651855\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.07729434967041 | KNN Loss: 3.0611572265625 | BCE Loss: 1.016136884689331\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.125973224639893 | KNN Loss: 3.0939903259277344 | BCE Loss: 1.0319827795028687\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.1420488357543945 | KNN Loss: 3.1120264530181885 | BCE Loss: 1.0300222635269165\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.118544578552246 | KNN Loss: 3.0789220333099365 | BCE Loss: 1.0396225452423096\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.128650665283203 | KNN Loss: 3.102505922317505 | BCE Loss: 1.0261445045471191\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.110069751739502 | KNN Loss: 3.111388921737671 | BCE Loss: 0.998680830001831\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.080657958984375 | KNN Loss: 3.078216075897217 | BCE Loss: 1.002441644668579\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.104516983032227 | KNN Loss: 3.085362434387207 | BCE Loss: 1.0191543102264404\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.100967884063721 | KNN Loss: 3.082653522491455 | BCE Loss: 1.0183143615722656\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.144239902496338 | KNN Loss: 3.0867841243743896 | BCE Loss: 1.0574558973312378\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.177571773529053 | KNN Loss: 3.1057045459747314 | BCE Loss: 1.0718673467636108\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.094701290130615 | KNN Loss: 3.075388193130493 | BCE Loss: 1.019313097000122\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.1343913078308105 | KNN Loss: 3.0775182247161865 | BCE Loss: 1.056873083114624\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.153003692626953 | KNN Loss: 3.1170129776000977 | BCE Loss: 1.035990595817566\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.135918140411377 | KNN Loss: 3.1064953804016113 | BCE Loss: 1.0294227600097656\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.1501970291137695 | KNN Loss: 3.111978530883789 | BCE Loss: 1.0382187366485596\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.127165794372559 | KNN Loss: 3.1162302494049072 | BCE Loss: 1.010935664176941\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.104275703430176 | KNN Loss: 3.0671157836914062 | BCE Loss: 1.0371596813201904\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.081495761871338 | KNN Loss: 3.0582785606384277 | BCE Loss: 1.0232170820236206\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.114223957061768 | KNN Loss: 3.085517644882202 | BCE Loss: 1.0287063121795654\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.119649410247803 | KNN Loss: 3.1039280891418457 | BCE Loss: 1.0157214403152466\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.100956439971924 | KNN Loss: 3.0809240341186523 | BCE Loss: 1.0200324058532715\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.115252494812012 | KNN Loss: 3.0854392051696777 | BCE Loss: 1.0298131704330444\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.112086296081543 | KNN Loss: 3.0810165405273438 | BCE Loss: 1.0310697555541992\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.098424434661865 | KNN Loss: 3.0671815872192383 | BCE Loss: 1.0312427282333374\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.09676456451416 | KNN Loss: 3.053027629852295 | BCE Loss: 1.0437366962432861\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.101799011230469 | KNN Loss: 3.097198724746704 | BCE Loss: 1.004600167274475\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.144139766693115 | KNN Loss: 3.101027011871338 | BCE Loss: 1.043112874031067\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.097559928894043 | KNN Loss: 3.0708091259002686 | BCE Loss: 1.0267506837844849\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.143801689147949 | KNN Loss: 3.1085128784179688 | BCE Loss: 1.0352885723114014\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.089669704437256 | KNN Loss: 3.0658655166625977 | BCE Loss: 1.0238040685653687\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.104986190795898 | KNN Loss: 3.0619795322418213 | BCE Loss: 1.0430065393447876\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.10642671585083 | KNN Loss: 3.0901756286621094 | BCE Loss: 1.0162510871887207\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.107843399047852 | KNN Loss: 3.08073091506958 | BCE Loss: 1.0271124839782715\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.080237865447998 | KNN Loss: 3.0587213039398193 | BCE Loss: 1.0215165615081787\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.117061138153076 | KNN Loss: 3.0823752880096436 | BCE Loss: 1.0346858501434326\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.079094886779785 | KNN Loss: 3.0794270038604736 | BCE Loss: 0.999667763710022\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.100595951080322 | KNN Loss: 3.0699679851531982 | BCE Loss: 1.030627965927124\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.1361284255981445 | KNN Loss: 3.0915470123291016 | BCE Loss: 1.0445811748504639\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.097160339355469 | KNN Loss: 3.079094886779785 | BCE Loss: 1.0180654525756836\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.105860233306885 | KNN Loss: 3.080962657928467 | BCE Loss: 1.024897575378418\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.110285758972168 | KNN Loss: 3.0813937187194824 | BCE Loss: 1.0288920402526855\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.080385684967041 | KNN Loss: 3.0544192790985107 | BCE Loss: 1.0259664058685303\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.122035026550293 | KNN Loss: 3.074784517288208 | BCE Loss: 1.0472502708435059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.101624965667725 | KNN Loss: 3.1017003059387207 | BCE Loss: 0.9999246001243591\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.122209548950195 | KNN Loss: 3.084055185317993 | BCE Loss: 1.0381546020507812\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.142438888549805 | KNN Loss: 3.110187530517578 | BCE Loss: 1.0322511196136475\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.084172248840332 | KNN Loss: 3.0965769290924072 | BCE Loss: 0.98759526014328\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.075799465179443 | KNN Loss: 3.048888683319092 | BCE Loss: 1.0269107818603516\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.079070568084717 | KNN Loss: 3.072615146636963 | BCE Loss: 1.006455421447754\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.090881824493408 | KNN Loss: 3.0759220123291016 | BCE Loss: 1.0149598121643066\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.09749698638916 | KNN Loss: 3.059884548187256 | BCE Loss: 1.0376125574111938\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.088372230529785 | KNN Loss: 3.0719423294067383 | BCE Loss: 1.0164297819137573\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.104648113250732 | KNN Loss: 3.077106475830078 | BCE Loss: 1.0275416374206543\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.0497846603393555 | KNN Loss: 3.0587363243103027 | BCE Loss: 0.9910482168197632\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.084238529205322 | KNN Loss: 3.05644154548645 | BCE Loss: 1.027796983718872\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.088730812072754 | KNN Loss: 3.090167999267578 | BCE Loss: 0.9985630512237549\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.110648155212402 | KNN Loss: 3.074805974960327 | BCE Loss: 1.0358424186706543\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.130573272705078 | KNN Loss: 3.0858991146087646 | BCE Loss: 1.0446741580963135\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.1120524406433105 | KNN Loss: 3.0812947750091553 | BCE Loss: 1.0307576656341553\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.070260047912598 | KNN Loss: 3.0394134521484375 | BCE Loss: 1.0308468341827393\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.144419193267822 | KNN Loss: 3.0845510959625244 | BCE Loss: 1.0598679780960083\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.112382411956787 | KNN Loss: 3.0928938388824463 | BCE Loss: 1.0194885730743408\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.054722309112549 | KNN Loss: 3.046247959136963 | BCE Loss: 1.0084742307662964\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.135323524475098 | KNN Loss: 3.1244654655456543 | BCE Loss: 1.0108582973480225\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.0986647605896 | KNN Loss: 3.051020860671997 | BCE Loss: 1.0476438999176025\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.124760150909424 | KNN Loss: 3.0872910022735596 | BCE Loss: 1.0374691486358643\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.140629768371582 | KNN Loss: 3.0979866981506348 | BCE Loss: 1.0426428318023682\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.15336799621582 | KNN Loss: 3.1125175952911377 | BCE Loss: 1.0408501625061035\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.151702404022217 | KNN Loss: 3.113215923309326 | BCE Loss: 1.0384864807128906\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.1486968994140625 | KNN Loss: 3.112412691116333 | BCE Loss: 1.0362842082977295\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.1000776290893555 | KNN Loss: 3.080491542816162 | BCE Loss: 1.0195863246917725\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.134438514709473 | KNN Loss: 3.0996296405792236 | BCE Loss: 1.03480863571167\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.116034507751465 | KNN Loss: 3.101907253265381 | BCE Loss: 1.014127254486084\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.135009288787842 | KNN Loss: 3.092468738555908 | BCE Loss: 1.042540431022644\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.101079940795898 | KNN Loss: 3.0703930854797363 | BCE Loss: 1.0306870937347412\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.0864129066467285 | KNN Loss: 3.091487407684326 | BCE Loss: 0.9949256777763367\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.117348670959473 | KNN Loss: 3.0838680267333984 | BCE Loss: 1.0334807634353638\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.125774383544922 | KNN Loss: 3.1222007274627686 | BCE Loss: 1.0035738945007324\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.0634565353393555 | KNN Loss: 3.0529916286468506 | BCE Loss: 1.0104650259017944\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.121104717254639 | KNN Loss: 3.0949878692626953 | BCE Loss: 1.0261167287826538\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.130660057067871 | KNN Loss: 3.0841400623321533 | BCE Loss: 1.0465202331542969\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.131150245666504 | KNN Loss: 3.1068010330200195 | BCE Loss: 1.024349331855774\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.078306198120117 | KNN Loss: 3.076092481613159 | BCE Loss: 1.002213716506958\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.092879295349121 | KNN Loss: 3.07505202293396 | BCE Loss: 1.0178275108337402\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.075328350067139 | KNN Loss: 3.0884571075439453 | BCE Loss: 0.9868713617324829\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.129057884216309 | KNN Loss: 3.1138131618499756 | BCE Loss: 1.0152446031570435\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.1245574951171875 | KNN Loss: 3.127070665359497 | BCE Loss: 0.9974870085716248\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.138948440551758 | KNN Loss: 3.1040945053100586 | BCE Loss: 1.0348539352416992\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.073241233825684 | KNN Loss: 3.066995620727539 | BCE Loss: 1.0062458515167236\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.088766574859619 | KNN Loss: 3.071913719177246 | BCE Loss: 1.016852855682373\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.083648681640625 | KNN Loss: 3.073991537094116 | BCE Loss: 1.0096571445465088\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.128108024597168 | KNN Loss: 3.0973312854766846 | BCE Loss: 1.0307769775390625\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.134470462799072 | KNN Loss: 3.094038248062134 | BCE Loss: 1.0404322147369385\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.149074554443359 | KNN Loss: 3.117450475692749 | BCE Loss: 1.0316238403320312\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.115325450897217 | KNN Loss: 3.060645580291748 | BCE Loss: 1.0546799898147583\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.107706069946289 | KNN Loss: 3.0753660202026367 | BCE Loss: 1.0323398113250732\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.089840888977051 | KNN Loss: 3.0932741165161133 | BCE Loss: 0.9965665340423584\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.130947113037109 | KNN Loss: 3.1004230976104736 | BCE Loss: 1.0305242538452148\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.101748943328857 | KNN Loss: 3.078012704849243 | BCE Loss: 1.0237361192703247\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.102140426635742 | KNN Loss: 3.0797970294952393 | BCE Loss: 1.0223432779312134\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.083242893218994 | KNN Loss: 3.0665500164031982 | BCE Loss: 1.016692876815796\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.089469909667969 | KNN Loss: 3.064199924468994 | BCE Loss: 1.0252697467803955\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.099954605102539 | KNN Loss: 3.082160234451294 | BCE Loss: 1.0177946090698242\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.110332012176514 | KNN Loss: 3.0890493392944336 | BCE Loss: 1.02128267288208\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.117491722106934 | KNN Loss: 3.078521490097046 | BCE Loss: 1.0389703512191772\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.080203056335449 | KNN Loss: 3.0689008235931396 | BCE Loss: 1.0113023519515991\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.152885913848877 | KNN Loss: 3.111149311065674 | BCE Loss: 1.0417364835739136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.088991641998291 | KNN Loss: 3.0749428272247314 | BCE Loss: 1.0140489339828491\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.11130428314209 | KNN Loss: 3.0946810245513916 | BCE Loss: 1.0166232585906982\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.143617630004883 | KNN Loss: 3.099778175354004 | BCE Loss: 1.0438392162322998\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.0449113845825195 | KNN Loss: 3.0432794094085693 | BCE Loss: 1.0016318559646606\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.152543544769287 | KNN Loss: 3.1343414783477783 | BCE Loss: 1.0182021856307983\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.158658027648926 | KNN Loss: 3.128305196762085 | BCE Loss: 1.0303528308868408\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.099148750305176 | KNN Loss: 3.0938189029693604 | BCE Loss: 1.005329966545105\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.047561168670654 | KNN Loss: 3.0574731826782227 | BCE Loss: 0.9900878667831421\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.1532673835754395 | KNN Loss: 3.1218223571777344 | BCE Loss: 1.031445026397705\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.168099403381348 | KNN Loss: 3.1354727745056152 | BCE Loss: 1.0326268672943115\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.1200456619262695 | KNN Loss: 3.111408233642578 | BCE Loss: 1.0086374282836914\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.080879211425781 | KNN Loss: 3.1088531017303467 | BCE Loss: 0.9720263481140137\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.133927345275879 | KNN Loss: 3.108938694000244 | BCE Loss: 1.0249884128570557\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.091879844665527 | KNN Loss: 3.0928761959075928 | BCE Loss: 0.9990035891532898\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.136440277099609 | KNN Loss: 3.1014256477355957 | BCE Loss: 1.0350143909454346\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.122529029846191 | KNN Loss: 3.0723767280578613 | BCE Loss: 1.05015230178833\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.116870880126953 | KNN Loss: 3.0585827827453613 | BCE Loss: 1.0582882165908813\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.052087306976318 | KNN Loss: 3.054562568664551 | BCE Loss: 0.9975246787071228\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.077723026275635 | KNN Loss: 3.0462253093719482 | BCE Loss: 1.0314977169036865\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.115357398986816 | KNN Loss: 3.0358028411865234 | BCE Loss: 1.079554796218872\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.103268623352051 | KNN Loss: 3.089808225631714 | BCE Loss: 1.013460636138916\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.13518762588501 | KNN Loss: 3.0969836711883545 | BCE Loss: 1.0382039546966553\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.115627288818359 | KNN Loss: 3.094905138015747 | BCE Loss: 1.0207219123840332\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.103443622589111 | KNN Loss: 3.078411340713501 | BCE Loss: 1.0250321626663208\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.1247711181640625 | KNN Loss: 3.080871820449829 | BCE Loss: 1.0438992977142334\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.124467849731445 | KNN Loss: 3.0716264247894287 | BCE Loss: 1.0528411865234375\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.101146221160889 | KNN Loss: 3.099283456802368 | BCE Loss: 1.0018627643585205\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.073139667510986 | KNN Loss: 3.098400592803955 | BCE Loss: 0.9747389554977417\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.107224941253662 | KNN Loss: 3.0852608680725098 | BCE Loss: 1.0219640731811523\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.140218734741211 | KNN Loss: 3.0992345809936523 | BCE Loss: 1.0409841537475586\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.055222034454346 | KNN Loss: 3.061619997024536 | BCE Loss: 0.9936021566390991\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.123457908630371 | KNN Loss: 3.1150453090667725 | BCE Loss: 1.0084123611450195\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.091431140899658 | KNN Loss: 3.0784926414489746 | BCE Loss: 1.0129386186599731\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.088386535644531 | KNN Loss: 3.085392951965332 | BCE Loss: 1.0029934644699097\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.093981742858887 | KNN Loss: 3.0776264667510986 | BCE Loss: 1.0163553953170776\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.1081414222717285 | KNN Loss: 3.047511100769043 | BCE Loss: 1.060630440711975\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.121091842651367 | KNN Loss: 3.0877721309661865 | BCE Loss: 1.0333197116851807\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.08979606628418 | KNN Loss: 3.0673234462738037 | BCE Loss: 1.0224723815917969\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.123105049133301 | KNN Loss: 3.074448823928833 | BCE Loss: 1.0486563444137573\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.099456787109375 | KNN Loss: 3.081629514694214 | BCE Loss: 1.017827033996582\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.134467124938965 | KNN Loss: 3.1035141944885254 | BCE Loss: 1.0309531688690186\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.09906005859375 | KNN Loss: 3.0758745670318604 | BCE Loss: 1.0231857299804688\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.102144241333008 | KNN Loss: 3.100010395050049 | BCE Loss: 1.002133846282959\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.083898544311523 | KNN Loss: 3.063750743865967 | BCE Loss: 1.0201475620269775\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.097476959228516 | KNN Loss: 3.098146915435791 | BCE Loss: 0.9993298053741455\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.1142401695251465 | KNN Loss: 3.094187021255493 | BCE Loss: 1.0200532674789429\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.060898780822754 | KNN Loss: 3.0880706310272217 | BCE Loss: 0.9728282690048218\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.084954261779785 | KNN Loss: 3.073150873184204 | BCE Loss: 1.011803388595581\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.11106014251709 | KNN Loss: 3.072113513946533 | BCE Loss: 1.0389468669891357\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.077751159667969 | KNN Loss: 3.063342571258545 | BCE Loss: 1.014408826828003\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.107548236846924 | KNN Loss: 3.0676515102386475 | BCE Loss: 1.0398966073989868\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.104452133178711 | KNN Loss: 3.080531120300293 | BCE Loss: 1.0239211320877075\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.089606285095215 | KNN Loss: 3.0617270469665527 | BCE Loss: 1.027879238128662\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.113408088684082 | KNN Loss: 3.081380844116211 | BCE Loss: 1.032027244567871\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.122313976287842 | KNN Loss: 3.066450595855713 | BCE Loss: 1.055863380432129\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.127797603607178 | KNN Loss: 3.087061882019043 | BCE Loss: 1.0407357215881348\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.0822343826293945 | KNN Loss: 3.071012258529663 | BCE Loss: 1.011222004890442\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.142818927764893 | KNN Loss: 3.1088645458221436 | BCE Loss: 1.0339542627334595\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.0854716300964355 | KNN Loss: 3.0726122856140137 | BCE Loss: 1.0128593444824219\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.104974746704102 | KNN Loss: 3.080944776535034 | BCE Loss: 1.0240299701690674\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.056138515472412 | KNN Loss: 3.0604450702667236 | BCE Loss: 0.995693564414978\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.083251953125 | KNN Loss: 3.0726587772369385 | BCE Loss: 1.0105934143066406\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.07376766204834 | KNN Loss: 3.0761828422546387 | BCE Loss: 0.9975849390029907\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.139225959777832 | KNN Loss: 3.106051445007324 | BCE Loss: 1.0331742763519287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.1043243408203125 | KNN Loss: 3.0825438499450684 | BCE Loss: 1.0217804908752441\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.088597297668457 | KNN Loss: 3.075810670852661 | BCE Loss: 1.0127863883972168\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.077401638031006 | KNN Loss: 3.041799306869507 | BCE Loss: 1.0356022119522095\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.063545227050781 | KNN Loss: 3.0408759117126465 | BCE Loss: 1.0226693153381348\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.133244514465332 | KNN Loss: 3.117233991622925 | BCE Loss: 1.0160107612609863\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.120431423187256 | KNN Loss: 3.1015899181365967 | BCE Loss: 1.0188416242599487\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.146996974945068 | KNN Loss: 3.0999042987823486 | BCE Loss: 1.0470927953720093\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.089232444763184 | KNN Loss: 3.0676262378692627 | BCE Loss: 1.0216064453125\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.110850811004639 | KNN Loss: 3.099726438522339 | BCE Loss: 1.0111244916915894\n",
      "Epoch   130: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.1092729568481445 | KNN Loss: 3.104395627975464 | BCE Loss: 1.0048774480819702\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.122642517089844 | KNN Loss: 3.1067163944244385 | BCE Loss: 1.0159258842468262\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.057133197784424 | KNN Loss: 3.076594591140747 | BCE Loss: 0.980538547039032\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.130529403686523 | KNN Loss: 3.1036722660064697 | BCE Loss: 1.0268568992614746\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.054470062255859 | KNN Loss: 3.05891752243042 | BCE Loss: 0.9955525398254395\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.14161491394043 | KNN Loss: 3.079580068588257 | BCE Loss: 1.0620348453521729\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.105185508728027 | KNN Loss: 3.0522165298461914 | BCE Loss: 1.052969217300415\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.101778984069824 | KNN Loss: 3.0948283672332764 | BCE Loss: 1.0069506168365479\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.082070350646973 | KNN Loss: 3.069293737411499 | BCE Loss: 1.0127766132354736\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.04239559173584 | KNN Loss: 3.027186632156372 | BCE Loss: 1.0152089595794678\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.113698959350586 | KNN Loss: 3.096550226211548 | BCE Loss: 1.017148494720459\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.114733695983887 | KNN Loss: 3.0936367511749268 | BCE Loss: 1.021097183227539\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.069072723388672 | KNN Loss: 3.07174015045166 | BCE Loss: 0.9973324537277222\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.102364540100098 | KNN Loss: 3.08990216255188 | BCE Loss: 1.0124622583389282\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.090946197509766 | KNN Loss: 3.0748789310455322 | BCE Loss: 1.0160672664642334\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.074775218963623 | KNN Loss: 3.080735206604004 | BCE Loss: 0.9940398931503296\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.160022735595703 | KNN Loss: 3.118579626083374 | BCE Loss: 1.0414433479309082\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.104190826416016 | KNN Loss: 3.0726218223571777 | BCE Loss: 1.0315687656402588\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.126560688018799 | KNN Loss: 3.099989891052246 | BCE Loss: 1.0265707969665527\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.093756675720215 | KNN Loss: 3.08565354347229 | BCE Loss: 1.008103370666504\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.083612442016602 | KNN Loss: 3.0865721702575684 | BCE Loss: 0.9970403909683228\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.081467628479004 | KNN Loss: 3.0577094554901123 | BCE Loss: 1.0237581729888916\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.112374305725098 | KNN Loss: 3.0854053497314453 | BCE Loss: 1.0269687175750732\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.107489585876465 | KNN Loss: 3.0959620475769043 | BCE Loss: 1.0115277767181396\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.068251609802246 | KNN Loss: 3.0509793758392334 | BCE Loss: 1.0172724723815918\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.127357482910156 | KNN Loss: 3.0974013805389404 | BCE Loss: 1.0299558639526367\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.097595691680908 | KNN Loss: 3.082216739654541 | BCE Loss: 1.0153789520263672\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.121369361877441 | KNN Loss: 3.066134452819824 | BCE Loss: 1.0552349090576172\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.065941333770752 | KNN Loss: 3.060441493988037 | BCE Loss: 1.0054997205734253\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.080887794494629 | KNN Loss: 3.0656003952026367 | BCE Loss: 1.015287160873413\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.074572563171387 | KNN Loss: 3.057666301727295 | BCE Loss: 1.0169062614440918\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.102743625640869 | KNN Loss: 3.0917181968688965 | BCE Loss: 1.0110255479812622\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.118981838226318 | KNN Loss: 3.0971896648406982 | BCE Loss: 1.0217921733856201\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.1017656326293945 | KNN Loss: 3.0684008598327637 | BCE Loss: 1.0333645343780518\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.107874393463135 | KNN Loss: 3.106783390045166 | BCE Loss: 1.0010910034179688\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.073760032653809 | KNN Loss: 3.0579111576080322 | BCE Loss: 1.0158486366271973\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.071993827819824 | KNN Loss: 3.053387403488159 | BCE Loss: 1.018606185913086\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.107501029968262 | KNN Loss: 3.0722928047180176 | BCE Loss: 1.0352081060409546\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.108956336975098 | KNN Loss: 3.0756356716156006 | BCE Loss: 1.033320665359497\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.129353046417236 | KNN Loss: 3.0739634037017822 | BCE Loss: 1.055389642715454\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.117412090301514 | KNN Loss: 3.0780324935913086 | BCE Loss: 1.0393797159194946\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.086241245269775 | KNN Loss: 3.0473060607910156 | BCE Loss: 1.0389350652694702\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.087246894836426 | KNN Loss: 3.0615787506103516 | BCE Loss: 1.0256682634353638\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.061514854431152 | KNN Loss: 3.057651996612549 | BCE Loss: 1.0038630962371826\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.139833450317383 | KNN Loss: 3.09517240524292 | BCE Loss: 1.0446608066558838\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.0744428634643555 | KNN Loss: 3.06900954246521 | BCE Loss: 1.005433201789856\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.120887756347656 | KNN Loss: 3.090050458908081 | BCE Loss: 1.0308375358581543\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.134600639343262 | KNN Loss: 3.108943223953247 | BCE Loss: 1.0256574153900146\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.085338592529297 | KNN Loss: 3.0569565296173096 | BCE Loss: 1.0283818244934082\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.083897590637207 | KNN Loss: 3.0511701107025146 | BCE Loss: 1.0327272415161133\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.108572959899902 | KNN Loss: 3.0842456817626953 | BCE Loss: 1.0243275165557861\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.107448577880859 | KNN Loss: 3.079293727874756 | BCE Loss: 1.0281546115875244\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.107123851776123 | KNN Loss: 3.0703506469726562 | BCE Loss: 1.0367730855941772\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.071064472198486 | KNN Loss: 3.057631254196167 | BCE Loss: 1.0134333372116089\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.087011337280273 | KNN Loss: 3.052297353744507 | BCE Loss: 1.0347142219543457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.118356704711914 | KNN Loss: 3.0806989669799805 | BCE Loss: 1.0376577377319336\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.10492467880249 | KNN Loss: 3.072922468185425 | BCE Loss: 1.032002329826355\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.024467468261719 | KNN Loss: 3.0274770259857178 | BCE Loss: 0.9969906210899353\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.090290069580078 | KNN Loss: 3.076848268508911 | BCE Loss: 1.0134419202804565\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.08042573928833 | KNN Loss: 3.1041672229766846 | BCE Loss: 0.9762583374977112\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.092552661895752 | KNN Loss: 3.054715871810913 | BCE Loss: 1.0378366708755493\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.127178192138672 | KNN Loss: 3.0826611518859863 | BCE Loss: 1.0445168018341064\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.079256057739258 | KNN Loss: 3.0691330432891846 | BCE Loss: 1.0101227760314941\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.038496494293213 | KNN Loss: 3.045496702194214 | BCE Loss: 0.992999792098999\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.065143585205078 | KNN Loss: 3.0728368759155273 | BCE Loss: 0.9923068284988403\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.109652519226074 | KNN Loss: 3.0946757793426514 | BCE Loss: 1.0149768590927124\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.101143836975098 | KNN Loss: 3.062633991241455 | BCE Loss: 1.0385100841522217\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.11635684967041 | KNN Loss: 3.089719533920288 | BCE Loss: 1.0266375541687012\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.072286605834961 | KNN Loss: 3.0669286251068115 | BCE Loss: 1.0053577423095703\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.100534439086914 | KNN Loss: 3.0824363231658936 | BCE Loss: 1.0180981159210205\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.132965564727783 | KNN Loss: 3.1127195358276367 | BCE Loss: 1.020245909690857\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.094447135925293 | KNN Loss: 3.0691137313842773 | BCE Loss: 1.0253335237503052\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.09688663482666 | KNN Loss: 3.042221784591675 | BCE Loss: 1.0546650886535645\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.075652599334717 | KNN Loss: 3.0712974071502686 | BCE Loss: 1.0043551921844482\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.037738800048828 | KNN Loss: 3.0463390350341797 | BCE Loss: 0.9913996458053589\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.085515975952148 | KNN Loss: 3.0734808444976807 | BCE Loss: 1.0120351314544678\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.067858695983887 | KNN Loss: 3.039860725402832 | BCE Loss: 1.0279982089996338\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.059170722961426 | KNN Loss: 3.048539638519287 | BCE Loss: 1.0106312036514282\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.052128791809082 | KNN Loss: 3.060872793197632 | BCE Loss: 0.9912559390068054\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.1279497146606445 | KNN Loss: 3.102435350418091 | BCE Loss: 1.0255143642425537\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.145985126495361 | KNN Loss: 3.08526873588562 | BCE Loss: 1.0607162714004517\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.106266498565674 | KNN Loss: 3.0687859058380127 | BCE Loss: 1.0374807119369507\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.133996963500977 | KNN Loss: 3.103290557861328 | BCE Loss: 1.0307061672210693\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.109631538391113 | KNN Loss: 3.0879130363464355 | BCE Loss: 1.0217186212539673\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.12614631652832 | KNN Loss: 3.111020088195801 | BCE Loss: 1.0151264667510986\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.086957931518555 | KNN Loss: 3.0922036170959473 | BCE Loss: 0.9947542548179626\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.085398197174072 | KNN Loss: 3.0788955688476562 | BCE Loss: 1.0065025091171265\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.094220161437988 | KNN Loss: 3.0651423931121826 | BCE Loss: 1.0290775299072266\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.109164237976074 | KNN Loss: 3.0801146030426025 | BCE Loss: 1.0290498733520508\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.115121841430664 | KNN Loss: 3.077944040298462 | BCE Loss: 1.037177562713623\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.1116790771484375 | KNN Loss: 3.068312883377075 | BCE Loss: 1.0433663129806519\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.109396457672119 | KNN Loss: 3.073604106903076 | BCE Loss: 1.035792350769043\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.086605072021484 | KNN Loss: 3.0619866847991943 | BCE Loss: 1.024618148803711\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.142049789428711 | KNN Loss: 3.135178565979004 | BCE Loss: 1.0068711042404175\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.0956549644470215 | KNN Loss: 3.0883231163024902 | BCE Loss: 1.0073317289352417\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.059414386749268 | KNN Loss: 3.059537649154663 | BCE Loss: 0.9998769164085388\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.133427619934082 | KNN Loss: 3.0868258476257324 | BCE Loss: 1.04660165309906\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.092400074005127 | KNN Loss: 3.079418182373047 | BCE Loss: 1.01298189163208\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.07637882232666 | KNN Loss: 3.059783935546875 | BCE Loss: 1.016594648361206\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.0486249923706055 | KNN Loss: 3.025554656982422 | BCE Loss: 1.0230703353881836\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.091211795806885 | KNN Loss: 3.060417890548706 | BCE Loss: 1.0307939052581787\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.037100791931152 | KNN Loss: 3.0549769401550293 | BCE Loss: 0.982123613357544\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.050621509552002 | KNN Loss: 3.037245988845825 | BCE Loss: 1.0133756399154663\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.1283392906188965 | KNN Loss: 3.072762966156006 | BCE Loss: 1.0555763244628906\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.071071624755859 | KNN Loss: 3.0611960887908936 | BCE Loss: 1.0098752975463867\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.137585639953613 | KNN Loss: 3.1264328956604004 | BCE Loss: 1.0111526250839233\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.090548992156982 | KNN Loss: 3.0420148372650146 | BCE Loss: 1.0485341548919678\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.049181938171387 | KNN Loss: 3.046663284301758 | BCE Loss: 1.002518892288208\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.107640743255615 | KNN Loss: 3.0935943126678467 | BCE Loss: 1.0140464305877686\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.094634056091309 | KNN Loss: 3.079023838043213 | BCE Loss: 1.0156102180480957\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.077635765075684 | KNN Loss: 3.0695321559906006 | BCE Loss: 1.008103370666504\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.057973861694336 | KNN Loss: 3.044004440307617 | BCE Loss: 1.0139694213867188\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.070249080657959 | KNN Loss: 3.0587642192840576 | BCE Loss: 1.0114848613739014\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.0830464363098145 | KNN Loss: 3.0516574382781982 | BCE Loss: 1.0313889980316162\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.1096367835998535 | KNN Loss: 3.065479278564453 | BCE Loss: 1.04415762424469\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.078656196594238 | KNN Loss: 3.0656182765960693 | BCE Loss: 1.0130376815795898\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.082339286804199 | KNN Loss: 3.0641729831695557 | BCE Loss: 1.0181663036346436\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.1248321533203125 | KNN Loss: 3.0856411457061768 | BCE Loss: 1.0391910076141357\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.0958967208862305 | KNN Loss: 3.084383249282837 | BCE Loss: 1.0115137100219727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.031757831573486 | KNN Loss: 3.048058032989502 | BCE Loss: 0.9836997985839844\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.110666275024414 | KNN Loss: 3.087629795074463 | BCE Loss: 1.0230364799499512\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.0825514793396 | KNN Loss: 3.107619285583496 | BCE Loss: 0.974932074546814\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.098141670227051 | KNN Loss: 3.08329701423645 | BCE Loss: 1.0148447751998901\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.091085433959961 | KNN Loss: 3.047450065612793 | BCE Loss: 1.0436352491378784\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.094181060791016 | KNN Loss: 3.064678907394409 | BCE Loss: 1.0295021533966064\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.0595197677612305 | KNN Loss: 3.0255987644195557 | BCE Loss: 1.0339207649230957\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.096003532409668 | KNN Loss: 3.0674545764923096 | BCE Loss: 1.0285487174987793\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.116974830627441 | KNN Loss: 3.1218483448028564 | BCE Loss: 0.9951262474060059\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.111395835876465 | KNN Loss: 3.085819721221924 | BCE Loss: 1.025575876235962\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.1316633224487305 | KNN Loss: 3.1179847717285156 | BCE Loss: 1.0136783123016357\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.02587366104126 | KNN Loss: 3.0443689823150635 | BCE Loss: 0.9815045595169067\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.11232328414917 | KNN Loss: 3.07497239112854 | BCE Loss: 1.0373510122299194\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.076427459716797 | KNN Loss: 3.0519397258758545 | BCE Loss: 1.024487853050232\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.108233451843262 | KNN Loss: 3.0801122188568115 | BCE Loss: 1.028120994567871\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.107821464538574 | KNN Loss: 3.08075213432312 | BCE Loss: 1.027069330215454\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.0910844802856445 | KNN Loss: 3.075169086456299 | BCE Loss: 1.0159152746200562\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.108126640319824 | KNN Loss: 3.0759105682373047 | BCE Loss: 1.0322163105010986\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.095093727111816 | KNN Loss: 3.0780932903289795 | BCE Loss: 1.017000675201416\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.042657852172852 | KNN Loss: 3.0255463123321533 | BCE Loss: 1.0171117782592773\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.115083694458008 | KNN Loss: 3.0924220085144043 | BCE Loss: 1.0226616859436035\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.090823650360107 | KNN Loss: 3.0936460494995117 | BCE Loss: 0.99717777967453\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.097343444824219 | KNN Loss: 3.0907750129699707 | BCE Loss: 1.006568193435669\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.056027412414551 | KNN Loss: 3.051734447479248 | BCE Loss: 1.0042929649353027\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.052312850952148 | KNN Loss: 3.0430033206939697 | BCE Loss: 1.0093094110488892\n",
      "Epoch   154: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.092717170715332 | KNN Loss: 3.077988862991333 | BCE Loss: 1.014728307723999\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.052110195159912 | KNN Loss: 3.0792667865753174 | BCE Loss: 0.972843587398529\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.133461952209473 | KNN Loss: 3.0891456604003906 | BCE Loss: 1.044316291809082\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.140020370483398 | KNN Loss: 3.0998547077178955 | BCE Loss: 1.0401657819747925\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.09815788269043 | KNN Loss: 3.071423292160034 | BCE Loss: 1.026734471321106\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.06725549697876 | KNN Loss: 3.062293529510498 | BCE Loss: 1.0049618482589722\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.0719146728515625 | KNN Loss: 3.0492992401123047 | BCE Loss: 1.022615671157837\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.077943325042725 | KNN Loss: 3.0680155754089355 | BCE Loss: 1.009927749633789\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.109023094177246 | KNN Loss: 3.076401948928833 | BCE Loss: 1.0326213836669922\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.099802017211914 | KNN Loss: 3.0595271587371826 | BCE Loss: 1.0402746200561523\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.116726875305176 | KNN Loss: 3.079913377761841 | BCE Loss: 1.036813735961914\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.104903221130371 | KNN Loss: 3.058309555053711 | BCE Loss: 1.046593427658081\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.0657806396484375 | KNN Loss: 3.077807903289795 | BCE Loss: 0.9879728555679321\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.091185569763184 | KNN Loss: 3.082515001296997 | BCE Loss: 1.008670687675476\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.089071273803711 | KNN Loss: 3.056367874145508 | BCE Loss: 1.0327032804489136\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.091774940490723 | KNN Loss: 3.0930838584899902 | BCE Loss: 0.9986908435821533\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.132386207580566 | KNN Loss: 3.089750289916992 | BCE Loss: 1.0426361560821533\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.0336432456970215 | KNN Loss: 3.023263454437256 | BCE Loss: 1.010379672050476\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.062634468078613 | KNN Loss: 3.0490424633026123 | BCE Loss: 1.0135918855667114\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.098697662353516 | KNN Loss: 3.0886213779449463 | BCE Loss: 1.0100761651992798\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.080436706542969 | KNN Loss: 3.0629384517669678 | BCE Loss: 1.0174981355667114\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.108382701873779 | KNN Loss: 3.0890281200408936 | BCE Loss: 1.0193547010421753\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.1372785568237305 | KNN Loss: 3.119048595428467 | BCE Loss: 1.0182299613952637\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.1314263343811035 | KNN Loss: 3.0876433849334717 | BCE Loss: 1.0437828302383423\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.125248908996582 | KNN Loss: 3.0786664485931396 | BCE Loss: 1.0465822219848633\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.078954219818115 | KNN Loss: 3.065281629562378 | BCE Loss: 1.0136725902557373\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.0701727867126465 | KNN Loss: 3.0686745643615723 | BCE Loss: 1.0014982223510742\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.048912048339844 | KNN Loss: 3.0703125 | BCE Loss: 0.9785993099212646\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.100420951843262 | KNN Loss: 3.072030544281006 | BCE Loss: 1.0283902883529663\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.141458511352539 | KNN Loss: 3.070934295654297 | BCE Loss: 1.0705244541168213\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.0458574295043945 | KNN Loss: 3.0550990104675293 | BCE Loss: 0.9907581806182861\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.093156337738037 | KNN Loss: 3.074392080307007 | BCE Loss: 1.0187641382217407\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.120584011077881 | KNN Loss: 3.0669093132019043 | BCE Loss: 1.0536746978759766\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.086960315704346 | KNN Loss: 3.0612387657165527 | BCE Loss: 1.025721549987793\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.118992805480957 | KNN Loss: 3.085108995437622 | BCE Loss: 1.0338835716247559\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.112627029418945 | KNN Loss: 3.0709595680236816 | BCE Loss: 1.0416676998138428\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.10936975479126 | KNN Loss: 3.090695858001709 | BCE Loss: 1.0186738967895508\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.109101295471191 | KNN Loss: 3.1110877990722656 | BCE Loss: 0.998013436794281\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.064289569854736 | KNN Loss: 3.04213285446167 | BCE Loss: 1.0221567153930664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.068169116973877 | KNN Loss: 3.0701870918273926 | BCE Loss: 0.9979819059371948\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.014050483703613 | KNN Loss: 3.025045394897461 | BCE Loss: 0.9890052080154419\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.101040840148926 | KNN Loss: 3.0744972229003906 | BCE Loss: 1.0265438556671143\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.110960483551025 | KNN Loss: 3.1106042861938477 | BCE Loss: 1.0003561973571777\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.108709335327148 | KNN Loss: 3.078617572784424 | BCE Loss: 1.0300917625427246\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.1109771728515625 | KNN Loss: 3.067495346069336 | BCE Loss: 1.0434819459915161\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.086967945098877 | KNN Loss: 3.080873727798462 | BCE Loss: 1.0060943365097046\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.137491226196289 | KNN Loss: 3.1138575077056885 | BCE Loss: 1.0236334800720215\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.0893378257751465 | KNN Loss: 3.074768543243408 | BCE Loss: 1.0145692825317383\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.033623218536377 | KNN Loss: 3.0326457023620605 | BCE Loss: 1.0009775161743164\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.127039432525635 | KNN Loss: 3.084393262863159 | BCE Loss: 1.0426462888717651\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.081608295440674 | KNN Loss: 3.084157943725586 | BCE Loss: 0.9974502921104431\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.05386209487915 | KNN Loss: 3.0746047496795654 | BCE Loss: 0.979257345199585\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.091285705566406 | KNN Loss: 3.0691943168640137 | BCE Loss: 1.0220913887023926\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.0882368087768555 | KNN Loss: 3.070361375808716 | BCE Loss: 1.01787531375885\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.084155082702637 | KNN Loss: 3.0636680126190186 | BCE Loss: 1.020486831665039\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.082460880279541 | KNN Loss: 3.048450231552124 | BCE Loss: 1.034010648727417\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.110350608825684 | KNN Loss: 3.0705316066741943 | BCE Loss: 1.0398188829421997\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.1557936668396 | KNN Loss: 3.1004951000213623 | BCE Loss: 1.0552986860275269\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.104624271392822 | KNN Loss: 3.0744190216064453 | BCE Loss: 1.030205249786377\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.072399616241455 | KNN Loss: 3.0704233646392822 | BCE Loss: 1.0019763708114624\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.08001708984375 | KNN Loss: 3.038015365600586 | BCE Loss: 1.042001724243164\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.153219699859619 | KNN Loss: 3.117677688598633 | BCE Loss: 1.0355418920516968\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.109212875366211 | KNN Loss: 3.0846078395843506 | BCE Loss: 1.0246052742004395\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.076192855834961 | KNN Loss: 3.0615925788879395 | BCE Loss: 1.0146005153656006\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.0870795249938965 | KNN Loss: 3.055638551712036 | BCE Loss: 1.0314408540725708\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.070263385772705 | KNN Loss: 3.0732421875 | BCE Loss: 0.9970213770866394\n",
      "Epoch   165: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.114209175109863 | KNN Loss: 3.0993943214416504 | BCE Loss: 1.014814853668213\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.075057029724121 | KNN Loss: 3.056290864944458 | BCE Loss: 1.018766164779663\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.111060619354248 | KNN Loss: 3.0960965156555176 | BCE Loss: 1.0149641036987305\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.135108947753906 | KNN Loss: 3.102419853210449 | BCE Loss: 1.0326893329620361\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.094902038574219 | KNN Loss: 3.075202703475952 | BCE Loss: 1.0196990966796875\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.024054527282715 | KNN Loss: 3.0154573917388916 | BCE Loss: 1.0085968971252441\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.086080551147461 | KNN Loss: 3.096952438354492 | BCE Loss: 0.989128053188324\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.083354949951172 | KNN Loss: 3.049938917160034 | BCE Loss: 1.0334160327911377\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.10268497467041 | KNN Loss: 3.089242696762085 | BCE Loss: 1.0134421586990356\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.1158127784729 | KNN Loss: 3.070382833480835 | BCE Loss: 1.0454299449920654\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.083907127380371 | KNN Loss: 3.0846238136291504 | BCE Loss: 0.9992830753326416\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.148334503173828 | KNN Loss: 3.1219630241394043 | BCE Loss: 1.0263715982437134\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.063439846038818 | KNN Loss: 3.0746631622314453 | BCE Loss: 0.9887768626213074\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.0615949630737305 | KNN Loss: 3.071049690246582 | BCE Loss: 0.9905451536178589\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.093750476837158 | KNN Loss: 3.061267614364624 | BCE Loss: 1.0324827432632446\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.112128734588623 | KNN Loss: 3.074472665786743 | BCE Loss: 1.0376560688018799\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.129066467285156 | KNN Loss: 3.0911481380462646 | BCE Loss: 1.0379183292388916\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.088958740234375 | KNN Loss: 3.075634717941284 | BCE Loss: 1.0133237838745117\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.058150291442871 | KNN Loss: 3.03391170501709 | BCE Loss: 1.0242388248443604\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.060650825500488 | KNN Loss: 3.0507805347442627 | BCE Loss: 1.009870171546936\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.1100568771362305 | KNN Loss: 3.0808520317077637 | BCE Loss: 1.0292046070098877\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.077803611755371 | KNN Loss: 3.053318500518799 | BCE Loss: 1.0244852304458618\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.068120002746582 | KNN Loss: 3.0629756450653076 | BCE Loss: 1.0051443576812744\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.079322814941406 | KNN Loss: 3.05865216255188 | BCE Loss: 1.0206708908081055\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.0606536865234375 | KNN Loss: 3.0417211055755615 | BCE Loss: 1.0189323425292969\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.0950212478637695 | KNN Loss: 3.0705201625823975 | BCE Loss: 1.0245013236999512\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.113791465759277 | KNN Loss: 3.091801643371582 | BCE Loss: 1.0219898223876953\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.120821475982666 | KNN Loss: 3.1018218994140625 | BCE Loss: 1.018999457359314\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.11148738861084 | KNN Loss: 3.0891401767730713 | BCE Loss: 1.0223469734191895\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.092389106750488 | KNN Loss: 3.0637712478637695 | BCE Loss: 1.0286178588867188\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.137664794921875 | KNN Loss: 3.1025257110595703 | BCE Loss: 1.0351393222808838\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.0810546875 | KNN Loss: 3.0405616760253906 | BCE Loss: 1.0404930114746094\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.095878601074219 | KNN Loss: 3.076272487640381 | BCE Loss: 1.0196062326431274\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.090673446655273 | KNN Loss: 3.0680642127990723 | BCE Loss: 1.0226091146469116\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.048972129821777 | KNN Loss: 3.044408082962036 | BCE Loss: 1.0045642852783203\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.147245407104492 | KNN Loss: 3.1367011070251465 | BCE Loss: 1.0105440616607666\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.10548210144043 | KNN Loss: 3.0906779766082764 | BCE Loss: 1.0148041248321533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.07960844039917 | KNN Loss: 3.0653884410858154 | BCE Loss: 1.0142199993133545\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.104319095611572 | KNN Loss: 3.0629022121429443 | BCE Loss: 1.0414170026779175\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.076900482177734 | KNN Loss: 3.06660532951355 | BCE Loss: 1.0102953910827637\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.063222885131836 | KNN Loss: 3.050391435623169 | BCE Loss: 1.012831687927246\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.062729358673096 | KNN Loss: 3.0519890785217285 | BCE Loss: 1.0107402801513672\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.078193187713623 | KNN Loss: 3.041116952896118 | BCE Loss: 1.0370761156082153\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.132658004760742 | KNN Loss: 3.088465929031372 | BCE Loss: 1.044191837310791\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.114150047302246 | KNN Loss: 3.059462070465088 | BCE Loss: 1.0546879768371582\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.077480316162109 | KNN Loss: 3.070476770401001 | BCE Loss: 1.0070037841796875\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.091428756713867 | KNN Loss: 3.077577829360962 | BCE Loss: 1.0138509273529053\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.048694610595703 | KNN Loss: 3.036226272583008 | BCE Loss: 1.0124683380126953\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.065888404846191 | KNN Loss: 3.051262140274048 | BCE Loss: 1.0146265029907227\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.103644371032715 | KNN Loss: 3.0842607021331787 | BCE Loss: 1.0193837881088257\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.107924461364746 | KNN Loss: 3.0661814212799072 | BCE Loss: 1.041743278503418\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.0603132247924805 | KNN Loss: 3.0858657360076904 | BCE Loss: 0.9744472503662109\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.125030517578125 | KNN Loss: 3.110464572906494 | BCE Loss: 1.0145657062530518\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.08931827545166 | KNN Loss: 3.0609307289123535 | BCE Loss: 1.0283877849578857\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.092410087585449 | KNN Loss: 3.065774917602539 | BCE Loss: 1.026634931564331\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.137624740600586 | KNN Loss: 3.0780279636383057 | BCE Loss: 1.0595970153808594\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.122514724731445 | KNN Loss: 3.0669007301330566 | BCE Loss: 1.0556138753890991\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.063023567199707 | KNN Loss: 3.057279348373413 | BCE Loss: 1.0057439804077148\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.0385308265686035 | KNN Loss: 3.0562586784362793 | BCE Loss: 0.9822719693183899\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.081092834472656 | KNN Loss: 3.0537898540496826 | BCE Loss: 1.0273032188415527\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.1040802001953125 | KNN Loss: 3.0853161811828613 | BCE Loss: 1.018763780593872\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.112638473510742 | KNN Loss: 3.08085560798645 | BCE Loss: 1.031782865524292\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.101738929748535 | KNN Loss: 3.098790407180786 | BCE Loss: 1.0029487609863281\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.058862209320068 | KNN Loss: 3.0661520957946777 | BCE Loss: 0.9927099943161011\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.100888252258301 | KNN Loss: 3.0779647827148438 | BCE Loss: 1.022923469543457\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.0975823402404785 | KNN Loss: 3.097782611846924 | BCE Loss: 0.9997998476028442\n",
      "Epoch   176: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.093440055847168 | KNN Loss: 3.101583957672119 | BCE Loss: 0.9918562769889832\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.057413101196289 | KNN Loss: 3.0425662994384766 | BCE Loss: 1.0148468017578125\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.12210750579834 | KNN Loss: 3.063504934310913 | BCE Loss: 1.0586023330688477\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.092373847961426 | KNN Loss: 3.072787046432495 | BCE Loss: 1.0195868015289307\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.074044227600098 | KNN Loss: 3.077462673187256 | BCE Loss: 0.9965816736221313\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.130115509033203 | KNN Loss: 3.0840752124786377 | BCE Loss: 1.0460405349731445\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.153914451599121 | KNN Loss: 3.0988266468048096 | BCE Loss: 1.0550875663757324\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.129707336425781 | KNN Loss: 3.092709541320801 | BCE Loss: 1.0369977951049805\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.125176429748535 | KNN Loss: 3.060662269592285 | BCE Loss: 1.06451416015625\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.102851390838623 | KNN Loss: 3.0690507888793945 | BCE Loss: 1.033800482749939\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.08098030090332 | KNN Loss: 3.0831499099731445 | BCE Loss: 0.9978305101394653\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.125694751739502 | KNN Loss: 3.0849881172180176 | BCE Loss: 1.0407065153121948\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.160374641418457 | KNN Loss: 3.123469114303589 | BCE Loss: 1.036905288696289\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.037571907043457 | KNN Loss: 3.0442252159118652 | BCE Loss: 0.9933465719223022\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.1148552894592285 | KNN Loss: 3.0694222450256348 | BCE Loss: 1.0454330444335938\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.094801902770996 | KNN Loss: 3.051038980484009 | BCE Loss: 1.0437626838684082\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.06766414642334 | KNN Loss: 3.0658578872680664 | BCE Loss: 1.0018060207366943\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.102085113525391 | KNN Loss: 3.082473039627075 | BCE Loss: 1.0196120738983154\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.104488849639893 | KNN Loss: 3.073885679244995 | BCE Loss: 1.030603289604187\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.099026679992676 | KNN Loss: 3.059218168258667 | BCE Loss: 1.0398085117340088\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.0383381843566895 | KNN Loss: 3.0405516624450684 | BCE Loss: 0.9977863430976868\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.115067958831787 | KNN Loss: 3.1091134548187256 | BCE Loss: 1.0059545040130615\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.054563999176025 | KNN Loss: 3.059020519256592 | BCE Loss: 0.9955433011054993\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.045719146728516 | KNN Loss: 3.020721673965454 | BCE Loss: 1.024997353553772\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.117486953735352 | KNN Loss: 3.101553440093994 | BCE Loss: 1.0159332752227783\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.145788192749023 | KNN Loss: 3.076030731201172 | BCE Loss: 1.0697576999664307\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.058684349060059 | KNN Loss: 3.0712714195251465 | BCE Loss: 0.9874128103256226\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.112053871154785 | KNN Loss: 3.071211814880371 | BCE Loss: 1.040841817855835\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.101375579833984 | KNN Loss: 3.069929361343384 | BCE Loss: 1.0314462184906006\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.051050186157227 | KNN Loss: 3.059628963470459 | BCE Loss: 0.9914214611053467\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.069344520568848 | KNN Loss: 3.0284316539764404 | BCE Loss: 1.0409129858016968\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.087323188781738 | KNN Loss: 3.0571553707122803 | BCE Loss: 1.030167818069458\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.104249954223633 | KNN Loss: 3.081904411315918 | BCE Loss: 1.0223453044891357\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.109350204467773 | KNN Loss: 3.072732925415039 | BCE Loss: 1.0366170406341553\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.065796852111816 | KNN Loss: 3.0465176105499268 | BCE Loss: 1.0192790031433105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.110685348510742 | KNN Loss: 3.0619139671325684 | BCE Loss: 1.048771619796753\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.053529739379883 | KNN Loss: 3.069918632507324 | BCE Loss: 0.9836112260818481\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.08386754989624 | KNN Loss: 3.070089817047119 | BCE Loss: 1.013777732849121\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.078964710235596 | KNN Loss: 3.0665690898895264 | BCE Loss: 1.0123955011367798\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.111300468444824 | KNN Loss: 3.065309524536133 | BCE Loss: 1.045991063117981\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.038296699523926 | KNN Loss: 3.0567877292633057 | BCE Loss: 0.9815092086791992\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.077134609222412 | KNN Loss: 3.0388717651367188 | BCE Loss: 1.038262963294983\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.114812850952148 | KNN Loss: 3.1141862869262695 | BCE Loss: 1.0006263256072998\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.066288471221924 | KNN Loss: 3.0462777614593506 | BCE Loss: 1.0200107097625732\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.067174434661865 | KNN Loss: 3.0647976398468018 | BCE Loss: 1.002376914024353\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.116679668426514 | KNN Loss: 3.0780129432678223 | BCE Loss: 1.0386667251586914\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.084044933319092 | KNN Loss: 3.064647912979126 | BCE Loss: 1.0193970203399658\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.1062822341918945 | KNN Loss: 3.088062047958374 | BCE Loss: 1.0182201862335205\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.092041015625 | KNN Loss: 3.0823049545288086 | BCE Loss: 1.0097358226776123\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.114354133605957 | KNN Loss: 3.085258722305298 | BCE Loss: 1.0290954113006592\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.1031341552734375 | KNN Loss: 3.0709080696105957 | BCE Loss: 1.0322260856628418\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.058202743530273 | KNN Loss: 3.049393892288208 | BCE Loss: 1.0088090896606445\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.0975518226623535 | KNN Loss: 3.0776655673980713 | BCE Loss: 1.0198862552642822\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.079289436340332 | KNN Loss: 3.054610252380371 | BCE Loss: 1.024679183959961\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.0870537757873535 | KNN Loss: 3.057173490524292 | BCE Loss: 1.0298802852630615\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.12615966796875 | KNN Loss: 3.062351703643799 | BCE Loss: 1.063807725906372\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.1119232177734375 | KNN Loss: 3.0999538898468018 | BCE Loss: 1.0119695663452148\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.0880866050720215 | KNN Loss: 3.0493152141571045 | BCE Loss: 1.0387715101242065\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.099905967712402 | KNN Loss: 3.078721761703491 | BCE Loss: 1.0211842060089111\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.066011905670166 | KNN Loss: 3.0573835372924805 | BCE Loss: 1.008628487586975\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.127161026000977 | KNN Loss: 3.1019675731658936 | BCE Loss: 1.025193691253662\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.062936782836914 | KNN Loss: 3.067561388015747 | BCE Loss: 0.9953755736351013\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.112663269042969 | KNN Loss: 3.054516315460205 | BCE Loss: 1.0581471920013428\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.096940040588379 | KNN Loss: 3.0521633625030518 | BCE Loss: 1.0447766780853271\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.113067626953125 | KNN Loss: 3.0574703216552734 | BCE Loss: 1.055597186088562\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.084456920623779 | KNN Loss: 3.071256160736084 | BCE Loss: 1.0132007598876953\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.061028957366943 | KNN Loss: 3.0673091411590576 | BCE Loss: 0.9937196373939514\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.07091760635376 | KNN Loss: 3.059263229370117 | BCE Loss: 1.0116544961929321\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.110616683959961 | KNN Loss: 3.107971668243408 | BCE Loss: 1.0026447772979736\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.116900444030762 | KNN Loss: 3.105433464050293 | BCE Loss: 1.0114667415618896\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.127531051635742 | KNN Loss: 3.0954623222351074 | BCE Loss: 1.0320689678192139\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.142317771911621 | KNN Loss: 3.105299949645996 | BCE Loss: 1.0370179414749146\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.153251647949219 | KNN Loss: 3.115407705307007 | BCE Loss: 1.0378437042236328\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.038960933685303 | KNN Loss: 3.0453155040740967 | BCE Loss: 0.9936456084251404\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.106375694274902 | KNN Loss: 3.080815076828003 | BCE Loss: 1.0255606174468994\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.090151786804199 | KNN Loss: 3.0363142490386963 | BCE Loss: 1.053837776184082\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.077764987945557 | KNN Loss: 3.0564348697662354 | BCE Loss: 1.0213299989700317\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.115503311157227 | KNN Loss: 3.100975275039673 | BCE Loss: 1.0145280361175537\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.051356792449951 | KNN Loss: 3.051387310028076 | BCE Loss: 0.9999696016311646\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.048946857452393 | KNN Loss: 3.049508571624756 | BCE Loss: 0.9994383454322815\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.049209117889404 | KNN Loss: 3.0380330085754395 | BCE Loss: 1.0111762285232544\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.113103866577148 | KNN Loss: 3.072678804397583 | BCE Loss: 1.0404253005981445\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.056313514709473 | KNN Loss: 3.0731825828552246 | BCE Loss: 0.983130931854248\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.0754899978637695 | KNN Loss: 3.0487940311431885 | BCE Loss: 1.0266962051391602\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.098662376403809 | KNN Loss: 3.0939316749572754 | BCE Loss: 1.0047305822372437\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.083596229553223 | KNN Loss: 3.075796127319336 | BCE Loss: 1.0077998638153076\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.068288803100586 | KNN Loss: 3.0454978942871094 | BCE Loss: 1.0227911472320557\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.087038040161133 | KNN Loss: 3.0663583278656006 | BCE Loss: 1.0206797122955322\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.0882158279418945 | KNN Loss: 3.0621399879455566 | BCE Loss: 1.0260757207870483\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.116239547729492 | KNN Loss: 3.0612142086029053 | BCE Loss: 1.0550251007080078\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.036733150482178 | KNN Loss: 3.026306629180908 | BCE Loss: 1.010426640510559\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.108037948608398 | KNN Loss: 3.0582330226898193 | BCE Loss: 1.0498046875\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.138678073883057 | KNN Loss: 3.0857391357421875 | BCE Loss: 1.0529389381408691\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.104340076446533 | KNN Loss: 3.085371255874634 | BCE Loss: 1.0189688205718994\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.110050678253174 | KNN Loss: 3.076871156692505 | BCE Loss: 1.033179521560669\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.042690277099609 | KNN Loss: 3.044449806213379 | BCE Loss: 0.9982404708862305\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.131924629211426 | KNN Loss: 3.0949671268463135 | BCE Loss: 1.0369575023651123\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.056571960449219 | KNN Loss: 3.041288375854492 | BCE Loss: 1.0152833461761475\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.060636043548584 | KNN Loss: 3.0379490852355957 | BCE Loss: 1.0226870775222778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.1161088943481445 | KNN Loss: 3.069572687149048 | BCE Loss: 1.0465362071990967\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.110224723815918 | KNN Loss: 3.0623161792755127 | BCE Loss: 1.0479086637496948\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.050692081451416 | KNN Loss: 3.0625481605529785 | BCE Loss: 0.988143801689148\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.058401584625244 | KNN Loss: 3.047548294067383 | BCE Loss: 1.0108532905578613\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.12888240814209 | KNN Loss: 3.0950584411621094 | BCE Loss: 1.0338237285614014\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.0584235191345215 | KNN Loss: 3.0505118370056152 | BCE Loss: 1.0079118013381958\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.054477691650391 | KNN Loss: 3.0411696434020996 | BCE Loss: 1.0133081674575806\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.060898780822754 | KNN Loss: 3.057616949081421 | BCE Loss: 1.003281831741333\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.047393798828125 | KNN Loss: 3.0619800090789795 | BCE Loss: 0.9854140281677246\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.081814289093018 | KNN Loss: 3.063007116317749 | BCE Loss: 1.018807291984558\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.077909469604492 | KNN Loss: 3.0532095432281494 | BCE Loss: 1.0246999263763428\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.068630695343018 | KNN Loss: 3.070976734161377 | BCE Loss: 0.997654139995575\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.094191074371338 | KNN Loss: 3.0677642822265625 | BCE Loss: 1.0264267921447754\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.123426914215088 | KNN Loss: 3.1035735607147217 | BCE Loss: 1.0198533535003662\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.082189083099365 | KNN Loss: 3.066983938217163 | BCE Loss: 1.0152051448822021\n",
      "Epoch   195: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.0863752365112305 | KNN Loss: 3.0813097953796387 | BCE Loss: 1.005065679550171\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.084151268005371 | KNN Loss: 3.0368564128875732 | BCE Loss: 1.0472948551177979\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.074149131774902 | KNN Loss: 3.0546443462371826 | BCE Loss: 1.0195045471191406\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.089323997497559 | KNN Loss: 3.069427013397217 | BCE Loss: 1.019897222518921\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.1154890060424805 | KNN Loss: 3.08967924118042 | BCE Loss: 1.0258097648620605\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.108262538909912 | KNN Loss: 3.059920310974121 | BCE Loss: 1.048342227935791\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.079910755157471 | KNN Loss: 3.064610719680786 | BCE Loss: 1.015299916267395\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.1059088706970215 | KNN Loss: 3.080500364303589 | BCE Loss: 1.0254085063934326\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.115901947021484 | KNN Loss: 3.0663797855377197 | BCE Loss: 1.049522042274475\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.119438171386719 | KNN Loss: 3.0904488563537598 | BCE Loss: 1.0289890766143799\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.1377177238464355 | KNN Loss: 3.11147141456604 | BCE Loss: 1.026246190071106\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.070249557495117 | KNN Loss: 3.074921131134033 | BCE Loss: 0.9953283071517944\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.0622477531433105 | KNN Loss: 3.0365426540374756 | BCE Loss: 1.0257049798965454\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.066353797912598 | KNN Loss: 3.0574023723602295 | BCE Loss: 1.0089516639709473\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.112332820892334 | KNN Loss: 3.0715973377227783 | BCE Loss: 1.0407354831695557\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.075176239013672 | KNN Loss: 3.0814614295959473 | BCE Loss: 0.9937149882316589\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.072218894958496 | KNN Loss: 3.0641908645629883 | BCE Loss: 1.0080277919769287\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.058566093444824 | KNN Loss: 3.024358034133911 | BCE Loss: 1.0342082977294922\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.057106018066406 | KNN Loss: 3.0398921966552734 | BCE Loss: 1.017214059829712\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.070928573608398 | KNN Loss: 3.0579211711883545 | BCE Loss: 1.0130071640014648\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.081303596496582 | KNN Loss: 3.051734209060669 | BCE Loss: 1.0295696258544922\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.04707145690918 | KNN Loss: 3.0337846279144287 | BCE Loss: 1.013286828994751\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.093550205230713 | KNN Loss: 3.0810625553131104 | BCE Loss: 1.012487769126892\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.091585159301758 | KNN Loss: 3.0755038261413574 | BCE Loss: 1.01608145236969\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.089376449584961 | KNN Loss: 3.0628440380096436 | BCE Loss: 1.0265324115753174\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.133063316345215 | KNN Loss: 3.1148529052734375 | BCE Loss: 1.018210530281067\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.131347179412842 | KNN Loss: 3.0998587608337402 | BCE Loss: 1.0314884185791016\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.081315517425537 | KNN Loss: 3.081998348236084 | BCE Loss: 0.9993172883987427\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.022696495056152 | KNN Loss: 3.044626235961914 | BCE Loss: 0.9780701398849487\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.0687479972839355 | KNN Loss: 3.050170660018921 | BCE Loss: 1.0185774564743042\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.062765121459961 | KNN Loss: 3.053211212158203 | BCE Loss: 1.0095537900924683\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.107958793640137 | KNN Loss: 3.080347776412964 | BCE Loss: 1.0276107788085938\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.098023891448975 | KNN Loss: 3.0683364868164062 | BCE Loss: 1.0296874046325684\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.0874762535095215 | KNN Loss: 3.0733489990234375 | BCE Loss: 1.0141271352767944\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.116812705993652 | KNN Loss: 3.081787109375 | BCE Loss: 1.0350255966186523\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.084333896636963 | KNN Loss: 3.045599937438965 | BCE Loss: 1.038733959197998\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.085212707519531 | KNN Loss: 3.0538249015808105 | BCE Loss: 1.0313880443572998\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.09227991104126 | KNN Loss: 3.0657453536987305 | BCE Loss: 1.0265346765518188\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.054311752319336 | KNN Loss: 3.049226760864258 | BCE Loss: 1.0050849914550781\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.061529159545898 | KNN Loss: 3.0640199184417725 | BCE Loss: 0.9975094199180603\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.151725769042969 | KNN Loss: 3.1074016094207764 | BCE Loss: 1.0443240404129028\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.124170780181885 | KNN Loss: 3.1022565364837646 | BCE Loss: 1.0219142436981201\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.112270355224609 | KNN Loss: 3.0857388973236084 | BCE Loss: 1.026531457901001\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.079720497131348 | KNN Loss: 3.0795767307281494 | BCE Loss: 1.0001435279846191\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.104028224945068 | KNN Loss: 3.0686793327331543 | BCE Loss: 1.035348892211914\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.064788818359375 | KNN Loss: 3.076127290725708 | BCE Loss: 0.9886617064476013\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.094745635986328 | KNN Loss: 3.0742249488830566 | BCE Loss: 1.0205209255218506\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.092379570007324 | KNN Loss: 3.064206838607788 | BCE Loss: 1.028172492980957\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.0737833976745605 | KNN Loss: 3.061206102371216 | BCE Loss: 1.0125772953033447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.109368324279785 | KNN Loss: 3.098463296890259 | BCE Loss: 1.0109049081802368\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.132368087768555 | KNN Loss: 3.112765073776245 | BCE Loss: 1.01960289478302\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.115475654602051 | KNN Loss: 3.1049609184265137 | BCE Loss: 1.0105146169662476\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.061158180236816 | KNN Loss: 3.0620696544647217 | BCE Loss: 0.9990884065628052\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.075298309326172 | KNN Loss: 3.051128625869751 | BCE Loss: 1.0241698026657104\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.054834365844727 | KNN Loss: 3.032728672027588 | BCE Loss: 1.0221054553985596\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.0728936195373535 | KNN Loss: 3.0860190391540527 | BCE Loss: 0.9868745803833008\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.123208045959473 | KNN Loss: 3.1173596382141113 | BCE Loss: 1.0058484077453613\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.058172702789307 | KNN Loss: 3.043182611465454 | BCE Loss: 1.014989972114563\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.041648864746094 | KNN Loss: 3.020512104034424 | BCE Loss: 1.0211365222930908\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.089120388031006 | KNN Loss: 3.0521457195281982 | BCE Loss: 1.0369746685028076\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.083121299743652 | KNN Loss: 3.054893732070923 | BCE Loss: 1.0282278060913086\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.113953113555908 | KNN Loss: 3.0821926593780518 | BCE Loss: 1.0317604541778564\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.034332275390625 | KNN Loss: 3.0461997985839844 | BCE Loss: 0.9881325960159302\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.07729434967041 | KNN Loss: 3.065692901611328 | BCE Loss: 1.011601209640503\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.115668773651123 | KNN Loss: 3.0805633068084717 | BCE Loss: 1.0351054668426514\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.098312854766846 | KNN Loss: 3.0844898223876953 | BCE Loss: 1.0138229131698608\n",
      "Epoch   206: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.128806114196777 | KNN Loss: 3.0844969749450684 | BCE Loss: 1.0443089008331299\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.076389312744141 | KNN Loss: 3.067572832107544 | BCE Loss: 1.0088164806365967\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.071922302246094 | KNN Loss: 3.065556764602661 | BCE Loss: 1.0063652992248535\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.089085102081299 | KNN Loss: 3.047682046890259 | BCE Loss: 1.04140305519104\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.085954189300537 | KNN Loss: 3.0688207149505615 | BCE Loss: 1.0171334743499756\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.098723888397217 | KNN Loss: 3.065247058868408 | BCE Loss: 1.0334768295288086\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.108418941497803 | KNN Loss: 3.075333833694458 | BCE Loss: 1.0330852270126343\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.125887393951416 | KNN Loss: 3.066572427749634 | BCE Loss: 1.0593148469924927\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.059156894683838 | KNN Loss: 3.0637869834899902 | BCE Loss: 0.995370090007782\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.116122722625732 | KNN Loss: 3.068533182144165 | BCE Loss: 1.0475895404815674\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.041971206665039 | KNN Loss: 3.0492987632751465 | BCE Loss: 0.9926724433898926\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.157729625701904 | KNN Loss: 3.0874457359313965 | BCE Loss: 1.0702837705612183\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.058920860290527 | KNN Loss: 3.057164430618286 | BCE Loss: 1.0017565488815308\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.036553382873535 | KNN Loss: 3.0491156578063965 | BCE Loss: 0.9874376058578491\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.059811592102051 | KNN Loss: 3.05830717086792 | BCE Loss: 1.0015041828155518\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.062560081481934 | KNN Loss: 3.0652434825897217 | BCE Loss: 0.9973166584968567\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.071042060852051 | KNN Loss: 3.0487091541290283 | BCE Loss: 1.0223329067230225\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.093372821807861 | KNN Loss: 3.0714404582977295 | BCE Loss: 1.0219322443008423\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.111496925354004 | KNN Loss: 3.064171075820923 | BCE Loss: 1.0473259687423706\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.090230464935303 | KNN Loss: 3.0761373043060303 | BCE Loss: 1.014093041419983\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.062081813812256 | KNN Loss: 3.0578184127807617 | BCE Loss: 1.0042632818222046\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.0645904541015625 | KNN Loss: 3.0682754516601562 | BCE Loss: 0.9963147640228271\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.062560081481934 | KNN Loss: 3.0492289066314697 | BCE Loss: 1.0133311748504639\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.062799453735352 | KNN Loss: 3.056196928024292 | BCE Loss: 1.0066025257110596\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.071374416351318 | KNN Loss: 3.058229684829712 | BCE Loss: 1.013144612312317\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.064580917358398 | KNN Loss: 3.058182954788208 | BCE Loss: 1.0063977241516113\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.08111572265625 | KNN Loss: 3.073169231414795 | BCE Loss: 1.007946252822876\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.072691917419434 | KNN Loss: 3.06567120552063 | BCE Loss: 1.0070208311080933\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.063534259796143 | KNN Loss: 3.052091598510742 | BCE Loss: 1.0114425420761108\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.089600563049316 | KNN Loss: 3.0547046661376953 | BCE Loss: 1.034895896911621\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.06094217300415 | KNN Loss: 3.038416862487793 | BCE Loss: 1.0225251913070679\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.103324890136719 | KNN Loss: 3.0557312965393066 | BCE Loss: 1.0475934743881226\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.063719749450684 | KNN Loss: 3.033169984817505 | BCE Loss: 1.0305497646331787\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.064009666442871 | KNN Loss: 3.048611640930176 | BCE Loss: 1.0153982639312744\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.117458820343018 | KNN Loss: 3.0780296325683594 | BCE Loss: 1.0394290685653687\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.027130126953125 | KNN Loss: 3.0467660427093506 | BCE Loss: 0.9803638458251953\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.077557563781738 | KNN Loss: 3.0824122428894043 | BCE Loss: 0.9951450824737549\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.100908279418945 | KNN Loss: 3.080712080001831 | BCE Loss: 1.0201960802078247\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.112728118896484 | KNN Loss: 3.079991340637207 | BCE Loss: 1.0327365398406982\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.050236701965332 | KNN Loss: 3.044433355331421 | BCE Loss: 1.0058032274246216\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.094675540924072 | KNN Loss: 3.06535267829895 | BCE Loss: 1.029322862625122\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.110961437225342 | KNN Loss: 3.093048095703125 | BCE Loss: 1.0179134607315063\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.073988914489746 | KNN Loss: 3.074924945831299 | BCE Loss: 0.9990639686584473\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.109004974365234 | KNN Loss: 3.0706288814544678 | BCE Loss: 1.0383763313293457\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.051185607910156 | KNN Loss: 3.042795181274414 | BCE Loss: 1.008390188217163\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.066291332244873 | KNN Loss: 3.0429317951202393 | BCE Loss: 1.0233594179153442\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.103458881378174 | KNN Loss: 3.054915189743042 | BCE Loss: 1.0485436916351318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.100217819213867 | KNN Loss: 3.093705892562866 | BCE Loss: 1.00651216506958\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.1015825271606445 | KNN Loss: 3.0775129795074463 | BCE Loss: 1.0240695476531982\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.0597686767578125 | KNN Loss: 3.0618488788604736 | BCE Loss: 0.9979199767112732\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.049434185028076 | KNN Loss: 3.071361780166626 | BCE Loss: 0.9780725240707397\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.069096088409424 | KNN Loss: 3.0365078449249268 | BCE Loss: 1.0325881242752075\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.067480564117432 | KNN Loss: 3.0328094959259033 | BCE Loss: 1.0346709489822388\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.077341079711914 | KNN Loss: 3.036071300506592 | BCE Loss: 1.0412697792053223\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.149571895599365 | KNN Loss: 3.0875260829925537 | BCE Loss: 1.062045931816101\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.158098220825195 | KNN Loss: 3.1161863803863525 | BCE Loss: 1.0419118404388428\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.064859390258789 | KNN Loss: 3.0504753589630127 | BCE Loss: 1.014384150505066\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.093780994415283 | KNN Loss: 3.061016798019409 | BCE Loss: 1.0327640771865845\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.053943157196045 | KNN Loss: 3.0506410598754883 | BCE Loss: 1.0033020973205566\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.042707443237305 | KNN Loss: 3.0426759719848633 | BCE Loss: 1.0000313520431519\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.117230415344238 | KNN Loss: 3.1033945083618164 | BCE Loss: 1.0138359069824219\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.069719314575195 | KNN Loss: 3.0564916133880615 | BCE Loss: 1.0132277011871338\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.037710189819336 | KNN Loss: 3.0490219593048096 | BCE Loss: 0.9886883497238159\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.109294414520264 | KNN Loss: 3.0599443912506104 | BCE Loss: 1.0493499040603638\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.087795257568359 | KNN Loss: 3.050816535949707 | BCE Loss: 1.036978840827942\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.058061599731445 | KNN Loss: 3.0705196857452393 | BCE Loss: 0.9875420928001404\n",
      "Epoch   217: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.063973903656006 | KNN Loss: 3.0718226432800293 | BCE Loss: 0.992151141166687\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.086115837097168 | KNN Loss: 3.078113079071045 | BCE Loss: 1.0080029964447021\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.120255470275879 | KNN Loss: 3.1049914360046387 | BCE Loss: 1.0152640342712402\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.057104587554932 | KNN Loss: 3.051342248916626 | BCE Loss: 1.0057623386383057\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.077884197235107 | KNN Loss: 3.0552496910095215 | BCE Loss: 1.0226346254348755\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.020512580871582 | KNN Loss: 3.018376111984253 | BCE Loss: 1.002136468887329\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.0804548263549805 | KNN Loss: 3.0756232738494873 | BCE Loss: 1.0048317909240723\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.0777788162231445 | KNN Loss: 3.0787434577941895 | BCE Loss: 0.9990352392196655\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.046561241149902 | KNN Loss: 3.0344696044921875 | BCE Loss: 1.012091875076294\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.096001148223877 | KNN Loss: 3.0572075843811035 | BCE Loss: 1.038793683052063\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.109604358673096 | KNN Loss: 3.079824447631836 | BCE Loss: 1.0297799110412598\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.078434944152832 | KNN Loss: 3.0714080333709717 | BCE Loss: 1.0070269107818604\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.083493232727051 | KNN Loss: 3.051731824874878 | BCE Loss: 1.0317612886428833\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.105688571929932 | KNN Loss: 3.0907106399536133 | BCE Loss: 1.0149779319763184\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.08748722076416 | KNN Loss: 3.056709051132202 | BCE Loss: 1.0307782888412476\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.0858330726623535 | KNN Loss: 3.0867044925689697 | BCE Loss: 0.9991284608840942\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.012013912200928 | KNN Loss: 3.026078462600708 | BCE Loss: 0.9859355092048645\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.060592174530029 | KNN Loss: 3.048147678375244 | BCE Loss: 1.0124446153640747\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.076182842254639 | KNN Loss: 3.0461223125457764 | BCE Loss: 1.0300605297088623\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.068274974822998 | KNN Loss: 3.061847448348999 | BCE Loss: 1.0064276456832886\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.096421241760254 | KNN Loss: 3.070643424987793 | BCE Loss: 1.02577805519104\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.10359001159668 | KNN Loss: 3.067476511001587 | BCE Loss: 1.0361132621765137\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.061463356018066 | KNN Loss: 3.0712034702301025 | BCE Loss: 0.9902600646018982\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.108983039855957 | KNN Loss: 3.0835609436035156 | BCE Loss: 1.025422215461731\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.048399448394775 | KNN Loss: 3.0355918407440186 | BCE Loss: 1.0128076076507568\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.125496864318848 | KNN Loss: 3.07255220413208 | BCE Loss: 1.0529448986053467\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.073241710662842 | KNN Loss: 3.0534884929656982 | BCE Loss: 1.0197532176971436\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.115070343017578 | KNN Loss: 3.0889029502868652 | BCE Loss: 1.026167392730713\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.074934005737305 | KNN Loss: 3.0623116493225098 | BCE Loss: 1.0126221179962158\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.110588073730469 | KNN Loss: 3.088301420211792 | BCE Loss: 1.0222867727279663\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.098644733428955 | KNN Loss: 3.070662260055542 | BCE Loss: 1.0279823541641235\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.081725120544434 | KNN Loss: 3.070030450820923 | BCE Loss: 1.0116949081420898\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.110569000244141 | KNN Loss: 3.08616304397583 | BCE Loss: 1.0244061946868896\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.085099697113037 | KNN Loss: 3.0693678855895996 | BCE Loss: 1.0157318115234375\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.069607734680176 | KNN Loss: 3.0456759929656982 | BCE Loss: 1.0239317417144775\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.075165271759033 | KNN Loss: 3.0462541580200195 | BCE Loss: 1.0289109945297241\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.120984077453613 | KNN Loss: 3.0639238357543945 | BCE Loss: 1.0570603609085083\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.0902419090271 | KNN Loss: 3.062544345855713 | BCE Loss: 1.0276975631713867\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.136026859283447 | KNN Loss: 3.0871005058288574 | BCE Loss: 1.0489262342453003\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.058689117431641 | KNN Loss: 3.060913324356079 | BCE Loss: 0.9977759122848511\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.042527198791504 | KNN Loss: 3.034036159515381 | BCE Loss: 1.0084909200668335\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.120363235473633 | KNN Loss: 3.082091808319092 | BCE Loss: 1.0382716655731201\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.069817066192627 | KNN Loss: 3.049968957901001 | BCE Loss: 1.0198482275009155\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.089390754699707 | KNN Loss: 3.0393853187561035 | BCE Loss: 1.0500056743621826\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.050936222076416 | KNN Loss: 3.051999568939209 | BCE Loss: 0.9989367723464966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.1001081466674805 | KNN Loss: 3.0563971996307373 | BCE Loss: 1.0437109470367432\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.0709452629089355 | KNN Loss: 3.0471765995025635 | BCE Loss: 1.023768663406372\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.061312198638916 | KNN Loss: 3.0470733642578125 | BCE Loss: 1.014238715171814\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.086259841918945 | KNN Loss: 3.069460391998291 | BCE Loss: 1.0167996883392334\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.090339183807373 | KNN Loss: 3.0428049564361572 | BCE Loss: 1.0475343465805054\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.119556427001953 | KNN Loss: 3.061906337738037 | BCE Loss: 1.057649850845337\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.0661797523498535 | KNN Loss: 3.0710203647613525 | BCE Loss: 0.9951594471931458\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.091028213500977 | KNN Loss: 3.083507537841797 | BCE Loss: 1.0075204372406006\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.0472869873046875 | KNN Loss: 3.040768623352051 | BCE Loss: 1.0065181255340576\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.056636810302734 | KNN Loss: 3.0219287872314453 | BCE Loss: 1.034708023071289\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.103111267089844 | KNN Loss: 3.0994815826416016 | BCE Loss: 1.0036299228668213\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.13952112197876 | KNN Loss: 3.0903255939483643 | BCE Loss: 1.0491955280303955\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.07881498336792 | KNN Loss: 3.040182590484619 | BCE Loss: 1.0386323928833008\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.134998321533203 | KNN Loss: 3.102250099182129 | BCE Loss: 1.0327482223510742\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.063677787780762 | KNN Loss: 3.0702691078186035 | BCE Loss: 0.9934088587760925\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.119527339935303 | KNN Loss: 3.076781749725342 | BCE Loss: 1.042745590209961\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.091175556182861 | KNN Loss: 3.0631461143493652 | BCE Loss: 1.0280295610427856\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.091248512268066 | KNN Loss: 3.047224760055542 | BCE Loss: 1.0440239906311035\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.060884475708008 | KNN Loss: 3.0576529502868652 | BCE Loss: 1.0032315254211426\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.041455268859863 | KNN Loss: 3.0393269062042236 | BCE Loss: 1.0021284818649292\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.132632255554199 | KNN Loss: 3.091820240020752 | BCE Loss: 1.0408122539520264\n",
      "Epoch   228: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.031838417053223 | KNN Loss: 3.038713216781616 | BCE Loss: 0.9931250810623169\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.061405658721924 | KNN Loss: 3.0486459732055664 | BCE Loss: 1.0127596855163574\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.092212677001953 | KNN Loss: 3.0900967121124268 | BCE Loss: 1.0021157264709473\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.115688800811768 | KNN Loss: 3.070876121520996 | BCE Loss: 1.0448126792907715\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.128359794616699 | KNN Loss: 3.093137264251709 | BCE Loss: 1.0352227687835693\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.076799392700195 | KNN Loss: 3.062819004058838 | BCE Loss: 1.0139802694320679\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.074058532714844 | KNN Loss: 3.077505350112915 | BCE Loss: 0.9965531229972839\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.092263698577881 | KNN Loss: 3.049090623855591 | BCE Loss: 1.0431729555130005\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.052893161773682 | KNN Loss: 3.0484883785247803 | BCE Loss: 1.0044047832489014\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.09731388092041 | KNN Loss: 3.073376178741455 | BCE Loss: 1.023937463760376\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.051994323730469 | KNN Loss: 3.0572359561920166 | BCE Loss: 0.994758129119873\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.091649055480957 | KNN Loss: 3.083019971847534 | BCE Loss: 1.0086289644241333\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.0863165855407715 | KNN Loss: 3.069124221801758 | BCE Loss: 1.0171923637390137\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.083337306976318 | KNN Loss: 3.060471296310425 | BCE Loss: 1.022866129875183\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.1080522537231445 | KNN Loss: 3.091866970062256 | BCE Loss: 1.0161855220794678\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.092060565948486 | KNN Loss: 3.0491883754730225 | BCE Loss: 1.0428723096847534\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.064386367797852 | KNN Loss: 3.0501868724823 | BCE Loss: 1.0141996145248413\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.114808082580566 | KNN Loss: 3.076665163040161 | BCE Loss: 1.0381426811218262\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.065946102142334 | KNN Loss: 3.0475332736968994 | BCE Loss: 1.0184128284454346\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.073005199432373 | KNN Loss: 3.0436339378356934 | BCE Loss: 1.0293712615966797\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.099313735961914 | KNN Loss: 3.0678231716156006 | BCE Loss: 1.0314908027648926\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.092782974243164 | KNN Loss: 3.0730702877044678 | BCE Loss: 1.0197124481201172\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.098738670349121 | KNN Loss: 3.041642904281616 | BCE Loss: 1.057096004486084\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.085687637329102 | KNN Loss: 3.076735258102417 | BCE Loss: 1.0089523792266846\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.107278823852539 | KNN Loss: 3.066040277481079 | BCE Loss: 1.0412386655807495\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.0672783851623535 | KNN Loss: 3.04900860786438 | BCE Loss: 1.0182698965072632\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.068397521972656 | KNN Loss: 3.067276954650879 | BCE Loss: 1.0011208057403564\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.175784111022949 | KNN Loss: 3.1214632987976074 | BCE Loss: 1.0543205738067627\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.0771026611328125 | KNN Loss: 3.044847249984741 | BCE Loss: 1.0322551727294922\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.053086280822754 | KNN Loss: 3.038203001022339 | BCE Loss: 1.0148835182189941\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.096255779266357 | KNN Loss: 3.069045066833496 | BCE Loss: 1.0272108316421509\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.067047119140625 | KNN Loss: 3.0454037189483643 | BCE Loss: 1.0216436386108398\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.075246810913086 | KNN Loss: 3.0463123321533203 | BCE Loss: 1.0289345979690552\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.078967571258545 | KNN Loss: 3.0654590129852295 | BCE Loss: 1.0135085582733154\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.108729839324951 | KNN Loss: 3.0807085037231445 | BCE Loss: 1.028021216392517\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.037135124206543 | KNN Loss: 3.0326006412506104 | BCE Loss: 1.0045344829559326\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.065035820007324 | KNN Loss: 3.04787540435791 | BCE Loss: 1.017160177230835\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.082772254943848 | KNN Loss: 3.0581815242767334 | BCE Loss: 1.0245907306671143\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.128762245178223 | KNN Loss: 3.094512462615967 | BCE Loss: 1.0342496633529663\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.048065185546875 | KNN Loss: 3.0378525257110596 | BCE Loss: 1.0102126598358154\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.093646049499512 | KNN Loss: 3.061760425567627 | BCE Loss: 1.0318853855133057\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.103626728057861 | KNN Loss: 3.0757362842559814 | BCE Loss: 1.0278903245925903\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.100008964538574 | KNN Loss: 3.0765504837036133 | BCE Loss: 1.0234583616256714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.087567329406738 | KNN Loss: 3.057976007461548 | BCE Loss: 1.0295915603637695\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.069497108459473 | KNN Loss: 3.0662550926208496 | BCE Loss: 1.0032422542572021\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.083083152770996 | KNN Loss: 3.0574429035186768 | BCE Loss: 1.0256404876708984\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.084417343139648 | KNN Loss: 3.0467917919158936 | BCE Loss: 1.037625789642334\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.092174053192139 | KNN Loss: 3.0481700897216797 | BCE Loss: 1.044003963470459\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.079317092895508 | KNN Loss: 3.064122438430786 | BCE Loss: 1.0151946544647217\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.039323806762695 | KNN Loss: 3.0614986419677734 | BCE Loss: 0.9778250455856323\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.103768348693848 | KNN Loss: 3.081108570098877 | BCE Loss: 1.0226595401763916\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.11639404296875 | KNN Loss: 3.0607070922851562 | BCE Loss: 1.0556869506835938\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.0875043869018555 | KNN Loss: 3.051872730255127 | BCE Loss: 1.0356316566467285\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.0996246337890625 | KNN Loss: 3.0511972904205322 | BCE Loss: 1.0484271049499512\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.072449684143066 | KNN Loss: 3.052250623703003 | BCE Loss: 1.020199179649353\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.101409912109375 | KNN Loss: 3.080674409866333 | BCE Loss: 1.020735740661621\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.050924777984619 | KNN Loss: 3.0600597858428955 | BCE Loss: 0.9908649921417236\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.097784042358398 | KNN Loss: 3.0614969730377197 | BCE Loss: 1.0362873077392578\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.060628890991211 | KNN Loss: 3.0666911602020264 | BCE Loss: 0.9939374923706055\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.079931259155273 | KNN Loss: 3.087721109390259 | BCE Loss: 0.9922102093696594\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.095132827758789 | KNN Loss: 3.078446626663208 | BCE Loss: 1.016685962677002\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.100369453430176 | KNN Loss: 3.0705642700195312 | BCE Loss: 1.0298054218292236\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.125125885009766 | KNN Loss: 3.07589054107666 | BCE Loss: 1.0492353439331055\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.0599164962768555 | KNN Loss: 3.0500309467315674 | BCE Loss: 1.0098857879638672\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.091668128967285 | KNN Loss: 3.06257700920105 | BCE Loss: 1.0290908813476562\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.064870834350586 | KNN Loss: 3.0587005615234375 | BCE Loss: 1.0061701536178589\n",
      "Epoch   239: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.070339202880859 | KNN Loss: 3.0605530738830566 | BCE Loss: 1.0097858905792236\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.061281204223633 | KNN Loss: 3.0439605712890625 | BCE Loss: 1.0173203945159912\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.07857608795166 | KNN Loss: 3.054325819015503 | BCE Loss: 1.0242500305175781\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.095500946044922 | KNN Loss: 3.060044527053833 | BCE Loss: 1.035456657409668\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.034877300262451 | KNN Loss: 3.0529026985168457 | BCE Loss: 0.981974720954895\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.137588024139404 | KNN Loss: 3.101080894470215 | BCE Loss: 1.0365071296691895\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.108163356781006 | KNN Loss: 3.0601911544799805 | BCE Loss: 1.0479722023010254\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.165131568908691 | KNN Loss: 3.136186122894287 | BCE Loss: 1.0289454460144043\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.073577880859375 | KNN Loss: 3.075148105621338 | BCE Loss: 0.9984297156333923\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.127786636352539 | KNN Loss: 3.105076789855957 | BCE Loss: 1.022709608078003\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.12899112701416 | KNN Loss: 3.088757038116455 | BCE Loss: 1.0402339696884155\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.110655307769775 | KNN Loss: 3.08837890625 | BCE Loss: 1.022276520729065\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.092804908752441 | KNN Loss: 3.056558609008789 | BCE Loss: 1.0362461805343628\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.04073429107666 | KNN Loss: 3.0302982330322266 | BCE Loss: 1.0104362964630127\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.101458549499512 | KNN Loss: 3.0661563873291016 | BCE Loss: 1.0353024005889893\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.093737602233887 | KNN Loss: 3.071207284927368 | BCE Loss: 1.022530198097229\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.085334777832031 | KNN Loss: 3.0694966316223145 | BCE Loss: 1.015838384628296\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.0832109451293945 | KNN Loss: 3.0631625652313232 | BCE Loss: 1.0200483798980713\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.08439826965332 | KNN Loss: 3.0629212856292725 | BCE Loss: 1.021477222442627\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.0948357582092285 | KNN Loss: 3.0770630836486816 | BCE Loss: 1.0177726745605469\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.067431449890137 | KNN Loss: 3.062326431274414 | BCE Loss: 1.0051052570343018\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.065443515777588 | KNN Loss: 3.0487711429595947 | BCE Loss: 1.0166723728179932\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.072004318237305 | KNN Loss: 3.047429323196411 | BCE Loss: 1.0245747566223145\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.081338882446289 | KNN Loss: 3.066257953643799 | BCE Loss: 1.0150809288024902\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.115213871002197 | KNN Loss: 3.0586700439453125 | BCE Loss: 1.0565438270568848\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.049113750457764 | KNN Loss: 3.0553622245788574 | BCE Loss: 0.9937514066696167\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.011857986450195 | KNN Loss: 3.0075623989105225 | BCE Loss: 1.0042955875396729\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.070740699768066 | KNN Loss: 3.0616631507873535 | BCE Loss: 1.0090774297714233\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.136188507080078 | KNN Loss: 3.1089580059051514 | BCE Loss: 1.0272307395935059\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.1045331954956055 | KNN Loss: 3.062541961669922 | BCE Loss: 1.0419914722442627\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.086446762084961 | KNN Loss: 3.0692784786224365 | BCE Loss: 1.0171680450439453\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.097512245178223 | KNN Loss: 3.07572603225708 | BCE Loss: 1.0217859745025635\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.043217658996582 | KNN Loss: 3.0254287719726562 | BCE Loss: 1.0177888870239258\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.091553688049316 | KNN Loss: 3.051809310913086 | BCE Loss: 1.03974449634552\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.038261413574219 | KNN Loss: 3.0554215908050537 | BCE Loss: 0.982839822769165\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.089901447296143 | KNN Loss: 3.0532820224761963 | BCE Loss: 1.0366195440292358\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.007302284240723 | KNN Loss: 3.0086567401885986 | BCE Loss: 0.9986456632614136\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.096800327301025 | KNN Loss: 3.073155403137207 | BCE Loss: 1.023645043373108\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.101604461669922 | KNN Loss: 3.0424811840057373 | BCE Loss: 1.0591235160827637\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.091110706329346 | KNN Loss: 3.0593860149383545 | BCE Loss: 1.0317246913909912\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.118173599243164 | KNN Loss: 3.094440221786499 | BCE Loss: 1.023733139038086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.031405448913574 | KNN Loss: 3.0304250717163086 | BCE Loss: 1.0009801387786865\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.058538436889648 | KNN Loss: 3.051798105239868 | BCE Loss: 1.0067400932312012\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.111529350280762 | KNN Loss: 3.0832135677337646 | BCE Loss: 1.028315544128418\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.083789825439453 | KNN Loss: 3.065896511077881 | BCE Loss: 1.0178935527801514\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.097379684448242 | KNN Loss: 3.071638584136963 | BCE Loss: 1.0257412195205688\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.059595584869385 | KNN Loss: 3.0547678470611572 | BCE Loss: 1.004827618598938\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.080341339111328 | KNN Loss: 3.059281826019287 | BCE Loss: 1.021059513092041\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.022783279418945 | KNN Loss: 3.017277479171753 | BCE Loss: 1.0055060386657715\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.103301048278809 | KNN Loss: 3.082036256790161 | BCE Loss: 1.0212645530700684\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.079288482666016 | KNN Loss: 3.0735924243927 | BCE Loss: 1.005696177482605\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.0794854164123535 | KNN Loss: 3.051494598388672 | BCE Loss: 1.0279909372329712\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.092778205871582 | KNN Loss: 3.072718620300293 | BCE Loss: 1.020059585571289\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.061330318450928 | KNN Loss: 3.0651895999908447 | BCE Loss: 0.9961408376693726\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.0908098220825195 | KNN Loss: 3.060716152191162 | BCE Loss: 1.030093789100647\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.092248916625977 | KNN Loss: 3.0447332859039307 | BCE Loss: 1.047515869140625\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.120694637298584 | KNN Loss: 3.1001179218292236 | BCE Loss: 1.02057683467865\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.1082000732421875 | KNN Loss: 3.080785036087036 | BCE Loss: 1.0274147987365723\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.090599060058594 | KNN Loss: 3.0716488361358643 | BCE Loss: 1.0189504623413086\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.11502742767334 | KNN Loss: 3.094503402709961 | BCE Loss: 1.0205237865447998\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.054265975952148 | KNN Loss: 3.0525918006896973 | BCE Loss: 1.0016744136810303\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.04709529876709 | KNN Loss: 3.0419228076934814 | BCE Loss: 1.0051722526550293\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.077742099761963 | KNN Loss: 3.028271436691284 | BCE Loss: 1.0494705438613892\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.08025598526001 | KNN Loss: 3.053027629852295 | BCE Loss: 1.0272283554077148\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.093211650848389 | KNN Loss: 3.0292370319366455 | BCE Loss: 1.0639744997024536\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.072613716125488 | KNN Loss: 3.049532413482666 | BCE Loss: 1.0230815410614014\n",
      "Epoch   250: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.061966896057129 | KNN Loss: 3.032585859298706 | BCE Loss: 1.0293810367584229\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.068062782287598 | KNN Loss: 3.0432677268981934 | BCE Loss: 1.0247949361801147\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.071481704711914 | KNN Loss: 3.045808792114258 | BCE Loss: 1.0256731510162354\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.035048484802246 | KNN Loss: 3.020195960998535 | BCE Loss: 1.0148522853851318\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.041533470153809 | KNN Loss: 3.0353946685791016 | BCE Loss: 1.0061386823654175\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.118779182434082 | KNN Loss: 3.0844385623931885 | BCE Loss: 1.0343406200408936\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.1032257080078125 | KNN Loss: 3.066375494003296 | BCE Loss: 1.0368502140045166\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.051586151123047 | KNN Loss: 3.042457103729248 | BCE Loss: 1.0091291666030884\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.069934844970703 | KNN Loss: 3.076160192489624 | BCE Loss: 0.9937746524810791\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.057164192199707 | KNN Loss: 3.057278871536255 | BCE Loss: 0.9998855590820312\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.054228782653809 | KNN Loss: 3.0149428844451904 | BCE Loss: 1.0392861366271973\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.0764079093933105 | KNN Loss: 3.0348660945892334 | BCE Loss: 1.0415418148040771\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.133790969848633 | KNN Loss: 3.107858896255493 | BCE Loss: 1.0259318351745605\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.10435152053833 | KNN Loss: 3.0692803859710693 | BCE Loss: 1.0350711345672607\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.112255573272705 | KNN Loss: 3.0668835639953613 | BCE Loss: 1.0453720092773438\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.080049991607666 | KNN Loss: 3.076371669769287 | BCE Loss: 1.003678321838379\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.081772804260254 | KNN Loss: 3.0796914100646973 | BCE Loss: 1.0020811557769775\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.089907646179199 | KNN Loss: 3.070391893386841 | BCE Loss: 1.0195155143737793\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.061403751373291 | KNN Loss: 3.031787633895874 | BCE Loss: 1.0296162366867065\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.048774242401123 | KNN Loss: 3.054741621017456 | BCE Loss: 0.9940325021743774\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.060985565185547 | KNN Loss: 3.050797939300537 | BCE Loss: 1.0101875066757202\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.122944355010986 | KNN Loss: 3.067470073699951 | BCE Loss: 1.0554742813110352\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.084018230438232 | KNN Loss: 3.0805797576904297 | BCE Loss: 1.0034383535385132\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.088186264038086 | KNN Loss: 3.082139253616333 | BCE Loss: 1.0060471296310425\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.114933967590332 | KNN Loss: 3.0645124912261963 | BCE Loss: 1.0504212379455566\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.118039131164551 | KNN Loss: 3.1003310680389404 | BCE Loss: 1.0177078247070312\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.0973310470581055 | KNN Loss: 3.0834078788757324 | BCE Loss: 1.0139230489730835\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.119892120361328 | KNN Loss: 3.0818612575531006 | BCE Loss: 1.0380311012268066\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.11567497253418 | KNN Loss: 3.090602397918701 | BCE Loss: 1.0250723361968994\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.070034980773926 | KNN Loss: 3.0443031787872314 | BCE Loss: 1.0257315635681152\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.097252368927002 | KNN Loss: 3.079582929611206 | BCE Loss: 1.017669439315796\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.063910961151123 | KNN Loss: 3.056910514831543 | BCE Loss: 1.00700044631958\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.1213698387146 | KNN Loss: 3.1018059253692627 | BCE Loss: 1.0195640325546265\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.0954790115356445 | KNN Loss: 3.074740171432495 | BCE Loss: 1.0207386016845703\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.118682384490967 | KNN Loss: 3.106635570526123 | BCE Loss: 1.0120466947555542\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.100994110107422 | KNN Loss: 3.079679012298584 | BCE Loss: 1.0213148593902588\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.1043853759765625 | KNN Loss: 3.055267095565796 | BCE Loss: 1.0491183996200562\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.0762763023376465 | KNN Loss: 3.087944746017456 | BCE Loss: 0.9883314371109009\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.1148505210876465 | KNN Loss: 3.049405813217163 | BCE Loss: 1.0654447078704834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.071317195892334 | KNN Loss: 3.065279960632324 | BCE Loss: 1.0060372352600098\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.046391487121582 | KNN Loss: 3.0492308139801025 | BCE Loss: 0.9971605539321899\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.084527969360352 | KNN Loss: 3.0566728115081787 | BCE Loss: 1.0278551578521729\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.104671001434326 | KNN Loss: 3.075524091720581 | BCE Loss: 1.0291470289230347\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.0445451736450195 | KNN Loss: 3.046260118484497 | BCE Loss: 0.9982848167419434\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.069267272949219 | KNN Loss: 3.048719882965088 | BCE Loss: 1.02054762840271\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.111875534057617 | KNN Loss: 3.0794360637664795 | BCE Loss: 1.0324397087097168\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.084146499633789 | KNN Loss: 3.0587103366851807 | BCE Loss: 1.0254359245300293\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.059422969818115 | KNN Loss: 3.0572099685668945 | BCE Loss: 1.0022128820419312\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.098018646240234 | KNN Loss: 3.081766128540039 | BCE Loss: 1.0162527561187744\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.084998607635498 | KNN Loss: 3.071794271469116 | BCE Loss: 1.0132043361663818\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.145576000213623 | KNN Loss: 3.086082935333252 | BCE Loss: 1.059493064880371\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.055267810821533 | KNN Loss: 3.0360774993896484 | BCE Loss: 1.0191904306411743\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.097295761108398 | KNN Loss: 3.0651469230651855 | BCE Loss: 1.0321485996246338\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.092062950134277 | KNN Loss: 3.0529298782348633 | BCE Loss: 1.0391333103179932\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.060518264770508 | KNN Loss: 3.0309486389160156 | BCE Loss: 1.0295698642730713\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.098684310913086 | KNN Loss: 3.0782968997955322 | BCE Loss: 1.0203875303268433\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.071547508239746 | KNN Loss: 3.047027349472046 | BCE Loss: 1.0245203971862793\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.084949016571045 | KNN Loss: 3.091334342956543 | BCE Loss: 0.9936148524284363\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.145265579223633 | KNN Loss: 3.1051218509674072 | BCE Loss: 1.0401439666748047\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.077207088470459 | KNN Loss: 3.0533487796783447 | BCE Loss: 1.0238583087921143\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.074436664581299 | KNN Loss: 3.075849771499634 | BCE Loss: 0.9985870122909546\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.066472053527832 | KNN Loss: 3.067535638809204 | BCE Loss: 0.9989362359046936\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.0575761795043945 | KNN Loss: 3.074749231338501 | BCE Loss: 0.9828270673751831\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.122893333435059 | KNN Loss: 3.0896244049072266 | BCE Loss: 1.033268690109253\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.083364486694336 | KNN Loss: 3.054023265838623 | BCE Loss: 1.029341220855713\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.058626174926758 | KNN Loss: 3.073028564453125 | BCE Loss: 0.9855977892875671\n",
      "Epoch   261: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.085627555847168 | KNN Loss: 3.063591241836548 | BCE Loss: 1.0220363140106201\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.097087860107422 | KNN Loss: 3.054776668548584 | BCE Loss: 1.0423109531402588\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.072998046875 | KNN Loss: 3.065253734588623 | BCE Loss: 1.0077444314956665\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.100686550140381 | KNN Loss: 3.0699288845062256 | BCE Loss: 1.0307576656341553\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.068425178527832 | KNN Loss: 3.04315447807312 | BCE Loss: 1.025270700454712\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.055261611938477 | KNN Loss: 3.047436475753784 | BCE Loss: 1.007825255393982\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.081509113311768 | KNN Loss: 3.0678584575653076 | BCE Loss: 1.0136505365371704\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.09981632232666 | KNN Loss: 3.0789783000946045 | BCE Loss: 1.0208380222320557\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.057560443878174 | KNN Loss: 3.0412418842315674 | BCE Loss: 1.0163185596466064\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.062475204467773 | KNN Loss: 3.054955005645752 | BCE Loss: 1.0075204372406006\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.07706880569458 | KNN Loss: 3.05435848236084 | BCE Loss: 1.0227103233337402\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.097271919250488 | KNN Loss: 3.0575408935546875 | BCE Loss: 1.0397312641143799\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.047483444213867 | KNN Loss: 3.052009344100952 | BCE Loss: 0.9954738616943359\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.053239822387695 | KNN Loss: 3.044821262359619 | BCE Loss: 1.0084186792373657\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.066130638122559 | KNN Loss: 3.029815912246704 | BCE Loss: 1.036314606666565\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.0838541984558105 | KNN Loss: 3.04484486579895 | BCE Loss: 1.0390093326568604\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.079709053039551 | KNN Loss: 3.0404956340789795 | BCE Loss: 1.0392134189605713\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.071500778198242 | KNN Loss: 3.04620361328125 | BCE Loss: 1.0252971649169922\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.095521926879883 | KNN Loss: 3.060161828994751 | BCE Loss: 1.0353599786758423\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.050873756408691 | KNN Loss: 3.0719661712646484 | BCE Loss: 0.978907585144043\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.086194038391113 | KNN Loss: 3.057835102081299 | BCE Loss: 1.0283586978912354\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.076857089996338 | KNN Loss: 3.0372629165649414 | BCE Loss: 1.039594054222107\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.097165584564209 | KNN Loss: 3.0715880393981934 | BCE Loss: 1.0255775451660156\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.116262435913086 | KNN Loss: 3.08139967918396 | BCE Loss: 1.034862756729126\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.076078414916992 | KNN Loss: 3.0409233570098877 | BCE Loss: 1.035155177116394\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.0587639808654785 | KNN Loss: 3.0473124980926514 | BCE Loss: 1.0114513635635376\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.0403523445129395 | KNN Loss: 3.0592265129089355 | BCE Loss: 0.9811257123947144\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.103756427764893 | KNN Loss: 3.0597786903381348 | BCE Loss: 1.0439777374267578\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.061067581176758 | KNN Loss: 3.0556185245513916 | BCE Loss: 1.0054492950439453\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.103164196014404 | KNN Loss: 3.0897889137268066 | BCE Loss: 1.0133752822875977\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.058456897735596 | KNN Loss: 3.0393638610839844 | BCE Loss: 1.0190929174423218\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.1093950271606445 | KNN Loss: 3.0628507137298584 | BCE Loss: 1.0465443134307861\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.135457515716553 | KNN Loss: 3.1162140369415283 | BCE Loss: 1.0192434787750244\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.0024309158325195 | KNN Loss: 3.0046701431274414 | BCE Loss: 0.9977610111236572\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.106090545654297 | KNN Loss: 3.055778741836548 | BCE Loss: 1.0503120422363281\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.056659698486328 | KNN Loss: 3.049032688140869 | BCE Loss: 1.007627248764038\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.035326957702637 | KNN Loss: 3.0197291374206543 | BCE Loss: 1.0155975818634033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.1019721031188965 | KNN Loss: 3.0672097206115723 | BCE Loss: 1.0347622632980347\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.076491355895996 | KNN Loss: 3.0590267181396484 | BCE Loss: 1.0174648761749268\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.0941314697265625 | KNN Loss: 3.0670557022094727 | BCE Loss: 1.0270755290985107\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.049969673156738 | KNN Loss: 3.03505802154541 | BCE Loss: 1.0149115324020386\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.101031303405762 | KNN Loss: 3.0845513343811035 | BCE Loss: 1.0164802074432373\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.052271842956543 | KNN Loss: 3.050604820251465 | BCE Loss: 1.0016671419143677\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.0339508056640625 | KNN Loss: 3.039543628692627 | BCE Loss: 0.9944073557853699\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.073513984680176 | KNN Loss: 3.062321186065674 | BCE Loss: 1.0111925601959229\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.090332984924316 | KNN Loss: 3.0801520347595215 | BCE Loss: 1.010181188583374\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.084237575531006 | KNN Loss: 3.0670180320739746 | BCE Loss: 1.0172195434570312\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.044543266296387 | KNN Loss: 3.0339531898498535 | BCE Loss: 1.0105901956558228\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.08320426940918 | KNN Loss: 3.022202968597412 | BCE Loss: 1.0610013008117676\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.060732364654541 | KNN Loss: 3.0640103816986084 | BCE Loss: 0.9967219829559326\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.061302185058594 | KNN Loss: 3.0485477447509766 | BCE Loss: 1.0127546787261963\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.082348823547363 | KNN Loss: 3.0477569103240967 | BCE Loss: 1.0345921516418457\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.047611713409424 | KNN Loss: 3.0428409576416016 | BCE Loss: 1.0047707557678223\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.130223274230957 | KNN Loss: 3.091301202774048 | BCE Loss: 1.0389220714569092\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.084699630737305 | KNN Loss: 3.0640101432800293 | BCE Loss: 1.0206893682479858\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.11573600769043 | KNN Loss: 3.0789613723754883 | BCE Loss: 1.0367748737335205\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.07166862487793 | KNN Loss: 3.046982526779175 | BCE Loss: 1.0246859788894653\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.095231056213379 | KNN Loss: 3.0923352241516113 | BCE Loss: 1.0028960704803467\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.139023303985596 | KNN Loss: 3.094810724258423 | BCE Loss: 1.0442125797271729\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.081120491027832 | KNN Loss: 3.03995943069458 | BCE Loss: 1.041161060333252\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.054838180541992 | KNN Loss: 3.052130937576294 | BCE Loss: 1.0027073621749878\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.089978218078613 | KNN Loss: 3.0836925506591797 | BCE Loss: 1.0062856674194336\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.091093063354492 | KNN Loss: 3.0607261657714844 | BCE Loss: 1.0303670167922974\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.083186149597168 | KNN Loss: 3.0427474975585938 | BCE Loss: 1.0404384136199951\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.052773475646973 | KNN Loss: 3.0462169647216797 | BCE Loss: 1.0065562725067139\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.0979766845703125 | KNN Loss: 3.0759809017181396 | BCE Loss: 1.0219957828521729\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.079570770263672 | KNN Loss: 3.07772159576416 | BCE Loss: 1.0018489360809326\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.064050674438477 | KNN Loss: 3.029878854751587 | BCE Loss: 1.0341718196868896\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.0395708084106445 | KNN Loss: 3.0465900897979736 | BCE Loss: 0.9929806590080261\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.071113586425781 | KNN Loss: 3.030738592147827 | BCE Loss: 1.0403751134872437\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.053225040435791 | KNN Loss: 3.0409750938415527 | BCE Loss: 1.0122498273849487\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.1482462882995605 | KNN Loss: 3.076936721801758 | BCE Loss: 1.0713094472885132\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.069597244262695 | KNN Loss: 3.0787153244018555 | BCE Loss: 0.9908820986747742\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.03085470199585 | KNN Loss: 3.0325698852539062 | BCE Loss: 0.998284637928009\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.102816581726074 | KNN Loss: 3.07924747467041 | BCE Loss: 1.023568868637085\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.054346084594727 | KNN Loss: 3.0448458194732666 | BCE Loss: 1.009500503540039\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.094810485839844 | KNN Loss: 3.0563766956329346 | BCE Loss: 1.0384340286254883\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.058377742767334 | KNN Loss: 3.0562350749969482 | BCE Loss: 1.0021427869796753\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.123869895935059 | KNN Loss: 3.070119857788086 | BCE Loss: 1.0537500381469727\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.059143543243408 | KNN Loss: 3.0388669967651367 | BCE Loss: 1.0202765464782715\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.077117919921875 | KNN Loss: 3.082294225692749 | BCE Loss: 0.9948235750198364\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.118630886077881 | KNN Loss: 3.0794687271118164 | BCE Loss: 1.0391621589660645\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.0579400062561035 | KNN Loss: 3.0541884899139404 | BCE Loss: 1.0037516355514526\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.10807991027832 | KNN Loss: 3.101900577545166 | BCE Loss: 1.0061795711517334\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.082798480987549 | KNN Loss: 3.0407729148864746 | BCE Loss: 1.0420254468917847\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.103825092315674 | KNN Loss: 3.0673599243164062 | BCE Loss: 1.0364652872085571\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.086592674255371 | KNN Loss: 3.0438849925994873 | BCE Loss: 1.0427074432373047\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.046457290649414 | KNN Loss: 3.07442569732666 | BCE Loss: 0.9720314145088196\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.0712432861328125 | KNN Loss: 3.0585262775421143 | BCE Loss: 1.0127172470092773\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.063019752502441 | KNN Loss: 3.047520160675049 | BCE Loss: 1.0154995918273926\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.089258193969727 | KNN Loss: 3.06038236618042 | BCE Loss: 1.0288759469985962\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.081610679626465 | KNN Loss: 3.0675413608551025 | BCE Loss: 1.0140693187713623\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.073739051818848 | KNN Loss: 3.057486057281494 | BCE Loss: 1.0162527561187744\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.080085754394531 | KNN Loss: 3.087141275405884 | BCE Loss: 0.9929443597793579\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.098646640777588 | KNN Loss: 3.087615489959717 | BCE Loss: 1.011031150817871\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.086543083190918 | KNN Loss: 3.0786795616149902 | BCE Loss: 1.0078635215759277\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.0699615478515625 | KNN Loss: 3.0675597190856934 | BCE Loss: 1.0024019479751587\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.041671276092529 | KNN Loss: 3.00978946685791 | BCE Loss: 1.0318818092346191\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.0532050132751465 | KNN Loss: 3.032045841217041 | BCE Loss: 1.021159291267395\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.0879011154174805 | KNN Loss: 3.0543220043182373 | BCE Loss: 1.033578872680664\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.076779365539551 | KNN Loss: 3.059385299682617 | BCE Loss: 1.0173940658569336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.097932815551758 | KNN Loss: 3.070021152496338 | BCE Loss: 1.0279115438461304\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.070549964904785 | KNN Loss: 3.0431389808654785 | BCE Loss: 1.0274109840393066\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.045168876647949 | KNN Loss: 3.018531560897827 | BCE Loss: 1.026637077331543\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.111894130706787 | KNN Loss: 3.0833990573883057 | BCE Loss: 1.028494954109192\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.09842586517334 | KNN Loss: 3.0603489875793457 | BCE Loss: 1.0380768775939941\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.047945022583008 | KNN Loss: 3.02748966217041 | BCE Loss: 1.020455241203308\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.0932135581970215 | KNN Loss: 3.0669822692871094 | BCE Loss: 1.026231288909912\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.081758499145508 | KNN Loss: 3.080761194229126 | BCE Loss: 1.000997543334961\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.07082986831665 | KNN Loss: 3.06960129737854 | BCE Loss: 1.0012284517288208\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.1203508377075195 | KNN Loss: 3.082831621170044 | BCE Loss: 1.0375194549560547\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.067406177520752 | KNN Loss: 3.058638334274292 | BCE Loss: 1.0087677240371704\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.117505073547363 | KNN Loss: 3.0572497844696045 | BCE Loss: 1.0602551698684692\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.125690460205078 | KNN Loss: 3.1062707901000977 | BCE Loss: 1.01941978931427\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.043615818023682 | KNN Loss: 3.048819065093994 | BCE Loss: 0.994796633720398\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.106767177581787 | KNN Loss: 3.0732903480529785 | BCE Loss: 1.0334768295288086\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.1257710456848145 | KNN Loss: 3.0810461044311523 | BCE Loss: 1.044724941253662\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.136102199554443 | KNN Loss: 3.0936532020568848 | BCE Loss: 1.0424489974975586\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.057038307189941 | KNN Loss: 3.0628209114074707 | BCE Loss: 0.9942172765731812\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.062631607055664 | KNN Loss: 3.0481433868408203 | BCE Loss: 1.0144879817962646\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.091904640197754 | KNN Loss: 3.066879987716675 | BCE Loss: 1.0250244140625\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.090044975280762 | KNN Loss: 3.0893561840057373 | BCE Loss: 1.0006887912750244\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.111523151397705 | KNN Loss: 3.0642144680023193 | BCE Loss: 1.0473086833953857\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.0955047607421875 | KNN Loss: 3.0615220069885254 | BCE Loss: 1.033982753753662\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.091579914093018 | KNN Loss: 3.0687952041625977 | BCE Loss: 1.0227845907211304\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.023303985595703 | KNN Loss: 3.048664093017578 | BCE Loss: 0.9746400713920593\n",
      "Epoch   282: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.072629928588867 | KNN Loss: 3.051344156265259 | BCE Loss: 1.0212856531143188\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.077147006988525 | KNN Loss: 3.0413811206817627 | BCE Loss: 1.0357657670974731\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.086285591125488 | KNN Loss: 3.0797786712646484 | BCE Loss: 1.0065066814422607\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.110958576202393 | KNN Loss: 3.07385516166687 | BCE Loss: 1.037103533744812\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.08206844329834 | KNN Loss: 3.067675828933716 | BCE Loss: 1.0143928527832031\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.046446800231934 | KNN Loss: 3.065317153930664 | BCE Loss: 0.9811297059059143\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.095027923583984 | KNN Loss: 3.0328235626220703 | BCE Loss: 1.062204360961914\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.01671838760376 | KNN Loss: 3.036977767944336 | BCE Loss: 0.9797406196594238\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.094018459320068 | KNN Loss: 3.084644317626953 | BCE Loss: 1.0093741416931152\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.076445579528809 | KNN Loss: 3.06335186958313 | BCE Loss: 1.0130935907363892\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.0850090980529785 | KNN Loss: 3.056572437286377 | BCE Loss: 1.0284366607666016\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.030427932739258 | KNN Loss: 3.034064292907715 | BCE Loss: 0.9963638782501221\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.064515113830566 | KNN Loss: 3.0592949390411377 | BCE Loss: 1.0052201747894287\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.0673418045043945 | KNN Loss: 3.0383594036102295 | BCE Loss: 1.0289826393127441\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.118124485015869 | KNN Loss: 3.0761349201202393 | BCE Loss: 1.0419895648956299\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.074139595031738 | KNN Loss: 3.0642242431640625 | BCE Loss: 1.0099152326583862\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.048099994659424 | KNN Loss: 3.066337823867798 | BCE Loss: 0.9817621111869812\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.1048665046691895 | KNN Loss: 3.0941176414489746 | BCE Loss: 1.0107488632202148\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.099943161010742 | KNN Loss: 3.0888259410858154 | BCE Loss: 1.0111172199249268\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.055182933807373 | KNN Loss: 3.0541791915893555 | BCE Loss: 1.001003623008728\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.052149295806885 | KNN Loss: 3.0392568111419678 | BCE Loss: 1.0128926038742065\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.057716369628906 | KNN Loss: 3.036055088043213 | BCE Loss: 1.0216612815856934\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.054118633270264 | KNN Loss: 3.051313638687134 | BCE Loss: 1.0028049945831299\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.110894203186035 | KNN Loss: 3.0878095626831055 | BCE Loss: 1.0230848789215088\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.025686264038086 | KNN Loss: 3.035266160964966 | BCE Loss: 0.9904201030731201\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.077294826507568 | KNN Loss: 3.037269353866577 | BCE Loss: 1.0400254726409912\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.073389053344727 | KNN Loss: 3.0562479496002197 | BCE Loss: 1.0171408653259277\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.059062957763672 | KNN Loss: 3.043609857559204 | BCE Loss: 1.0154533386230469\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.124075889587402 | KNN Loss: 3.110257863998413 | BCE Loss: 1.0138177871704102\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.136310577392578 | KNN Loss: 3.094724178314209 | BCE Loss: 1.0415863990783691\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.1054911613464355 | KNN Loss: 3.0614712238311768 | BCE Loss: 1.0440199375152588\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.100727081298828 | KNN Loss: 3.080726146697998 | BCE Loss: 1.0200011730194092\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.1022186279296875 | KNN Loss: 3.0810344219207764 | BCE Loss: 1.0211844444274902\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.097212791442871 | KNN Loss: 3.063163995742798 | BCE Loss: 1.0340487957000732\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.061922073364258 | KNN Loss: 3.0432167053222656 | BCE Loss: 1.0187052488327026\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.138169765472412 | KNN Loss: 3.0984511375427246 | BCE Loss: 1.0397186279296875\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.036482810974121 | KNN Loss: 3.0287373065948486 | BCE Loss: 1.007745623588562\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.126282691955566 | KNN Loss: 3.1026663780212402 | BCE Loss: 1.023616075515747\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.1011738777160645 | KNN Loss: 3.069810628890991 | BCE Loss: 1.0313632488250732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.065742492675781 | KNN Loss: 3.05542254447937 | BCE Loss: 1.0103198289871216\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.105730056762695 | KNN Loss: 3.084167003631592 | BCE Loss: 1.0215630531311035\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.023646831512451 | KNN Loss: 3.0563290119171143 | BCE Loss: 0.9673178195953369\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.048339366912842 | KNN Loss: 3.04000186920166 | BCE Loss: 1.0083376169204712\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.029666423797607 | KNN Loss: 3.0634779930114746 | BCE Loss: 0.9661886096000671\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.064867973327637 | KNN Loss: 3.086627960205078 | BCE Loss: 0.978239893913269\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.046764373779297 | KNN Loss: 3.036769390106201 | BCE Loss: 1.0099949836730957\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.099961280822754 | KNN Loss: 3.0812606811523438 | BCE Loss: 1.0187008380889893\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.089217185974121 | KNN Loss: 3.081756353378296 | BCE Loss: 1.0074608325958252\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.069282531738281 | KNN Loss: 3.0613975524902344 | BCE Loss: 1.0078850984573364\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.0762810707092285 | KNN Loss: 3.0624678134918213 | BCE Loss: 1.0138132572174072\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.063333988189697 | KNN Loss: 3.0478947162628174 | BCE Loss: 1.0154392719268799\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.083849906921387 | KNN Loss: 3.0615200996398926 | BCE Loss: 1.0223300457000732\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.068779945373535 | KNN Loss: 3.0357444286346436 | BCE Loss: 1.0330355167388916\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.1343889236450195 | KNN Loss: 3.1001176834106445 | BCE Loss: 1.034271240234375\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.078801155090332 | KNN Loss: 3.042325973510742 | BCE Loss: 1.0364750623703003\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.055466175079346 | KNN Loss: 3.0541434288024902 | BCE Loss: 1.001322627067566\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.089221954345703 | KNN Loss: 3.086517572402954 | BCE Loss: 1.002704381942749\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.130239963531494 | KNN Loss: 3.092648983001709 | BCE Loss: 1.0375908613204956\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.0966901779174805 | KNN Loss: 3.0768959522247314 | BCE Loss: 1.0197944641113281\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.125216484069824 | KNN Loss: 3.078123092651367 | BCE Loss: 1.0470932722091675\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.079708576202393 | KNN Loss: 3.0752387046813965 | BCE Loss: 1.0044699907302856\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.077547073364258 | KNN Loss: 3.0427143573760986 | BCE Loss: 1.0348327159881592\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.098836898803711 | KNN Loss: 3.061352491378784 | BCE Loss: 1.0374846458435059\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.061241149902344 | KNN Loss: 3.0367608070373535 | BCE Loss: 1.0244803428649902\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.043153762817383 | KNN Loss: 3.0279996395111084 | BCE Loss: 1.0151543617248535\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.164013862609863 | KNN Loss: 3.135545015335083 | BCE Loss: 1.0284686088562012\n",
      "Epoch   293: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.130130290985107 | KNN Loss: 3.090104579925537 | BCE Loss: 1.0400258302688599\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.10142707824707 | KNN Loss: 3.0681536197662354 | BCE Loss: 1.0332733392715454\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.137309551239014 | KNN Loss: 3.0971686840057373 | BCE Loss: 1.0401408672332764\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.074709892272949 | KNN Loss: 3.0562620162963867 | BCE Loss: 1.018447995185852\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.090836524963379 | KNN Loss: 3.0632882118225098 | BCE Loss: 1.0275484323501587\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.0793585777282715 | KNN Loss: 3.0934486389160156 | BCE Loss: 0.9859099984169006\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.072403907775879 | KNN Loss: 3.042851209640503 | BCE Loss: 1.029552936553955\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.144693851470947 | KNN Loss: 3.1154534816741943 | BCE Loss: 1.029240369796753\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.067373275756836 | KNN Loss: 3.0604419708251953 | BCE Loss: 1.0069315433502197\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.071782112121582 | KNN Loss: 3.0490527153015137 | BCE Loss: 1.022729516029358\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.062853813171387 | KNN Loss: 3.0611732006073 | BCE Loss: 1.001680850982666\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.097625732421875 | KNN Loss: 3.077188730239868 | BCE Loss: 1.0204367637634277\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.084821701049805 | KNN Loss: 3.0701916217803955 | BCE Loss: 1.0146299600601196\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.055393218994141 | KNN Loss: 3.027827262878418 | BCE Loss: 1.0275661945343018\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.133660793304443 | KNN Loss: 3.1261372566223145 | BCE Loss: 1.0075234174728394\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.03980827331543 | KNN Loss: 3.033214569091797 | BCE Loss: 1.006593942642212\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.06464958190918 | KNN Loss: 3.052902936935425 | BCE Loss: 1.011746883392334\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.038430213928223 | KNN Loss: 3.0443778038024902 | BCE Loss: 0.9940524697303772\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.117624282836914 | KNN Loss: 3.083468198776245 | BCE Loss: 1.0341559648513794\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.114081382751465 | KNN Loss: 3.0930964946746826 | BCE Loss: 1.0209848880767822\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.064195156097412 | KNN Loss: 3.0159380435943604 | BCE Loss: 1.0482571125030518\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.054288864135742 | KNN Loss: 3.041440010070801 | BCE Loss: 1.0128486156463623\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.091672420501709 | KNN Loss: 3.0754034519195557 | BCE Loss: 1.0162690877914429\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.065821647644043 | KNN Loss: 3.0608246326446533 | BCE Loss: 1.0049970149993896\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.09623908996582 | KNN Loss: 3.090749740600586 | BCE Loss: 1.0054893493652344\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.070013523101807 | KNN Loss: 3.0483312606811523 | BCE Loss: 1.0216823816299438\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.030227184295654 | KNN Loss: 3.029514789581299 | BCE Loss: 1.000712275505066\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.063673973083496 | KNN Loss: 3.058624267578125 | BCE Loss: 1.005049705505371\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.029241561889648 | KNN Loss: 3.033677339553833 | BCE Loss: 0.9955639839172363\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.064845085144043 | KNN Loss: 3.062238931655884 | BCE Loss: 1.0026062726974487\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.095307350158691 | KNN Loss: 3.070831060409546 | BCE Loss: 1.024476170539856\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.080195426940918 | KNN Loss: 3.0482888221740723 | BCE Loss: 1.0319068431854248\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.096956729888916 | KNN Loss: 3.054222345352173 | BCE Loss: 1.0427343845367432\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.057909965515137 | KNN Loss: 3.042630672454834 | BCE Loss: 1.0152791738510132\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.079892158508301 | KNN Loss: 3.0691404342651367 | BCE Loss: 1.0107519626617432\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.105740070343018 | KNN Loss: 3.071471929550171 | BCE Loss: 1.0342680215835571\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.127171516418457 | KNN Loss: 3.093721628189087 | BCE Loss: 1.0334501266479492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.0399885177612305 | KNN Loss: 3.018707036972046 | BCE Loss: 1.0212817192077637\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.08001708984375 | KNN Loss: 3.057105779647827 | BCE Loss: 1.0229110717773438\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.095734596252441 | KNN Loss: 3.0582337379455566 | BCE Loss: 1.0375010967254639\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.0829010009765625 | KNN Loss: 3.037047863006592 | BCE Loss: 1.0458531379699707\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.080162048339844 | KNN Loss: 3.0853521823883057 | BCE Loss: 0.9948100447654724\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.053709983825684 | KNN Loss: 3.058764696121216 | BCE Loss: 0.9949450492858887\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.114897727966309 | KNN Loss: 3.0739548206329346 | BCE Loss: 1.0409431457519531\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.080594062805176 | KNN Loss: 3.073326826095581 | BCE Loss: 1.0072673559188843\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.069430351257324 | KNN Loss: 3.057252883911133 | BCE Loss: 1.0121777057647705\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.066122055053711 | KNN Loss: 3.0543124675750732 | BCE Loss: 1.0118093490600586\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.0669264793396 | KNN Loss: 3.063753366470337 | BCE Loss: 1.0031729936599731\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.118812561035156 | KNN Loss: 3.1145713329315186 | BCE Loss: 1.0042412281036377\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.096561431884766 | KNN Loss: 3.0870649814605713 | BCE Loss: 1.0094964504241943\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.088888645172119 | KNN Loss: 3.0727357864379883 | BCE Loss: 1.0161527395248413\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.050004959106445 | KNN Loss: 3.04779314994812 | BCE Loss: 1.0022119283676147\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.0884552001953125 | KNN Loss: 3.0773348808288574 | BCE Loss: 1.0111205577850342\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.074730396270752 | KNN Loss: 3.0403597354888916 | BCE Loss: 1.0343706607818604\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.081878662109375 | KNN Loss: 3.0584957599639893 | BCE Loss: 1.0233826637268066\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.075821876525879 | KNN Loss: 3.0618679523468018 | BCE Loss: 1.013953685760498\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.089775562286377 | KNN Loss: 3.0582447052001953 | BCE Loss: 1.0315308570861816\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.095720291137695 | KNN Loss: 3.086620330810547 | BCE Loss: 1.0090999603271484\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.066027641296387 | KNN Loss: 3.064450263977051 | BCE Loss: 1.0015774965286255\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.092668533325195 | KNN Loss: 3.0764777660369873 | BCE Loss: 1.016190767288208\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.032291412353516 | KNN Loss: 3.0208635330200195 | BCE Loss: 1.011427640914917\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.092357635498047 | KNN Loss: 3.08378005027771 | BCE Loss: 1.008577823638916\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.106213569641113 | KNN Loss: 3.060237169265747 | BCE Loss: 1.0459764003753662\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.094542026519775 | KNN Loss: 3.083505153656006 | BCE Loss: 1.0110368728637695\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.077511787414551 | KNN Loss: 3.0668551921844482 | BCE Loss: 1.0106568336486816\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.081096649169922 | KNN Loss: 3.0470261573791504 | BCE Loss: 1.034070611000061\n",
      "Epoch   304: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.050046920776367 | KNN Loss: 3.0488064289093018 | BCE Loss: 1.0012407302856445\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.0757832527160645 | KNN Loss: 3.0621531009674072 | BCE Loss: 1.0136300325393677\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.063009738922119 | KNN Loss: 3.0220649242401123 | BCE Loss: 1.0409448146820068\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.065059661865234 | KNN Loss: 3.038884401321411 | BCE Loss: 1.0261750221252441\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.076452255249023 | KNN Loss: 3.0595240592956543 | BCE Loss: 1.01692795753479\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.060009956359863 | KNN Loss: 3.047621965408325 | BCE Loss: 1.012387990951538\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.06886100769043 | KNN Loss: 3.0643527507781982 | BCE Loss: 1.0045082569122314\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.077539920806885 | KNN Loss: 3.046409845352173 | BCE Loss: 1.0311301946640015\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.134629249572754 | KNN Loss: 3.0691206455230713 | BCE Loss: 1.0655083656311035\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.105944633483887 | KNN Loss: 3.105210065841675 | BCE Loss: 1.000734567642212\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.055156230926514 | KNN Loss: 3.055283546447754 | BCE Loss: 0.9998725652694702\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.094686031341553 | KNN Loss: 3.0611510276794434 | BCE Loss: 1.033535122871399\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.081220626831055 | KNN Loss: 3.0752999782562256 | BCE Loss: 1.0059208869934082\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.112343788146973 | KNN Loss: 3.0830078125 | BCE Loss: 1.029335856437683\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.044551372528076 | KNN Loss: 3.0332181453704834 | BCE Loss: 1.0113331079483032\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.0694427490234375 | KNN Loss: 3.0467047691345215 | BCE Loss: 1.022737741470337\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.067646026611328 | KNN Loss: 3.0608103275299072 | BCE Loss: 1.0068359375\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.050770282745361 | KNN Loss: 3.0412752628326416 | BCE Loss: 1.0094950199127197\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.0945234298706055 | KNN Loss: 3.057441234588623 | BCE Loss: 1.0370819568634033\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.089716911315918 | KNN Loss: 3.0659635066986084 | BCE Loss: 1.0237535238265991\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.105224132537842 | KNN Loss: 3.057365655899048 | BCE Loss: 1.047858476638794\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.045907020568848 | KNN Loss: 3.0363831520080566 | BCE Loss: 1.0095241069793701\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.07794189453125 | KNN Loss: 3.05269718170166 | BCE Loss: 1.0252447128295898\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.121457099914551 | KNN Loss: 3.06593656539917 | BCE Loss: 1.0555202960968018\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.055568695068359 | KNN Loss: 3.0327866077423096 | BCE Loss: 1.0227818489074707\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.0811004638671875 | KNN Loss: 3.0482430458068848 | BCE Loss: 1.0328572988510132\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.053645133972168 | KNN Loss: 3.0338211059570312 | BCE Loss: 1.0198242664337158\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.105795383453369 | KNN Loss: 3.0810165405273438 | BCE Loss: 1.0247788429260254\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.117278099060059 | KNN Loss: 3.081829071044922 | BCE Loss: 1.0354491472244263\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.12027645111084 | KNN Loss: 3.091325044631958 | BCE Loss: 1.0289514064788818\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.089251518249512 | KNN Loss: 3.0634348392486572 | BCE Loss: 1.0258164405822754\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.069727897644043 | KNN Loss: 3.0612447261810303 | BCE Loss: 1.0084829330444336\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.07155179977417 | KNN Loss: 3.0894696712493896 | BCE Loss: 0.982082188129425\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.093814849853516 | KNN Loss: 3.0573928356170654 | BCE Loss: 1.0364220142364502\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.078234672546387 | KNN Loss: 3.0411667823791504 | BCE Loss: 1.0370676517486572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.1005120277404785 | KNN Loss: 3.069261312484741 | BCE Loss: 1.0312507152557373\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.0732316970825195 | KNN Loss: 3.035371780395508 | BCE Loss: 1.0378600358963013\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.088452339172363 | KNN Loss: 3.0697457790374756 | BCE Loss: 1.0187065601348877\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.045154571533203 | KNN Loss: 3.040414571762085 | BCE Loss: 1.0047398805618286\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.10651969909668 | KNN Loss: 3.0534355640411377 | BCE Loss: 1.053084373474121\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.108852386474609 | KNN Loss: 3.078925132751465 | BCE Loss: 1.0299270153045654\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.077104091644287 | KNN Loss: 3.056283712387085 | BCE Loss: 1.0208204984664917\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.14307165145874 | KNN Loss: 3.103580951690674 | BCE Loss: 1.0394906997680664\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.052792549133301 | KNN Loss: 3.0333926677703857 | BCE Loss: 1.0193997621536255\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.054042339324951 | KNN Loss: 3.057969331741333 | BCE Loss: 0.9960730671882629\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.045650959014893 | KNN Loss: 3.046254873275757 | BCE Loss: 0.9993960857391357\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.083014488220215 | KNN Loss: 3.0631542205810547 | BCE Loss: 1.0198602676391602\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.044073581695557 | KNN Loss: 3.028806447982788 | BCE Loss: 1.015267252922058\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.079612731933594 | KNN Loss: 3.047872304916382 | BCE Loss: 1.031740665435791\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.061939239501953 | KNN Loss: 3.065542221069336 | BCE Loss: 0.9963970184326172\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.039978981018066 | KNN Loss: 3.048006534576416 | BCE Loss: 0.9919725060462952\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.097370147705078 | KNN Loss: 3.0841336250305176 | BCE Loss: 1.0132365226745605\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.037673473358154 | KNN Loss: 3.041891574859619 | BCE Loss: 0.9957818984985352\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.037999153137207 | KNN Loss: 3.028273105621338 | BCE Loss: 1.0097262859344482\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.0626325607299805 | KNN Loss: 3.035794496536255 | BCE Loss: 1.0268380641937256\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.079436302185059 | KNN Loss: 3.0529441833496094 | BCE Loss: 1.0264918804168701\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.092222213745117 | KNN Loss: 3.0682456493377686 | BCE Loss: 1.0239763259887695\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.092560768127441 | KNN Loss: 3.0709385871887207 | BCE Loss: 1.0216219425201416\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.127821922302246 | KNN Loss: 3.0728673934936523 | BCE Loss: 1.0549544095993042\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.112736225128174 | KNN Loss: 3.072742462158203 | BCE Loss: 1.0399937629699707\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.0768561363220215 | KNN Loss: 3.048243999481201 | BCE Loss: 1.0286120176315308\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.090058326721191 | KNN Loss: 3.0751829147338867 | BCE Loss: 1.0148751735687256\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.0698957443237305 | KNN Loss: 3.056497097015381 | BCE Loss: 1.0133986473083496\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.075378894805908 | KNN Loss: 3.0887527465820312 | BCE Loss: 0.9866260290145874\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.054754257202148 | KNN Loss: 3.0545008182525635 | BCE Loss: 1.000253438949585\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.12672758102417 | KNN Loss: 3.103424549102783 | BCE Loss: 1.0233030319213867\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.1222686767578125 | KNN Loss: 3.107586145401001 | BCE Loss: 1.0146827697753906\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.118287086486816 | KNN Loss: 3.0910727977752686 | BCE Loss: 1.0272140502929688\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.095231056213379 | KNN Loss: 3.065049171447754 | BCE Loss: 1.0301820039749146\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.118344306945801 | KNN Loss: 3.0779435634613037 | BCE Loss: 1.0404009819030762\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.053472518920898 | KNN Loss: 3.0341455936431885 | BCE Loss: 1.0193266868591309\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.03331184387207 | KNN Loss: 3.0231728553771973 | BCE Loss: 1.010138750076294\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.053606986999512 | KNN Loss: 3.0422019958496094 | BCE Loss: 1.0114048719406128\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.09414529800415 | KNN Loss: 3.0668373107910156 | BCE Loss: 1.0273079872131348\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.1002936363220215 | KNN Loss: 3.08614182472229 | BCE Loss: 1.0141518115997314\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.114447116851807 | KNN Loss: 3.0655341148376465 | BCE Loss: 1.0489130020141602\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.088183403015137 | KNN Loss: 3.06748366355896 | BCE Loss: 1.0206995010375977\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.036420822143555 | KNN Loss: 3.0350537300109863 | BCE Loss: 1.0013669729232788\n",
      "Epoch   317: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.08648681640625 | KNN Loss: 3.049746036529541 | BCE Loss: 1.036740779876709\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.095612525939941 | KNN Loss: 3.05549693107605 | BCE Loss: 1.0401153564453125\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.079109191894531 | KNN Loss: 3.055551290512085 | BCE Loss: 1.0235579013824463\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.111594200134277 | KNN Loss: 3.0635550022125244 | BCE Loss: 1.0480389595031738\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.09171199798584 | KNN Loss: 3.059831380844116 | BCE Loss: 1.0318806171417236\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.06026554107666 | KNN Loss: 3.0494472980499268 | BCE Loss: 1.0108182430267334\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.0449934005737305 | KNN Loss: 3.0307154655456543 | BCE Loss: 1.0142779350280762\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.100825786590576 | KNN Loss: 3.0852043628692627 | BCE Loss: 1.015621304512024\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.040892124176025 | KNN Loss: 3.0390758514404297 | BCE Loss: 1.0018162727355957\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.0763163566589355 | KNN Loss: 3.0502686500549316 | BCE Loss: 1.0260478258132935\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.056821346282959 | KNN Loss: 3.030395030975342 | BCE Loss: 1.0264261960983276\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.073511123657227 | KNN Loss: 3.0495636463165283 | BCE Loss: 1.0239472389221191\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.097179412841797 | KNN Loss: 3.0581672191619873 | BCE Loss: 1.0390124320983887\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.08609676361084 | KNN Loss: 3.0652918815612793 | BCE Loss: 1.0208048820495605\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.071319580078125 | KNN Loss: 3.0394387245178223 | BCE Loss: 1.0318808555603027\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.052927017211914 | KNN Loss: 3.0340399742126465 | BCE Loss: 1.0188871622085571\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.116850852966309 | KNN Loss: 3.0876410007476807 | BCE Loss: 1.0292096138000488\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.126824378967285 | KNN Loss: 3.0897114276885986 | BCE Loss: 1.0371131896972656\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.071621894836426 | KNN Loss: 3.0558769702911377 | BCE Loss: 1.0157448053359985\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.033060073852539 | KNN Loss: 3.0309393405914307 | BCE Loss: 1.0021207332611084\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.029293537139893 | KNN Loss: 3.0194966793060303 | BCE Loss: 1.0097968578338623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.084763526916504 | KNN Loss: 3.0778989791870117 | BCE Loss: 1.006864309310913\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.096271991729736 | KNN Loss: 3.0633974075317383 | BCE Loss: 1.032874584197998\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.096059799194336 | KNN Loss: 3.0861282348632812 | BCE Loss: 1.0099313259124756\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.074652194976807 | KNN Loss: 3.0500199794769287 | BCE Loss: 1.024632215499878\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.119438171386719 | KNN Loss: 3.0931994915008545 | BCE Loss: 1.0262386798858643\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.072399616241455 | KNN Loss: 3.0183281898498535 | BCE Loss: 1.054071307182312\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.075682640075684 | KNN Loss: 3.076514959335327 | BCE Loss: 0.999167799949646\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.059852123260498 | KNN Loss: 3.0519766807556152 | BCE Loss: 1.0078754425048828\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.076097011566162 | KNN Loss: 3.053314208984375 | BCE Loss: 1.0227829217910767\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.056077003479004 | KNN Loss: 3.0356101989746094 | BCE Loss: 1.0204665660858154\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.07155179977417 | KNN Loss: 3.051987886428833 | BCE Loss: 1.019563913345337\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.143598556518555 | KNN Loss: 3.107327461242676 | BCE Loss: 1.036271095275879\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.0646891593933105 | KNN Loss: 3.0471034049987793 | BCE Loss: 1.0175858736038208\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.059844970703125 | KNN Loss: 3.070119619369507 | BCE Loss: 0.9897254705429077\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.121482849121094 | KNN Loss: 3.093139410018921 | BCE Loss: 1.028343677520752\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.068285942077637 | KNN Loss: 3.045624256134033 | BCE Loss: 1.0226616859436035\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.110481262207031 | KNN Loss: 3.119220495223999 | BCE Loss: 0.9912610054016113\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.0797119140625 | KNN Loss: 3.07969069480896 | BCE Loss: 1.000020980834961\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.033137321472168 | KNN Loss: 3.0409317016601562 | BCE Loss: 0.9922058582305908\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.083362579345703 | KNN Loss: 3.051887273788452 | BCE Loss: 1.0314750671386719\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.031883716583252 | KNN Loss: 3.0448734760284424 | BCE Loss: 0.98701012134552\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.0575079917907715 | KNN Loss: 3.0526914596557617 | BCE Loss: 1.0048165321350098\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.123494625091553 | KNN Loss: 3.0948665142059326 | BCE Loss: 1.0286279916763306\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.080885410308838 | KNN Loss: 3.0608596801757812 | BCE Loss: 1.0200257301330566\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.063509941101074 | KNN Loss: 3.0650265216827393 | BCE Loss: 0.998483419418335\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.129965782165527 | KNN Loss: 3.108128547668457 | BCE Loss: 1.0218372344970703\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.088788032531738 | KNN Loss: 3.0559611320495605 | BCE Loss: 1.0328270196914673\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.054078102111816 | KNN Loss: 3.041630506515503 | BCE Loss: 1.0124478340148926\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.097102165222168 | KNN Loss: 3.073456048965454 | BCE Loss: 1.0236461162567139\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.090558052062988 | KNN Loss: 3.0591413974761963 | BCE Loss: 1.031416416168213\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.097092151641846 | KNN Loss: 3.051443338394165 | BCE Loss: 1.0456489324569702\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.070530891418457 | KNN Loss: 3.043191432952881 | BCE Loss: 1.0273395776748657\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.132893085479736 | KNN Loss: 3.0948588848114014 | BCE Loss: 1.0380343198776245\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.051686763763428 | KNN Loss: 3.0681185722351074 | BCE Loss: 0.9835683107376099\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.097665786743164 | KNN Loss: 3.0781214237213135 | BCE Loss: 1.0195443630218506\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.083789825439453 | KNN Loss: 3.091219663619995 | BCE Loss: 0.992570161819458\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.073256492614746 | KNN Loss: 3.0599100589752197 | BCE Loss: 1.0133461952209473\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.087906360626221 | KNN Loss: 3.0464446544647217 | BCE Loss: 1.041461706161499\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.123805522918701 | KNN Loss: 3.0905497074127197 | BCE Loss: 1.033255696296692\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.108023166656494 | KNN Loss: 3.0591723918914795 | BCE Loss: 1.0488507747650146\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.066041469573975 | KNN Loss: 3.057223081588745 | BCE Loss: 1.0088183879852295\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.102139472961426 | KNN Loss: 3.090100049972534 | BCE Loss: 1.0120396614074707\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.070276260375977 | KNN Loss: 3.0472960472106934 | BCE Loss: 1.0229802131652832\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.083383560180664 | KNN Loss: 3.0587337017059326 | BCE Loss: 1.0246496200561523\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.138125896453857 | KNN Loss: 3.1016788482666016 | BCE Loss: 1.0364470481872559\n",
      "Epoch   328: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.05873441696167 | KNN Loss: 3.056776523590088 | BCE Loss: 1.0019577741622925\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.078080177307129 | KNN Loss: 3.0556883811950684 | BCE Loss: 1.0223915576934814\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.079696178436279 | KNN Loss: 3.0641865730285645 | BCE Loss: 1.0155094861984253\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.109959602355957 | KNN Loss: 3.074154853820801 | BCE Loss: 1.0358047485351562\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.106224536895752 | KNN Loss: 3.0836846828460693 | BCE Loss: 1.0225399732589722\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.1431732177734375 | KNN Loss: 3.0958075523376465 | BCE Loss: 1.047365665435791\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.0954413414001465 | KNN Loss: 3.0601553916931152 | BCE Loss: 1.0352860689163208\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.071862697601318 | KNN Loss: 3.047506093978882 | BCE Loss: 1.0243566036224365\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.104683876037598 | KNN Loss: 3.0706682205200195 | BCE Loss: 1.0340156555175781\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.057368755340576 | KNN Loss: 3.0276238918304443 | BCE Loss: 1.0297449827194214\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.081811904907227 | KNN Loss: 3.0722239017486572 | BCE Loss: 1.0095878839492798\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.052431106567383 | KNN Loss: 3.0524206161499023 | BCE Loss: 1.0000104904174805\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.099667072296143 | KNN Loss: 3.0900704860687256 | BCE Loss: 1.0095964670181274\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.099969863891602 | KNN Loss: 3.0631585121154785 | BCE Loss: 1.036811113357544\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.128748893737793 | KNN Loss: 3.055213212966919 | BCE Loss: 1.0735359191894531\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.075647354125977 | KNN Loss: 3.0584142208099365 | BCE Loss: 1.017232894897461\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.073185920715332 | KNN Loss: 3.0450215339660645 | BCE Loss: 1.0281643867492676\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.088194847106934 | KNN Loss: 3.060765266418457 | BCE Loss: 1.0274295806884766\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.085714340209961 | KNN Loss: 3.0587151050567627 | BCE Loss: 1.0269989967346191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.098315238952637 | KNN Loss: 3.064304828643799 | BCE Loss: 1.0340101718902588\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.104279041290283 | KNN Loss: 3.082552194595337 | BCE Loss: 1.0217269659042358\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.1098175048828125 | KNN Loss: 3.065845489501953 | BCE Loss: 1.0439722537994385\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.080868244171143 | KNN Loss: 3.0816409587860107 | BCE Loss: 0.9992272853851318\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.064018726348877 | KNN Loss: 3.082602024078369 | BCE Loss: 0.9814167022705078\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.077089309692383 | KNN Loss: 3.048429489135742 | BCE Loss: 1.0286600589752197\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.069936275482178 | KNN Loss: 3.053239107131958 | BCE Loss: 1.0166971683502197\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.0792155265808105 | KNN Loss: 3.055415153503418 | BCE Loss: 1.0238003730773926\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.072936534881592 | KNN Loss: 3.0610036849975586 | BCE Loss: 1.0119328498840332\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.138556480407715 | KNN Loss: 3.072727680206299 | BCE Loss: 1.065828561782837\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.068968296051025 | KNN Loss: 3.06626033782959 | BCE Loss: 1.0027079582214355\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.108345031738281 | KNN Loss: 3.056558132171631 | BCE Loss: 1.0517868995666504\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.060346603393555 | KNN Loss: 3.04296612739563 | BCE Loss: 1.0173803567886353\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.00968599319458 | KNN Loss: 3.0274105072021484 | BCE Loss: 0.982275664806366\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.049620628356934 | KNN Loss: 3.030846118927002 | BCE Loss: 1.0187747478485107\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.090341091156006 | KNN Loss: 3.0492961406707764 | BCE Loss: 1.041045069694519\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.073663711547852 | KNN Loss: 3.052582025527954 | BCE Loss: 1.021081566810608\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.067307472229004 | KNN Loss: 3.0553054809570312 | BCE Loss: 1.0120017528533936\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.101059913635254 | KNN Loss: 3.0712242126464844 | BCE Loss: 1.0298357009887695\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.032054424285889 | KNN Loss: 3.037233352661133 | BCE Loss: 0.9948212504386902\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.111846446990967 | KNN Loss: 3.0788192749023438 | BCE Loss: 1.033027172088623\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.069020748138428 | KNN Loss: 3.036740779876709 | BCE Loss: 1.0322799682617188\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.064390659332275 | KNN Loss: 3.08135724067688 | BCE Loss: 0.9830335378646851\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.073237419128418 | KNN Loss: 3.0585951805114746 | BCE Loss: 1.0146420001983643\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.038730144500732 | KNN Loss: 3.0426857471466064 | BCE Loss: 0.996044397354126\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.072388648986816 | KNN Loss: 3.042419195175171 | BCE Loss: 1.0299694538116455\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.033623695373535 | KNN Loss: 3.0168890953063965 | BCE Loss: 1.0167343616485596\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.052024841308594 | KNN Loss: 3.0388832092285156 | BCE Loss: 1.0131416320800781\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.111462116241455 | KNN Loss: 3.0869624614715576 | BCE Loss: 1.024499773979187\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.043421745300293 | KNN Loss: 3.0336554050445557 | BCE Loss: 1.0097665786743164\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.056069374084473 | KNN Loss: 3.0452098846435547 | BCE Loss: 1.010859727859497\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.079991817474365 | KNN Loss: 3.049281120300293 | BCE Loss: 1.0307106971740723\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.070589065551758 | KNN Loss: 3.050246477127075 | BCE Loss: 1.0203423500061035\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.0728983879089355 | KNN Loss: 3.066378593444824 | BCE Loss: 1.0065197944641113\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.089791297912598 | KNN Loss: 3.0743913650512695 | BCE Loss: 1.0153998136520386\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.057398796081543 | KNN Loss: 3.0495235919952393 | BCE Loss: 1.0078754425048828\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.090706825256348 | KNN Loss: 3.0681471824645996 | BCE Loss: 1.022559404373169\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.064455986022949 | KNN Loss: 3.04589581489563 | BCE Loss: 1.0185600519180298\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.139718055725098 | KNN Loss: 3.1148664951324463 | BCE Loss: 1.0248515605926514\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.050717353820801 | KNN Loss: 3.0233919620513916 | BCE Loss: 1.0273256301879883\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.13340950012207 | KNN Loss: 3.106846332550049 | BCE Loss: 1.0265631675720215\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.053155899047852 | KNN Loss: 3.0277934074401855 | BCE Loss: 1.025362253189087\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.084338188171387 | KNN Loss: 3.066985607147217 | BCE Loss: 1.0173527002334595\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.071946620941162 | KNN Loss: 3.066236972808838 | BCE Loss: 1.0057096481323242\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.088560104370117 | KNN Loss: 3.0616393089294434 | BCE Loss: 1.0269206762313843\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.090321063995361 | KNN Loss: 3.075812578201294 | BCE Loss: 1.0145084857940674\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.044038772583008 | KNN Loss: 3.0342841148376465 | BCE Loss: 1.0097547769546509\n",
      "Epoch   339: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.074427604675293 | KNN Loss: 3.072097063064575 | BCE Loss: 1.0023305416107178\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.082071304321289 | KNN Loss: 3.063326597213745 | BCE Loss: 1.018744707107544\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.1025590896606445 | KNN Loss: 3.0552191734313965 | BCE Loss: 1.0473400354385376\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.082228660583496 | KNN Loss: 3.0737645626068115 | BCE Loss: 1.0084640979766846\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.085116863250732 | KNN Loss: 3.0588817596435547 | BCE Loss: 1.0262351036071777\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.078410625457764 | KNN Loss: 3.060666799545288 | BCE Loss: 1.017743706703186\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.0649871826171875 | KNN Loss: 3.0616843700408936 | BCE Loss: 1.003303050994873\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.063447952270508 | KNN Loss: 3.041844129562378 | BCE Loss: 1.021604061126709\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.074051856994629 | KNN Loss: 3.0843074321746826 | BCE Loss: 0.9897444248199463\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.030309677124023 | KNN Loss: 3.038330554962158 | BCE Loss: 0.9919789433479309\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.052689552307129 | KNN Loss: 3.033792018890381 | BCE Loss: 1.018897294998169\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.067797660827637 | KNN Loss: 3.055682420730591 | BCE Loss: 1.0121150016784668\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.050097465515137 | KNN Loss: 3.037245988845825 | BCE Loss: 1.0128514766693115\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.0672783851623535 | KNN Loss: 3.047318935394287 | BCE Loss: 1.019959568977356\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.042338848114014 | KNN Loss: 3.0268938541412354 | BCE Loss: 1.0154449939727783\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.131039142608643 | KNN Loss: 3.109773635864258 | BCE Loss: 1.0212656259536743\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.076164245605469 | KNN Loss: 3.061622142791748 | BCE Loss: 1.0145423412322998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.07908821105957 | KNN Loss: 3.065523624420166 | BCE Loss: 1.0135648250579834\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.105158805847168 | KNN Loss: 3.085634231567383 | BCE Loss: 1.0195248126983643\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.073190689086914 | KNN Loss: 3.0670089721679688 | BCE Loss: 1.0061814785003662\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.098124980926514 | KNN Loss: 3.0828466415405273 | BCE Loss: 1.0152784585952759\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.0344953536987305 | KNN Loss: 3.04754376411438 | BCE Loss: 0.9869518280029297\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.025619983673096 | KNN Loss: 3.0167596340179443 | BCE Loss: 1.0088603496551514\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.063281059265137 | KNN Loss: 3.0475542545318604 | BCE Loss: 1.015726923942566\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.0590596199035645 | KNN Loss: 3.042356491088867 | BCE Loss: 1.0167032480239868\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.100564956665039 | KNN Loss: 3.089705467224121 | BCE Loss: 1.010859727859497\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.0536932945251465 | KNN Loss: 3.0569827556610107 | BCE Loss: 0.9967106580734253\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.073637962341309 | KNN Loss: 3.0482733249664307 | BCE Loss: 1.025364875793457\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.099376678466797 | KNN Loss: 3.0519375801086426 | BCE Loss: 1.0474393367767334\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.085474014282227 | KNN Loss: 3.061992645263672 | BCE Loss: 1.0234813690185547\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.063397407531738 | KNN Loss: 3.033352851867676 | BCE Loss: 1.0300445556640625\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.075301170349121 | KNN Loss: 3.0395262241363525 | BCE Loss: 1.035774827003479\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.061359882354736 | KNN Loss: 3.0540518760681152 | BCE Loss: 1.0073078870773315\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.118006706237793 | KNN Loss: 3.0825724601745605 | BCE Loss: 1.0354344844818115\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.103025436401367 | KNN Loss: 3.055603265762329 | BCE Loss: 1.0474222898483276\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.07285213470459 | KNN Loss: 3.054990768432617 | BCE Loss: 1.0178616046905518\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.090781211853027 | KNN Loss: 3.0614068508148193 | BCE Loss: 1.029374599456787\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.070006847381592 | KNN Loss: 3.0473520755767822 | BCE Loss: 1.0226547718048096\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.095390796661377 | KNN Loss: 3.0459823608398438 | BCE Loss: 1.0494085550308228\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.0762834548950195 | KNN Loss: 3.0716118812561035 | BCE Loss: 1.004671335220337\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.086268424987793 | KNN Loss: 3.051760196685791 | BCE Loss: 1.034508466720581\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.121411323547363 | KNN Loss: 3.0983850955963135 | BCE Loss: 1.0230259895324707\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.0744218826293945 | KNN Loss: 3.0545175075531006 | BCE Loss: 1.0199042558670044\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.041374206542969 | KNN Loss: 3.0204358100891113 | BCE Loss: 1.0209381580352783\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.0860276222229 | KNN Loss: 3.0389904975891113 | BCE Loss: 1.047037124633789\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.118392467498779 | KNN Loss: 3.0847294330596924 | BCE Loss: 1.033663034439087\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.086880683898926 | KNN Loss: 3.0634663105010986 | BCE Loss: 1.023414134979248\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.041934967041016 | KNN Loss: 3.0228819847106934 | BCE Loss: 1.0190527439117432\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.078547477722168 | KNN Loss: 3.0688068866729736 | BCE Loss: 1.0097407102584839\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.1429853439331055 | KNN Loss: 3.1057357788085938 | BCE Loss: 1.0372498035430908\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.043517589569092 | KNN Loss: 3.058870315551758 | BCE Loss: 0.9846470952033997\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.040891170501709 | KNN Loss: 3.043839931488037 | BCE Loss: 0.9970512390136719\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.075250148773193 | KNN Loss: 3.0456504821777344 | BCE Loss: 1.029599666595459\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.079637050628662 | KNN Loss: 3.070739269256592 | BCE Loss: 1.0088977813720703\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.0450849533081055 | KNN Loss: 3.0413777828216553 | BCE Loss: 1.0037070512771606\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.086444854736328 | KNN Loss: 3.046787738800049 | BCE Loss: 1.0396568775177002\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.085608005523682 | KNN Loss: 3.0723674297332764 | BCE Loss: 1.0132406949996948\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.097964763641357 | KNN Loss: 3.0657031536102295 | BCE Loss: 1.0322614908218384\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.0453691482543945 | KNN Loss: 3.0507283210754395 | BCE Loss: 0.9946408867835999\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.042723178863525 | KNN Loss: 3.047325849533081 | BCE Loss: 0.9953973293304443\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.048239231109619 | KNN Loss: 3.0545971393585205 | BCE Loss: 0.9936420917510986\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.04287052154541 | KNN Loss: 3.0220437049865723 | BCE Loss: 1.020827054977417\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.097121238708496 | KNN Loss: 3.0527267456054688 | BCE Loss: 1.0443944931030273\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.089165687561035 | KNN Loss: 3.0451247692108154 | BCE Loss: 1.0440411567687988\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.076359748840332 | KNN Loss: 3.064055919647217 | BCE Loss: 1.0123038291931152\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.100439071655273 | KNN Loss: 3.0562753677368164 | BCE Loss: 1.044163703918457\n",
      "Epoch   350: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.060039043426514 | KNN Loss: 3.0290253162384033 | BCE Loss: 1.0310136079788208\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.075661659240723 | KNN Loss: 3.060176134109497 | BCE Loss: 1.0154857635498047\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.051589012145996 | KNN Loss: 3.0403857231140137 | BCE Loss: 1.0112032890319824\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.089870452880859 | KNN Loss: 3.0765268802642822 | BCE Loss: 1.0133438110351562\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.052001953125 | KNN Loss: 3.0438625812530518 | BCE Loss: 1.0081391334533691\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.095893383026123 | KNN Loss: 3.080453395843506 | BCE Loss: 1.0154399871826172\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.05800724029541 | KNN Loss: 3.061584234237671 | BCE Loss: 0.9964231252670288\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.084988594055176 | KNN Loss: 3.089104652404785 | BCE Loss: 0.9958838224411011\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.077802658081055 | KNN Loss: 3.066843032836914 | BCE Loss: 1.0109596252441406\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.115086555480957 | KNN Loss: 3.0763046741485596 | BCE Loss: 1.0387818813323975\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.04816198348999 | KNN Loss: 3.0423712730407715 | BCE Loss: 1.0057908296585083\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.110949993133545 | KNN Loss: 3.0591328144073486 | BCE Loss: 1.0518170595169067\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.141016960144043 | KNN Loss: 3.0863237380981445 | BCE Loss: 1.0546932220458984\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.108913898468018 | KNN Loss: 3.056654453277588 | BCE Loss: 1.0522594451904297\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.112967014312744 | KNN Loss: 3.091432809829712 | BCE Loss: 1.0215343236923218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.032246112823486 | KNN Loss: 3.0293757915496826 | BCE Loss: 1.0028703212738037\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.069103240966797 | KNN Loss: 3.0453760623931885 | BCE Loss: 1.0237271785736084\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.054629325866699 | KNN Loss: 3.0577046871185303 | BCE Loss: 0.996924638748169\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.050727844238281 | KNN Loss: 3.0572831630706787 | BCE Loss: 0.993444561958313\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.109753608703613 | KNN Loss: 3.0856170654296875 | BCE Loss: 1.0241365432739258\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.0666022300720215 | KNN Loss: 3.0489020347595215 | BCE Loss: 1.0177001953125\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.080479145050049 | KNN Loss: 3.0562362670898438 | BCE Loss: 1.0242429971694946\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.065407752990723 | KNN Loss: 3.030540704727173 | BCE Loss: 1.0348669290542603\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.080446720123291 | KNN Loss: 3.0928432941436768 | BCE Loss: 0.9876036047935486\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.099924087524414 | KNN Loss: 3.0894274711608887 | BCE Loss: 1.0104964971542358\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.0816144943237305 | KNN Loss: 3.07147479057312 | BCE Loss: 1.0101398229599\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.070327281951904 | KNN Loss: 3.0327179431915283 | BCE Loss: 1.0376092195510864\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.139148235321045 | KNN Loss: 3.0716750621795654 | BCE Loss: 1.0674731731414795\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.059107780456543 | KNN Loss: 3.055246353149414 | BCE Loss: 1.0038613080978394\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.062389850616455 | KNN Loss: 3.0451126098632812 | BCE Loss: 1.0172772407531738\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.115727424621582 | KNN Loss: 3.1113624572753906 | BCE Loss: 1.0043649673461914\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.061622619628906 | KNN Loss: 3.055202007293701 | BCE Loss: 1.006420373916626\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.075356960296631 | KNN Loss: 3.053248405456543 | BCE Loss: 1.022108554840088\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.097190856933594 | KNN Loss: 3.048658609390259 | BCE Loss: 1.0485320091247559\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.087964057922363 | KNN Loss: 3.0729475021362305 | BCE Loss: 1.015016794204712\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.0854082107543945 | KNN Loss: 3.0917117595672607 | BCE Loss: 0.9936965107917786\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.074631690979004 | KNN Loss: 3.0503177642822266 | BCE Loss: 1.0243136882781982\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.108899116516113 | KNN Loss: 3.0793275833129883 | BCE Loss: 1.0295716524124146\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.065911769866943 | KNN Loss: 3.0445847511291504 | BCE Loss: 1.021327018737793\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.117998123168945 | KNN Loss: 3.0993893146514893 | BCE Loss: 1.018608570098877\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.123895168304443 | KNN Loss: 3.0956385135650635 | BCE Loss: 1.0282565355300903\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.020754814147949 | KNN Loss: 3.038522481918335 | BCE Loss: 0.9822323322296143\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.096891403198242 | KNN Loss: 3.0764622688293457 | BCE Loss: 1.0204288959503174\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.090895652770996 | KNN Loss: 3.0592849254608154 | BCE Loss: 1.0316107273101807\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.052306175231934 | KNN Loss: 3.0621955394744873 | BCE Loss: 0.9901108741760254\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.0949530601501465 | KNN Loss: 3.0654261112213135 | BCE Loss: 1.0295270681381226\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.030805587768555 | KNN Loss: 3.031787633895874 | BCE Loss: 0.9990181922912598\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.083061218261719 | KNN Loss: 3.0615198612213135 | BCE Loss: 1.0215413570404053\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.066783905029297 | KNN Loss: 3.044833183288574 | BCE Loss: 1.021950602531433\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.084315776824951 | KNN Loss: 3.0668785572052 | BCE Loss: 1.0174371004104614\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.133827209472656 | KNN Loss: 3.1252071857452393 | BCE Loss: 1.008620262145996\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.073808193206787 | KNN Loss: 3.0644726753234863 | BCE Loss: 1.0093355178833008\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.03957986831665 | KNN Loss: 3.0425100326538086 | BCE Loss: 0.9970698952674866\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.037144184112549 | KNN Loss: 3.0553221702575684 | BCE Loss: 0.98182213306427\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.107816696166992 | KNN Loss: 3.0835766792297363 | BCE Loss: 1.0242397785186768\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.075979232788086 | KNN Loss: 3.0743298530578613 | BCE Loss: 1.0016496181488037\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.053456783294678 | KNN Loss: 3.046048164367676 | BCE Loss: 1.007408618927002\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.117701053619385 | KNN Loss: 3.093295097351074 | BCE Loss: 1.0244059562683105\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.138889312744141 | KNN Loss: 3.0895235538482666 | BCE Loss: 1.049365758895874\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.105000972747803 | KNN Loss: 3.060184955596924 | BCE Loss: 1.044816017150879\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.101165771484375 | KNN Loss: 3.0912387371063232 | BCE Loss: 1.0099271535873413\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.131584167480469 | KNN Loss: 3.118350028991699 | BCE Loss: 1.0132341384887695\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.102407932281494 | KNN Loss: 3.0984017848968506 | BCE Loss: 1.004006028175354\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.060379981994629 | KNN Loss: 3.042362689971924 | BCE Loss: 1.0180175304412842\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.139376640319824 | KNN Loss: 3.1216609477996826 | BCE Loss: 1.0177156925201416\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.071460723876953 | KNN Loss: 3.0226826667785645 | BCE Loss: 1.0487780570983887\n",
      "Epoch   361: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.0573248863220215 | KNN Loss: 3.0376944541931152 | BCE Loss: 1.0196303129196167\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.0922160148620605 | KNN Loss: 3.094430446624756 | BCE Loss: 0.9977855682373047\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.096710681915283 | KNN Loss: 3.076995372772217 | BCE Loss: 1.0197153091430664\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.085376262664795 | KNN Loss: 3.0369863510131836 | BCE Loss: 1.0483899116516113\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.058014869689941 | KNN Loss: 3.0310044288635254 | BCE Loss: 1.027010202407837\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.077517509460449 | KNN Loss: 3.049494504928589 | BCE Loss: 1.0280227661132812\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.05599308013916 | KNN Loss: 3.049234390258789 | BCE Loss: 1.0067589282989502\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.045627593994141 | KNN Loss: 3.0403659343719482 | BCE Loss: 1.0052618980407715\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.082461833953857 | KNN Loss: 3.0700008869171143 | BCE Loss: 1.0124609470367432\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.091850280761719 | KNN Loss: 3.055474281311035 | BCE Loss: 1.0363759994506836\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.063834190368652 | KNN Loss: 3.058162212371826 | BCE Loss: 1.0056719779968262\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.056267738342285 | KNN Loss: 3.0473718643188477 | BCE Loss: 1.0088956356048584\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.092038154602051 | KNN Loss: 3.06354022026062 | BCE Loss: 1.0284976959228516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.097436904907227 | KNN Loss: 3.058626651763916 | BCE Loss: 1.0388102531433105\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.1158833503723145 | KNN Loss: 3.0995635986328125 | BCE Loss: 1.0163198709487915\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.100815773010254 | KNN Loss: 3.058811902999878 | BCE Loss: 1.042003870010376\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.0848188400268555 | KNN Loss: 3.0716888904571533 | BCE Loss: 1.013129711151123\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.061610221862793 | KNN Loss: 3.0529844760894775 | BCE Loss: 1.0086255073547363\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.064194679260254 | KNN Loss: 3.0640203952789307 | BCE Loss: 1.0001745223999023\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.0738325119018555 | KNN Loss: 3.064962387084961 | BCE Loss: 1.008870244026184\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.062845230102539 | KNN Loss: 3.0551962852478027 | BCE Loss: 1.0076487064361572\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.105020999908447 | KNN Loss: 3.087550640106201 | BCE Loss: 1.017470359802246\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.042365550994873 | KNN Loss: 3.029529333114624 | BCE Loss: 1.0128363370895386\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.111222267150879 | KNN Loss: 3.0554986000061035 | BCE Loss: 1.0557239055633545\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.069634437561035 | KNN Loss: 3.0431272983551025 | BCE Loss: 1.0265069007873535\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.084209442138672 | KNN Loss: 3.033665180206299 | BCE Loss: 1.050544261932373\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.049241542816162 | KNN Loss: 3.051278829574585 | BCE Loss: 0.9979626536369324\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.101816177368164 | KNN Loss: 3.0545825958251953 | BCE Loss: 1.0472337007522583\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.095152378082275 | KNN Loss: 3.062241554260254 | BCE Loss: 1.032910704612732\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.075629711151123 | KNN Loss: 3.080946683883667 | BCE Loss: 0.9946831464767456\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.027333736419678 | KNN Loss: 3.0256500244140625 | BCE Loss: 1.0016837120056152\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.076571464538574 | KNN Loss: 3.0595059394836426 | BCE Loss: 1.0170655250549316\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.0763325691223145 | KNN Loss: 3.0377109050750732 | BCE Loss: 1.0386215448379517\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.158730983734131 | KNN Loss: 3.1124074459075928 | BCE Loss: 1.046323537826538\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.060361862182617 | KNN Loss: 3.0348360538482666 | BCE Loss: 1.0255255699157715\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.0904541015625 | KNN Loss: 3.0757534503936768 | BCE Loss: 1.0147004127502441\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.05870246887207 | KNN Loss: 3.0478808879852295 | BCE Loss: 1.0108215808868408\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.120700836181641 | KNN Loss: 3.05595326423645 | BCE Loss: 1.0647475719451904\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.115058898925781 | KNN Loss: 3.0754599571228027 | BCE Loss: 1.0395987033843994\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.1304497718811035 | KNN Loss: 3.0974066257476807 | BCE Loss: 1.0330432653427124\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.0810747146606445 | KNN Loss: 3.050271987915039 | BCE Loss: 1.0308027267456055\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.116909503936768 | KNN Loss: 3.1133980751037598 | BCE Loss: 1.0035115480422974\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.046316146850586 | KNN Loss: 3.03975772857666 | BCE Loss: 1.0065581798553467\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.075380325317383 | KNN Loss: 3.0828464031219482 | BCE Loss: 0.9925337433815002\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.087149620056152 | KNN Loss: 3.069063663482666 | BCE Loss: 1.0180859565734863\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.1438307762146 | KNN Loss: 3.097382068634033 | BCE Loss: 1.046448826789856\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.137500286102295 | KNN Loss: 3.09194016456604 | BCE Loss: 1.0455601215362549\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.08940315246582 | KNN Loss: 3.076931953430176 | BCE Loss: 1.0124711990356445\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.0277276039123535 | KNN Loss: 3.0265517234802246 | BCE Loss: 1.001175880432129\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.070489883422852 | KNN Loss: 3.051971197128296 | BCE Loss: 1.0185189247131348\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.0536580085754395 | KNN Loss: 3.03047776222229 | BCE Loss: 1.023180365562439\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.115317344665527 | KNN Loss: 3.077554941177368 | BCE Loss: 1.03776216506958\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.098128318786621 | KNN Loss: 3.0871801376342773 | BCE Loss: 1.0109481811523438\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.088517189025879 | KNN Loss: 3.0513482093811035 | BCE Loss: 1.0371689796447754\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.089536190032959 | KNN Loss: 3.0801780223846436 | BCE Loss: 1.0093581676483154\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.1048784255981445 | KNN Loss: 3.088780403137207 | BCE Loss: 1.0160982608795166\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.0850911140441895 | KNN Loss: 3.0780959129333496 | BCE Loss: 1.0069952011108398\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.059604167938232 | KNN Loss: 3.049933671951294 | BCE Loss: 1.0096704959869385\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.061872482299805 | KNN Loss: 3.0359904766082764 | BCE Loss: 1.0258822441101074\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.06281852722168 | KNN Loss: 3.0400052070617676 | BCE Loss: 1.022813081741333\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.086025714874268 | KNN Loss: 3.053025722503662 | BCE Loss: 1.032999873161316\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.043436050415039 | KNN Loss: 3.0327603816986084 | BCE Loss: 1.0106756687164307\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.109750747680664 | KNN Loss: 3.0819950103759766 | BCE Loss: 1.0277559757232666\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.121852397918701 | KNN Loss: 3.098306655883789 | BCE Loss: 1.023545742034912\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.080422878265381 | KNN Loss: 3.0607168674468994 | BCE Loss: 1.019706130027771\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.091778755187988 | KNN Loss: 3.062973976135254 | BCE Loss: 1.0288050174713135\n",
      "Epoch   372: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 4.04775333404541 | KNN Loss: 3.0208053588867188 | BCE Loss: 1.0269482135772705\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.0998029708862305 | KNN Loss: 3.0738706588745117 | BCE Loss: 1.0259323120117188\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.089873313903809 | KNN Loss: 3.064145803451538 | BCE Loss: 1.025727391242981\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.102377891540527 | KNN Loss: 3.0744988918304443 | BCE Loss: 1.027879238128662\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.094472408294678 | KNN Loss: 3.06683611869812 | BCE Loss: 1.0276362895965576\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.082550048828125 | KNN Loss: 3.0699472427368164 | BCE Loss: 1.012602686882019\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.086284637451172 | KNN Loss: 3.0376546382904053 | BCE Loss: 1.0486302375793457\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.0917067527771 | KNN Loss: 3.0842702388763428 | BCE Loss: 1.0074363946914673\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.063283443450928 | KNN Loss: 3.048175573348999 | BCE Loss: 1.0151078701019287\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.143421649932861 | KNN Loss: 3.102008581161499 | BCE Loss: 1.0414130687713623\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.086759567260742 | KNN Loss: 3.0345213413238525 | BCE Loss: 1.0522382259368896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.09687614440918 | KNN Loss: 3.0753509998321533 | BCE Loss: 1.0215251445770264\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.116168022155762 | KNN Loss: 3.085193634033203 | BCE Loss: 1.0309746265411377\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.091175556182861 | KNN Loss: 3.0607194900512695 | BCE Loss: 1.0304559469223022\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.146644115447998 | KNN Loss: 3.1129567623138428 | BCE Loss: 1.0336874723434448\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.118609428405762 | KNN Loss: 3.0652854442596436 | BCE Loss: 1.0533239841461182\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.095663547515869 | KNN Loss: 3.04640531539917 | BCE Loss: 1.0492582321166992\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.099172592163086 | KNN Loss: 3.0700058937072754 | BCE Loss: 1.0291668176651\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.079672336578369 | KNN Loss: 3.048884630203247 | BCE Loss: 1.0307875871658325\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.072147369384766 | KNN Loss: 3.0417420864105225 | BCE Loss: 1.0304051637649536\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.078413963317871 | KNN Loss: 3.070404052734375 | BCE Loss: 1.0080097913742065\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.086906433105469 | KNN Loss: 3.044668674468994 | BCE Loss: 1.042237639427185\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.128786087036133 | KNN Loss: 3.123586893081665 | BCE Loss: 1.0051993131637573\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.066854953765869 | KNN Loss: 3.0550496578216553 | BCE Loss: 1.0118051767349243\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.0559163093566895 | KNN Loss: 3.038944959640503 | BCE Loss: 1.016971468925476\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.076834201812744 | KNN Loss: 3.0664243698120117 | BCE Loss: 1.0104098320007324\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.089885711669922 | KNN Loss: 3.074923276901245 | BCE Loss: 1.0149626731872559\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.074677467346191 | KNN Loss: 3.070486545562744 | BCE Loss: 1.0041910409927368\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.086849212646484 | KNN Loss: 3.0729849338531494 | BCE Loss: 1.0138641595840454\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.044985771179199 | KNN Loss: 3.043391466140747 | BCE Loss: 1.0015943050384521\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.0759077072143555 | KNN Loss: 3.066728115081787 | BCE Loss: 1.0091798305511475\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.105355262756348 | KNN Loss: 3.079245090484619 | BCE Loss: 1.0261104106903076\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.132566928863525 | KNN Loss: 3.088874101638794 | BCE Loss: 1.0436928272247314\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.070289611816406 | KNN Loss: 3.0576584339141846 | BCE Loss: 1.0126314163208008\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.079329490661621 | KNN Loss: 3.0380704402923584 | BCE Loss: 1.0412591695785522\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.049516677856445 | KNN Loss: 3.0505294799804688 | BCE Loss: 0.998987078666687\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.086478233337402 | KNN Loss: 3.062587261199951 | BCE Loss: 1.0238912105560303\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.083442211151123 | KNN Loss: 3.064188241958618 | BCE Loss: 1.0192539691925049\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.158595085144043 | KNN Loss: 3.1234006881713867 | BCE Loss: 1.0351946353912354\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.092598915100098 | KNN Loss: 3.0722761154174805 | BCE Loss: 1.0203230381011963\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.099151134490967 | KNN Loss: 3.073212146759033 | BCE Loss: 1.0259389877319336\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.134947776794434 | KNN Loss: 3.0870745182037354 | BCE Loss: 1.0478734970092773\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.074500560760498 | KNN Loss: 3.0388693809509277 | BCE Loss: 1.0356311798095703\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.1052165031433105 | KNN Loss: 3.0393402576446533 | BCE Loss: 1.0658762454986572\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.059844017028809 | KNN Loss: 3.0455729961395264 | BCE Loss: 1.0142707824707031\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.062374114990234 | KNN Loss: 3.0428388118743896 | BCE Loss: 1.0195354223251343\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.029429912567139 | KNN Loss: 3.025228261947632 | BCE Loss: 1.0042016506195068\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.072310924530029 | KNN Loss: 3.0627124309539795 | BCE Loss: 1.0095984935760498\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.074448108673096 | KNN Loss: 3.0364420413970947 | BCE Loss: 1.0380061864852905\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.1283278465271 | KNN Loss: 3.0937187671661377 | BCE Loss: 1.034609079360962\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.07716178894043 | KNN Loss: 3.0702366828918457 | BCE Loss: 1.0069248676300049\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.100676536560059 | KNN Loss: 3.0843026638031006 | BCE Loss: 1.016374111175537\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.0547685623168945 | KNN Loss: 3.0375936031341553 | BCE Loss: 1.0171751976013184\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.097468376159668 | KNN Loss: 3.0612471103668213 | BCE Loss: 1.0362210273742676\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.072318077087402 | KNN Loss: 3.064645767211914 | BCE Loss: 1.0076723098754883\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.067743301391602 | KNN Loss: 3.0760042667388916 | BCE Loss: 0.9917392730712891\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.116526126861572 | KNN Loss: 3.0890908241271973 | BCE Loss: 1.0274354219436646\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.074344635009766 | KNN Loss: 3.054058074951172 | BCE Loss: 1.0202863216400146\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.08120584487915 | KNN Loss: 3.055760383605957 | BCE Loss: 1.0254454612731934\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.095799922943115 | KNN Loss: 3.0597963333129883 | BCE Loss: 1.036003589630127\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.082533836364746 | KNN Loss: 3.0627801418304443 | BCE Loss: 1.0197534561157227\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.072434425354004 | KNN Loss: 3.059224843978882 | BCE Loss: 1.0132094621658325\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.065799713134766 | KNN Loss: 3.062130928039551 | BCE Loss: 1.003669023513794\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.079071044921875 | KNN Loss: 3.0453433990478516 | BCE Loss: 1.0337278842926025\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.106596946716309 | KNN Loss: 3.0463104248046875 | BCE Loss: 1.0602866411209106\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.102889060974121 | KNN Loss: 3.09022855758667 | BCE Loss: 1.012660264968872\n",
      "Epoch   383: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.1232218742370605 | KNN Loss: 3.0707859992980957 | BCE Loss: 1.0524358749389648\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.065757751464844 | KNN Loss: 3.045788049697876 | BCE Loss: 1.0199697017669678\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.11090087890625 | KNN Loss: 3.0717709064483643 | BCE Loss: 1.0391302108764648\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.098622798919678 | KNN Loss: 3.07646107673645 | BCE Loss: 1.0221617221832275\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.0674920082092285 | KNN Loss: 3.056333065032959 | BCE Loss: 1.01115882396698\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.117405891418457 | KNN Loss: 3.0975353717803955 | BCE Loss: 1.019870400428772\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.1077799797058105 | KNN Loss: 3.05448842048645 | BCE Loss: 1.05329167842865\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.0809006690979 | KNN Loss: 3.0638504028320312 | BCE Loss: 1.0170503854751587\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.046470642089844 | KNN Loss: 3.023524761199951 | BCE Loss: 1.0229461193084717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.083115100860596 | KNN Loss: 3.0481956005096436 | BCE Loss: 1.0349193811416626\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.042105674743652 | KNN Loss: 3.040719985961914 | BCE Loss: 1.0013854503631592\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.090600490570068 | KNN Loss: 3.064639091491699 | BCE Loss: 1.0259613990783691\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.065023422241211 | KNN Loss: 3.058389902114868 | BCE Loss: 1.0066334009170532\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.0854010581970215 | KNN Loss: 3.0810868740081787 | BCE Loss: 1.0043141841888428\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.038599491119385 | KNN Loss: 3.022996664047241 | BCE Loss: 1.0156028270721436\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.103513240814209 | KNN Loss: 3.096059560775757 | BCE Loss: 1.0074536800384521\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.053197860717773 | KNN Loss: 3.059389114379883 | BCE Loss: 0.9938089847564697\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.08709192276001 | KNN Loss: 3.034552574157715 | BCE Loss: 1.052539348602295\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.062719821929932 | KNN Loss: 3.0568089485168457 | BCE Loss: 1.0059107542037964\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.101480484008789 | KNN Loss: 3.078186511993408 | BCE Loss: 1.02329421043396\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.111696243286133 | KNN Loss: 3.0657474994659424 | BCE Loss: 1.0459487438201904\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.037740707397461 | KNN Loss: 3.028074264526367 | BCE Loss: 1.0096662044525146\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.045875549316406 | KNN Loss: 3.0357322692871094 | BCE Loss: 1.0101432800292969\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.0988078117370605 | KNN Loss: 3.078256845474243 | BCE Loss: 1.0205509662628174\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.0669660568237305 | KNN Loss: 3.051832437515259 | BCE Loss: 1.0151337385177612\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.071089744567871 | KNN Loss: 3.0377731323242188 | BCE Loss: 1.033316731452942\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.048280239105225 | KNN Loss: 3.0335512161254883 | BCE Loss: 1.0147290229797363\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.050781726837158 | KNN Loss: 3.0390126705169678 | BCE Loss: 1.0117689371109009\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.070789337158203 | KNN Loss: 3.0674846172332764 | BCE Loss: 1.0033048391342163\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.103518486022949 | KNN Loss: 3.068678617477417 | BCE Loss: 1.0348398685455322\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.05778169631958 | KNN Loss: 3.0485057830810547 | BCE Loss: 1.009276032447815\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.127288341522217 | KNN Loss: 3.094151020050049 | BCE Loss: 1.0331374406814575\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.1108269691467285 | KNN Loss: 3.0586652755737305 | BCE Loss: 1.052161693572998\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.0438618659973145 | KNN Loss: 3.0370211601257324 | BCE Loss: 1.0068405866622925\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.077919960021973 | KNN Loss: 3.0764999389648438 | BCE Loss: 1.0014197826385498\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.140630722045898 | KNN Loss: 3.1027474403381348 | BCE Loss: 1.0378830432891846\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.084779262542725 | KNN Loss: 3.0746653079986572 | BCE Loss: 1.010114073753357\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.090333461761475 | KNN Loss: 3.0555174350738525 | BCE Loss: 1.0348161458969116\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.122172832489014 | KNN Loss: 3.0708224773406982 | BCE Loss: 1.0513502359390259\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.087209701538086 | KNN Loss: 3.0514137744903564 | BCE Loss: 1.0357959270477295\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.084690093994141 | KNN Loss: 3.0816519260406494 | BCE Loss: 1.0030380487442017\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.071099758148193 | KNN Loss: 3.0584843158721924 | BCE Loss: 1.0126153230667114\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.058876991271973 | KNN Loss: 3.045229196548462 | BCE Loss: 1.0136479139328003\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.066627025604248 | KNN Loss: 3.0535852909088135 | BCE Loss: 1.0130417346954346\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.057860374450684 | KNN Loss: 3.0430266857147217 | BCE Loss: 1.014833927154541\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.083864212036133 | KNN Loss: 3.0594024658203125 | BCE Loss: 1.0244619846343994\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.128176689147949 | KNN Loss: 3.081153631210327 | BCE Loss: 1.0470231771469116\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.093952178955078 | KNN Loss: 3.071626663208008 | BCE Loss: 1.0223252773284912\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.062768936157227 | KNN Loss: 3.044248342514038 | BCE Loss: 1.0185208320617676\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.091811180114746 | KNN Loss: 3.052625894546509 | BCE Loss: 1.0391855239868164\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.073585510253906 | KNN Loss: 3.0543129444122314 | BCE Loss: 1.0192726850509644\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.100615501403809 | KNN Loss: 3.086186408996582 | BCE Loss: 1.0144290924072266\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.048501491546631 | KNN Loss: 3.015087604522705 | BCE Loss: 1.0334137678146362\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.104153156280518 | KNN Loss: 3.080734968185425 | BCE Loss: 1.0234183073043823\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.12261962890625 | KNN Loss: 3.075512409210205 | BCE Loss: 1.047107219696045\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.0430450439453125 | KNN Loss: 3.0386838912963867 | BCE Loss: 1.0043609142303467\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.027104377746582 | KNN Loss: 3.0213801860809326 | BCE Loss: 1.005724310874939\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.06949520111084 | KNN Loss: 3.0535082817077637 | BCE Loss: 1.0159869194030762\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.071212291717529 | KNN Loss: 3.0755200386047363 | BCE Loss: 0.9956921339035034\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.115525245666504 | KNN Loss: 3.076228618621826 | BCE Loss: 1.0392963886260986\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.1279072761535645 | KNN Loss: 3.0861451625823975 | BCE Loss: 1.041762113571167\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.062027931213379 | KNN Loss: 3.0415992736816406 | BCE Loss: 1.0204287767410278\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.070533752441406 | KNN Loss: 3.0789194107055664 | BCE Loss: 0.9916142225265503\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.103978157043457 | KNN Loss: 3.0597572326660156 | BCE Loss: 1.0442206859588623\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.082176208496094 | KNN Loss: 3.061513662338257 | BCE Loss: 1.0206623077392578\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.07766580581665 | KNN Loss: 3.0435690879821777 | BCE Loss: 1.0340967178344727\n",
      "Epoch   394: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.048679351806641 | KNN Loss: 3.043785810470581 | BCE Loss: 1.00489342212677\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.0781145095825195 | KNN Loss: 3.0667808055877686 | BCE Loss: 1.01133394241333\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.078448295593262 | KNN Loss: 3.0641589164733887 | BCE Loss: 1.014289379119873\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.110328197479248 | KNN Loss: 3.0737557411193848 | BCE Loss: 1.0365723371505737\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.052568435668945 | KNN Loss: 3.0524208545684814 | BCE Loss: 1.0001473426818848\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.086499214172363 | KNN Loss: 3.070167303085327 | BCE Loss: 1.016331672668457\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.061022758483887 | KNN Loss: 3.056901216506958 | BCE Loss: 1.0041213035583496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.1250224113464355 | KNN Loss: 3.077732801437378 | BCE Loss: 1.0472896099090576\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.070103645324707 | KNN Loss: 3.035731554031372 | BCE Loss: 1.0343722105026245\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.046183109283447 | KNN Loss: 3.0474846363067627 | BCE Loss: 0.9986982941627502\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.07952356338501 | KNN Loss: 3.084867000579834 | BCE Loss: 0.9946564435958862\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.190280914306641 | KNN Loss: 3.127986431121826 | BCE Loss: 1.0622942447662354\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.0727972984313965 | KNN Loss: 3.0358119010925293 | BCE Loss: 1.0369852781295776\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.095860481262207 | KNN Loss: 3.0900018215179443 | BCE Loss: 1.0058584213256836\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.092868804931641 | KNN Loss: 3.0713047981262207 | BCE Loss: 1.0215638875961304\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.021114349365234 | KNN Loss: 3.014915943145752 | BCE Loss: 1.006198525428772\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.080754280090332 | KNN Loss: 3.0560309886932373 | BCE Loss: 1.0247230529785156\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.060834884643555 | KNN Loss: 3.0263967514038086 | BCE Loss: 1.034437894821167\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.05789041519165 | KNN Loss: 3.045063018798828 | BCE Loss: 1.0128275156021118\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.073284149169922 | KNN Loss: 3.048281669616699 | BCE Loss: 1.0250025987625122\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.027388572692871 | KNN Loss: 3.035104990005493 | BCE Loss: 0.992283821105957\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.0895538330078125 | KNN Loss: 3.067293643951416 | BCE Loss: 1.0222599506378174\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.102743148803711 | KNN Loss: 3.060479164123535 | BCE Loss: 1.0422639846801758\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.050877094268799 | KNN Loss: 3.022482395172119 | BCE Loss: 1.0283945798873901\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.126699924468994 | KNN Loss: 3.0927951335906982 | BCE Loss: 1.0339046716690063\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.111409664154053 | KNN Loss: 3.081458330154419 | BCE Loss: 1.0299513339996338\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.100578308105469 | KNN Loss: 3.085134506225586 | BCE Loss: 1.015444040298462\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.145713806152344 | KNN Loss: 3.1198668479919434 | BCE Loss: 1.0258468389511108\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.057751178741455 | KNN Loss: 3.048661708831787 | BCE Loss: 1.009089469909668\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.065120697021484 | KNN Loss: 3.0556347370147705 | BCE Loss: 1.0094858407974243\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.1089982986450195 | KNN Loss: 3.089287281036377 | BCE Loss: 1.019710898399353\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.101446151733398 | KNN Loss: 3.0697686672210693 | BCE Loss: 1.03167724609375\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.0636467933654785 | KNN Loss: 3.064771890640259 | BCE Loss: 0.9988749027252197\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.066717147827148 | KNN Loss: 3.054619073867798 | BCE Loss: 1.0120978355407715\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.112185955047607 | KNN Loss: 3.0934536457061768 | BCE Loss: 1.0187324285507202\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.063485145568848 | KNN Loss: 3.0539965629577637 | BCE Loss: 1.009488582611084\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.080383777618408 | KNN Loss: 3.050433874130249 | BCE Loss: 1.0299499034881592\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.062528610229492 | KNN Loss: 3.0582387447357178 | BCE Loss: 1.0042896270751953\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.095813274383545 | KNN Loss: 3.071295976638794 | BCE Loss: 1.024517297744751\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.075287818908691 | KNN Loss: 3.0523159503936768 | BCE Loss: 1.0229716300964355\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.073657035827637 | KNN Loss: 3.041198492050171 | BCE Loss: 1.0324585437774658\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.097162246704102 | KNN Loss: 3.0707271099090576 | BCE Loss: 1.026435375213623\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.078866004943848 | KNN Loss: 3.0507853031158447 | BCE Loss: 1.0280808210372925\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.062170505523682 | KNN Loss: 3.062403917312622 | BCE Loss: 0.9997665882110596\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.057668209075928 | KNN Loss: 3.0654451847076416 | BCE Loss: 0.9922228455543518\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.0875043869018555 | KNN Loss: 3.0571937561035156 | BCE Loss: 1.0303107500076294\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.030488967895508 | KNN Loss: 3.0193231105804443 | BCE Loss: 1.0111656188964844\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.088294982910156 | KNN Loss: 3.060990571975708 | BCE Loss: 1.0273046493530273\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.125295162200928 | KNN Loss: 3.0956218242645264 | BCE Loss: 1.0296733379364014\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.129374980926514 | KNN Loss: 3.0789401531219482 | BCE Loss: 1.050434947013855\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.0409417152404785 | KNN Loss: 3.0333571434020996 | BCE Loss: 1.007584571838379\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.123839378356934 | KNN Loss: 3.0866026878356934 | BCE Loss: 1.0372364521026611\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.061439514160156 | KNN Loss: 3.047780990600586 | BCE Loss: 1.0136582851409912\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.094146728515625 | KNN Loss: 3.0524163246154785 | BCE Loss: 1.0417301654815674\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.107004642486572 | KNN Loss: 3.0718765258789062 | BCE Loss: 1.035128116607666\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.114164352416992 | KNN Loss: 3.082937479019165 | BCE Loss: 1.0312269926071167\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.064135551452637 | KNN Loss: 3.072382688522339 | BCE Loss: 0.9917529821395874\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.1462836265563965 | KNN Loss: 3.0993714332580566 | BCE Loss: 1.0469121932983398\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.064216136932373 | KNN Loss: 3.0527400970458984 | BCE Loss: 1.0114760398864746\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.083189964294434 | KNN Loss: 3.051060438156128 | BCE Loss: 1.0321297645568848\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.086522102355957 | KNN Loss: 3.0740914344787598 | BCE Loss: 1.0124304294586182\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.052250385284424 | KNN Loss: 3.029158592224121 | BCE Loss: 1.0230916738510132\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.145369529724121 | KNN Loss: 3.1230990886688232 | BCE Loss: 1.0222702026367188\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.068420886993408 | KNN Loss: 3.074711799621582 | BCE Loss: 0.9937092065811157\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.091449737548828 | KNN Loss: 3.0829639434814453 | BCE Loss: 1.0084855556488037\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.087270736694336 | KNN Loss: 3.0865554809570312 | BCE Loss: 1.0007152557373047\n",
      "Epoch   405: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.063863277435303 | KNN Loss: 3.0446054935455322 | BCE Loss: 1.019257664680481\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.058319091796875 | KNN Loss: 3.0256917476654053 | BCE Loss: 1.0326271057128906\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.085413932800293 | KNN Loss: 3.0468902587890625 | BCE Loss: 1.03852379322052\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.119929313659668 | KNN Loss: 3.1009440422058105 | BCE Loss: 1.018985390663147\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.082334995269775 | KNN Loss: 3.0486133098602295 | BCE Loss: 1.0337215662002563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.05124568939209 | KNN Loss: 3.045185089111328 | BCE Loss: 1.0060608386993408\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.094038009643555 | KNN Loss: 3.076324701309204 | BCE Loss: 1.0177130699157715\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.091113090515137 | KNN Loss: 3.074333429336548 | BCE Loss: 1.0167794227600098\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.097942352294922 | KNN Loss: 3.0645244121551514 | BCE Loss: 1.0334177017211914\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.1038737297058105 | KNN Loss: 3.08347487449646 | BCE Loss: 1.0203988552093506\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.077759742736816 | KNN Loss: 3.062635898590088 | BCE Loss: 1.0151240825653076\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.086764335632324 | KNN Loss: 3.087003707885742 | BCE Loss: 0.999760627746582\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.107909202575684 | KNN Loss: 3.086360216140747 | BCE Loss: 1.0215487480163574\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.036627292633057 | KNN Loss: 3.040173292160034 | BCE Loss: 0.9964540004730225\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.139560699462891 | KNN Loss: 3.112264633178711 | BCE Loss: 1.0272958278656006\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.101869583129883 | KNN Loss: 3.0610196590423584 | BCE Loss: 1.0408498048782349\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.113483428955078 | KNN Loss: 3.0926337242126465 | BCE Loss: 1.0208494663238525\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.077337265014648 | KNN Loss: 3.0525622367858887 | BCE Loss: 1.0247752666473389\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.151636600494385 | KNN Loss: 3.0960147380828857 | BCE Loss: 1.0556219816207886\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.087701320648193 | KNN Loss: 3.092834711074829 | BCE Loss: 0.9948667883872986\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.122353553771973 | KNN Loss: 3.0914700031280518 | BCE Loss: 1.0308833122253418\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.073485374450684 | KNN Loss: 3.0569400787353516 | BCE Loss: 1.0165451765060425\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.075925827026367 | KNN Loss: 3.048102855682373 | BCE Loss: 1.0278228521347046\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.094559669494629 | KNN Loss: 3.0646653175354004 | BCE Loss: 1.0298941135406494\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.09733772277832 | KNN Loss: 3.0683319568634033 | BCE Loss: 1.029005527496338\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.085569858551025 | KNN Loss: 3.050719976425171 | BCE Loss: 1.0348498821258545\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.074515342712402 | KNN Loss: 3.0694897174835205 | BCE Loss: 1.0050257444381714\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.062948703765869 | KNN Loss: 3.053898572921753 | BCE Loss: 1.0090502500534058\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.139100074768066 | KNN Loss: 3.099609613418579 | BCE Loss: 1.0394906997680664\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.067736625671387 | KNN Loss: 3.0597469806671143 | BCE Loss: 1.007989764213562\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.064953327178955 | KNN Loss: 3.061062812805176 | BCE Loss: 1.0038905143737793\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.07615852355957 | KNN Loss: 3.0624184608459473 | BCE Loss: 1.0137401819229126\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.034275054931641 | KNN Loss: 3.0298171043395996 | BCE Loss: 1.004457950592041\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.100704193115234 | KNN Loss: 3.068416118621826 | BCE Loss: 1.0322881937026978\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.048606872558594 | KNN Loss: 3.0593557357788086 | BCE Loss: 0.9892511367797852\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.086112976074219 | KNN Loss: 3.072674036026001 | BCE Loss: 1.0134389400482178\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.073897361755371 | KNN Loss: 3.056718349456787 | BCE Loss: 1.0171787738800049\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.085671424865723 | KNN Loss: 3.0588796138763428 | BCE Loss: 1.0267915725708008\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.10551643371582 | KNN Loss: 3.07515549659729 | BCE Loss: 1.0303609371185303\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.066895961761475 | KNN Loss: 3.0286178588867188 | BCE Loss: 1.0382782220840454\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.137337684631348 | KNN Loss: 3.0862081050872803 | BCE Loss: 1.0511295795440674\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.042295932769775 | KNN Loss: 3.0345163345336914 | BCE Loss: 1.0077794790267944\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.080666542053223 | KNN Loss: 3.0561680793762207 | BCE Loss: 1.0244982242584229\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.044349670410156 | KNN Loss: 3.0537109375 | BCE Loss: 0.9906389713287354\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.043613910675049 | KNN Loss: 3.0498359203338623 | BCE Loss: 0.993777871131897\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.069859504699707 | KNN Loss: 3.0560452938079834 | BCE Loss: 1.0138139724731445\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.1356377601623535 | KNN Loss: 3.0822136402130127 | BCE Loss: 1.0534240007400513\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.052114486694336 | KNN Loss: 3.041614294052124 | BCE Loss: 1.0104999542236328\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.048120975494385 | KNN Loss: 3.0305604934692383 | BCE Loss: 1.017560601234436\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.059640884399414 | KNN Loss: 3.0527760982513428 | BCE Loss: 1.0068650245666504\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.141058921813965 | KNN Loss: 3.100221872329712 | BCE Loss: 1.040837049484253\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.0648393630981445 | KNN Loss: 3.0508100986480713 | BCE Loss: 1.0140292644500732\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.0570454597473145 | KNN Loss: 3.04315185546875 | BCE Loss: 1.0138936042785645\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.159221172332764 | KNN Loss: 3.093315839767456 | BCE Loss: 1.0659053325653076\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.146426200866699 | KNN Loss: 3.1043543815612793 | BCE Loss: 1.0420717000961304\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.0887861251831055 | KNN Loss: 3.0726816654205322 | BCE Loss: 1.0161046981811523\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.0895161628723145 | KNN Loss: 3.0250396728515625 | BCE Loss: 1.064476490020752\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.019106388092041 | KNN Loss: 3.0323026180267334 | BCE Loss: 0.9868038892745972\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.0658979415893555 | KNN Loss: 3.0494818687438965 | BCE Loss: 1.0164159536361694\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.086954116821289 | KNN Loss: 3.0606396198272705 | BCE Loss: 1.0263144969940186\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.0902533531188965 | KNN Loss: 3.0633389949798584 | BCE Loss: 1.026914358139038\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.125046253204346 | KNN Loss: 3.101600170135498 | BCE Loss: 1.023445963859558\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.141584396362305 | KNN Loss: 3.086928367614746 | BCE Loss: 1.0546562671661377\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.095646858215332 | KNN Loss: 3.0715198516845703 | BCE Loss: 1.0241271257400513\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.092793941497803 | KNN Loss: 3.066835641860962 | BCE Loss: 1.0259582996368408\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.057584762573242 | KNN Loss: 3.0489444732666016 | BCE Loss: 1.0086405277252197\n",
      "Epoch   416: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.065856456756592 | KNN Loss: 3.0796666145324707 | BCE Loss: 0.9861900210380554\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.099872589111328 | KNN Loss: 3.0714340209960938 | BCE Loss: 1.028438687324524\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.04322624206543 | KNN Loss: 3.0279338359832764 | BCE Loss: 1.0152921676635742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.087550163269043 | KNN Loss: 3.0498249530792236 | BCE Loss: 1.0377252101898193\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.081151008605957 | KNN Loss: 3.032632827758789 | BCE Loss: 1.0485179424285889\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.088641166687012 | KNN Loss: 3.0709195137023926 | BCE Loss: 1.01772141456604\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.086776256561279 | KNN Loss: 3.0423665046691895 | BCE Loss: 1.0444097518920898\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.097029685974121 | KNN Loss: 3.052994728088379 | BCE Loss: 1.044034719467163\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.042773246765137 | KNN Loss: 3.0644874572753906 | BCE Loss: 0.978285551071167\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.114668846130371 | KNN Loss: 3.0660581588745117 | BCE Loss: 1.0486106872558594\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.092311859130859 | KNN Loss: 3.0503692626953125 | BCE Loss: 1.0419425964355469\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.057292461395264 | KNN Loss: 3.0444066524505615 | BCE Loss: 1.0128858089447021\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.07097053527832 | KNN Loss: 3.042440176010132 | BCE Loss: 1.0285301208496094\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.104823112487793 | KNN Loss: 3.072585344314575 | BCE Loss: 1.0322380065917969\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.079624652862549 | KNN Loss: 3.079767942428589 | BCE Loss: 0.9998565912246704\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.073029041290283 | KNN Loss: 3.0405962467193604 | BCE Loss: 1.0324329137802124\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.07926082611084 | KNN Loss: 3.0783028602600098 | BCE Loss: 1.000957727432251\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.047710418701172 | KNN Loss: 3.0365097522735596 | BCE Loss: 1.0112005472183228\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.089410781860352 | KNN Loss: 3.0680830478668213 | BCE Loss: 1.0213276147842407\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.065896987915039 | KNN Loss: 3.052421808242798 | BCE Loss: 1.013474941253662\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.061938285827637 | KNN Loss: 3.0591135025024414 | BCE Loss: 1.0028250217437744\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.066080570220947 | KNN Loss: 3.065523147583008 | BCE Loss: 1.0005574226379395\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.095836639404297 | KNN Loss: 3.065469264984131 | BCE Loss: 1.030367374420166\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.118710041046143 | KNN Loss: 3.0970962047576904 | BCE Loss: 1.0216139554977417\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.120180606842041 | KNN Loss: 3.092806816101074 | BCE Loss: 1.0273739099502563\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.048321723937988 | KNN Loss: 3.0476393699645996 | BCE Loss: 1.0006822347640991\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.125307083129883 | KNN Loss: 3.090758800506592 | BCE Loss: 1.034548044204712\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.0836358070373535 | KNN Loss: 3.0375850200653076 | BCE Loss: 1.0460509061813354\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.0509867668151855 | KNN Loss: 3.045041084289551 | BCE Loss: 1.0059456825256348\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.049709320068359 | KNN Loss: 3.0580201148986816 | BCE Loss: 0.9916890859603882\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.124225616455078 | KNN Loss: 3.07914662361145 | BCE Loss: 1.045079231262207\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.107488632202148 | KNN Loss: 3.0868053436279297 | BCE Loss: 1.0206831693649292\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.069605350494385 | KNN Loss: 3.083650588989258 | BCE Loss: 0.985954761505127\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.072834014892578 | KNN Loss: 3.0708999633789062 | BCE Loss: 1.001934289932251\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.089959144592285 | KNN Loss: 3.0787885189056396 | BCE Loss: 1.0111703872680664\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.052855014801025 | KNN Loss: 3.0476763248443604 | BCE Loss: 1.005178689956665\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.027543067932129 | KNN Loss: 3.0364139080047607 | BCE Loss: 0.9911293983459473\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.108890533447266 | KNN Loss: 3.064560890197754 | BCE Loss: 1.0443295240402222\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.071457862854004 | KNN Loss: 3.042088747024536 | BCE Loss: 1.0293692350387573\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.13270902633667 | KNN Loss: 3.092290163040161 | BCE Loss: 1.0404187440872192\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.075162887573242 | KNN Loss: 3.054922342300415 | BCE Loss: 1.0202407836914062\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.082010269165039 | KNN Loss: 3.032834768295288 | BCE Loss: 1.04917573928833\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.089547157287598 | KNN Loss: 3.072683811187744 | BCE Loss: 1.0168631076812744\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.085848808288574 | KNN Loss: 3.0412375926971436 | BCE Loss: 1.0446114540100098\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.089232444763184 | KNN Loss: 3.0662331581115723 | BCE Loss: 1.0229991674423218\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.081765174865723 | KNN Loss: 3.0517213344573975 | BCE Loss: 1.0300437211990356\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.031602382659912 | KNN Loss: 3.042754888534546 | BCE Loss: 0.9888473749160767\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.033022403717041 | KNN Loss: 3.0240323543548584 | BCE Loss: 1.008989930152893\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.103022575378418 | KNN Loss: 3.0807228088378906 | BCE Loss: 1.022299885749817\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.046034812927246 | KNN Loss: 3.0466830730438232 | BCE Loss: 0.9993519186973572\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.149761199951172 | KNN Loss: 3.0998618602752686 | BCE Loss: 1.0498991012573242\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.085683345794678 | KNN Loss: 3.0894083976745605 | BCE Loss: 0.9962748289108276\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.07490348815918 | KNN Loss: 3.0610945224761963 | BCE Loss: 1.0138089656829834\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.0504937171936035 | KNN Loss: 3.037410259246826 | BCE Loss: 1.0130833387374878\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.059589385986328 | KNN Loss: 3.0509374141693115 | BCE Loss: 1.0086520910263062\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.099628448486328 | KNN Loss: 3.1053459644317627 | BCE Loss: 0.9942823648452759\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.089937686920166 | KNN Loss: 3.09417724609375 | BCE Loss: 0.9957603216171265\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.057264804840088 | KNN Loss: 3.0599191188812256 | BCE Loss: 0.9973456859588623\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.120187282562256 | KNN Loss: 3.0852482318878174 | BCE Loss: 1.0349390506744385\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 3.9992854595184326 | KNN Loss: 3.0104494094848633 | BCE Loss: 0.9888360500335693\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.0547590255737305 | KNN Loss: 3.0378494262695312 | BCE Loss: 1.0169097185134888\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.047262191772461 | KNN Loss: 3.0285425186157227 | BCE Loss: 1.0187196731567383\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.071028709411621 | KNN Loss: 3.0435073375701904 | BCE Loss: 1.0275211334228516\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.08497428894043 | KNN Loss: 3.073704957962036 | BCE Loss: 1.0112690925598145\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.099143981933594 | KNN Loss: 3.078000545501709 | BCE Loss: 1.0211434364318848\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.1323089599609375 | KNN Loss: 3.0997979640960693 | BCE Loss: 1.032510757446289\n",
      "Epoch   427: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.089317321777344 | KNN Loss: 3.0722286701202393 | BCE Loss: 1.017088532447815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.094338417053223 | KNN Loss: 3.074815273284912 | BCE Loss: 1.0195233821868896\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.074184417724609 | KNN Loss: 3.061771869659424 | BCE Loss: 1.0124123096466064\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.060611724853516 | KNN Loss: 3.047820806503296 | BCE Loss: 1.0127910375595093\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.1168694496154785 | KNN Loss: 3.057279348373413 | BCE Loss: 1.0595901012420654\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.040396690368652 | KNN Loss: 3.023773670196533 | BCE Loss: 1.01662278175354\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.0471086502075195 | KNN Loss: 3.0114212036132812 | BCE Loss: 1.0356872081756592\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.086655616760254 | KNN Loss: 3.0692930221557617 | BCE Loss: 1.017362356185913\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.0512285232543945 | KNN Loss: 3.0401103496551514 | BCE Loss: 1.0111181735992432\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.100071430206299 | KNN Loss: 3.058641195297241 | BCE Loss: 1.0414303541183472\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.116621971130371 | KNN Loss: 3.095957040786743 | BCE Loss: 1.020665168762207\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.067683696746826 | KNN Loss: 3.0509581565856934 | BCE Loss: 1.0167254209518433\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.052999496459961 | KNN Loss: 3.0206449031829834 | BCE Loss: 1.0323543548583984\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.0745086669921875 | KNN Loss: 3.0742604732513428 | BCE Loss: 1.0002481937408447\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.0552167892456055 | KNN Loss: 3.0695512294769287 | BCE Loss: 0.9856653809547424\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.084995269775391 | KNN Loss: 3.068645715713501 | BCE Loss: 1.0163495540618896\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.053957939147949 | KNN Loss: 3.050642728805542 | BCE Loss: 1.0033152103424072\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.038472652435303 | KNN Loss: 3.0180978775024414 | BCE Loss: 1.0203747749328613\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.118756294250488 | KNN Loss: 3.0872790813446045 | BCE Loss: 1.0314769744873047\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.074247360229492 | KNN Loss: 3.07580304145813 | BCE Loss: 0.9984444975852966\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.094961643218994 | KNN Loss: 3.053767204284668 | BCE Loss: 1.0411943197250366\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.0897321701049805 | KNN Loss: 3.035245895385742 | BCE Loss: 1.0544863939285278\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.025915145874023 | KNN Loss: 3.031559467315674 | BCE Loss: 0.9943555593490601\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.077511310577393 | KNN Loss: 3.0637385845184326 | BCE Loss: 1.0137726068496704\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.080387592315674 | KNN Loss: 3.045952081680298 | BCE Loss: 1.034435510635376\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.11911678314209 | KNN Loss: 3.0822980403900146 | BCE Loss: 1.0368189811706543\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.1100311279296875 | KNN Loss: 3.076450824737549 | BCE Loss: 1.0335800647735596\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.107564926147461 | KNN Loss: 3.0718979835510254 | BCE Loss: 1.0356671810150146\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.080510139465332 | KNN Loss: 3.067984104156494 | BCE Loss: 1.0125257968902588\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.060367584228516 | KNN Loss: 3.0412912368774414 | BCE Loss: 1.0190763473510742\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.080590724945068 | KNN Loss: 3.0413718223571777 | BCE Loss: 1.0392189025878906\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.072657108306885 | KNN Loss: 3.068956136703491 | BCE Loss: 1.003701090812683\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.087404251098633 | KNN Loss: 3.0519919395446777 | BCE Loss: 1.0354125499725342\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.065072536468506 | KNN Loss: 3.054565668106079 | BCE Loss: 1.0105067491531372\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.10213565826416 | KNN Loss: 3.091066837310791 | BCE Loss: 1.0110687017440796\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.136926651000977 | KNN Loss: 3.066885471343994 | BCE Loss: 1.0700411796569824\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.037282466888428 | KNN Loss: 3.041538715362549 | BCE Loss: 0.9957437515258789\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.166731357574463 | KNN Loss: 3.093219041824341 | BCE Loss: 1.073512315750122\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.057645320892334 | KNN Loss: 3.0485966205596924 | BCE Loss: 1.009048581123352\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.076666831970215 | KNN Loss: 3.0569560527801514 | BCE Loss: 1.019710898399353\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.084888935089111 | KNN Loss: 3.0837881565093994 | BCE Loss: 1.0011008977890015\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.099231243133545 | KNN Loss: 3.0901997089385986 | BCE Loss: 1.0090315341949463\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.085003852844238 | KNN Loss: 3.0654895305633545 | BCE Loss: 1.0195144414901733\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 4.096489906311035 | KNN Loss: 3.056373119354248 | BCE Loss: 1.040116548538208\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.078399181365967 | KNN Loss: 3.0696918964385986 | BCE Loss: 1.0087072849273682\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.133494853973389 | KNN Loss: 3.097822666168213 | BCE Loss: 1.0356721878051758\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.094723701477051 | KNN Loss: 3.089111089706421 | BCE Loss: 1.005612850189209\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.060163497924805 | KNN Loss: 3.0579400062561035 | BCE Loss: 1.0022237300872803\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.092658042907715 | KNN Loss: 3.055119037628174 | BCE Loss: 1.0375392436981201\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.058890342712402 | KNN Loss: 3.0411834716796875 | BCE Loss: 1.0177066326141357\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.057153224945068 | KNN Loss: 3.037010669708252 | BCE Loss: 1.020142674446106\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.082855224609375 | KNN Loss: 3.0551137924194336 | BCE Loss: 1.0277416706085205\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.104783058166504 | KNN Loss: 3.0753519535064697 | BCE Loss: 1.0294313430786133\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.108838081359863 | KNN Loss: 3.106497287750244 | BCE Loss: 1.0023409128189087\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.101836681365967 | KNN Loss: 3.057317018508911 | BCE Loss: 1.0445197820663452\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.046014308929443 | KNN Loss: 3.0589394569396973 | BCE Loss: 0.9870749115943909\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.144019603729248 | KNN Loss: 3.1103663444519043 | BCE Loss: 1.0336531400680542\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.049012184143066 | KNN Loss: 3.0263800621032715 | BCE Loss: 1.022632122039795\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.097105979919434 | KNN Loss: 3.059154510498047 | BCE Loss: 1.0379517078399658\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.0594916343688965 | KNN Loss: 3.061192512512207 | BCE Loss: 0.9982991814613342\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.10891056060791 | KNN Loss: 3.08461594581604 | BCE Loss: 1.0242947340011597\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.049404144287109 | KNN Loss: 3.034677028656006 | BCE Loss: 1.0147271156311035\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.128885269165039 | KNN Loss: 3.081545829772949 | BCE Loss: 1.047339677810669\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.057261943817139 | KNN Loss: 3.029625654220581 | BCE Loss: 1.0276362895965576\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.06755256652832 | KNN Loss: 3.0699212551116943 | BCE Loss: 0.997631311416626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.081905364990234 | KNN Loss: 3.057138442993164 | BCE Loss: 1.0247671604156494\n",
      "Epoch   438: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.101387977600098 | KNN Loss: 3.0439934730529785 | BCE Loss: 1.05739426612854\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.114784240722656 | KNN Loss: 3.0964395999908447 | BCE Loss: 1.0183446407318115\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.092372417449951 | KNN Loss: 3.0818772315979004 | BCE Loss: 1.0104953050613403\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.0650739669799805 | KNN Loss: 3.054426908493042 | BCE Loss: 1.0106472969055176\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.1188859939575195 | KNN Loss: 3.0869786739349365 | BCE Loss: 1.031907558441162\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.107195854187012 | KNN Loss: 3.0797200202941895 | BCE Loss: 1.0274758338928223\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.095781326293945 | KNN Loss: 3.088127374649048 | BCE Loss: 1.0076537132263184\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.06861686706543 | KNN Loss: 3.050088882446289 | BCE Loss: 1.0185279846191406\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.074004173278809 | KNN Loss: 3.079895496368408 | BCE Loss: 0.9941085577011108\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.095032691955566 | KNN Loss: 3.062840700149536 | BCE Loss: 1.0321917533874512\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.1001691818237305 | KNN Loss: 3.05519700050354 | BCE Loss: 1.0449724197387695\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.0557427406311035 | KNN Loss: 3.043210506439209 | BCE Loss: 1.012532114982605\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.1090545654296875 | KNN Loss: 3.103850841522217 | BCE Loss: 1.0052037239074707\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.08436918258667 | KNN Loss: 3.067538022994995 | BCE Loss: 1.0168311595916748\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.136715412139893 | KNN Loss: 3.0688798427581787 | BCE Loss: 1.0678354501724243\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.034573078155518 | KNN Loss: 3.0331923961639404 | BCE Loss: 1.0013805627822876\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.072175025939941 | KNN Loss: 3.0577423572540283 | BCE Loss: 1.014432668685913\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.108129501342773 | KNN Loss: 3.08072829246521 | BCE Loss: 1.027401089668274\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.081918239593506 | KNN Loss: 3.082982063293457 | BCE Loss: 0.9989362955093384\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.120095252990723 | KNN Loss: 3.092348098754883 | BCE Loss: 1.027747392654419\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.077332973480225 | KNN Loss: 3.068631887435913 | BCE Loss: 1.0087010860443115\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.086695671081543 | KNN Loss: 3.0476486682891846 | BCE Loss: 1.0390472412109375\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.110208511352539 | KNN Loss: 3.091085433959961 | BCE Loss: 1.019122838973999\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.104428291320801 | KNN Loss: 3.0856473445892334 | BCE Loss: 1.0187807083129883\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.067303657531738 | KNN Loss: 3.044738531112671 | BCE Loss: 1.0225653648376465\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.052221298217773 | KNN Loss: 3.0201802253723145 | BCE Loss: 1.032041072845459\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.105230808258057 | KNN Loss: 3.0882861614227295 | BCE Loss: 1.0169446468353271\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.133813858032227 | KNN Loss: 3.107816696166992 | BCE Loss: 1.025997281074524\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.158282279968262 | KNN Loss: 3.0987162590026855 | BCE Loss: 1.0595661401748657\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.085040092468262 | KNN Loss: 3.053765296936035 | BCE Loss: 1.0312745571136475\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.0880842208862305 | KNN Loss: 3.0516510009765625 | BCE Loss: 1.036433458328247\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.0711445808410645 | KNN Loss: 3.0499932765960693 | BCE Loss: 1.0211513042449951\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.117579460144043 | KNN Loss: 3.087261199951172 | BCE Loss: 1.030318260192871\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.094381332397461 | KNN Loss: 3.075648307800293 | BCE Loss: 1.0187327861785889\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.095738410949707 | KNN Loss: 3.093419313430786 | BCE Loss: 1.002319097518921\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.0990729331970215 | KNN Loss: 3.077350616455078 | BCE Loss: 1.0217223167419434\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.07077169418335 | KNN Loss: 3.0485386848449707 | BCE Loss: 1.022233009338379\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.080682754516602 | KNN Loss: 3.0379719734191895 | BCE Loss: 1.0427110195159912\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.066102027893066 | KNN Loss: 3.059093952178955 | BCE Loss: 1.0070083141326904\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.083117961883545 | KNN Loss: 3.056896209716797 | BCE Loss: 1.0262216329574585\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.059726715087891 | KNN Loss: 3.0168068408966064 | BCE Loss: 1.0429197549819946\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.145068645477295 | KNN Loss: 3.1059820652008057 | BCE Loss: 1.0390864610671997\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.081955909729004 | KNN Loss: 3.0594563484191895 | BCE Loss: 1.0224995613098145\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.094839572906494 | KNN Loss: 3.087010383605957 | BCE Loss: 1.0078290700912476\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.075752258300781 | KNN Loss: 3.079943895339966 | BCE Loss: 0.9958081245422363\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.129167079925537 | KNN Loss: 3.087122678756714 | BCE Loss: 1.0420444011688232\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.099983215332031 | KNN Loss: 3.060133218765259 | BCE Loss: 1.0398502349853516\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.147624492645264 | KNN Loss: 3.1272261142730713 | BCE Loss: 1.0203983783721924\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.059216499328613 | KNN Loss: 3.0532376766204834 | BCE Loss: 1.0059788227081299\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.130419731140137 | KNN Loss: 3.1063730716705322 | BCE Loss: 1.0240464210510254\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.047452926635742 | KNN Loss: 3.062591552734375 | BCE Loss: 0.9848616123199463\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.082793235778809 | KNN Loss: 3.0924477577209473 | BCE Loss: 0.9903453588485718\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.077191352844238 | KNN Loss: 3.0585780143737793 | BCE Loss: 1.0186131000518799\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.104750156402588 | KNN Loss: 3.0856757164001465 | BCE Loss: 1.019074559211731\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.067849159240723 | KNN Loss: 3.055894613265991 | BCE Loss: 1.0119543075561523\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.103933811187744 | KNN Loss: 3.0526435375213623 | BCE Loss: 1.0512902736663818\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.067933082580566 | KNN Loss: 3.035874366760254 | BCE Loss: 1.0320587158203125\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.050655841827393 | KNN Loss: 3.0573978424072266 | BCE Loss: 0.9932578206062317\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.077581405639648 | KNN Loss: 3.0577950477600098 | BCE Loss: 1.0197864770889282\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.068205833435059 | KNN Loss: 3.0555155277252197 | BCE Loss: 1.0126901865005493\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.090700149536133 | KNN Loss: 3.0628583431243896 | BCE Loss: 1.0278418064117432\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.086184024810791 | KNN Loss: 3.1070356369018555 | BCE Loss: 0.9791485071182251\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.109503746032715 | KNN Loss: 3.092878818511963 | BCE Loss: 1.016624927520752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.087451457977295 | KNN Loss: 3.0844593048095703 | BCE Loss: 1.0029921531677246\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.091189861297607 | KNN Loss: 3.087977170944214 | BCE Loss: 1.003212571144104\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.084078311920166 | KNN Loss: 3.0850090980529785 | BCE Loss: 0.9990692138671875\n",
      "Epoch   449: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.108673095703125 | KNN Loss: 3.070420265197754 | BCE Loss: 1.038252592086792\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.087719440460205 | KNN Loss: 3.068009853363037 | BCE Loss: 1.019709587097168\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.104234218597412 | KNN Loss: 3.080981492996216 | BCE Loss: 1.0232527256011963\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.097179889678955 | KNN Loss: 3.0841450691223145 | BCE Loss: 1.0130349397659302\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.082257270812988 | KNN Loss: 3.05574107170105 | BCE Loss: 1.0265161991119385\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.049720287322998 | KNN Loss: 3.0427052974700928 | BCE Loss: 1.0070148706436157\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.071600914001465 | KNN Loss: 3.068098783493042 | BCE Loss: 1.0035018920898438\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.04573917388916 | KNN Loss: 3.058678388595581 | BCE Loss: 0.9870609045028687\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.102960586547852 | KNN Loss: 3.0886857509613037 | BCE Loss: 1.0142748355865479\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.076398849487305 | KNN Loss: 3.063582420349121 | BCE Loss: 1.0128164291381836\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.056797027587891 | KNN Loss: 3.0531933307647705 | BCE Loss: 1.0036035776138306\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.1340460777282715 | KNN Loss: 3.0917954444885254 | BCE Loss: 1.042250633239746\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.085948467254639 | KNN Loss: 3.062983274459839 | BCE Loss: 1.0229653120040894\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.111686706542969 | KNN Loss: 3.065390110015869 | BCE Loss: 1.0462967157363892\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.097215175628662 | KNN Loss: 3.080714464187622 | BCE Loss: 1.01650071144104\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.049259662628174 | KNN Loss: 3.0537633895874023 | BCE Loss: 0.9954964518547058\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.122498512268066 | KNN Loss: 3.070890426635742 | BCE Loss: 1.0516078472137451\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.0917558670043945 | KNN Loss: 3.0831823348999023 | BCE Loss: 1.008573293685913\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.0498270988464355 | KNN Loss: 3.04431414604187 | BCE Loss: 1.0055129528045654\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.0942182540893555 | KNN Loss: 3.0878028869628906 | BCE Loss: 1.0064153671264648\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.139957427978516 | KNN Loss: 3.1134121417999268 | BCE Loss: 1.0265452861785889\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.098343372344971 | KNN Loss: 3.0864574909210205 | BCE Loss: 1.0118858814239502\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.060111045837402 | KNN Loss: 3.0611367225646973 | BCE Loss: 0.998974084854126\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.002030849456787 | KNN Loss: 3.0347046852111816 | BCE Loss: 0.967326283454895\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.12600040435791 | KNN Loss: 3.093564748764038 | BCE Loss: 1.0324358940124512\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.102947235107422 | KNN Loss: 3.089840888977051 | BCE Loss: 1.0131062269210815\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.128823757171631 | KNN Loss: 3.091024875640869 | BCE Loss: 1.0377988815307617\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.101441383361816 | KNN Loss: 3.0664265155792236 | BCE Loss: 1.0350146293640137\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.08051872253418 | KNN Loss: 3.045257806777954 | BCE Loss: 1.0352609157562256\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.092207431793213 | KNN Loss: 3.0674397945404053 | BCE Loss: 1.0247676372528076\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.079095840454102 | KNN Loss: 3.0653367042541504 | BCE Loss: 1.0137591361999512\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.103945732116699 | KNN Loss: 3.0921525955200195 | BCE Loss: 1.0117931365966797\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.082502841949463 | KNN Loss: 3.028684616088867 | BCE Loss: 1.0538182258605957\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.0807414054870605 | KNN Loss: 3.03440260887146 | BCE Loss: 1.046338677406311\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.095180034637451 | KNN Loss: 3.0662996768951416 | BCE Loss: 1.0288803577423096\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.105248928070068 | KNN Loss: 3.0976760387420654 | BCE Loss: 1.007572889328003\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.100160121917725 | KNN Loss: 3.0579488277435303 | BCE Loss: 1.0422111749649048\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.029257774353027 | KNN Loss: 3.0152475833892822 | BCE Loss: 1.014009952545166\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.100528240203857 | KNN Loss: 3.0822927951812744 | BCE Loss: 1.0182353258132935\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.071199417114258 | KNN Loss: 3.077125072479248 | BCE Loss: 0.9940745830535889\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.096343040466309 | KNN Loss: 3.051936149597168 | BCE Loss: 1.0444068908691406\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.063999652862549 | KNN Loss: 3.0683534145355225 | BCE Loss: 0.9956461787223816\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.100813865661621 | KNN Loss: 3.097402334213257 | BCE Loss: 1.0034112930297852\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.075684547424316 | KNN Loss: 3.0712170600891113 | BCE Loss: 1.0044673681259155\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.068157196044922 | KNN Loss: 3.069514513015747 | BCE Loss: 0.9986425042152405\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.0978779792785645 | KNN Loss: 3.0731210708618164 | BCE Loss: 1.0247567892074585\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.111862659454346 | KNN Loss: 3.0776207447052 | BCE Loss: 1.0342419147491455\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.052813529968262 | KNN Loss: 3.0379321575164795 | BCE Loss: 1.0148816108703613\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.103733062744141 | KNN Loss: 3.0714187622070312 | BCE Loss: 1.0323140621185303\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.050694942474365 | KNN Loss: 3.0582191944122314 | BCE Loss: 0.9924756288528442\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.053259372711182 | KNN Loss: 3.043668031692505 | BCE Loss: 1.0095913410186768\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.1183552742004395 | KNN Loss: 3.087137222290039 | BCE Loss: 1.03121817111969\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.088329315185547 | KNN Loss: 3.078579902648926 | BCE Loss: 1.009749412536621\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.088967800140381 | KNN Loss: 3.058572292327881 | BCE Loss: 1.0303956270217896\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.044012069702148 | KNN Loss: 3.0243070125579834 | BCE Loss: 1.0197052955627441\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.028860092163086 | KNN Loss: 3.035830497741699 | BCE Loss: 0.9930298328399658\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.0292816162109375 | KNN Loss: 3.0265424251556396 | BCE Loss: 1.0027389526367188\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.021711826324463 | KNN Loss: 3.0315957069396973 | BCE Loss: 0.9901161193847656\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.06613826751709 | KNN Loss: 3.036576986312866 | BCE Loss: 1.0295610427856445\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.06283712387085 | KNN Loss: 3.018758535385132 | BCE Loss: 1.0440785884857178\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.108973026275635 | KNN Loss: 3.1229355335235596 | BCE Loss: 0.9860373735427856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.105681419372559 | KNN Loss: 3.0701873302459717 | BCE Loss: 1.035494327545166\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.093100547790527 | KNN Loss: 3.058084726333618 | BCE Loss: 1.0350158214569092\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.093588829040527 | KNN Loss: 3.0786702632904053 | BCE Loss: 1.014918565750122\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.063531875610352 | KNN Loss: 3.0682661533355713 | BCE Loss: 0.9952654838562012\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.095851421356201 | KNN Loss: 3.084588050842285 | BCE Loss: 1.011263370513916\n",
      "Epoch   460: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.03083610534668 | KNN Loss: 3.045865774154663 | BCE Loss: 0.9849704504013062\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.088153839111328 | KNN Loss: 3.0413007736206055 | BCE Loss: 1.0468533039093018\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.078197956085205 | KNN Loss: 3.057583808898926 | BCE Loss: 1.0206142663955688\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.090505599975586 | KNN Loss: 3.0618627071380615 | BCE Loss: 1.0286431312561035\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.052379131317139 | KNN Loss: 3.0266926288604736 | BCE Loss: 1.0256866216659546\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.093503475189209 | KNN Loss: 3.0849697589874268 | BCE Loss: 1.0085337162017822\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.110684871673584 | KNN Loss: 3.079212188720703 | BCE Loss: 1.0314726829528809\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.069707870483398 | KNN Loss: 3.067607879638672 | BCE Loss: 1.0020999908447266\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.080126762390137 | KNN Loss: 3.069305896759033 | BCE Loss: 1.0108208656311035\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.0784912109375 | KNN Loss: 3.0488829612731934 | BCE Loss: 1.0296084880828857\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.060351371765137 | KNN Loss: 3.047173023223877 | BCE Loss: 1.0131783485412598\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.056797027587891 | KNN Loss: 3.0427820682525635 | BCE Loss: 1.0140148401260376\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.0342631340026855 | KNN Loss: 3.051199197769165 | BCE Loss: 0.9830639362335205\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.048709392547607 | KNN Loss: 3.0456929206848145 | BCE Loss: 1.0030163526535034\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.098893642425537 | KNN Loss: 3.061852216720581 | BCE Loss: 1.037041425704956\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.124599933624268 | KNN Loss: 3.1028385162353516 | BCE Loss: 1.021761417388916\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.093373775482178 | KNN Loss: 3.0493876934051514 | BCE Loss: 1.0439860820770264\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.064216613769531 | KNN Loss: 3.0263473987579346 | BCE Loss: 1.0378693342208862\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.102431297302246 | KNN Loss: 3.06369948387146 | BCE Loss: 1.0387318134307861\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.0940141677856445 | KNN Loss: 3.0577759742736816 | BCE Loss: 1.0362379550933838\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.077352523803711 | KNN Loss: 3.0521702766418457 | BCE Loss: 1.0251822471618652\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.07276725769043 | KNN Loss: 3.088223695755005 | BCE Loss: 0.9845436215400696\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.127613067626953 | KNN Loss: 3.0848584175109863 | BCE Loss: 1.0427547693252563\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.120445251464844 | KNN Loss: 3.102052927017212 | BCE Loss: 1.018392562866211\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.062641143798828 | KNN Loss: 3.084078073501587 | BCE Loss: 0.978563129901886\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.124753475189209 | KNN Loss: 3.0736777782440186 | BCE Loss: 1.0510756969451904\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.041418075561523 | KNN Loss: 3.043565511703491 | BCE Loss: 0.997852623462677\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.0428266525268555 | KNN Loss: 3.0372121334075928 | BCE Loss: 1.0056142807006836\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.121659755706787 | KNN Loss: 3.107083559036255 | BCE Loss: 1.0145761966705322\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.084349632263184 | KNN Loss: 3.060994863510132 | BCE Loss: 1.0233546495437622\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.084447860717773 | KNN Loss: 3.0499696731567383 | BCE Loss: 1.0344783067703247\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.080117702484131 | KNN Loss: 3.0434370040893555 | BCE Loss: 1.0366805791854858\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.094554901123047 | KNN Loss: 3.062981128692627 | BCE Loss: 1.03157377243042\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.117924690246582 | KNN Loss: 3.098259687423706 | BCE Loss: 1.0196647644042969\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.056718826293945 | KNN Loss: 3.045884609222412 | BCE Loss: 1.0108344554901123\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.052802562713623 | KNN Loss: 3.0539488792419434 | BCE Loss: 0.9988538026809692\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.117679119110107 | KNN Loss: 3.0975637435913086 | BCE Loss: 1.0201152563095093\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.072542190551758 | KNN Loss: 3.0521838665008545 | BCE Loss: 1.0203580856323242\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.121191501617432 | KNN Loss: 3.083756923675537 | BCE Loss: 1.037434458732605\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.073028564453125 | KNN Loss: 3.0767385959625244 | BCE Loss: 0.9962900876998901\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.111873149871826 | KNN Loss: 3.0826454162597656 | BCE Loss: 1.029227614402771\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.0326619148254395 | KNN Loss: 3.042241334915161 | BCE Loss: 0.9904204607009888\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.107128143310547 | KNN Loss: 3.082249879837036 | BCE Loss: 1.0248785018920898\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.0429792404174805 | KNN Loss: 3.048703908920288 | BCE Loss: 0.9942752122879028\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.063712120056152 | KNN Loss: 3.0309598445892334 | BCE Loss: 1.032752275466919\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.076791286468506 | KNN Loss: 3.0411694049835205 | BCE Loss: 1.0356217622756958\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.085920333862305 | KNN Loss: 3.083935499191284 | BCE Loss: 1.00198495388031\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.105271339416504 | KNN Loss: 3.0549585819244385 | BCE Loss: 1.0503127574920654\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.110722541809082 | KNN Loss: 3.077792167663574 | BCE Loss: 1.0329302549362183\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.128796577453613 | KNN Loss: 3.0840866565704346 | BCE Loss: 1.0447099208831787\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.078934192657471 | KNN Loss: 3.084287166595459 | BCE Loss: 0.9946469664573669\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.075964450836182 | KNN Loss: 3.0820987224578857 | BCE Loss: 0.9938657283782959\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.0745720863342285 | KNN Loss: 3.0430240631103516 | BCE Loss: 1.0315481424331665\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.054800987243652 | KNN Loss: 3.047485113143921 | BCE Loss: 1.0073161125183105\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.118923187255859 | KNN Loss: 3.092491388320923 | BCE Loss: 1.0264320373535156\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.060766220092773 | KNN Loss: 3.0563228130340576 | BCE Loss: 1.004443645477295\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.112530708312988 | KNN Loss: 3.0699362754821777 | BCE Loss: 1.0425944328308105\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.121084213256836 | KNN Loss: 3.092167615890503 | BCE Loss: 1.028916358947754\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.094189643859863 | KNN Loss: 3.0442841053009033 | BCE Loss: 1.04990553855896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.071141242980957 | KNN Loss: 3.0663435459136963 | BCE Loss: 1.0047974586486816\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.078019618988037 | KNN Loss: 3.0741524696350098 | BCE Loss: 1.0038670301437378\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.061554908752441 | KNN Loss: 3.0299925804138184 | BCE Loss: 1.0315624475479126\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.113236427307129 | KNN Loss: 3.0986287593841553 | BCE Loss: 1.0146076679229736\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.0313591957092285 | KNN Loss: 3.0465540885925293 | BCE Loss: 0.9848049879074097\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.147325038909912 | KNN Loss: 3.1089487075805664 | BCE Loss: 1.0383764505386353\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.082969665527344 | KNN Loss: 3.075295925140381 | BCE Loss: 1.0076736211776733\n",
      "Epoch   471: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.088470935821533 | KNN Loss: 3.072383403778076 | BCE Loss: 1.0160876512527466\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.165297508239746 | KNN Loss: 3.1189727783203125 | BCE Loss: 1.046324610710144\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.0484089851379395 | KNN Loss: 3.0444021224975586 | BCE Loss: 1.0040067434310913\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.102503776550293 | KNN Loss: 3.0812807083129883 | BCE Loss: 1.0212230682373047\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.097809791564941 | KNN Loss: 3.0784075260162354 | BCE Loss: 1.0194025039672852\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.091704845428467 | KNN Loss: 3.0595216751098633 | BCE Loss: 1.0321831703186035\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.064667224884033 | KNN Loss: 3.0656914710998535 | BCE Loss: 0.9989758729934692\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.1021599769592285 | KNN Loss: 3.0848443508148193 | BCE Loss: 1.0173156261444092\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.082928657531738 | KNN Loss: 3.0680298805236816 | BCE Loss: 1.0148988962173462\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.0726776123046875 | KNN Loss: 3.065464496612549 | BCE Loss: 1.0072132349014282\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.082101345062256 | KNN Loss: 3.0475759506225586 | BCE Loss: 1.0345253944396973\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.094701766967773 | KNN Loss: 3.0502612590789795 | BCE Loss: 1.0444402694702148\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.1038408279418945 | KNN Loss: 3.062699794769287 | BCE Loss: 1.0411407947540283\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.129384994506836 | KNN Loss: 3.1109654903411865 | BCE Loss: 1.0184195041656494\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.153576850891113 | KNN Loss: 3.121640205383301 | BCE Loss: 1.0319366455078125\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.055596351623535 | KNN Loss: 3.028399705886841 | BCE Loss: 1.0271966457366943\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.1498212814331055 | KNN Loss: 3.1057558059692383 | BCE Loss: 1.0440657138824463\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.129075527191162 | KNN Loss: 3.0583250522613525 | BCE Loss: 1.07075035572052\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.078690052032471 | KNN Loss: 3.049633026123047 | BCE Loss: 1.0290569067001343\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.069668292999268 | KNN Loss: 3.055590867996216 | BCE Loss: 1.0140774250030518\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.080686569213867 | KNN Loss: 3.073608636856079 | BCE Loss: 1.007077932357788\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.091304302215576 | KNN Loss: 3.0518782138824463 | BCE Loss: 1.0394260883331299\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.099294662475586 | KNN Loss: 3.0753073692321777 | BCE Loss: 1.0239872932434082\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.120041847229004 | KNN Loss: 3.087383985519409 | BCE Loss: 1.0326581001281738\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.0566205978393555 | KNN Loss: 3.0560288429260254 | BCE Loss: 1.00059175491333\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.05465841293335 | KNN Loss: 3.043274164199829 | BCE Loss: 1.0113842487335205\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.091814041137695 | KNN Loss: 3.077646017074585 | BCE Loss: 1.0141680240631104\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.147629737854004 | KNN Loss: 3.1096951961517334 | BCE Loss: 1.0379345417022705\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.071232318878174 | KNN Loss: 3.0346226692199707 | BCE Loss: 1.0366095304489136\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.021791458129883 | KNN Loss: 3.032431125640869 | BCE Loss: 0.9893605709075928\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.05259370803833 | KNN Loss: 3.042463541030884 | BCE Loss: 1.0101301670074463\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.057971000671387 | KNN Loss: 3.0411975383758545 | BCE Loss: 1.0167737007141113\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.112678527832031 | KNN Loss: 3.090101957321167 | BCE Loss: 1.0225764513015747\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.121360778808594 | KNN Loss: 3.0611469745635986 | BCE Loss: 1.0602138042449951\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.058270454406738 | KNN Loss: 3.027036428451538 | BCE Loss: 1.0312342643737793\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.09230899810791 | KNN Loss: 3.080939531326294 | BCE Loss: 1.011369228363037\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.061239719390869 | KNN Loss: 3.0363359451293945 | BCE Loss: 1.0249038934707642\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.056905746459961 | KNN Loss: 3.053515672683716 | BCE Loss: 1.0033903121948242\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.083534240722656 | KNN Loss: 3.0563108921051025 | BCE Loss: 1.0272235870361328\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.030859470367432 | KNN Loss: 3.034942865371704 | BCE Loss: 0.9959167242050171\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.06114387512207 | KNN Loss: 3.0483598709106445 | BCE Loss: 1.0127840042114258\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.079990386962891 | KNN Loss: 3.0500059127807617 | BCE Loss: 1.029984474182129\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.099359512329102 | KNN Loss: 3.0666391849517822 | BCE Loss: 1.0327203273773193\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.1100921630859375 | KNN Loss: 3.0980277061462402 | BCE Loss: 1.0120646953582764\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.071259498596191 | KNN Loss: 3.036160469055176 | BCE Loss: 1.0350992679595947\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.049749374389648 | KNN Loss: 3.0209436416625977 | BCE Loss: 1.0288057327270508\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.046513080596924 | KNN Loss: 3.0507287979125977 | BCE Loss: 0.9957844018936157\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.0454630851745605 | KNN Loss: 3.051374673843384 | BCE Loss: 0.9940882921218872\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.108527183532715 | KNN Loss: 3.0629541873931885 | BCE Loss: 1.0455727577209473\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.093818664550781 | KNN Loss: 3.0562286376953125 | BCE Loss: 1.0375897884368896\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.086864471435547 | KNN Loss: 3.0855109691619873 | BCE Loss: 1.0013532638549805\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.092277526855469 | KNN Loss: 3.064467191696167 | BCE Loss: 1.0278103351593018\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.075493812561035 | KNN Loss: 3.048771858215332 | BCE Loss: 1.0267219543457031\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.124978065490723 | KNN Loss: 3.0816917419433594 | BCE Loss: 1.0432865619659424\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.081120491027832 | KNN Loss: 3.0646779537200928 | BCE Loss: 1.0164425373077393\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.084271430969238 | KNN Loss: 3.0691752433776855 | BCE Loss: 1.0150961875915527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.089734077453613 | KNN Loss: 3.046983242034912 | BCE Loss: 1.042750597000122\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.052148818969727 | KNN Loss: 3.0352730751037598 | BCE Loss: 1.016875982284546\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.065838813781738 | KNN Loss: 3.047142267227173 | BCE Loss: 1.0186963081359863\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.11040735244751 | KNN Loss: 3.1161696910858154 | BCE Loss: 0.9942377805709839\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.094037055969238 | KNN Loss: 3.091932535171509 | BCE Loss: 1.0021045207977295\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.109869956970215 | KNN Loss: 3.0841615200042725 | BCE Loss: 1.0257084369659424\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.079349517822266 | KNN Loss: 3.076646327972412 | BCE Loss: 1.0027031898498535\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.082265853881836 | KNN Loss: 3.038120746612549 | BCE Loss: 1.0441449880599976\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.091885566711426 | KNN Loss: 3.0827338695526123 | BCE Loss: 1.0091516971588135\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.069299697875977 | KNN Loss: 3.0651164054870605 | BCE Loss: 1.004183292388916\n",
      "Epoch   482: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.086461067199707 | KNN Loss: 3.072554111480713 | BCE Loss: 1.0139069557189941\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.090374946594238 | KNN Loss: 3.0938868522644043 | BCE Loss: 0.9964880347251892\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.0478315353393555 | KNN Loss: 3.0537006855010986 | BCE Loss: 0.9941306710243225\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.066179275512695 | KNN Loss: 3.0255753993988037 | BCE Loss: 1.0406041145324707\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.07659912109375 | KNN Loss: 3.046757459640503 | BCE Loss: 1.0298415422439575\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.094363212585449 | KNN Loss: 3.1070525646209717 | BCE Loss: 0.9873107075691223\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.110441207885742 | KNN Loss: 3.0801634788513184 | BCE Loss: 1.0302777290344238\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.084888458251953 | KNN Loss: 3.0837714672088623 | BCE Loss: 1.0011167526245117\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.1461687088012695 | KNN Loss: 3.0927608013153076 | BCE Loss: 1.0534076690673828\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.035313606262207 | KNN Loss: 3.0429627895355225 | BCE Loss: 0.9923505783081055\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.071750164031982 | KNN Loss: 3.065870523452759 | BCE Loss: 1.0058796405792236\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.058688640594482 | KNN Loss: 3.0448598861694336 | BCE Loss: 1.0138288736343384\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.109408855438232 | KNN Loss: 3.070711851119995 | BCE Loss: 1.0386970043182373\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.067670822143555 | KNN Loss: 3.0647621154785156 | BCE Loss: 1.00290846824646\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.100369453430176 | KNN Loss: 3.0722174644470215 | BCE Loss: 1.0281519889831543\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.059761047363281 | KNN Loss: 3.023491859436035 | BCE Loss: 1.0362693071365356\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.04334831237793 | KNN Loss: 3.0479238033294678 | BCE Loss: 0.9954242706298828\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.06046199798584 | KNN Loss: 3.0626397132873535 | BCE Loss: 0.9978224039077759\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.063440799713135 | KNN Loss: 3.0239405632019043 | BCE Loss: 1.0395002365112305\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.043434143066406 | KNN Loss: 3.0408053398132324 | BCE Loss: 1.0026285648345947\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.095432281494141 | KNN Loss: 3.0679214000701904 | BCE Loss: 1.0275108814239502\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.124757766723633 | KNN Loss: 3.063601493835449 | BCE Loss: 1.0611565113067627\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.163776397705078 | KNN Loss: 3.1142003536224365 | BCE Loss: 1.0495760440826416\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.059046745300293 | KNN Loss: 3.0322718620300293 | BCE Loss: 1.0267747640609741\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.047541618347168 | KNN Loss: 3.0398857593536377 | BCE Loss: 1.0076560974121094\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.08823823928833 | KNN Loss: 3.067324161529541 | BCE Loss: 1.020914077758789\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.1153244972229 | KNN Loss: 3.087965965270996 | BCE Loss: 1.0273585319519043\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.042270660400391 | KNN Loss: 3.0414133071899414 | BCE Loss: 1.0008574724197388\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.085885047912598 | KNN Loss: 3.0788230895996094 | BCE Loss: 1.0070619583129883\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.102145671844482 | KNN Loss: 3.079178810119629 | BCE Loss: 1.0229668617248535\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.122681617736816 | KNN Loss: 3.102962017059326 | BCE Loss: 1.0197194814682007\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.076306343078613 | KNN Loss: 3.0616812705993652 | BCE Loss: 1.014625072479248\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.048001766204834 | KNN Loss: 3.03039813041687 | BCE Loss: 1.0176037549972534\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.063289642333984 | KNN Loss: 3.040421962738037 | BCE Loss: 1.0228676795959473\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.117462158203125 | KNN Loss: 3.075214385986328 | BCE Loss: 1.0422475337982178\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.106574058532715 | KNN Loss: 3.0785675048828125 | BCE Loss: 1.0280064344406128\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.0761260986328125 | KNN Loss: 3.046717405319214 | BCE Loss: 1.0294084548950195\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.083029747009277 | KNN Loss: 3.0764050483703613 | BCE Loss: 1.0066248178482056\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.115190505981445 | KNN Loss: 3.081432342529297 | BCE Loss: 1.033758282661438\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.059512138366699 | KNN Loss: 3.051079034805298 | BCE Loss: 1.0084333419799805\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.067548751831055 | KNN Loss: 3.053922414779663 | BCE Loss: 1.0136263370513916\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.026313781738281 | KNN Loss: 3.045616626739502 | BCE Loss: 0.9806971549987793\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.127140045166016 | KNN Loss: 3.0904908180236816 | BCE Loss: 1.036649227142334\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.072742938995361 | KNN Loss: 3.0677080154418945 | BCE Loss: 1.0050348043441772\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.046675682067871 | KNN Loss: 3.0416555404663086 | BCE Loss: 1.0050201416015625\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.082601070404053 | KNN Loss: 3.0687694549560547 | BCE Loss: 1.013831615447998\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.07090950012207 | KNN Loss: 3.0609333515167236 | BCE Loss: 1.0099761486053467\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.0876545906066895 | KNN Loss: 3.075252056121826 | BCE Loss: 1.0124025344848633\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.077520370483398 | KNN Loss: 3.0570433139801025 | BCE Loss: 1.0204768180847168\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.068054676055908 | KNN Loss: 3.0509982109069824 | BCE Loss: 1.0170565843582153\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.052743911743164 | KNN Loss: 3.0196852684020996 | BCE Loss: 1.0330586433410645\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.134925842285156 | KNN Loss: 3.093883514404297 | BCE Loss: 1.0410423278808594\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.107194423675537 | KNN Loss: 3.0808424949645996 | BCE Loss: 1.026352047920227\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.1125407218933105 | KNN Loss: 3.099822521209717 | BCE Loss: 1.0127182006835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.123971939086914 | KNN Loss: 3.0755860805511475 | BCE Loss: 1.0483858585357666\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.075064182281494 | KNN Loss: 3.0567729473114014 | BCE Loss: 1.0182912349700928\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.0984954833984375 | KNN Loss: 3.062119483947754 | BCE Loss: 1.0363757610321045\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.119644641876221 | KNN Loss: 3.0852420330047607 | BCE Loss: 1.03440260887146\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.0983662605285645 | KNN Loss: 3.0622236728668213 | BCE Loss: 1.0361427068710327\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.060647964477539 | KNN Loss: 3.026803731918335 | BCE Loss: 1.033844232559204\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.0682172775268555 | KNN Loss: 3.0446863174438477 | BCE Loss: 1.0235308408737183\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.107405662536621 | KNN Loss: 3.1067516803741455 | BCE Loss: 1.0006541013717651\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.100244998931885 | KNN Loss: 3.0884063243865967 | BCE Loss: 1.011838674545288\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.0408759117126465 | KNN Loss: 3.044368028640747 | BCE Loss: 0.9965077638626099\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.068971157073975 | KNN Loss: 3.0383715629577637 | BCE Loss: 1.0305994749069214\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.05817985534668 | KNN Loss: 3.0482308864593506 | BCE Loss: 1.0099492073059082\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.079901695251465 | KNN Loss: 3.056615114212036 | BCE Loss: 1.0232865810394287\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.05616569519043 | KNN Loss: 3.0454304218292236 | BCE Loss: 1.010735273361206\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.046818256378174 | KNN Loss: 3.0347490310668945 | BCE Loss: 1.0120691061019897\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.136774063110352 | KNN Loss: 3.0970981121063232 | BCE Loss: 1.0396759510040283\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.058012008666992 | KNN Loss: 3.047417640686035 | BCE Loss: 1.010594367980957\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.08619499206543 | KNN Loss: 3.0693552494049072 | BCE Loss: 1.0168395042419434\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.0605340003967285 | KNN Loss: 3.0489580631256104 | BCE Loss: 1.0115759372711182\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.071266174316406 | KNN Loss: 3.055612564086914 | BCE Loss: 1.015653371810913\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.049997329711914 | KNN Loss: 3.0298080444335938 | BCE Loss: 1.0201890468597412\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.038362979888916 | KNN Loss: 3.0345396995544434 | BCE Loss: 1.003823161125183\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.0427141189575195 | KNN Loss: 3.0388402938842773 | BCE Loss: 1.0038740634918213\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.126195907592773 | KNN Loss: 3.105015277862549 | BCE Loss: 1.0211803913116455\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.105256080627441 | KNN Loss: 3.0761358737945557 | BCE Loss: 1.0291199684143066\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.047506809234619 | KNN Loss: 3.0619094371795654 | BCE Loss: 0.9855973124504089\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.079509258270264 | KNN Loss: 3.059082269668579 | BCE Loss: 1.020426869392395\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.050721168518066 | KNN Loss: 3.053501605987549 | BCE Loss: 0.9972196817398071\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.025264263153076 | KNN Loss: 3.01804780960083 | BCE Loss: 1.007216453552246\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.0685577392578125 | KNN Loss: 3.0199625492095947 | BCE Loss: 1.0485950708389282\n",
      "Epoch   496: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.073573589324951 | KNN Loss: 3.027188777923584 | BCE Loss: 1.0463848114013672\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.063473701477051 | KNN Loss: 3.0561459064483643 | BCE Loss: 1.0073275566101074\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.067142963409424 | KNN Loss: 3.068415403366089 | BCE Loss: 0.9987274408340454\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.076380252838135 | KNN Loss: 3.0514700412750244 | BCE Loss: 1.0249100923538208\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.097579479217529 | KNN Loss: 3.065587282180786 | BCE Loss: 1.0319921970367432\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.096346378326416 | KNN Loss: 3.0888562202453613 | BCE Loss: 1.0074901580810547\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.15010929107666 | KNN Loss: 3.104529619216919 | BCE Loss: 1.0455799102783203\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.077147483825684 | KNN Loss: 3.060002565383911 | BCE Loss: 1.017145037651062\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.082570552825928 | KNN Loss: 3.0382049083709717 | BCE Loss: 1.0443655252456665\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.092499732971191 | KNN Loss: 3.076441526412964 | BCE Loss: 1.0160579681396484\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.117925643920898 | KNN Loss: 3.072456121444702 | BCE Loss: 1.0454695224761963\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.066946029663086 | KNN Loss: 3.052238702774048 | BCE Loss: 1.014707088470459\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.125544548034668 | KNN Loss: 3.0893092155456543 | BCE Loss: 1.0362355709075928\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.074362754821777 | KNN Loss: 3.053159475326538 | BCE Loss: 1.0212033987045288\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.03857421875 | KNN Loss: 3.0248141288757324 | BCE Loss: 1.0137600898742676\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.054628372192383 | KNN Loss: 3.0562191009521484 | BCE Loss: 0.9984090328216553\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.069413185119629 | KNN Loss: 3.056661367416382 | BCE Loss: 1.0127520561218262\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.132429122924805 | KNN Loss: 3.116250514984131 | BCE Loss: 1.016178846359253\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.058928489685059 | KNN Loss: 3.058168411254883 | BCE Loss: 1.0007599592208862\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.095688819885254 | KNN Loss: 3.0679187774658203 | BCE Loss: 1.0277700424194336\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.114762783050537 | KNN Loss: 3.0726516246795654 | BCE Loss: 1.0421111583709717\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.099501609802246 | KNN Loss: 3.0640463829040527 | BCE Loss: 1.0354549884796143\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.040168285369873 | KNN Loss: 3.0387988090515137 | BCE Loss: 1.0013693571090698\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.106674671173096 | KNN Loss: 3.084526777267456 | BCE Loss: 1.0221478939056396\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6970,  3.1057,  2.7477,  3.4826,  3.7074,  0.7246,  2.7431,  2.0297,\n",
      "          2.1912,  1.7263,  2.2342,  1.8351,  0.9432,  1.6760,  1.1199,  1.4433,\n",
      "          2.3246,  3.3025,  2.7007,  2.4342,  1.5816,  3.0688,  2.2299,  2.5344,\n",
      "          1.9169,  1.8646,  2.2183,  1.4689,  1.4033,  0.3506, -0.2870,  1.0944,\n",
      "          0.3384,  0.9997,  1.2990,  1.5376,  1.1457,  3.5365,  0.7669,  1.3839,\n",
      "          1.0160, -0.7720, -0.3782,  2.4335,  1.7112,  0.5022, -0.3516,  0.0256,\n",
      "          1.3029,  2.5729,  1.9033,  0.2487,  1.4146,  0.5848, -0.5768,  1.2612,\n",
      "          1.6574,  1.1411,  1.4519,  1.8085,  0.7603,  0.7590,  0.2704,  1.9292,\n",
      "          1.4704,  1.3470, -1.8991,  0.3703,  2.5268,  2.0931,  2.6446,  0.3866,\n",
      "          1.0189,  2.6251,  2.0826,  1.2561,  0.2715,  0.7285,  0.2521,  1.5650,\n",
      "          0.0886,  0.5018,  1.9238, -0.3570,  0.3787, -1.0158, -2.3823, -0.4354,\n",
      "          0.5506, -1.8535,  0.3873, -0.0160, -0.5293, -1.0027,  0.3250,  1.4222,\n",
      "         -0.8213, -0.8191,  0.0890,  1.2542,  0.7868, -1.2481,  0.9366,  0.9071,\n",
      "         -1.1940, -1.1748, -0.2108,  0.0986, -1.2149, -1.6094, -0.4223, -2.7469,\n",
      "         -0.3190,  1.8326,  1.2573, -0.2475, -0.5117,  0.0353,  1.5080, -2.7883,\n",
      "          0.0949, -0.1175,  0.4318, -0.7287,  0.0874, -0.7906, -0.9631,  0.9751,\n",
      "          0.2380, -0.6854,  0.2914, -0.6571, -1.3161, -0.3669, -0.5377,  0.8700,\n",
      "         -0.4537,  0.1505, -2.0636, -1.0027, -1.4598,  0.6686, -1.9057, -1.0138,\n",
      "         -1.0226, -0.6515, -1.6184, -1.1355, -2.4349, -1.0361, -1.3144, -0.4831,\n",
      "         -1.7002,  0.4746, -1.5154, -0.7457, -3.4060,  0.0916, -0.1388, -0.7333,\n",
      "         -2.2814, -1.6688, -1.2203, -1.4499, -2.3834, -2.2489, -3.5563]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.5563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.7074, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ee60b69bee4840bb2a0be38110e444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 85.35it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fc6e21389b4fa58216ba94306c6f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cae9abb7ce4ea1a09b91268b80d161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac036ebe95745a5b600484eea87b0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 019 | Total loss: 9.621 | Reg loss: 0.012 | Tree loss: 9.621 | Accuracy: 0.000000 | 0.821 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 019 | Total loss: 9.618 | Reg loss: 0.011 | Tree loss: 9.618 | Accuracy: 0.000000 | 0.784 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 019 | Total loss: 9.617 | Reg loss: 0.010 | Tree loss: 9.617 | Accuracy: 0.000000 | 0.775 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 019 | Total loss: 9.615 | Reg loss: 0.009 | Tree loss: 9.615 | Accuracy: 0.000000 | 0.772 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 019 | Total loss: 9.613 | Reg loss: 0.009 | Tree loss: 9.613 | Accuracy: 0.000000 | 0.769 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 019 | Total loss: 9.612 | Reg loss: 0.008 | Tree loss: 9.612 | Accuracy: 0.000000 | 0.768 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 019 | Total loss: 9.609 | Reg loss: 0.007 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.768 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 019 | Total loss: 9.609 | Reg loss: 0.007 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.768 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 019 | Total loss: 9.606 | Reg loss: 0.006 | Tree loss: 9.606 | Accuracy: 0.000000 | 0.768 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 019 | Total loss: 9.603 | Reg loss: 0.006 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.768 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 019 | Total loss: 9.602 | Reg loss: 0.006 | Tree loss: 9.602 | Accuracy: 0.000000 | 0.769 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 019 | Total loss: 9.600 | Reg loss: 0.006 | Tree loss: 9.600 | Accuracy: 0.000000 | 0.77 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 019 | Total loss: 9.599 | Reg loss: 0.006 | Tree loss: 9.599 | Accuracy: 0.000000 | 0.77 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 019 | Total loss: 9.596 | Reg loss: 0.005 | Tree loss: 9.596 | Accuracy: 0.000000 | 0.771 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 019 | Total loss: 9.595 | Reg loss: 0.005 | Tree loss: 9.595 | Accuracy: 0.000000 | 0.771 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 019 | Total loss: 9.594 | Reg loss: 0.006 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.771 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 019 | Total loss: 9.593 | Reg loss: 0.006 | Tree loss: 9.593 | Accuracy: 0.000000 | 0.771 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 019 | Total loss: 9.591 | Reg loss: 0.006 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.772 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 019 | Total loss: 9.592 | Reg loss: 0.006 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.771 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 019 | Total loss: 9.598 | Reg loss: 0.003 | Tree loss: 9.598 | Accuracy: 0.000000 | 0.777 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 019 | Total loss: 9.598 | Reg loss: 0.003 | Tree loss: 9.598 | Accuracy: 0.000000 | 0.775 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 019 | Total loss: 9.596 | Reg loss: 0.003 | Tree loss: 9.596 | Accuracy: 0.000000 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 019 | Total loss: 9.593 | Reg loss: 0.003 | Tree loss: 9.593 | Accuracy: 0.000000 | 0.772 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 019 | Total loss: 9.591 | Reg loss: 0.003 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 019 | Total loss: 9.593 | Reg loss: 0.004 | Tree loss: 9.593 | Accuracy: 0.000000 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 019 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.772 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 019 | Total loss: 9.588 | Reg loss: 0.004 | Tree loss: 9.588 | Accuracy: 0.000000 | 0.772 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 019 | Total loss: 9.589 | Reg loss: 0.004 | Tree loss: 9.589 | Accuracy: 0.003906 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 019 | Total loss: 9.584 | Reg loss: 0.004 | Tree loss: 9.584 | Accuracy: 0.005859 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 019 | Total loss: 9.583 | Reg loss: 0.005 | Tree loss: 9.583 | Accuracy: 0.011719 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 019 | Total loss: 9.582 | Reg loss: 0.005 | Tree loss: 9.582 | Accuracy: 0.011719 | 0.773 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 019 | Total loss: 9.582 | Reg loss: 0.005 | Tree loss: 9.582 | Accuracy: 0.019531 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 019 | Total loss: 9.581 | Reg loss: 0.005 | Tree loss: 9.581 | Accuracy: 0.023438 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 019 | Total loss: 9.574 | Reg loss: 0.005 | Tree loss: 9.574 | Accuracy: 0.027344 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 019 | Total loss: 9.578 | Reg loss: 0.005 | Tree loss: 9.578 | Accuracy: 0.039062 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 019 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.025391 | 0.774 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 019 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.025391 | 0.775 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 019 | Total loss: 9.575 | Reg loss: 0.006 | Tree loss: 9.575 | Accuracy: 0.045296 | 0.776 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 019 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.007812 | 0.778 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 019 | Total loss: 9.582 | Reg loss: 0.004 | Tree loss: 9.582 | Accuracy: 0.019531 | 0.778 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 019 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.017578 | 0.777 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 019 | Total loss: 9.581 | Reg loss: 0.004 | Tree loss: 9.581 | Accuracy: 0.025391 | 0.777 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 019 | Total loss: 9.580 | Reg loss: 0.004 | Tree loss: 9.580 | Accuracy: 0.027344 | 0.778 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 019 | Total loss: 9.578 | Reg loss: 0.004 | Tree loss: 9.578 | Accuracy: 0.042969 | 0.78 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 019 | Total loss: 9.578 | Reg loss: 0.005 | Tree loss: 9.578 | Accuracy: 0.046875 | 0.781 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 019 | Total loss: 9.573 | Reg loss: 0.005 | Tree loss: 9.573 | Accuracy: 0.048828 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 019 | Total loss: 9.574 | Reg loss: 0.005 | Tree loss: 9.574 | Accuracy: 0.039062 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 019 | Total loss: 9.572 | Reg loss: 0.005 | Tree loss: 9.572 | Accuracy: 0.050781 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 019 | Total loss: 9.571 | Reg loss: 0.005 | Tree loss: 9.571 | Accuracy: 0.050781 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 019 | Total loss: 9.569 | Reg loss: 0.005 | Tree loss: 9.569 | Accuracy: 0.056641 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 019 | Total loss: 9.567 | Reg loss: 0.006 | Tree loss: 9.567 | Accuracy: 0.054688 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 019 | Total loss: 9.567 | Reg loss: 0.006 | Tree loss: 9.567 | Accuracy: 0.048828 | 0.782 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 019 | Total loss: 9.568 | Reg loss: 0.006 | Tree loss: 9.568 | Accuracy: 0.044922 | 0.783 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 019 | Total loss: 9.565 | Reg loss: 0.006 | Tree loss: 9.565 | Accuracy: 0.039062 | 0.784 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 019 | Total loss: 9.565 | Reg loss: 0.006 | Tree loss: 9.565 | Accuracy: 0.031250 | 0.784 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 019 | Total loss: 9.563 | Reg loss: 0.006 | Tree loss: 9.563 | Accuracy: 0.050781 | 0.783 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 019 | Total loss: 9.562 | Reg loss: 0.006 | Tree loss: 9.562 | Accuracy: 0.031359 | 0.784 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 019 | Total loss: 9.572 | Reg loss: 0.005 | Tree loss: 9.572 | Accuracy: 0.046875 | 0.785 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Batch: 001 / 019 | Total loss: 9.569 | Reg loss: 0.005 | Tree loss: 9.569 | Accuracy: 0.048828 | 0.785 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 019 | Total loss: 9.567 | Reg loss: 0.005 | Tree loss: 9.567 | Accuracy: 0.056641 | 0.784 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 019 | Total loss: 9.568 | Reg loss: 0.005 | Tree loss: 9.568 | Accuracy: 0.066406 | 0.784 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 019 | Total loss: 9.568 | Reg loss: 0.005 | Tree loss: 9.568 | Accuracy: 0.048828 | 0.784 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 019 | Total loss: 9.567 | Reg loss: 0.005 | Tree loss: 9.567 | Accuracy: 0.048828 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 019 | Total loss: 9.563 | Reg loss: 0.005 | Tree loss: 9.563 | Accuracy: 0.054688 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 019 | Total loss: 9.562 | Reg loss: 0.006 | Tree loss: 9.562 | Accuracy: 0.044922 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 019 | Total loss: 9.560 | Reg loss: 0.006 | Tree loss: 9.560 | Accuracy: 0.058594 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 019 | Total loss: 9.560 | Reg loss: 0.006 | Tree loss: 9.560 | Accuracy: 0.054688 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 019 | Total loss: 9.561 | Reg loss: 0.006 | Tree loss: 9.561 | Accuracy: 0.070312 | 0.783 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 019 | Total loss: 9.557 | Reg loss: 0.006 | Tree loss: 9.557 | Accuracy: 0.054688 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 019 | Total loss: 9.559 | Reg loss: 0.006 | Tree loss: 9.559 | Accuracy: 0.070312 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 019 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.068359 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 019 | Total loss: 9.557 | Reg loss: 0.007 | Tree loss: 9.557 | Accuracy: 0.056641 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 019 | Total loss: 9.553 | Reg loss: 0.007 | Tree loss: 9.553 | Accuracy: 0.056641 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 019 | Total loss: 9.553 | Reg loss: 0.007 | Tree loss: 9.553 | Accuracy: 0.087891 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 019 | Total loss: 9.553 | Reg loss: 0.007 | Tree loss: 9.553 | Accuracy: 0.052734 | 0.782 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 019 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.073171 | 0.782 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 019 | Total loss: 9.559 | Reg loss: 0.006 | Tree loss: 9.559 | Accuracy: 0.052734 | 0.783 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 019 | Total loss: 9.557 | Reg loss: 0.006 | Tree loss: 9.557 | Accuracy: 0.078125 | 0.782 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 019 | Total loss: 9.559 | Reg loss: 0.006 | Tree loss: 9.559 | Accuracy: 0.048828 | 0.782 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 019 | Total loss: 9.556 | Reg loss: 0.006 | Tree loss: 9.556 | Accuracy: 0.062500 | 0.782 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 019 | Total loss: 9.553 | Reg loss: 0.006 | Tree loss: 9.553 | Accuracy: 0.099609 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 019 | Total loss: 9.553 | Reg loss: 0.006 | Tree loss: 9.553 | Accuracy: 0.066406 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 019 | Total loss: 9.552 | Reg loss: 0.007 | Tree loss: 9.552 | Accuracy: 0.091797 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 019 | Total loss: 9.551 | Reg loss: 0.007 | Tree loss: 9.551 | Accuracy: 0.072266 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 019 | Total loss: 9.553 | Reg loss: 0.007 | Tree loss: 9.553 | Accuracy: 0.072266 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 019 | Total loss: 9.548 | Reg loss: 0.007 | Tree loss: 9.548 | Accuracy: 0.080078 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 019 | Total loss: 9.547 | Reg loss: 0.007 | Tree loss: 9.547 | Accuracy: 0.074219 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 019 | Total loss: 9.543 | Reg loss: 0.007 | Tree loss: 9.543 | Accuracy: 0.070312 | 0.781 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 019 | Total loss: 9.543 | Reg loss: 0.008 | Tree loss: 9.543 | Accuracy: 0.066406 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 019 | Total loss: 9.540 | Reg loss: 0.008 | Tree loss: 9.540 | Accuracy: 0.076172 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 019 | Total loss: 9.545 | Reg loss: 0.008 | Tree loss: 9.545 | Accuracy: 0.066406 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 019 | Total loss: 9.537 | Reg loss: 0.008 | Tree loss: 9.537 | Accuracy: 0.078125 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 019 | Total loss: 9.537 | Reg loss: 0.009 | Tree loss: 9.537 | Accuracy: 0.105469 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 019 | Total loss: 9.536 | Reg loss: 0.009 | Tree loss: 9.536 | Accuracy: 0.068359 | 0.78 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 019 | Total loss: 9.531 | Reg loss: 0.009 | Tree loss: 9.531 | Accuracy: 0.104530 | 0.78 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 019 | Total loss: 9.545 | Reg loss: 0.007 | Tree loss: 9.545 | Accuracy: 0.072266 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 019 | Total loss: 9.540 | Reg loss: 0.007 | Tree loss: 9.540 | Accuracy: 0.080078 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 019 | Total loss: 9.542 | Reg loss: 0.007 | Tree loss: 9.542 | Accuracy: 0.097656 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 019 | Total loss: 9.538 | Reg loss: 0.007 | Tree loss: 9.538 | Accuracy: 0.101562 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 019 | Total loss: 9.537 | Reg loss: 0.008 | Tree loss: 9.537 | Accuracy: 0.076172 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 019 | Total loss: 9.534 | Reg loss: 0.008 | Tree loss: 9.534 | Accuracy: 0.082031 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 019 | Total loss: 9.534 | Reg loss: 0.008 | Tree loss: 9.534 | Accuracy: 0.066406 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 019 | Total loss: 9.536 | Reg loss: 0.008 | Tree loss: 9.536 | Accuracy: 0.068359 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 019 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.078125 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 019 | Total loss: 9.526 | Reg loss: 0.009 | Tree loss: 9.526 | Accuracy: 0.076172 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 019 | Total loss: 9.524 | Reg loss: 0.009 | Tree loss: 9.524 | Accuracy: 0.072266 | 0.781 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 019 | Total loss: 9.524 | Reg loss: 0.009 | Tree loss: 9.524 | Accuracy: 0.074219 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 019 | Total loss: 9.518 | Reg loss: 0.009 | Tree loss: 9.518 | Accuracy: 0.085938 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 019 | Total loss: 9.518 | Reg loss: 0.010 | Tree loss: 9.518 | Accuracy: 0.074219 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 019 | Total loss: 9.517 | Reg loss: 0.010 | Tree loss: 9.517 | Accuracy: 0.089844 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 019 | Total loss: 9.512 | Reg loss: 0.010 | Tree loss: 9.512 | Accuracy: 0.050781 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 019 | Total loss: 9.511 | Reg loss: 0.011 | Tree loss: 9.511 | Accuracy: 0.082031 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 019 | Total loss: 9.505 | Reg loss: 0.011 | Tree loss: 9.505 | Accuracy: 0.062500 | 0.78 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 019 | Total loss: 9.499 | Reg loss: 0.011 | Tree loss: 9.499 | Accuracy: 0.087108 | 0.78 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 019 | Total loss: 9.527 | Reg loss: 0.009 | Tree loss: 9.527 | Accuracy: 0.070312 | 0.781 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 019 | Total loss: 9.519 | Reg loss: 0.009 | Tree loss: 9.519 | Accuracy: 0.074219 | 0.781 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 019 | Total loss: 9.521 | Reg loss: 0.009 | Tree loss: 9.521 | Accuracy: 0.046875 | 0.781 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 019 | Total loss: 9.512 | Reg loss: 0.009 | Tree loss: 9.512 | Accuracy: 0.072266 | 0.781 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 004 / 019 | Total loss: 9.509 | Reg loss: 0.009 | Tree loss: 9.509 | Accuracy: 0.083984 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 019 | Total loss: 9.500 | Reg loss: 0.009 | Tree loss: 9.500 | Accuracy: 0.091797 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 019 | Total loss: 9.498 | Reg loss: 0.010 | Tree loss: 9.498 | Accuracy: 0.097656 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 019 | Total loss: 9.502 | Reg loss: 0.010 | Tree loss: 9.502 | Accuracy: 0.070312 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 019 | Total loss: 9.494 | Reg loss: 0.010 | Tree loss: 9.494 | Accuracy: 0.074219 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 019 | Total loss: 9.492 | Reg loss: 0.011 | Tree loss: 9.492 | Accuracy: 0.091797 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 019 | Total loss: 9.480 | Reg loss: 0.011 | Tree loss: 9.480 | Accuracy: 0.070312 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 019 | Total loss: 9.480 | Reg loss: 0.011 | Tree loss: 9.480 | Accuracy: 0.076172 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 019 | Total loss: 9.467 | Reg loss: 0.012 | Tree loss: 9.467 | Accuracy: 0.070312 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 019 | Total loss: 9.468 | Reg loss: 0.012 | Tree loss: 9.468 | Accuracy: 0.087891 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 019 | Total loss: 9.457 | Reg loss: 0.012 | Tree loss: 9.457 | Accuracy: 0.062500 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 019 | Total loss: 9.450 | Reg loss: 0.013 | Tree loss: 9.450 | Accuracy: 0.074219 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 019 | Total loss: 9.451 | Reg loss: 0.013 | Tree loss: 9.451 | Accuracy: 0.066406 | 0.78 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 019 | Total loss: 9.435 | Reg loss: 0.013 | Tree loss: 9.435 | Accuracy: 0.044922 | 0.779 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 019 | Total loss: 9.438 | Reg loss: 0.014 | Tree loss: 9.438 | Accuracy: 0.045296 | 0.779 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 019 | Total loss: 9.481 | Reg loss: 0.010 | Tree loss: 9.481 | Accuracy: 0.070312 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 019 | Total loss: 9.476 | Reg loss: 0.010 | Tree loss: 9.476 | Accuracy: 0.058594 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 019 | Total loss: 9.465 | Reg loss: 0.011 | Tree loss: 9.465 | Accuracy: 0.087891 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 019 | Total loss: 9.461 | Reg loss: 0.011 | Tree loss: 9.461 | Accuracy: 0.082031 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 019 | Total loss: 9.456 | Reg loss: 0.011 | Tree loss: 9.456 | Accuracy: 0.074219 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 019 | Total loss: 9.445 | Reg loss: 0.011 | Tree loss: 9.445 | Accuracy: 0.087891 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 019 | Total loss: 9.443 | Reg loss: 0.011 | Tree loss: 9.443 | Accuracy: 0.072266 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 019 | Total loss: 9.432 | Reg loss: 0.012 | Tree loss: 9.432 | Accuracy: 0.083984 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 019 | Total loss: 9.413 | Reg loss: 0.012 | Tree loss: 9.413 | Accuracy: 0.072266 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 019 | Total loss: 9.417 | Reg loss: 0.012 | Tree loss: 9.417 | Accuracy: 0.072266 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 019 | Total loss: 9.411 | Reg loss: 0.013 | Tree loss: 9.411 | Accuracy: 0.087891 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 019 | Total loss: 9.388 | Reg loss: 0.013 | Tree loss: 9.388 | Accuracy: 0.066406 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 019 | Total loss: 9.399 | Reg loss: 0.014 | Tree loss: 9.399 | Accuracy: 0.060547 | 0.779 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 019 | Total loss: 9.379 | Reg loss: 0.014 | Tree loss: 9.379 | Accuracy: 0.050781 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 019 | Total loss: 9.364 | Reg loss: 0.014 | Tree loss: 9.364 | Accuracy: 0.060547 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 019 | Total loss: 9.353 | Reg loss: 0.015 | Tree loss: 9.353 | Accuracy: 0.078125 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 019 | Total loss: 9.352 | Reg loss: 0.015 | Tree loss: 9.352 | Accuracy: 0.044922 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 019 | Total loss: 9.342 | Reg loss: 0.016 | Tree loss: 9.342 | Accuracy: 0.074219 | 0.78 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 019 | Total loss: 9.322 | Reg loss: 0.016 | Tree loss: 9.322 | Accuracy: 0.052265 | 0.78 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 019 | Total loss: 9.414 | Reg loss: 0.012 | Tree loss: 9.414 | Accuracy: 0.089844 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 019 | Total loss: 9.388 | Reg loss: 0.012 | Tree loss: 9.388 | Accuracy: 0.082031 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 019 | Total loss: 9.396 | Reg loss: 0.012 | Tree loss: 9.396 | Accuracy: 0.060547 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 019 | Total loss: 9.380 | Reg loss: 0.013 | Tree loss: 9.380 | Accuracy: 0.064453 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 019 | Total loss: 9.367 | Reg loss: 0.013 | Tree loss: 9.367 | Accuracy: 0.068359 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 019 | Total loss: 9.356 | Reg loss: 0.013 | Tree loss: 9.356 | Accuracy: 0.064453 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 019 | Total loss: 9.346 | Reg loss: 0.013 | Tree loss: 9.346 | Accuracy: 0.052734 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 019 | Total loss: 9.326 | Reg loss: 0.014 | Tree loss: 9.326 | Accuracy: 0.074219 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 019 | Total loss: 9.319 | Reg loss: 0.014 | Tree loss: 9.319 | Accuracy: 0.070312 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 019 | Total loss: 9.294 | Reg loss: 0.014 | Tree loss: 9.294 | Accuracy: 0.085938 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 019 | Total loss: 9.290 | Reg loss: 0.015 | Tree loss: 9.290 | Accuracy: 0.068359 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 019 | Total loss: 9.292 | Reg loss: 0.015 | Tree loss: 9.292 | Accuracy: 0.054688 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 019 | Total loss: 9.268 | Reg loss: 0.015 | Tree loss: 9.268 | Accuracy: 0.070312 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 019 | Total loss: 9.257 | Reg loss: 0.016 | Tree loss: 9.257 | Accuracy: 0.052734 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 019 | Total loss: 9.241 | Reg loss: 0.016 | Tree loss: 9.241 | Accuracy: 0.044922 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 019 | Total loss: 9.212 | Reg loss: 0.017 | Tree loss: 9.212 | Accuracy: 0.052734 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 019 | Total loss: 9.211 | Reg loss: 0.017 | Tree loss: 9.211 | Accuracy: 0.048828 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 019 | Total loss: 9.173 | Reg loss: 0.017 | Tree loss: 9.173 | Accuracy: 0.050781 | 0.782 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 019 | Total loss: 9.176 | Reg loss: 0.018 | Tree loss: 9.176 | Accuracy: 0.031359 | 0.782 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 019 | Total loss: 9.299 | Reg loss: 0.014 | Tree loss: 9.299 | Accuracy: 0.062500 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 019 | Total loss: 9.283 | Reg loss: 0.014 | Tree loss: 9.283 | Accuracy: 0.085938 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 019 | Total loss: 9.265 | Reg loss: 0.014 | Tree loss: 9.265 | Accuracy: 0.087891 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 019 | Total loss: 9.257 | Reg loss: 0.014 | Tree loss: 9.257 | Accuracy: 0.058594 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 019 | Total loss: 9.246 | Reg loss: 0.014 | Tree loss: 9.246 | Accuracy: 0.066406 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 019 | Total loss: 9.236 | Reg loss: 0.015 | Tree loss: 9.236 | Accuracy: 0.054688 | 0.784 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 019 | Total loss: 9.222 | Reg loss: 0.015 | Tree loss: 9.222 | Accuracy: 0.062500 | 0.785 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 007 / 019 | Total loss: 9.191 | Reg loss: 0.015 | Tree loss: 9.191 | Accuracy: 0.076172 | 0.785 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 019 | Total loss: 9.187 | Reg loss: 0.016 | Tree loss: 9.187 | Accuracy: 0.064453 | 0.785 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 019 | Total loss: 9.160 | Reg loss: 0.016 | Tree loss: 9.160 | Accuracy: 0.058594 | 0.786 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 019 | Total loss: 9.128 | Reg loss: 0.016 | Tree loss: 9.128 | Accuracy: 0.050781 | 0.786 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 019 | Total loss: 9.126 | Reg loss: 0.017 | Tree loss: 9.126 | Accuracy: 0.054688 | 0.786 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 019 | Total loss: 9.113 | Reg loss: 0.017 | Tree loss: 9.113 | Accuracy: 0.052734 | 0.786 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 019 | Total loss: 9.087 | Reg loss: 0.017 | Tree loss: 9.087 | Accuracy: 0.068359 | 0.787 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 019 | Total loss: 9.061 | Reg loss: 0.018 | Tree loss: 9.061 | Accuracy: 0.050781 | 0.787 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 019 | Total loss: 9.070 | Reg loss: 0.018 | Tree loss: 9.070 | Accuracy: 0.062500 | 0.787 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 019 | Total loss: 9.038 | Reg loss: 0.019 | Tree loss: 9.038 | Accuracy: 0.048828 | 0.787 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 019 | Total loss: 9.006 | Reg loss: 0.019 | Tree loss: 9.006 | Accuracy: 0.068359 | 0.787 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 019 | Total loss: 8.990 | Reg loss: 0.019 | Tree loss: 8.990 | Accuracy: 0.041812 | 0.788 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 019 | Total loss: 9.158 | Reg loss: 0.015 | Tree loss: 9.158 | Accuracy: 0.068359 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 019 | Total loss: 9.147 | Reg loss: 0.016 | Tree loss: 9.147 | Accuracy: 0.048828 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 019 | Total loss: 9.119 | Reg loss: 0.016 | Tree loss: 9.119 | Accuracy: 0.056641 | 0.792 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 019 | Total loss: 9.101 | Reg loss: 0.016 | Tree loss: 9.101 | Accuracy: 0.056641 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 019 | Total loss: 9.087 | Reg loss: 0.016 | Tree loss: 9.087 | Accuracy: 0.042969 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 019 | Total loss: 9.047 | Reg loss: 0.016 | Tree loss: 9.047 | Accuracy: 0.056641 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 019 | Total loss: 9.024 | Reg loss: 0.016 | Tree loss: 9.024 | Accuracy: 0.072266 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 019 | Total loss: 9.033 | Reg loss: 0.017 | Tree loss: 9.033 | Accuracy: 0.046875 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 019 | Total loss: 9.012 | Reg loss: 0.017 | Tree loss: 9.012 | Accuracy: 0.044922 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 019 | Total loss: 8.991 | Reg loss: 0.017 | Tree loss: 8.991 | Accuracy: 0.056641 | 0.793 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 019 | Total loss: 8.962 | Reg loss: 0.018 | Tree loss: 8.962 | Accuracy: 0.064453 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 019 | Total loss: 8.957 | Reg loss: 0.018 | Tree loss: 8.957 | Accuracy: 0.044922 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 019 | Total loss: 8.934 | Reg loss: 0.018 | Tree loss: 8.934 | Accuracy: 0.041016 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 019 | Total loss: 8.929 | Reg loss: 0.019 | Tree loss: 8.929 | Accuracy: 0.052734 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 019 | Total loss: 8.879 | Reg loss: 0.019 | Tree loss: 8.879 | Accuracy: 0.064453 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 019 | Total loss: 8.847 | Reg loss: 0.019 | Tree loss: 8.847 | Accuracy: 0.062500 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 019 | Total loss: 8.860 | Reg loss: 0.020 | Tree loss: 8.860 | Accuracy: 0.054688 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 019 | Total loss: 8.817 | Reg loss: 0.020 | Tree loss: 8.817 | Accuracy: 0.072266 | 0.794 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 019 | Total loss: 8.811 | Reg loss: 0.021 | Tree loss: 8.811 | Accuracy: 0.041812 | 0.794 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 019 | Total loss: 8.995 | Reg loss: 0.017 | Tree loss: 8.995 | Accuracy: 0.041016 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 019 | Total loss: 8.967 | Reg loss: 0.017 | Tree loss: 8.967 | Accuracy: 0.064453 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 019 | Total loss: 8.951 | Reg loss: 0.017 | Tree loss: 8.951 | Accuracy: 0.044922 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 019 | Total loss: 8.922 | Reg loss: 0.017 | Tree loss: 8.922 | Accuracy: 0.054688 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 019 | Total loss: 8.897 | Reg loss: 0.017 | Tree loss: 8.897 | Accuracy: 0.042969 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 019 | Total loss: 8.879 | Reg loss: 0.018 | Tree loss: 8.879 | Accuracy: 0.048828 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 019 | Total loss: 8.846 | Reg loss: 0.018 | Tree loss: 8.846 | Accuracy: 0.070312 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 019 | Total loss: 8.846 | Reg loss: 0.018 | Tree loss: 8.846 | Accuracy: 0.052734 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 019 | Total loss: 8.818 | Reg loss: 0.018 | Tree loss: 8.818 | Accuracy: 0.044922 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 019 | Total loss: 8.805 | Reg loss: 0.019 | Tree loss: 8.805 | Accuracy: 0.070312 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 019 | Total loss: 8.792 | Reg loss: 0.019 | Tree loss: 8.792 | Accuracy: 0.048828 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 019 | Total loss: 8.762 | Reg loss: 0.019 | Tree loss: 8.762 | Accuracy: 0.044922 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 019 | Total loss: 8.708 | Reg loss: 0.019 | Tree loss: 8.708 | Accuracy: 0.062500 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 019 | Total loss: 8.694 | Reg loss: 0.020 | Tree loss: 8.694 | Accuracy: 0.066406 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 019 | Total loss: 8.679 | Reg loss: 0.020 | Tree loss: 8.679 | Accuracy: 0.072266 | 0.799 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 019 | Total loss: 8.656 | Reg loss: 0.020 | Tree loss: 8.656 | Accuracy: 0.042969 | 0.8 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 019 | Total loss: 8.650 | Reg loss: 0.021 | Tree loss: 8.650 | Accuracy: 0.044922 | 0.8 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 019 | Total loss: 8.612 | Reg loss: 0.021 | Tree loss: 8.612 | Accuracy: 0.058594 | 0.8 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 019 | Total loss: 8.589 | Reg loss: 0.021 | Tree loss: 8.589 | Accuracy: 0.038328 | 0.8 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 019 | Total loss: 8.799 | Reg loss: 0.018 | Tree loss: 8.799 | Accuracy: 0.046875 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 019 | Total loss: 8.765 | Reg loss: 0.018 | Tree loss: 8.765 | Accuracy: 0.054688 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 019 | Total loss: 8.743 | Reg loss: 0.018 | Tree loss: 8.743 | Accuracy: 0.060547 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 019 | Total loss: 8.755 | Reg loss: 0.019 | Tree loss: 8.755 | Accuracy: 0.044922 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 019 | Total loss: 8.705 | Reg loss: 0.019 | Tree loss: 8.705 | Accuracy: 0.052734 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 019 | Total loss: 8.703 | Reg loss: 0.019 | Tree loss: 8.703 | Accuracy: 0.029297 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 019 | Total loss: 8.630 | Reg loss: 0.019 | Tree loss: 8.630 | Accuracy: 0.054688 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 019 | Total loss: 8.608 | Reg loss: 0.019 | Tree loss: 8.608 | Accuracy: 0.072266 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 019 | Total loss: 8.599 | Reg loss: 0.019 | Tree loss: 8.599 | Accuracy: 0.060547 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 019 | Total loss: 8.585 | Reg loss: 0.020 | Tree loss: 8.585 | Accuracy: 0.062500 | 0.804 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 010 / 019 | Total loss: 8.543 | Reg loss: 0.020 | Tree loss: 8.543 | Accuracy: 0.068359 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 019 | Total loss: 8.519 | Reg loss: 0.020 | Tree loss: 8.519 | Accuracy: 0.056641 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 019 | Total loss: 8.532 | Reg loss: 0.020 | Tree loss: 8.532 | Accuracy: 0.041016 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 019 | Total loss: 8.509 | Reg loss: 0.021 | Tree loss: 8.509 | Accuracy: 0.056641 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 019 | Total loss: 8.482 | Reg loss: 0.021 | Tree loss: 8.482 | Accuracy: 0.058594 | 0.805 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 019 | Total loss: 8.450 | Reg loss: 0.021 | Tree loss: 8.450 | Accuracy: 0.048828 | 0.805 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 019 | Total loss: 8.454 | Reg loss: 0.022 | Tree loss: 8.454 | Accuracy: 0.058594 | 0.805 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 019 | Total loss: 8.426 | Reg loss: 0.022 | Tree loss: 8.426 | Accuracy: 0.035156 | 0.804 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 019 | Total loss: 8.407 | Reg loss: 0.022 | Tree loss: 8.407 | Accuracy: 0.055749 | 0.804 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 019 | Total loss: 8.595 | Reg loss: 0.019 | Tree loss: 8.595 | Accuracy: 0.060547 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 019 | Total loss: 8.567 | Reg loss: 0.019 | Tree loss: 8.567 | Accuracy: 0.052734 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 019 | Total loss: 8.560 | Reg loss: 0.020 | Tree loss: 8.560 | Accuracy: 0.044922 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 019 | Total loss: 8.529 | Reg loss: 0.020 | Tree loss: 8.529 | Accuracy: 0.044922 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 019 | Total loss: 8.521 | Reg loss: 0.020 | Tree loss: 8.521 | Accuracy: 0.027344 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 019 | Total loss: 8.441 | Reg loss: 0.020 | Tree loss: 8.441 | Accuracy: 0.074219 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 019 | Total loss: 8.464 | Reg loss: 0.020 | Tree loss: 8.464 | Accuracy: 0.050781 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 019 | Total loss: 8.426 | Reg loss: 0.020 | Tree loss: 8.426 | Accuracy: 0.041016 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 019 | Total loss: 8.385 | Reg loss: 0.020 | Tree loss: 8.385 | Accuracy: 0.062500 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 019 | Total loss: 8.355 | Reg loss: 0.021 | Tree loss: 8.355 | Accuracy: 0.058594 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 019 | Total loss: 8.344 | Reg loss: 0.021 | Tree loss: 8.344 | Accuracy: 0.046875 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 019 | Total loss: 8.312 | Reg loss: 0.021 | Tree loss: 8.312 | Accuracy: 0.066406 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 019 | Total loss: 8.316 | Reg loss: 0.021 | Tree loss: 8.316 | Accuracy: 0.058594 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 019 | Total loss: 8.301 | Reg loss: 0.022 | Tree loss: 8.301 | Accuracy: 0.054688 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 019 | Total loss: 8.265 | Reg loss: 0.022 | Tree loss: 8.265 | Accuracy: 0.050781 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 019 | Total loss: 8.234 | Reg loss: 0.022 | Tree loss: 8.234 | Accuracy: 0.066406 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 019 | Total loss: 8.236 | Reg loss: 0.022 | Tree loss: 8.236 | Accuracy: 0.052734 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 019 | Total loss: 8.196 | Reg loss: 0.023 | Tree loss: 8.196 | Accuracy: 0.046875 | 0.808 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 019 | Total loss: 8.152 | Reg loss: 0.023 | Tree loss: 8.152 | Accuracy: 0.059233 | 0.808 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 019 | Total loss: 8.371 | Reg loss: 0.020 | Tree loss: 8.371 | Accuracy: 0.058594 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 019 | Total loss: 8.364 | Reg loss: 0.020 | Tree loss: 8.364 | Accuracy: 0.037109 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 019 | Total loss: 8.343 | Reg loss: 0.020 | Tree loss: 8.343 | Accuracy: 0.035156 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 019 | Total loss: 8.342 | Reg loss: 0.021 | Tree loss: 8.342 | Accuracy: 0.044922 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 019 | Total loss: 8.255 | Reg loss: 0.021 | Tree loss: 8.255 | Accuracy: 0.050781 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 019 | Total loss: 8.242 | Reg loss: 0.021 | Tree loss: 8.242 | Accuracy: 0.062500 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 019 | Total loss: 8.235 | Reg loss: 0.021 | Tree loss: 8.235 | Accuracy: 0.070312 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 019 | Total loss: 8.194 | Reg loss: 0.021 | Tree loss: 8.194 | Accuracy: 0.056641 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 019 | Total loss: 8.152 | Reg loss: 0.021 | Tree loss: 8.152 | Accuracy: 0.064453 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 019 | Total loss: 8.152 | Reg loss: 0.021 | Tree loss: 8.152 | Accuracy: 0.058594 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 019 | Total loss: 8.151 | Reg loss: 0.022 | Tree loss: 8.151 | Accuracy: 0.042969 | 0.812 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 019 | Total loss: 8.118 | Reg loss: 0.022 | Tree loss: 8.118 | Accuracy: 0.060547 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 019 | Total loss: 8.111 | Reg loss: 0.022 | Tree loss: 8.111 | Accuracy: 0.044922 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 019 | Total loss: 8.062 | Reg loss: 0.022 | Tree loss: 8.062 | Accuracy: 0.060547 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 019 | Total loss: 8.044 | Reg loss: 0.022 | Tree loss: 8.044 | Accuracy: 0.072266 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 019 | Total loss: 8.042 | Reg loss: 0.023 | Tree loss: 8.042 | Accuracy: 0.050781 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 019 | Total loss: 8.014 | Reg loss: 0.023 | Tree loss: 8.014 | Accuracy: 0.044922 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 019 | Total loss: 7.994 | Reg loss: 0.023 | Tree loss: 7.994 | Accuracy: 0.056641 | 0.813 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 019 | Total loss: 8.031 | Reg loss: 0.023 | Tree loss: 8.031 | Accuracy: 0.038328 | 0.813 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 019 | Total loss: 8.183 | Reg loss: 0.021 | Tree loss: 8.183 | Accuracy: 0.046875 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 019 | Total loss: 8.114 | Reg loss: 0.021 | Tree loss: 8.114 | Accuracy: 0.068359 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 019 | Total loss: 8.106 | Reg loss: 0.021 | Tree loss: 8.106 | Accuracy: 0.056641 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 019 | Total loss: 8.094 | Reg loss: 0.021 | Tree loss: 8.094 | Accuracy: 0.060547 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 019 | Total loss: 8.056 | Reg loss: 0.021 | Tree loss: 8.056 | Accuracy: 0.068359 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 019 | Total loss: 8.061 | Reg loss: 0.022 | Tree loss: 8.061 | Accuracy: 0.060547 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 019 | Total loss: 7.995 | Reg loss: 0.022 | Tree loss: 7.995 | Accuracy: 0.062500 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 019 | Total loss: 7.987 | Reg loss: 0.022 | Tree loss: 7.987 | Accuracy: 0.037109 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 019 | Total loss: 7.974 | Reg loss: 0.022 | Tree loss: 7.974 | Accuracy: 0.052734 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 019 | Total loss: 7.956 | Reg loss: 0.022 | Tree loss: 7.956 | Accuracy: 0.044922 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 019 | Total loss: 7.912 | Reg loss: 0.022 | Tree loss: 7.912 | Accuracy: 0.054688 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 019 | Total loss: 7.904 | Reg loss: 0.022 | Tree loss: 7.904 | Accuracy: 0.044922 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 019 | Total loss: 7.890 | Reg loss: 0.023 | Tree loss: 7.890 | Accuracy: 0.062500 | 0.816 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 013 / 019 | Total loss: 7.900 | Reg loss: 0.023 | Tree loss: 7.900 | Accuracy: 0.046875 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 019 | Total loss: 7.881 | Reg loss: 0.023 | Tree loss: 7.881 | Accuracy: 0.062500 | 0.816 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 019 | Total loss: 7.809 | Reg loss: 0.023 | Tree loss: 7.809 | Accuracy: 0.046875 | 0.817 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 019 | Total loss: 7.822 | Reg loss: 0.023 | Tree loss: 7.822 | Accuracy: 0.037109 | 0.817 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 019 | Total loss: 7.821 | Reg loss: 0.024 | Tree loss: 7.821 | Accuracy: 0.044922 | 0.817 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 019 | Total loss: 7.746 | Reg loss: 0.024 | Tree loss: 7.746 | Accuracy: 0.062718 | 0.817 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 019 | Total loss: 7.954 | Reg loss: 0.022 | Tree loss: 7.954 | Accuracy: 0.052734 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 019 | Total loss: 7.898 | Reg loss: 0.022 | Tree loss: 7.898 | Accuracy: 0.052734 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 019 | Total loss: 7.891 | Reg loss: 0.022 | Tree loss: 7.891 | Accuracy: 0.058594 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 019 | Total loss: 7.884 | Reg loss: 0.022 | Tree loss: 7.884 | Accuracy: 0.048828 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 019 | Total loss: 7.880 | Reg loss: 0.022 | Tree loss: 7.880 | Accuracy: 0.042969 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 019 | Total loss: 7.832 | Reg loss: 0.022 | Tree loss: 7.832 | Accuracy: 0.041016 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 019 | Total loss: 7.825 | Reg loss: 0.022 | Tree loss: 7.825 | Accuracy: 0.052734 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 019 | Total loss: 7.795 | Reg loss: 0.022 | Tree loss: 7.795 | Accuracy: 0.044922 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 019 | Total loss: 7.774 | Reg loss: 0.023 | Tree loss: 7.774 | Accuracy: 0.068359 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 019 | Total loss: 7.748 | Reg loss: 0.023 | Tree loss: 7.748 | Accuracy: 0.052734 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 019 | Total loss: 7.731 | Reg loss: 0.023 | Tree loss: 7.731 | Accuracy: 0.048828 | 0.82 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 019 | Total loss: 7.698 | Reg loss: 0.023 | Tree loss: 7.698 | Accuracy: 0.066406 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 019 | Total loss: 7.652 | Reg loss: 0.023 | Tree loss: 7.652 | Accuracy: 0.058594 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 019 | Total loss: 7.670 | Reg loss: 0.023 | Tree loss: 7.670 | Accuracy: 0.054688 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 019 | Total loss: 7.666 | Reg loss: 0.023 | Tree loss: 7.666 | Accuracy: 0.064453 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 019 | Total loss: 7.645 | Reg loss: 0.024 | Tree loss: 7.645 | Accuracy: 0.039062 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 019 | Total loss: 7.589 | Reg loss: 0.024 | Tree loss: 7.589 | Accuracy: 0.066406 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 019 | Total loss: 7.587 | Reg loss: 0.024 | Tree loss: 7.587 | Accuracy: 0.042969 | 0.821 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 019 | Total loss: 7.607 | Reg loss: 0.024 | Tree loss: 7.607 | Accuracy: 0.066202 | 0.821 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 019 | Total loss: 7.741 | Reg loss: 0.022 | Tree loss: 7.741 | Accuracy: 0.050781 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 019 | Total loss: 7.751 | Reg loss: 0.022 | Tree loss: 7.751 | Accuracy: 0.044922 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 019 | Total loss: 7.716 | Reg loss: 0.023 | Tree loss: 7.716 | Accuracy: 0.048828 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 019 | Total loss: 7.653 | Reg loss: 0.023 | Tree loss: 7.653 | Accuracy: 0.070312 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 019 | Total loss: 7.655 | Reg loss: 0.023 | Tree loss: 7.655 | Accuracy: 0.044922 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 019 | Total loss: 7.642 | Reg loss: 0.023 | Tree loss: 7.642 | Accuracy: 0.039062 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 019 | Total loss: 7.628 | Reg loss: 0.023 | Tree loss: 7.628 | Accuracy: 0.048828 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 019 | Total loss: 7.609 | Reg loss: 0.023 | Tree loss: 7.609 | Accuracy: 0.046875 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 019 | Total loss: 7.560 | Reg loss: 0.023 | Tree loss: 7.560 | Accuracy: 0.056641 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 019 | Total loss: 7.551 | Reg loss: 0.023 | Tree loss: 7.551 | Accuracy: 0.054688 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 019 | Total loss: 7.543 | Reg loss: 0.023 | Tree loss: 7.543 | Accuracy: 0.052734 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 019 | Total loss: 7.475 | Reg loss: 0.023 | Tree loss: 7.475 | Accuracy: 0.048828 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 019 | Total loss: 7.480 | Reg loss: 0.023 | Tree loss: 7.480 | Accuracy: 0.052734 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 019 | Total loss: 7.474 | Reg loss: 0.024 | Tree loss: 7.474 | Accuracy: 0.058594 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 019 | Total loss: 7.475 | Reg loss: 0.024 | Tree loss: 7.475 | Accuracy: 0.042969 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 019 | Total loss: 7.382 | Reg loss: 0.024 | Tree loss: 7.382 | Accuracy: 0.089844 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 019 | Total loss: 7.383 | Reg loss: 0.024 | Tree loss: 7.383 | Accuracy: 0.058594 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 019 | Total loss: 7.374 | Reg loss: 0.024 | Tree loss: 7.374 | Accuracy: 0.056641 | 0.824 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 019 | Total loss: 7.448 | Reg loss: 0.024 | Tree loss: 7.448 | Accuracy: 0.048780 | 0.824 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 019 | Total loss: 7.527 | Reg loss: 0.023 | Tree loss: 7.527 | Accuracy: 0.058594 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 019 | Total loss: 7.560 | Reg loss: 0.023 | Tree loss: 7.560 | Accuracy: 0.037109 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 019 | Total loss: 7.520 | Reg loss: 0.023 | Tree loss: 7.520 | Accuracy: 0.037109 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 019 | Total loss: 7.469 | Reg loss: 0.023 | Tree loss: 7.469 | Accuracy: 0.048828 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 019 | Total loss: 7.456 | Reg loss: 0.023 | Tree loss: 7.456 | Accuracy: 0.050781 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 019 | Total loss: 7.378 | Reg loss: 0.023 | Tree loss: 7.378 | Accuracy: 0.066406 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 019 | Total loss: 7.433 | Reg loss: 0.023 | Tree loss: 7.433 | Accuracy: 0.048828 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 019 | Total loss: 7.393 | Reg loss: 0.023 | Tree loss: 7.393 | Accuracy: 0.060547 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 019 | Total loss: 7.359 | Reg loss: 0.023 | Tree loss: 7.359 | Accuracy: 0.052734 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 019 | Total loss: 7.379 | Reg loss: 0.023 | Tree loss: 7.379 | Accuracy: 0.052734 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 019 | Total loss: 7.337 | Reg loss: 0.024 | Tree loss: 7.337 | Accuracy: 0.064453 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 019 | Total loss: 7.339 | Reg loss: 0.024 | Tree loss: 7.339 | Accuracy: 0.058594 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 019 | Total loss: 7.278 | Reg loss: 0.024 | Tree loss: 7.278 | Accuracy: 0.054688 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 019 | Total loss: 7.273 | Reg loss: 0.024 | Tree loss: 7.273 | Accuracy: 0.062500 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 019 | Total loss: 7.261 | Reg loss: 0.024 | Tree loss: 7.261 | Accuracy: 0.050781 | 0.826 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 019 | Total loss: 7.234 | Reg loss: 0.024 | Tree loss: 7.234 | Accuracy: 0.046875 | 0.827 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 016 / 019 | Total loss: 7.237 | Reg loss: 0.024 | Tree loss: 7.237 | Accuracy: 0.046875 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 019 | Total loss: 7.188 | Reg loss: 0.024 | Tree loss: 7.188 | Accuracy: 0.068359 | 0.827 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 019 | Total loss: 7.228 | Reg loss: 0.025 | Tree loss: 7.228 | Accuracy: 0.048780 | 0.827 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 019 | Total loss: 7.352 | Reg loss: 0.023 | Tree loss: 7.352 | Accuracy: 0.041016 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 019 | Total loss: 7.291 | Reg loss: 0.023 | Tree loss: 7.291 | Accuracy: 0.060547 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 019 | Total loss: 7.288 | Reg loss: 0.023 | Tree loss: 7.288 | Accuracy: 0.062500 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 019 | Total loss: 7.283 | Reg loss: 0.023 | Tree loss: 7.283 | Accuracy: 0.046875 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 019 | Total loss: 7.271 | Reg loss: 0.023 | Tree loss: 7.271 | Accuracy: 0.064453 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 019 | Total loss: 7.240 | Reg loss: 0.023 | Tree loss: 7.240 | Accuracy: 0.060547 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 019 | Total loss: 7.200 | Reg loss: 0.024 | Tree loss: 7.200 | Accuracy: 0.052734 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 019 | Total loss: 7.220 | Reg loss: 0.024 | Tree loss: 7.220 | Accuracy: 0.044922 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 019 | Total loss: 7.195 | Reg loss: 0.024 | Tree loss: 7.195 | Accuracy: 0.060547 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 019 | Total loss: 7.139 | Reg loss: 0.024 | Tree loss: 7.139 | Accuracy: 0.054688 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 019 | Total loss: 7.139 | Reg loss: 0.024 | Tree loss: 7.139 | Accuracy: 0.052734 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 019 | Total loss: 7.118 | Reg loss: 0.024 | Tree loss: 7.118 | Accuracy: 0.062500 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 019 | Total loss: 7.132 | Reg loss: 0.024 | Tree loss: 7.132 | Accuracy: 0.062500 | 0.829 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 019 | Total loss: 7.140 | Reg loss: 0.024 | Tree loss: 7.140 | Accuracy: 0.050781 | 0.828 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 019 | Total loss: 7.103 | Reg loss: 0.024 | Tree loss: 7.103 | Accuracy: 0.062500 | 0.828 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 019 | Total loss: 7.015 | Reg loss: 0.024 | Tree loss: 7.015 | Accuracy: 0.044922 | 0.828 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 019 | Total loss: 7.082 | Reg loss: 0.024 | Tree loss: 7.082 | Accuracy: 0.052734 | 0.828 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 019 | Total loss: 7.059 | Reg loss: 0.025 | Tree loss: 7.059 | Accuracy: 0.025391 | 0.828 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 019 | Total loss: 7.016 | Reg loss: 0.025 | Tree loss: 7.016 | Accuracy: 0.055749 | 0.828 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 019 | Total loss: 7.120 | Reg loss: 0.024 | Tree loss: 7.120 | Accuracy: 0.056641 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 019 | Total loss: 7.094 | Reg loss: 0.024 | Tree loss: 7.094 | Accuracy: 0.041016 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 019 | Total loss: 7.096 | Reg loss: 0.024 | Tree loss: 7.096 | Accuracy: 0.066406 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 019 | Total loss: 7.066 | Reg loss: 0.024 | Tree loss: 7.066 | Accuracy: 0.070312 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 019 | Total loss: 7.086 | Reg loss: 0.024 | Tree loss: 7.086 | Accuracy: 0.052734 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 019 | Total loss: 7.038 | Reg loss: 0.024 | Tree loss: 7.038 | Accuracy: 0.056641 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 019 | Total loss: 7.023 | Reg loss: 0.024 | Tree loss: 7.023 | Accuracy: 0.064453 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 019 | Total loss: 7.051 | Reg loss: 0.024 | Tree loss: 7.051 | Accuracy: 0.042969 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 019 | Total loss: 7.015 | Reg loss: 0.024 | Tree loss: 7.015 | Accuracy: 0.029297 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 019 | Total loss: 7.018 | Reg loss: 0.024 | Tree loss: 7.018 | Accuracy: 0.039062 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 019 | Total loss: 7.008 | Reg loss: 0.024 | Tree loss: 7.008 | Accuracy: 0.060547 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 019 | Total loss: 6.940 | Reg loss: 0.024 | Tree loss: 6.940 | Accuracy: 0.052734 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 019 | Total loss: 6.959 | Reg loss: 0.024 | Tree loss: 6.959 | Accuracy: 0.054688 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 019 | Total loss: 6.910 | Reg loss: 0.024 | Tree loss: 6.910 | Accuracy: 0.064453 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 019 | Total loss: 6.945 | Reg loss: 0.024 | Tree loss: 6.945 | Accuracy: 0.070312 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 019 | Total loss: 6.893 | Reg loss: 0.024 | Tree loss: 6.893 | Accuracy: 0.050781 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 019 | Total loss: 6.869 | Reg loss: 0.025 | Tree loss: 6.869 | Accuracy: 0.048828 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 019 | Total loss: 6.895 | Reg loss: 0.025 | Tree loss: 6.895 | Accuracy: 0.037109 | 0.831 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 019 | Total loss: 6.808 | Reg loss: 0.025 | Tree loss: 6.808 | Accuracy: 0.062718 | 0.831 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 019 | Total loss: 6.976 | Reg loss: 0.024 | Tree loss: 6.976 | Accuracy: 0.056641 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 019 | Total loss: 6.976 | Reg loss: 0.024 | Tree loss: 6.976 | Accuracy: 0.058594 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 019 | Total loss: 6.936 | Reg loss: 0.024 | Tree loss: 6.936 | Accuracy: 0.054688 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 019 | Total loss: 6.867 | Reg loss: 0.024 | Tree loss: 6.867 | Accuracy: 0.062500 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 019 | Total loss: 6.926 | Reg loss: 0.024 | Tree loss: 6.926 | Accuracy: 0.050781 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 019 | Total loss: 6.869 | Reg loss: 0.024 | Tree loss: 6.869 | Accuracy: 0.054688 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 019 | Total loss: 6.870 | Reg loss: 0.024 | Tree loss: 6.870 | Accuracy: 0.050781 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 019 | Total loss: 6.823 | Reg loss: 0.024 | Tree loss: 6.823 | Accuracy: 0.060547 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 019 | Total loss: 6.838 | Reg loss: 0.024 | Tree loss: 6.838 | Accuracy: 0.052734 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 019 | Total loss: 6.796 | Reg loss: 0.024 | Tree loss: 6.796 | Accuracy: 0.052734 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 019 | Total loss: 6.767 | Reg loss: 0.024 | Tree loss: 6.767 | Accuracy: 0.058594 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 019 | Total loss: 6.815 | Reg loss: 0.024 | Tree loss: 6.815 | Accuracy: 0.052734 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 019 | Total loss: 6.756 | Reg loss: 0.024 | Tree loss: 6.756 | Accuracy: 0.042969 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 019 | Total loss: 6.759 | Reg loss: 0.024 | Tree loss: 6.759 | Accuracy: 0.052734 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 019 | Total loss: 6.760 | Reg loss: 0.025 | Tree loss: 6.760 | Accuracy: 0.048828 | 0.833 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 019 | Total loss: 6.697 | Reg loss: 0.025 | Tree loss: 6.697 | Accuracy: 0.050781 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 019 | Total loss: 6.718 | Reg loss: 0.025 | Tree loss: 6.718 | Accuracy: 0.039062 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 019 | Total loss: 6.723 | Reg loss: 0.025 | Tree loss: 6.723 | Accuracy: 0.046875 | 0.832 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 019 | Total loss: 6.658 | Reg loss: 0.025 | Tree loss: 6.658 | Accuracy: 0.083624 | 0.832 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 019 | Total loss: 6.800 | Reg loss: 0.024 | Tree loss: 6.800 | Accuracy: 0.050781 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 019 | Total loss: 6.824 | Reg loss: 0.024 | Tree loss: 6.824 | Accuracy: 0.042969 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 019 | Total loss: 6.770 | Reg loss: 0.024 | Tree loss: 6.770 | Accuracy: 0.052734 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 019 | Total loss: 6.745 | Reg loss: 0.024 | Tree loss: 6.745 | Accuracy: 0.050781 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 019 | Total loss: 6.754 | Reg loss: 0.024 | Tree loss: 6.754 | Accuracy: 0.046875 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 019 | Total loss: 6.743 | Reg loss: 0.024 | Tree loss: 6.743 | Accuracy: 0.035156 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 019 | Total loss: 6.688 | Reg loss: 0.024 | Tree loss: 6.688 | Accuracy: 0.074219 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 019 | Total loss: 6.656 | Reg loss: 0.024 | Tree loss: 6.656 | Accuracy: 0.068359 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 019 | Total loss: 6.669 | Reg loss: 0.024 | Tree loss: 6.669 | Accuracy: 0.074219 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 019 | Total loss: 6.636 | Reg loss: 0.024 | Tree loss: 6.636 | Accuracy: 0.046875 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 019 | Total loss: 6.579 | Reg loss: 0.024 | Tree loss: 6.579 | Accuracy: 0.062500 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 019 | Total loss: 6.574 | Reg loss: 0.024 | Tree loss: 6.574 | Accuracy: 0.044922 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 019 | Total loss: 6.597 | Reg loss: 0.024 | Tree loss: 6.597 | Accuracy: 0.052734 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 019 | Total loss: 6.596 | Reg loss: 0.025 | Tree loss: 6.596 | Accuracy: 0.041016 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 019 | Total loss: 6.562 | Reg loss: 0.025 | Tree loss: 6.562 | Accuracy: 0.062500 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 019 | Total loss: 6.542 | Reg loss: 0.025 | Tree loss: 6.542 | Accuracy: 0.066406 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 019 | Total loss: 6.554 | Reg loss: 0.025 | Tree loss: 6.554 | Accuracy: 0.041016 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 019 | Total loss: 6.534 | Reg loss: 0.025 | Tree loss: 6.534 | Accuracy: 0.058594 | 0.834 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 019 | Total loss: 6.523 | Reg loss: 0.025 | Tree loss: 6.523 | Accuracy: 0.038328 | 0.834 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 019 | Total loss: 6.631 | Reg loss: 0.024 | Tree loss: 6.631 | Accuracy: 0.050781 | 0.836 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 019 | Total loss: 6.632 | Reg loss: 0.024 | Tree loss: 6.632 | Accuracy: 0.052734 | 0.836 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 019 | Total loss: 6.604 | Reg loss: 0.024 | Tree loss: 6.604 | Accuracy: 0.048828 | 0.836 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 019 | Total loss: 6.562 | Reg loss: 0.024 | Tree loss: 6.562 | Accuracy: 0.050781 | 0.836 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 019 | Total loss: 6.537 | Reg loss: 0.024 | Tree loss: 6.537 | Accuracy: 0.068359 | 0.836 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 019 | Total loss: 6.536 | Reg loss: 0.024 | Tree loss: 6.536 | Accuracy: 0.054688 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 019 | Total loss: 6.543 | Reg loss: 0.024 | Tree loss: 6.543 | Accuracy: 0.058594 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 019 | Total loss: 6.509 | Reg loss: 0.024 | Tree loss: 6.509 | Accuracy: 0.044922 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 019 | Total loss: 6.506 | Reg loss: 0.024 | Tree loss: 6.506 | Accuracy: 0.052734 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 019 | Total loss: 6.504 | Reg loss: 0.024 | Tree loss: 6.504 | Accuracy: 0.044922 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 019 | Total loss: 6.462 | Reg loss: 0.024 | Tree loss: 6.462 | Accuracy: 0.066406 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 019 | Total loss: 6.435 | Reg loss: 0.024 | Tree loss: 6.435 | Accuracy: 0.060547 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 019 | Total loss: 6.455 | Reg loss: 0.024 | Tree loss: 6.455 | Accuracy: 0.039062 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 019 | Total loss: 6.404 | Reg loss: 0.025 | Tree loss: 6.404 | Accuracy: 0.076172 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 019 | Total loss: 6.407 | Reg loss: 0.025 | Tree loss: 6.407 | Accuracy: 0.041016 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 019 | Total loss: 6.394 | Reg loss: 0.025 | Tree loss: 6.394 | Accuracy: 0.042969 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 019 | Total loss: 6.368 | Reg loss: 0.025 | Tree loss: 6.368 | Accuracy: 0.062500 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 019 | Total loss: 6.411 | Reg loss: 0.025 | Tree loss: 6.411 | Accuracy: 0.044922 | 0.835 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 019 | Total loss: 6.356 | Reg loss: 0.025 | Tree loss: 6.356 | Accuracy: 0.059233 | 0.835 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 019 | Total loss: 6.430 | Reg loss: 0.024 | Tree loss: 6.430 | Accuracy: 0.054688 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 019 | Total loss: 6.456 | Reg loss: 0.024 | Tree loss: 6.456 | Accuracy: 0.050781 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 019 | Total loss: 6.397 | Reg loss: 0.024 | Tree loss: 6.397 | Accuracy: 0.048828 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 019 | Total loss: 6.391 | Reg loss: 0.024 | Tree loss: 6.391 | Accuracy: 0.064453 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 019 | Total loss: 6.389 | Reg loss: 0.024 | Tree loss: 6.389 | Accuracy: 0.046875 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 019 | Total loss: 6.404 | Reg loss: 0.024 | Tree loss: 6.404 | Accuracy: 0.048828 | 0.837 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 019 | Total loss: 6.424 | Reg loss: 0.024 | Tree loss: 6.424 | Accuracy: 0.037109 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 019 | Total loss: 6.430 | Reg loss: 0.024 | Tree loss: 6.430 | Accuracy: 0.039062 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 019 | Total loss: 6.351 | Reg loss: 0.024 | Tree loss: 6.351 | Accuracy: 0.066406 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 019 | Total loss: 6.350 | Reg loss: 0.024 | Tree loss: 6.350 | Accuracy: 0.041016 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 019 | Total loss: 6.291 | Reg loss: 0.024 | Tree loss: 6.291 | Accuracy: 0.050781 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 019 | Total loss: 6.284 | Reg loss: 0.024 | Tree loss: 6.284 | Accuracy: 0.068359 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 019 | Total loss: 6.293 | Reg loss: 0.025 | Tree loss: 6.293 | Accuracy: 0.048828 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 019 | Total loss: 6.263 | Reg loss: 0.025 | Tree loss: 6.263 | Accuracy: 0.074219 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 019 | Total loss: 6.244 | Reg loss: 0.025 | Tree loss: 6.244 | Accuracy: 0.068359 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 019 | Total loss: 6.236 | Reg loss: 0.025 | Tree loss: 6.236 | Accuracy: 0.052734 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 019 | Total loss: 6.230 | Reg loss: 0.025 | Tree loss: 6.230 | Accuracy: 0.044922 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 019 | Total loss: 6.179 | Reg loss: 0.025 | Tree loss: 6.179 | Accuracy: 0.064453 | 0.836 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 019 | Total loss: 6.263 | Reg loss: 0.025 | Tree loss: 6.263 | Accuracy: 0.041812 | 0.836 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 019 | Total loss: 6.276 | Reg loss: 0.024 | Tree loss: 6.276 | Accuracy: 0.068359 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 019 | Total loss: 6.305 | Reg loss: 0.024 | Tree loss: 6.305 | Accuracy: 0.050781 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 019 | Total loss: 6.256 | Reg loss: 0.024 | Tree loss: 6.256 | Accuracy: 0.074219 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 019 | Total loss: 6.223 | Reg loss: 0.024 | Tree loss: 6.223 | Accuracy: 0.046875 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 019 | Total loss: 6.298 | Reg loss: 0.024 | Tree loss: 6.298 | Accuracy: 0.050781 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 019 | Total loss: 6.205 | Reg loss: 0.024 | Tree loss: 6.205 | Accuracy: 0.052734 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 019 | Total loss: 6.239 | Reg loss: 0.024 | Tree loss: 6.239 | Accuracy: 0.039062 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 019 | Total loss: 6.209 | Reg loss: 0.024 | Tree loss: 6.209 | Accuracy: 0.064453 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 019 | Total loss: 6.220 | Reg loss: 0.024 | Tree loss: 6.220 | Accuracy: 0.056641 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 019 | Total loss: 6.184 | Reg loss: 0.024 | Tree loss: 6.184 | Accuracy: 0.062500 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 019 | Total loss: 6.157 | Reg loss: 0.024 | Tree loss: 6.157 | Accuracy: 0.044922 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 019 | Total loss: 6.132 | Reg loss: 0.024 | Tree loss: 6.132 | Accuracy: 0.060547 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 019 | Total loss: 6.134 | Reg loss: 0.025 | Tree loss: 6.134 | Accuracy: 0.054688 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 019 | Total loss: 6.095 | Reg loss: 0.025 | Tree loss: 6.095 | Accuracy: 0.052734 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 019 | Total loss: 6.099 | Reg loss: 0.025 | Tree loss: 6.099 | Accuracy: 0.052734 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 019 | Total loss: 6.126 | Reg loss: 0.025 | Tree loss: 6.126 | Accuracy: 0.048828 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 019 | Total loss: 6.127 | Reg loss: 0.025 | Tree loss: 6.127 | Accuracy: 0.039062 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 019 | Total loss: 6.074 | Reg loss: 0.025 | Tree loss: 6.074 | Accuracy: 0.042969 | 0.838 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 019 | Total loss: 6.045 | Reg loss: 0.025 | Tree loss: 6.045 | Accuracy: 0.055749 | 0.837 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 019 | Total loss: 6.162 | Reg loss: 0.024 | Tree loss: 6.162 | Accuracy: 0.046875 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 019 | Total loss: 6.135 | Reg loss: 0.024 | Tree loss: 6.135 | Accuracy: 0.060547 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 019 | Total loss: 6.140 | Reg loss: 0.024 | Tree loss: 6.140 | Accuracy: 0.042969 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 019 | Total loss: 6.119 | Reg loss: 0.024 | Tree loss: 6.119 | Accuracy: 0.060547 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 019 | Total loss: 6.038 | Reg loss: 0.024 | Tree loss: 6.038 | Accuracy: 0.064453 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 019 | Total loss: 6.099 | Reg loss: 0.024 | Tree loss: 6.099 | Accuracy: 0.050781 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 019 | Total loss: 6.060 | Reg loss: 0.024 | Tree loss: 6.060 | Accuracy: 0.044922 | 0.84 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 019 | Total loss: 6.076 | Reg loss: 0.024 | Tree loss: 6.076 | Accuracy: 0.054688 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 019 | Total loss: 6.022 | Reg loss: 0.024 | Tree loss: 6.022 | Accuracy: 0.050781 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 019 | Total loss: 6.031 | Reg loss: 0.024 | Tree loss: 6.031 | Accuracy: 0.056641 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 019 | Total loss: 6.036 | Reg loss: 0.024 | Tree loss: 6.036 | Accuracy: 0.068359 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 019 | Total loss: 6.002 | Reg loss: 0.024 | Tree loss: 6.002 | Accuracy: 0.054688 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 019 | Total loss: 5.967 | Reg loss: 0.025 | Tree loss: 5.967 | Accuracy: 0.060547 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 019 | Total loss: 5.992 | Reg loss: 0.025 | Tree loss: 5.992 | Accuracy: 0.050781 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 019 | Total loss: 5.991 | Reg loss: 0.025 | Tree loss: 5.991 | Accuracy: 0.037109 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 019 | Total loss: 5.997 | Reg loss: 0.025 | Tree loss: 5.997 | Accuracy: 0.044922 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 019 | Total loss: 5.925 | Reg loss: 0.025 | Tree loss: 5.925 | Accuracy: 0.072266 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 019 | Total loss: 5.928 | Reg loss: 0.025 | Tree loss: 5.928 | Accuracy: 0.042969 | 0.839 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 019 | Total loss: 5.934 | Reg loss: 0.025 | Tree loss: 5.934 | Accuracy: 0.052265 | 0.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 019 | Total loss: 6.043 | Reg loss: 0.024 | Tree loss: 6.043 | Accuracy: 0.044922 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 019 | Total loss: 5.943 | Reg loss: 0.024 | Tree loss: 5.943 | Accuracy: 0.076172 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 019 | Total loss: 6.049 | Reg loss: 0.024 | Tree loss: 6.049 | Accuracy: 0.042969 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 019 | Total loss: 5.939 | Reg loss: 0.024 | Tree loss: 5.939 | Accuracy: 0.048828 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 019 | Total loss: 5.961 | Reg loss: 0.024 | Tree loss: 5.961 | Accuracy: 0.042969 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 019 | Total loss: 5.908 | Reg loss: 0.024 | Tree loss: 5.908 | Accuracy: 0.058594 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 019 | Total loss: 5.934 | Reg loss: 0.024 | Tree loss: 5.934 | Accuracy: 0.050781 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 019 | Total loss: 5.911 | Reg loss: 0.024 | Tree loss: 5.911 | Accuracy: 0.042969 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 019 | Total loss: 5.914 | Reg loss: 0.024 | Tree loss: 5.914 | Accuracy: 0.062500 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 019 | Total loss: 5.885 | Reg loss: 0.024 | Tree loss: 5.885 | Accuracy: 0.060547 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 019 | Total loss: 5.856 | Reg loss: 0.024 | Tree loss: 5.856 | Accuracy: 0.056641 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 019 | Total loss: 5.822 | Reg loss: 0.024 | Tree loss: 5.822 | Accuracy: 0.064453 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 019 | Total loss: 5.868 | Reg loss: 0.024 | Tree loss: 5.868 | Accuracy: 0.039062 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 019 | Total loss: 5.846 | Reg loss: 0.025 | Tree loss: 5.846 | Accuracy: 0.039062 | 0.841 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 019 | Total loss: 5.835 | Reg loss: 0.025 | Tree loss: 5.835 | Accuracy: 0.056641 | 0.84 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 019 | Total loss: 5.884 | Reg loss: 0.025 | Tree loss: 5.884 | Accuracy: 0.052734 | 0.84 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 019 | Total loss: 5.849 | Reg loss: 0.025 | Tree loss: 5.849 | Accuracy: 0.054688 | 0.84 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 019 | Total loss: 5.782 | Reg loss: 0.025 | Tree loss: 5.782 | Accuracy: 0.066406 | 0.84 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 019 | Total loss: 5.742 | Reg loss: 0.025 | Tree loss: 5.742 | Accuracy: 0.059233 | 0.84 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 019 | Total loss: 5.850 | Reg loss: 0.024 | Tree loss: 5.850 | Accuracy: 0.056641 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 019 | Total loss: 5.857 | Reg loss: 0.024 | Tree loss: 5.857 | Accuracy: 0.068359 | 0.842 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 002 / 019 | Total loss: 5.861 | Reg loss: 0.024 | Tree loss: 5.861 | Accuracy: 0.054688 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 019 | Total loss: 5.851 | Reg loss: 0.024 | Tree loss: 5.851 | Accuracy: 0.039062 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 019 | Total loss: 5.841 | Reg loss: 0.024 | Tree loss: 5.841 | Accuracy: 0.042969 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 019 | Total loss: 5.802 | Reg loss: 0.024 | Tree loss: 5.802 | Accuracy: 0.058594 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 019 | Total loss: 5.768 | Reg loss: 0.024 | Tree loss: 5.768 | Accuracy: 0.052734 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 019 | Total loss: 5.772 | Reg loss: 0.024 | Tree loss: 5.772 | Accuracy: 0.052734 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 019 | Total loss: 5.821 | Reg loss: 0.024 | Tree loss: 5.821 | Accuracy: 0.041016 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 019 | Total loss: 5.779 | Reg loss: 0.024 | Tree loss: 5.779 | Accuracy: 0.058594 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 019 | Total loss: 5.742 | Reg loss: 0.024 | Tree loss: 5.742 | Accuracy: 0.056641 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 019 | Total loss: 5.728 | Reg loss: 0.024 | Tree loss: 5.728 | Accuracy: 0.064453 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 019 | Total loss: 5.709 | Reg loss: 0.024 | Tree loss: 5.709 | Accuracy: 0.039062 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 019 | Total loss: 5.714 | Reg loss: 0.024 | Tree loss: 5.714 | Accuracy: 0.066406 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 019 | Total loss: 5.679 | Reg loss: 0.024 | Tree loss: 5.679 | Accuracy: 0.041016 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 019 | Total loss: 5.681 | Reg loss: 0.025 | Tree loss: 5.681 | Accuracy: 0.050781 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 019 | Total loss: 5.660 | Reg loss: 0.025 | Tree loss: 5.660 | Accuracy: 0.058594 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 019 | Total loss: 5.662 | Reg loss: 0.025 | Tree loss: 5.662 | Accuracy: 0.062500 | 0.842 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 019 | Total loss: 5.661 | Reg loss: 0.025 | Tree loss: 5.661 | Accuracy: 0.052265 | 0.842 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 019 | Total loss: 5.688 | Reg loss: 0.024 | Tree loss: 5.688 | Accuracy: 0.050781 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 019 | Total loss: 5.709 | Reg loss: 0.024 | Tree loss: 5.709 | Accuracy: 0.042969 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 019 | Total loss: 5.685 | Reg loss: 0.024 | Tree loss: 5.685 | Accuracy: 0.068359 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 019 | Total loss: 5.699 | Reg loss: 0.024 | Tree loss: 5.699 | Accuracy: 0.070312 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 019 | Total loss: 5.706 | Reg loss: 0.024 | Tree loss: 5.706 | Accuracy: 0.054688 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 019 | Total loss: 5.681 | Reg loss: 0.024 | Tree loss: 5.681 | Accuracy: 0.048828 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 019 | Total loss: 5.653 | Reg loss: 0.024 | Tree loss: 5.653 | Accuracy: 0.056641 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 019 | Total loss: 5.671 | Reg loss: 0.024 | Tree loss: 5.671 | Accuracy: 0.058594 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 019 | Total loss: 5.613 | Reg loss: 0.024 | Tree loss: 5.613 | Accuracy: 0.044922 | 0.843 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 019 | Total loss: 5.673 | Reg loss: 0.024 | Tree loss: 5.673 | Accuracy: 0.048828 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 019 | Total loss: 5.578 | Reg loss: 0.024 | Tree loss: 5.578 | Accuracy: 0.058594 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 019 | Total loss: 5.614 | Reg loss: 0.024 | Tree loss: 5.614 | Accuracy: 0.054688 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 019 | Total loss: 5.635 | Reg loss: 0.024 | Tree loss: 5.635 | Accuracy: 0.050781 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 019 | Total loss: 5.574 | Reg loss: 0.024 | Tree loss: 5.574 | Accuracy: 0.050781 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 019 | Total loss: 5.559 | Reg loss: 0.024 | Tree loss: 5.559 | Accuracy: 0.048828 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 019 | Total loss: 5.584 | Reg loss: 0.024 | Tree loss: 5.584 | Accuracy: 0.046875 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 019 | Total loss: 5.569 | Reg loss: 0.024 | Tree loss: 5.569 | Accuracy: 0.041016 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 019 | Total loss: 5.566 | Reg loss: 0.024 | Tree loss: 5.566 | Accuracy: 0.050781 | 0.842 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 019 | Total loss: 5.544 | Reg loss: 0.025 | Tree loss: 5.544 | Accuracy: 0.083624 | 0.842 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 019 | Total loss: 5.529 | Reg loss: 0.024 | Tree loss: 5.529 | Accuracy: 0.066406 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 019 | Total loss: 5.592 | Reg loss: 0.024 | Tree loss: 5.592 | Accuracy: 0.046875 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 019 | Total loss: 5.590 | Reg loss: 0.024 | Tree loss: 5.590 | Accuracy: 0.076172 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 019 | Total loss: 5.559 | Reg loss: 0.024 | Tree loss: 5.559 | Accuracy: 0.060547 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 019 | Total loss: 5.581 | Reg loss: 0.024 | Tree loss: 5.581 | Accuracy: 0.060547 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 019 | Total loss: 5.568 | Reg loss: 0.024 | Tree loss: 5.568 | Accuracy: 0.041016 | 0.844 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 019 | Total loss: 5.558 | Reg loss: 0.024 | Tree loss: 5.558 | Accuracy: 0.046875 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 019 | Total loss: 5.536 | Reg loss: 0.024 | Tree loss: 5.536 | Accuracy: 0.048828 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 019 | Total loss: 5.541 | Reg loss: 0.024 | Tree loss: 5.541 | Accuracy: 0.041016 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 019 | Total loss: 5.469 | Reg loss: 0.024 | Tree loss: 5.469 | Accuracy: 0.064453 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 019 | Total loss: 5.496 | Reg loss: 0.024 | Tree loss: 5.496 | Accuracy: 0.054688 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 019 | Total loss: 5.489 | Reg loss: 0.024 | Tree loss: 5.489 | Accuracy: 0.056641 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 019 | Total loss: 5.482 | Reg loss: 0.024 | Tree loss: 5.482 | Accuracy: 0.041016 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 019 | Total loss: 5.500 | Reg loss: 0.024 | Tree loss: 5.500 | Accuracy: 0.054688 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 019 | Total loss: 5.425 | Reg loss: 0.024 | Tree loss: 5.425 | Accuracy: 0.048828 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 019 | Total loss: 5.488 | Reg loss: 0.024 | Tree loss: 5.488 | Accuracy: 0.046875 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 019 | Total loss: 5.456 | Reg loss: 0.024 | Tree loss: 5.456 | Accuracy: 0.062500 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 019 | Total loss: 5.386 | Reg loss: 0.024 | Tree loss: 5.386 | Accuracy: 0.054688 | 0.843 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 019 | Total loss: 5.414 | Reg loss: 0.024 | Tree loss: 5.414 | Accuracy: 0.038328 | 0.843 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 019 | Total loss: 5.493 | Reg loss: 0.024 | Tree loss: 5.493 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 019 | Total loss: 5.449 | Reg loss: 0.024 | Tree loss: 5.449 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 019 | Total loss: 5.455 | Reg loss: 0.024 | Tree loss: 5.455 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 019 | Total loss: 5.448 | Reg loss: 0.024 | Tree loss: 5.448 | Accuracy: 0.052734 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 019 | Total loss: 5.477 | Reg loss: 0.024 | Tree loss: 5.477 | Accuracy: 0.037109 | 0.845 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 005 / 019 | Total loss: 5.430 | Reg loss: 0.024 | Tree loss: 5.430 | Accuracy: 0.062500 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 019 | Total loss: 5.389 | Reg loss: 0.024 | Tree loss: 5.389 | Accuracy: 0.050781 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 019 | Total loss: 5.395 | Reg loss: 0.024 | Tree loss: 5.395 | Accuracy: 0.056641 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 019 | Total loss: 5.387 | Reg loss: 0.024 | Tree loss: 5.387 | Accuracy: 0.056641 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 019 | Total loss: 5.334 | Reg loss: 0.024 | Tree loss: 5.334 | Accuracy: 0.056641 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 019 | Total loss: 5.406 | Reg loss: 0.024 | Tree loss: 5.406 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 019 | Total loss: 5.393 | Reg loss: 0.024 | Tree loss: 5.393 | Accuracy: 0.048828 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 019 | Total loss: 5.432 | Reg loss: 0.024 | Tree loss: 5.432 | Accuracy: 0.037109 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 019 | Total loss: 5.343 | Reg loss: 0.024 | Tree loss: 5.343 | Accuracy: 0.058594 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 019 | Total loss: 5.333 | Reg loss: 0.024 | Tree loss: 5.333 | Accuracy: 0.066406 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 019 | Total loss: 5.307 | Reg loss: 0.024 | Tree loss: 5.307 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 019 | Total loss: 5.300 | Reg loss: 0.024 | Tree loss: 5.300 | Accuracy: 0.068359 | 0.844 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 019 | Total loss: 5.353 | Reg loss: 0.024 | Tree loss: 5.353 | Accuracy: 0.041016 | 0.844 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 019 | Total loss: 5.302 | Reg loss: 0.024 | Tree loss: 5.302 | Accuracy: 0.048780 | 0.844 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 019 | Total loss: 5.400 | Reg loss: 0.024 | Tree loss: 5.400 | Accuracy: 0.050781 | 0.846 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 019 | Total loss: 5.377 | Reg loss: 0.024 | Tree loss: 5.377 | Accuracy: 0.064453 | 0.846 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 019 | Total loss: 5.319 | Reg loss: 0.024 | Tree loss: 5.319 | Accuracy: 0.060547 | 0.846 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 019 | Total loss: 5.323 | Reg loss: 0.024 | Tree loss: 5.323 | Accuracy: 0.046875 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 019 | Total loss: 5.298 | Reg loss: 0.024 | Tree loss: 5.298 | Accuracy: 0.037109 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 019 | Total loss: 5.358 | Reg loss: 0.024 | Tree loss: 5.358 | Accuracy: 0.031250 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 019 | Total loss: 5.276 | Reg loss: 0.024 | Tree loss: 5.276 | Accuracy: 0.058594 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 019 | Total loss: 5.293 | Reg loss: 0.024 | Tree loss: 5.293 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 019 | Total loss: 5.273 | Reg loss: 0.024 | Tree loss: 5.273 | Accuracy: 0.062500 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 019 | Total loss: 5.283 | Reg loss: 0.024 | Tree loss: 5.283 | Accuracy: 0.060547 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 019 | Total loss: 5.206 | Reg loss: 0.024 | Tree loss: 5.206 | Accuracy: 0.072266 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 019 | Total loss: 5.255 | Reg loss: 0.024 | Tree loss: 5.255 | Accuracy: 0.054688 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 019 | Total loss: 5.259 | Reg loss: 0.024 | Tree loss: 5.259 | Accuracy: 0.048828 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 019 | Total loss: 5.244 | Reg loss: 0.024 | Tree loss: 5.244 | Accuracy: 0.044922 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 019 | Total loss: 5.228 | Reg loss: 0.024 | Tree loss: 5.228 | Accuracy: 0.050781 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 019 | Total loss: 5.266 | Reg loss: 0.024 | Tree loss: 5.266 | Accuracy: 0.066406 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 019 | Total loss: 5.253 | Reg loss: 0.024 | Tree loss: 5.253 | Accuracy: 0.041016 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 019 | Total loss: 5.192 | Reg loss: 0.024 | Tree loss: 5.192 | Accuracy: 0.050781 | 0.845 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 019 | Total loss: 5.198 | Reg loss: 0.024 | Tree loss: 5.198 | Accuracy: 0.066202 | 0.844 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 019 | Total loss: 5.265 | Reg loss: 0.024 | Tree loss: 5.265 | Accuracy: 0.050781 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 019 | Total loss: 5.235 | Reg loss: 0.024 | Tree loss: 5.235 | Accuracy: 0.052734 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 019 | Total loss: 5.239 | Reg loss: 0.024 | Tree loss: 5.239 | Accuracy: 0.052734 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 019 | Total loss: 5.212 | Reg loss: 0.024 | Tree loss: 5.212 | Accuracy: 0.068359 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 019 | Total loss: 5.240 | Reg loss: 0.024 | Tree loss: 5.240 | Accuracy: 0.052734 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 019 | Total loss: 5.238 | Reg loss: 0.024 | Tree loss: 5.238 | Accuracy: 0.042969 | 0.846 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 019 | Total loss: 5.179 | Reg loss: 0.024 | Tree loss: 5.179 | Accuracy: 0.048828 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 019 | Total loss: 5.231 | Reg loss: 0.024 | Tree loss: 5.231 | Accuracy: 0.048828 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 019 | Total loss: 5.192 | Reg loss: 0.024 | Tree loss: 5.192 | Accuracy: 0.044922 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 019 | Total loss: 5.155 | Reg loss: 0.024 | Tree loss: 5.155 | Accuracy: 0.052734 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 019 | Total loss: 5.140 | Reg loss: 0.024 | Tree loss: 5.140 | Accuracy: 0.052734 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 019 | Total loss: 5.169 | Reg loss: 0.024 | Tree loss: 5.169 | Accuracy: 0.041016 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 019 | Total loss: 5.157 | Reg loss: 0.024 | Tree loss: 5.157 | Accuracy: 0.056641 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 019 | Total loss: 5.116 | Reg loss: 0.024 | Tree loss: 5.116 | Accuracy: 0.080078 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 019 | Total loss: 5.097 | Reg loss: 0.024 | Tree loss: 5.097 | Accuracy: 0.048828 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 019 | Total loss: 5.070 | Reg loss: 0.024 | Tree loss: 5.070 | Accuracy: 0.058594 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 019 | Total loss: 5.153 | Reg loss: 0.024 | Tree loss: 5.153 | Accuracy: 0.062500 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 019 | Total loss: 5.100 | Reg loss: 0.024 | Tree loss: 5.100 | Accuracy: 0.052734 | 0.845 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 019 | Total loss: 5.101 | Reg loss: 0.024 | Tree loss: 5.101 | Accuracy: 0.045296 | 0.845 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 019 | Total loss: 5.210 | Reg loss: 0.024 | Tree loss: 5.210 | Accuracy: 0.050781 | 0.847 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 019 | Total loss: 5.094 | Reg loss: 0.024 | Tree loss: 5.094 | Accuracy: 0.064453 | 0.847 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 019 | Total loss: 5.113 | Reg loss: 0.024 | Tree loss: 5.113 | Accuracy: 0.037109 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 019 | Total loss: 5.145 | Reg loss: 0.024 | Tree loss: 5.145 | Accuracy: 0.042969 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 019 | Total loss: 5.124 | Reg loss: 0.024 | Tree loss: 5.124 | Accuracy: 0.042969 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 019 | Total loss: 5.081 | Reg loss: 0.024 | Tree loss: 5.081 | Accuracy: 0.070312 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 019 | Total loss: 5.100 | Reg loss: 0.024 | Tree loss: 5.100 | Accuracy: 0.058594 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 019 | Total loss: 5.100 | Reg loss: 0.024 | Tree loss: 5.100 | Accuracy: 0.054688 | 0.846 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 008 / 019 | Total loss: 5.098 | Reg loss: 0.024 | Tree loss: 5.098 | Accuracy: 0.052734 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 019 | Total loss: 5.038 | Reg loss: 0.024 | Tree loss: 5.038 | Accuracy: 0.062500 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 019 | Total loss: 5.108 | Reg loss: 0.024 | Tree loss: 5.108 | Accuracy: 0.041016 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 019 | Total loss: 5.039 | Reg loss: 0.024 | Tree loss: 5.039 | Accuracy: 0.062500 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 019 | Total loss: 5.059 | Reg loss: 0.024 | Tree loss: 5.059 | Accuracy: 0.039062 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 019 | Total loss: 5.044 | Reg loss: 0.024 | Tree loss: 5.044 | Accuracy: 0.066406 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 019 | Total loss: 5.035 | Reg loss: 0.024 | Tree loss: 5.035 | Accuracy: 0.060547 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 019 | Total loss: 5.036 | Reg loss: 0.024 | Tree loss: 5.036 | Accuracy: 0.052734 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 019 | Total loss: 4.989 | Reg loss: 0.024 | Tree loss: 4.989 | Accuracy: 0.041016 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 019 | Total loss: 4.980 | Reg loss: 0.024 | Tree loss: 4.980 | Accuracy: 0.056641 | 0.846 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 019 | Total loss: 4.968 | Reg loss: 0.024 | Tree loss: 4.968 | Accuracy: 0.066202 | 0.846 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 019 | Total loss: 5.059 | Reg loss: 0.024 | Tree loss: 5.059 | Accuracy: 0.042969 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 019 | Total loss: 5.012 | Reg loss: 0.024 | Tree loss: 5.012 | Accuracy: 0.046875 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 019 | Total loss: 5.038 | Reg loss: 0.024 | Tree loss: 5.038 | Accuracy: 0.056641 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 019 | Total loss: 4.999 | Reg loss: 0.024 | Tree loss: 4.999 | Accuracy: 0.037109 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 019 | Total loss: 5.017 | Reg loss: 0.024 | Tree loss: 5.017 | Accuracy: 0.050781 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 019 | Total loss: 5.015 | Reg loss: 0.024 | Tree loss: 5.015 | Accuracy: 0.041016 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 019 | Total loss: 4.992 | Reg loss: 0.024 | Tree loss: 4.992 | Accuracy: 0.060547 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 019 | Total loss: 5.008 | Reg loss: 0.024 | Tree loss: 5.008 | Accuracy: 0.062500 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 019 | Total loss: 4.970 | Reg loss: 0.024 | Tree loss: 4.970 | Accuracy: 0.062500 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 019 | Total loss: 5.014 | Reg loss: 0.024 | Tree loss: 5.014 | Accuracy: 0.050781 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 019 | Total loss: 4.927 | Reg loss: 0.024 | Tree loss: 4.927 | Accuracy: 0.070312 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 019 | Total loss: 4.940 | Reg loss: 0.024 | Tree loss: 4.940 | Accuracy: 0.058594 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 019 | Total loss: 4.970 | Reg loss: 0.024 | Tree loss: 4.970 | Accuracy: 0.050781 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 019 | Total loss: 4.942 | Reg loss: 0.024 | Tree loss: 4.942 | Accuracy: 0.082031 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 019 | Total loss: 4.957 | Reg loss: 0.024 | Tree loss: 4.957 | Accuracy: 0.041016 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 019 | Total loss: 4.975 | Reg loss: 0.024 | Tree loss: 4.975 | Accuracy: 0.041016 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 019 | Total loss: 4.904 | Reg loss: 0.024 | Tree loss: 4.904 | Accuracy: 0.052734 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 019 | Total loss: 4.907 | Reg loss: 0.024 | Tree loss: 4.907 | Accuracy: 0.060547 | 0.847 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 019 | Total loss: 4.930 | Reg loss: 0.024 | Tree loss: 4.930 | Accuracy: 0.045296 | 0.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 019 | Total loss: 4.892 | Reg loss: 0.024 | Tree loss: 4.892 | Accuracy: 0.064453 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 019 | Total loss: 4.930 | Reg loss: 0.024 | Tree loss: 4.930 | Accuracy: 0.046875 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 019 | Total loss: 4.902 | Reg loss: 0.024 | Tree loss: 4.902 | Accuracy: 0.058594 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 019 | Total loss: 4.909 | Reg loss: 0.024 | Tree loss: 4.909 | Accuracy: 0.058594 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 019 | Total loss: 4.908 | Reg loss: 0.024 | Tree loss: 4.908 | Accuracy: 0.066406 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 019 | Total loss: 4.954 | Reg loss: 0.024 | Tree loss: 4.954 | Accuracy: 0.052734 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 019 | Total loss: 4.937 | Reg loss: 0.024 | Tree loss: 4.937 | Accuracy: 0.050781 | 0.848 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 019 | Total loss: 4.956 | Reg loss: 0.024 | Tree loss: 4.956 | Accuracy: 0.064453 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 019 | Total loss: 4.928 | Reg loss: 0.024 | Tree loss: 4.928 | Accuracy: 0.056641 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 019 | Total loss: 4.903 | Reg loss: 0.024 | Tree loss: 4.903 | Accuracy: 0.044922 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 019 | Total loss: 4.855 | Reg loss: 0.024 | Tree loss: 4.855 | Accuracy: 0.050781 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 019 | Total loss: 4.887 | Reg loss: 0.024 | Tree loss: 4.887 | Accuracy: 0.033203 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 019 | Total loss: 4.828 | Reg loss: 0.024 | Tree loss: 4.828 | Accuracy: 0.064453 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 019 | Total loss: 4.873 | Reg loss: 0.024 | Tree loss: 4.873 | Accuracy: 0.052734 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 019 | Total loss: 4.836 | Reg loss: 0.024 | Tree loss: 4.836 | Accuracy: 0.046875 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 019 | Total loss: 4.856 | Reg loss: 0.024 | Tree loss: 4.856 | Accuracy: 0.044922 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 019 | Total loss: 4.870 | Reg loss: 0.024 | Tree loss: 4.870 | Accuracy: 0.052734 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 019 | Total loss: 4.809 | Reg loss: 0.024 | Tree loss: 4.809 | Accuracy: 0.054688 | 0.847 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 019 | Total loss: 4.823 | Reg loss: 0.024 | Tree loss: 4.823 | Accuracy: 0.052265 | 0.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 019 | Total loss: 4.871 | Reg loss: 0.024 | Tree loss: 4.871 | Accuracy: 0.060547 | 0.849 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 019 | Total loss: 4.845 | Reg loss: 0.024 | Tree loss: 4.845 | Accuracy: 0.052734 | 0.849 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 019 | Total loss: 4.858 | Reg loss: 0.024 | Tree loss: 4.858 | Accuracy: 0.062500 | 0.849 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 019 | Total loss: 4.831 | Reg loss: 0.024 | Tree loss: 4.831 | Accuracy: 0.062500 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 019 | Total loss: 4.818 | Reg loss: 0.024 | Tree loss: 4.818 | Accuracy: 0.048828 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 019 | Total loss: 4.822 | Reg loss: 0.024 | Tree loss: 4.822 | Accuracy: 0.044922 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 019 | Total loss: 4.833 | Reg loss: 0.024 | Tree loss: 4.833 | Accuracy: 0.044922 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 019 | Total loss: 4.810 | Reg loss: 0.024 | Tree loss: 4.810 | Accuracy: 0.058594 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 019 | Total loss: 4.800 | Reg loss: 0.024 | Tree loss: 4.800 | Accuracy: 0.035156 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 019 | Total loss: 4.813 | Reg loss: 0.024 | Tree loss: 4.813 | Accuracy: 0.046875 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 019 | Total loss: 4.789 | Reg loss: 0.024 | Tree loss: 4.789 | Accuracy: 0.042969 | 0.848 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Batch: 011 / 019 | Total loss: 4.788 | Reg loss: 0.024 | Tree loss: 4.788 | Accuracy: 0.056641 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 019 | Total loss: 4.777 | Reg loss: 0.024 | Tree loss: 4.777 | Accuracy: 0.054688 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 019 | Total loss: 4.779 | Reg loss: 0.024 | Tree loss: 4.779 | Accuracy: 0.066406 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 019 | Total loss: 4.785 | Reg loss: 0.024 | Tree loss: 4.785 | Accuracy: 0.050781 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 019 | Total loss: 4.772 | Reg loss: 0.024 | Tree loss: 4.772 | Accuracy: 0.054688 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 019 | Total loss: 4.768 | Reg loss: 0.024 | Tree loss: 4.768 | Accuracy: 0.050781 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 019 | Total loss: 4.740 | Reg loss: 0.024 | Tree loss: 4.740 | Accuracy: 0.068359 | 0.848 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 019 | Total loss: 4.748 | Reg loss: 0.024 | Tree loss: 4.748 | Accuracy: 0.055749 | 0.848 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 019 | Total loss: 4.746 | Reg loss: 0.024 | Tree loss: 4.746 | Accuracy: 0.060547 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 019 | Total loss: 4.739 | Reg loss: 0.024 | Tree loss: 4.739 | Accuracy: 0.054688 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 019 | Total loss: 4.764 | Reg loss: 0.024 | Tree loss: 4.764 | Accuracy: 0.054688 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 019 | Total loss: 4.773 | Reg loss: 0.024 | Tree loss: 4.773 | Accuracy: 0.058594 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 019 | Total loss: 4.716 | Reg loss: 0.024 | Tree loss: 4.716 | Accuracy: 0.046875 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 019 | Total loss: 4.766 | Reg loss: 0.024 | Tree loss: 4.766 | Accuracy: 0.050781 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 019 | Total loss: 4.718 | Reg loss: 0.024 | Tree loss: 4.718 | Accuracy: 0.064453 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 019 | Total loss: 4.717 | Reg loss: 0.024 | Tree loss: 4.717 | Accuracy: 0.066406 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 019 | Total loss: 4.751 | Reg loss: 0.024 | Tree loss: 4.751 | Accuracy: 0.042969 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 019 | Total loss: 4.739 | Reg loss: 0.024 | Tree loss: 4.739 | Accuracy: 0.066406 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 019 | Total loss: 4.700 | Reg loss: 0.024 | Tree loss: 4.700 | Accuracy: 0.058594 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 019 | Total loss: 4.707 | Reg loss: 0.024 | Tree loss: 4.707 | Accuracy: 0.039062 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 019 | Total loss: 4.750 | Reg loss: 0.024 | Tree loss: 4.750 | Accuracy: 0.044922 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 019 | Total loss: 4.746 | Reg loss: 0.024 | Tree loss: 4.746 | Accuracy: 0.046875 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 019 | Total loss: 4.665 | Reg loss: 0.024 | Tree loss: 4.665 | Accuracy: 0.050781 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 019 | Total loss: 4.680 | Reg loss: 0.024 | Tree loss: 4.680 | Accuracy: 0.068359 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 019 | Total loss: 4.691 | Reg loss: 0.024 | Tree loss: 4.691 | Accuracy: 0.046875 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 019 | Total loss: 4.695 | Reg loss: 0.024 | Tree loss: 4.695 | Accuracy: 0.048828 | 0.849 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 019 | Total loss: 4.661 | Reg loss: 0.024 | Tree loss: 4.661 | Accuracy: 0.041812 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 019 | Total loss: 4.670 | Reg loss: 0.024 | Tree loss: 4.670 | Accuracy: 0.074219 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 019 | Total loss: 4.696 | Reg loss: 0.024 | Tree loss: 4.696 | Accuracy: 0.041016 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 019 | Total loss: 4.661 | Reg loss: 0.024 | Tree loss: 4.661 | Accuracy: 0.064453 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 019 | Total loss: 4.695 | Reg loss: 0.024 | Tree loss: 4.695 | Accuracy: 0.044922 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 019 | Total loss: 4.699 | Reg loss: 0.024 | Tree loss: 4.699 | Accuracy: 0.039062 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 019 | Total loss: 4.664 | Reg loss: 0.024 | Tree loss: 4.664 | Accuracy: 0.072266 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 019 | Total loss: 4.681 | Reg loss: 0.024 | Tree loss: 4.681 | Accuracy: 0.074219 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 019 | Total loss: 4.669 | Reg loss: 0.024 | Tree loss: 4.669 | Accuracy: 0.050781 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 019 | Total loss: 4.652 | Reg loss: 0.024 | Tree loss: 4.652 | Accuracy: 0.062500 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 019 | Total loss: 4.627 | Reg loss: 0.024 | Tree loss: 4.627 | Accuracy: 0.046875 | 0.85 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 019 | Total loss: 4.668 | Reg loss: 0.024 | Tree loss: 4.668 | Accuracy: 0.044922 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 019 | Total loss: 4.636 | Reg loss: 0.024 | Tree loss: 4.636 | Accuracy: 0.056641 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 019 | Total loss: 4.636 | Reg loss: 0.024 | Tree loss: 4.636 | Accuracy: 0.048828 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 019 | Total loss: 4.609 | Reg loss: 0.024 | Tree loss: 4.609 | Accuracy: 0.044922 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 019 | Total loss: 4.592 | Reg loss: 0.024 | Tree loss: 4.592 | Accuracy: 0.060547 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 019 | Total loss: 4.625 | Reg loss: 0.024 | Tree loss: 4.625 | Accuracy: 0.048828 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 019 | Total loss: 4.622 | Reg loss: 0.024 | Tree loss: 4.622 | Accuracy: 0.037109 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 019 | Total loss: 4.596 | Reg loss: 0.024 | Tree loss: 4.596 | Accuracy: 0.058594 | 0.849 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 019 | Total loss: 4.600 | Reg loss: 0.024 | Tree loss: 4.600 | Accuracy: 0.041812 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 019 | Total loss: 4.599 | Reg loss: 0.024 | Tree loss: 4.599 | Accuracy: 0.068359 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 019 | Total loss: 4.618 | Reg loss: 0.024 | Tree loss: 4.618 | Accuracy: 0.058594 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 019 | Total loss: 4.636 | Reg loss: 0.024 | Tree loss: 4.636 | Accuracy: 0.060547 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 019 | Total loss: 4.610 | Reg loss: 0.024 | Tree loss: 4.610 | Accuracy: 0.042969 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 019 | Total loss: 4.603 | Reg loss: 0.024 | Tree loss: 4.603 | Accuracy: 0.048828 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 019 | Total loss: 4.586 | Reg loss: 0.024 | Tree loss: 4.586 | Accuracy: 0.046875 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 019 | Total loss: 4.585 | Reg loss: 0.024 | Tree loss: 4.585 | Accuracy: 0.054688 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 019 | Total loss: 4.571 | Reg loss: 0.024 | Tree loss: 4.571 | Accuracy: 0.050781 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 019 | Total loss: 4.579 | Reg loss: 0.024 | Tree loss: 4.579 | Accuracy: 0.060547 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 019 | Total loss: 4.593 | Reg loss: 0.024 | Tree loss: 4.593 | Accuracy: 0.050781 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 019 | Total loss: 4.554 | Reg loss: 0.024 | Tree loss: 4.554 | Accuracy: 0.041016 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 019 | Total loss: 4.556 | Reg loss: 0.024 | Tree loss: 4.556 | Accuracy: 0.056641 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 019 | Total loss: 4.528 | Reg loss: 0.024 | Tree loss: 4.528 | Accuracy: 0.052734 | 0.85 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 019 | Total loss: 4.541 | Reg loss: 0.024 | Tree loss: 4.541 | Accuracy: 0.060547 | 0.85 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 014 / 019 | Total loss: 4.572 | Reg loss: 0.024 | Tree loss: 4.572 | Accuracy: 0.052734 | 0.849 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 019 | Total loss: 4.550 | Reg loss: 0.024 | Tree loss: 4.550 | Accuracy: 0.039062 | 0.849 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 019 | Total loss: 4.531 | Reg loss: 0.024 | Tree loss: 4.531 | Accuracy: 0.072266 | 0.849 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 019 | Total loss: 4.564 | Reg loss: 0.024 | Tree loss: 4.564 | Accuracy: 0.054688 | 0.849 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 019 | Total loss: 4.612 | Reg loss: 0.024 | Tree loss: 4.612 | Accuracy: 0.038328 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 019 | Total loss: 4.556 | Reg loss: 0.024 | Tree loss: 4.556 | Accuracy: 0.056641 | 0.851 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 019 | Total loss: 4.537 | Reg loss: 0.024 | Tree loss: 4.537 | Accuracy: 0.054688 | 0.851 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 019 | Total loss: 4.511 | Reg loss: 0.024 | Tree loss: 4.511 | Accuracy: 0.042969 | 0.851 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 019 | Total loss: 4.532 | Reg loss: 0.024 | Tree loss: 4.532 | Accuracy: 0.062500 | 0.851 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 019 | Total loss: 4.558 | Reg loss: 0.024 | Tree loss: 4.558 | Accuracy: 0.070312 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 019 | Total loss: 4.529 | Reg loss: 0.024 | Tree loss: 4.529 | Accuracy: 0.066406 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 019 | Total loss: 4.543 | Reg loss: 0.024 | Tree loss: 4.543 | Accuracy: 0.046875 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 019 | Total loss: 4.533 | Reg loss: 0.024 | Tree loss: 4.533 | Accuracy: 0.035156 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 019 | Total loss: 4.563 | Reg loss: 0.024 | Tree loss: 4.563 | Accuracy: 0.029297 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 019 | Total loss: 4.542 | Reg loss: 0.024 | Tree loss: 4.542 | Accuracy: 0.052734 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 019 | Total loss: 4.501 | Reg loss: 0.024 | Tree loss: 4.501 | Accuracy: 0.058594 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 019 | Total loss: 4.471 | Reg loss: 0.024 | Tree loss: 4.471 | Accuracy: 0.054688 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 019 | Total loss: 4.453 | Reg loss: 0.024 | Tree loss: 4.453 | Accuracy: 0.058594 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 019 | Total loss: 4.461 | Reg loss: 0.024 | Tree loss: 4.461 | Accuracy: 0.064453 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 019 | Total loss: 4.441 | Reg loss: 0.024 | Tree loss: 4.441 | Accuracy: 0.062500 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 019 | Total loss: 4.460 | Reg loss: 0.024 | Tree loss: 4.460 | Accuracy: 0.058594 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 019 | Total loss: 4.512 | Reg loss: 0.024 | Tree loss: 4.512 | Accuracy: 0.046875 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 019 | Total loss: 4.494 | Reg loss: 0.024 | Tree loss: 4.494 | Accuracy: 0.048828 | 0.85 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 019 | Total loss: 4.517 | Reg loss: 0.024 | Tree loss: 4.517 | Accuracy: 0.041812 | 0.85 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 019 | Total loss: 4.483 | Reg loss: 0.024 | Tree loss: 4.483 | Accuracy: 0.048828 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 019 | Total loss: 4.507 | Reg loss: 0.024 | Tree loss: 4.507 | Accuracy: 0.052734 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 019 | Total loss: 4.461 | Reg loss: 0.024 | Tree loss: 4.461 | Accuracy: 0.062500 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 019 | Total loss: 4.530 | Reg loss: 0.024 | Tree loss: 4.530 | Accuracy: 0.046875 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 019 | Total loss: 4.456 | Reg loss: 0.024 | Tree loss: 4.456 | Accuracy: 0.062500 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 019 | Total loss: 4.452 | Reg loss: 0.024 | Tree loss: 4.452 | Accuracy: 0.066406 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 019 | Total loss: 4.446 | Reg loss: 0.024 | Tree loss: 4.446 | Accuracy: 0.046875 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 019 | Total loss: 4.485 | Reg loss: 0.024 | Tree loss: 4.485 | Accuracy: 0.048828 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 019 | Total loss: 4.505 | Reg loss: 0.024 | Tree loss: 4.505 | Accuracy: 0.039062 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 019 | Total loss: 4.416 | Reg loss: 0.024 | Tree loss: 4.416 | Accuracy: 0.066406 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 019 | Total loss: 4.416 | Reg loss: 0.024 | Tree loss: 4.416 | Accuracy: 0.062500 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 019 | Total loss: 4.458 | Reg loss: 0.024 | Tree loss: 4.458 | Accuracy: 0.037109 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 019 | Total loss: 4.455 | Reg loss: 0.024 | Tree loss: 4.455 | Accuracy: 0.042969 | 0.851 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 019 | Total loss: 4.396 | Reg loss: 0.024 | Tree loss: 4.396 | Accuracy: 0.050781 | 0.85 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 019 | Total loss: 4.417 | Reg loss: 0.024 | Tree loss: 4.417 | Accuracy: 0.062500 | 0.85 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 019 | Total loss: 4.411 | Reg loss: 0.024 | Tree loss: 4.411 | Accuracy: 0.060547 | 0.85 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 019 | Total loss: 4.413 | Reg loss: 0.024 | Tree loss: 4.413 | Accuracy: 0.060547 | 0.85 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 019 | Total loss: 4.396 | Reg loss: 0.024 | Tree loss: 4.396 | Accuracy: 0.041016 | 0.85 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 019 | Total loss: 4.397 | Reg loss: 0.024 | Tree loss: 4.397 | Accuracy: 0.062718 | 0.85 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 019 | Total loss: 4.437 | Reg loss: 0.024 | Tree loss: 4.437 | Accuracy: 0.046875 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 019 | Total loss: 4.420 | Reg loss: 0.023 | Tree loss: 4.420 | Accuracy: 0.058594 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 019 | Total loss: 4.418 | Reg loss: 0.023 | Tree loss: 4.418 | Accuracy: 0.054688 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 019 | Total loss: 4.420 | Reg loss: 0.023 | Tree loss: 4.420 | Accuracy: 0.054688 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 019 | Total loss: 4.400 | Reg loss: 0.023 | Tree loss: 4.400 | Accuracy: 0.058594 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 019 | Total loss: 4.434 | Reg loss: 0.023 | Tree loss: 4.434 | Accuracy: 0.046875 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 019 | Total loss: 4.406 | Reg loss: 0.023 | Tree loss: 4.406 | Accuracy: 0.058594 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 019 | Total loss: 4.384 | Reg loss: 0.023 | Tree loss: 4.384 | Accuracy: 0.048828 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 019 | Total loss: 4.377 | Reg loss: 0.023 | Tree loss: 4.377 | Accuracy: 0.066406 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 019 | Total loss: 4.382 | Reg loss: 0.024 | Tree loss: 4.382 | Accuracy: 0.054688 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 019 | Total loss: 4.361 | Reg loss: 0.024 | Tree loss: 4.361 | Accuracy: 0.072266 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 019 | Total loss: 4.409 | Reg loss: 0.024 | Tree loss: 4.409 | Accuracy: 0.060547 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 019 | Total loss: 4.362 | Reg loss: 0.024 | Tree loss: 4.362 | Accuracy: 0.054688 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 019 | Total loss: 4.371 | Reg loss: 0.024 | Tree loss: 4.371 | Accuracy: 0.041016 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 019 | Total loss: 4.398 | Reg loss: 0.024 | Tree loss: 4.398 | Accuracy: 0.044922 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 019 | Total loss: 4.380 | Reg loss: 0.024 | Tree loss: 4.380 | Accuracy: 0.035156 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 019 | Total loss: 4.303 | Reg loss: 0.024 | Tree loss: 4.303 | Accuracy: 0.056641 | 0.851 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 017 / 019 | Total loss: 4.376 | Reg loss: 0.024 | Tree loss: 4.376 | Accuracy: 0.048828 | 0.851 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 019 | Total loss: 4.357 | Reg loss: 0.024 | Tree loss: 4.357 | Accuracy: 0.055749 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 019 | Total loss: 4.407 | Reg loss: 0.023 | Tree loss: 4.407 | Accuracy: 0.046875 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 019 | Total loss: 4.354 | Reg loss: 0.023 | Tree loss: 4.354 | Accuracy: 0.064453 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 019 | Total loss: 4.370 | Reg loss: 0.023 | Tree loss: 4.370 | Accuracy: 0.048828 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 019 | Total loss: 4.350 | Reg loss: 0.023 | Tree loss: 4.350 | Accuracy: 0.056641 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 019 | Total loss: 4.353 | Reg loss: 0.023 | Tree loss: 4.353 | Accuracy: 0.062500 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 019 | Total loss: 4.346 | Reg loss: 0.023 | Tree loss: 4.346 | Accuracy: 0.054688 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 019 | Total loss: 4.321 | Reg loss: 0.023 | Tree loss: 4.321 | Accuracy: 0.080078 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 019 | Total loss: 4.321 | Reg loss: 0.023 | Tree loss: 4.321 | Accuracy: 0.078125 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 019 | Total loss: 4.334 | Reg loss: 0.023 | Tree loss: 4.334 | Accuracy: 0.072266 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 019 | Total loss: 4.319 | Reg loss: 0.023 | Tree loss: 4.319 | Accuracy: 0.074219 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 019 | Total loss: 4.361 | Reg loss: 0.023 | Tree loss: 4.361 | Accuracy: 0.066406 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 019 | Total loss: 4.346 | Reg loss: 0.023 | Tree loss: 4.346 | Accuracy: 0.078125 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 019 | Total loss: 4.307 | Reg loss: 0.023 | Tree loss: 4.307 | Accuracy: 0.078125 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 019 | Total loss: 4.333 | Reg loss: 0.023 | Tree loss: 4.333 | Accuracy: 0.066406 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 019 | Total loss: 4.341 | Reg loss: 0.023 | Tree loss: 4.341 | Accuracy: 0.072266 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 019 | Total loss: 4.303 | Reg loss: 0.023 | Tree loss: 4.303 | Accuracy: 0.076172 | 0.852 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 019 | Total loss: 4.331 | Reg loss: 0.023 | Tree loss: 4.331 | Accuracy: 0.080078 | 0.851 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 019 | Total loss: 4.275 | Reg loss: 0.023 | Tree loss: 4.275 | Accuracy: 0.085938 | 0.851 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 019 | Total loss: 4.264 | Reg loss: 0.023 | Tree loss: 4.264 | Accuracy: 0.076655 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 019 | Total loss: 4.294 | Reg loss: 0.023 | Tree loss: 4.294 | Accuracy: 0.082031 | 0.853 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 019 | Total loss: 4.333 | Reg loss: 0.023 | Tree loss: 4.333 | Accuracy: 0.072266 | 0.853 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 019 | Total loss: 4.330 | Reg loss: 0.023 | Tree loss: 4.330 | Accuracy: 0.064453 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 019 | Total loss: 4.279 | Reg loss: 0.023 | Tree loss: 4.279 | Accuracy: 0.068359 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 019 | Total loss: 4.308 | Reg loss: 0.023 | Tree loss: 4.308 | Accuracy: 0.074219 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 019 | Total loss: 4.282 | Reg loss: 0.023 | Tree loss: 4.282 | Accuracy: 0.062500 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 019 | Total loss: 4.264 | Reg loss: 0.023 | Tree loss: 4.264 | Accuracy: 0.087891 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 019 | Total loss: 4.290 | Reg loss: 0.023 | Tree loss: 4.290 | Accuracy: 0.080078 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 019 | Total loss: 4.270 | Reg loss: 0.023 | Tree loss: 4.270 | Accuracy: 0.105469 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 019 | Total loss: 4.294 | Reg loss: 0.023 | Tree loss: 4.294 | Accuracy: 0.083984 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 019 | Total loss: 4.266 | Reg loss: 0.023 | Tree loss: 4.266 | Accuracy: 0.080078 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 019 | Total loss: 4.318 | Reg loss: 0.023 | Tree loss: 4.318 | Accuracy: 0.072266 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 019 | Total loss: 4.248 | Reg loss: 0.023 | Tree loss: 4.248 | Accuracy: 0.087891 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 019 | Total loss: 4.297 | Reg loss: 0.023 | Tree loss: 4.297 | Accuracy: 0.058594 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 019 | Total loss: 4.261 | Reg loss: 0.023 | Tree loss: 4.261 | Accuracy: 0.085938 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 019 | Total loss: 4.267 | Reg loss: 0.023 | Tree loss: 4.267 | Accuracy: 0.072266 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 019 | Total loss: 4.251 | Reg loss: 0.023 | Tree loss: 4.251 | Accuracy: 0.091797 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 019 | Total loss: 4.257 | Reg loss: 0.023 | Tree loss: 4.257 | Accuracy: 0.060547 | 0.852 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 019 | Total loss: 4.278 | Reg loss: 0.023 | Tree loss: 4.278 | Accuracy: 0.087108 | 0.852 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 019 | Total loss: 4.250 | Reg loss: 0.023 | Tree loss: 4.250 | Accuracy: 0.105469 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 019 | Total loss: 4.284 | Reg loss: 0.023 | Tree loss: 4.284 | Accuracy: 0.070312 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 019 | Total loss: 4.247 | Reg loss: 0.023 | Tree loss: 4.247 | Accuracy: 0.070312 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 019 | Total loss: 4.280 | Reg loss: 0.023 | Tree loss: 4.280 | Accuracy: 0.072266 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 019 | Total loss: 4.282 | Reg loss: 0.023 | Tree loss: 4.282 | Accuracy: 0.056641 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 019 | Total loss: 4.253 | Reg loss: 0.023 | Tree loss: 4.253 | Accuracy: 0.085938 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 019 | Total loss: 4.204 | Reg loss: 0.023 | Tree loss: 4.204 | Accuracy: 0.095703 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 019 | Total loss: 4.231 | Reg loss: 0.023 | Tree loss: 4.231 | Accuracy: 0.070312 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 019 | Total loss: 4.250 | Reg loss: 0.023 | Tree loss: 4.250 | Accuracy: 0.080078 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 019 | Total loss: 4.234 | Reg loss: 0.023 | Tree loss: 4.234 | Accuracy: 0.078125 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 019 | Total loss: 4.222 | Reg loss: 0.023 | Tree loss: 4.222 | Accuracy: 0.093750 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 019 | Total loss: 4.221 | Reg loss: 0.023 | Tree loss: 4.221 | Accuracy: 0.078125 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 019 | Total loss: 4.198 | Reg loss: 0.023 | Tree loss: 4.198 | Accuracy: 0.064453 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 019 | Total loss: 4.215 | Reg loss: 0.023 | Tree loss: 4.215 | Accuracy: 0.080078 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 019 | Total loss: 4.193 | Reg loss: 0.023 | Tree loss: 4.193 | Accuracy: 0.089844 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 019 | Total loss: 4.258 | Reg loss: 0.023 | Tree loss: 4.258 | Accuracy: 0.066406 | 0.853 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 019 | Total loss: 4.213 | Reg loss: 0.023 | Tree loss: 4.213 | Accuracy: 0.072266 | 0.852 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 019 | Total loss: 4.192 | Reg loss: 0.023 | Tree loss: 4.192 | Accuracy: 0.078125 | 0.852 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 019 | Total loss: 4.258 | Reg loss: 0.023 | Tree loss: 4.258 | Accuracy: 0.055749 | 0.852 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 000 / 019 | Total loss: 4.201 | Reg loss: 0.023 | Tree loss: 4.201 | Accuracy: 0.083984 | 0.854 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 019 | Total loss: 4.219 | Reg loss: 0.023 | Tree loss: 4.219 | Accuracy: 0.095703 | 0.854 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 019 | Total loss: 4.223 | Reg loss: 0.023 | Tree loss: 4.223 | Accuracy: 0.074219 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 019 | Total loss: 4.207 | Reg loss: 0.023 | Tree loss: 4.207 | Accuracy: 0.064453 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 019 | Total loss: 4.183 | Reg loss: 0.023 | Tree loss: 4.183 | Accuracy: 0.093750 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 019 | Total loss: 4.191 | Reg loss: 0.023 | Tree loss: 4.191 | Accuracy: 0.066406 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 019 | Total loss: 4.225 | Reg loss: 0.023 | Tree loss: 4.225 | Accuracy: 0.082031 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 019 | Total loss: 4.148 | Reg loss: 0.023 | Tree loss: 4.148 | Accuracy: 0.087891 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 019 | Total loss: 4.220 | Reg loss: 0.023 | Tree loss: 4.220 | Accuracy: 0.070312 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 019 | Total loss: 4.169 | Reg loss: 0.023 | Tree loss: 4.169 | Accuracy: 0.078125 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 019 | Total loss: 4.205 | Reg loss: 0.023 | Tree loss: 4.205 | Accuracy: 0.064453 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 019 | Total loss: 4.159 | Reg loss: 0.023 | Tree loss: 4.159 | Accuracy: 0.080078 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 019 | Total loss: 4.151 | Reg loss: 0.023 | Tree loss: 4.151 | Accuracy: 0.080078 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 019 | Total loss: 4.135 | Reg loss: 0.023 | Tree loss: 4.135 | Accuracy: 0.093750 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 019 | Total loss: 4.188 | Reg loss: 0.023 | Tree loss: 4.188 | Accuracy: 0.089844 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 019 | Total loss: 4.210 | Reg loss: 0.023 | Tree loss: 4.210 | Accuracy: 0.074219 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 019 | Total loss: 4.189 | Reg loss: 0.023 | Tree loss: 4.189 | Accuracy: 0.064453 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 019 | Total loss: 4.202 | Reg loss: 0.023 | Tree loss: 4.202 | Accuracy: 0.060547 | 0.853 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 019 | Total loss: 4.191 | Reg loss: 0.023 | Tree loss: 4.191 | Accuracy: 0.062718 | 0.853 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 019 | Total loss: 4.209 | Reg loss: 0.023 | Tree loss: 4.209 | Accuracy: 0.072266 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 019 | Total loss: 4.185 | Reg loss: 0.023 | Tree loss: 4.185 | Accuracy: 0.064453 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 019 | Total loss: 4.119 | Reg loss: 0.023 | Tree loss: 4.119 | Accuracy: 0.074219 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 019 | Total loss: 4.173 | Reg loss: 0.023 | Tree loss: 4.173 | Accuracy: 0.062500 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 019 | Total loss: 4.165 | Reg loss: 0.023 | Tree loss: 4.165 | Accuracy: 0.068359 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 019 | Total loss: 4.175 | Reg loss: 0.023 | Tree loss: 4.175 | Accuracy: 0.068359 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 019 | Total loss: 4.154 | Reg loss: 0.023 | Tree loss: 4.154 | Accuracy: 0.083984 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 019 | Total loss: 4.132 | Reg loss: 0.023 | Tree loss: 4.132 | Accuracy: 0.095703 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 019 | Total loss: 4.180 | Reg loss: 0.023 | Tree loss: 4.180 | Accuracy: 0.080078 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 019 | Total loss: 4.158 | Reg loss: 0.023 | Tree loss: 4.158 | Accuracy: 0.080078 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 019 | Total loss: 4.148 | Reg loss: 0.023 | Tree loss: 4.148 | Accuracy: 0.076172 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 019 | Total loss: 4.136 | Reg loss: 0.023 | Tree loss: 4.136 | Accuracy: 0.085938 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 019 | Total loss: 4.109 | Reg loss: 0.023 | Tree loss: 4.109 | Accuracy: 0.093750 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 019 | Total loss: 4.126 | Reg loss: 0.023 | Tree loss: 4.126 | Accuracy: 0.083984 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 019 | Total loss: 4.117 | Reg loss: 0.023 | Tree loss: 4.117 | Accuracy: 0.087891 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 019 | Total loss: 4.114 | Reg loss: 0.023 | Tree loss: 4.114 | Accuracy: 0.076172 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 019 | Total loss: 4.161 | Reg loss: 0.023 | Tree loss: 4.161 | Accuracy: 0.074219 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 019 | Total loss: 4.123 | Reg loss: 0.023 | Tree loss: 4.123 | Accuracy: 0.064453 | 0.854 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 019 | Total loss: 4.105 | Reg loss: 0.023 | Tree loss: 4.105 | Accuracy: 0.083624 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 019 | Total loss: 4.118 | Reg loss: 0.023 | Tree loss: 4.118 | Accuracy: 0.087891 | 0.855 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 019 | Total loss: 4.124 | Reg loss: 0.023 | Tree loss: 4.124 | Accuracy: 0.087891 | 0.855 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 019 | Total loss: 4.146 | Reg loss: 0.023 | Tree loss: 4.146 | Accuracy: 0.058594 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 019 | Total loss: 4.135 | Reg loss: 0.023 | Tree loss: 4.135 | Accuracy: 0.087891 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 019 | Total loss: 4.133 | Reg loss: 0.023 | Tree loss: 4.133 | Accuracy: 0.076172 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 019 | Total loss: 4.109 | Reg loss: 0.023 | Tree loss: 4.109 | Accuracy: 0.087891 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 019 | Total loss: 4.118 | Reg loss: 0.023 | Tree loss: 4.118 | Accuracy: 0.083984 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 019 | Total loss: 4.140 | Reg loss: 0.023 | Tree loss: 4.140 | Accuracy: 0.072266 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 019 | Total loss: 4.119 | Reg loss: 0.023 | Tree loss: 4.119 | Accuracy: 0.082031 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 019 | Total loss: 4.128 | Reg loss: 0.023 | Tree loss: 4.128 | Accuracy: 0.082031 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 019 | Total loss: 4.091 | Reg loss: 0.023 | Tree loss: 4.091 | Accuracy: 0.074219 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 019 | Total loss: 4.067 | Reg loss: 0.023 | Tree loss: 4.067 | Accuracy: 0.095703 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 019 | Total loss: 4.103 | Reg loss: 0.023 | Tree loss: 4.103 | Accuracy: 0.064453 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 019 | Total loss: 4.111 | Reg loss: 0.023 | Tree loss: 4.111 | Accuracy: 0.070312 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 019 | Total loss: 4.056 | Reg loss: 0.023 | Tree loss: 4.056 | Accuracy: 0.070312 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 019 | Total loss: 4.105 | Reg loss: 0.023 | Tree loss: 4.105 | Accuracy: 0.056641 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 019 | Total loss: 4.036 | Reg loss: 0.023 | Tree loss: 4.036 | Accuracy: 0.095703 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 019 | Total loss: 4.111 | Reg loss: 0.023 | Tree loss: 4.111 | Accuracy: 0.066406 | 0.854 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 019 | Total loss: 4.104 | Reg loss: 0.023 | Tree loss: 4.104 | Accuracy: 0.069686 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 019 | Total loss: 4.094 | Reg loss: 0.023 | Tree loss: 4.094 | Accuracy: 0.076172 | 0.855 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 019 | Total loss: 4.097 | Reg loss: 0.023 | Tree loss: 4.097 | Accuracy: 0.083984 | 0.855 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 019 | Total loss: 4.073 | Reg loss: 0.023 | Tree loss: 4.073 | Accuracy: 0.080078 | 0.855 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 003 / 019 | Total loss: 4.086 | Reg loss: 0.023 | Tree loss: 4.086 | Accuracy: 0.091797 | 0.855 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 019 | Total loss: 4.071 | Reg loss: 0.023 | Tree loss: 4.071 | Accuracy: 0.074219 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 019 | Total loss: 4.079 | Reg loss: 0.023 | Tree loss: 4.079 | Accuracy: 0.097656 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 019 | Total loss: 4.091 | Reg loss: 0.023 | Tree loss: 4.091 | Accuracy: 0.068359 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 019 | Total loss: 4.090 | Reg loss: 0.023 | Tree loss: 4.090 | Accuracy: 0.072266 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 019 | Total loss: 4.075 | Reg loss: 0.023 | Tree loss: 4.075 | Accuracy: 0.070312 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 019 | Total loss: 4.067 | Reg loss: 0.023 | Tree loss: 4.067 | Accuracy: 0.068359 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 019 | Total loss: 4.094 | Reg loss: 0.023 | Tree loss: 4.094 | Accuracy: 0.058594 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 019 | Total loss: 4.029 | Reg loss: 0.023 | Tree loss: 4.029 | Accuracy: 0.097656 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 019 | Total loss: 4.111 | Reg loss: 0.023 | Tree loss: 4.111 | Accuracy: 0.060547 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 019 | Total loss: 4.060 | Reg loss: 0.023 | Tree loss: 4.060 | Accuracy: 0.083984 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 019 | Total loss: 4.062 | Reg loss: 0.023 | Tree loss: 4.062 | Accuracy: 0.068359 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 019 | Total loss: 4.036 | Reg loss: 0.023 | Tree loss: 4.036 | Accuracy: 0.082031 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 019 | Total loss: 4.042 | Reg loss: 0.023 | Tree loss: 4.042 | Accuracy: 0.078125 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 019 | Total loss: 4.052 | Reg loss: 0.023 | Tree loss: 4.052 | Accuracy: 0.085938 | 0.854 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 019 | Total loss: 4.013 | Reg loss: 0.023 | Tree loss: 4.013 | Accuracy: 0.073171 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 019 | Total loss: 4.097 | Reg loss: 0.023 | Tree loss: 4.097 | Accuracy: 0.062500 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 019 | Total loss: 4.030 | Reg loss: 0.023 | Tree loss: 4.030 | Accuracy: 0.074219 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 019 | Total loss: 4.026 | Reg loss: 0.023 | Tree loss: 4.026 | Accuracy: 0.091797 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 019 | Total loss: 4.011 | Reg loss: 0.023 | Tree loss: 4.011 | Accuracy: 0.119141 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 019 | Total loss: 4.043 | Reg loss: 0.023 | Tree loss: 4.043 | Accuracy: 0.074219 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 019 | Total loss: 4.032 | Reg loss: 0.023 | Tree loss: 4.032 | Accuracy: 0.056641 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 019 | Total loss: 4.061 | Reg loss: 0.023 | Tree loss: 4.061 | Accuracy: 0.087891 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 019 | Total loss: 4.026 | Reg loss: 0.023 | Tree loss: 4.026 | Accuracy: 0.078125 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 019 | Total loss: 4.054 | Reg loss: 0.023 | Tree loss: 4.054 | Accuracy: 0.066406 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 019 | Total loss: 4.080 | Reg loss: 0.023 | Tree loss: 4.080 | Accuracy: 0.062500 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 019 | Total loss: 3.997 | Reg loss: 0.023 | Tree loss: 3.997 | Accuracy: 0.103516 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 019 | Total loss: 4.023 | Reg loss: 0.023 | Tree loss: 4.023 | Accuracy: 0.089844 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 019 | Total loss: 4.077 | Reg loss: 0.023 | Tree loss: 4.077 | Accuracy: 0.060547 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 019 | Total loss: 4.047 | Reg loss: 0.023 | Tree loss: 4.047 | Accuracy: 0.062500 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 019 | Total loss: 4.009 | Reg loss: 0.023 | Tree loss: 4.009 | Accuracy: 0.072266 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 019 | Total loss: 4.012 | Reg loss: 0.023 | Tree loss: 4.012 | Accuracy: 0.074219 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 019 | Total loss: 4.024 | Reg loss: 0.023 | Tree loss: 4.024 | Accuracy: 0.070312 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 019 | Total loss: 3.994 | Reg loss: 0.023 | Tree loss: 3.994 | Accuracy: 0.087891 | 0.855 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 019 | Total loss: 4.046 | Reg loss: 0.023 | Tree loss: 4.046 | Accuracy: 0.080139 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 019 | Total loss: 4.033 | Reg loss: 0.023 | Tree loss: 4.033 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 019 | Total loss: 4.035 | Reg loss: 0.023 | Tree loss: 4.035 | Accuracy: 0.064453 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 019 | Total loss: 3.982 | Reg loss: 0.023 | Tree loss: 3.982 | Accuracy: 0.095703 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 019 | Total loss: 4.026 | Reg loss: 0.023 | Tree loss: 4.026 | Accuracy: 0.064453 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 019 | Total loss: 4.012 | Reg loss: 0.023 | Tree loss: 4.012 | Accuracy: 0.076172 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 019 | Total loss: 4.018 | Reg loss: 0.023 | Tree loss: 4.018 | Accuracy: 0.060547 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 019 | Total loss: 3.992 | Reg loss: 0.023 | Tree loss: 3.992 | Accuracy: 0.072266 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 019 | Total loss: 4.003 | Reg loss: 0.023 | Tree loss: 4.003 | Accuracy: 0.080078 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 019 | Total loss: 3.998 | Reg loss: 0.023 | Tree loss: 3.998 | Accuracy: 0.083984 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 019 | Total loss: 4.039 | Reg loss: 0.023 | Tree loss: 4.039 | Accuracy: 0.072266 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 019 | Total loss: 3.955 | Reg loss: 0.023 | Tree loss: 3.955 | Accuracy: 0.082031 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 019 | Total loss: 3.978 | Reg loss: 0.023 | Tree loss: 3.978 | Accuracy: 0.082031 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 019 | Total loss: 3.985 | Reg loss: 0.023 | Tree loss: 3.985 | Accuracy: 0.083984 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 019 | Total loss: 3.999 | Reg loss: 0.023 | Tree loss: 3.999 | Accuracy: 0.093750 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 019 | Total loss: 4.000 | Reg loss: 0.023 | Tree loss: 4.000 | Accuracy: 0.072266 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 019 | Total loss: 4.044 | Reg loss: 0.023 | Tree loss: 4.044 | Accuracy: 0.078125 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 019 | Total loss: 3.983 | Reg loss: 0.023 | Tree loss: 3.983 | Accuracy: 0.093750 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 019 | Total loss: 3.988 | Reg loss: 0.023 | Tree loss: 3.988 | Accuracy: 0.072266 | 0.855 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 019 | Total loss: 3.990 | Reg loss: 0.023 | Tree loss: 3.990 | Accuracy: 0.073171 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 019 | Total loss: 3.995 | Reg loss: 0.023 | Tree loss: 3.995 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 019 | Total loss: 3.964 | Reg loss: 0.023 | Tree loss: 3.964 | Accuracy: 0.089844 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 019 | Total loss: 3.990 | Reg loss: 0.023 | Tree loss: 3.990 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 019 | Total loss: 3.965 | Reg loss: 0.023 | Tree loss: 3.965 | Accuracy: 0.082031 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 019 | Total loss: 3.988 | Reg loss: 0.023 | Tree loss: 3.988 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 019 | Total loss: 3.977 | Reg loss: 0.023 | Tree loss: 3.977 | Accuracy: 0.062500 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 006 / 019 | Total loss: 3.969 | Reg loss: 0.023 | Tree loss: 3.969 | Accuracy: 0.087891 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 019 | Total loss: 3.985 | Reg loss: 0.023 | Tree loss: 3.985 | Accuracy: 0.080078 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 019 | Total loss: 3.973 | Reg loss: 0.023 | Tree loss: 3.973 | Accuracy: 0.080078 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 019 | Total loss: 3.948 | Reg loss: 0.023 | Tree loss: 3.948 | Accuracy: 0.105469 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 019 | Total loss: 3.988 | Reg loss: 0.023 | Tree loss: 3.988 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 019 | Total loss: 3.961 | Reg loss: 0.023 | Tree loss: 3.961 | Accuracy: 0.082031 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 019 | Total loss: 3.997 | Reg loss: 0.023 | Tree loss: 3.997 | Accuracy: 0.066406 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 019 | Total loss: 3.957 | Reg loss: 0.023 | Tree loss: 3.957 | Accuracy: 0.072266 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 019 | Total loss: 3.980 | Reg loss: 0.023 | Tree loss: 3.980 | Accuracy: 0.074219 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 019 | Total loss: 3.948 | Reg loss: 0.023 | Tree loss: 3.948 | Accuracy: 0.087891 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 019 | Total loss: 3.915 | Reg loss: 0.023 | Tree loss: 3.915 | Accuracy: 0.082031 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 019 | Total loss: 3.997 | Reg loss: 0.023 | Tree loss: 3.997 | Accuracy: 0.064453 | 0.856 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 019 | Total loss: 3.997 | Reg loss: 0.023 | Tree loss: 3.997 | Accuracy: 0.073171 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 019 | Total loss: 3.944 | Reg loss: 0.023 | Tree loss: 3.944 | Accuracy: 0.087891 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 019 | Total loss: 3.978 | Reg loss: 0.023 | Tree loss: 3.978 | Accuracy: 0.062500 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 019 | Total loss: 3.975 | Reg loss: 0.023 | Tree loss: 3.975 | Accuracy: 0.076172 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 019 | Total loss: 3.947 | Reg loss: 0.023 | Tree loss: 3.947 | Accuracy: 0.082031 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 019 | Total loss: 3.941 | Reg loss: 0.023 | Tree loss: 3.941 | Accuracy: 0.087891 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 019 | Total loss: 3.948 | Reg loss: 0.023 | Tree loss: 3.948 | Accuracy: 0.068359 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 019 | Total loss: 3.957 | Reg loss: 0.023 | Tree loss: 3.957 | Accuracy: 0.060547 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 019 | Total loss: 3.935 | Reg loss: 0.023 | Tree loss: 3.935 | Accuracy: 0.066406 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 019 | Total loss: 3.961 | Reg loss: 0.023 | Tree loss: 3.961 | Accuracy: 0.058594 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 019 | Total loss: 3.938 | Reg loss: 0.023 | Tree loss: 3.938 | Accuracy: 0.093750 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 019 | Total loss: 3.926 | Reg loss: 0.023 | Tree loss: 3.926 | Accuracy: 0.074219 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 019 | Total loss: 3.942 | Reg loss: 0.023 | Tree loss: 3.942 | Accuracy: 0.082031 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 019 | Total loss: 3.951 | Reg loss: 0.023 | Tree loss: 3.951 | Accuracy: 0.078125 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 019 | Total loss: 3.916 | Reg loss: 0.023 | Tree loss: 3.916 | Accuracy: 0.087891 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 019 | Total loss: 3.958 | Reg loss: 0.023 | Tree loss: 3.958 | Accuracy: 0.070312 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 019 | Total loss: 3.962 | Reg loss: 0.023 | Tree loss: 3.962 | Accuracy: 0.078125 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 019 | Total loss: 3.901 | Reg loss: 0.023 | Tree loss: 3.901 | Accuracy: 0.099609 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 019 | Total loss: 3.922 | Reg loss: 0.023 | Tree loss: 3.922 | Accuracy: 0.076172 | 0.856 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 019 | Total loss: 3.928 | Reg loss: 0.023 | Tree loss: 3.928 | Accuracy: 0.087108 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 019 | Total loss: 3.942 | Reg loss: 0.023 | Tree loss: 3.942 | Accuracy: 0.074219 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 019 | Total loss: 3.929 | Reg loss: 0.023 | Tree loss: 3.929 | Accuracy: 0.080078 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 019 | Total loss: 3.921 | Reg loss: 0.023 | Tree loss: 3.921 | Accuracy: 0.093750 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 019 | Total loss: 3.946 | Reg loss: 0.023 | Tree loss: 3.946 | Accuracy: 0.080078 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 019 | Total loss: 3.914 | Reg loss: 0.023 | Tree loss: 3.914 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 019 | Total loss: 3.908 | Reg loss: 0.023 | Tree loss: 3.908 | Accuracy: 0.070312 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 019 | Total loss: 3.921 | Reg loss: 0.023 | Tree loss: 3.921 | Accuracy: 0.089844 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 019 | Total loss: 3.954 | Reg loss: 0.023 | Tree loss: 3.954 | Accuracy: 0.060547 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 019 | Total loss: 3.921 | Reg loss: 0.023 | Tree loss: 3.921 | Accuracy: 0.060547 | 0.857 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 019 | Total loss: 3.921 | Reg loss: 0.023 | Tree loss: 3.921 | Accuracy: 0.076172 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 019 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.095703 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 019 | Total loss: 3.914 | Reg loss: 0.023 | Tree loss: 3.914 | Accuracy: 0.072266 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 019 | Total loss: 3.893 | Reg loss: 0.023 | Tree loss: 3.893 | Accuracy: 0.083984 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 019 | Total loss: 3.897 | Reg loss: 0.023 | Tree loss: 3.897 | Accuracy: 0.085938 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 019 | Total loss: 3.920 | Reg loss: 0.023 | Tree loss: 3.920 | Accuracy: 0.091797 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 019 | Total loss: 3.946 | Reg loss: 0.023 | Tree loss: 3.946 | Accuracy: 0.056641 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 019 | Total loss: 3.863 | Reg loss: 0.023 | Tree loss: 3.863 | Accuracy: 0.083984 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 019 | Total loss: 3.906 | Reg loss: 0.023 | Tree loss: 3.906 | Accuracy: 0.074219 | 0.856 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 019 | Total loss: 3.922 | Reg loss: 0.023 | Tree loss: 3.922 | Accuracy: 0.066202 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 019 | Total loss: 3.888 | Reg loss: 0.023 | Tree loss: 3.888 | Accuracy: 0.085938 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 019 | Total loss: 3.900 | Reg loss: 0.023 | Tree loss: 3.900 | Accuracy: 0.076172 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 019 | Total loss: 3.924 | Reg loss: 0.023 | Tree loss: 3.924 | Accuracy: 0.070312 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 019 | Total loss: 3.879 | Reg loss: 0.023 | Tree loss: 3.879 | Accuracy: 0.089844 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 019 | Total loss: 3.880 | Reg loss: 0.023 | Tree loss: 3.880 | Accuracy: 0.085938 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 019 | Total loss: 3.920 | Reg loss: 0.023 | Tree loss: 3.920 | Accuracy: 0.060547 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 019 | Total loss: 3.865 | Reg loss: 0.023 | Tree loss: 3.865 | Accuracy: 0.095703 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 019 | Total loss: 3.911 | Reg loss: 0.023 | Tree loss: 3.911 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 019 | Total loss: 3.896 | Reg loss: 0.023 | Tree loss: 3.896 | Accuracy: 0.085938 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 009 / 019 | Total loss: 3.897 | Reg loss: 0.023 | Tree loss: 3.897 | Accuracy: 0.064453 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 019 | Total loss: 3.886 | Reg loss: 0.023 | Tree loss: 3.886 | Accuracy: 0.070312 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 019 | Total loss: 3.880 | Reg loss: 0.023 | Tree loss: 3.880 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 019 | Total loss: 3.901 | Reg loss: 0.023 | Tree loss: 3.901 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 019 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.076172 | 0.857 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 019 | Total loss: 3.901 | Reg loss: 0.023 | Tree loss: 3.901 | Accuracy: 0.074219 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 019 | Total loss: 3.883 | Reg loss: 0.023 | Tree loss: 3.883 | Accuracy: 0.076172 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 019 | Total loss: 3.860 | Reg loss: 0.023 | Tree loss: 3.860 | Accuracy: 0.103516 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 019 | Total loss: 3.892 | Reg loss: 0.023 | Tree loss: 3.892 | Accuracy: 0.085938 | 0.856 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 019 | Total loss: 3.885 | Reg loss: 0.023 | Tree loss: 3.885 | Accuracy: 0.059233 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 019 | Total loss: 3.902 | Reg loss: 0.023 | Tree loss: 3.902 | Accuracy: 0.078125 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 019 | Total loss: 3.878 | Reg loss: 0.023 | Tree loss: 3.878 | Accuracy: 0.062500 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 019 | Total loss: 3.895 | Reg loss: 0.023 | Tree loss: 3.895 | Accuracy: 0.087891 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 019 | Total loss: 3.927 | Reg loss: 0.023 | Tree loss: 3.927 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 019 | Total loss: 3.891 | Reg loss: 0.023 | Tree loss: 3.891 | Accuracy: 0.078125 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 019 | Total loss: 3.874 | Reg loss: 0.023 | Tree loss: 3.874 | Accuracy: 0.097656 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 019 | Total loss: 3.883 | Reg loss: 0.023 | Tree loss: 3.883 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 019 | Total loss: 3.862 | Reg loss: 0.023 | Tree loss: 3.862 | Accuracy: 0.082031 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 019 | Total loss: 3.864 | Reg loss: 0.023 | Tree loss: 3.864 | Accuracy: 0.058594 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 019 | Total loss: 3.848 | Reg loss: 0.023 | Tree loss: 3.848 | Accuracy: 0.076172 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 019 | Total loss: 3.849 | Reg loss: 0.023 | Tree loss: 3.849 | Accuracy: 0.080078 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 019 | Total loss: 3.881 | Reg loss: 0.023 | Tree loss: 3.881 | Accuracy: 0.060547 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 019 | Total loss: 3.829 | Reg loss: 0.023 | Tree loss: 3.829 | Accuracy: 0.085938 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 019 | Total loss: 3.865 | Reg loss: 0.023 | Tree loss: 3.865 | Accuracy: 0.070312 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 019 | Total loss: 3.857 | Reg loss: 0.023 | Tree loss: 3.857 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 019 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.101562 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 019 | Total loss: 3.877 | Reg loss: 0.023 | Tree loss: 3.877 | Accuracy: 0.062500 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 019 | Total loss: 3.837 | Reg loss: 0.023 | Tree loss: 3.837 | Accuracy: 0.087891 | 0.857 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 019 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.101045 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 019 | Total loss: 3.872 | Reg loss: 0.023 | Tree loss: 3.872 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 019 | Total loss: 3.850 | Reg loss: 0.023 | Tree loss: 3.850 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 019 | Total loss: 3.853 | Reg loss: 0.023 | Tree loss: 3.853 | Accuracy: 0.076172 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 019 | Total loss: 3.886 | Reg loss: 0.023 | Tree loss: 3.886 | Accuracy: 0.074219 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 019 | Total loss: 3.871 | Reg loss: 0.023 | Tree loss: 3.871 | Accuracy: 0.087891 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 019 | Total loss: 3.881 | Reg loss: 0.023 | Tree loss: 3.881 | Accuracy: 0.074219 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 019 | Total loss: 3.868 | Reg loss: 0.023 | Tree loss: 3.868 | Accuracy: 0.080078 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 019 | Total loss: 3.839 | Reg loss: 0.023 | Tree loss: 3.839 | Accuracy: 0.089844 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 019 | Total loss: 3.837 | Reg loss: 0.023 | Tree loss: 3.837 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 019 | Total loss: 3.858 | Reg loss: 0.023 | Tree loss: 3.858 | Accuracy: 0.083984 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 019 | Total loss: 3.798 | Reg loss: 0.023 | Tree loss: 3.798 | Accuracy: 0.101562 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 019 | Total loss: 3.842 | Reg loss: 0.023 | Tree loss: 3.842 | Accuracy: 0.080078 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 019 | Total loss: 3.824 | Reg loss: 0.023 | Tree loss: 3.824 | Accuracy: 0.056641 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 019 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.076172 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 019 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.085938 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 019 | Total loss: 3.794 | Reg loss: 0.023 | Tree loss: 3.794 | Accuracy: 0.093750 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 019 | Total loss: 3.857 | Reg loss: 0.023 | Tree loss: 3.857 | Accuracy: 0.068359 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 019 | Total loss: 3.873 | Reg loss: 0.023 | Tree loss: 3.873 | Accuracy: 0.064453 | 0.857 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 019 | Total loss: 3.822 | Reg loss: 0.023 | Tree loss: 3.822 | Accuracy: 0.059233 | 0.857 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 019 | Total loss: 3.824 | Reg loss: 0.023 | Tree loss: 3.824 | Accuracy: 0.097656 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 019 | Total loss: 3.826 | Reg loss: 0.023 | Tree loss: 3.826 | Accuracy: 0.089844 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 019 | Total loss: 3.836 | Reg loss: 0.023 | Tree loss: 3.836 | Accuracy: 0.076172 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 019 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.089844 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 019 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.072266 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 019 | Total loss: 3.808 | Reg loss: 0.023 | Tree loss: 3.808 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 019 | Total loss: 3.823 | Reg loss: 0.023 | Tree loss: 3.823 | Accuracy: 0.082031 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 019 | Total loss: 3.833 | Reg loss: 0.023 | Tree loss: 3.833 | Accuracy: 0.083984 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 019 | Total loss: 3.840 | Reg loss: 0.023 | Tree loss: 3.840 | Accuracy: 0.082031 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 019 | Total loss: 3.876 | Reg loss: 0.023 | Tree loss: 3.876 | Accuracy: 0.058594 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 019 | Total loss: 3.806 | Reg loss: 0.023 | Tree loss: 3.806 | Accuracy: 0.093750 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 019 | Total loss: 3.818 | Reg loss: 0.023 | Tree loss: 3.818 | Accuracy: 0.068359 | 0.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 012 / 019 | Total loss: 3.843 | Reg loss: 0.023 | Tree loss: 3.843 | Accuracy: 0.066406 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 019 | Total loss: 3.825 | Reg loss: 0.023 | Tree loss: 3.825 | Accuracy: 0.089844 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 019 | Total loss: 3.824 | Reg loss: 0.023 | Tree loss: 3.824 | Accuracy: 0.072266 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 019 | Total loss: 3.828 | Reg loss: 0.023 | Tree loss: 3.828 | Accuracy: 0.080078 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 019 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.060547 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 019 | Total loss: 3.783 | Reg loss: 0.023 | Tree loss: 3.783 | Accuracy: 0.072266 | 0.857 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 019 | Total loss: 3.802 | Reg loss: 0.023 | Tree loss: 3.802 | Accuracy: 0.052265 | 0.857 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 019 | Total loss: 3.802 | Reg loss: 0.023 | Tree loss: 3.802 | Accuracy: 0.095703 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 019 | Total loss: 3.831 | Reg loss: 0.023 | Tree loss: 3.831 | Accuracy: 0.085938 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 019 | Total loss: 3.806 | Reg loss: 0.023 | Tree loss: 3.806 | Accuracy: 0.076172 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 019 | Total loss: 3.821 | Reg loss: 0.023 | Tree loss: 3.821 | Accuracy: 0.087891 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 019 | Total loss: 3.808 | Reg loss: 0.023 | Tree loss: 3.808 | Accuracy: 0.066406 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 019 | Total loss: 3.834 | Reg loss: 0.023 | Tree loss: 3.834 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 019 | Total loss: 3.814 | Reg loss: 0.023 | Tree loss: 3.814 | Accuracy: 0.062500 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 019 | Total loss: 3.794 | Reg loss: 0.023 | Tree loss: 3.794 | Accuracy: 0.087891 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 019 | Total loss: 3.808 | Reg loss: 0.023 | Tree loss: 3.808 | Accuracy: 0.066406 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 019 | Total loss: 3.771 | Reg loss: 0.023 | Tree loss: 3.771 | Accuracy: 0.085938 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 019 | Total loss: 3.820 | Reg loss: 0.023 | Tree loss: 3.820 | Accuracy: 0.093750 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 019 | Total loss: 3.804 | Reg loss: 0.023 | Tree loss: 3.804 | Accuracy: 0.078125 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 019 | Total loss: 3.801 | Reg loss: 0.023 | Tree loss: 3.801 | Accuracy: 0.072266 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 019 | Total loss: 3.796 | Reg loss: 0.023 | Tree loss: 3.796 | Accuracy: 0.083984 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 019 | Total loss: 3.834 | Reg loss: 0.023 | Tree loss: 3.834 | Accuracy: 0.068359 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 019 | Total loss: 3.766 | Reg loss: 0.023 | Tree loss: 3.766 | Accuracy: 0.068359 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 019 | Total loss: 3.795 | Reg loss: 0.023 | Tree loss: 3.795 | Accuracy: 0.070312 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 019 | Total loss: 3.800 | Reg loss: 0.023 | Tree loss: 3.800 | Accuracy: 0.076172 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 019 | Total loss: 3.784 | Reg loss: 0.023 | Tree loss: 3.784 | Accuracy: 0.069686 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 019 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 019 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 019 | Total loss: 3.807 | Reg loss: 0.023 | Tree loss: 3.807 | Accuracy: 0.062500 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 019 | Total loss: 3.800 | Reg loss: 0.023 | Tree loss: 3.800 | Accuracy: 0.076172 | 0.859 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 019 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.060547 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 019 | Total loss: 3.732 | Reg loss: 0.023 | Tree loss: 3.732 | Accuracy: 0.095703 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 019 | Total loss: 3.790 | Reg loss: 0.023 | Tree loss: 3.790 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 019 | Total loss: 3.745 | Reg loss: 0.023 | Tree loss: 3.745 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 019 | Total loss: 3.805 | Reg loss: 0.023 | Tree loss: 3.805 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 019 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 019 | Total loss: 3.849 | Reg loss: 0.023 | Tree loss: 3.849 | Accuracy: 0.068359 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 019 | Total loss: 3.742 | Reg loss: 0.023 | Tree loss: 3.742 | Accuracy: 0.095703 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 019 | Total loss: 3.754 | Reg loss: 0.023 | Tree loss: 3.754 | Accuracy: 0.095703 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 019 | Total loss: 3.812 | Reg loss: 0.023 | Tree loss: 3.812 | Accuracy: 0.070312 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 019 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.080078 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 019 | Total loss: 3.838 | Reg loss: 0.023 | Tree loss: 3.838 | Accuracy: 0.058594 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 019 | Total loss: 3.764 | Reg loss: 0.023 | Tree loss: 3.764 | Accuracy: 0.093750 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 019 | Total loss: 3.769 | Reg loss: 0.023 | Tree loss: 3.769 | Accuracy: 0.085938 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 019 | Total loss: 3.807 | Reg loss: 0.023 | Tree loss: 3.807 | Accuracy: 0.069686 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 019 | Total loss: 3.788 | Reg loss: 0.023 | Tree loss: 3.788 | Accuracy: 0.064453 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 019 | Total loss: 3.765 | Reg loss: 0.023 | Tree loss: 3.765 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 019 | Total loss: 3.833 | Reg loss: 0.023 | Tree loss: 3.833 | Accuracy: 0.064453 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 019 | Total loss: 3.786 | Reg loss: 0.023 | Tree loss: 3.786 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 019 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 019 | Total loss: 3.753 | Reg loss: 0.023 | Tree loss: 3.753 | Accuracy: 0.062500 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 019 | Total loss: 3.784 | Reg loss: 0.023 | Tree loss: 3.784 | Accuracy: 0.083984 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 019 | Total loss: 3.753 | Reg loss: 0.023 | Tree loss: 3.753 | Accuracy: 0.091797 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 019 | Total loss: 3.781 | Reg loss: 0.023 | Tree loss: 3.781 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 019 | Total loss: 3.768 | Reg loss: 0.023 | Tree loss: 3.768 | Accuracy: 0.089844 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 019 | Total loss: 3.746 | Reg loss: 0.023 | Tree loss: 3.746 | Accuracy: 0.093750 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 019 | Total loss: 3.774 | Reg loss: 0.023 | Tree loss: 3.774 | Accuracy: 0.083984 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 019 | Total loss: 3.760 | Reg loss: 0.023 | Tree loss: 3.760 | Accuracy: 0.072266 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 019 | Total loss: 3.757 | Reg loss: 0.023 | Tree loss: 3.757 | Accuracy: 0.085938 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 019 | Total loss: 3.779 | Reg loss: 0.023 | Tree loss: 3.779 | Accuracy: 0.072266 | 0.858 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 015 / 019 | Total loss: 3.759 | Reg loss: 0.023 | Tree loss: 3.759 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 019 | Total loss: 3.739 | Reg loss: 0.023 | Tree loss: 3.739 | Accuracy: 0.082031 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 019 | Total loss: 3.770 | Reg loss: 0.023 | Tree loss: 3.770 | Accuracy: 0.074219 | 0.858 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 019 | Total loss: 3.723 | Reg loss: 0.023 | Tree loss: 3.723 | Accuracy: 0.090592 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 019 | Total loss: 3.794 | Reg loss: 0.023 | Tree loss: 3.794 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 019 | Total loss: 3.792 | Reg loss: 0.023 | Tree loss: 3.792 | Accuracy: 0.080078 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 019 | Total loss: 3.747 | Reg loss: 0.023 | Tree loss: 3.747 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 019 | Total loss: 3.759 | Reg loss: 0.023 | Tree loss: 3.759 | Accuracy: 0.080078 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 019 | Total loss: 3.742 | Reg loss: 0.023 | Tree loss: 3.742 | Accuracy: 0.101562 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 019 | Total loss: 3.738 | Reg loss: 0.023 | Tree loss: 3.738 | Accuracy: 0.089844 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 019 | Total loss: 3.773 | Reg loss: 0.023 | Tree loss: 3.773 | Accuracy: 0.085938 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 019 | Total loss: 3.720 | Reg loss: 0.023 | Tree loss: 3.720 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 019 | Total loss: 3.730 | Reg loss: 0.023 | Tree loss: 3.730 | Accuracy: 0.076172 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 019 | Total loss: 3.753 | Reg loss: 0.023 | Tree loss: 3.753 | Accuracy: 0.089844 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 019 | Total loss: 3.777 | Reg loss: 0.023 | Tree loss: 3.777 | Accuracy: 0.056641 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 019 | Total loss: 3.797 | Reg loss: 0.023 | Tree loss: 3.797 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 019 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 019 | Total loss: 3.746 | Reg loss: 0.023 | Tree loss: 3.746 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 019 | Total loss: 3.699 | Reg loss: 0.023 | Tree loss: 3.699 | Accuracy: 0.076172 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 019 | Total loss: 3.708 | Reg loss: 0.023 | Tree loss: 3.708 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 019 | Total loss: 3.768 | Reg loss: 0.023 | Tree loss: 3.768 | Accuracy: 0.062500 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 019 | Total loss: 3.781 | Reg loss: 0.023 | Tree loss: 3.781 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 019 | Total loss: 3.748 | Reg loss: 0.023 | Tree loss: 3.748 | Accuracy: 0.055749 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 019 | Total loss: 3.738 | Reg loss: 0.023 | Tree loss: 3.738 | Accuracy: 0.083984 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 019 | Total loss: 3.695 | Reg loss: 0.023 | Tree loss: 3.695 | Accuracy: 0.093750 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 019 | Total loss: 3.737 | Reg loss: 0.023 | Tree loss: 3.737 | Accuracy: 0.060547 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 019 | Total loss: 3.745 | Reg loss: 0.023 | Tree loss: 3.745 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 019 | Total loss: 3.726 | Reg loss: 0.023 | Tree loss: 3.726 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 019 | Total loss: 3.783 | Reg loss: 0.023 | Tree loss: 3.783 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 019 | Total loss: 3.717 | Reg loss: 0.023 | Tree loss: 3.717 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 019 | Total loss: 3.738 | Reg loss: 0.023 | Tree loss: 3.738 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 019 | Total loss: 3.758 | Reg loss: 0.023 | Tree loss: 3.758 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 019 | Total loss: 3.754 | Reg loss: 0.023 | Tree loss: 3.754 | Accuracy: 0.068359 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 019 | Total loss: 3.763 | Reg loss: 0.023 | Tree loss: 3.763 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 019 | Total loss: 3.747 | Reg loss: 0.023 | Tree loss: 3.747 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 019 | Total loss: 3.741 | Reg loss: 0.023 | Tree loss: 3.741 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 019 | Total loss: 3.747 | Reg loss: 0.023 | Tree loss: 3.747 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 019 | Total loss: 3.727 | Reg loss: 0.023 | Tree loss: 3.727 | Accuracy: 0.089844 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 019 | Total loss: 3.710 | Reg loss: 0.023 | Tree loss: 3.710 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 019 | Total loss: 3.706 | Reg loss: 0.023 | Tree loss: 3.706 | Accuracy: 0.099609 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 019 | Total loss: 3.743 | Reg loss: 0.023 | Tree loss: 3.743 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 019 | Total loss: 3.727 | Reg loss: 0.023 | Tree loss: 3.727 | Accuracy: 0.069686 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 019 | Total loss: 3.748 | Reg loss: 0.023 | Tree loss: 3.748 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 019 | Total loss: 3.706 | Reg loss: 0.023 | Tree loss: 3.706 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 019 | Total loss: 3.733 | Reg loss: 0.023 | Tree loss: 3.733 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 019 | Total loss: 3.732 | Reg loss: 0.023 | Tree loss: 3.732 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 019 | Total loss: 3.750 | Reg loss: 0.023 | Tree loss: 3.750 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 019 | Total loss: 3.690 | Reg loss: 0.023 | Tree loss: 3.690 | Accuracy: 0.099609 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 019 | Total loss: 3.740 | Reg loss: 0.023 | Tree loss: 3.740 | Accuracy: 0.050781 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 019 | Total loss: 3.724 | Reg loss: 0.023 | Tree loss: 3.724 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 019 | Total loss: 3.715 | Reg loss: 0.023 | Tree loss: 3.715 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 019 | Total loss: 3.701 | Reg loss: 0.023 | Tree loss: 3.701 | Accuracy: 0.089844 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 019 | Total loss: 3.706 | Reg loss: 0.023 | Tree loss: 3.706 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 019 | Total loss: 3.681 | Reg loss: 0.023 | Tree loss: 3.681 | Accuracy: 0.097656 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 019 | Total loss: 3.712 | Reg loss: 0.023 | Tree loss: 3.712 | Accuracy: 0.085938 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 019 | Total loss: 3.755 | Reg loss: 0.023 | Tree loss: 3.755 | Accuracy: 0.060547 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 019 | Total loss: 3.738 | Reg loss: 0.023 | Tree loss: 3.738 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 019 | Total loss: 3.733 | Reg loss: 0.023 | Tree loss: 3.733 | Accuracy: 0.080078 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 019 | Total loss: 3.732 | Reg loss: 0.023 | Tree loss: 3.732 | Accuracy: 0.085938 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 018 / 019 | Total loss: 3.731 | Reg loss: 0.023 | Tree loss: 3.731 | Accuracy: 0.052265 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 019 | Total loss: 3.703 | Reg loss: 0.023 | Tree loss: 3.703 | Accuracy: 0.066406 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 019 | Total loss: 3.741 | Reg loss: 0.023 | Tree loss: 3.741 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 019 | Total loss: 3.708 | Reg loss: 0.023 | Tree loss: 3.708 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 019 | Total loss: 3.721 | Reg loss: 0.023 | Tree loss: 3.721 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 019 | Total loss: 3.722 | Reg loss: 0.023 | Tree loss: 3.722 | Accuracy: 0.058594 | 0.86 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 019 | Total loss: 3.734 | Reg loss: 0.023 | Tree loss: 3.734 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 019 | Total loss: 3.721 | Reg loss: 0.023 | Tree loss: 3.721 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 019 | Total loss: 3.710 | Reg loss: 0.023 | Tree loss: 3.710 | Accuracy: 0.083984 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 019 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 019 | Total loss: 3.690 | Reg loss: 0.023 | Tree loss: 3.690 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 019 | Total loss: 3.728 | Reg loss: 0.023 | Tree loss: 3.728 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 019 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.080078 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 019 | Total loss: 3.748 | Reg loss: 0.023 | Tree loss: 3.748 | Accuracy: 0.068359 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 019 | Total loss: 3.654 | Reg loss: 0.023 | Tree loss: 3.654 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 019 | Total loss: 3.699 | Reg loss: 0.023 | Tree loss: 3.699 | Accuracy: 0.103516 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.064453 | 0.859 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 019 | Total loss: 3.663 | Reg loss: 0.023 | Tree loss: 3.663 | Accuracy: 0.111498 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 019 | Total loss: 3.694 | Reg loss: 0.023 | Tree loss: 3.694 | Accuracy: 0.085938 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 019 | Total loss: 3.714 | Reg loss: 0.023 | Tree loss: 3.714 | Accuracy: 0.083984 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 019 | Total loss: 3.712 | Reg loss: 0.023 | Tree loss: 3.712 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 019 | Total loss: 3.716 | Reg loss: 0.023 | Tree loss: 3.716 | Accuracy: 0.062500 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 019 | Total loss: 3.687 | Reg loss: 0.023 | Tree loss: 3.687 | Accuracy: 0.080078 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 019 | Total loss: 3.703 | Reg loss: 0.023 | Tree loss: 3.703 | Accuracy: 0.068359 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 019 | Total loss: 3.671 | Reg loss: 0.023 | Tree loss: 3.671 | Accuracy: 0.097656 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 019 | Total loss: 3.716 | Reg loss: 0.023 | Tree loss: 3.716 | Accuracy: 0.066406 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 019 | Total loss: 3.682 | Reg loss: 0.023 | Tree loss: 3.682 | Accuracy: 0.078125 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 019 | Total loss: 3.710 | Reg loss: 0.023 | Tree loss: 3.710 | Accuracy: 0.087891 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 019 | Total loss: 3.661 | Reg loss: 0.023 | Tree loss: 3.661 | Accuracy: 0.082031 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 019 | Total loss: 3.718 | Reg loss: 0.023 | Tree loss: 3.718 | Accuracy: 0.074219 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 019 | Total loss: 3.724 | Reg loss: 0.023 | Tree loss: 3.724 | Accuracy: 0.052734 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 019 | Total loss: 3.691 | Reg loss: 0.023 | Tree loss: 3.691 | Accuracy: 0.085938 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 019 | Total loss: 3.674 | Reg loss: 0.023 | Tree loss: 3.674 | Accuracy: 0.089844 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 019 | Total loss: 3.683 | Reg loss: 0.023 | Tree loss: 3.683 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 019 | Total loss: 3.674 | Reg loss: 0.023 | Tree loss: 3.674 | Accuracy: 0.070312 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.072266 | 0.859 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 019 | Total loss: 3.682 | Reg loss: 0.023 | Tree loss: 3.682 | Accuracy: 0.101045 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 019 | Total loss: 3.696 | Reg loss: 0.023 | Tree loss: 3.696 | Accuracy: 0.062500 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 019 | Total loss: 3.691 | Reg loss: 0.023 | Tree loss: 3.691 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 019 | Total loss: 3.667 | Reg loss: 0.023 | Tree loss: 3.667 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 019 | Total loss: 3.707 | Reg loss: 0.023 | Tree loss: 3.707 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 019 | Total loss: 3.636 | Reg loss: 0.023 | Tree loss: 3.636 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 019 | Total loss: 3.705 | Reg loss: 0.023 | Tree loss: 3.705 | Accuracy: 0.066406 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 019 | Total loss: 3.670 | Reg loss: 0.023 | Tree loss: 3.670 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 019 | Total loss: 3.668 | Reg loss: 0.023 | Tree loss: 3.668 | Accuracy: 0.099609 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 019 | Total loss: 3.706 | Reg loss: 0.023 | Tree loss: 3.706 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 019 | Total loss: 3.675 | Reg loss: 0.023 | Tree loss: 3.675 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 019 | Total loss: 3.663 | Reg loss: 0.023 | Tree loss: 3.663 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 019 | Total loss: 3.690 | Reg loss: 0.023 | Tree loss: 3.690 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 019 | Total loss: 3.715 | Reg loss: 0.023 | Tree loss: 3.715 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 019 | Total loss: 3.676 | Reg loss: 0.023 | Tree loss: 3.676 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 019 | Total loss: 3.673 | Reg loss: 0.023 | Tree loss: 3.673 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 019 | Total loss: 3.689 | Reg loss: 0.023 | Tree loss: 3.689 | Accuracy: 0.066406 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 019 | Total loss: 3.715 | Reg loss: 0.023 | Tree loss: 3.715 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 019 | Total loss: 3.672 | Reg loss: 0.023 | Tree loss: 3.672 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 019 | Total loss: 3.674 | Reg loss: 0.023 | Tree loss: 3.674 | Accuracy: 0.062718 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 000 / 019 | Total loss: 3.686 | Reg loss: 0.023 | Tree loss: 3.686 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 019 | Total loss: 3.665 | Reg loss: 0.023 | Tree loss: 3.665 | Accuracy: 0.091797 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 019 | Total loss: 3.692 | Reg loss: 0.023 | Tree loss: 3.692 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 019 | Total loss: 3.638 | Reg loss: 0.023 | Tree loss: 3.638 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 019 | Total loss: 3.677 | Reg loss: 0.023 | Tree loss: 3.677 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 019 | Total loss: 3.656 | Reg loss: 0.023 | Tree loss: 3.656 | Accuracy: 0.089844 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 019 | Total loss: 3.711 | Reg loss: 0.023 | Tree loss: 3.711 | Accuracy: 0.052734 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 019 | Total loss: 3.702 | Reg loss: 0.023 | Tree loss: 3.702 | Accuracy: 0.058594 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 019 | Total loss: 3.688 | Reg loss: 0.023 | Tree loss: 3.688 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 019 | Total loss: 3.706 | Reg loss: 0.023 | Tree loss: 3.706 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 019 | Total loss: 3.629 | Reg loss: 0.023 | Tree loss: 3.629 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 019 | Total loss: 3.668 | Reg loss: 0.023 | Tree loss: 3.668 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 019 | Total loss: 3.671 | Reg loss: 0.023 | Tree loss: 3.671 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 019 | Total loss: 3.655 | Reg loss: 0.023 | Tree loss: 3.655 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 019 | Total loss: 3.644 | Reg loss: 0.023 | Tree loss: 3.644 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 019 | Total loss: 3.663 | Reg loss: 0.023 | Tree loss: 3.663 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 019 | Total loss: 3.681 | Reg loss: 0.023 | Tree loss: 3.681 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 019 | Total loss: 3.671 | Reg loss: 0.023 | Tree loss: 3.671 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 019 | Total loss: 3.670 | Reg loss: 0.023 | Tree loss: 3.670 | Accuracy: 0.101045 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 019 | Total loss: 3.694 | Reg loss: 0.023 | Tree loss: 3.694 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 019 | Total loss: 3.656 | Reg loss: 0.023 | Tree loss: 3.656 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 019 | Total loss: 3.648 | Reg loss: 0.023 | Tree loss: 3.648 | Accuracy: 0.093750 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 019 | Total loss: 3.652 | Reg loss: 0.023 | Tree loss: 3.652 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 019 | Total loss: 3.665 | Reg loss: 0.023 | Tree loss: 3.665 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 019 | Total loss: 3.638 | Reg loss: 0.023 | Tree loss: 3.638 | Accuracy: 0.103516 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 019 | Total loss: 3.677 | Reg loss: 0.023 | Tree loss: 3.677 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 019 | Total loss: 3.646 | Reg loss: 0.023 | Tree loss: 3.646 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 019 | Total loss: 3.693 | Reg loss: 0.023 | Tree loss: 3.693 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 019 | Total loss: 3.687 | Reg loss: 0.023 | Tree loss: 3.687 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 019 | Total loss: 3.712 | Reg loss: 0.023 | Tree loss: 3.712 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 019 | Total loss: 3.641 | Reg loss: 0.023 | Tree loss: 3.641 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 019 | Total loss: 3.664 | Reg loss: 0.023 | Tree loss: 3.664 | Accuracy: 0.062500 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 019 | Total loss: 3.645 | Reg loss: 0.023 | Tree loss: 3.645 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 019 | Total loss: 3.660 | Reg loss: 0.023 | Tree loss: 3.660 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 019 | Total loss: 3.628 | Reg loss: 0.023 | Tree loss: 3.628 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 019 | Total loss: 3.673 | Reg loss: 0.023 | Tree loss: 3.673 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 019 | Total loss: 3.681 | Reg loss: 0.023 | Tree loss: 3.681 | Accuracy: 0.062718 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 019 | Total loss: 3.665 | Reg loss: 0.023 | Tree loss: 3.665 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 019 | Total loss: 3.677 | Reg loss: 0.023 | Tree loss: 3.677 | Accuracy: 0.060547 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 019 | Total loss: 3.631 | Reg loss: 0.023 | Tree loss: 3.631 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 019 | Total loss: 3.652 | Reg loss: 0.023 | Tree loss: 3.652 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 019 | Total loss: 3.653 | Reg loss: 0.023 | Tree loss: 3.653 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 019 | Total loss: 3.662 | Reg loss: 0.023 | Tree loss: 3.662 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 019 | Total loss: 3.629 | Reg loss: 0.023 | Tree loss: 3.629 | Accuracy: 0.097656 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 019 | Total loss: 3.647 | Reg loss: 0.023 | Tree loss: 3.647 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 019 | Total loss: 3.648 | Reg loss: 0.023 | Tree loss: 3.648 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 019 | Total loss: 3.663 | Reg loss: 0.023 | Tree loss: 3.663 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 019 | Total loss: 3.653 | Reg loss: 0.023 | Tree loss: 3.653 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 019 | Total loss: 3.666 | Reg loss: 0.023 | Tree loss: 3.666 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 019 | Total loss: 3.632 | Reg loss: 0.023 | Tree loss: 3.632 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 019 | Total loss: 3.659 | Reg loss: 0.023 | Tree loss: 3.659 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 019 | Total loss: 3.636 | Reg loss: 0.023 | Tree loss: 3.636 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 019 | Total loss: 3.635 | Reg loss: 0.023 | Tree loss: 3.635 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 019 | Total loss: 3.657 | Reg loss: 0.023 | Tree loss: 3.657 | Accuracy: 0.089844 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 019 | Total loss: 3.659 | Reg loss: 0.023 | Tree loss: 3.659 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 019 | Total loss: 3.642 | Reg loss: 0.023 | Tree loss: 3.642 | Accuracy: 0.066202 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 019 | Total loss: 3.668 | Reg loss: 0.023 | Tree loss: 3.668 | Accuracy: 0.060547 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 019 | Total loss: 3.624 | Reg loss: 0.023 | Tree loss: 3.624 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 019 | Total loss: 3.657 | Reg loss: 0.023 | Tree loss: 3.657 | Accuracy: 0.070312 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 003 / 019 | Total loss: 3.635 | Reg loss: 0.023 | Tree loss: 3.635 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 019 | Total loss: 3.657 | Reg loss: 0.023 | Tree loss: 3.657 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 019 | Total loss: 3.617 | Reg loss: 0.023 | Tree loss: 3.617 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 019 | Total loss: 3.628 | Reg loss: 0.023 | Tree loss: 3.628 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 019 | Total loss: 3.684 | Reg loss: 0.023 | Tree loss: 3.684 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 019 | Total loss: 3.636 | Reg loss: 0.023 | Tree loss: 3.636 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 019 | Total loss: 3.620 | Reg loss: 0.023 | Tree loss: 3.620 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 019 | Total loss: 3.647 | Reg loss: 0.023 | Tree loss: 3.647 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 019 | Total loss: 3.649 | Reg loss: 0.023 | Tree loss: 3.649 | Accuracy: 0.062500 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 019 | Total loss: 3.645 | Reg loss: 0.023 | Tree loss: 3.645 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 019 | Total loss: 3.631 | Reg loss: 0.023 | Tree loss: 3.631 | Accuracy: 0.076172 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 019 | Total loss: 3.649 | Reg loss: 0.023 | Tree loss: 3.649 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 019 | Total loss: 3.629 | Reg loss: 0.023 | Tree loss: 3.629 | Accuracy: 0.089844 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 019 | Total loss: 3.630 | Reg loss: 0.023 | Tree loss: 3.630 | Accuracy: 0.093750 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 019 | Total loss: 3.651 | Reg loss: 0.023 | Tree loss: 3.651 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 019 | Total loss: 3.618 | Reg loss: 0.023 | Tree loss: 3.618 | Accuracy: 0.108014 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 019 | Total loss: 3.595 | Reg loss: 0.023 | Tree loss: 3.595 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 019 | Total loss: 3.652 | Reg loss: 0.023 | Tree loss: 3.652 | Accuracy: 0.060547 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 019 | Total loss: 3.617 | Reg loss: 0.023 | Tree loss: 3.617 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 019 | Total loss: 3.669 | Reg loss: 0.023 | Tree loss: 3.669 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 019 | Total loss: 3.669 | Reg loss: 0.023 | Tree loss: 3.669 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 019 | Total loss: 3.633 | Reg loss: 0.023 | Tree loss: 3.633 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 019 | Total loss: 3.640 | Reg loss: 0.023 | Tree loss: 3.640 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 019 | Total loss: 3.607 | Reg loss: 0.023 | Tree loss: 3.607 | Accuracy: 0.095703 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 019 | Total loss: 3.613 | Reg loss: 0.023 | Tree loss: 3.613 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 019 | Total loss: 3.613 | Reg loss: 0.023 | Tree loss: 3.613 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 019 | Total loss: 3.648 | Reg loss: 0.023 | Tree loss: 3.648 | Accuracy: 0.091797 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 019 | Total loss: 3.639 | Reg loss: 0.023 | Tree loss: 3.639 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 019 | Total loss: 3.618 | Reg loss: 0.023 | Tree loss: 3.618 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 019 | Total loss: 3.677 | Reg loss: 0.023 | Tree loss: 3.677 | Accuracy: 0.048828 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 019 | Total loss: 3.611 | Reg loss: 0.023 | Tree loss: 3.611 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 019 | Total loss: 3.659 | Reg loss: 0.023 | Tree loss: 3.659 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 019 | Total loss: 3.664 | Reg loss: 0.023 | Tree loss: 3.664 | Accuracy: 0.052265 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 019 | Total loss: 3.632 | Reg loss: 0.023 | Tree loss: 3.632 | Accuracy: 0.082031 | 0.861 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 019 | Total loss: 3.605 | Reg loss: 0.023 | Tree loss: 3.605 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 019 | Total loss: 3.641 | Reg loss: 0.023 | Tree loss: 3.641 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 019 | Total loss: 3.620 | Reg loss: 0.023 | Tree loss: 3.620 | Accuracy: 0.060547 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 019 | Total loss: 3.618 | Reg loss: 0.023 | Tree loss: 3.618 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 019 | Total loss: 3.639 | Reg loss: 0.023 | Tree loss: 3.639 | Accuracy: 0.091797 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 019 | Total loss: 3.630 | Reg loss: 0.023 | Tree loss: 3.630 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 019 | Total loss: 3.614 | Reg loss: 0.023 | Tree loss: 3.614 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 019 | Total loss: 3.634 | Reg loss: 0.023 | Tree loss: 3.634 | Accuracy: 0.058594 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 019 | Total loss: 3.622 | Reg loss: 0.023 | Tree loss: 3.622 | Accuracy: 0.070312 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 019 | Total loss: 3.622 | Reg loss: 0.023 | Tree loss: 3.622 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 019 | Total loss: 3.625 | Reg loss: 0.023 | Tree loss: 3.625 | Accuracy: 0.101562 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 019 | Total loss: 3.631 | Reg loss: 0.023 | Tree loss: 3.631 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.093750 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 019 | Total loss: 3.634 | Reg loss: 0.023 | Tree loss: 3.634 | Accuracy: 0.087891 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 019 | Total loss: 3.614 | Reg loss: 0.023 | Tree loss: 3.614 | Accuracy: 0.083984 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 019 | Total loss: 3.605 | Reg loss: 0.023 | Tree loss: 3.605 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 019 | Total loss: 3.666 | Reg loss: 0.023 | Tree loss: 3.666 | Accuracy: 0.069686 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 019 | Total loss: 3.637 | Reg loss: 0.023 | Tree loss: 3.637 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 019 | Total loss: 3.640 | Reg loss: 0.023 | Tree loss: 3.640 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 019 | Total loss: 3.606 | Reg loss: 0.023 | Tree loss: 3.606 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 019 | Total loss: 3.619 | Reg loss: 0.023 | Tree loss: 3.619 | Accuracy: 0.066406 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 019 | Total loss: 3.612 | Reg loss: 0.023 | Tree loss: 3.612 | Accuracy: 0.087891 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 006 / 019 | Total loss: 3.610 | Reg loss: 0.023 | Tree loss: 3.610 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 019 | Total loss: 3.604 | Reg loss: 0.023 | Tree loss: 3.604 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 019 | Total loss: 3.589 | Reg loss: 0.023 | Tree loss: 3.589 | Accuracy: 0.091797 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 019 | Total loss: 3.617 | Reg loss: 0.023 | Tree loss: 3.617 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 019 | Total loss: 3.604 | Reg loss: 0.023 | Tree loss: 3.604 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.058594 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 019 | Total loss: 3.607 | Reg loss: 0.023 | Tree loss: 3.607 | Accuracy: 0.068359 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 019 | Total loss: 3.610 | Reg loss: 0.023 | Tree loss: 3.610 | Accuracy: 0.091797 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 019 | Total loss: 3.601 | Reg loss: 0.023 | Tree loss: 3.601 | Accuracy: 0.080078 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 019 | Total loss: 3.603 | Reg loss: 0.023 | Tree loss: 3.603 | Accuracy: 0.095703 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 019 | Total loss: 3.639 | Reg loss: 0.023 | Tree loss: 3.639 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 019 | Total loss: 3.654 | Reg loss: 0.023 | Tree loss: 3.654 | Accuracy: 0.062500 | 0.86 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 019 | Total loss: 3.626 | Reg loss: 0.023 | Tree loss: 3.626 | Accuracy: 0.059233 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 019 | Total loss: 3.621 | Reg loss: 0.023 | Tree loss: 3.621 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.097656 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 019 | Total loss: 3.617 | Reg loss: 0.023 | Tree loss: 3.617 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 019 | Total loss: 3.604 | Reg loss: 0.023 | Tree loss: 3.604 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 019 | Total loss: 3.601 | Reg loss: 0.023 | Tree loss: 3.601 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 019 | Total loss: 3.608 | Reg loss: 0.023 | Tree loss: 3.608 | Accuracy: 0.068359 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 019 | Total loss: 3.613 | Reg loss: 0.023 | Tree loss: 3.613 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 019 | Total loss: 3.570 | Reg loss: 0.023 | Tree loss: 3.570 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 019 | Total loss: 3.619 | Reg loss: 0.023 | Tree loss: 3.619 | Accuracy: 0.058594 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 019 | Total loss: 3.606 | Reg loss: 0.023 | Tree loss: 3.606 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 019 | Total loss: 3.638 | Reg loss: 0.023 | Tree loss: 3.638 | Accuracy: 0.062500 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 019 | Total loss: 3.622 | Reg loss: 0.023 | Tree loss: 3.622 | Accuracy: 0.074219 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 019 | Total loss: 3.584 | Reg loss: 0.023 | Tree loss: 3.584 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 019 | Total loss: 3.621 | Reg loss: 0.023 | Tree loss: 3.621 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 019 | Total loss: 3.633 | Reg loss: 0.023 | Tree loss: 3.633 | Accuracy: 0.082031 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.085938 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 019 | Total loss: 3.618 | Reg loss: 0.023 | Tree loss: 3.618 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 019 | Total loss: 3.581 | Reg loss: 0.023 | Tree loss: 3.581 | Accuracy: 0.076655 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.087891 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 019 | Total loss: 3.587 | Reg loss: 0.023 | Tree loss: 3.587 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 019 | Total loss: 3.590 | Reg loss: 0.023 | Tree loss: 3.590 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 019 | Total loss: 3.623 | Reg loss: 0.023 | Tree loss: 3.623 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 019 | Total loss: 3.596 | Reg loss: 0.023 | Tree loss: 3.596 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 019 | Total loss: 3.620 | Reg loss: 0.023 | Tree loss: 3.620 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 019 | Total loss: 3.612 | Reg loss: 0.023 | Tree loss: 3.612 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 019 | Total loss: 3.633 | Reg loss: 0.023 | Tree loss: 3.633 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 019 | Total loss: 3.590 | Reg loss: 0.023 | Tree loss: 3.590 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 019 | Total loss: 3.584 | Reg loss: 0.023 | Tree loss: 3.584 | Accuracy: 0.068359 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 019 | Total loss: 3.595 | Reg loss: 0.023 | Tree loss: 3.595 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 019 | Total loss: 3.570 | Reg loss: 0.023 | Tree loss: 3.570 | Accuracy: 0.097656 | 0.86 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 019 | Total loss: 3.591 | Reg loss: 0.023 | Tree loss: 3.591 | Accuracy: 0.097656 | 0.86 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.078125 | 0.86 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 019 | Total loss: 3.627 | Reg loss: 0.023 | Tree loss: 3.627 | Accuracy: 0.072266 | 0.86 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 019 | Total loss: 3.594 | Reg loss: 0.023 | Tree loss: 3.594 | Accuracy: 0.064453 | 0.86 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 019 | Total loss: 3.613 | Reg loss: 0.023 | Tree loss: 3.613 | Accuracy: 0.069686 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 019 | Total loss: 3.606 | Reg loss: 0.023 | Tree loss: 3.606 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 019 | Total loss: 3.627 | Reg loss: 0.023 | Tree loss: 3.627 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 019 | Total loss: 3.557 | Reg loss: 0.023 | Tree loss: 3.557 | Accuracy: 0.105469 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 019 | Total loss: 3.582 | Reg loss: 0.023 | Tree loss: 3.582 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 019 | Total loss: 3.596 | Reg loss: 0.023 | Tree loss: 3.596 | Accuracy: 0.056641 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 019 | Total loss: 3.592 | Reg loss: 0.023 | Tree loss: 3.592 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 019 | Total loss: 3.608 | Reg loss: 0.023 | Tree loss: 3.608 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 019 | Total loss: 3.584 | Reg loss: 0.023 | Tree loss: 3.584 | Accuracy: 0.091797 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 019 | Total loss: 3.575 | Reg loss: 0.023 | Tree loss: 3.575 | Accuracy: 0.082031 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 009 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.087891 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 019 | Total loss: 3.611 | Reg loss: 0.023 | Tree loss: 3.611 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 019 | Total loss: 3.589 | Reg loss: 0.023 | Tree loss: 3.589 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 019 | Total loss: 3.582 | Reg loss: 0.023 | Tree loss: 3.582 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 019 | Total loss: 3.611 | Reg loss: 0.023 | Tree loss: 3.611 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 019 | Total loss: 3.577 | Reg loss: 0.023 | Tree loss: 3.577 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 019 | Total loss: 3.618 | Reg loss: 0.023 | Tree loss: 3.618 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 019 | Total loss: 3.582 | Reg loss: 0.023 | Tree loss: 3.582 | Accuracy: 0.082031 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 019 | Total loss: 3.593 | Reg loss: 0.023 | Tree loss: 3.593 | Accuracy: 0.066406 | 0.861 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 019 | Total loss: 3.561 | Reg loss: 0.023 | Tree loss: 3.561 | Accuracy: 0.066202 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 019 | Total loss: 3.589 | Reg loss: 0.023 | Tree loss: 3.589 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 019 | Total loss: 3.583 | Reg loss: 0.023 | Tree loss: 3.583 | Accuracy: 0.097656 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 019 | Total loss: 3.581 | Reg loss: 0.023 | Tree loss: 3.581 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 019 | Total loss: 3.582 | Reg loss: 0.023 | Tree loss: 3.582 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 019 | Total loss: 3.591 | Reg loss: 0.023 | Tree loss: 3.591 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 019 | Total loss: 3.603 | Reg loss: 0.023 | Tree loss: 3.603 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 019 | Total loss: 3.593 | Reg loss: 0.023 | Tree loss: 3.593 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 019 | Total loss: 3.571 | Reg loss: 0.023 | Tree loss: 3.571 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 019 | Total loss: 3.599 | Reg loss: 0.023 | Tree loss: 3.599 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 019 | Total loss: 3.614 | Reg loss: 0.023 | Tree loss: 3.614 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 019 | Total loss: 3.577 | Reg loss: 0.023 | Tree loss: 3.577 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.087891 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 019 | Total loss: 3.609 | Reg loss: 0.023 | Tree loss: 3.609 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 019 | Total loss: 3.573 | Reg loss: 0.023 | Tree loss: 3.573 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 019 | Total loss: 3.599 | Reg loss: 0.023 | Tree loss: 3.599 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 019 | Total loss: 3.553 | Reg loss: 0.023 | Tree loss: 3.553 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 019 | Total loss: 3.605 | Reg loss: 0.023 | Tree loss: 3.605 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 019 | Total loss: 3.556 | Reg loss: 0.023 | Tree loss: 3.556 | Accuracy: 0.080139 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 019 | Total loss: 3.596 | Reg loss: 0.023 | Tree loss: 3.596 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 019 | Total loss: 3.594 | Reg loss: 0.023 | Tree loss: 3.594 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 019 | Total loss: 3.595 | Reg loss: 0.023 | Tree loss: 3.595 | Accuracy: 0.089844 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.039062 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 019 | Total loss: 3.567 | Reg loss: 0.023 | Tree loss: 3.567 | Accuracy: 0.082031 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 019 | Total loss: 3.581 | Reg loss: 0.023 | Tree loss: 3.581 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 019 | Total loss: 3.579 | Reg loss: 0.023 | Tree loss: 3.579 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 019 | Total loss: 3.604 | Reg loss: 0.023 | Tree loss: 3.604 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 019 | Total loss: 3.596 | Reg loss: 0.023 | Tree loss: 3.596 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.103516 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 019 | Total loss: 3.559 | Reg loss: 0.023 | Tree loss: 3.559 | Accuracy: 0.066406 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.089844 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 019 | Total loss: 3.536 | Reg loss: 0.023 | Tree loss: 3.536 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 019 | Total loss: 3.590 | Reg loss: 0.023 | Tree loss: 3.590 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 019 | Total loss: 3.536 | Reg loss: 0.023 | Tree loss: 3.536 | Accuracy: 0.095703 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.091797 | 0.861 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.062718 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 019 | Total loss: 3.516 | Reg loss: 0.023 | Tree loss: 3.516 | Accuracy: 0.099609 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 019 | Total loss: 3.603 | Reg loss: 0.023 | Tree loss: 3.603 | Accuracy: 0.068359 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 019 | Total loss: 3.565 | Reg loss: 0.023 | Tree loss: 3.565 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 019 | Total loss: 3.609 | Reg loss: 0.023 | Tree loss: 3.609 | Accuracy: 0.066406 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 019 | Total loss: 3.600 | Reg loss: 0.023 | Tree loss: 3.600 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 019 | Total loss: 3.540 | Reg loss: 0.023 | Tree loss: 3.540 | Accuracy: 0.105469 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 019 | Total loss: 3.612 | Reg loss: 0.023 | Tree loss: 3.612 | Accuracy: 0.070312 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 012 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 019 | Total loss: 3.575 | Reg loss: 0.023 | Tree loss: 3.575 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 019 | Total loss: 3.559 | Reg loss: 0.023 | Tree loss: 3.559 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 019 | Total loss: 3.602 | Reg loss: 0.023 | Tree loss: 3.602 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 019 | Total loss: 3.554 | Reg loss: 0.023 | Tree loss: 3.554 | Accuracy: 0.089844 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 019 | Total loss: 3.589 | Reg loss: 0.023 | Tree loss: 3.589 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.062718 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 019 | Total loss: 3.567 | Reg loss: 0.023 | Tree loss: 3.567 | Accuracy: 0.068359 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.089844 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 019 | Total loss: 3.570 | Reg loss: 0.023 | Tree loss: 3.570 | Accuracy: 0.082031 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 019 | Total loss: 3.615 | Reg loss: 0.023 | Tree loss: 3.615 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 019 | Total loss: 3.579 | Reg loss: 0.023 | Tree loss: 3.579 | Accuracy: 0.064453 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 019 | Total loss: 3.541 | Reg loss: 0.023 | Tree loss: 3.541 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 019 | Total loss: 3.561 | Reg loss: 0.023 | Tree loss: 3.561 | Accuracy: 0.066406 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.087891 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 019 | Total loss: 3.570 | Reg loss: 0.023 | Tree loss: 3.570 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 019 | Total loss: 3.582 | Reg loss: 0.023 | Tree loss: 3.582 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.091797 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 019 | Total loss: 3.561 | Reg loss: 0.023 | Tree loss: 3.561 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 019 | Total loss: 3.571 | Reg loss: 0.023 | Tree loss: 3.571 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 019 | Total loss: 3.576 | Reg loss: 0.023 | Tree loss: 3.576 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 019 | Total loss: 3.546 | Reg loss: 0.023 | Tree loss: 3.546 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 019 | Total loss: 3.569 | Reg loss: 0.023 | Tree loss: 3.569 | Accuracy: 0.060547 | 0.861 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 019 | Total loss: 3.585 | Reg loss: 0.023 | Tree loss: 3.585 | Accuracy: 0.094077 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 019 | Total loss: 3.581 | Reg loss: 0.023 | Tree loss: 3.581 | Accuracy: 0.056641 | 0.862 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 019 | Total loss: 3.583 | Reg loss: 0.023 | Tree loss: 3.583 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 019 | Total loss: 3.535 | Reg loss: 0.023 | Tree loss: 3.535 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 019 | Total loss: 3.587 | Reg loss: 0.023 | Tree loss: 3.587 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 019 | Total loss: 3.579 | Reg loss: 0.023 | Tree loss: 3.579 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 019 | Total loss: 3.578 | Reg loss: 0.023 | Tree loss: 3.578 | Accuracy: 0.060547 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 019 | Total loss: 3.586 | Reg loss: 0.023 | Tree loss: 3.586 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 019 | Total loss: 3.522 | Reg loss: 0.023 | Tree loss: 3.522 | Accuracy: 0.068359 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 019 | Total loss: 3.544 | Reg loss: 0.023 | Tree loss: 3.544 | Accuracy: 0.095703 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 019 | Total loss: 3.552 | Reg loss: 0.023 | Tree loss: 3.552 | Accuracy: 0.097656 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 019 | Total loss: 3.583 | Reg loss: 0.023 | Tree loss: 3.583 | Accuracy: 0.052734 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 019 | Total loss: 3.573 | Reg loss: 0.023 | Tree loss: 3.573 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 019 | Total loss: 3.548 | Reg loss: 0.023 | Tree loss: 3.548 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 019 | Total loss: 3.540 | Reg loss: 0.023 | Tree loss: 3.540 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 019 | Total loss: 3.579 | Reg loss: 0.023 | Tree loss: 3.579 | Accuracy: 0.083984 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 019 | Total loss: 3.566 | Reg loss: 0.023 | Tree loss: 3.566 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 019 | Total loss: 3.483 | Reg loss: 0.023 | Tree loss: 3.483 | Accuracy: 0.101045 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 019 | Total loss: 3.567 | Reg loss: 0.023 | Tree loss: 3.567 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 019 | Total loss: 3.583 | Reg loss: 0.023 | Tree loss: 3.583 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 019 | Total loss: 3.526 | Reg loss: 0.023 | Tree loss: 3.526 | Accuracy: 0.113281 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 019 | Total loss: 3.556 | Reg loss: 0.023 | Tree loss: 3.556 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 019 | Total loss: 3.593 | Reg loss: 0.023 | Tree loss: 3.593 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 019 | Total loss: 3.576 | Reg loss: 0.023 | Tree loss: 3.576 | Accuracy: 0.060547 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 019 | Total loss: 3.560 | Reg loss: 0.023 | Tree loss: 3.560 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 019 | Total loss: 3.570 | Reg loss: 0.023 | Tree loss: 3.570 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 019 | Total loss: 3.590 | Reg loss: 0.023 | Tree loss: 3.590 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 019 | Total loss: 3.578 | Reg loss: 0.023 | Tree loss: 3.578 | Accuracy: 0.064453 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.093750 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.070312 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 015 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.087891 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 019 | Total loss: 3.541 | Reg loss: 0.023 | Tree loss: 3.541 | Accuracy: 0.078125 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 019 | Total loss: 3.577 | Reg loss: 0.023 | Tree loss: 3.577 | Accuracy: 0.082031 | 0.861 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.062718 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 019 | Total loss: 3.553 | Reg loss: 0.023 | Tree loss: 3.553 | Accuracy: 0.111328 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 019 | Total loss: 3.584 | Reg loss: 0.023 | Tree loss: 3.584 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 019 | Total loss: 3.553 | Reg loss: 0.023 | Tree loss: 3.553 | Accuracy: 0.060547 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 019 | Total loss: 3.575 | Reg loss: 0.023 | Tree loss: 3.575 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 019 | Total loss: 3.524 | Reg loss: 0.023 | Tree loss: 3.524 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 019 | Total loss: 3.543 | Reg loss: 0.023 | Tree loss: 3.543 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 019 | Total loss: 3.595 | Reg loss: 0.023 | Tree loss: 3.595 | Accuracy: 0.058594 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 019 | Total loss: 3.563 | Reg loss: 0.023 | Tree loss: 3.563 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 019 | Total loss: 3.540 | Reg loss: 0.023 | Tree loss: 3.540 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 019 | Total loss: 3.562 | Reg loss: 0.023 | Tree loss: 3.562 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 019 | Total loss: 3.539 | Reg loss: 0.023 | Tree loss: 3.539 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 019 | Total loss: 3.566 | Reg loss: 0.023 | Tree loss: 3.566 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 019 | Total loss: 3.525 | Reg loss: 0.023 | Tree loss: 3.525 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 019 | Total loss: 3.546 | Reg loss: 0.023 | Tree loss: 3.546 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.099609 | 0.861 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 019 | Total loss: 3.543 | Reg loss: 0.023 | Tree loss: 3.543 | Accuracy: 0.076172 | 0.861 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 019 | Total loss: 3.546 | Reg loss: 0.023 | Tree loss: 3.546 | Accuracy: 0.062500 | 0.861 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.069686 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 019 | Total loss: 3.569 | Reg loss: 0.023 | Tree loss: 3.569 | Accuracy: 0.060547 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 019 | Total loss: 3.544 | Reg loss: 0.023 | Tree loss: 3.544 | Accuracy: 0.093750 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 019 | Total loss: 3.528 | Reg loss: 0.023 | Tree loss: 3.528 | Accuracy: 0.099609 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 019 | Total loss: 3.521 | Reg loss: 0.023 | Tree loss: 3.521 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 019 | Total loss: 3.579 | Reg loss: 0.023 | Tree loss: 3.579 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 019 | Total loss: 3.564 | Reg loss: 0.023 | Tree loss: 3.564 | Accuracy: 0.070312 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.097656 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 019 | Total loss: 3.566 | Reg loss: 0.023 | Tree loss: 3.566 | Accuracy: 0.080078 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.095703 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 019 | Total loss: 3.560 | Reg loss: 0.023 | Tree loss: 3.560 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 019 | Total loss: 3.556 | Reg loss: 0.023 | Tree loss: 3.556 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 019 | Total loss: 3.553 | Reg loss: 0.023 | Tree loss: 3.553 | Accuracy: 0.058594 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 019 | Total loss: 3.532 | Reg loss: 0.023 | Tree loss: 3.532 | Accuracy: 0.072266 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 019 | Total loss: 3.541 | Reg loss: 0.023 | Tree loss: 3.541 | Accuracy: 0.074219 | 0.861 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 019 | Total loss: 3.498 | Reg loss: 0.023 | Tree loss: 3.498 | Accuracy: 0.104530 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 019 | Total loss: 3.529 | Reg loss: 0.023 | Tree loss: 3.529 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 019 | Total loss: 3.565 | Reg loss: 0.023 | Tree loss: 3.565 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 019 | Total loss: 3.525 | Reg loss: 0.023 | Tree loss: 3.525 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 019 | Total loss: 3.534 | Reg loss: 0.023 | Tree loss: 3.534 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 019 | Total loss: 3.524 | Reg loss: 0.023 | Tree loss: 3.524 | Accuracy: 0.109375 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 019 | Total loss: 3.562 | Reg loss: 0.023 | Tree loss: 3.562 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 019 | Total loss: 3.548 | Reg loss: 0.023 | Tree loss: 3.548 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 019 | Total loss: 3.532 | Reg loss: 0.023 | Tree loss: 3.532 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.056641 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 019 | Total loss: 3.543 | Reg loss: 0.023 | Tree loss: 3.543 | Accuracy: 0.103516 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 019 | Total loss: 3.551 | Reg loss: 0.023 | Tree loss: 3.551 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 019 | Total loss: 3.560 | Reg loss: 0.023 | Tree loss: 3.560 | Accuracy: 0.085938 | 0.861 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 019 | Total loss: 3.549 | Reg loss: 0.023 | Tree loss: 3.549 | Accuracy: 0.050781 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 018 / 019 | Total loss: 3.557 | Reg loss: 0.023 | Tree loss: 3.557 | Accuracy: 0.090592 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 019 | Total loss: 3.543 | Reg loss: 0.023 | Tree loss: 3.543 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 019 | Total loss: 3.555 | Reg loss: 0.023 | Tree loss: 3.555 | Accuracy: 0.048828 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 019 | Total loss: 3.495 | Reg loss: 0.023 | Tree loss: 3.495 | Accuracy: 0.107422 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.097656 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 019 | Total loss: 3.551 | Reg loss: 0.023 | Tree loss: 3.551 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 019 | Total loss: 3.566 | Reg loss: 0.023 | Tree loss: 3.566 | Accuracy: 0.058594 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 019 | Total loss: 3.498 | Reg loss: 0.023 | Tree loss: 3.498 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 019 | Total loss: 3.587 | Reg loss: 0.023 | Tree loss: 3.587 | Accuracy: 0.060547 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.099609 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.058594 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.097656 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 019 | Total loss: 3.544 | Reg loss: 0.023 | Tree loss: 3.544 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.048780 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 019 | Total loss: 3.516 | Reg loss: 0.023 | Tree loss: 3.516 | Accuracy: 0.115234 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 019 | Total loss: 3.573 | Reg loss: 0.023 | Tree loss: 3.573 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 019 | Total loss: 3.536 | Reg loss: 0.023 | Tree loss: 3.536 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 019 | Total loss: 3.535 | Reg loss: 0.023 | Tree loss: 3.535 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 019 | Total loss: 3.575 | Reg loss: 0.023 | Tree loss: 3.575 | Accuracy: 0.058594 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 019 | Total loss: 3.512 | Reg loss: 0.023 | Tree loss: 3.512 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 019 | Total loss: 3.540 | Reg loss: 0.023 | Tree loss: 3.540 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 019 | Total loss: 3.503 | Reg loss: 0.023 | Tree loss: 3.503 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.050781 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 019 | Total loss: 3.518 | Reg loss: 0.023 | Tree loss: 3.518 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 019 | Total loss: 3.565 | Reg loss: 0.023 | Tree loss: 3.565 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 019 | Total loss: 3.509 | Reg loss: 0.023 | Tree loss: 3.509 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 019 | Total loss: 3.545 | Reg loss: 0.023 | Tree loss: 3.545 | Accuracy: 0.076655 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 019 | Total loss: 3.502 | Reg loss: 0.023 | Tree loss: 3.502 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.101562 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 019 | Total loss: 3.506 | Reg loss: 0.023 | Tree loss: 3.506 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 019 | Total loss: 3.558 | Reg loss: 0.023 | Tree loss: 3.558 | Accuracy: 0.052734 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 019 | Total loss: 3.597 | Reg loss: 0.023 | Tree loss: 3.597 | Accuracy: 0.044922 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 019 | Total loss: 3.572 | Reg loss: 0.023 | Tree loss: 3.572 | Accuracy: 0.044922 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 019 | Total loss: 3.526 | Reg loss: 0.023 | Tree loss: 3.526 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.097656 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 019 | Total loss: 3.494 | Reg loss: 0.023 | Tree loss: 3.494 | Accuracy: 0.095703 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 019 | Total loss: 3.509 | Reg loss: 0.023 | Tree loss: 3.509 | Accuracy: 0.097656 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 019 | Total loss: 3.552 | Reg loss: 0.023 | Tree loss: 3.552 | Accuracy: 0.054688 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 019 | Total loss: 3.492 | Reg loss: 0.023 | Tree loss: 3.492 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 019 | Total loss: 3.505 | Reg loss: 0.023 | Tree loss: 3.505 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 019 | Total loss: 3.580 | Reg loss: 0.023 | Tree loss: 3.580 | Accuracy: 0.073171 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 000 / 019 | Total loss: 3.546 | Reg loss: 0.023 | Tree loss: 3.546 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 019 | Total loss: 3.534 | Reg loss: 0.023 | Tree loss: 3.534 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 019 | Total loss: 3.552 | Reg loss: 0.023 | Tree loss: 3.552 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 019 | Total loss: 3.511 | Reg loss: 0.023 | Tree loss: 3.511 | Accuracy: 0.095703 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 019 | Total loss: 3.538 | Reg loss: 0.023 | Tree loss: 3.538 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 019 | Total loss: 3.556 | Reg loss: 0.023 | Tree loss: 3.556 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 019 | Total loss: 3.548 | Reg loss: 0.023 | Tree loss: 3.548 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 019 | Total loss: 3.536 | Reg loss: 0.023 | Tree loss: 3.536 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 019 | Total loss: 3.473 | Reg loss: 0.023 | Tree loss: 3.473 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 019 | Total loss: 3.533 | Reg loss: 0.023 | Tree loss: 3.533 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 019 | Total loss: 3.533 | Reg loss: 0.023 | Tree loss: 3.533 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 019 | Total loss: 3.490 | Reg loss: 0.023 | Tree loss: 3.490 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 019 | Total loss: 3.525 | Reg loss: 0.023 | Tree loss: 3.525 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.083624 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 019 | Total loss: 3.502 | Reg loss: 0.023 | Tree loss: 3.502 | Accuracy: 0.093750 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 019 | Total loss: 3.547 | Reg loss: 0.023 | Tree loss: 3.547 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 019 | Total loss: 3.525 | Reg loss: 0.023 | Tree loss: 3.525 | Accuracy: 0.093750 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 019 | Total loss: 3.565 | Reg loss: 0.023 | Tree loss: 3.565 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 019 | Total loss: 3.539 | Reg loss: 0.023 | Tree loss: 3.539 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 019 | Total loss: 3.496 | Reg loss: 0.023 | Tree loss: 3.496 | Accuracy: 0.111328 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 019 | Total loss: 3.528 | Reg loss: 0.023 | Tree loss: 3.528 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 019 | Total loss: 3.499 | Reg loss: 0.023 | Tree loss: 3.499 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 019 | Total loss: 3.501 | Reg loss: 0.023 | Tree loss: 3.501 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 019 | Total loss: 3.529 | Reg loss: 0.023 | Tree loss: 3.529 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 019 | Total loss: 3.526 | Reg loss: 0.023 | Tree loss: 3.526 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 019 | Total loss: 3.552 | Reg loss: 0.023 | Tree loss: 3.552 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.062718 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 019 | Total loss: 3.549 | Reg loss: 0.023 | Tree loss: 3.549 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 019 | Total loss: 3.505 | Reg loss: 0.023 | Tree loss: 3.505 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 019 | Total loss: 3.510 | Reg loss: 0.023 | Tree loss: 3.510 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 019 | Total loss: 3.529 | Reg loss: 0.023 | Tree loss: 3.529 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 019 | Total loss: 3.518 | Reg loss: 0.023 | Tree loss: 3.518 | Accuracy: 0.093750 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 019 | Total loss: 3.550 | Reg loss: 0.023 | Tree loss: 3.550 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 019 | Total loss: 3.498 | Reg loss: 0.023 | Tree loss: 3.498 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 019 | Total loss: 3.503 | Reg loss: 0.023 | Tree loss: 3.503 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.095703 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 019 | Total loss: 3.522 | Reg loss: 0.023 | Tree loss: 3.522 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 019 | Total loss: 3.504 | Reg loss: 0.023 | Tree loss: 3.504 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 019 | Total loss: 3.551 | Reg loss: 0.023 | Tree loss: 3.551 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 019 | Total loss: 3.507 | Reg loss: 0.023 | Tree loss: 3.507 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 019 | Total loss: 3.492 | Reg loss: 0.023 | Tree loss: 3.492 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 019 | Total loss: 3.502 | Reg loss: 0.023 | Tree loss: 3.502 | Accuracy: 0.083624 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 019 | Total loss: 3.505 | Reg loss: 0.023 | Tree loss: 3.505 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 019 | Total loss: 3.533 | Reg loss: 0.023 | Tree loss: 3.533 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 019 | Total loss: 3.478 | Reg loss: 0.023 | Tree loss: 3.478 | Accuracy: 0.072266 | 0.862 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 003 / 019 | Total loss: 3.493 | Reg loss: 0.023 | Tree loss: 3.493 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 019 | Total loss: 3.516 | Reg loss: 0.023 | Tree loss: 3.516 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 019 | Total loss: 3.518 | Reg loss: 0.023 | Tree loss: 3.518 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 019 | Total loss: 3.524 | Reg loss: 0.023 | Tree loss: 3.524 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 019 | Total loss: 3.536 | Reg loss: 0.023 | Tree loss: 3.536 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.062500 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 019 | Total loss: 3.532 | Reg loss: 0.023 | Tree loss: 3.532 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 019 | Total loss: 3.505 | Reg loss: 0.023 | Tree loss: 3.505 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 019 | Total loss: 3.480 | Reg loss: 0.023 | Tree loss: 3.480 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 019 | Total loss: 3.526 | Reg loss: 0.023 | Tree loss: 3.526 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 019 | Total loss: 3.490 | Reg loss: 0.023 | Tree loss: 3.490 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 019 | Total loss: 3.548 | Reg loss: 0.023 | Tree loss: 3.548 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 019 | Total loss: 3.525 | Reg loss: 0.023 | Tree loss: 3.525 | Accuracy: 0.074219 | 0.862 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 019 | Total loss: 3.534 | Reg loss: 0.023 | Tree loss: 3.534 | Accuracy: 0.090592 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 019 | Total loss: 3.481 | Reg loss: 0.023 | Tree loss: 3.481 | Accuracy: 0.103516 | 0.863 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 019 | Total loss: 3.487 | Reg loss: 0.023 | Tree loss: 3.487 | Accuracy: 0.066406 | 0.863 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 019 | Total loss: 3.510 | Reg loss: 0.023 | Tree loss: 3.510 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.070312 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 019 | Total loss: 3.524 | Reg loss: 0.023 | Tree loss: 3.524 | Accuracy: 0.066406 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 019 | Total loss: 3.520 | Reg loss: 0.023 | Tree loss: 3.520 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.052734 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.060547 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 019 | Total loss: 3.503 | Reg loss: 0.023 | Tree loss: 3.503 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 019 | Total loss: 3.502 | Reg loss: 0.023 | Tree loss: 3.502 | Accuracy: 0.093750 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 019 | Total loss: 3.498 | Reg loss: 0.023 | Tree loss: 3.498 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.091797 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 019 | Total loss: 3.512 | Reg loss: 0.023 | Tree loss: 3.512 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.080078 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 019 | Total loss: 3.497 | Reg loss: 0.023 | Tree loss: 3.497 | Accuracy: 0.089844 | 0.862 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 019 | Total loss: 3.537 | Reg loss: 0.023 | Tree loss: 3.537 | Accuracy: 0.069686 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 019 | Total loss: 3.491 | Reg loss: 0.023 | Tree loss: 3.491 | Accuracy: 0.080078 | 0.863 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.060547 | 0.863 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 019 | Total loss: 3.523 | Reg loss: 0.023 | Tree loss: 3.523 | Accuracy: 0.095703 | 0.863 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 019 | Total loss: 3.499 | Reg loss: 0.023 | Tree loss: 3.499 | Accuracy: 0.095703 | 0.863 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 019 | Total loss: 3.490 | Reg loss: 0.023 | Tree loss: 3.490 | Accuracy: 0.074219 | 0.863 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 019 | Total loss: 3.523 | Reg loss: 0.023 | Tree loss: 3.523 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 019 | Total loss: 3.533 | Reg loss: 0.023 | Tree loss: 3.533 | Accuracy: 0.068359 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 019 | Total loss: 3.539 | Reg loss: 0.023 | Tree loss: 3.539 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 019 | Total loss: 3.546 | Reg loss: 0.023 | Tree loss: 3.546 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 019 | Total loss: 3.486 | Reg loss: 0.023 | Tree loss: 3.486 | Accuracy: 0.105469 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 019 | Total loss: 3.552 | Reg loss: 0.023 | Tree loss: 3.552 | Accuracy: 0.054688 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 019 | Total loss: 3.461 | Reg loss: 0.023 | Tree loss: 3.461 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 019 | Total loss: 3.479 | Reg loss: 0.023 | Tree loss: 3.479 | Accuracy: 0.087891 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 019 | Total loss: 3.489 | Reg loss: 0.023 | Tree loss: 3.489 | Accuracy: 0.056641 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 019 | Total loss: 3.513 | Reg loss: 0.023 | Tree loss: 3.513 | Accuracy: 0.072266 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 019 | Total loss: 3.518 | Reg loss: 0.023 | Tree loss: 3.518 | Accuracy: 0.082031 | 0.862 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 019 | Total loss: 3.491 | Reg loss: 0.023 | Tree loss: 3.491 | Accuracy: 0.073171 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 019 | Total loss: 3.507 | Reg loss: 0.023 | Tree loss: 3.507 | Accuracy: 0.085938 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 019 | Total loss: 3.527 | Reg loss: 0.023 | Tree loss: 3.527 | Accuracy: 0.066406 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 019 | Total loss: 3.493 | Reg loss: 0.023 | Tree loss: 3.493 | Accuracy: 0.091797 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 019 | Total loss: 3.493 | Reg loss: 0.023 | Tree loss: 3.493 | Accuracy: 0.062500 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 019 | Total loss: 3.463 | Reg loss: 0.023 | Tree loss: 3.463 | Accuracy: 0.103516 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 019 | Total loss: 3.523 | Reg loss: 0.023 | Tree loss: 3.523 | Accuracy: 0.068359 | 0.863 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 006 / 019 | Total loss: 3.495 | Reg loss: 0.023 | Tree loss: 3.495 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.074219 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 019 | Total loss: 3.489 | Reg loss: 0.023 | Tree loss: 3.489 | Accuracy: 0.074219 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 019 | Total loss: 3.492 | Reg loss: 0.023 | Tree loss: 3.492 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 019 | Total loss: 3.519 | Reg loss: 0.023 | Tree loss: 3.519 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 019 | Total loss: 3.505 | Reg loss: 0.023 | Tree loss: 3.505 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 019 | Total loss: 3.522 | Reg loss: 0.023 | Tree loss: 3.522 | Accuracy: 0.068359 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 019 | Total loss: 3.510 | Reg loss: 0.023 | Tree loss: 3.510 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 019 | Total loss: 3.530 | Reg loss: 0.023 | Tree loss: 3.530 | Accuracy: 0.056641 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.072266 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 019 | Total loss: 3.501 | Reg loss: 0.023 | Tree loss: 3.501 | Accuracy: 0.074219 | 0.863 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 019 | Total loss: 3.504 | Reg loss: 0.023 | Tree loss: 3.504 | Accuracy: 0.083984 | 0.862 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 019 | Total loss: 3.554 | Reg loss: 0.023 | Tree loss: 3.554 | Accuracy: 0.066202 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.083984 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 019 | Total loss: 3.513 | Reg loss: 0.023 | Tree loss: 3.513 | Accuracy: 0.085938 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 019 | Total loss: 3.481 | Reg loss: 0.023 | Tree loss: 3.481 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 019 | Total loss: 3.497 | Reg loss: 0.023 | Tree loss: 3.497 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.068359 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 019 | Total loss: 3.553 | Reg loss: 0.023 | Tree loss: 3.553 | Accuracy: 0.064453 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 019 | Total loss: 3.494 | Reg loss: 0.023 | Tree loss: 3.494 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 019 | Total loss: 3.479 | Reg loss: 0.023 | Tree loss: 3.479 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 019 | Total loss: 3.501 | Reg loss: 0.023 | Tree loss: 3.501 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 019 | Total loss: 3.497 | Reg loss: 0.023 | Tree loss: 3.497 | Accuracy: 0.070312 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 019 | Total loss: 3.497 | Reg loss: 0.023 | Tree loss: 3.497 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 019 | Total loss: 3.526 | Reg loss: 0.023 | Tree loss: 3.526 | Accuracy: 0.068359 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 019 | Total loss: 3.479 | Reg loss: 0.023 | Tree loss: 3.479 | Accuracy: 0.097656 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 019 | Total loss: 3.514 | Reg loss: 0.023 | Tree loss: 3.514 | Accuracy: 0.068359 | 0.863 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 019 | Total loss: 3.466 | Reg loss: 0.023 | Tree loss: 3.466 | Accuracy: 0.078125 | 0.862 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 019 | Total loss: 3.490 | Reg loss: 0.023 | Tree loss: 3.490 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.076172 | 0.862 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 019 | Total loss: 3.531 | Reg loss: 0.023 | Tree loss: 3.531 | Accuracy: 0.064453 | 0.862 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 019 | Total loss: 3.540 | Reg loss: 0.023 | Tree loss: 3.540 | Accuracy: 0.069686 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 019 | Total loss: 3.511 | Reg loss: 0.023 | Tree loss: 3.511 | Accuracy: 0.064453 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 019 | Total loss: 3.511 | Reg loss: 0.023 | Tree loss: 3.511 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 019 | Total loss: 3.513 | Reg loss: 0.023 | Tree loss: 3.513 | Accuracy: 0.070312 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 019 | Total loss: 3.515 | Reg loss: 0.023 | Tree loss: 3.515 | Accuracy: 0.064453 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 019 | Total loss: 3.495 | Reg loss: 0.023 | Tree loss: 3.495 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 019 | Total loss: 3.508 | Reg loss: 0.023 | Tree loss: 3.508 | Accuracy: 0.062500 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 019 | Total loss: 3.504 | Reg loss: 0.023 | Tree loss: 3.504 | Accuracy: 0.066406 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 019 | Total loss: 3.488 | Reg loss: 0.023 | Tree loss: 3.488 | Accuracy: 0.087891 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 019 | Total loss: 3.522 | Reg loss: 0.023 | Tree loss: 3.522 | Accuracy: 0.074219 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 019 | Total loss: 3.532 | Reg loss: 0.023 | Tree loss: 3.532 | Accuracy: 0.070312 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 019 | Total loss: 3.498 | Reg loss: 0.023 | Tree loss: 3.498 | Accuracy: 0.089844 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 019 | Total loss: 3.482 | Reg loss: 0.023 | Tree loss: 3.482 | Accuracy: 0.101562 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 019 | Total loss: 3.495 | Reg loss: 0.023 | Tree loss: 3.495 | Accuracy: 0.099609 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 019 | Total loss: 3.517 | Reg loss: 0.023 | Tree loss: 3.517 | Accuracy: 0.082031 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 019 | Total loss: 3.493 | Reg loss: 0.023 | Tree loss: 3.493 | Accuracy: 0.078125 | 0.863 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 019 | Total loss: 3.507 | Reg loss: 0.023 | Tree loss: 3.507 | Accuracy: 0.056641 | 0.862 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 019 | Total loss: 3.462 | Reg loss: 0.023 | Tree loss: 3.462 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 019 | Total loss: 3.470 | Reg loss: 0.023 | Tree loss: 3.470 | Accuracy: 0.085938 | 0.862 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 019 | Total loss: 3.542 | Reg loss: 0.023 | Tree loss: 3.542 | Accuracy: 0.076655 | 0.862 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91df9966e009490bab832d91a1745d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23df764c90ff4ae2978ced01d691f891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be37f7b6537479294aedbbf9551c3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5443ed1fa44daf819a48c994319854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 10.0\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "9503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "Average comprehensibility: 44.109375\n",
      "std comprehensibility: 2.418142284766345\n",
      "var comprehensibility: 5.847412109375\n",
      "minimum comprehensibility: 40\n",
      "maximum comprehensibility: 52\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
