{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 128\n",
    "tree_depth = 8\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.153863906860352 | KNN Loss: 6.230323791503906 | BCE Loss: 1.9235402345657349\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.229302406311035 | KNN Loss: 6.230156898498535 | BCE Loss: 1.999145746231079\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.185433387756348 | KNN Loss: 6.230011940002441 | BCE Loss: 1.9554216861724854\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.193683624267578 | KNN Loss: 6.230076313018799 | BCE Loss: 1.9636069536209106\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.184626579284668 | KNN Loss: 6.2299370765686035 | BCE Loss: 1.9546892642974854\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.163071632385254 | KNN Loss: 6.229653358459473 | BCE Loss: 1.9334180355072021\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.137682914733887 | KNN Loss: 6.229648113250732 | BCE Loss: 1.9080350399017334\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.162541389465332 | KNN Loss: 6.229461669921875 | BCE Loss: 1.933079719543457\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.118985176086426 | KNN Loss: 6.229536533355713 | BCE Loss: 1.889448881149292\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.142929077148438 | KNN Loss: 6.229275703430176 | BCE Loss: 1.9136528968811035\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.12154769897461 | KNN Loss: 6.229067802429199 | BCE Loss: 1.8924803733825684\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.144716262817383 | KNN Loss: 6.229196071624756 | BCE Loss: 1.915520191192627\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.145401000976562 | KNN Loss: 6.228511810302734 | BCE Loss: 1.9168891906738281\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.131311416625977 | KNN Loss: 6.228824615478516 | BCE Loss: 1.9024865627288818\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.066969871520996 | KNN Loss: 6.228710174560547 | BCE Loss: 1.8382600545883179\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.104442596435547 | KNN Loss: 6.228391647338867 | BCE Loss: 1.8760511875152588\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.100446701049805 | KNN Loss: 6.228266716003418 | BCE Loss: 1.8721802234649658\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.059196472167969 | KNN Loss: 6.227923393249512 | BCE Loss: 1.8312734365463257\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.095220565795898 | KNN Loss: 6.227579593658447 | BCE Loss: 1.8676412105560303\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.044354438781738 | KNN Loss: 6.227630138397217 | BCE Loss: 1.8167243003845215\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.097719192504883 | KNN Loss: 6.226822376251221 | BCE Loss: 1.870896577835083\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.067414283752441 | KNN Loss: 6.22684383392334 | BCE Loss: 1.840570092201233\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.046445846557617 | KNN Loss: 6.226562976837158 | BCE Loss: 1.8198823928833008\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.01649284362793 | KNN Loss: 6.226336479187012 | BCE Loss: 1.7901562452316284\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.041465759277344 | KNN Loss: 6.226116180419922 | BCE Loss: 1.8153499364852905\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.022844314575195 | KNN Loss: 6.225617408752441 | BCE Loss: 1.7972265481948853\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.997509956359863 | KNN Loss: 6.225058555603027 | BCE Loss: 1.7724515199661255\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.003561019897461 | KNN Loss: 6.2242279052734375 | BCE Loss: 1.7793328762054443\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.940597057342529 | KNN Loss: 6.223990440368652 | BCE Loss: 1.716606616973877\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 8.001953125 | KNN Loss: 6.222801208496094 | BCE Loss: 1.7791520357131958\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.968283176422119 | KNN Loss: 6.2233123779296875 | BCE Loss: 1.744970679283142\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.956923961639404 | KNN Loss: 6.2220869064331055 | BCE Loss: 1.7348370552062988\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.902044773101807 | KNN Loss: 6.222376823425293 | BCE Loss: 1.6796679496765137\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.926777362823486 | KNN Loss: 6.21981143951416 | BCE Loss: 1.7069659233093262\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.885090351104736 | KNN Loss: 6.220095634460449 | BCE Loss: 1.664994716644287\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.857508659362793 | KNN Loss: 6.218662261962891 | BCE Loss: 1.6388461589813232\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.853891372680664 | KNN Loss: 6.21834659576416 | BCE Loss: 1.635544776916504\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.876283168792725 | KNN Loss: 6.217724800109863 | BCE Loss: 1.6585583686828613\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.870704174041748 | KNN Loss: 6.215367317199707 | BCE Loss: 1.655336856842041\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.832825660705566 | KNN Loss: 6.214102745056152 | BCE Loss: 1.618722677230835\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.827480316162109 | KNN Loss: 6.2122578620910645 | BCE Loss: 1.615222692489624\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.82239294052124 | KNN Loss: 6.2112860679626465 | BCE Loss: 1.6111067533493042\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.763332366943359 | KNN Loss: 6.207844257354736 | BCE Loss: 1.555487871170044\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.750465393066406 | KNN Loss: 6.20518684387207 | BCE Loss: 1.5452783107757568\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.720391750335693 | KNN Loss: 6.203319549560547 | BCE Loss: 1.5170722007751465\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.717005729675293 | KNN Loss: 6.198976039886475 | BCE Loss: 1.5180296897888184\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.698523044586182 | KNN Loss: 6.192815780639648 | BCE Loss: 1.5057072639465332\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.649903297424316 | KNN Loss: 6.193786144256592 | BCE Loss: 1.4561169147491455\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.63020133972168 | KNN Loss: 6.184977054595947 | BCE Loss: 1.4452242851257324\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.5934038162231445 | KNN Loss: 6.182863235473633 | BCE Loss: 1.4105408191680908\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.557951927185059 | KNN Loss: 6.174767971038818 | BCE Loss: 1.3831837177276611\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.563529014587402 | KNN Loss: 6.166167736053467 | BCE Loss: 1.397361397743225\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.508270740509033 | KNN Loss: 6.161741733551025 | BCE Loss: 1.3465290069580078\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.490508556365967 | KNN Loss: 6.157637596130371 | BCE Loss: 1.3328710794448853\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.453672885894775 | KNN Loss: 6.140854358673096 | BCE Loss: 1.3128185272216797\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.438794136047363 | KNN Loss: 6.136620044708252 | BCE Loss: 1.3021740913391113\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.412671089172363 | KNN Loss: 6.11793851852417 | BCE Loss: 1.2947323322296143\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.397709369659424 | KNN Loss: 6.108307838439941 | BCE Loss: 1.2894015312194824\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.351540565490723 | KNN Loss: 6.090646266937256 | BCE Loss: 1.260894536972046\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.325039863586426 | KNN Loss: 6.084701061248779 | BCE Loss: 1.2403388023376465\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.270391464233398 | KNN Loss: 6.060339450836182 | BCE Loss: 1.2100518941879272\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.244955539703369 | KNN Loss: 6.046128749847412 | BCE Loss: 1.198826789855957\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.212120532989502 | KNN Loss: 6.026167392730713 | BCE Loss: 1.185953140258789\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.146071434020996 | KNN Loss: 5.985598564147949 | BCE Loss: 1.1604727506637573\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.14503288269043 | KNN Loss: 5.96385383605957 | BCE Loss: 1.1811790466308594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.0865478515625 | KNN Loss: 5.937524795532227 | BCE Loss: 1.1490232944488525\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.04889440536499 | KNN Loss: 5.923275470733643 | BCE Loss: 1.1256190538406372\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.000793933868408 | KNN Loss: 5.882357597351074 | BCE Loss: 1.118436336517334\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.964555740356445 | KNN Loss: 5.85343074798584 | BCE Loss: 1.1111252307891846\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.921308517456055 | KNN Loss: 5.802789211273193 | BCE Loss: 1.1185193061828613\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.896669864654541 | KNN Loss: 5.802331447601318 | BCE Loss: 1.0943384170532227\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.820957183837891 | KNN Loss: 5.726874828338623 | BCE Loss: 1.0940823554992676\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.794429779052734 | KNN Loss: 5.678097248077393 | BCE Loss: 1.116332769393921\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.766641616821289 | KNN Loss: 5.654980659484863 | BCE Loss: 1.1116607189178467\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 6.6904191970825195 | KNN Loss: 5.59511137008667 | BCE Loss: 1.0953078269958496\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 6.70416784286499 | KNN Loss: 5.584178447723389 | BCE Loss: 1.1199893951416016\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 6.630037784576416 | KNN Loss: 5.528404235839844 | BCE Loss: 1.1016334295272827\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 6.547292232513428 | KNN Loss: 5.488637447357178 | BCE Loss: 1.05865478515625\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 6.52039098739624 | KNN Loss: 5.448948383331299 | BCE Loss: 1.0714426040649414\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 6.489464282989502 | KNN Loss: 5.40797758102417 | BCE Loss: 1.0814865827560425\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 6.520149230957031 | KNN Loss: 5.419844150543213 | BCE Loss: 1.1003053188323975\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 6.421998977661133 | KNN Loss: 5.349334716796875 | BCE Loss: 1.072664499282837\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 6.4337568283081055 | KNN Loss: 5.334257125854492 | BCE Loss: 1.0994994640350342\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 6.394307613372803 | KNN Loss: 5.311588287353516 | BCE Loss: 1.082719326019287\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 6.369666576385498 | KNN Loss: 5.2721428871154785 | BCE Loss: 1.0975236892700195\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 6.356287002563477 | KNN Loss: 5.268697261810303 | BCE Loss: 1.087589979171753\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 6.305850982666016 | KNN Loss: 5.229687213897705 | BCE Loss: 1.0761640071868896\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 6.2967071533203125 | KNN Loss: 5.2136454582214355 | BCE Loss: 1.0830618143081665\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 6.266397476196289 | KNN Loss: 5.20259952545166 | BCE Loss: 1.0637977123260498\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 6.308460235595703 | KNN Loss: 5.215117454528809 | BCE Loss: 1.0933430194854736\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 6.249405860900879 | KNN Loss: 5.181247711181641 | BCE Loss: 1.0681583881378174\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 6.227463722229004 | KNN Loss: 5.164358615875244 | BCE Loss: 1.0631052255630493\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 6.250500679016113 | KNN Loss: 5.175586223602295 | BCE Loss: 1.0749146938323975\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 6.212810516357422 | KNN Loss: 5.16084098815918 | BCE Loss: 1.0519696474075317\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 6.202720642089844 | KNN Loss: 5.12778377532959 | BCE Loss: 1.074937105178833\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 6.203164100646973 | KNN Loss: 5.142337799072266 | BCE Loss: 1.060826063156128\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.2081122398376465 | KNN Loss: 5.13226842880249 | BCE Loss: 1.0758439302444458\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 6.156020164489746 | KNN Loss: 5.11257791519165 | BCE Loss: 1.0434420108795166\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.229008674621582 | KNN Loss: 5.151733875274658 | BCE Loss: 1.0772747993469238\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.171841144561768 | KNN Loss: 5.1242475509643555 | BCE Loss: 1.0475934743881226\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.19583797454834 | KNN Loss: 5.120002269744873 | BCE Loss: 1.0758354663848877\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.1943559646606445 | KNN Loss: 5.126948833465576 | BCE Loss: 1.0674068927764893\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.188420295715332 | KNN Loss: 5.105844497680664 | BCE Loss: 1.082576036453247\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.201587677001953 | KNN Loss: 5.106543064117432 | BCE Loss: 1.095044732093811\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.162696361541748 | KNN Loss: 5.096621513366699 | BCE Loss: 1.0660747289657593\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.158598899841309 | KNN Loss: 5.097654819488525 | BCE Loss: 1.0609440803527832\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.142136573791504 | KNN Loss: 5.091959476470947 | BCE Loss: 1.0501773357391357\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.233790397644043 | KNN Loss: 5.151246547698975 | BCE Loss: 1.0825436115264893\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.166699409484863 | KNN Loss: 5.101314067840576 | BCE Loss: 1.065385103225708\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.174082279205322 | KNN Loss: 5.1029229164123535 | BCE Loss: 1.0711592435836792\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.218874931335449 | KNN Loss: 5.121923923492432 | BCE Loss: 1.0969507694244385\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.15342903137207 | KNN Loss: 5.084900856018066 | BCE Loss: 1.068528413772583\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.162107467651367 | KNN Loss: 5.096001625061035 | BCE Loss: 1.066105842590332\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.132686138153076 | KNN Loss: 5.076675891876221 | BCE Loss: 1.0560102462768555\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.180891036987305 | KNN Loss: 5.09614896774292 | BCE Loss: 1.0847421884536743\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.122200012207031 | KNN Loss: 5.076586723327637 | BCE Loss: 1.0456135272979736\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.150851249694824 | KNN Loss: 5.083016872406006 | BCE Loss: 1.0678341388702393\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.13776969909668 | KNN Loss: 5.085068225860596 | BCE Loss: 1.052701473236084\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.11666202545166 | KNN Loss: 5.074153900146484 | BCE Loss: 1.0425081253051758\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.140599250793457 | KNN Loss: 5.083915710449219 | BCE Loss: 1.0566833019256592\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.10689640045166 | KNN Loss: 5.065361022949219 | BCE Loss: 1.0415351390838623\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.1334757804870605 | KNN Loss: 5.074631214141846 | BCE Loss: 1.0588446855545044\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.114011764526367 | KNN Loss: 5.086197853088379 | BCE Loss: 1.0278141498565674\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.156500339508057 | KNN Loss: 5.076467037200928 | BCE Loss: 1.080033302307129\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.108380317687988 | KNN Loss: 5.0689849853515625 | BCE Loss: 1.0393953323364258\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.170421123504639 | KNN Loss: 5.081712245941162 | BCE Loss: 1.088708758354187\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.135612487792969 | KNN Loss: 5.0712103843688965 | BCE Loss: 1.0644021034240723\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.139069557189941 | KNN Loss: 5.0646257400512695 | BCE Loss: 1.0744439363479614\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.135549545288086 | KNN Loss: 5.077836513519287 | BCE Loss: 1.057713270187378\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.1456499099731445 | KNN Loss: 5.065340995788574 | BCE Loss: 1.0803090333938599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.182769298553467 | KNN Loss: 5.102409362792969 | BCE Loss: 1.0803598165512085\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.118555068969727 | KNN Loss: 5.076013088226318 | BCE Loss: 1.042541742324829\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.144984245300293 | KNN Loss: 5.0769243240356445 | BCE Loss: 1.0680601596832275\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.151422500610352 | KNN Loss: 5.071323394775391 | BCE Loss: 1.080099105834961\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.122405052185059 | KNN Loss: 5.060118675231934 | BCE Loss: 1.062286138534546\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.124342918395996 | KNN Loss: 5.078787326812744 | BCE Loss: 1.0455553531646729\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.1400980949401855 | KNN Loss: 5.086660861968994 | BCE Loss: 1.0534371137619019\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.175093173980713 | KNN Loss: 5.101563930511475 | BCE Loss: 1.0735293626785278\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.144304275512695 | KNN Loss: 5.075284481048584 | BCE Loss: 1.0690195560455322\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.139830112457275 | KNN Loss: 5.077171325683594 | BCE Loss: 1.062658667564392\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.15024471282959 | KNN Loss: 5.065664291381836 | BCE Loss: 1.084580659866333\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.098047733306885 | KNN Loss: 5.052783966064453 | BCE Loss: 1.0452638864517212\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.095313549041748 | KNN Loss: 5.060986518859863 | BCE Loss: 1.0343270301818848\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.131633281707764 | KNN Loss: 5.0562238693237305 | BCE Loss: 1.0754092931747437\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.12808084487915 | KNN Loss: 5.058366298675537 | BCE Loss: 1.0697144269943237\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.188741683959961 | KNN Loss: 5.103573322296143 | BCE Loss: 1.0851681232452393\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.121502876281738 | KNN Loss: 5.081260681152344 | BCE Loss: 1.0402419567108154\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.141439914703369 | KNN Loss: 5.071885108947754 | BCE Loss: 1.0695548057556152\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.141241550445557 | KNN Loss: 5.081300735473633 | BCE Loss: 1.0599406957626343\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.1191253662109375 | KNN Loss: 5.0659403800964355 | BCE Loss: 1.053185224533081\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.120907783508301 | KNN Loss: 5.054940700531006 | BCE Loss: 1.065967321395874\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.158159255981445 | KNN Loss: 5.101677417755127 | BCE Loss: 1.0564818382263184\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.12595796585083 | KNN Loss: 5.07050895690918 | BCE Loss: 1.0554490089416504\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.141597270965576 | KNN Loss: 5.0918049812316895 | BCE Loss: 1.0497922897338867\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.114592552185059 | KNN Loss: 5.049818515777588 | BCE Loss: 1.0647740364074707\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.1129937171936035 | KNN Loss: 5.06030797958374 | BCE Loss: 1.0526857376098633\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.1554059982299805 | KNN Loss: 5.118156433105469 | BCE Loss: 1.0372498035430908\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.118106842041016 | KNN Loss: 5.059262275695801 | BCE Loss: 1.0588443279266357\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.1314215660095215 | KNN Loss: 5.0590667724609375 | BCE Loss: 1.072354793548584\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.093353271484375 | KNN Loss: 5.05683708190918 | BCE Loss: 1.0365159511566162\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.089539051055908 | KNN Loss: 5.057389736175537 | BCE Loss: 1.032149314880371\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.13748836517334 | KNN Loss: 5.052892684936523 | BCE Loss: 1.0845959186553955\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.137816905975342 | KNN Loss: 5.0742669105529785 | BCE Loss: 1.0635499954223633\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.143558502197266 | KNN Loss: 5.099110126495361 | BCE Loss: 1.0444482564926147\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.121131420135498 | KNN Loss: 5.064886569976807 | BCE Loss: 1.0562447309494019\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.129479885101318 | KNN Loss: 5.073835849761963 | BCE Loss: 1.0556440353393555\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.098318099975586 | KNN Loss: 5.05119514465332 | BCE Loss: 1.0471231937408447\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.12108039855957 | KNN Loss: 5.067251205444336 | BCE Loss: 1.0538291931152344\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.089953422546387 | KNN Loss: 5.056083679199219 | BCE Loss: 1.033869743347168\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.0949578285217285 | KNN Loss: 5.061453819274902 | BCE Loss: 1.0335040092468262\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.11517333984375 | KNN Loss: 5.04783296585083 | BCE Loss: 1.067340612411499\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.145351886749268 | KNN Loss: 5.0638508796691895 | BCE Loss: 1.0815010070800781\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.122674465179443 | KNN Loss: 5.047572135925293 | BCE Loss: 1.0751023292541504\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.103972434997559 | KNN Loss: 5.048774719238281 | BCE Loss: 1.0551977157592773\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.1737871170043945 | KNN Loss: 5.113194465637207 | BCE Loss: 1.0605928897857666\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.135375499725342 | KNN Loss: 5.069095134735107 | BCE Loss: 1.0662803649902344\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.095472812652588 | KNN Loss: 5.046959400177002 | BCE Loss: 1.0485135316848755\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.111627101898193 | KNN Loss: 5.054919719696045 | BCE Loss: 1.056707501411438\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.080229759216309 | KNN Loss: 5.045613765716553 | BCE Loss: 1.0346157550811768\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.120266437530518 | KNN Loss: 5.058412551879883 | BCE Loss: 1.0618540048599243\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.084385871887207 | KNN Loss: 5.042764186859131 | BCE Loss: 1.0416219234466553\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.12065315246582 | KNN Loss: 5.046041011810303 | BCE Loss: 1.0746119022369385\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.11250638961792 | KNN Loss: 5.063435077667236 | BCE Loss: 1.0490713119506836\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.07591438293457 | KNN Loss: 5.043341636657715 | BCE Loss: 1.0325729846954346\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.121662139892578 | KNN Loss: 5.045845031738281 | BCE Loss: 1.0758172273635864\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.1187286376953125 | KNN Loss: 5.047532081604004 | BCE Loss: 1.0711965560913086\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.092309951782227 | KNN Loss: 5.043216228485107 | BCE Loss: 1.0490936040878296\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.108037948608398 | KNN Loss: 5.052297592163086 | BCE Loss: 1.0557401180267334\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.10118293762207 | KNN Loss: 5.0514397621154785 | BCE Loss: 1.049743413925171\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.089461803436279 | KNN Loss: 5.057185649871826 | BCE Loss: 1.0322761535644531\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.12403678894043 | KNN Loss: 5.057862758636475 | BCE Loss: 1.066173791885376\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.140017986297607 | KNN Loss: 5.054818153381348 | BCE Loss: 1.0851998329162598\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.120200157165527 | KNN Loss: 5.0506272315979 | BCE Loss: 1.069573163986206\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.106241226196289 | KNN Loss: 5.06349515914917 | BCE Loss: 1.0427463054656982\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.1164631843566895 | KNN Loss: 5.0528645515441895 | BCE Loss: 1.0635985136032104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.176115989685059 | KNN Loss: 5.059687614440918 | BCE Loss: 1.1164283752441406\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.091367721557617 | KNN Loss: 5.044297218322754 | BCE Loss: 1.0470705032348633\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.087880611419678 | KNN Loss: 5.050999641418457 | BCE Loss: 1.0368809700012207\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.07302188873291 | KNN Loss: 5.0403151512146 | BCE Loss: 1.0327069759368896\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.10439395904541 | KNN Loss: 5.044567108154297 | BCE Loss: 1.0598266124725342\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.067069053649902 | KNN Loss: 5.060004711151123 | BCE Loss: 1.0070645809173584\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.131958484649658 | KNN Loss: 5.058707237243652 | BCE Loss: 1.0732513666152954\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.127476692199707 | KNN Loss: 5.06211519241333 | BCE Loss: 1.065361499786377\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.0760111808776855 | KNN Loss: 5.0450215339660645 | BCE Loss: 1.030989646911621\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.124858856201172 | KNN Loss: 5.050825595855713 | BCE Loss: 1.074033260345459\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.117886543273926 | KNN Loss: 5.055337905883789 | BCE Loss: 1.0625485181808472\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.092383861541748 | KNN Loss: 5.042572498321533 | BCE Loss: 1.0498113632202148\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.064708709716797 | KNN Loss: 5.032039642333984 | BCE Loss: 1.032668948173523\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.11153507232666 | KNN Loss: 5.048654556274414 | BCE Loss: 1.0628803968429565\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.176276206970215 | KNN Loss: 5.100766181945801 | BCE Loss: 1.0755102634429932\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.079738616943359 | KNN Loss: 5.047840118408203 | BCE Loss: 1.0318984985351562\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.112700939178467 | KNN Loss: 5.064107418060303 | BCE Loss: 1.0485934019088745\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.114282608032227 | KNN Loss: 5.05800199508667 | BCE Loss: 1.056280493736267\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.0990190505981445 | KNN Loss: 5.042149066925049 | BCE Loss: 1.0568697452545166\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.099421501159668 | KNN Loss: 5.048576831817627 | BCE Loss: 1.0508447885513306\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.13923978805542 | KNN Loss: 5.075879096984863 | BCE Loss: 1.0633608102798462\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.103035926818848 | KNN Loss: 5.072722434997559 | BCE Loss: 1.030313491821289\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.106619834899902 | KNN Loss: 5.039981842041016 | BCE Loss: 1.0666377544403076\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.125856399536133 | KNN Loss: 5.0473151206970215 | BCE Loss: 1.0785412788391113\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.115104675292969 | KNN Loss: 5.04710578918457 | BCE Loss: 1.0679991245269775\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.104095935821533 | KNN Loss: 5.047917366027832 | BCE Loss: 1.0561784505844116\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.115860462188721 | KNN Loss: 5.04312801361084 | BCE Loss: 1.0727324485778809\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.100318431854248 | KNN Loss: 5.041877746582031 | BCE Loss: 1.0584406852722168\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.082977771759033 | KNN Loss: 5.049145221710205 | BCE Loss: 1.0338325500488281\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.106045722961426 | KNN Loss: 5.060034275054932 | BCE Loss: 1.046011209487915\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.114044666290283 | KNN Loss: 5.043461322784424 | BCE Loss: 1.070583462715149\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.169496059417725 | KNN Loss: 5.109383583068848 | BCE Loss: 1.0601125955581665\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.075300693511963 | KNN Loss: 5.03261661529541 | BCE Loss: 1.0426840782165527\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.097787380218506 | KNN Loss: 5.052680969238281 | BCE Loss: 1.0451065301895142\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.13225793838501 | KNN Loss: 5.083356857299805 | BCE Loss: 1.0489009618759155\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.102173328399658 | KNN Loss: 5.058651924133301 | BCE Loss: 1.0435212850570679\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.098472595214844 | KNN Loss: 5.063815593719482 | BCE Loss: 1.0346570014953613\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.071467399597168 | KNN Loss: 5.036375045776367 | BCE Loss: 1.0350925922393799\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.128119945526123 | KNN Loss: 5.056893825531006 | BCE Loss: 1.0712260007858276\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.122632026672363 | KNN Loss: 5.051441192626953 | BCE Loss: 1.071190595626831\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.081151008605957 | KNN Loss: 5.038116455078125 | BCE Loss: 1.0430347919464111\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.101498603820801 | KNN Loss: 5.069294452667236 | BCE Loss: 1.0322043895721436\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.102022171020508 | KNN Loss: 5.048488616943359 | BCE Loss: 1.0535335540771484\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.0796356201171875 | KNN Loss: 5.0425496101379395 | BCE Loss: 1.037085771560669\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.101626396179199 | KNN Loss: 5.063828945159912 | BCE Loss: 1.037797451019287\n",
      "Epoch    40: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.094372749328613 | KNN Loss: 5.041811466217041 | BCE Loss: 1.0525612831115723\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.123089790344238 | KNN Loss: 5.049858093261719 | BCE Loss: 1.0732316970825195\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.091756820678711 | KNN Loss: 5.041251182556152 | BCE Loss: 1.0505058765411377\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.1064324378967285 | KNN Loss: 5.061882019042969 | BCE Loss: 1.0445505380630493\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.115287780761719 | KNN Loss: 5.068287372589111 | BCE Loss: 1.0470002889633179\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.119997978210449 | KNN Loss: 5.058892726898193 | BCE Loss: 1.0611053705215454\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.076396465301514 | KNN Loss: 5.047948837280273 | BCE Loss: 1.0284476280212402\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.0906171798706055 | KNN Loss: 5.0342116355896 | BCE Loss: 1.0564054250717163\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.154664993286133 | KNN Loss: 5.081185340881348 | BCE Loss: 1.0734798908233643\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.096915245056152 | KNN Loss: 5.046643257141113 | BCE Loss: 1.05027174949646\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.109986782073975 | KNN Loss: 5.040904998779297 | BCE Loss: 1.0690819025039673\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.083137512207031 | KNN Loss: 5.040369033813477 | BCE Loss: 1.0427683591842651\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.137219429016113 | KNN Loss: 5.074880123138428 | BCE Loss: 1.0623390674591064\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.121560096740723 | KNN Loss: 5.046545505523682 | BCE Loss: 1.0750147104263306\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.098912239074707 | KNN Loss: 5.0535712242126465 | BCE Loss: 1.0453410148620605\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.105046272277832 | KNN Loss: 5.050962924957275 | BCE Loss: 1.0540833473205566\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.076936721801758 | KNN Loss: 5.031693458557129 | BCE Loss: 1.0452430248260498\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.123043060302734 | KNN Loss: 5.056155681610107 | BCE Loss: 1.066887378692627\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.120212554931641 | KNN Loss: 5.0626349449157715 | BCE Loss: 1.0575778484344482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.08839225769043 | KNN Loss: 5.038623809814453 | BCE Loss: 1.0497686862945557\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.06320333480835 | KNN Loss: 5.032927989959717 | BCE Loss: 1.0302753448486328\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.117987632751465 | KNN Loss: 5.043788909912109 | BCE Loss: 1.0741989612579346\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.118149280548096 | KNN Loss: 5.034641265869141 | BCE Loss: 1.0835081338882446\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.092503547668457 | KNN Loss: 5.027092933654785 | BCE Loss: 1.0654107332229614\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.100611686706543 | KNN Loss: 5.050936222076416 | BCE Loss: 1.0496752262115479\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.132219314575195 | KNN Loss: 5.0503973960876465 | BCE Loss: 1.0818217992782593\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.072502136230469 | KNN Loss: 5.048242568969727 | BCE Loss: 1.0242595672607422\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.09926176071167 | KNN Loss: 5.028000831604004 | BCE Loss: 1.0712610483169556\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.067967414855957 | KNN Loss: 5.046067714691162 | BCE Loss: 1.021899938583374\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.105452537536621 | KNN Loss: 5.0391316413879395 | BCE Loss: 1.0663206577301025\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.091855525970459 | KNN Loss: 5.044573783874512 | BCE Loss: 1.0472818613052368\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.092594146728516 | KNN Loss: 5.046378135681152 | BCE Loss: 1.0462158918380737\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.093534469604492 | KNN Loss: 5.029752254486084 | BCE Loss: 1.063781976699829\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.078065872192383 | KNN Loss: 5.034926414489746 | BCE Loss: 1.0431393384933472\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.137090682983398 | KNN Loss: 5.092038154602051 | BCE Loss: 1.0450525283813477\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.082406044006348 | KNN Loss: 5.035421848297119 | BCE Loss: 1.0469841957092285\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.1363911628723145 | KNN Loss: 5.046477794647217 | BCE Loss: 1.0899134874343872\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.083422660827637 | KNN Loss: 5.038708686828613 | BCE Loss: 1.0447138547897339\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.079617500305176 | KNN Loss: 5.031179904937744 | BCE Loss: 1.048437476158142\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.1035356521606445 | KNN Loss: 5.0488457679748535 | BCE Loss: 1.0546901226043701\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.189451217651367 | KNN Loss: 5.100345611572266 | BCE Loss: 1.0891057252883911\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.08228874206543 | KNN Loss: 5.031303405761719 | BCE Loss: 1.05098557472229\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.071911811828613 | KNN Loss: 5.037363529205322 | BCE Loss: 1.034548282623291\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.094214916229248 | KNN Loss: 5.038382530212402 | BCE Loss: 1.0558323860168457\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.108047008514404 | KNN Loss: 5.035248756408691 | BCE Loss: 1.0727981328964233\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.070084095001221 | KNN Loss: 5.032077312469482 | BCE Loss: 1.0380067825317383\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.12761116027832 | KNN Loss: 5.076009750366211 | BCE Loss: 1.0516014099121094\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.069104194641113 | KNN Loss: 5.036525726318359 | BCE Loss: 1.032578468322754\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.053670883178711 | KNN Loss: 5.0300703048706055 | BCE Loss: 1.0236008167266846\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.096421718597412 | KNN Loss: 5.057828426361084 | BCE Loss: 1.0385932922363281\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.1013593673706055 | KNN Loss: 5.037725448608398 | BCE Loss: 1.063633918762207\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.068770408630371 | KNN Loss: 5.033894062042236 | BCE Loss: 1.0348762273788452\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.112799644470215 | KNN Loss: 5.056122779846191 | BCE Loss: 1.0566771030426025\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.0763349533081055 | KNN Loss: 5.026158809661865 | BCE Loss: 1.0501763820648193\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.097724437713623 | KNN Loss: 5.0408430099487305 | BCE Loss: 1.0568815469741821\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.085716724395752 | KNN Loss: 5.043678283691406 | BCE Loss: 1.0420384407043457\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.068448066711426 | KNN Loss: 5.030541896820068 | BCE Loss: 1.0379061698913574\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.078104019165039 | KNN Loss: 5.035968780517578 | BCE Loss: 1.042135238647461\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.115606307983398 | KNN Loss: 5.032066822052002 | BCE Loss: 1.0835397243499756\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.070314407348633 | KNN Loss: 5.037989616394043 | BCE Loss: 1.0323245525360107\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.093677997589111 | KNN Loss: 5.046671390533447 | BCE Loss: 1.0470064878463745\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.093514442443848 | KNN Loss: 5.039046287536621 | BCE Loss: 1.0544683933258057\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.063563346862793 | KNN Loss: 5.041194915771484 | BCE Loss: 1.0223686695098877\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.151843070983887 | KNN Loss: 5.091753959655762 | BCE Loss: 1.060088872909546\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.112758636474609 | KNN Loss: 5.057880878448486 | BCE Loss: 1.054877519607544\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.0494890213012695 | KNN Loss: 5.0272393226623535 | BCE Loss: 1.0222499370574951\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.135250568389893 | KNN Loss: 5.055093765258789 | BCE Loss: 1.080156683921814\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.0774760246276855 | KNN Loss: 5.0475311279296875 | BCE Loss: 1.0299447774887085\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.088083267211914 | KNN Loss: 5.032230377197266 | BCE Loss: 1.055853009223938\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.100327968597412 | KNN Loss: 5.045060634613037 | BCE Loss: 1.0552672147750854\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.163423538208008 | KNN Loss: 5.079416751861572 | BCE Loss: 1.0840067863464355\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.09942626953125 | KNN Loss: 5.033867835998535 | BCE Loss: 1.0655581951141357\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.084044456481934 | KNN Loss: 5.041310787200928 | BCE Loss: 1.0427334308624268\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.080840587615967 | KNN Loss: 5.036698341369629 | BCE Loss: 1.0441423654556274\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.155564785003662 | KNN Loss: 5.087160587310791 | BCE Loss: 1.068404197692871\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.1119537353515625 | KNN Loss: 5.037816047668457 | BCE Loss: 1.0741374492645264\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.061488151550293 | KNN Loss: 5.032900333404541 | BCE Loss: 1.028587818145752\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.0634050369262695 | KNN Loss: 5.045976161956787 | BCE Loss: 1.0174291133880615\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.085562229156494 | KNN Loss: 5.028598308563232 | BCE Loss: 1.0569638013839722\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.082699775695801 | KNN Loss: 5.052906036376953 | BCE Loss: 1.0297935009002686\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.0773491859436035 | KNN Loss: 5.032292366027832 | BCE Loss: 1.0450568199157715\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.086197853088379 | KNN Loss: 5.039379119873047 | BCE Loss: 1.046818733215332\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.098313808441162 | KNN Loss: 5.03834867477417 | BCE Loss: 1.0599651336669922\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.118791580200195 | KNN Loss: 5.068840026855469 | BCE Loss: 1.0499515533447266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.051682949066162 | KNN Loss: 5.028656959533691 | BCE Loss: 1.0230261087417603\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.083219528198242 | KNN Loss: 5.0402140617370605 | BCE Loss: 1.0430057048797607\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.18244743347168 | KNN Loss: 5.131138801574707 | BCE Loss: 1.0513088703155518\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.079896450042725 | KNN Loss: 5.0313334465026855 | BCE Loss: 1.0485631227493286\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.06731653213501 | KNN Loss: 5.034237861633301 | BCE Loss: 1.0330787897109985\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.079414367675781 | KNN Loss: 5.027195453643799 | BCE Loss: 1.0522189140319824\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.058646202087402 | KNN Loss: 5.027067184448242 | BCE Loss: 1.031578779220581\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.072388648986816 | KNN Loss: 5.057621955871582 | BCE Loss: 1.0147664546966553\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.082666397094727 | KNN Loss: 5.027498722076416 | BCE Loss: 1.0551679134368896\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.111413955688477 | KNN Loss: 5.059812068939209 | BCE Loss: 1.0516021251678467\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.103776931762695 | KNN Loss: 5.051847457885742 | BCE Loss: 1.0519297122955322\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.098423957824707 | KNN Loss: 5.034924507141113 | BCE Loss: 1.0634996891021729\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.091639518737793 | KNN Loss: 5.029526710510254 | BCE Loss: 1.062112808227539\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.105611324310303 | KNN Loss: 5.031345367431641 | BCE Loss: 1.074265956878662\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.088242530822754 | KNN Loss: 5.042941570281982 | BCE Loss: 1.045301079750061\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.094501495361328 | KNN Loss: 5.039648532867432 | BCE Loss: 1.0548532009124756\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.123536109924316 | KNN Loss: 5.034194469451904 | BCE Loss: 1.089341402053833\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.090289115905762 | KNN Loss: 5.042994976043701 | BCE Loss: 1.04729425907135\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.093936443328857 | KNN Loss: 5.052513122558594 | BCE Loss: 1.0414234399795532\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.099575996398926 | KNN Loss: 5.041378974914551 | BCE Loss: 1.058196783065796\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.086273193359375 | KNN Loss: 5.038674354553223 | BCE Loss: 1.0475988388061523\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.140450477600098 | KNN Loss: 5.072659015655518 | BCE Loss: 1.0677915811538696\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.122989654541016 | KNN Loss: 5.043381690979004 | BCE Loss: 1.0796079635620117\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.069067478179932 | KNN Loss: 5.026360511779785 | BCE Loss: 1.042706847190857\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.1385955810546875 | KNN Loss: 5.089450836181641 | BCE Loss: 1.0491447448730469\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.0883026123046875 | KNN Loss: 5.030219078063965 | BCE Loss: 1.0580835342407227\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.099876403808594 | KNN Loss: 5.035455703735352 | BCE Loss: 1.0644209384918213\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.101171493530273 | KNN Loss: 5.0409159660339355 | BCE Loss: 1.060255765914917\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.107977867126465 | KNN Loss: 5.051216125488281 | BCE Loss: 1.0567617416381836\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.075284957885742 | KNN Loss: 5.041060924530029 | BCE Loss: 1.034224033355713\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.065136909484863 | KNN Loss: 5.033406734466553 | BCE Loss: 1.0317299365997314\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.137543201446533 | KNN Loss: 5.050719738006592 | BCE Loss: 1.0868233442306519\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.124856948852539 | KNN Loss: 5.053122520446777 | BCE Loss: 1.0717346668243408\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.087677478790283 | KNN Loss: 5.039365291595459 | BCE Loss: 1.0483123064041138\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.113557815551758 | KNN Loss: 5.032303810119629 | BCE Loss: 1.0812538862228394\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.100684642791748 | KNN Loss: 5.063054084777832 | BCE Loss: 1.0376306772232056\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.073270797729492 | KNN Loss: 5.025272369384766 | BCE Loss: 1.0479984283447266\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.073000431060791 | KNN Loss: 5.0358052253723145 | BCE Loss: 1.0371952056884766\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.06986141204834 | KNN Loss: 5.029531002044678 | BCE Loss: 1.040330410003662\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.0570244789123535 | KNN Loss: 5.027482509613037 | BCE Loss: 1.0295419692993164\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.094559669494629 | KNN Loss: 5.049095153808594 | BCE Loss: 1.0454647541046143\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.081693172454834 | KNN Loss: 5.033050537109375 | BCE Loss: 1.0486427545547485\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.098728179931641 | KNN Loss: 5.036563873291016 | BCE Loss: 1.062164306640625\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.139060974121094 | KNN Loss: 5.047548294067383 | BCE Loss: 1.09151291847229\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.067357063293457 | KNN Loss: 5.026141166687012 | BCE Loss: 1.0412157773971558\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.082366943359375 | KNN Loss: 5.030402183532715 | BCE Loss: 1.0519649982452393\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.087035179138184 | KNN Loss: 5.054207801818848 | BCE Loss: 1.0328274965286255\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.052041530609131 | KNN Loss: 5.035913467407227 | BCE Loss: 1.0161280632019043\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.1078782081604 | KNN Loss: 5.03910493850708 | BCE Loss: 1.0687732696533203\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.060846328735352 | KNN Loss: 5.04163932800293 | BCE Loss: 1.0192067623138428\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.105691432952881 | KNN Loss: 5.03339147567749 | BCE Loss: 1.0723000764846802\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.074414253234863 | KNN Loss: 5.038753986358643 | BCE Loss: 1.0356601476669312\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.073739528656006 | KNN Loss: 5.0260443687438965 | BCE Loss: 1.0476951599121094\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.083880424499512 | KNN Loss: 5.019002437591553 | BCE Loss: 1.064877986907959\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.069826602935791 | KNN Loss: 5.029800891876221 | BCE Loss: 1.0400257110595703\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.077069282531738 | KNN Loss: 5.037701606750488 | BCE Loss: 1.03936767578125\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.0897088050842285 | KNN Loss: 5.04380989074707 | BCE Loss: 1.0458989143371582\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.05664587020874 | KNN Loss: 5.032249450683594 | BCE Loss: 1.024396300315857\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.111641883850098 | KNN Loss: 5.062722682952881 | BCE Loss: 1.0489192008972168\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.162470817565918 | KNN Loss: 5.081162929534912 | BCE Loss: 1.0813080072402954\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.080660820007324 | KNN Loss: 5.031099319458008 | BCE Loss: 1.0495617389678955\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.091050148010254 | KNN Loss: 5.0347418785095215 | BCE Loss: 1.0563080310821533\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.160172462463379 | KNN Loss: 5.1169657707214355 | BCE Loss: 1.0432064533233643\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.107539176940918 | KNN Loss: 5.028828144073486 | BCE Loss: 1.0787112712860107\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.051063060760498 | KNN Loss: 5.022002220153809 | BCE Loss: 1.0290608406066895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.081455230712891 | KNN Loss: 5.0207953453063965 | BCE Loss: 1.0606601238250732\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.071977615356445 | KNN Loss: 5.035012722015381 | BCE Loss: 1.0369646549224854\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.0942583084106445 | KNN Loss: 5.040613651275635 | BCE Loss: 1.0536445379257202\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.0997796058654785 | KNN Loss: 5.057556629180908 | BCE Loss: 1.0422230958938599\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.0909223556518555 | KNN Loss: 5.046536922454834 | BCE Loss: 1.0443851947784424\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.096246242523193 | KNN Loss: 5.0279083251953125 | BCE Loss: 1.0683379173278809\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.063228607177734 | KNN Loss: 5.024147987365723 | BCE Loss: 1.0390805006027222\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.145074367523193 | KNN Loss: 5.084218502044678 | BCE Loss: 1.0608559846878052\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.049915313720703 | KNN Loss: 5.034862518310547 | BCE Loss: 1.0150529146194458\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.102288722991943 | KNN Loss: 5.037583827972412 | BCE Loss: 1.0647048950195312\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.10798978805542 | KNN Loss: 5.053567886352539 | BCE Loss: 1.0544217824935913\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.107480049133301 | KNN Loss: 5.054006576538086 | BCE Loss: 1.0534734725952148\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.078041076660156 | KNN Loss: 5.035525798797607 | BCE Loss: 1.042515516281128\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.093940734863281 | KNN Loss: 5.028566837310791 | BCE Loss: 1.0653741359710693\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.0761823654174805 | KNN Loss: 5.025978088378906 | BCE Loss: 1.0502040386199951\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.111014366149902 | KNN Loss: 5.034106254577637 | BCE Loss: 1.0769082307815552\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.102871894836426 | KNN Loss: 5.04905891418457 | BCE Loss: 1.0538129806518555\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.077045440673828 | KNN Loss: 5.022292137145996 | BCE Loss: 1.054753065109253\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.041432857513428 | KNN Loss: 5.026251316070557 | BCE Loss: 1.015181541442871\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.05964469909668 | KNN Loss: 5.03703498840332 | BCE Loss: 1.0226097106933594\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.085247993469238 | KNN Loss: 5.037243366241455 | BCE Loss: 1.0480045080184937\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.094362258911133 | KNN Loss: 5.051249980926514 | BCE Loss: 1.0431122779846191\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.080718040466309 | KNN Loss: 5.030871391296387 | BCE Loss: 1.0498466491699219\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.093547344207764 | KNN Loss: 5.0209641456604 | BCE Loss: 1.0725833177566528\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.097383975982666 | KNN Loss: 5.035026550292969 | BCE Loss: 1.0623574256896973\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.044824600219727 | KNN Loss: 5.021483898162842 | BCE Loss: 1.0233409404754639\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.093281269073486 | KNN Loss: 5.0257649421691895 | BCE Loss: 1.0675163269042969\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.07801628112793 | KNN Loss: 5.033608913421631 | BCE Loss: 1.0444073677062988\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.085677146911621 | KNN Loss: 5.0698957443237305 | BCE Loss: 1.0157814025878906\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.074058532714844 | KNN Loss: 5.032461643218994 | BCE Loss: 1.0415970087051392\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.072725772857666 | KNN Loss: 5.035111904144287 | BCE Loss: 1.0376137495040894\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.103743553161621 | KNN Loss: 5.080430507659912 | BCE Loss: 1.023313283920288\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.0533766746521 | KNN Loss: 5.028383731842041 | BCE Loss: 1.0249929428100586\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.091978073120117 | KNN Loss: 5.042960166931152 | BCE Loss: 1.0490179061889648\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.120323181152344 | KNN Loss: 5.072908401489258 | BCE Loss: 1.0474145412445068\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.0842108726501465 | KNN Loss: 5.024641513824463 | BCE Loss: 1.0595693588256836\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.03692102432251 | KNN Loss: 5.026434898376465 | BCE Loss: 1.0104862451553345\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.116591930389404 | KNN Loss: 5.065551280975342 | BCE Loss: 1.051040768623352\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.070343017578125 | KNN Loss: 5.039940357208252 | BCE Loss: 1.0304028987884521\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.060697555541992 | KNN Loss: 5.041327476501465 | BCE Loss: 1.0193699598312378\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.075745582580566 | KNN Loss: 5.036641597747803 | BCE Loss: 1.0391039848327637\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.083344459533691 | KNN Loss: 5.023240089416504 | BCE Loss: 1.0601043701171875\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.096286773681641 | KNN Loss: 5.038168430328369 | BCE Loss: 1.0581181049346924\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.088291168212891 | KNN Loss: 5.020729064941406 | BCE Loss: 1.0675623416900635\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.121791839599609 | KNN Loss: 5.038247108459473 | BCE Loss: 1.0835449695587158\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.163209915161133 | KNN Loss: 5.0878753662109375 | BCE Loss: 1.0753345489501953\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.116552352905273 | KNN Loss: 5.081120491027832 | BCE Loss: 1.0354317426681519\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.0571441650390625 | KNN Loss: 5.015722274780273 | BCE Loss: 1.0414221286773682\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.077223777770996 | KNN Loss: 5.019783020019531 | BCE Loss: 1.0574405193328857\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.152421474456787 | KNN Loss: 5.084858417510986 | BCE Loss: 1.0675631761550903\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.096379280090332 | KNN Loss: 5.030031204223633 | BCE Loss: 1.0663478374481201\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.064967155456543 | KNN Loss: 5.020776271820068 | BCE Loss: 1.0441908836364746\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.049072742462158 | KNN Loss: 5.029510021209717 | BCE Loss: 1.0195626020431519\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.0531816482543945 | KNN Loss: 5.031627655029297 | BCE Loss: 1.0215537548065186\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.104375839233398 | KNN Loss: 5.053704261779785 | BCE Loss: 1.0506715774536133\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.083713054656982 | KNN Loss: 5.021406650543213 | BCE Loss: 1.06230628490448\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.051026344299316 | KNN Loss: 5.022707462310791 | BCE Loss: 1.0283191204071045\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.090084075927734 | KNN Loss: 5.032073974609375 | BCE Loss: 1.058010220527649\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.086555004119873 | KNN Loss: 5.013439178466797 | BCE Loss: 1.0731157064437866\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.083454132080078 | KNN Loss: 5.01950216293335 | BCE Loss: 1.0639517307281494\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.099610328674316 | KNN Loss: 5.0431671142578125 | BCE Loss: 1.056443452835083\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.098202228546143 | KNN Loss: 5.038975238800049 | BCE Loss: 1.0592269897460938\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.113076686859131 | KNN Loss: 5.044951915740967 | BCE Loss: 1.068124771118164\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.066731929779053 | KNN Loss: 5.017056465148926 | BCE Loss: 1.049675464630127\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.11170768737793 | KNN Loss: 5.074717998504639 | BCE Loss: 1.0369898080825806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.081385135650635 | KNN Loss: 5.027251243591309 | BCE Loss: 1.0541338920593262\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.100461483001709 | KNN Loss: 5.051543712615967 | BCE Loss: 1.0489176511764526\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.106805801391602 | KNN Loss: 5.0235209465026855 | BCE Loss: 1.0832850933074951\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.104742050170898 | KNN Loss: 5.03533935546875 | BCE Loss: 1.0694029331207275\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.057454586029053 | KNN Loss: 5.023622512817383 | BCE Loss: 1.03383207321167\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.130719184875488 | KNN Loss: 5.086127758026123 | BCE Loss: 1.0445916652679443\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.0970258712768555 | KNN Loss: 5.046356678009033 | BCE Loss: 1.0506694316864014\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.068598747253418 | KNN Loss: 5.029611110687256 | BCE Loss: 1.0389878749847412\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.086506366729736 | KNN Loss: 5.027214050292969 | BCE Loss: 1.0592923164367676\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.046512603759766 | KNN Loss: 5.013960838317871 | BCE Loss: 1.032551884651184\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.096286773681641 | KNN Loss: 5.034470558166504 | BCE Loss: 1.0618162155151367\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.040881633758545 | KNN Loss: 5.0181121826171875 | BCE Loss: 1.0227694511413574\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.078253269195557 | KNN Loss: 5.030736923217773 | BCE Loss: 1.0475164651870728\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.030769348144531 | KNN Loss: 5.012608528137207 | BCE Loss: 1.0181610584259033\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.11094856262207 | KNN Loss: 5.060568332672119 | BCE Loss: 1.050379991531372\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.078996181488037 | KNN Loss: 5.043999195098877 | BCE Loss: 1.0349969863891602\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.039416313171387 | KNN Loss: 5.011691570281982 | BCE Loss: 1.0277245044708252\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.055904388427734 | KNN Loss: 5.020513534545898 | BCE Loss: 1.035391092300415\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.074438095092773 | KNN Loss: 5.022758960723877 | BCE Loss: 1.0516793727874756\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.062492370605469 | KNN Loss: 5.018764495849609 | BCE Loss: 1.0437278747558594\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.043882369995117 | KNN Loss: 5.016905307769775 | BCE Loss: 1.026977300643921\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.0709309577941895 | KNN Loss: 5.021751403808594 | BCE Loss: 1.0491795539855957\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.079712390899658 | KNN Loss: 5.048708915710449 | BCE Loss: 1.031003475189209\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.063995361328125 | KNN Loss: 5.035417079925537 | BCE Loss: 1.0285780429840088\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.075705528259277 | KNN Loss: 5.020802974700928 | BCE Loss: 1.0549027919769287\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.064808368682861 | KNN Loss: 5.016903400421143 | BCE Loss: 1.0479049682617188\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.138284683227539 | KNN Loss: 5.068226337432861 | BCE Loss: 1.0700585842132568\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.047884464263916 | KNN Loss: 5.032590866088867 | BCE Loss: 1.0152937173843384\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.061067581176758 | KNN Loss: 5.033397674560547 | BCE Loss: 1.0276696681976318\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.146299362182617 | KNN Loss: 5.06716775894165 | BCE Loss: 1.0791314840316772\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.081384181976318 | KNN Loss: 5.033206462860107 | BCE Loss: 1.0481775999069214\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.08708381652832 | KNN Loss: 5.040833473205566 | BCE Loss: 1.046250343322754\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.064201831817627 | KNN Loss: 5.035940170288086 | BCE Loss: 1.0282617807388306\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.073040008544922 | KNN Loss: 5.036446571350098 | BCE Loss: 1.0365934371948242\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.036449909210205 | KNN Loss: 5.012171268463135 | BCE Loss: 1.0242786407470703\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.07955265045166 | KNN Loss: 5.036559104919434 | BCE Loss: 1.0429935455322266\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.12692928314209 | KNN Loss: 5.045501232147217 | BCE Loss: 1.081427812576294\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.111062049865723 | KNN Loss: 5.041258335113525 | BCE Loss: 1.0698034763336182\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.061243057250977 | KNN Loss: 5.023688316345215 | BCE Loss: 1.0375548601150513\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.064766883850098 | KNN Loss: 5.029404640197754 | BCE Loss: 1.0353624820709229\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.108753681182861 | KNN Loss: 5.03189754486084 | BCE Loss: 1.076856017112732\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.074665069580078 | KNN Loss: 5.0129547119140625 | BCE Loss: 1.0617105960845947\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.095005035400391 | KNN Loss: 5.0369391441345215 | BCE Loss: 1.0580660104751587\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.104564189910889 | KNN Loss: 5.034336090087891 | BCE Loss: 1.070228099822998\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.066348552703857 | KNN Loss: 5.010437488555908 | BCE Loss: 1.0559110641479492\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.092031478881836 | KNN Loss: 5.023430347442627 | BCE Loss: 1.0686012506484985\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.0746846199035645 | KNN Loss: 5.051354885101318 | BCE Loss: 1.0233296155929565\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.07017707824707 | KNN Loss: 5.019856929779053 | BCE Loss: 1.0503203868865967\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.113458156585693 | KNN Loss: 5.042964458465576 | BCE Loss: 1.0704938173294067\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.052926063537598 | KNN Loss: 5.029435634613037 | BCE Loss: 1.0234906673431396\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.079364776611328 | KNN Loss: 5.019007205963135 | BCE Loss: 1.060357689857483\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.0754289627075195 | KNN Loss: 5.033533573150635 | BCE Loss: 1.0418956279754639\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.096836090087891 | KNN Loss: 5.021334171295166 | BCE Loss: 1.075501799583435\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.105809688568115 | KNN Loss: 5.054480075836182 | BCE Loss: 1.0513296127319336\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.070446014404297 | KNN Loss: 5.033315181732178 | BCE Loss: 1.0371308326721191\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.089371204376221 | KNN Loss: 5.046180248260498 | BCE Loss: 1.0431910753250122\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.0502824783325195 | KNN Loss: 5.028073310852051 | BCE Loss: 1.0222092866897583\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.1191534996032715 | KNN Loss: 5.067680835723877 | BCE Loss: 1.0514726638793945\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.056889533996582 | KNN Loss: 5.01918888092041 | BCE Loss: 1.0377004146575928\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.0822649002075195 | KNN Loss: 5.060138702392578 | BCE Loss: 1.0221261978149414\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.092097759246826 | KNN Loss: 5.045348644256592 | BCE Loss: 1.0467489957809448\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.06146240234375 | KNN Loss: 5.014420986175537 | BCE Loss: 1.047041654586792\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.082424640655518 | KNN Loss: 5.019181251525879 | BCE Loss: 1.0632435083389282\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.100399971008301 | KNN Loss: 5.031931400299072 | BCE Loss: 1.0684688091278076\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.050166130065918 | KNN Loss: 5.027437686920166 | BCE Loss: 1.0227282047271729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.077889442443848 | KNN Loss: 5.0281291007995605 | BCE Loss: 1.0497605800628662\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.073517322540283 | KNN Loss: 5.028764247894287 | BCE Loss: 1.0447531938552856\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.07501220703125 | KNN Loss: 5.019989967346191 | BCE Loss: 1.055022120475769\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.068759918212891 | KNN Loss: 5.01613187789917 | BCE Loss: 1.0526278018951416\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.067033767700195 | KNN Loss: 5.030699729919434 | BCE Loss: 1.0363339185714722\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.070094108581543 | KNN Loss: 5.020450115203857 | BCE Loss: 1.0496439933776855\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.068534851074219 | KNN Loss: 5.034926414489746 | BCE Loss: 1.0336081981658936\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.074002265930176 | KNN Loss: 5.04095458984375 | BCE Loss: 1.0330476760864258\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.055542945861816 | KNN Loss: 5.021111011505127 | BCE Loss: 1.0344316959381104\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.07063102722168 | KNN Loss: 5.038688659667969 | BCE Loss: 1.03194260597229\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.045519828796387 | KNN Loss: 5.016138076782227 | BCE Loss: 1.0293817520141602\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.092060565948486 | KNN Loss: 5.017220497131348 | BCE Loss: 1.0748400688171387\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.056084632873535 | KNN Loss: 5.020523548126221 | BCE Loss: 1.035560965538025\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.113740921020508 | KNN Loss: 5.030880928039551 | BCE Loss: 1.082859754562378\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.029636383056641 | KNN Loss: 5.005558967590332 | BCE Loss: 1.024077296257019\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.0371856689453125 | KNN Loss: 5.012011528015137 | BCE Loss: 1.0251740217208862\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.107662200927734 | KNN Loss: 5.047174453735352 | BCE Loss: 1.060487985610962\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.100634574890137 | KNN Loss: 5.0468525886535645 | BCE Loss: 1.0537819862365723\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.063788414001465 | KNN Loss: 5.019669055938721 | BCE Loss: 1.0441193580627441\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.073574066162109 | KNN Loss: 5.018616199493408 | BCE Loss: 1.0549579858779907\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.092650890350342 | KNN Loss: 5.0544962882995605 | BCE Loss: 1.0381546020507812\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.070087909698486 | KNN Loss: 5.025454521179199 | BCE Loss: 1.0446332693099976\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.043159008026123 | KNN Loss: 5.017698764801025 | BCE Loss: 1.025460124015808\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.052913665771484 | KNN Loss: 5.0231099128723145 | BCE Loss: 1.02980375289917\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.097388744354248 | KNN Loss: 5.048470973968506 | BCE Loss: 1.0489177703857422\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.104707717895508 | KNN Loss: 5.038179874420166 | BCE Loss: 1.066528081893921\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.109549045562744 | KNN Loss: 5.0528717041015625 | BCE Loss: 1.0566774606704712\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.083176136016846 | KNN Loss: 5.024349689483643 | BCE Loss: 1.0588265657424927\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.073790550231934 | KNN Loss: 5.034595012664795 | BCE Loss: 1.0391957759857178\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.100647449493408 | KNN Loss: 5.038412094116211 | BCE Loss: 1.0622353553771973\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.1243743896484375 | KNN Loss: 5.053255081176758 | BCE Loss: 1.0711193084716797\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.046591281890869 | KNN Loss: 5.010376453399658 | BCE Loss: 1.036214828491211\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.092619895935059 | KNN Loss: 5.045594215393066 | BCE Loss: 1.0470259189605713\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.07850980758667 | KNN Loss: 5.0307698249816895 | BCE Loss: 1.04774010181427\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.040785789489746 | KNN Loss: 5.010229587554932 | BCE Loss: 1.0305564403533936\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.074089050292969 | KNN Loss: 5.029168605804443 | BCE Loss: 1.044920563697815\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.076748847961426 | KNN Loss: 5.029141902923584 | BCE Loss: 1.0476068258285522\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.0778045654296875 | KNN Loss: 5.0410284996032715 | BCE Loss: 1.036775827407837\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.084470272064209 | KNN Loss: 5.016237258911133 | BCE Loss: 1.0682331323623657\n",
      "Epoch    93: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.132421493530273 | KNN Loss: 5.08112096786499 | BCE Loss: 1.0513006448745728\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.051979064941406 | KNN Loss: 5.022817611694336 | BCE Loss: 1.0291613340377808\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.049835205078125 | KNN Loss: 5.010122299194336 | BCE Loss: 1.0397131443023682\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.069775104522705 | KNN Loss: 5.0183329582214355 | BCE Loss: 1.0514421463012695\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.098382472991943 | KNN Loss: 5.049846649169922 | BCE Loss: 1.0485358238220215\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.075279235839844 | KNN Loss: 5.0118231773376465 | BCE Loss: 1.0634560585021973\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.068850517272949 | KNN Loss: 5.013345718383789 | BCE Loss: 1.0555046796798706\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.067147254943848 | KNN Loss: 5.011396884918213 | BCE Loss: 1.0557502508163452\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.077252388000488 | KNN Loss: 5.045666694641113 | BCE Loss: 1.031585454940796\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.072882175445557 | KNN Loss: 5.025650501251221 | BCE Loss: 1.0472315549850464\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.112753868103027 | KNN Loss: 5.054807662963867 | BCE Loss: 1.057945966720581\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.051368713378906 | KNN Loss: 5.0207905769348145 | BCE Loss: 1.0305781364440918\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.084734916687012 | KNN Loss: 5.032899379730225 | BCE Loss: 1.051835536956787\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.068145751953125 | KNN Loss: 5.0307230949401855 | BCE Loss: 1.03742253780365\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.08671236038208 | KNN Loss: 5.059288024902344 | BCE Loss: 1.0274242162704468\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.029890060424805 | KNN Loss: 5.008169174194336 | BCE Loss: 1.0217208862304688\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.0781097412109375 | KNN Loss: 5.028682231903076 | BCE Loss: 1.0494275093078613\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.080735206604004 | KNN Loss: 5.022155284881592 | BCE Loss: 1.0585801601409912\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.046371936798096 | KNN Loss: 5.025500774383545 | BCE Loss: 1.0208711624145508\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.067464351654053 | KNN Loss: 5.040218353271484 | BCE Loss: 1.0272459983825684\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.061478137969971 | KNN Loss: 5.020598411560059 | BCE Loss: 1.040879726409912\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.115572929382324 | KNN Loss: 5.066926002502441 | BCE Loss: 1.048647165298462\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.067720413208008 | KNN Loss: 5.0171308517456055 | BCE Loss: 1.0505893230438232\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.06699275970459 | KNN Loss: 5.048675537109375 | BCE Loss: 1.0183172225952148\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.025885581970215 | KNN Loss: 5.004211902618408 | BCE Loss: 1.0216736793518066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.062525749206543 | KNN Loss: 5.021864891052246 | BCE Loss: 1.0406606197357178\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.047896385192871 | KNN Loss: 5.01005220413208 | BCE Loss: 1.037843942642212\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.08022403717041 | KNN Loss: 5.022010326385498 | BCE Loss: 1.0582139492034912\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.079639434814453 | KNN Loss: 5.029745101928711 | BCE Loss: 1.0498942136764526\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.05873441696167 | KNN Loss: 5.007699012756348 | BCE Loss: 1.0510354042053223\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.02940559387207 | KNN Loss: 5.0172505378723145 | BCE Loss: 1.0121550559997559\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.0686235427856445 | KNN Loss: 5.0143232345581055 | BCE Loss: 1.05430006980896\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.061220169067383 | KNN Loss: 5.025240898132324 | BCE Loss: 1.0359795093536377\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.078807830810547 | KNN Loss: 5.020413398742676 | BCE Loss: 1.058394193649292\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.061069965362549 | KNN Loss: 5.018908500671387 | BCE Loss: 1.0421613454818726\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.070314884185791 | KNN Loss: 5.014571666717529 | BCE Loss: 1.0557433366775513\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.043993949890137 | KNN Loss: 5.022889137268066 | BCE Loss: 1.0211050510406494\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.056541442871094 | KNN Loss: 5.022824764251709 | BCE Loss: 1.0337164402008057\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.07788610458374 | KNN Loss: 5.042170524597168 | BCE Loss: 1.0357155799865723\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.062950134277344 | KNN Loss: 5.011995792388916 | BCE Loss: 1.0509545803070068\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.066972732543945 | KNN Loss: 5.021809101104736 | BCE Loss: 1.0451637506484985\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.10375452041626 | KNN Loss: 5.034186363220215 | BCE Loss: 1.0695680379867554\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.065242767333984 | KNN Loss: 5.0398640632629395 | BCE Loss: 1.0253785848617554\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.094324111938477 | KNN Loss: 5.0311360359191895 | BCE Loss: 1.063187837600708\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.065209865570068 | KNN Loss: 5.032160758972168 | BCE Loss: 1.0330491065979004\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.029988765716553 | KNN Loss: 5.010059833526611 | BCE Loss: 1.0199289321899414\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.065579414367676 | KNN Loss: 5.019252777099609 | BCE Loss: 1.0463266372680664\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.066612720489502 | KNN Loss: 5.021319389343262 | BCE Loss: 1.0452933311462402\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.095097541809082 | KNN Loss: 5.039628505706787 | BCE Loss: 1.0554689168930054\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.116394996643066 | KNN Loss: 5.060850143432617 | BCE Loss: 1.0555446147918701\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.081815719604492 | KNN Loss: 5.021048545837402 | BCE Loss: 1.0607671737670898\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.11502742767334 | KNN Loss: 5.0814642906188965 | BCE Loss: 1.0335631370544434\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.0672712326049805 | KNN Loss: 5.027092933654785 | BCE Loss: 1.0401780605316162\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.043890953063965 | KNN Loss: 5.016049861907959 | BCE Loss: 1.027841329574585\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.06245231628418 | KNN Loss: 5.02890682220459 | BCE Loss: 1.0335454940795898\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.077881336212158 | KNN Loss: 5.015468120574951 | BCE Loss: 1.0624133348464966\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.049313068389893 | KNN Loss: 5.028153419494629 | BCE Loss: 1.0211595296859741\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.1006999015808105 | KNN Loss: 5.046504020690918 | BCE Loss: 1.0541958808898926\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.062378883361816 | KNN Loss: 5.025577545166016 | BCE Loss: 1.0368010997772217\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.072772026062012 | KNN Loss: 5.017809867858887 | BCE Loss: 1.0549620389938354\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.0660600662231445 | KNN Loss: 5.020035743713379 | BCE Loss: 1.0460240840911865\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.075921535491943 | KNN Loss: 5.010952472686768 | BCE Loss: 1.0649690628051758\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.0535149574279785 | KNN Loss: 5.008552551269531 | BCE Loss: 1.0449624061584473\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.086575031280518 | KNN Loss: 5.043882369995117 | BCE Loss: 1.04269278049469\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.054657936096191 | KNN Loss: 5.015872001647949 | BCE Loss: 1.0387858152389526\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.076998710632324 | KNN Loss: 5.032862186431885 | BCE Loss: 1.0441367626190186\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.064568996429443 | KNN Loss: 5.006256580352783 | BCE Loss: 1.0583124160766602\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.066786766052246 | KNN Loss: 5.033273696899414 | BCE Loss: 1.0335131883621216\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.070624351501465 | KNN Loss: 5.044923305511475 | BCE Loss: 1.0257012844085693\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.07734489440918 | KNN Loss: 5.024735450744629 | BCE Loss: 1.0526096820831299\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.0636138916015625 | KNN Loss: 5.025547981262207 | BCE Loss: 1.0380659103393555\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.032070159912109 | KNN Loss: 5.0249128341674805 | BCE Loss: 1.007157325744629\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.100869178771973 | KNN Loss: 5.043858528137207 | BCE Loss: 1.0570108890533447\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.102154731750488 | KNN Loss: 5.033514976501465 | BCE Loss: 1.0686399936676025\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.0883283615112305 | KNN Loss: 5.034092903137207 | BCE Loss: 1.0542354583740234\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.095287322998047 | KNN Loss: 5.042507648468018 | BCE Loss: 1.0527797937393188\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.056374549865723 | KNN Loss: 5.01760196685791 | BCE Loss: 1.038772463798523\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.071194648742676 | KNN Loss: 5.008708477020264 | BCE Loss: 1.0624860525131226\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.075920581817627 | KNN Loss: 5.018946170806885 | BCE Loss: 1.0569742918014526\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.072049140930176 | KNN Loss: 5.020081996917725 | BCE Loss: 1.0519673824310303\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.056021690368652 | KNN Loss: 5.02341890335083 | BCE Loss: 1.0326027870178223\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.037191867828369 | KNN Loss: 5.022347927093506 | BCE Loss: 1.0148439407348633\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.072239875793457 | KNN Loss: 5.046205043792725 | BCE Loss: 1.0260347127914429\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.065671920776367 | KNN Loss: 5.013908386230469 | BCE Loss: 1.0517632961273193\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.088172912597656 | KNN Loss: 5.038086414337158 | BCE Loss: 1.050086498260498\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.061053276062012 | KNN Loss: 5.013463020324707 | BCE Loss: 1.0475901365280151\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.076516151428223 | KNN Loss: 5.047182083129883 | BCE Loss: 1.0293339490890503\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.076141834259033 | KNN Loss: 5.055167198181152 | BCE Loss: 1.0209746360778809\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.088168621063232 | KNN Loss: 5.028877258300781 | BCE Loss: 1.0592913627624512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.063564777374268 | KNN Loss: 5.0297393798828125 | BCE Loss: 1.033825397491455\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.081810474395752 | KNN Loss: 5.050789833068848 | BCE Loss: 1.0310207605361938\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.0637922286987305 | KNN Loss: 5.02841329574585 | BCE Loss: 1.0353788137435913\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.06151008605957 | KNN Loss: 5.005863189697266 | BCE Loss: 1.0556470155715942\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.0906243324279785 | KNN Loss: 5.015073299407959 | BCE Loss: 1.0755510330200195\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.058589458465576 | KNN Loss: 5.010581970214844 | BCE Loss: 1.048007607460022\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.1202311515808105 | KNN Loss: 5.082742691040039 | BCE Loss: 1.0374884605407715\n",
      "Epoch   109: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.032408714294434 | KNN Loss: 5.009064674377441 | BCE Loss: 1.0233441591262817\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.060249328613281 | KNN Loss: 5.005973815917969 | BCE Loss: 1.0542755126953125\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.058180332183838 | KNN Loss: 5.031402587890625 | BCE Loss: 1.026777744293213\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.063842296600342 | KNN Loss: 5.0349249839782715 | BCE Loss: 1.0289174318313599\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.114323139190674 | KNN Loss: 5.023275852203369 | BCE Loss: 1.0910474061965942\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.070477485656738 | KNN Loss: 5.017024517059326 | BCE Loss: 1.0534532070159912\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.049128532409668 | KNN Loss: 5.017170429229736 | BCE Loss: 1.0319578647613525\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.033138751983643 | KNN Loss: 5.000890254974365 | BCE Loss: 1.0322484970092773\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.02744197845459 | KNN Loss: 5.01363468170166 | BCE Loss: 1.0138070583343506\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.122136116027832 | KNN Loss: 5.046420574188232 | BCE Loss: 1.07571542263031\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.061236381530762 | KNN Loss: 5.025041580200195 | BCE Loss: 1.0361948013305664\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.027851104736328 | KNN Loss: 5.014612674713135 | BCE Loss: 1.0132386684417725\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.032512187957764 | KNN Loss: 5.015277862548828 | BCE Loss: 1.017234444618225\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.064302921295166 | KNN Loss: 5.02866792678833 | BCE Loss: 1.0356351137161255\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.134620189666748 | KNN Loss: 5.0838165283203125 | BCE Loss: 1.0508036613464355\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.076456069946289 | KNN Loss: 5.038858413696289 | BCE Loss: 1.037597894668579\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.048334121704102 | KNN Loss: 5.016941547393799 | BCE Loss: 1.0313928127288818\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.0559306144714355 | KNN Loss: 5.029846668243408 | BCE Loss: 1.026084065437317\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.023947715759277 | KNN Loss: 5.009762287139893 | BCE Loss: 1.0141854286193848\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.040238380432129 | KNN Loss: 5.013005256652832 | BCE Loss: 1.027233362197876\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.0632219314575195 | KNN Loss: 5.00535249710083 | BCE Loss: 1.0578694343566895\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.1028900146484375 | KNN Loss: 5.044867992401123 | BCE Loss: 1.0580222606658936\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.103044509887695 | KNN Loss: 5.027099609375 | BCE Loss: 1.0759451389312744\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.124765872955322 | KNN Loss: 5.0516791343688965 | BCE Loss: 1.0730867385864258\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.083446979522705 | KNN Loss: 5.047887802124023 | BCE Loss: 1.0355591773986816\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.1183671951293945 | KNN Loss: 5.08514404296875 | BCE Loss: 1.0332229137420654\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.046076774597168 | KNN Loss: 5.009927272796631 | BCE Loss: 1.036149501800537\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.021409034729004 | KNN Loss: 5.010989189147949 | BCE Loss: 1.0104200839996338\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.053719997406006 | KNN Loss: 5.010441303253174 | BCE Loss: 1.0432788133621216\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.0487871170043945 | KNN Loss: 5.008439064025879 | BCE Loss: 1.0403480529785156\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.053924083709717 | KNN Loss: 5.0111846923828125 | BCE Loss: 1.0427395105361938\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.058562278747559 | KNN Loss: 5.04102897644043 | BCE Loss: 1.0175334215164185\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.0803914070129395 | KNN Loss: 5.0344624519348145 | BCE Loss: 1.045928955078125\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.072977066040039 | KNN Loss: 5.041248321533203 | BCE Loss: 1.031728982925415\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.115850448608398 | KNN Loss: 5.064144134521484 | BCE Loss: 1.0517065525054932\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.063470840454102 | KNN Loss: 5.025089740753174 | BCE Loss: 1.0383812189102173\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.06773567199707 | KNN Loss: 5.008174896240234 | BCE Loss: 1.0595605373382568\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.051979064941406 | KNN Loss: 5.012240886688232 | BCE Loss: 1.039738416671753\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.076174736022949 | KNN Loss: 5.046905517578125 | BCE Loss: 1.0292692184448242\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.0806989669799805 | KNN Loss: 5.0218095779418945 | BCE Loss: 1.0588891506195068\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.0851969718933105 | KNN Loss: 5.028851509094238 | BCE Loss: 1.0563453435897827\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.071351051330566 | KNN Loss: 5.0141119956970215 | BCE Loss: 1.057239055633545\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.081696033477783 | KNN Loss: 5.011587619781494 | BCE Loss: 1.070108413696289\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.07505989074707 | KNN Loss: 5.030198097229004 | BCE Loss: 1.044861912727356\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.078720569610596 | KNN Loss: 5.0250020027160645 | BCE Loss: 1.0537185668945312\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.06834602355957 | KNN Loss: 5.034326076507568 | BCE Loss: 1.034019947052002\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.067028522491455 | KNN Loss: 5.01279878616333 | BCE Loss: 1.0542298555374146\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.05108642578125 | KNN Loss: 5.014420509338379 | BCE Loss: 1.036665916442871\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.0374979972839355 | KNN Loss: 5.012152671813965 | BCE Loss: 1.0253453254699707\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.0620574951171875 | KNN Loss: 5.023160457611084 | BCE Loss: 1.0388967990875244\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.088692665100098 | KNN Loss: 5.028205394744873 | BCE Loss: 1.060487151145935\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.059648036956787 | KNN Loss: 5.043334007263184 | BCE Loss: 1.016314148902893\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.116990089416504 | KNN Loss: 5.0482563972473145 | BCE Loss: 1.0687334537506104\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.059264659881592 | KNN Loss: 4.998968601226807 | BCE Loss: 1.0602961778640747\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.058372497558594 | KNN Loss: 5.010668754577637 | BCE Loss: 1.047703504562378\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.055105686187744 | KNN Loss: 5.0077433586120605 | BCE Loss: 1.0473623275756836\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.050946235656738 | KNN Loss: 5.018486499786377 | BCE Loss: 1.0324599742889404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.073353290557861 | KNN Loss: 5.014732837677002 | BCE Loss: 1.0586204528808594\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.047101974487305 | KNN Loss: 5.023948669433594 | BCE Loss: 1.0231531858444214\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.05424690246582 | KNN Loss: 5.0101118087768555 | BCE Loss: 1.0441348552703857\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.054615497589111 | KNN Loss: 5.0177106857299805 | BCE Loss: 1.0369046926498413\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.044525623321533 | KNN Loss: 5.0132575035095215 | BCE Loss: 1.0312682390213013\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.043309211730957 | KNN Loss: 4.997982501983643 | BCE Loss: 1.045326828956604\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.057426452636719 | KNN Loss: 5.015480995178223 | BCE Loss: 1.0419456958770752\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.053518295288086 | KNN Loss: 5.036980628967285 | BCE Loss: 1.0165374279022217\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.076365947723389 | KNN Loss: 5.0205888748168945 | BCE Loss: 1.0557769536972046\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.117552757263184 | KNN Loss: 5.041823863983154 | BCE Loss: 1.0757291316986084\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.0244951248168945 | KNN Loss: 5.007680416107178 | BCE Loss: 1.0168148279190063\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.062831878662109 | KNN Loss: 5.015233039855957 | BCE Loss: 1.047598958015442\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.048500061035156 | KNN Loss: 5.011943817138672 | BCE Loss: 1.0365562438964844\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.0547380447387695 | KNN Loss: 5.006410598754883 | BCE Loss: 1.0483276844024658\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.123582363128662 | KNN Loss: 5.070988655090332 | BCE Loss: 1.0525935888290405\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.124086856842041 | KNN Loss: 5.087289333343506 | BCE Loss: 1.0367975234985352\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.091866970062256 | KNN Loss: 5.036040782928467 | BCE Loss: 1.0558260679244995\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.052353382110596 | KNN Loss: 5.023877143859863 | BCE Loss: 1.028476357460022\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.0785675048828125 | KNN Loss: 5.032440185546875 | BCE Loss: 1.0461275577545166\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.057411193847656 | KNN Loss: 5.029665470123291 | BCE Loss: 1.0277459621429443\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.070408821105957 | KNN Loss: 5.018641948699951 | BCE Loss: 1.0517668724060059\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.080556869506836 | KNN Loss: 5.030348300933838 | BCE Loss: 1.050208330154419\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.024826526641846 | KNN Loss: 5.01179838180542 | BCE Loss: 1.0130281448364258\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.066400051116943 | KNN Loss: 5.0212812423706055 | BCE Loss: 1.045118808746338\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.033600330352783 | KNN Loss: 5.020654678344727 | BCE Loss: 1.0129457712173462\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.055545806884766 | KNN Loss: 5.020472526550293 | BCE Loss: 1.0350730419158936\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.075925827026367 | KNN Loss: 5.036980152130127 | BCE Loss: 1.0389455556869507\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.036569595336914 | KNN Loss: 5.005353927612305 | BCE Loss: 1.0312154293060303\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.052934646606445 | KNN Loss: 5.020905017852783 | BCE Loss: 1.0320298671722412\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.032721519470215 | KNN Loss: 5.033611297607422 | BCE Loss: 0.9991101026535034\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.056105613708496 | KNN Loss: 5.034341335296631 | BCE Loss: 1.0217645168304443\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.062410354614258 | KNN Loss: 5.007112979888916 | BCE Loss: 1.0552971363067627\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.064154148101807 | KNN Loss: 5.0113348960876465 | BCE Loss: 1.0528192520141602\n",
      "Epoch   124: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.098423004150391 | KNN Loss: 5.019465446472168 | BCE Loss: 1.0789575576782227\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.020229339599609 | KNN Loss: 5.013850688934326 | BCE Loss: 1.0063787698745728\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.072513580322266 | KNN Loss: 5.017473220825195 | BCE Loss: 1.0550405979156494\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.060726165771484 | KNN Loss: 5.013617038726807 | BCE Loss: 1.0471093654632568\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.063089370727539 | KNN Loss: 5.02357816696167 | BCE Loss: 1.0395112037658691\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.046480178833008 | KNN Loss: 5.014127731323242 | BCE Loss: 1.0323524475097656\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.066634654998779 | KNN Loss: 5.003851413726807 | BCE Loss: 1.0627833604812622\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.090060234069824 | KNN Loss: 5.032390594482422 | BCE Loss: 1.0576698780059814\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.0345048904418945 | KNN Loss: 5.00587272644043 | BCE Loss: 1.0286319255828857\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.035433769226074 | KNN Loss: 5.016005992889404 | BCE Loss: 1.0194275379180908\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.065282821655273 | KNN Loss: 4.995345115661621 | BCE Loss: 1.0699375867843628\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.05344820022583 | KNN Loss: 5.0088887214660645 | BCE Loss: 1.0445595979690552\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.046899795532227 | KNN Loss: 5.004350185394287 | BCE Loss: 1.04254949092865\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.060767650604248 | KNN Loss: 5.011201858520508 | BCE Loss: 1.0495657920837402\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.049316883087158 | KNN Loss: 5.021764755249023 | BCE Loss: 1.0275520086288452\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.059844017028809 | KNN Loss: 5.014502048492432 | BCE Loss: 1.045342206954956\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.0626068115234375 | KNN Loss: 5.003491401672363 | BCE Loss: 1.0591156482696533\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.055812835693359 | KNN Loss: 5.013814926147461 | BCE Loss: 1.0419977903366089\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.078226089477539 | KNN Loss: 5.057490348815918 | BCE Loss: 1.020735740661621\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.07405948638916 | KNN Loss: 5.004571914672852 | BCE Loss: 1.0694875717163086\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.045243263244629 | KNN Loss: 5.022172927856445 | BCE Loss: 1.0230705738067627\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.049253463745117 | KNN Loss: 5.029698848724365 | BCE Loss: 1.0195543766021729\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.061310291290283 | KNN Loss: 5.007757186889648 | BCE Loss: 1.0535531044006348\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.062971115112305 | KNN Loss: 5.018258094787598 | BCE Loss: 1.0447131395339966\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.028752326965332 | KNN Loss: 5.003279209136963 | BCE Loss: 1.0254732370376587\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.074445724487305 | KNN Loss: 5.038343906402588 | BCE Loss: 1.036102056503296\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.0196757316589355 | KNN Loss: 5.01707124710083 | BCE Loss: 1.0026044845581055\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.063831806182861 | KNN Loss: 5.028939723968506 | BCE Loss: 1.034891963005066\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.036401748657227 | KNN Loss: 5.016414642333984 | BCE Loss: 1.0199873447418213\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.0333662033081055 | KNN Loss: 5.016415596008301 | BCE Loss: 1.0169503688812256\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.091275215148926 | KNN Loss: 5.030930519104004 | BCE Loss: 1.060344934463501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.066904067993164 | KNN Loss: 5.023560047149658 | BCE Loss: 1.043344259262085\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.065980911254883 | KNN Loss: 5.008481979370117 | BCE Loss: 1.0574991703033447\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.0414018630981445 | KNN Loss: 5.012219429016113 | BCE Loss: 1.0291826725006104\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.047889709472656 | KNN Loss: 5.012051105499268 | BCE Loss: 1.0358388423919678\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.05029296875 | KNN Loss: 5.018232822418213 | BCE Loss: 1.0320602655410767\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.030124664306641 | KNN Loss: 5.004165172576904 | BCE Loss: 1.0259597301483154\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.0670671463012695 | KNN Loss: 5.02721643447876 | BCE Loss: 1.0398504734039307\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.115882873535156 | KNN Loss: 5.082324504852295 | BCE Loss: 1.0335586071014404\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.061288833618164 | KNN Loss: 5.018934726715088 | BCE Loss: 1.0423542261123657\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.073156356811523 | KNN Loss: 5.0220465660095215 | BCE Loss: 1.051110029220581\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.041866779327393 | KNN Loss: 5.0308427810668945 | BCE Loss: 1.0110241174697876\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.084120750427246 | KNN Loss: 5.027876377105713 | BCE Loss: 1.0562444925308228\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.057660102844238 | KNN Loss: 5.007762432098389 | BCE Loss: 1.0498976707458496\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.053420066833496 | KNN Loss: 5.030815601348877 | BCE Loss: 1.0226047039031982\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.04730224609375 | KNN Loss: 5.019110679626465 | BCE Loss: 1.0281914472579956\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.06228494644165 | KNN Loss: 5.010629177093506 | BCE Loss: 1.0516557693481445\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.055058479309082 | KNN Loss: 5.02604341506958 | BCE Loss: 1.029015302658081\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.051810264587402 | KNN Loss: 5.018373012542725 | BCE Loss: 1.0334373712539673\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.076510429382324 | KNN Loss: 5.016911029815674 | BCE Loss: 1.0595993995666504\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.105463981628418 | KNN Loss: 5.058163166046143 | BCE Loss: 1.0473006963729858\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.036761283874512 | KNN Loss: 5.016810894012451 | BCE Loss: 1.019950270652771\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.039263725280762 | KNN Loss: 5.003532886505127 | BCE Loss: 1.0357308387756348\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.078043460845947 | KNN Loss: 5.079022407531738 | BCE Loss: 0.9990211725234985\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.078649520874023 | KNN Loss: 5.023124694824219 | BCE Loss: 1.0555247068405151\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.0407819747924805 | KNN Loss: 5.013291835784912 | BCE Loss: 1.0274901390075684\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.053928852081299 | KNN Loss: 5.0120368003845215 | BCE Loss: 1.0418919324874878\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.075684547424316 | KNN Loss: 5.046231269836426 | BCE Loss: 1.0294530391693115\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.073416709899902 | KNN Loss: 5.010569095611572 | BCE Loss: 1.062847375869751\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.063101768493652 | KNN Loss: 5.016139030456543 | BCE Loss: 1.0469629764556885\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.0540056228637695 | KNN Loss: 5.030849933624268 | BCE Loss: 1.0231554508209229\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.028566837310791 | KNN Loss: 5.001794815063477 | BCE Loss: 1.0267720222473145\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.061621189117432 | KNN Loss: 5.005561828613281 | BCE Loss: 1.05605947971344\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.0561981201171875 | KNN Loss: 5.014664649963379 | BCE Loss: 1.0415335893630981\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.0606889724731445 | KNN Loss: 5.022456645965576 | BCE Loss: 1.0382325649261475\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.024693489074707 | KNN Loss: 5.003958702087402 | BCE Loss: 1.0207346677780151\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.027146339416504 | KNN Loss: 5.0162787437438965 | BCE Loss: 1.010867714881897\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.037971019744873 | KNN Loss: 5.003659248352051 | BCE Loss: 1.0343117713928223\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.041741847991943 | KNN Loss: 5.003854751586914 | BCE Loss: 1.0378870964050293\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.0667724609375 | KNN Loss: 5.005465507507324 | BCE Loss: 1.0613071918487549\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.0831193923950195 | KNN Loss: 5.032042026519775 | BCE Loss: 1.0510776042938232\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.078843593597412 | KNN Loss: 5.007503032684326 | BCE Loss: 1.071340560913086\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.084571838378906 | KNN Loss: 5.034169673919678 | BCE Loss: 1.0504019260406494\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.011114597320557 | KNN Loss: 4.994475364685059 | BCE Loss: 1.016639232635498\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.080540657043457 | KNN Loss: 5.014786243438721 | BCE Loss: 1.0657542943954468\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.054908752441406 | KNN Loss: 5.012982368469238 | BCE Loss: 1.0419261455535889\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.086093902587891 | KNN Loss: 5.0294928550720215 | BCE Loss: 1.0566011667251587\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.069584369659424 | KNN Loss: 5.004263877868652 | BCE Loss: 1.065320611000061\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.0976786613464355 | KNN Loss: 5.074091911315918 | BCE Loss: 1.0235867500305176\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.05958366394043 | KNN Loss: 5.030795097351074 | BCE Loss: 1.0287885665893555\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.044135093688965 | KNN Loss: 5.007115364074707 | BCE Loss: 1.037019968032837\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.05247688293457 | KNN Loss: 5.020231246948242 | BCE Loss: 1.0322456359863281\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.0751800537109375 | KNN Loss: 5.023843765258789 | BCE Loss: 1.0513361692428589\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.054884910583496 | KNN Loss: 5.027344226837158 | BCE Loss: 1.027540683746338\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.025361061096191 | KNN Loss: 5.013050556182861 | BCE Loss: 1.012310266494751\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.081604480743408 | KNN Loss: 5.022713661193848 | BCE Loss: 1.05889093875885\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.088410377502441 | KNN Loss: 5.03186559677124 | BCE Loss: 1.056544542312622\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.049922466278076 | KNN Loss: 5.014166831970215 | BCE Loss: 1.0357555150985718\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.067195892333984 | KNN Loss: 5.021759033203125 | BCE Loss: 1.0454366207122803\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.039044380187988 | KNN Loss: 5.004386901855469 | BCE Loss: 1.0346577167510986\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.082939147949219 | KNN Loss: 5.028213977813721 | BCE Loss: 1.054725170135498\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.126377582550049 | KNN Loss: 5.093180179595947 | BCE Loss: 1.0331975221633911\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.083536148071289 | KNN Loss: 5.053364276885986 | BCE Loss: 1.0301716327667236\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.039838790893555 | KNN Loss: 5.012596607208252 | BCE Loss: 1.0272419452667236\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.045003890991211 | KNN Loss: 5.014824867248535 | BCE Loss: 1.0301791429519653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.033400535583496 | KNN Loss: 5.008886337280273 | BCE Loss: 1.0245139598846436\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.037459850311279 | KNN Loss: 5.001664161682129 | BCE Loss: 1.0357956886291504\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.042452812194824 | KNN Loss: 5.025990009307861 | BCE Loss: 1.016462802886963\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.055884838104248 | KNN Loss: 5.005781650543213 | BCE Loss: 1.0501031875610352\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.05653190612793 | KNN Loss: 5.019332408905029 | BCE Loss: 1.0371993780136108\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.030691146850586 | KNN Loss: 5.0150675773620605 | BCE Loss: 1.0156235694885254\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.0553717613220215 | KNN Loss: 5.010310173034668 | BCE Loss: 1.0450615882873535\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.07576322555542 | KNN Loss: 5.068942546844482 | BCE Loss: 1.006820797920227\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.041509628295898 | KNN Loss: 5.017478942871094 | BCE Loss: 1.0240304470062256\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.056333541870117 | KNN Loss: 5.014055252075195 | BCE Loss: 1.0422780513763428\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.080002784729004 | KNN Loss: 5.017560958862305 | BCE Loss: 1.0624420642852783\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.099921226501465 | KNN Loss: 5.061161041259766 | BCE Loss: 1.0387604236602783\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.021692276000977 | KNN Loss: 5.009152412414551 | BCE Loss: 1.0125399827957153\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.046051502227783 | KNN Loss: 5.005224704742432 | BCE Loss: 1.040826678276062\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.044911861419678 | KNN Loss: 5.013123035430908 | BCE Loss: 1.03178870677948\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.058631896972656 | KNN Loss: 5.012542724609375 | BCE Loss: 1.0460894107818604\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.050963878631592 | KNN Loss: 5.022948265075684 | BCE Loss: 1.0280156135559082\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.0900468826293945 | KNN Loss: 5.0345139503479 | BCE Loss: 1.0555328130722046\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.083948135375977 | KNN Loss: 5.015797138214111 | BCE Loss: 1.0681511163711548\n",
      "Epoch   143: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.058330535888672 | KNN Loss: 5.018836498260498 | BCE Loss: 1.0394937992095947\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.024515151977539 | KNN Loss: 5.004687786102295 | BCE Loss: 1.0198274850845337\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 5.997371673583984 | KNN Loss: 4.998936653137207 | BCE Loss: 0.9984347820281982\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.058777332305908 | KNN Loss: 5.014811992645264 | BCE Loss: 1.0439653396606445\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.022565841674805 | KNN Loss: 5.021228313446045 | BCE Loss: 1.0013372898101807\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.075919151306152 | KNN Loss: 5.018441677093506 | BCE Loss: 1.0574772357940674\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.079885482788086 | KNN Loss: 5.027482509613037 | BCE Loss: 1.0524029731750488\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.033188819885254 | KNN Loss: 5.011975288391113 | BCE Loss: 1.0212137699127197\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.054633140563965 | KNN Loss: 5.019980430603027 | BCE Loss: 1.0346527099609375\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.055893421173096 | KNN Loss: 5.018064975738525 | BCE Loss: 1.0378284454345703\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.055823802947998 | KNN Loss: 5.030969619750977 | BCE Loss: 1.0248541831970215\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.027029037475586 | KNN Loss: 5.0161967277526855 | BCE Loss: 1.0108325481414795\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.076523780822754 | KNN Loss: 5.023531436920166 | BCE Loss: 1.052992343902588\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.028467655181885 | KNN Loss: 5.009822845458984 | BCE Loss: 1.0186448097229004\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.033297538757324 | KNN Loss: 5.013158321380615 | BCE Loss: 1.020139217376709\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.0604248046875 | KNN Loss: 5.016739845275879 | BCE Loss: 1.0436850786209106\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.0590434074401855 | KNN Loss: 5.006832122802734 | BCE Loss: 1.0522112846374512\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.046274662017822 | KNN Loss: 5.010390758514404 | BCE Loss: 1.035883903503418\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.073728084564209 | KNN Loss: 5.0145978927612305 | BCE Loss: 1.0591301918029785\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.004315376281738 | KNN Loss: 5.00277853012085 | BCE Loss: 1.0015368461608887\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.063290596008301 | KNN Loss: 5.027633190155029 | BCE Loss: 1.035657286643982\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.050997734069824 | KNN Loss: 5.009979724884033 | BCE Loss: 1.041017770767212\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.027485370635986 | KNN Loss: 5.008177757263184 | BCE Loss: 1.0193074941635132\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.072856903076172 | KNN Loss: 5.070919036865234 | BCE Loss: 1.001937985420227\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.047220706939697 | KNN Loss: 5.0066118240356445 | BCE Loss: 1.0406088829040527\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.079298973083496 | KNN Loss: 5.0303730964660645 | BCE Loss: 1.0489259958267212\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.050880432128906 | KNN Loss: 5.0052714347839355 | BCE Loss: 1.0456092357635498\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.042869567871094 | KNN Loss: 5.0189714431762695 | BCE Loss: 1.0238983631134033\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.0391669273376465 | KNN Loss: 5.004586219787598 | BCE Loss: 1.0345807075500488\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.092746734619141 | KNN Loss: 5.05385684967041 | BCE Loss: 1.03889000415802\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.063260078430176 | KNN Loss: 5.020155429840088 | BCE Loss: 1.0431044101715088\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.076844215393066 | KNN Loss: 5.004465103149414 | BCE Loss: 1.0723791122436523\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.068746566772461 | KNN Loss: 5.024254322052002 | BCE Loss: 1.0444923639297485\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.073098659515381 | KNN Loss: 5.052525520324707 | BCE Loss: 1.0205730199813843\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.06428337097168 | KNN Loss: 5.038608551025391 | BCE Loss: 1.0256749391555786\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.027334213256836 | KNN Loss: 5.012999534606934 | BCE Loss: 1.0143345594406128\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.034905433654785 | KNN Loss: 5.013287544250488 | BCE Loss: 1.0216178894042969\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.026554107666016 | KNN Loss: 5.016582489013672 | BCE Loss: 1.0099713802337646\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.060049533843994 | KNN Loss: 5.0252790451049805 | BCE Loss: 1.0347706079483032\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.101935863494873 | KNN Loss: 5.047658443450928 | BCE Loss: 1.0542774200439453\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.058734893798828 | KNN Loss: 5.008452415466309 | BCE Loss: 1.0502827167510986\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.05049991607666 | KNN Loss: 5.008752822875977 | BCE Loss: 1.041746973991394\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.0363006591796875 | KNN Loss: 5.016051769256592 | BCE Loss: 1.0202487707138062\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.061533451080322 | KNN Loss: 5.027901649475098 | BCE Loss: 1.033631682395935\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.065958499908447 | KNN Loss: 5.018581390380859 | BCE Loss: 1.047377109527588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.052748680114746 | KNN Loss: 5.014116287231445 | BCE Loss: 1.0386326313018799\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.062108993530273 | KNN Loss: 5.021981239318848 | BCE Loss: 1.0401275157928467\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.048460960388184 | KNN Loss: 5.0100250244140625 | BCE Loss: 1.0384360551834106\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.045115947723389 | KNN Loss: 5.008676528930664 | BCE Loss: 1.0364394187927246\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.060601234436035 | KNN Loss: 5.034937858581543 | BCE Loss: 1.0256634950637817\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.066564559936523 | KNN Loss: 5.035656452178955 | BCE Loss: 1.0309078693389893\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.053614616394043 | KNN Loss: 5.010173797607422 | BCE Loss: 1.0434409379959106\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.041092395782471 | KNN Loss: 5.017368316650391 | BCE Loss: 1.0237239599227905\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.0630035400390625 | KNN Loss: 5.005370140075684 | BCE Loss: 1.057633638381958\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.073763847351074 | KNN Loss: 5.02314567565918 | BCE Loss: 1.0506184101104736\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.044434547424316 | KNN Loss: 5.00731897354126 | BCE Loss: 1.0371155738830566\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.0443220138549805 | KNN Loss: 5.017210006713867 | BCE Loss: 1.0271120071411133\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.060656547546387 | KNN Loss: 5.006590843200684 | BCE Loss: 1.0540657043457031\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.041568756103516 | KNN Loss: 5.011520862579346 | BCE Loss: 1.0300476551055908\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.081293106079102 | KNN Loss: 5.007204055786133 | BCE Loss: 1.0740888118743896\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.039424419403076 | KNN Loss: 4.9992995262146 | BCE Loss: 1.0401248931884766\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.055342674255371 | KNN Loss: 5.031486988067627 | BCE Loss: 1.0238558053970337\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.013908386230469 | KNN Loss: 5.0037360191345215 | BCE Loss: 1.0101726055145264\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.039898872375488 | KNN Loss: 5.012885093688965 | BCE Loss: 1.0270140171051025\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.062596321105957 | KNN Loss: 5.029305458068848 | BCE Loss: 1.0332906246185303\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.072221755981445 | KNN Loss: 5.020143508911133 | BCE Loss: 1.0520784854888916\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.056273460388184 | KNN Loss: 5.009978771209717 | BCE Loss: 1.0462944507598877\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.0370635986328125 | KNN Loss: 5.001065731048584 | BCE Loss: 1.0359981060028076\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.044272422790527 | KNN Loss: 5.017451286315918 | BCE Loss: 1.0268208980560303\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.049506187438965 | KNN Loss: 5.022045612335205 | BCE Loss: 1.0274608135223389\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.100296974182129 | KNN Loss: 5.038168907165527 | BCE Loss: 1.0621281862258911\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.082938194274902 | KNN Loss: 5.042382717132568 | BCE Loss: 1.0405552387237549\n",
      "Epoch   155: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.04890775680542 | KNN Loss: 5.002170085906982 | BCE Loss: 1.046737790107727\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.030220985412598 | KNN Loss: 5.006871700286865 | BCE Loss: 1.023349404335022\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.043815612792969 | KNN Loss: 5.011029243469238 | BCE Loss: 1.0327861309051514\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.057125091552734 | KNN Loss: 5.013129234313965 | BCE Loss: 1.0439960956573486\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.032733917236328 | KNN Loss: 5.020523548126221 | BCE Loss: 1.0122101306915283\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.077071189880371 | KNN Loss: 5.024528503417969 | BCE Loss: 1.0525426864624023\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.030241966247559 | KNN Loss: 5.019400119781494 | BCE Loss: 1.0108416080474854\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.039391040802002 | KNN Loss: 5.0114827156066895 | BCE Loss: 1.0279083251953125\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.0345659255981445 | KNN Loss: 5.004520416259766 | BCE Loss: 1.030045509338379\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.0623674392700195 | KNN Loss: 5.009037017822266 | BCE Loss: 1.0533303022384644\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.065179347991943 | KNN Loss: 5.0201826095581055 | BCE Loss: 1.0449968576431274\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.084403038024902 | KNN Loss: 5.0409345626831055 | BCE Loss: 1.0434682369232178\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.057986259460449 | KNN Loss: 5.020895481109619 | BCE Loss: 1.037090539932251\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.106652736663818 | KNN Loss: 5.049063205718994 | BCE Loss: 1.0575895309448242\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.056544780731201 | KNN Loss: 5.024909019470215 | BCE Loss: 1.0316357612609863\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.079737663269043 | KNN Loss: 5.0338897705078125 | BCE Loss: 1.0458476543426514\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.0688676834106445 | KNN Loss: 5.024090766906738 | BCE Loss: 1.0447766780853271\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.062163829803467 | KNN Loss: 5.0304460525512695 | BCE Loss: 1.0317177772521973\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.072858810424805 | KNN Loss: 5.028896331787109 | BCE Loss: 1.0439625978469849\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.0564727783203125 | KNN Loss: 5.022899150848389 | BCE Loss: 1.0335733890533447\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.0507683753967285 | KNN Loss: 5.0223307609558105 | BCE Loss: 1.028437614440918\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.0408244132995605 | KNN Loss: 5.0151047706604 | BCE Loss: 1.0257196426391602\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.0250959396362305 | KNN Loss: 5.021103858947754 | BCE Loss: 1.0039918422698975\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.046081066131592 | KNN Loss: 5.009575366973877 | BCE Loss: 1.0365055799484253\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.063817501068115 | KNN Loss: 5.029394626617432 | BCE Loss: 1.0344228744506836\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.051810264587402 | KNN Loss: 5.017518997192383 | BCE Loss: 1.03429114818573\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.093308448791504 | KNN Loss: 5.032121181488037 | BCE Loss: 1.0611873865127563\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.034646034240723 | KNN Loss: 5.007331848144531 | BCE Loss: 1.0273139476776123\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.043686866760254 | KNN Loss: 5.003060817718506 | BCE Loss: 1.0406262874603271\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.018354892730713 | KNN Loss: 5.012209892272949 | BCE Loss: 1.0061450004577637\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.039430141448975 | KNN Loss: 5.0231499671936035 | BCE Loss: 1.016280174255371\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.0176825523376465 | KNN Loss: 5.000139236450195 | BCE Loss: 1.0175434350967407\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.0425262451171875 | KNN Loss: 5.005234241485596 | BCE Loss: 1.0372918844223022\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.067032814025879 | KNN Loss: 5.00994348526001 | BCE Loss: 1.05708909034729\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.090516090393066 | KNN Loss: 5.035923957824707 | BCE Loss: 1.0545918941497803\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.042527198791504 | KNN Loss: 5.0093560218811035 | BCE Loss: 1.0331711769104004\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.071417808532715 | KNN Loss: 5.0047783851623535 | BCE Loss: 1.0666391849517822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.048313617706299 | KNN Loss: 5.020328521728516 | BCE Loss: 1.0279849767684937\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.0476393699646 | KNN Loss: 5.01847505569458 | BCE Loss: 1.02916419506073\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.050023078918457 | KNN Loss: 5.015835285186768 | BCE Loss: 1.0341877937316895\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.037081718444824 | KNN Loss: 5.018431186676025 | BCE Loss: 1.018650770187378\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.0707855224609375 | KNN Loss: 5.024952411651611 | BCE Loss: 1.0458333492279053\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.058978080749512 | KNN Loss: 5.018710613250732 | BCE Loss: 1.0402674674987793\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.032491683959961 | KNN Loss: 5.0050177574157715 | BCE Loss: 1.0274739265441895\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.016548156738281 | KNN Loss: 5.016944885253906 | BCE Loss: 0.9996031522750854\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.071753025054932 | KNN Loss: 5.019177436828613 | BCE Loss: 1.0525755882263184\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.054531097412109 | KNN Loss: 5.007567882537842 | BCE Loss: 1.0469629764556885\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.019670486450195 | KNN Loss: 5.015730381011963 | BCE Loss: 1.003940224647522\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.018325328826904 | KNN Loss: 5.018582820892334 | BCE Loss: 0.9997425079345703\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.055364608764648 | KNN Loss: 5.009477615356445 | BCE Loss: 1.045886754989624\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.0109734535217285 | KNN Loss: 5.001043796539307 | BCE Loss: 1.0099296569824219\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.043713092803955 | KNN Loss: 5.005985260009766 | BCE Loss: 1.037727952003479\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.068476676940918 | KNN Loss: 5.044190883636475 | BCE Loss: 1.0242855548858643\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.055286407470703 | KNN Loss: 5.024330139160156 | BCE Loss: 1.0309562683105469\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.025084495544434 | KNN Loss: 4.997716903686523 | BCE Loss: 1.027367353439331\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.040722846984863 | KNN Loss: 5.01125431060791 | BCE Loss: 1.0294685363769531\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.031060695648193 | KNN Loss: 5.0147528648376465 | BCE Loss: 1.0163078308105469\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.014197826385498 | KNN Loss: 5.006598949432373 | BCE Loss: 1.007598876953125\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.037421226501465 | KNN Loss: 5.019830226898193 | BCE Loss: 1.0175909996032715\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.042766571044922 | KNN Loss: 5.007771968841553 | BCE Loss: 1.0349947214126587\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.0294508934021 | KNN Loss: 4.997458457946777 | BCE Loss: 1.0319924354553223\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.064294815063477 | KNN Loss: 5.002687931060791 | BCE Loss: 1.0616068840026855\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.077261924743652 | KNN Loss: 5.030997276306152 | BCE Loss: 1.0462647676467896\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.010888576507568 | KNN Loss: 4.999014377593994 | BCE Loss: 1.0118741989135742\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.061398983001709 | KNN Loss: 5.01321268081665 | BCE Loss: 1.0481863021850586\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.051558494567871 | KNN Loss: 5.016135215759277 | BCE Loss: 1.0354231595993042\n",
      "Epoch   166: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.065948486328125 | KNN Loss: 5.029716968536377 | BCE Loss: 1.0362313985824585\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.080380439758301 | KNN Loss: 5.040943145751953 | BCE Loss: 1.0394375324249268\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.063375473022461 | KNN Loss: 5.019216537475586 | BCE Loss: 1.0441588163375854\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.037240982055664 | KNN Loss: 5.019028663635254 | BCE Loss: 1.0182124376296997\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.0738630294799805 | KNN Loss: 5.022181034088135 | BCE Loss: 1.0516821146011353\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.055263519287109 | KNN Loss: 5.012643337249756 | BCE Loss: 1.0426201820373535\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.073235034942627 | KNN Loss: 5.0198283195495605 | BCE Loss: 1.0534067153930664\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.0661444664001465 | KNN Loss: 5.036390781402588 | BCE Loss: 1.0297536849975586\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.052325248718262 | KNN Loss: 5.0238213539123535 | BCE Loss: 1.0285038948059082\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.092920780181885 | KNN Loss: 5.065516948699951 | BCE Loss: 1.0274038314819336\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.0552077293396 | KNN Loss: 5.005710601806641 | BCE Loss: 1.0494970083236694\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.069194316864014 | KNN Loss: 5.016863822937012 | BCE Loss: 1.052330493927002\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.088386535644531 | KNN Loss: 5.013014793395996 | BCE Loss: 1.0753719806671143\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.024466514587402 | KNN Loss: 5.013467311859131 | BCE Loss: 1.0109992027282715\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.068554878234863 | KNN Loss: 5.013601303100586 | BCE Loss: 1.0549533367156982\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.081806182861328 | KNN Loss: 5.0400071144104 | BCE Loss: 1.0417988300323486\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.047588348388672 | KNN Loss: 5.00051736831665 | BCE Loss: 1.047070860862732\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.046814918518066 | KNN Loss: 5.020264148712158 | BCE Loss: 1.026550531387329\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.088431358337402 | KNN Loss: 5.020208835601807 | BCE Loss: 1.0682222843170166\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.04736328125 | KNN Loss: 5.01753568649292 | BCE Loss: 1.0298277139663696\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.0737457275390625 | KNN Loss: 5.039947509765625 | BCE Loss: 1.0337979793548584\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.030395030975342 | KNN Loss: 5.009653091430664 | BCE Loss: 1.0207418203353882\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.061639308929443 | KNN Loss: 5.029565811157227 | BCE Loss: 1.0320734977722168\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.034149169921875 | KNN Loss: 5.00081729888916 | BCE Loss: 1.0333318710327148\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.053297519683838 | KNN Loss: 5.055479526519775 | BCE Loss: 0.9978180527687073\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.07372522354126 | KNN Loss: 5.026634216308594 | BCE Loss: 1.047091007232666\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.085862159729004 | KNN Loss: 5.043752670288086 | BCE Loss: 1.0421092510223389\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.082254409790039 | KNN Loss: 5.052849769592285 | BCE Loss: 1.0294045209884644\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.051233291625977 | KNN Loss: 5.000351428985596 | BCE Loss: 1.0508819818496704\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.036856651306152 | KNN Loss: 5.01263952255249 | BCE Loss: 1.0242173671722412\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.097990036010742 | KNN Loss: 5.050278663635254 | BCE Loss: 1.0477111339569092\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.043316841125488 | KNN Loss: 5.002931118011475 | BCE Loss: 1.0403854846954346\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.058290004730225 | KNN Loss: 5.040986061096191 | BCE Loss: 1.0173038244247437\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.072265148162842 | KNN Loss: 5.009063243865967 | BCE Loss: 1.063201904296875\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.067951202392578 | KNN Loss: 5.036417484283447 | BCE Loss: 1.0315337181091309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.039434909820557 | KNN Loss: 4.999897480010986 | BCE Loss: 1.0395375490188599\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.071295261383057 | KNN Loss: 5.017430305480957 | BCE Loss: 1.05386483669281\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.0592122077941895 | KNN Loss: 5.0185980796813965 | BCE Loss: 1.040614128112793\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.067696571350098 | KNN Loss: 5.010709285736084 | BCE Loss: 1.0569872856140137\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.070507049560547 | KNN Loss: 5.0191731452941895 | BCE Loss: 1.0513336658477783\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.051589488983154 | KNN Loss: 5.007046699523926 | BCE Loss: 1.044542908668518\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.032484531402588 | KNN Loss: 5.005403995513916 | BCE Loss: 1.0270806550979614\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.059352397918701 | KNN Loss: 5.01471471786499 | BCE Loss: 1.0446375608444214\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.082200527191162 | KNN Loss: 5.01840877532959 | BCE Loss: 1.0637917518615723\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.0965752601623535 | KNN Loss: 5.04433012008667 | BCE Loss: 1.0522452592849731\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.061842441558838 | KNN Loss: 5.005556106567383 | BCE Loss: 1.0562864542007446\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.042226314544678 | KNN Loss: 5.007501125335693 | BCE Loss: 1.0347251892089844\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.066157341003418 | KNN Loss: 5.019487380981445 | BCE Loss: 1.0466700792312622\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.085448265075684 | KNN Loss: 5.047933578491211 | BCE Loss: 1.0375148057937622\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.0463643074035645 | KNN Loss: 5.03209114074707 | BCE Loss: 1.0142731666564941\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.007110595703125 | KNN Loss: 5.006606578826904 | BCE Loss: 1.0005041360855103\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.053017616271973 | KNN Loss: 5.0076985359191895 | BCE Loss: 1.0453193187713623\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.062833786010742 | KNN Loss: 5.02827262878418 | BCE Loss: 1.0345613956451416\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.051619052886963 | KNN Loss: 4.999302864074707 | BCE Loss: 1.0523161888122559\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.027604579925537 | KNN Loss: 4.991217613220215 | BCE Loss: 1.0363868474960327\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.035693168640137 | KNN Loss: 5.004907131195068 | BCE Loss: 1.0307862758636475\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.055710792541504 | KNN Loss: 4.997124195098877 | BCE Loss: 1.058586597442627\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.093584060668945 | KNN Loss: 5.019748687744141 | BCE Loss: 1.0738356113433838\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.072488307952881 | KNN Loss: 5.025204658508301 | BCE Loss: 1.04728364944458\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.057450294494629 | KNN Loss: 5.023519039154053 | BCE Loss: 1.0339314937591553\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.093273162841797 | KNN Loss: 5.04589319229126 | BCE Loss: 1.0473798513412476\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.091287136077881 | KNN Loss: 5.068228721618652 | BCE Loss: 1.0230584144592285\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.056634902954102 | KNN Loss: 5.0130205154418945 | BCE Loss: 1.043614149093628\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.035109519958496 | KNN Loss: 4.990444183349609 | BCE Loss: 1.0446653366088867\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.059510231018066 | KNN Loss: 5.028203010559082 | BCE Loss: 1.0313072204589844\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.0391154289245605 | KNN Loss: 5.002109527587891 | BCE Loss: 1.03700590133667\n",
      "Epoch   177: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.057936668395996 | KNN Loss: 5.03032922744751 | BCE Loss: 1.0276074409484863\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.03847599029541 | KNN Loss: 5.009131908416748 | BCE Loss: 1.0293443202972412\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.098674774169922 | KNN Loss: 5.002336502075195 | BCE Loss: 1.0963380336761475\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.1133036613464355 | KNN Loss: 5.051912784576416 | BCE Loss: 1.06139075756073\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.071396827697754 | KNN Loss: 5.033945083618164 | BCE Loss: 1.0374515056610107\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.039724349975586 | KNN Loss: 5.00150203704834 | BCE Loss: 1.0382224321365356\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.057523250579834 | KNN Loss: 5.00667142868042 | BCE Loss: 1.050851821899414\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.050371170043945 | KNN Loss: 5.0263237953186035 | BCE Loss: 1.0240471363067627\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.079018592834473 | KNN Loss: 5.018016338348389 | BCE Loss: 1.061002492904663\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.07579231262207 | KNN Loss: 5.032856464385986 | BCE Loss: 1.0429356098175049\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.052969932556152 | KNN Loss: 5.0475969314575195 | BCE Loss: 1.0053727626800537\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.070012092590332 | KNN Loss: 5.033239841461182 | BCE Loss: 1.0367720127105713\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.063808441162109 | KNN Loss: 5.0221428871154785 | BCE Loss: 1.04166579246521\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.103801727294922 | KNN Loss: 5.048742771148682 | BCE Loss: 1.0550589561462402\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.038673400878906 | KNN Loss: 5.007535457611084 | BCE Loss: 1.0311381816864014\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.017386436462402 | KNN Loss: 5.016504287719727 | BCE Loss: 1.0008823871612549\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.070243835449219 | KNN Loss: 5.0135955810546875 | BCE Loss: 1.0566480159759521\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.080574035644531 | KNN Loss: 5.011847019195557 | BCE Loss: 1.0687267780303955\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.031872749328613 | KNN Loss: 5.009407997131348 | BCE Loss: 1.0224649906158447\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.058944225311279 | KNN Loss: 4.999625205993652 | BCE Loss: 1.0593189001083374\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.05484676361084 | KNN Loss: 5.022619247436523 | BCE Loss: 1.0322275161743164\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.0152482986450195 | KNN Loss: 5.016462326049805 | BCE Loss: 0.9987857341766357\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.054193019866943 | KNN Loss: 5.019459247589111 | BCE Loss: 1.034733772277832\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.023317813873291 | KNN Loss: 5.000752925872803 | BCE Loss: 1.0225648880004883\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.049873352050781 | KNN Loss: 5.009098052978516 | BCE Loss: 1.0407755374908447\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.0529351234436035 | KNN Loss: 5.003498554229736 | BCE Loss: 1.0494365692138672\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.0601654052734375 | KNN Loss: 5.011683464050293 | BCE Loss: 1.0484817028045654\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.095263957977295 | KNN Loss: 5.041428565979004 | BCE Loss: 1.053835391998291\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.0320634841918945 | KNN Loss: 5.002663612365723 | BCE Loss: 1.0293998718261719\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.10505485534668 | KNN Loss: 5.037852764129639 | BCE Loss: 1.067202091217041\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.079387187957764 | KNN Loss: 5.018489360809326 | BCE Loss: 1.0608978271484375\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.067488193511963 | KNN Loss: 5.03091287612915 | BCE Loss: 1.0365753173828125\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.108948707580566 | KNN Loss: 5.082859992980957 | BCE Loss: 1.0260885953903198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.051867961883545 | KNN Loss: 5.025279998779297 | BCE Loss: 1.0265878438949585\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.048334121704102 | KNN Loss: 5.00639009475708 | BCE Loss: 1.0419442653656006\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.048981189727783 | KNN Loss: 5.022535800933838 | BCE Loss: 1.0264452695846558\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.0886383056640625 | KNN Loss: 5.065757751464844 | BCE Loss: 1.0228805541992188\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.04541015625 | KNN Loss: 5.014416694641113 | BCE Loss: 1.0309933423995972\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.0607171058654785 | KNN Loss: 5.014969348907471 | BCE Loss: 1.0457477569580078\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.062707424163818 | KNN Loss: 5.0274338722229 | BCE Loss: 1.035273551940918\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.028414726257324 | KNN Loss: 5.012734889984131 | BCE Loss: 1.0156800746917725\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.050687313079834 | KNN Loss: 5.010551452636719 | BCE Loss: 1.0401358604431152\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.0294904708862305 | KNN Loss: 5.011430263519287 | BCE Loss: 1.0180604457855225\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.063596248626709 | KNN Loss: 5.016172885894775 | BCE Loss: 1.0474233627319336\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.016319274902344 | KNN Loss: 5.0013628005981445 | BCE Loss: 1.0149563550949097\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.04560661315918 | KNN Loss: 5.009337902069092 | BCE Loss: 1.0362684726715088\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.032321929931641 | KNN Loss: 5.011190891265869 | BCE Loss: 1.0211310386657715\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.0462188720703125 | KNN Loss: 4.999831676483154 | BCE Loss: 1.046386957168579\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.048652648925781 | KNN Loss: 5.0052947998046875 | BCE Loss: 1.0433576107025146\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.090986251831055 | KNN Loss: 5.064309597015381 | BCE Loss: 1.0266765356063843\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.048995494842529 | KNN Loss: 5.002281665802002 | BCE Loss: 1.0467137098312378\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.025414943695068 | KNN Loss: 5.0098557472229 | BCE Loss: 1.015559196472168\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.051855087280273 | KNN Loss: 5.014703273773193 | BCE Loss: 1.03715181350708\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.033860206604004 | KNN Loss: 5.021811008453369 | BCE Loss: 1.0120491981506348\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.0043206214904785 | KNN Loss: 5.003756523132324 | BCE Loss: 1.0005642175674438\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.04920768737793 | KNN Loss: 5.017961025238037 | BCE Loss: 1.0312464237213135\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.057958602905273 | KNN Loss: 5.007265090942383 | BCE Loss: 1.0506937503814697\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.041188716888428 | KNN Loss: 5.019793510437012 | BCE Loss: 1.0213950872421265\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.067739486694336 | KNN Loss: 5.041322708129883 | BCE Loss: 1.0264170169830322\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.086994647979736 | KNN Loss: 5.031745433807373 | BCE Loss: 1.0552493333816528\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.060551643371582 | KNN Loss: 5.026416778564453 | BCE Loss: 1.0341346263885498\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.136832237243652 | KNN Loss: 5.103297233581543 | BCE Loss: 1.0335350036621094\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.076567649841309 | KNN Loss: 5.025396823883057 | BCE Loss: 1.051171064376831\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.103305816650391 | KNN Loss: 5.0463547706604 | BCE Loss: 1.0569508075714111\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.08183479309082 | KNN Loss: 5.016720771789551 | BCE Loss: 1.06511390209198\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.048643112182617 | KNN Loss: 5.004751205444336 | BCE Loss: 1.0438917875289917\n",
      "Epoch   188: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.037374496459961 | KNN Loss: 5.021022796630859 | BCE Loss: 1.016351580619812\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.061898708343506 | KNN Loss: 5.003868103027344 | BCE Loss: 1.058030605316162\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.074490547180176 | KNN Loss: 5.029991626739502 | BCE Loss: 1.0444986820220947\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.124439239501953 | KNN Loss: 5.105611324310303 | BCE Loss: 1.0188279151916504\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.027765274047852 | KNN Loss: 5.009246349334717 | BCE Loss: 1.0185189247131348\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.036290645599365 | KNN Loss: 5.020535945892334 | BCE Loss: 1.0157546997070312\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.084971904754639 | KNN Loss: 5.032759189605713 | BCE Loss: 1.0522128343582153\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.068758964538574 | KNN Loss: 5.033053398132324 | BCE Loss: 1.035705327987671\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.064094543457031 | KNN Loss: 5.005317687988281 | BCE Loss: 1.05877685546875\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.040306091308594 | KNN Loss: 5.002378463745117 | BCE Loss: 1.0379277467727661\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.063179969787598 | KNN Loss: 4.9992241859436035 | BCE Loss: 1.0639557838439941\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.048090934753418 | KNN Loss: 5.027998447418213 | BCE Loss: 1.020092487335205\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.032352924346924 | KNN Loss: 5.008544445037842 | BCE Loss: 1.0238085985183716\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.030360698699951 | KNN Loss: 5.010614395141602 | BCE Loss: 1.01974618434906\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.043558597564697 | KNN Loss: 5.0191802978515625 | BCE Loss: 1.0243782997131348\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.046308517456055 | KNN Loss: 5.00687313079834 | BCE Loss: 1.039435625076294\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.072724342346191 | KNN Loss: 5.023316383361816 | BCE Loss: 1.0494080781936646\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.044363975524902 | KNN Loss: 5.013092041015625 | BCE Loss: 1.0312716960906982\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.092267036437988 | KNN Loss: 5.0457234382629395 | BCE Loss: 1.0465433597564697\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.070476531982422 | KNN Loss: 5.024258136749268 | BCE Loss: 1.0462182760238647\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.068735122680664 | KNN Loss: 5.016334533691406 | BCE Loss: 1.0524003505706787\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.062519073486328 | KNN Loss: 4.999703884124756 | BCE Loss: 1.0628154277801514\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.051942825317383 | KNN Loss: 5.0243821144104 | BCE Loss: 1.0275609493255615\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.099306106567383 | KNN Loss: 5.048614025115967 | BCE Loss: 1.050691843032837\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.056224346160889 | KNN Loss: 5.023105144500732 | BCE Loss: 1.0331193208694458\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.025585174560547 | KNN Loss: 4.998935222625732 | BCE Loss: 1.0266499519348145\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.075101852416992 | KNN Loss: 5.024938583374023 | BCE Loss: 1.0501635074615479\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.0481367111206055 | KNN Loss: 5.010272979736328 | BCE Loss: 1.0378634929656982\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.030036926269531 | KNN Loss: 5.005789279937744 | BCE Loss: 1.024247646331787\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.047271728515625 | KNN Loss: 5.010952472686768 | BCE Loss: 1.0363190174102783\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.060270309448242 | KNN Loss: 5.009147644042969 | BCE Loss: 1.0511224269866943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.04957389831543 | KNN Loss: 5.013742446899414 | BCE Loss: 1.0358312129974365\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.062358856201172 | KNN Loss: 5.032676696777344 | BCE Loss: 1.0296823978424072\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.057497978210449 | KNN Loss: 4.992732048034668 | BCE Loss: 1.0647661685943604\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.031486988067627 | KNN Loss: 5.010270118713379 | BCE Loss: 1.021216869354248\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.059513092041016 | KNN Loss: 5.015792369842529 | BCE Loss: 1.0437204837799072\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.049061298370361 | KNN Loss: 5.013402938842773 | BCE Loss: 1.035658359527588\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.07982063293457 | KNN Loss: 5.01749324798584 | BCE Loss: 1.0623271465301514\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.075778961181641 | KNN Loss: 5.025097846984863 | BCE Loss: 1.0506808757781982\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.057319164276123 | KNN Loss: 5.008772850036621 | BCE Loss: 1.048546314239502\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.068948745727539 | KNN Loss: 5.02839469909668 | BCE Loss: 1.0405540466308594\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.031167030334473 | KNN Loss: 5.007657527923584 | BCE Loss: 1.0235096216201782\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.061258316040039 | KNN Loss: 5.010055065155029 | BCE Loss: 1.0512030124664307\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.0761399269104 | KNN Loss: 5.014326095581055 | BCE Loss: 1.0618139505386353\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.036351680755615 | KNN Loss: 5.006917476654053 | BCE Loss: 1.029434323310852\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.0499725341796875 | KNN Loss: 5.0386857986450195 | BCE Loss: 1.0112864971160889\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.066381931304932 | KNN Loss: 5.009371757507324 | BCE Loss: 1.0570101737976074\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.063050746917725 | KNN Loss: 5.024426460266113 | BCE Loss: 1.0386242866516113\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.045989036560059 | KNN Loss: 5.0235419273376465 | BCE Loss: 1.0224473476409912\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.044301986694336 | KNN Loss: 5.003457069396973 | BCE Loss: 1.0408446788787842\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.092173099517822 | KNN Loss: 5.022032260894775 | BCE Loss: 1.0701408386230469\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.076148986816406 | KNN Loss: 5.044102668762207 | BCE Loss: 1.0320463180541992\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.016752243041992 | KNN Loss: 5.0005693435668945 | BCE Loss: 1.0161826610565186\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.0631561279296875 | KNN Loss: 5.01591157913208 | BCE Loss: 1.0472447872161865\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.0290656089782715 | KNN Loss: 5.0051703453063965 | BCE Loss: 1.023895263671875\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.038510322570801 | KNN Loss: 5.035355567932129 | BCE Loss: 1.0031545162200928\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.066934108734131 | KNN Loss: 5.005923271179199 | BCE Loss: 1.0610109567642212\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.058405876159668 | KNN Loss: 5.014698505401611 | BCE Loss: 1.043707251548767\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.077805519104004 | KNN Loss: 5.021764278411865 | BCE Loss: 1.0560410022735596\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.0422468185424805 | KNN Loss: 5.020472049713135 | BCE Loss: 1.0217746496200562\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.05156135559082 | KNN Loss: 5.017664909362793 | BCE Loss: 1.0338962078094482\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.050868034362793 | KNN Loss: 5.01437520980835 | BCE Loss: 1.0364927053451538\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.068511009216309 | KNN Loss: 5.035397052764893 | BCE Loss: 1.0331141948699951\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.057323932647705 | KNN Loss: 5.0220746994018555 | BCE Loss: 1.0352493524551392\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.005810737609863 | KNN Loss: 5.003890037536621 | BCE Loss: 1.0019205808639526\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.061357498168945 | KNN Loss: 5.045617580413818 | BCE Loss: 1.015739917755127\n",
      "Epoch   199: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.064568042755127 | KNN Loss: 5.033265113830566 | BCE Loss: 1.03130304813385\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.016506195068359 | KNN Loss: 4.997761249542236 | BCE Loss: 1.018744945526123\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.085096836090088 | KNN Loss: 5.029777526855469 | BCE Loss: 1.0553191900253296\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.045405387878418 | KNN Loss: 5.005308151245117 | BCE Loss: 1.0400972366333008\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.0623087882995605 | KNN Loss: 5.0218825340271 | BCE Loss: 1.040426254272461\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.063297748565674 | KNN Loss: 5.007870197296143 | BCE Loss: 1.0554276704788208\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.051889419555664 | KNN Loss: 5.022060871124268 | BCE Loss: 1.0298285484313965\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.01325798034668 | KNN Loss: 5.004495620727539 | BCE Loss: 1.0087621212005615\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.043006420135498 | KNN Loss: 5.019500732421875 | BCE Loss: 1.0235055685043335\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.091174125671387 | KNN Loss: 5.055225849151611 | BCE Loss: 1.0359481573104858\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.043883323669434 | KNN Loss: 5.022693157196045 | BCE Loss: 1.0211901664733887\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.041906356811523 | KNN Loss: 5.023571014404297 | BCE Loss: 1.0183353424072266\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.0854172706604 | KNN Loss: 5.011622905731201 | BCE Loss: 1.0737943649291992\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.060995101928711 | KNN Loss: 5.009323596954346 | BCE Loss: 1.0516712665557861\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.058386325836182 | KNN Loss: 4.999548435211182 | BCE Loss: 1.0588377714157104\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.0973944664001465 | KNN Loss: 5.043475151062012 | BCE Loss: 1.0539193153381348\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.101147174835205 | KNN Loss: 5.046350002288818 | BCE Loss: 1.0547971725463867\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.024811744689941 | KNN Loss: 5.018274307250977 | BCE Loss: 1.0065375566482544\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.06026554107666 | KNN Loss: 5.038002967834473 | BCE Loss: 1.0222625732421875\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.036774635314941 | KNN Loss: 5.003415584564209 | BCE Loss: 1.0333592891693115\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.031184196472168 | KNN Loss: 5.013030052185059 | BCE Loss: 1.0181543827056885\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.027379512786865 | KNN Loss: 4.9950385093688965 | BCE Loss: 1.0323410034179688\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.031639575958252 | KNN Loss: 5.008821487426758 | BCE Loss: 1.0228180885314941\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.148014068603516 | KNN Loss: 5.093236923217773 | BCE Loss: 1.0547771453857422\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.0665388107299805 | KNN Loss: 5.013341903686523 | BCE Loss: 1.0531970262527466\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.081298351287842 | KNN Loss: 5.050626754760742 | BCE Loss: 1.0306715965270996\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.143231391906738 | KNN Loss: 5.121932029724121 | BCE Loss: 1.0212992429733276\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.033083915710449 | KNN Loss: 5.009812355041504 | BCE Loss: 1.0232715606689453\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.021249771118164 | KNN Loss: 4.999669075012207 | BCE Loss: 1.021580696105957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.076411247253418 | KNN Loss: 5.041466236114502 | BCE Loss: 1.0349451303482056\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.043013572692871 | KNN Loss: 5.000006198883057 | BCE Loss: 1.0430076122283936\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.077662467956543 | KNN Loss: 5.026898384094238 | BCE Loss: 1.0507640838623047\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.078774452209473 | KNN Loss: 5.0721893310546875 | BCE Loss: 1.006584882736206\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.088666915893555 | KNN Loss: 5.054577827453613 | BCE Loss: 1.0340888500213623\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.059669494628906 | KNN Loss: 5.041635990142822 | BCE Loss: 1.018033504486084\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.085874557495117 | KNN Loss: 5.027984142303467 | BCE Loss: 1.0578904151916504\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.056391716003418 | KNN Loss: 5.010897159576416 | BCE Loss: 1.045494794845581\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.059429168701172 | KNN Loss: 5.013786792755127 | BCE Loss: 1.045642375946045\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.038208961486816 | KNN Loss: 5.002755165100098 | BCE Loss: 1.0354536771774292\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.080715179443359 | KNN Loss: 5.018510341644287 | BCE Loss: 1.0622050762176514\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.123235702514648 | KNN Loss: 5.044482707977295 | BCE Loss: 1.0787532329559326\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.0409440994262695 | KNN Loss: 5.010133743286133 | BCE Loss: 1.0308101177215576\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.048096656799316 | KNN Loss: 5.028931140899658 | BCE Loss: 1.019165277481079\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.0133056640625 | KNN Loss: 5.0043182373046875 | BCE Loss: 1.0089876651763916\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.036014556884766 | KNN Loss: 5.026064872741699 | BCE Loss: 1.009949803352356\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.06673526763916 | KNN Loss: 5.009215354919434 | BCE Loss: 1.0575200319290161\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.047679901123047 | KNN Loss: 5.014740467071533 | BCE Loss: 1.0329396724700928\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.069731712341309 | KNN Loss: 5.024032115936279 | BCE Loss: 1.0456995964050293\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.046238899230957 | KNN Loss: 5.008387565612793 | BCE Loss: 1.037851095199585\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.042548179626465 | KNN Loss: 5.006515979766846 | BCE Loss: 1.0360324382781982\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.040524482727051 | KNN Loss: 5.003544807434082 | BCE Loss: 1.0369794368743896\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.029928207397461 | KNN Loss: 5.004852294921875 | BCE Loss: 1.0250757932662964\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.076969623565674 | KNN Loss: 5.021216869354248 | BCE Loss: 1.0557527542114258\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.041547775268555 | KNN Loss: 5.014902591705322 | BCE Loss: 1.0266449451446533\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.039421558380127 | KNN Loss: 4.993517875671387 | BCE Loss: 1.0459036827087402\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.074932098388672 | KNN Loss: 5.016541004180908 | BCE Loss: 1.0583909749984741\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.055824279785156 | KNN Loss: 5.010724067687988 | BCE Loss: 1.0451000928878784\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.0549397468566895 | KNN Loss: 5.011852264404297 | BCE Loss: 1.0430876016616821\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.030453681945801 | KNN Loss: 5.005712032318115 | BCE Loss: 1.0247414112091064\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.048029899597168 | KNN Loss: 5.018021106719971 | BCE Loss: 1.0300090312957764\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.057798385620117 | KNN Loss: 5.022082805633545 | BCE Loss: 1.0357158184051514\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.052495956420898 | KNN Loss: 5.022973537445068 | BCE Loss: 1.0295226573944092\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.03560733795166 | KNN Loss: 5.00834321975708 | BCE Loss: 1.0272643566131592\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.047677040100098 | KNN Loss: 5.001865386962891 | BCE Loss: 1.045811653137207\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.115328788757324 | KNN Loss: 5.073354244232178 | BCE Loss: 1.0419743061065674\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.05569314956665 | KNN Loss: 5.0265350341796875 | BCE Loss: 1.0291582345962524\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.062993049621582 | KNN Loss: 5.037851810455322 | BCE Loss: 1.0251412391662598\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.05179500579834 | KNN Loss: 5.011475563049316 | BCE Loss: 1.0403194427490234\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.127956390380859 | KNN Loss: 5.06551456451416 | BCE Loss: 1.0624418258666992\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.05985164642334 | KNN Loss: 5.02758264541626 | BCE Loss: 1.0322692394256592\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.036092281341553 | KNN Loss: 5.018030166625977 | BCE Loss: 1.0180621147155762\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.067022323608398 | KNN Loss: 5.008965492248535 | BCE Loss: 1.0580568313598633\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.0499114990234375 | KNN Loss: 5.001850605010986 | BCE Loss: 1.0480611324310303\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.034266471862793 | KNN Loss: 5.019611835479736 | BCE Loss: 1.0146543979644775\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.017327308654785 | KNN Loss: 5.002849578857422 | BCE Loss: 1.0144777297973633\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.054715156555176 | KNN Loss: 5.012023448944092 | BCE Loss: 1.042691707611084\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.096197128295898 | KNN Loss: 5.059826374053955 | BCE Loss: 1.0363705158233643\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.060588836669922 | KNN Loss: 5.025685787200928 | BCE Loss: 1.0349032878875732\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.069757461547852 | KNN Loss: 5.025506973266602 | BCE Loss: 1.044250249862671\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.0385355949401855 | KNN Loss: 5.008518218994141 | BCE Loss: 1.030017375946045\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.0458664894104 | KNN Loss: 5.014063358306885 | BCE Loss: 1.0318031311035156\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.062721252441406 | KNN Loss: 5.024611473083496 | BCE Loss: 1.0381100177764893\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.081296920776367 | KNN Loss: 5.03969144821167 | BCE Loss: 1.0416053533554077\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.0322980880737305 | KNN Loss: 5.017179012298584 | BCE Loss: 1.015118956565857\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.072910308837891 | KNN Loss: 5.03461217880249 | BCE Loss: 1.03829824924469\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.04754638671875 | KNN Loss: 5.026776313781738 | BCE Loss: 1.0207700729370117\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.040782451629639 | KNN Loss: 5.029669284820557 | BCE Loss: 1.0111130475997925\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.041431427001953 | KNN Loss: 5.016500473022461 | BCE Loss: 1.0249311923980713\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.063404560089111 | KNN Loss: 5.019499778747559 | BCE Loss: 1.0439047813415527\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.0734758377075195 | KNN Loss: 5.006407737731934 | BCE Loss: 1.0670678615570068\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.016646862030029 | KNN Loss: 5.026495933532715 | BCE Loss: 0.990151047706604\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.037570953369141 | KNN Loss: 5.011409759521484 | BCE Loss: 1.0261613130569458\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.0812482833862305 | KNN Loss: 5.051877021789551 | BCE Loss: 1.0293710231781006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.107327461242676 | KNN Loss: 5.058116912841797 | BCE Loss: 1.049210786819458\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.0383782386779785 | KNN Loss: 5.016050815582275 | BCE Loss: 1.0223274230957031\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.040165901184082 | KNN Loss: 5.0120463371276855 | BCE Loss: 1.0281195640563965\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.037905693054199 | KNN Loss: 5.023917198181152 | BCE Loss: 1.0139882564544678\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.030048370361328 | KNN Loss: 5.010637283325195 | BCE Loss: 1.0194108486175537\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.077371120452881 | KNN Loss: 5.01474142074585 | BCE Loss: 1.0626298189163208\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.027690887451172 | KNN Loss: 5.010293483734131 | BCE Loss: 1.017397165298462\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.046992301940918 | KNN Loss: 4.997705936431885 | BCE Loss: 1.0492866039276123\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.027586460113525 | KNN Loss: 5.015208721160889 | BCE Loss: 1.0123776197433472\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.0415472984313965 | KNN Loss: 5.018491744995117 | BCE Loss: 1.0230555534362793\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.037230968475342 | KNN Loss: 5.020118236541748 | BCE Loss: 1.0171126127243042\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.031393051147461 | KNN Loss: 5.005661964416504 | BCE Loss: 1.025730848312378\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.031787872314453 | KNN Loss: 5.011476516723633 | BCE Loss: 1.0203115940093994\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.04050350189209 | KNN Loss: 5.00137996673584 | BCE Loss: 1.0391234159469604\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.067017555236816 | KNN Loss: 5.011236667633057 | BCE Loss: 1.0557810068130493\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.046464920043945 | KNN Loss: 5.0356340408325195 | BCE Loss: 1.0108311176300049\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.077620983123779 | KNN Loss: 5.009110450744629 | BCE Loss: 1.0685105323791504\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.0450005531311035 | KNN Loss: 5.012475490570068 | BCE Loss: 1.0325250625610352\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.054374694824219 | KNN Loss: 5.01215124130249 | BCE Loss: 1.042223572731018\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.058764457702637 | KNN Loss: 5.031813621520996 | BCE Loss: 1.0269505977630615\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.058356285095215 | KNN Loss: 5.017141342163086 | BCE Loss: 1.0412147045135498\n",
      "Epoch   218: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.04838228225708 | KNN Loss: 5.019061088562012 | BCE Loss: 1.0293211936950684\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.065783500671387 | KNN Loss: 5.0206298828125 | BCE Loss: 1.0451536178588867\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.031137943267822 | KNN Loss: 5.0127482414245605 | BCE Loss: 1.0183897018432617\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.087345123291016 | KNN Loss: 5.052701950073242 | BCE Loss: 1.0346434116363525\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.053254127502441 | KNN Loss: 4.998109340667725 | BCE Loss: 1.0551447868347168\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.042665481567383 | KNN Loss: 5.017848014831543 | BCE Loss: 1.0248172283172607\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.043481349945068 | KNN Loss: 5.019742012023926 | BCE Loss: 1.0237393379211426\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.03953742980957 | KNN Loss: 5.010277271270752 | BCE Loss: 1.0292599201202393\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.0717387199401855 | KNN Loss: 5.023568630218506 | BCE Loss: 1.0481702089309692\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.046802520751953 | KNN Loss: 5.0229411125183105 | BCE Loss: 1.023861289024353\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.09910774230957 | KNN Loss: 5.04533576965332 | BCE Loss: 1.05377197265625\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.034751892089844 | KNN Loss: 5.016920566558838 | BCE Loss: 1.0178312063217163\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.025897979736328 | KNN Loss: 5.029911041259766 | BCE Loss: 0.995986819267273\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.072174549102783 | KNN Loss: 5.03353214263916 | BCE Loss: 1.0386425256729126\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.066257476806641 | KNN Loss: 5.010427474975586 | BCE Loss: 1.0558297634124756\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.074677467346191 | KNN Loss: 5.033465385437012 | BCE Loss: 1.0412122011184692\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.092382907867432 | KNN Loss: 5.057960510253906 | BCE Loss: 1.0344222784042358\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.067896842956543 | KNN Loss: 5.01615571975708 | BCE Loss: 1.051741123199463\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.06257963180542 | KNN Loss: 5.025827407836914 | BCE Loss: 1.0367522239685059\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.0111589431762695 | KNN Loss: 5.002598762512207 | BCE Loss: 1.0085601806640625\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.044900894165039 | KNN Loss: 5.018015384674072 | BCE Loss: 1.0268853902816772\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.036682605743408 | KNN Loss: 5.003675937652588 | BCE Loss: 1.0330066680908203\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.12385892868042 | KNN Loss: 5.047252178192139 | BCE Loss: 1.0766068696975708\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.034014701843262 | KNN Loss: 5.006760597229004 | BCE Loss: 1.0272539854049683\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.036848545074463 | KNN Loss: 5.019475936889648 | BCE Loss: 1.017372488975525\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.04401969909668 | KNN Loss: 5.0179853439331055 | BCE Loss: 1.0260341167449951\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.0538330078125 | KNN Loss: 5.028674602508545 | BCE Loss: 1.025158166885376\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.032625198364258 | KNN Loss: 5.007441520690918 | BCE Loss: 1.025183916091919\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.067803382873535 | KNN Loss: 5.008186340332031 | BCE Loss: 1.0596168041229248\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.041867733001709 | KNN Loss: 5.017910003662109 | BCE Loss: 1.02395761013031\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.049868106842041 | KNN Loss: 5.015217304229736 | BCE Loss: 1.0346508026123047\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.074134826660156 | KNN Loss: 5.028385639190674 | BCE Loss: 1.0457491874694824\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.064365863800049 | KNN Loss: 5.029242515563965 | BCE Loss: 1.035123348236084\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.055958271026611 | KNN Loss: 5.0217390060424805 | BCE Loss: 1.0342192649841309\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.074926853179932 | KNN Loss: 5.026460647583008 | BCE Loss: 1.0484660863876343\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.104727745056152 | KNN Loss: 5.074519157409668 | BCE Loss: 1.0302083492279053\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.0395379066467285 | KNN Loss: 5.000086307525635 | BCE Loss: 1.0394517183303833\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.065647125244141 | KNN Loss: 5.013184547424316 | BCE Loss: 1.0524626970291138\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.106342315673828 | KNN Loss: 5.048603057861328 | BCE Loss: 1.057739496231079\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.04641056060791 | KNN Loss: 5.008455753326416 | BCE Loss: 1.037954568862915\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.038904190063477 | KNN Loss: 5.009193420410156 | BCE Loss: 1.0297107696533203\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.051367282867432 | KNN Loss: 5.01638126373291 | BCE Loss: 1.0349860191345215\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.069967746734619 | KNN Loss: 5.044164657592773 | BCE Loss: 1.0258030891418457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.075874328613281 | KNN Loss: 5.043334484100342 | BCE Loss: 1.0325398445129395\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.046077251434326 | KNN Loss: 5.01769495010376 | BCE Loss: 1.0283823013305664\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.115257263183594 | KNN Loss: 5.076364517211914 | BCE Loss: 1.0388925075531006\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.017975330352783 | KNN Loss: 5.009749889373779 | BCE Loss: 1.008225440979004\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.046792984008789 | KNN Loss: 5.017925262451172 | BCE Loss: 1.0288677215576172\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.091151714324951 | KNN Loss: 5.052041053771973 | BCE Loss: 1.039110541343689\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.066736221313477 | KNN Loss: 5.037837982177734 | BCE Loss: 1.0288984775543213\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.074506759643555 | KNN Loss: 5.015486717224121 | BCE Loss: 1.0590200424194336\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.029462814331055 | KNN Loss: 5.005612373352051 | BCE Loss: 1.0238502025604248\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.081750392913818 | KNN Loss: 5.014817237854004 | BCE Loss: 1.0669331550598145\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.041402816772461 | KNN Loss: 5.028351306915283 | BCE Loss: 1.0130515098571777\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.0617876052856445 | KNN Loss: 5.022034645080566 | BCE Loss: 1.0397530794143677\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.056203365325928 | KNN Loss: 5.018709659576416 | BCE Loss: 1.0374938249588013\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.045509338378906 | KNN Loss: 5.012704849243164 | BCE Loss: 1.0328044891357422\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.077655792236328 | KNN Loss: 5.020724773406982 | BCE Loss: 1.0569310188293457\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.075827121734619 | KNN Loss: 5.061089515686035 | BCE Loss: 1.014737606048584\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.048606872558594 | KNN Loss: 5.017881870269775 | BCE Loss: 1.0307250022888184\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.0113959312438965 | KNN Loss: 5.015864849090576 | BCE Loss: 0.9955309629440308\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.038080215454102 | KNN Loss: 5.004533290863037 | BCE Loss: 1.033547043800354\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.068753242492676 | KNN Loss: 5.024237632751465 | BCE Loss: 1.0445154905319214\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.0435991287231445 | KNN Loss: 5.029175281524658 | BCE Loss: 1.0144236087799072\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.076777458190918 | KNN Loss: 5.018984794616699 | BCE Loss: 1.0577926635742188\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.08413553237915 | KNN Loss: 5.010737419128418 | BCE Loss: 1.0733981132507324\n",
      "Epoch   229: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.069155693054199 | KNN Loss: 5.007755279541016 | BCE Loss: 1.0614006519317627\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.044271469116211 | KNN Loss: 5.031350612640381 | BCE Loss: 1.0129210948944092\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.047372341156006 | KNN Loss: 5.005500316619873 | BCE Loss: 1.0418719053268433\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.04817008972168 | KNN Loss: 5.00934362411499 | BCE Loss: 1.0388262271881104\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.042991638183594 | KNN Loss: 5.008855819702148 | BCE Loss: 1.0341356992721558\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.070764541625977 | KNN Loss: 5.021813869476318 | BCE Loss: 1.0489509105682373\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.056209087371826 | KNN Loss: 5.020248889923096 | BCE Loss: 1.0359601974487305\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.0554304122924805 | KNN Loss: 5.019804954528809 | BCE Loss: 1.0356252193450928\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.066184997558594 | KNN Loss: 5.020811557769775 | BCE Loss: 1.0453736782073975\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.0317583084106445 | KNN Loss: 5.008471965789795 | BCE Loss: 1.0232865810394287\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.043430328369141 | KNN Loss: 5.006784439086914 | BCE Loss: 1.0366461277008057\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.027108192443848 | KNN Loss: 5.003946781158447 | BCE Loss: 1.0231614112854004\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.0543341636657715 | KNN Loss: 5.025347709655762 | BCE Loss: 1.0289864540100098\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.035139083862305 | KNN Loss: 4.994013786315918 | BCE Loss: 1.0411252975463867\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.031975746154785 | KNN Loss: 5.031040191650391 | BCE Loss: 1.0009355545043945\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.04196834564209 | KNN Loss: 5.003152370452881 | BCE Loss: 1.0388157367706299\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.057315826416016 | KNN Loss: 5.025108337402344 | BCE Loss: 1.0322072505950928\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.08904504776001 | KNN Loss: 5.038325786590576 | BCE Loss: 1.0507192611694336\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.025766372680664 | KNN Loss: 5.013744354248047 | BCE Loss: 1.0120221376419067\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.06342077255249 | KNN Loss: 5.064504623413086 | BCE Loss: 0.99891597032547\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.05767297744751 | KNN Loss: 5.017302513122559 | BCE Loss: 1.0403705835342407\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.055498123168945 | KNN Loss: 5.0151166915893555 | BCE Loss: 1.0403815507888794\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.03214168548584 | KNN Loss: 5.0052056312561035 | BCE Loss: 1.0269362926483154\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.072442054748535 | KNN Loss: 5.03125 | BCE Loss: 1.041191816329956\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.050056457519531 | KNN Loss: 5.018640995025635 | BCE Loss: 1.0314154624938965\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.047759532928467 | KNN Loss: 5.007920265197754 | BCE Loss: 1.0398393869400024\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.045422554016113 | KNN Loss: 5.015466213226318 | BCE Loss: 1.029956340789795\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.06306266784668 | KNN Loss: 5.0518622398376465 | BCE Loss: 1.011200189590454\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.0592145919799805 | KNN Loss: 5.020380020141602 | BCE Loss: 1.0388343334197998\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.059393405914307 | KNN Loss: 5.035886287689209 | BCE Loss: 1.0235071182250977\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.065438270568848 | KNN Loss: 5.025339603424072 | BCE Loss: 1.0400984287261963\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.091164588928223 | KNN Loss: 5.0493011474609375 | BCE Loss: 1.0418634414672852\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.0337300300598145 | KNN Loss: 5.005934238433838 | BCE Loss: 1.027795672416687\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.066177845001221 | KNN Loss: 5.012941360473633 | BCE Loss: 1.0532363653182983\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.038851261138916 | KNN Loss: 5.006023406982422 | BCE Loss: 1.0328278541564941\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.052763938903809 | KNN Loss: 5.017902851104736 | BCE Loss: 1.0348609685897827\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.069897174835205 | KNN Loss: 5.004063129425049 | BCE Loss: 1.0658339262008667\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.066554069519043 | KNN Loss: 5.022074222564697 | BCE Loss: 1.0444800853729248\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.051087856292725 | KNN Loss: 5.033838272094727 | BCE Loss: 1.0172494649887085\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.07528018951416 | KNN Loss: 5.03676700592041 | BCE Loss: 1.038513422012329\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.050601959228516 | KNN Loss: 4.997500419616699 | BCE Loss: 1.053101658821106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.100404739379883 | KNN Loss: 5.044919013977051 | BCE Loss: 1.0554859638214111\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.03847599029541 | KNN Loss: 4.996885299682617 | BCE Loss: 1.041590690612793\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.087399005889893 | KNN Loss: 5.049571514129639 | BCE Loss: 1.037827491760254\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.013916969299316 | KNN Loss: 4.998538494110107 | BCE Loss: 1.0153785943984985\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.03904914855957 | KNN Loss: 5.014948844909668 | BCE Loss: 1.0241000652313232\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.052900314331055 | KNN Loss: 5.003842353820801 | BCE Loss: 1.049057960510254\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.0802903175354 | KNN Loss: 5.021372318267822 | BCE Loss: 1.0589181184768677\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.082122802734375 | KNN Loss: 5.017178535461426 | BCE Loss: 1.0649445056915283\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.044382572174072 | KNN Loss: 5.01473331451416 | BCE Loss: 1.029649257659912\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.060188293457031 | KNN Loss: 5.004271030426025 | BCE Loss: 1.055917501449585\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.074281692504883 | KNN Loss: 5.03908634185791 | BCE Loss: 1.0351954698562622\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.036365985870361 | KNN Loss: 5.016434669494629 | BCE Loss: 1.0199313163757324\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.037814617156982 | KNN Loss: 5.019730091094971 | BCE Loss: 1.0180846452713013\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.063909530639648 | KNN Loss: 5.012519359588623 | BCE Loss: 1.051390290260315\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.0464019775390625 | KNN Loss: 5.008733749389648 | BCE Loss: 1.037668228149414\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.044663429260254 | KNN Loss: 4.998310089111328 | BCE Loss: 1.0463535785675049\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.048859596252441 | KNN Loss: 5.024672508239746 | BCE Loss: 1.0241872072219849\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.033709526062012 | KNN Loss: 5.0136919021606445 | BCE Loss: 1.0200175046920776\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.034483909606934 | KNN Loss: 5.024319171905518 | BCE Loss: 1.0101648569107056\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.030698776245117 | KNN Loss: 5.0069580078125 | BCE Loss: 1.0237406492233276\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.038481712341309 | KNN Loss: 5.00424861907959 | BCE Loss: 1.0342329740524292\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.029906749725342 | KNN Loss: 5.003572463989258 | BCE Loss: 1.026334285736084\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.053365230560303 | KNN Loss: 5.003287315368652 | BCE Loss: 1.05007803440094\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.030801296234131 | KNN Loss: 5.0186567306518555 | BCE Loss: 1.012144684791565\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.052876949310303 | KNN Loss: 5.034532070159912 | BCE Loss: 1.0183449983596802\n",
      "Epoch   240: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.0882887840271 | KNN Loss: 5.030676364898682 | BCE Loss: 1.057612419128418\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.053312301635742 | KNN Loss: 5.030828475952148 | BCE Loss: 1.0224837064743042\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.036426544189453 | KNN Loss: 5.018309593200684 | BCE Loss: 1.0181169509887695\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.063642501831055 | KNN Loss: 5.012686729431152 | BCE Loss: 1.050955891609192\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.037337779998779 | KNN Loss: 5.003916263580322 | BCE Loss: 1.033421516418457\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.05671501159668 | KNN Loss: 5.020438194274902 | BCE Loss: 1.036276936531067\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.033778190612793 | KNN Loss: 5.0100297927856445 | BCE Loss: 1.0237486362457275\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.050438404083252 | KNN Loss: 5.014294147491455 | BCE Loss: 1.0361441373825073\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.072402000427246 | KNN Loss: 5.016687393188477 | BCE Loss: 1.05571448802948\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.073172569274902 | KNN Loss: 5.037964344024658 | BCE Loss: 1.035207986831665\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.053312301635742 | KNN Loss: 5.041755676269531 | BCE Loss: 1.0115563869476318\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.064117908477783 | KNN Loss: 5.041631698608398 | BCE Loss: 1.0224862098693848\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.075521469116211 | KNN Loss: 5.020280838012695 | BCE Loss: 1.0552403926849365\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.057724952697754 | KNN Loss: 5.029440402984619 | BCE Loss: 1.0282847881317139\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.037095069885254 | KNN Loss: 5.022769927978516 | BCE Loss: 1.0143253803253174\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.025298118591309 | KNN Loss: 5.042328834533691 | BCE Loss: 0.9829690456390381\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.061556816101074 | KNN Loss: 5.013317108154297 | BCE Loss: 1.0482394695281982\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.043334007263184 | KNN Loss: 5.0112996101379395 | BCE Loss: 1.0320345163345337\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.051528453826904 | KNN Loss: 4.998075008392334 | BCE Loss: 1.0534534454345703\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.009523868560791 | KNN Loss: 5.009835243225098 | BCE Loss: 0.9996887445449829\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.05786657333374 | KNN Loss: 5.0359721183776855 | BCE Loss: 1.0218943357467651\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.0385847091674805 | KNN Loss: 5.002596855163574 | BCE Loss: 1.0359880924224854\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.040471076965332 | KNN Loss: 5.0131025314331055 | BCE Loss: 1.0273687839508057\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.030972480773926 | KNN Loss: 5.02083158493042 | BCE Loss: 1.0101408958435059\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.053734302520752 | KNN Loss: 5.010302543640137 | BCE Loss: 1.0434317588806152\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.070460319519043 | KNN Loss: 5.008937358856201 | BCE Loss: 1.0615227222442627\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.0492143630981445 | KNN Loss: 5.009637832641602 | BCE Loss: 1.039576530456543\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.037761211395264 | KNN Loss: 5.003171443939209 | BCE Loss: 1.0345897674560547\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.087132453918457 | KNN Loss: 5.063674449920654 | BCE Loss: 1.0234581232070923\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.045154571533203 | KNN Loss: 5.019715785980225 | BCE Loss: 1.0254385471343994\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.084872722625732 | KNN Loss: 5.031414031982422 | BCE Loss: 1.0534586906433105\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.083144187927246 | KNN Loss: 5.030421257019043 | BCE Loss: 1.0527228116989136\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.051821231842041 | KNN Loss: 5.0007758140563965 | BCE Loss: 1.0510454177856445\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.091344356536865 | KNN Loss: 5.068500995635986 | BCE Loss: 1.0228434801101685\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.080337047576904 | KNN Loss: 5.008503437042236 | BCE Loss: 1.0718334913253784\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.051451206207275 | KNN Loss: 5.006620407104492 | BCE Loss: 1.0448307991027832\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.043867111206055 | KNN Loss: 5.02831506729126 | BCE Loss: 1.0155521631240845\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.030341148376465 | KNN Loss: 5.011085033416748 | BCE Loss: 1.0192561149597168\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.029284477233887 | KNN Loss: 5.001744747161865 | BCE Loss: 1.0275397300720215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.076076507568359 | KNN Loss: 5.017348766326904 | BCE Loss: 1.058727502822876\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.054228782653809 | KNN Loss: 5.003281593322754 | BCE Loss: 1.0509474277496338\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.058218479156494 | KNN Loss: 5.0096049308776855 | BCE Loss: 1.0486135482788086\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.064805030822754 | KNN Loss: 5.00779390335083 | BCE Loss: 1.057011365890503\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.037850856781006 | KNN Loss: 5.013367652893066 | BCE Loss: 1.024483323097229\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.062819480895996 | KNN Loss: 5.0047831535339355 | BCE Loss: 1.0580365657806396\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.045943260192871 | KNN Loss: 5.017796039581299 | BCE Loss: 1.0281474590301514\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.049020767211914 | KNN Loss: 5.00939416885376 | BCE Loss: 1.0396265983581543\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.032451152801514 | KNN Loss: 5.008932590484619 | BCE Loss: 1.0235185623168945\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.056949615478516 | KNN Loss: 5.016418933868408 | BCE Loss: 1.0405306816101074\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.058045387268066 | KNN Loss: 5.009062767028809 | BCE Loss: 1.0489827394485474\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.055373191833496 | KNN Loss: 5.015016555786133 | BCE Loss: 1.0403563976287842\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.033229827880859 | KNN Loss: 5.01194429397583 | BCE Loss: 1.0212852954864502\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.041015625 | KNN Loss: 4.996727466583252 | BCE Loss: 1.0442883968353271\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.058368682861328 | KNN Loss: 5.018433570861816 | BCE Loss: 1.0399353504180908\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.030814170837402 | KNN Loss: 5.013614177703857 | BCE Loss: 1.0171998739242554\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.076128005981445 | KNN Loss: 5.0294060707092285 | BCE Loss: 1.0467218160629272\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.040509223937988 | KNN Loss: 5.007726669311523 | BCE Loss: 1.032782793045044\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.06477165222168 | KNN Loss: 5.0312042236328125 | BCE Loss: 1.0335674285888672\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.0376505851745605 | KNN Loss: 5.004873752593994 | BCE Loss: 1.0327768325805664\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.098501205444336 | KNN Loss: 5.019392967224121 | BCE Loss: 1.0791082382202148\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.027637481689453 | KNN Loss: 5.031065940856934 | BCE Loss: 0.9965717792510986\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.0454559326171875 | KNN Loss: 5.0159125328063965 | BCE Loss: 1.029543399810791\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.050009250640869 | KNN Loss: 5.015822410583496 | BCE Loss: 1.034186840057373\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.074855804443359 | KNN Loss: 5.0463948249816895 | BCE Loss: 1.0284607410430908\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.052283763885498 | KNN Loss: 5.017347812652588 | BCE Loss: 1.0349360704421997\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.032919883728027 | KNN Loss: 4.995689868927002 | BCE Loss: 1.0372298955917358\n",
      "Epoch   251: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.02026891708374 | KNN Loss: 4.99844217300415 | BCE Loss: 1.0218266248703003\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.057520866394043 | KNN Loss: 5.003742218017578 | BCE Loss: 1.0537786483764648\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.0102643966674805 | KNN Loss: 5.00703239440918 | BCE Loss: 1.0032320022583008\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.086601734161377 | KNN Loss: 5.031203746795654 | BCE Loss: 1.0553979873657227\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.1019206047058105 | KNN Loss: 5.023128032684326 | BCE Loss: 1.0787924528121948\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.098060607910156 | KNN Loss: 5.045781135559082 | BCE Loss: 1.0522797107696533\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.035715103149414 | KNN Loss: 5.013499736785889 | BCE Loss: 1.0222156047821045\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.05930233001709 | KNN Loss: 5.03922700881958 | BCE Loss: 1.0200753211975098\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.042446613311768 | KNN Loss: 5.004751682281494 | BCE Loss: 1.037695050239563\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.052497863769531 | KNN Loss: 5.019801139831543 | BCE Loss: 1.0326969623565674\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.093777656555176 | KNN Loss: 5.0559983253479 | BCE Loss: 1.0377793312072754\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.075299263000488 | KNN Loss: 5.0069379806518555 | BCE Loss: 1.0683611631393433\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.096323013305664 | KNN Loss: 5.031562328338623 | BCE Loss: 1.064760684967041\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.0833001136779785 | KNN Loss: 5.044719696044922 | BCE Loss: 1.038580298423767\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.0632524490356445 | KNN Loss: 5.01566219329834 | BCE Loss: 1.0475904941558838\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.047057151794434 | KNN Loss: 5.015646457672119 | BCE Loss: 1.031410574913025\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.040774345397949 | KNN Loss: 5.015561580657959 | BCE Loss: 1.0252130031585693\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.042858123779297 | KNN Loss: 5.002535343170166 | BCE Loss: 1.04032301902771\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.0632710456848145 | KNN Loss: 5.013556957244873 | BCE Loss: 1.0497140884399414\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.045922756195068 | KNN Loss: 5.002991199493408 | BCE Loss: 1.0429316759109497\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.092679977416992 | KNN Loss: 5.061896800994873 | BCE Loss: 1.03078293800354\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.057427406311035 | KNN Loss: 5.024195671081543 | BCE Loss: 1.033231496810913\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.144679069519043 | KNN Loss: 5.097654819488525 | BCE Loss: 1.0470240116119385\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.049217700958252 | KNN Loss: 5.0153069496154785 | BCE Loss: 1.0339106321334839\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.0386271476745605 | KNN Loss: 5.0068440437316895 | BCE Loss: 1.031783103942871\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.051685333251953 | KNN Loss: 5.039044380187988 | BCE Loss: 1.012641191482544\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.043251037597656 | KNN Loss: 5.001117706298828 | BCE Loss: 1.0421335697174072\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.040106296539307 | KNN Loss: 5.007011413574219 | BCE Loss: 1.033094882965088\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.051351070404053 | KNN Loss: 5.026132106781006 | BCE Loss: 1.0252189636230469\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.049421310424805 | KNN Loss: 5.005071640014648 | BCE Loss: 1.0443499088287354\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.020711421966553 | KNN Loss: 5.005012512207031 | BCE Loss: 1.0156989097595215\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.054139137268066 | KNN Loss: 5.0080342292785645 | BCE Loss: 1.046104907989502\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.0392165184021 | KNN Loss: 5.011296272277832 | BCE Loss: 1.0279202461242676\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.048626899719238 | KNN Loss: 5.0075812339782715 | BCE Loss: 1.0410454273223877\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.061362266540527 | KNN Loss: 5.007896423339844 | BCE Loss: 1.0534656047821045\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.0821919441223145 | KNN Loss: 5.058853626251221 | BCE Loss: 1.0233383178710938\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.030790328979492 | KNN Loss: 4.999477386474609 | BCE Loss: 1.0313128232955933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.0404276847839355 | KNN Loss: 5.000545978546143 | BCE Loss: 1.039881706237793\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.00697660446167 | KNN Loss: 5.000911235809326 | BCE Loss: 1.0060654878616333\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.116912841796875 | KNN Loss: 5.053160667419434 | BCE Loss: 1.0637521743774414\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.043320655822754 | KNN Loss: 5.029742240905762 | BCE Loss: 1.0135784149169922\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.0607075691223145 | KNN Loss: 5.013584136962891 | BCE Loss: 1.0471233129501343\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.0650248527526855 | KNN Loss: 5.014961242675781 | BCE Loss: 1.0500637292861938\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.068080902099609 | KNN Loss: 5.01578950881958 | BCE Loss: 1.0522913932800293\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.0836005210876465 | KNN Loss: 5.024886608123779 | BCE Loss: 1.0587140321731567\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.066292762756348 | KNN Loss: 5.030194282531738 | BCE Loss: 1.0360984802246094\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.055776119232178 | KNN Loss: 5.009037971496582 | BCE Loss: 1.0467380285263062\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.070904731750488 | KNN Loss: 5.010020732879639 | BCE Loss: 1.0608837604522705\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.037835121154785 | KNN Loss: 5.014151096343994 | BCE Loss: 1.023683786392212\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.0717692375183105 | KNN Loss: 5.047591209411621 | BCE Loss: 1.0241779088974\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.044474124908447 | KNN Loss: 5.010059833526611 | BCE Loss: 1.034414291381836\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.014518737792969 | KNN Loss: 5.0149970054626465 | BCE Loss: 0.9995219707489014\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.084939002990723 | KNN Loss: 5.062580585479736 | BCE Loss: 1.0223584175109863\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.036435127258301 | KNN Loss: 5.006232261657715 | BCE Loss: 1.030202865600586\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.065995693206787 | KNN Loss: 5.026941776275635 | BCE Loss: 1.039054036140442\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.0626420974731445 | KNN Loss: 5.0084638595581055 | BCE Loss: 1.0541781187057495\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.0593109130859375 | KNN Loss: 5.00816535949707 | BCE Loss: 1.0511457920074463\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.037751197814941 | KNN Loss: 4.998776435852051 | BCE Loss: 1.0389747619628906\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.066319465637207 | KNN Loss: 5.017146587371826 | BCE Loss: 1.04917311668396\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.059764385223389 | KNN Loss: 5.035101413726807 | BCE Loss: 1.0246628522872925\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.0824079513549805 | KNN Loss: 5.038217544555664 | BCE Loss: 1.0441902875900269\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.055716514587402 | KNN Loss: 5.00434684753418 | BCE Loss: 1.0513696670532227\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.05071496963501 | KNN Loss: 5.029475688934326 | BCE Loss: 1.0212392807006836\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.097716808319092 | KNN Loss: 5.032063961029053 | BCE Loss: 1.0656529664993286\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.064544677734375 | KNN Loss: 5.02894926071167 | BCE Loss: 1.035595178604126\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.060538291931152 | KNN Loss: 5.016075611114502 | BCE Loss: 1.0444626808166504\n",
      "Epoch   262: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.030186176300049 | KNN Loss: 5.01748514175415 | BCE Loss: 1.0127010345458984\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.044957160949707 | KNN Loss: 5.00668478012085 | BCE Loss: 1.0382726192474365\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.039607048034668 | KNN Loss: 4.9993577003479 | BCE Loss: 1.0402491092681885\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.0521745681762695 | KNN Loss: 5.01663875579834 | BCE Loss: 1.0355358123779297\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.060830116271973 | KNN Loss: 5.012850284576416 | BCE Loss: 1.0479795932769775\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.0631303787231445 | KNN Loss: 5.019192695617676 | BCE Loss: 1.0439374446868896\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.010568618774414 | KNN Loss: 5.007270336151123 | BCE Loss: 1.0032984018325806\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.013487339019775 | KNN Loss: 5.00870418548584 | BCE Loss: 1.004783034324646\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.0701470375061035 | KNN Loss: 5.011338710784912 | BCE Loss: 1.0588082075119019\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.054844856262207 | KNN Loss: 5.010160446166992 | BCE Loss: 1.0446844100952148\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.044442176818848 | KNN Loss: 5.026754379272461 | BCE Loss: 1.0176875591278076\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.051469326019287 | KNN Loss: 5.004554748535156 | BCE Loss: 1.0469144582748413\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.088632583618164 | KNN Loss: 5.0500311851501465 | BCE Loss: 1.0386013984680176\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.055074214935303 | KNN Loss: 5.032496452331543 | BCE Loss: 1.0225776433944702\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.074061393737793 | KNN Loss: 5.0237717628479 | BCE Loss: 1.0502898693084717\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.097822189331055 | KNN Loss: 5.0274152755737305 | BCE Loss: 1.0704070329666138\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.0368194580078125 | KNN Loss: 5.009072303771973 | BCE Loss: 1.0277472734451294\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.047835350036621 | KNN Loss: 5.025534152984619 | BCE Loss: 1.022301197052002\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.056156158447266 | KNN Loss: 5.012104511260986 | BCE Loss: 1.0440516471862793\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.048521041870117 | KNN Loss: 5.034017562866211 | BCE Loss: 1.0145034790039062\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.067505836486816 | KNN Loss: 5.010850429534912 | BCE Loss: 1.0566556453704834\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.044059753417969 | KNN Loss: 4.994410037994385 | BCE Loss: 1.0496494770050049\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.077976703643799 | KNN Loss: 5.02979850769043 | BCE Loss: 1.0481781959533691\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.055976390838623 | KNN Loss: 5.014126777648926 | BCE Loss: 1.0418497323989868\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.061795234680176 | KNN Loss: 5.013490676879883 | BCE Loss: 1.048304796218872\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.061920166015625 | KNN Loss: 5.014884948730469 | BCE Loss: 1.0470349788665771\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.045289516448975 | KNN Loss: 5.005767345428467 | BCE Loss: 1.0395220518112183\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.031886100769043 | KNN Loss: 5.01588773727417 | BCE Loss: 1.0159984827041626\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.0305070877075195 | KNN Loss: 5.009332180023193 | BCE Loss: 1.0211749076843262\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.072485446929932 | KNN Loss: 5.032983779907227 | BCE Loss: 1.0395015478134155\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.029993057250977 | KNN Loss: 4.999696731567383 | BCE Loss: 1.0302960872650146\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.048921585083008 | KNN Loss: 5.011472225189209 | BCE Loss: 1.0374492406845093\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.094316482543945 | KNN Loss: 5.0337233543396 | BCE Loss: 1.0605931282043457\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.075438976287842 | KNN Loss: 5.030235767364502 | BCE Loss: 1.0452033281326294\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.08249568939209 | KNN Loss: 5.023100852966309 | BCE Loss: 1.0593950748443604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.04449462890625 | KNN Loss: 5.001646518707275 | BCE Loss: 1.0428483486175537\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.021522045135498 | KNN Loss: 5.005043983459473 | BCE Loss: 1.0164779424667358\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.012213706970215 | KNN Loss: 5.006466865539551 | BCE Loss: 1.0057470798492432\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.079848289489746 | KNN Loss: 5.036489963531494 | BCE Loss: 1.043358564376831\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.043050289154053 | KNN Loss: 5.009645462036133 | BCE Loss: 1.03340482711792\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.0369133949279785 | KNN Loss: 5.024896621704102 | BCE Loss: 1.0120166540145874\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.0317463874816895 | KNN Loss: 5.00429630279541 | BCE Loss: 1.0274499654769897\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.057718276977539 | KNN Loss: 5.016977310180664 | BCE Loss: 1.040741205215454\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.070259094238281 | KNN Loss: 5.01625394821167 | BCE Loss: 1.0540053844451904\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.06169319152832 | KNN Loss: 5.020364284515381 | BCE Loss: 1.041329026222229\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.038706302642822 | KNN Loss: 5.009759902954102 | BCE Loss: 1.0289465188980103\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.041131019592285 | KNN Loss: 4.992916584014893 | BCE Loss: 1.0482141971588135\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.041412353515625 | KNN Loss: 5.0207719802856445 | BCE Loss: 1.020640254020691\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.035813808441162 | KNN Loss: 5.022958755493164 | BCE Loss: 1.012855052947998\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.111567497253418 | KNN Loss: 5.056236267089844 | BCE Loss: 1.0553312301635742\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.048135757446289 | KNN Loss: 5.013108253479004 | BCE Loss: 1.0350277423858643\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.044649124145508 | KNN Loss: 5.004519939422607 | BCE Loss: 1.0401294231414795\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.0670671463012695 | KNN Loss: 5.009167194366455 | BCE Loss: 1.0578997135162354\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.0412397384643555 | KNN Loss: 5.011867523193359 | BCE Loss: 1.029372215270996\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.091368675231934 | KNN Loss: 5.050321102142334 | BCE Loss: 1.0410473346710205\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.0585527420043945 | KNN Loss: 5.024670600891113 | BCE Loss: 1.0338823795318604\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.0715742111206055 | KNN Loss: 5.044548988342285 | BCE Loss: 1.0270253419876099\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.041369438171387 | KNN Loss: 5.020576477050781 | BCE Loss: 1.020792841911316\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.037724494934082 | KNN Loss: 5.006711483001709 | BCE Loss: 1.0310132503509521\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.0733137130737305 | KNN Loss: 5.007134437561035 | BCE Loss: 1.0661793947219849\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.031770706176758 | KNN Loss: 5.014800071716309 | BCE Loss: 1.0169706344604492\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.022918224334717 | KNN Loss: 5.002615451812744 | BCE Loss: 1.0203027725219727\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.084407806396484 | KNN Loss: 5.033519268035889 | BCE Loss: 1.0508885383605957\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.065328121185303 | KNN Loss: 5.010029315948486 | BCE Loss: 1.0552988052368164\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.047210693359375 | KNN Loss: 5.01143217086792 | BCE Loss: 1.035778284072876\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.074856758117676 | KNN Loss: 5.026884078979492 | BCE Loss: 1.0479726791381836\n",
      "Epoch   273: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.078855514526367 | KNN Loss: 5.026395320892334 | BCE Loss: 1.052459955215454\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.050115585327148 | KNN Loss: 5.012551307678223 | BCE Loss: 1.0375640392303467\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.071448802947998 | KNN Loss: 5.036644458770752 | BCE Loss: 1.0348042249679565\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.035881996154785 | KNN Loss: 5.0052409172058105 | BCE Loss: 1.0306410789489746\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.013814926147461 | KNN Loss: 4.9988837242126465 | BCE Loss: 1.0149312019348145\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.040834426879883 | KNN Loss: 5.005034446716309 | BCE Loss: 1.0357999801635742\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.071685791015625 | KNN Loss: 4.995059013366699 | BCE Loss: 1.0766267776489258\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.051391124725342 | KNN Loss: 5.038127899169922 | BCE Loss: 1.01326322555542\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.040859222412109 | KNN Loss: 5.0277910232543945 | BCE Loss: 1.0130681991577148\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.078969478607178 | KNN Loss: 4.996405124664307 | BCE Loss: 1.0825644731521606\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.018713474273682 | KNN Loss: 5.0091705322265625 | BCE Loss: 1.0095430612564087\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.062248706817627 | KNN Loss: 5.048859596252441 | BCE Loss: 1.013389229774475\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.050016403198242 | KNN Loss: 5.031662940979004 | BCE Loss: 1.0183533430099487\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.11480188369751 | KNN Loss: 5.032675743103027 | BCE Loss: 1.0821261405944824\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.08784818649292 | KNN Loss: 5.063682556152344 | BCE Loss: 1.0241657495498657\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.025363922119141 | KNN Loss: 5.014471054077148 | BCE Loss: 1.0108928680419922\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.04339075088501 | KNN Loss: 5.0081634521484375 | BCE Loss: 1.0352274179458618\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.03226375579834 | KNN Loss: 4.997750282287598 | BCE Loss: 1.0345135927200317\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.0750603675842285 | KNN Loss: 5.022513389587402 | BCE Loss: 1.0525468587875366\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.078612804412842 | KNN Loss: 5.040319919586182 | BCE Loss: 1.0382928848266602\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.064657211303711 | KNN Loss: 5.023099422454834 | BCE Loss: 1.041557788848877\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.042922019958496 | KNN Loss: 5.022877216339111 | BCE Loss: 1.0200445652008057\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.015762805938721 | KNN Loss: 5.018579959869385 | BCE Loss: 0.9971828460693359\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.0769147872924805 | KNN Loss: 5.022877216339111 | BCE Loss: 1.0540378093719482\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.03012752532959 | KNN Loss: 5.019984245300293 | BCE Loss: 1.0101430416107178\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.060356140136719 | KNN Loss: 5.02733039855957 | BCE Loss: 1.0330259799957275\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.077520847320557 | KNN Loss: 5.027861595153809 | BCE Loss: 1.0496593713760376\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.055812835693359 | KNN Loss: 5.004025936126709 | BCE Loss: 1.0517871379852295\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.047163963317871 | KNN Loss: 5.017970561981201 | BCE Loss: 1.0291932821273804\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.046936988830566 | KNN Loss: 5.008763313293457 | BCE Loss: 1.0381734371185303\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.103298187255859 | KNN Loss: 5.0523762702941895 | BCE Loss: 1.05092191696167\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.0553483963012695 | KNN Loss: 5.011721134185791 | BCE Loss: 1.043627381324768\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.033712863922119 | KNN Loss: 5.018792629241943 | BCE Loss: 1.0149203538894653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.031328201293945 | KNN Loss: 5.010830402374268 | BCE Loss: 1.0204976797103882\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.067998886108398 | KNN Loss: 5.040978908538818 | BCE Loss: 1.0270200967788696\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.113465785980225 | KNN Loss: 5.053899765014648 | BCE Loss: 1.0595660209655762\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.045914173126221 | KNN Loss: 4.998716831207275 | BCE Loss: 1.0471973419189453\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.039690017700195 | KNN Loss: 5.006494045257568 | BCE Loss: 1.033195972442627\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.049155235290527 | KNN Loss: 5.013298988342285 | BCE Loss: 1.0358562469482422\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.065033912658691 | KNN Loss: 5.043952941894531 | BCE Loss: 1.0210812091827393\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.081622123718262 | KNN Loss: 5.042003631591797 | BCE Loss: 1.0396184921264648\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.046286582946777 | KNN Loss: 5.013247489929199 | BCE Loss: 1.0330389738082886\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.071436882019043 | KNN Loss: 5.068676471710205 | BCE Loss: 1.002760648727417\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.034817695617676 | KNN Loss: 5.00914192199707 | BCE Loss: 1.0256757736206055\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.045856475830078 | KNN Loss: 5.0076985359191895 | BCE Loss: 1.0381579399108887\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.026003837585449 | KNN Loss: 5.004321098327637 | BCE Loss: 1.0216827392578125\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.039880275726318 | KNN Loss: 5.000964641571045 | BCE Loss: 1.0389156341552734\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.031643390655518 | KNN Loss: 5.004048824310303 | BCE Loss: 1.0275944471359253\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.067503929138184 | KNN Loss: 5.016434192657471 | BCE Loss: 1.051069736480713\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.094537734985352 | KNN Loss: 5.00726842880249 | BCE Loss: 1.0872695446014404\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.095102787017822 | KNN Loss: 5.049665927886963 | BCE Loss: 1.045436978340149\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.101626396179199 | KNN Loss: 5.050279140472412 | BCE Loss: 1.0513474941253662\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.019359588623047 | KNN Loss: 5.015774726867676 | BCE Loss: 1.003584623336792\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.065619468688965 | KNN Loss: 5.040166854858398 | BCE Loss: 1.0254526138305664\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.068861961364746 | KNN Loss: 5.036869049072266 | BCE Loss: 1.0319926738739014\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.022408962249756 | KNN Loss: 5.003195285797119 | BCE Loss: 1.0192136764526367\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.009809494018555 | KNN Loss: 5.001453876495361 | BCE Loss: 1.0083556175231934\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.019359588623047 | KNN Loss: 5.009073734283447 | BCE Loss: 1.01028573513031\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.068753719329834 | KNN Loss: 5.037851810455322 | BCE Loss: 1.0309019088745117\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.017780780792236 | KNN Loss: 5.010701656341553 | BCE Loss: 1.0070791244506836\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.071707248687744 | KNN Loss: 5.007418632507324 | BCE Loss: 1.06428861618042\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.0907135009765625 | KNN Loss: 5.043574333190918 | BCE Loss: 1.0471389293670654\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.038789749145508 | KNN Loss: 5.008272647857666 | BCE Loss: 1.0305171012878418\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.043420791625977 | KNN Loss: 5.020240783691406 | BCE Loss: 1.0231802463531494\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.089117050170898 | KNN Loss: 5.054132461547852 | BCE Loss: 1.0349845886230469\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.082768440246582 | KNN Loss: 5.046738147735596 | BCE Loss: 1.0360305309295654\n",
      "Epoch   284: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.082160949707031 | KNN Loss: 5.004809379577637 | BCE Loss: 1.0773518085479736\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.051726341247559 | KNN Loss: 5.018139362335205 | BCE Loss: 1.033586859703064\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.03404426574707 | KNN Loss: 5.004656791687012 | BCE Loss: 1.0293872356414795\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.074695110321045 | KNN Loss: 5.029085159301758 | BCE Loss: 1.0456098318099976\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.032162189483643 | KNN Loss: 5.0248188972473145 | BCE Loss: 1.0073432922363281\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.015633583068848 | KNN Loss: 5.003641128540039 | BCE Loss: 1.0119925737380981\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.064476013183594 | KNN Loss: 5.0127081871032715 | BCE Loss: 1.0517677068710327\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.061199188232422 | KNN Loss: 5.0176682472229 | BCE Loss: 1.0435309410095215\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.062516212463379 | KNN Loss: 5.018134117126465 | BCE Loss: 1.044382095336914\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.063475608825684 | KNN Loss: 5.038970947265625 | BCE Loss: 1.0245044231414795\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.0216875076293945 | KNN Loss: 5.00444221496582 | BCE Loss: 1.0172455310821533\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.0276312828063965 | KNN Loss: 5.021866798400879 | BCE Loss: 1.0057644844055176\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.050062656402588 | KNN Loss: 5.020017147064209 | BCE Loss: 1.030045509338379\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.086516857147217 | KNN Loss: 5.037022590637207 | BCE Loss: 1.0494942665100098\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.06376838684082 | KNN Loss: 5.0072021484375 | BCE Loss: 1.0565659999847412\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.100374698638916 | KNN Loss: 5.011131286621094 | BCE Loss: 1.0892435312271118\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.051517486572266 | KNN Loss: 5.010802745819092 | BCE Loss: 1.0407145023345947\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.041626453399658 | KNN Loss: 4.9997663497924805 | BCE Loss: 1.0418601036071777\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.065092086791992 | KNN Loss: 5.011563301086426 | BCE Loss: 1.0535287857055664\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.098569869995117 | KNN Loss: 5.058382511138916 | BCE Loss: 1.0401873588562012\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.039862632751465 | KNN Loss: 4.994297981262207 | BCE Loss: 1.0455644130706787\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.065356254577637 | KNN Loss: 5.025582313537598 | BCE Loss: 1.03977370262146\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.0388288497924805 | KNN Loss: 5.013570308685303 | BCE Loss: 1.0252583026885986\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.0685834884643555 | KNN Loss: 5.00698184967041 | BCE Loss: 1.0616014003753662\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.145081996917725 | KNN Loss: 5.101058006286621 | BCE Loss: 1.0440239906311035\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.082644462585449 | KNN Loss: 5.058614253997803 | BCE Loss: 1.024030327796936\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.034760475158691 | KNN Loss: 5.006021976470947 | BCE Loss: 1.0287383794784546\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.073530197143555 | KNN Loss: 5.020405292510986 | BCE Loss: 1.0531249046325684\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.012204647064209 | KNN Loss: 4.997727394104004 | BCE Loss: 1.014477252960205\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.020357131958008 | KNN Loss: 5.0087127685546875 | BCE Loss: 1.0116441249847412\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.059243679046631 | KNN Loss: 5.0083184242248535 | BCE Loss: 1.0509252548217773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.084571838378906 | KNN Loss: 5.0586628913879395 | BCE Loss: 1.025909185409546\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.065525054931641 | KNN Loss: 5.025134563446045 | BCE Loss: 1.0403902530670166\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.049985408782959 | KNN Loss: 5.028318881988525 | BCE Loss: 1.0216666460037231\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.048116207122803 | KNN Loss: 5.0104241371154785 | BCE Loss: 1.0376920700073242\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.03642463684082 | KNN Loss: 5.006043434143066 | BCE Loss: 1.0303810834884644\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.052399635314941 | KNN Loss: 5.0444865226745605 | BCE Loss: 1.0079128742218018\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.075341701507568 | KNN Loss: 5.013823509216309 | BCE Loss: 1.0615183115005493\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.096243858337402 | KNN Loss: 5.049172401428223 | BCE Loss: 1.0470714569091797\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.0511345863342285 | KNN Loss: 5.011162281036377 | BCE Loss: 1.039972186088562\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.129076957702637 | KNN Loss: 5.0663604736328125 | BCE Loss: 1.0627164840698242\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.0288238525390625 | KNN Loss: 5.018954753875732 | BCE Loss: 1.00986909866333\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.118449687957764 | KNN Loss: 5.062084674835205 | BCE Loss: 1.0563650131225586\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.072774410247803 | KNN Loss: 5.008651256561279 | BCE Loss: 1.0641231536865234\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.0780930519104 | KNN Loss: 5.028773784637451 | BCE Loss: 1.0493191480636597\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.0552897453308105 | KNN Loss: 5.013480186462402 | BCE Loss: 1.0418096780776978\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.064902305603027 | KNN Loss: 5.032809257507324 | BCE Loss: 1.0320932865142822\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.015220642089844 | KNN Loss: 5.001394271850586 | BCE Loss: 1.0138264894485474\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.048740386962891 | KNN Loss: 5.004657745361328 | BCE Loss: 1.0440828800201416\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.03166389465332 | KNN Loss: 5.027738571166992 | BCE Loss: 1.0039253234863281\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.057782173156738 | KNN Loss: 5.035568714141846 | BCE Loss: 1.022213339805603\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.075829982757568 | KNN Loss: 5.016899585723877 | BCE Loss: 1.0589302778244019\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.058465957641602 | KNN Loss: 5.016302108764648 | BCE Loss: 1.0421640872955322\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.080740451812744 | KNN Loss: 5.01494836807251 | BCE Loss: 1.0657920837402344\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.044194221496582 | KNN Loss: 5.0241169929504395 | BCE Loss: 1.0200769901275635\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.047013282775879 | KNN Loss: 5.020944118499756 | BCE Loss: 1.0260692834854126\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.027828216552734 | KNN Loss: 4.997709274291992 | BCE Loss: 1.030118703842163\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.044939994812012 | KNN Loss: 5.002001762390137 | BCE Loss: 1.042937994003296\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.0280961990356445 | KNN Loss: 5.014418601989746 | BCE Loss: 1.0136775970458984\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.040259838104248 | KNN Loss: 5.001874923706055 | BCE Loss: 1.0383847951889038\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.024411678314209 | KNN Loss: 5.001051902770996 | BCE Loss: 1.0233596563339233\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.113708972930908 | KNN Loss: 5.075909614562988 | BCE Loss: 1.03779935836792\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.059055328369141 | KNN Loss: 5.007938861846924 | BCE Loss: 1.0511164665222168\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.055730819702148 | KNN Loss: 5.0120649337768555 | BCE Loss: 1.043665885925293\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.020324230194092 | KNN Loss: 5.003716468811035 | BCE Loss: 1.0166078805923462\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.043213844299316 | KNN Loss: 5.023783206939697 | BCE Loss: 1.01943039894104\n",
      "Epoch   295: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.022377014160156 | KNN Loss: 4.999363422393799 | BCE Loss: 1.023013710975647\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.064423561096191 | KNN Loss: 5.0111870765686035 | BCE Loss: 1.053236722946167\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.038100242614746 | KNN Loss: 5.012405872344971 | BCE Loss: 1.0256946086883545\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.062444686889648 | KNN Loss: 5.0232110023498535 | BCE Loss: 1.0392335653305054\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.071352958679199 | KNN Loss: 5.008121967315674 | BCE Loss: 1.063231110572815\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.045995235443115 | KNN Loss: 5.0134968757629395 | BCE Loss: 1.0324982404708862\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.045960426330566 | KNN Loss: 5.009501934051514 | BCE Loss: 1.0364587306976318\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.103365898132324 | KNN Loss: 5.064721584320068 | BCE Loss: 1.0386440753936768\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.0506439208984375 | KNN Loss: 5.027731895446777 | BCE Loss: 1.022911787033081\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.00272798538208 | KNN Loss: 5.0108819007873535 | BCE Loss: 0.9918462038040161\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.050037860870361 | KNN Loss: 5.01177453994751 | BCE Loss: 1.0382634401321411\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.038181304931641 | KNN Loss: 5.006887912750244 | BCE Loss: 1.0312936305999756\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.054125785827637 | KNN Loss: 5.017613410949707 | BCE Loss: 1.0365121364593506\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.064240455627441 | KNN Loss: 5.035823822021484 | BCE Loss: 1.028416395187378\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.094746112823486 | KNN Loss: 5.040429592132568 | BCE Loss: 1.054316520690918\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.026451587677002 | KNN Loss: 5.008927822113037 | BCE Loss: 1.0175236463546753\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.051672458648682 | KNN Loss: 5.012053966522217 | BCE Loss: 1.0396186113357544\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.053999900817871 | KNN Loss: 5.0084733963012695 | BCE Loss: 1.0455262660980225\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.0325751304626465 | KNN Loss: 5.01993465423584 | BCE Loss: 1.0126404762268066\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.066789150238037 | KNN Loss: 5.039157390594482 | BCE Loss: 1.0276318788528442\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.019680500030518 | KNN Loss: 4.99873161315918 | BCE Loss: 1.0209490060806274\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.035462856292725 | KNN Loss: 4.99485969543457 | BCE Loss: 1.0406032800674438\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.104580879211426 | KNN Loss: 5.061609268188477 | BCE Loss: 1.0429713726043701\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.038731575012207 | KNN Loss: 5.022731781005859 | BCE Loss: 1.0159997940063477\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.070899963378906 | KNN Loss: 5.028764724731445 | BCE Loss: 1.042135238647461\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.058323860168457 | KNN Loss: 5.008893013000488 | BCE Loss: 1.0494306087493896\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.073971748352051 | KNN Loss: 5.044069290161133 | BCE Loss: 1.0299023389816284\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.03300666809082 | KNN Loss: 5.0139970779418945 | BCE Loss: 1.0190095901489258\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.025022506713867 | KNN Loss: 5.000917911529541 | BCE Loss: 1.0241048336029053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.061354637145996 | KNN Loss: 5.0059614181518555 | BCE Loss: 1.055393099784851\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.077587127685547 | KNN Loss: 5.029234886169434 | BCE Loss: 1.0483524799346924\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.0340070724487305 | KNN Loss: 5.02151346206665 | BCE Loss: 1.01249361038208\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.073529243469238 | KNN Loss: 5.00510835647583 | BCE Loss: 1.068420648574829\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.057488918304443 | KNN Loss: 5.019248008728027 | BCE Loss: 1.0382410287857056\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.062248229980469 | KNN Loss: 5.009426593780518 | BCE Loss: 1.0528218746185303\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.032425880432129 | KNN Loss: 5.002382755279541 | BCE Loss: 1.030043363571167\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.032160758972168 | KNN Loss: 5.00398588180542 | BCE Loss: 1.0281751155853271\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.064098834991455 | KNN Loss: 5.036895751953125 | BCE Loss: 1.02720308303833\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.046326160430908 | KNN Loss: 5.010857582092285 | BCE Loss: 1.0354684591293335\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.071257591247559 | KNN Loss: 5.045889377593994 | BCE Loss: 1.0253684520721436\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.062203884124756 | KNN Loss: 5.01564359664917 | BCE Loss: 1.0465604066848755\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.038036346435547 | KNN Loss: 5.007654190063477 | BCE Loss: 1.0303823947906494\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.061104774475098 | KNN Loss: 5.0105366706848145 | BCE Loss: 1.0505681037902832\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.131789684295654 | KNN Loss: 5.105128765106201 | BCE Loss: 1.0266609191894531\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.040929794311523 | KNN Loss: 5.010138988494873 | BCE Loss: 1.03079092502594\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.079485893249512 | KNN Loss: 5.019779205322266 | BCE Loss: 1.0597065687179565\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.062350273132324 | KNN Loss: 5.040274620056152 | BCE Loss: 1.0220754146575928\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.046731472015381 | KNN Loss: 5.006032943725586 | BCE Loss: 1.040698528289795\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.062723159790039 | KNN Loss: 5.024133205413818 | BCE Loss: 1.0385901927947998\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.018642902374268 | KNN Loss: 5.022279739379883 | BCE Loss: 0.9963630437850952\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.069375038146973 | KNN Loss: 5.012804985046387 | BCE Loss: 1.0565698146820068\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.0468645095825195 | KNN Loss: 5.013972282409668 | BCE Loss: 1.0328919887542725\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.058220863342285 | KNN Loss: 5.017516136169434 | BCE Loss: 1.0407047271728516\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.077186107635498 | KNN Loss: 5.009881973266602 | BCE Loss: 1.0673041343688965\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.031651020050049 | KNN Loss: 4.999200820922852 | BCE Loss: 1.0324500799179077\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.03474760055542 | KNN Loss: 5.015467166900635 | BCE Loss: 1.0192804336547852\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.04325008392334 | KNN Loss: 5.017335414886475 | BCE Loss: 1.0259149074554443\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.0401997566223145 | KNN Loss: 5.002678871154785 | BCE Loss: 1.0375207662582397\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.038150787353516 | KNN Loss: 5.01876163482666 | BCE Loss: 1.0193891525268555\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.084336280822754 | KNN Loss: 5.0510382652282715 | BCE Loss: 1.0332978963851929\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.037901878356934 | KNN Loss: 5.011579990386963 | BCE Loss: 1.0263216495513916\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.031073570251465 | KNN Loss: 5.012795448303223 | BCE Loss: 1.0182780027389526\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.071731090545654 | KNN Loss: 5.0194010734558105 | BCE Loss: 1.0523300170898438\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.073390483856201 | KNN Loss: 5.026782035827637 | BCE Loss: 1.046608567237854\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.082277774810791 | KNN Loss: 5.042713642120361 | BCE Loss: 1.0395641326904297\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.013824462890625 | KNN Loss: 5.007310390472412 | BCE Loss: 1.0065139532089233\n",
      "Epoch   306: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.091553688049316 | KNN Loss: 5.015038967132568 | BCE Loss: 1.076514720916748\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.075798034667969 | KNN Loss: 5.013340473175049 | BCE Loss: 1.0624573230743408\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.034786224365234 | KNN Loss: 5.002155780792236 | BCE Loss: 1.0326303243637085\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.0615997314453125 | KNN Loss: 5.03419828414917 | BCE Loss: 1.0274016857147217\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.088850021362305 | KNN Loss: 5.078627586364746 | BCE Loss: 1.0102224349975586\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.055960655212402 | KNN Loss: 5.017236232757568 | BCE Loss: 1.038724660873413\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.088141441345215 | KNN Loss: 5.021882057189941 | BCE Loss: 1.0662591457366943\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.062357425689697 | KNN Loss: 5.023442268371582 | BCE Loss: 1.0389150381088257\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.076992988586426 | KNN Loss: 5.009758949279785 | BCE Loss: 1.0672338008880615\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.092044830322266 | KNN Loss: 5.044932842254639 | BCE Loss: 1.047111988067627\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.038481712341309 | KNN Loss: 5.008309841156006 | BCE Loss: 1.0301717519760132\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.04649543762207 | KNN Loss: 5.000998497009277 | BCE Loss: 1.0454968214035034\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.107034683227539 | KNN Loss: 5.022880554199219 | BCE Loss: 1.0841541290283203\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.069490432739258 | KNN Loss: 5.017613887786865 | BCE Loss: 1.0518765449523926\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.022616386413574 | KNN Loss: 5.021359443664551 | BCE Loss: 1.0012571811676025\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.045257568359375 | KNN Loss: 5.005035400390625 | BCE Loss: 1.0402220487594604\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.060572147369385 | KNN Loss: 5.0060014724731445 | BCE Loss: 1.0545707941055298\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.02773380279541 | KNN Loss: 4.993415832519531 | BCE Loss: 1.034318208694458\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.173879146575928 | KNN Loss: 5.10499906539917 | BCE Loss: 1.0688800811767578\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.063987731933594 | KNN Loss: 5.048266887664795 | BCE Loss: 1.0157207250595093\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.105372428894043 | KNN Loss: 5.044177532196045 | BCE Loss: 1.061194658279419\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.019687652587891 | KNN Loss: 5.019617557525635 | BCE Loss: 1.0000700950622559\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.081914901733398 | KNN Loss: 5.047577381134033 | BCE Loss: 1.0343377590179443\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.038317680358887 | KNN Loss: 5.009121894836426 | BCE Loss: 1.0291956663131714\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.075058937072754 | KNN Loss: 5.055522441864014 | BCE Loss: 1.0195364952087402\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.037006378173828 | KNN Loss: 4.998915195465088 | BCE Loss: 1.0380914211273193\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.031296730041504 | KNN Loss: 5.013085842132568 | BCE Loss: 1.0182106494903564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.050946235656738 | KNN Loss: 5.010064601898193 | BCE Loss: 1.040881872177124\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.08287239074707 | KNN Loss: 5.050397872924805 | BCE Loss: 1.0324745178222656\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.0667266845703125 | KNN Loss: 5.0120062828063965 | BCE Loss: 1.0547206401824951\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.068122386932373 | KNN Loss: 5.00664758682251 | BCE Loss: 1.0614749193191528\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.060889720916748 | KNN Loss: 5.00822114944458 | BCE Loss: 1.0526686906814575\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.016558647155762 | KNN Loss: 5.019903182983398 | BCE Loss: 0.9966555833816528\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.031740665435791 | KNN Loss: 5.019370079040527 | BCE Loss: 1.0123704671859741\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.070547580718994 | KNN Loss: 5.025796890258789 | BCE Loss: 1.044750690460205\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.067560195922852 | KNN Loss: 5.0401082038879395 | BCE Loss: 1.0274522304534912\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.029908180236816 | KNN Loss: 5.002898216247559 | BCE Loss: 1.0270097255706787\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.044775485992432 | KNN Loss: 4.998493194580078 | BCE Loss: 1.0462822914123535\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.019200801849365 | KNN Loss: 4.99436616897583 | BCE Loss: 1.0248347520828247\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.048794269561768 | KNN Loss: 5.021713733673096 | BCE Loss: 1.0270806550979614\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.063998699188232 | KNN Loss: 5.0109453201293945 | BCE Loss: 1.053053379058838\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.050859451293945 | KNN Loss: 4.998112201690674 | BCE Loss: 1.052747130393982\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.112284183502197 | KNN Loss: 5.044822692871094 | BCE Loss: 1.067461371421814\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.065425872802734 | KNN Loss: 5.012734889984131 | BCE Loss: 1.052690863609314\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.0464324951171875 | KNN Loss: 5.030324459075928 | BCE Loss: 1.0161080360412598\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.040764331817627 | KNN Loss: 5.015138149261475 | BCE Loss: 1.0256261825561523\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.062694549560547 | KNN Loss: 5.014094829559326 | BCE Loss: 1.0485999584197998\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.038385391235352 | KNN Loss: 5.007805347442627 | BCE Loss: 1.0305798053741455\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.042455673217773 | KNN Loss: 5.003693103790283 | BCE Loss: 1.0387623310089111\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.069029331207275 | KNN Loss: 5.051318645477295 | BCE Loss: 1.0177106857299805\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.060340404510498 | KNN Loss: 5.022923469543457 | BCE Loss: 1.037416934967041\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.056148052215576 | KNN Loss: 5.020455837249756 | BCE Loss: 1.0356922149658203\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.054696083068848 | KNN Loss: 5.005152225494385 | BCE Loss: 1.0495437383651733\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.067875385284424 | KNN Loss: 5.024715900421143 | BCE Loss: 1.0431594848632812\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.034343719482422 | KNN Loss: 5.018898963928223 | BCE Loss: 1.0154445171356201\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.036397933959961 | KNN Loss: 4.996516704559326 | BCE Loss: 1.0398814678192139\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.008540153503418 | KNN Loss: 5.005453586578369 | BCE Loss: 1.0030863285064697\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.083220958709717 | KNN Loss: 5.022789478302002 | BCE Loss: 1.0604313611984253\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.0472283363342285 | KNN Loss: 5.018405914306641 | BCE Loss: 1.028822422027588\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.11556339263916 | KNN Loss: 5.055054187774658 | BCE Loss: 1.060509443283081\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.032636642456055 | KNN Loss: 5.027660846710205 | BCE Loss: 1.0049757957458496\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.046475887298584 | KNN Loss: 5.005913734436035 | BCE Loss: 1.0405621528625488\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.05656099319458 | KNN Loss: 5.0150251388549805 | BCE Loss: 1.04153573513031\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.006319046020508 | KNN Loss: 5.003048896789551 | BCE Loss: 1.0032702684402466\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.04304838180542 | KNN Loss: 5.018951416015625 | BCE Loss: 1.0240970849990845\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.082204818725586 | KNN Loss: 5.02016544342041 | BCE Loss: 1.0620396137237549\n",
      "Epoch   317: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.024260520935059 | KNN Loss: 5.001734733581543 | BCE Loss: 1.0225255489349365\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.032126426696777 | KNN Loss: 5.009069442749023 | BCE Loss: 1.023056983947754\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.0261430740356445 | KNN Loss: 5.00508975982666 | BCE Loss: 1.021053433418274\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.030581474304199 | KNN Loss: 5.010741233825684 | BCE Loss: 1.0198404788970947\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.102617263793945 | KNN Loss: 5.061934471130371 | BCE Loss: 1.0406830310821533\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.063365459442139 | KNN Loss: 5.033446311950684 | BCE Loss: 1.029919147491455\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.060441017150879 | KNN Loss: 5.006948947906494 | BCE Loss: 1.0534923076629639\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.031281471252441 | KNN Loss: 5.003113269805908 | BCE Loss: 1.0281680822372437\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.08348274230957 | KNN Loss: 5.046968460083008 | BCE Loss: 1.0365145206451416\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.025424480438232 | KNN Loss: 5.0210137367248535 | BCE Loss: 1.0044108629226685\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.130134105682373 | KNN Loss: 5.058510780334473 | BCE Loss: 1.0716233253479004\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.043323516845703 | KNN Loss: 5.024158954620361 | BCE Loss: 1.0191644430160522\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.026039123535156 | KNN Loss: 5.005866050720215 | BCE Loss: 1.020173192024231\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.04830265045166 | KNN Loss: 5.027316093444824 | BCE Loss: 1.0209866762161255\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.04902982711792 | KNN Loss: 5.012747764587402 | BCE Loss: 1.0362820625305176\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.04182243347168 | KNN Loss: 5.009675979614258 | BCE Loss: 1.0321464538574219\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.082542896270752 | KNN Loss: 5.035623550415039 | BCE Loss: 1.0469192266464233\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.0644378662109375 | KNN Loss: 5.004115104675293 | BCE Loss: 1.0603227615356445\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.042166709899902 | KNN Loss: 4.9978346824646 | BCE Loss: 1.0443317890167236\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.035800933837891 | KNN Loss: 5.009191989898682 | BCE Loss: 1.026609182357788\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.07423734664917 | KNN Loss: 5.041140556335449 | BCE Loss: 1.0330967903137207\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.122799873352051 | KNN Loss: 5.055237770080566 | BCE Loss: 1.0675623416900635\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.016134262084961 | KNN Loss: 5.0024895668029785 | BCE Loss: 1.0136445760726929\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.0591888427734375 | KNN Loss: 5.022740364074707 | BCE Loss: 1.0364487171173096\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.060946941375732 | KNN Loss: 5.013694763183594 | BCE Loss: 1.0472521781921387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.0484490394592285 | KNN Loss: 5.014145851135254 | BCE Loss: 1.034303069114685\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.083335876464844 | KNN Loss: 5.063794136047363 | BCE Loss: 1.01954185962677\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.043814659118652 | KNN Loss: 5.00516939163208 | BCE Loss: 1.0386455059051514\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.097609996795654 | KNN Loss: 5.064860820770264 | BCE Loss: 1.0327491760253906\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.043438911437988 | KNN Loss: 5.002867221832275 | BCE Loss: 1.0405714511871338\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.0720391273498535 | KNN Loss: 5.010136604309082 | BCE Loss: 1.0619025230407715\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.026899337768555 | KNN Loss: 5.004232406616211 | BCE Loss: 1.0226666927337646\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.087615489959717 | KNN Loss: 5.036663055419922 | BCE Loss: 1.050952434539795\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.066280364990234 | KNN Loss: 5.011344909667969 | BCE Loss: 1.0549356937408447\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.048989772796631 | KNN Loss: 4.996068000793457 | BCE Loss: 1.0529218912124634\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.051459312438965 | KNN Loss: 5.014415740966797 | BCE Loss: 1.0370436906814575\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.058657169342041 | KNN Loss: 5.023571491241455 | BCE Loss: 1.0350857973098755\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.0365095138549805 | KNN Loss: 5.008531093597412 | BCE Loss: 1.0279784202575684\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.06025505065918 | KNN Loss: 5.003451347351074 | BCE Loss: 1.0568039417266846\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.063385963439941 | KNN Loss: 5.013864994049072 | BCE Loss: 1.0495212078094482\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.080488204956055 | KNN Loss: 5.017773628234863 | BCE Loss: 1.0627148151397705\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.054458141326904 | KNN Loss: 5.013566017150879 | BCE Loss: 1.0408920049667358\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.0868072509765625 | KNN Loss: 5.06360387802124 | BCE Loss: 1.0232033729553223\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.067487716674805 | KNN Loss: 5.050525665283203 | BCE Loss: 1.0169620513916016\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.129099369049072 | KNN Loss: 5.0641069412231445 | BCE Loss: 1.0649924278259277\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.053030490875244 | KNN Loss: 5.0101494789123535 | BCE Loss: 1.0428810119628906\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.069735527038574 | KNN Loss: 5.008237361907959 | BCE Loss: 1.0614981651306152\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.074772834777832 | KNN Loss: 5.021152973175049 | BCE Loss: 1.0536198616027832\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.063910961151123 | KNN Loss: 5.0297956466674805 | BCE Loss: 1.0341153144836426\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.046602725982666 | KNN Loss: 5.008399963378906 | BCE Loss: 1.0382026433944702\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.049464225769043 | KNN Loss: 5.018664836883545 | BCE Loss: 1.030799150466919\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.038722038269043 | KNN Loss: 5.009969711303711 | BCE Loss: 1.0287524461746216\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.084049224853516 | KNN Loss: 5.0605878829956055 | BCE Loss: 1.0234613418579102\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.08237361907959 | KNN Loss: 5.048014163970947 | BCE Loss: 1.0343596935272217\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.070891857147217 | KNN Loss: 5.005873680114746 | BCE Loss: 1.0650181770324707\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.065285682678223 | KNN Loss: 5.030051231384277 | BCE Loss: 1.0352346897125244\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.096746444702148 | KNN Loss: 5.050115585327148 | BCE Loss: 1.0466309785842896\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.075550556182861 | KNN Loss: 5.060685634613037 | BCE Loss: 1.0148649215698242\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.035909652709961 | KNN Loss: 5.00106954574585 | BCE Loss: 1.0348402261734009\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.050127029418945 | KNN Loss: 5.022327423095703 | BCE Loss: 1.0277996063232422\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.043198585510254 | KNN Loss: 5.00578498840332 | BCE Loss: 1.0374135971069336\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.049607276916504 | KNN Loss: 5.015013217926025 | BCE Loss: 1.0345938205718994\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.046621322631836 | KNN Loss: 5.010974884033203 | BCE Loss: 1.0356464385986328\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.041677951812744 | KNN Loss: 5.019790172576904 | BCE Loss: 1.0218877792358398\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.039588451385498 | KNN Loss: 5.0172247886657715 | BCE Loss: 1.0223637819290161\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.0532379150390625 | KNN Loss: 5.021823883056641 | BCE Loss: 1.0314137935638428\n",
      "Epoch   328: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.035289287567139 | KNN Loss: 5.007626533508301 | BCE Loss: 1.027662754058838\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.032415390014648 | KNN Loss: 5.014102458953857 | BCE Loss: 1.018312931060791\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.0770182609558105 | KNN Loss: 5.04478120803833 | BCE Loss: 1.0322370529174805\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.087262153625488 | KNN Loss: 5.0279622077941895 | BCE Loss: 1.0592998266220093\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.052673816680908 | KNN Loss: 5.005990505218506 | BCE Loss: 1.0466833114624023\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.0448102951049805 | KNN Loss: 5.009159564971924 | BCE Loss: 1.0356509685516357\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.0485734939575195 | KNN Loss: 5.015606880187988 | BCE Loss: 1.0329666137695312\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.028347969055176 | KNN Loss: 5.005723476409912 | BCE Loss: 1.0226244926452637\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.037742614746094 | KNN Loss: 5.010431289672852 | BCE Loss: 1.0273114442825317\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.056364059448242 | KNN Loss: 5.015563011169434 | BCE Loss: 1.0408012866973877\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.098670959472656 | KNN Loss: 5.0451741218566895 | BCE Loss: 1.0534969568252563\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.038049697875977 | KNN Loss: 5.028127193450928 | BCE Loss: 1.0099225044250488\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.050283432006836 | KNN Loss: 5.00026273727417 | BCE Loss: 1.050020694732666\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.020429611206055 | KNN Loss: 5.001104831695557 | BCE Loss: 1.0193250179290771\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.09990119934082 | KNN Loss: 5.025965690612793 | BCE Loss: 1.0739357471466064\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.062188625335693 | KNN Loss: 5.037288188934326 | BCE Loss: 1.0249003171920776\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.042333126068115 | KNN Loss: 5.02766227722168 | BCE Loss: 1.014670729637146\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.027501106262207 | KNN Loss: 5.019352436065674 | BCE Loss: 1.0081489086151123\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.052872657775879 | KNN Loss: 5.005496025085449 | BCE Loss: 1.0473768711090088\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.051052093505859 | KNN Loss: 5.028946399688721 | BCE Loss: 1.0221056938171387\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.128207206726074 | KNN Loss: 5.106272220611572 | BCE Loss: 1.0219348669052124\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.090548992156982 | KNN Loss: 5.042648792266846 | BCE Loss: 1.0479001998901367\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.072717189788818 | KNN Loss: 5.02018928527832 | BCE Loss: 1.052527904510498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.065479755401611 | KNN Loss: 5.022884845733643 | BCE Loss: 1.0425949096679688\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.044946193695068 | KNN Loss: 5.010969638824463 | BCE Loss: 1.033976435661316\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.0918989181518555 | KNN Loss: 5.042323589324951 | BCE Loss: 1.0495752096176147\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.0352606773376465 | KNN Loss: 5.007987976074219 | BCE Loss: 1.0272728204727173\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.070341110229492 | KNN Loss: 5.015366077423096 | BCE Loss: 1.0549752712249756\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.039263725280762 | KNN Loss: 5.014280319213867 | BCE Loss: 1.0249831676483154\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.06982946395874 | KNN Loss: 5.021162033081055 | BCE Loss: 1.0486674308776855\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.045799255371094 | KNN Loss: 5.017697811126709 | BCE Loss: 1.0281013250350952\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.091381549835205 | KNN Loss: 5.011120319366455 | BCE Loss: 1.0802613496780396\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.012171745300293 | KNN Loss: 5.00559663772583 | BCE Loss: 1.006575107574463\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.026045799255371 | KNN Loss: 5.00745153427124 | BCE Loss: 1.0185943841934204\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.041446685791016 | KNN Loss: 5.009601593017578 | BCE Loss: 1.0318453311920166\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.029253959655762 | KNN Loss: 5.0057525634765625 | BCE Loss: 1.0235013961791992\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.0848588943481445 | KNN Loss: 5.0328545570373535 | BCE Loss: 1.052004337310791\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.0523295402526855 | KNN Loss: 5.019433498382568 | BCE Loss: 1.0328960418701172\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.054925441741943 | KNN Loss: 5.031637668609619 | BCE Loss: 1.0232878923416138\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.024150848388672 | KNN Loss: 5.005009174346924 | BCE Loss: 1.019141674041748\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.038855075836182 | KNN Loss: 5.013495445251465 | BCE Loss: 1.0253595113754272\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.050576210021973 | KNN Loss: 5.019447326660156 | BCE Loss: 1.0311288833618164\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.062188148498535 | KNN Loss: 5.039148330688477 | BCE Loss: 1.0230398178100586\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.013597011566162 | KNN Loss: 5.002124309539795 | BCE Loss: 1.0114728212356567\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.081963539123535 | KNN Loss: 5.058053493499756 | BCE Loss: 1.0239098072052002\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.058597564697266 | KNN Loss: 5.000448703765869 | BCE Loss: 1.058148741722107\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.036462783813477 | KNN Loss: 5.0088114738464355 | BCE Loss: 1.0276511907577515\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.049854278564453 | KNN Loss: 5.0299835205078125 | BCE Loss: 1.0198705196380615\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.133462905883789 | KNN Loss: 5.064409255981445 | BCE Loss: 1.0690534114837646\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.065346717834473 | KNN Loss: 5.048280715942383 | BCE Loss: 1.0170657634735107\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.108736515045166 | KNN Loss: 5.056155204772949 | BCE Loss: 1.0525813102722168\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.034028053283691 | KNN Loss: 5.023766040802002 | BCE Loss: 1.010262131690979\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.048944473266602 | KNN Loss: 5.014588832855225 | BCE Loss: 1.034355878829956\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.060441017150879 | KNN Loss: 5.010474681854248 | BCE Loss: 1.0499660968780518\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.073790550231934 | KNN Loss: 5.017101764678955 | BCE Loss: 1.056688666343689\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.0598225593566895 | KNN Loss: 5.046014785766602 | BCE Loss: 1.013807773590088\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.030868053436279 | KNN Loss: 5.006805419921875 | BCE Loss: 1.0240626335144043\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.065829753875732 | KNN Loss: 5.034780502319336 | BCE Loss: 1.0310492515563965\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.045966148376465 | KNN Loss: 4.996208190917969 | BCE Loss: 1.049757957458496\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.040925025939941 | KNN Loss: 5.031116485595703 | BCE Loss: 1.0098083019256592\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.064701080322266 | KNN Loss: 5.030277252197266 | BCE Loss: 1.034423828125\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.057380676269531 | KNN Loss: 5.003118515014648 | BCE Loss: 1.0542621612548828\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.030117511749268 | KNN Loss: 5.018797397613525 | BCE Loss: 1.0113199949264526\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.064658164978027 | KNN Loss: 5.020498752593994 | BCE Loss: 1.044159173965454\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.058117866516113 | KNN Loss: 5.010491371154785 | BCE Loss: 1.047626256942749\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.047196388244629 | KNN Loss: 5.0090837478637695 | BCE Loss: 1.0381126403808594\n",
      "Epoch   339: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.150415420532227 | KNN Loss: 5.0813822746276855 | BCE Loss: 1.0690332651138306\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.051431655883789 | KNN Loss: 5.021467208862305 | BCE Loss: 1.0299644470214844\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.061379432678223 | KNN Loss: 5.030219554901123 | BCE Loss: 1.0311596393585205\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.016502857208252 | KNN Loss: 5.022883892059326 | BCE Loss: 0.993618905544281\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.05787992477417 | KNN Loss: 5.0179548263549805 | BCE Loss: 1.039925217628479\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.034671306610107 | KNN Loss: 5.014970302581787 | BCE Loss: 1.0197010040283203\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.056004524230957 | KNN Loss: 5.014817237854004 | BCE Loss: 1.041187047958374\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.036794185638428 | KNN Loss: 5.011769771575928 | BCE Loss: 1.0250244140625\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.048349857330322 | KNN Loss: 5.025266647338867 | BCE Loss: 1.023083209991455\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.0956878662109375 | KNN Loss: 5.053501129150391 | BCE Loss: 1.0421864986419678\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.041010856628418 | KNN Loss: 5.029959201812744 | BCE Loss: 1.0110516548156738\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.073428153991699 | KNN Loss: 5.031480312347412 | BCE Loss: 1.041947841644287\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.030886650085449 | KNN Loss: 5.007942199707031 | BCE Loss: 1.022944688796997\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.0764923095703125 | KNN Loss: 5.003060817718506 | BCE Loss: 1.0734317302703857\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.041280746459961 | KNN Loss: 5.011468887329102 | BCE Loss: 1.0298118591308594\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.043503761291504 | KNN Loss: 5.005468845367432 | BCE Loss: 1.0380351543426514\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.11297607421875 | KNN Loss: 5.053813457489014 | BCE Loss: 1.0591623783111572\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.047723770141602 | KNN Loss: 5.011281967163086 | BCE Loss: 1.0364415645599365\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.056727409362793 | KNN Loss: 5.003391742706299 | BCE Loss: 1.0533359050750732\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.045104026794434 | KNN Loss: 5.004520416259766 | BCE Loss: 1.040583610534668\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.085026741027832 | KNN Loss: 5.0216288566589355 | BCE Loss: 1.0633978843688965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.044277191162109 | KNN Loss: 5.002429962158203 | BCE Loss: 1.0418472290039062\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.087509632110596 | KNN Loss: 5.021456718444824 | BCE Loss: 1.066052794456482\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.054943561553955 | KNN Loss: 4.995555877685547 | BCE Loss: 1.0593876838684082\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.101193904876709 | KNN Loss: 5.061398506164551 | BCE Loss: 1.0397953987121582\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.058645248413086 | KNN Loss: 5.0242767333984375 | BCE Loss: 1.034368634223938\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.0269317626953125 | KNN Loss: 5.0104804039001465 | BCE Loss: 1.0164514780044556\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.0362701416015625 | KNN Loss: 5.021463871002197 | BCE Loss: 1.0148060321807861\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.062179088592529 | KNN Loss: 5.004223823547363 | BCE Loss: 1.0579551458358765\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.0833234786987305 | KNN Loss: 5.045074939727783 | BCE Loss: 1.0382485389709473\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.023805618286133 | KNN Loss: 4.999352931976318 | BCE Loss: 1.0244524478912354\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.054116249084473 | KNN Loss: 5.017660617828369 | BCE Loss: 1.0364556312561035\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.066578388214111 | KNN Loss: 5.046382427215576 | BCE Loss: 1.0201959609985352\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.059780597686768 | KNN Loss: 5.01334285736084 | BCE Loss: 1.0464378595352173\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.038344383239746 | KNN Loss: 5.003355026245117 | BCE Loss: 1.0349892377853394\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.054072380065918 | KNN Loss: 5.029469966888428 | BCE Loss: 1.0246022939682007\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.073531150817871 | KNN Loss: 5.02897310256958 | BCE Loss: 1.044558048248291\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.062130928039551 | KNN Loss: 5.027578353881836 | BCE Loss: 1.0345523357391357\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.047329902648926 | KNN Loss: 5.013375282287598 | BCE Loss: 1.0339547395706177\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.031805515289307 | KNN Loss: 5.0221099853515625 | BCE Loss: 1.0096955299377441\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.011432647705078 | KNN Loss: 5.004820823669434 | BCE Loss: 1.0066120624542236\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.035853385925293 | KNN Loss: 5.010848045349121 | BCE Loss: 1.0250053405761719\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.051436424255371 | KNN Loss: 5.0102009773254395 | BCE Loss: 1.0412352085113525\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.067015171051025 | KNN Loss: 5.015223503112793 | BCE Loss: 1.0517916679382324\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.066359996795654 | KNN Loss: 5.019808292388916 | BCE Loss: 1.0465517044067383\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.084517478942871 | KNN Loss: 5.048720836639404 | BCE Loss: 1.0357966423034668\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.042316436767578 | KNN Loss: 5.026029586791992 | BCE Loss: 1.016286849975586\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.044076442718506 | KNN Loss: 5.014912128448486 | BCE Loss: 1.029164433479309\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.03048038482666 | KNN Loss: 5.006758689880371 | BCE Loss: 1.0237219333648682\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.053267955780029 | KNN Loss: 5.014076232910156 | BCE Loss: 1.039191722869873\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.083144664764404 | KNN Loss: 5.024155616760254 | BCE Loss: 1.0589890480041504\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.053150177001953 | KNN Loss: 5.011058807373047 | BCE Loss: 1.0420913696289062\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.0408525466918945 | KNN Loss: 5.011260509490967 | BCE Loss: 1.0295919179916382\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.049098968505859 | KNN Loss: 5.018436908721924 | BCE Loss: 1.0306618213653564\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.030683517456055 | KNN Loss: 5.012935638427734 | BCE Loss: 1.0177476406097412\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.054692268371582 | KNN Loss: 5.02535343170166 | BCE Loss: 1.0293385982513428\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.078884601593018 | KNN Loss: 5.020354270935059 | BCE Loss: 1.0585304498672485\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.045833110809326 | KNN Loss: 5.0117411613464355 | BCE Loss: 1.034091830253601\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.038865089416504 | KNN Loss: 5.010994911193848 | BCE Loss: 1.0278704166412354\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.002780914306641 | KNN Loss: 4.995316982269287 | BCE Loss: 1.0074641704559326\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.045529365539551 | KNN Loss: 4.996432304382324 | BCE Loss: 1.0490968227386475\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.02646017074585 | KNN Loss: 5.006904125213623 | BCE Loss: 1.0195560455322266\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.042092323303223 | KNN Loss: 5.006946563720703 | BCE Loss: 1.0351457595825195\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.093202590942383 | KNN Loss: 5.062998294830322 | BCE Loss: 1.0302045345306396\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.081110954284668 | KNN Loss: 5.039928913116455 | BCE Loss: 1.0411819219589233\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.030764579772949 | KNN Loss: 5.010359287261963 | BCE Loss: 1.0204052925109863\n",
      "Epoch   350: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.067981719970703 | KNN Loss: 5.025312900543213 | BCE Loss: 1.0426685810089111\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.084774017333984 | KNN Loss: 5.038885593414307 | BCE Loss: 1.0458883047103882\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.039318561553955 | KNN Loss: 5.038228511810303 | BCE Loss: 1.0010900497436523\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.057244777679443 | KNN Loss: 5.011258125305176 | BCE Loss: 1.0459866523742676\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.046206951141357 | KNN Loss: 5.026899814605713 | BCE Loss: 1.019307017326355\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.04097318649292 | KNN Loss: 5.021131992340088 | BCE Loss: 1.019841194152832\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.071896553039551 | KNN Loss: 5.0258331298828125 | BCE Loss: 1.0460631847381592\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.07524299621582 | KNN Loss: 5.011369228363037 | BCE Loss: 1.063873529434204\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.02772855758667 | KNN Loss: 5.001001834869385 | BCE Loss: 1.0267267227172852\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.0215535163879395 | KNN Loss: 5.005724906921387 | BCE Loss: 1.0158286094665527\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.0195536613464355 | KNN Loss: 5.009478569030762 | BCE Loss: 1.0100750923156738\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.086122512817383 | KNN Loss: 5.043555736541748 | BCE Loss: 1.0425666570663452\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.086349010467529 | KNN Loss: 5.044124603271484 | BCE Loss: 1.0422242879867554\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.002781867980957 | KNN Loss: 4.991078853607178 | BCE Loss: 1.0117032527923584\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.025698184967041 | KNN Loss: 5.004894733428955 | BCE Loss: 1.0208033323287964\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.035808563232422 | KNN Loss: 5.024523735046387 | BCE Loss: 1.0112849473953247\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.054784774780273 | KNN Loss: 5.015397071838379 | BCE Loss: 1.0393874645233154\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.048103332519531 | KNN Loss: 5.014796257019043 | BCE Loss: 1.0333073139190674\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.025509834289551 | KNN Loss: 5.022061347961426 | BCE Loss: 1.003448486328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.074705123901367 | KNN Loss: 5.00678014755249 | BCE Loss: 1.0679250955581665\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.069143295288086 | KNN Loss: 5.030079364776611 | BCE Loss: 1.0390639305114746\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.079097270965576 | KNN Loss: 5.022827625274658 | BCE Loss: 1.056269645690918\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.037054538726807 | KNN Loss: 4.996549606323242 | BCE Loss: 1.0405049324035645\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.071198463439941 | KNN Loss: 5.032068252563477 | BCE Loss: 1.0391300916671753\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.01717472076416 | KNN Loss: 5.002819061279297 | BCE Loss: 1.0143558979034424\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.054311752319336 | KNN Loss: 5.0203752517700195 | BCE Loss: 1.0339365005493164\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.072945594787598 | KNN Loss: 5.025470733642578 | BCE Loss: 1.0474748611450195\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.065352439880371 | KNN Loss: 5.002928256988525 | BCE Loss: 1.0624239444732666\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.022135257720947 | KNN Loss: 4.999263286590576 | BCE Loss: 1.0228720903396606\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.029354095458984 | KNN Loss: 5.001781940460205 | BCE Loss: 1.0275722742080688\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.0502190589904785 | KNN Loss: 5.022618293762207 | BCE Loss: 1.0276007652282715\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.04666805267334 | KNN Loss: 5.023983478546143 | BCE Loss: 1.0226846933364868\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.055132865905762 | KNN Loss: 5.027673721313477 | BCE Loss: 1.0274592638015747\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.035362720489502 | KNN Loss: 4.996880531311035 | BCE Loss: 1.0384823083877563\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.066617965698242 | KNN Loss: 5.029837131500244 | BCE Loss: 1.036780595779419\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.0201311111450195 | KNN Loss: 5.005723476409912 | BCE Loss: 1.0144073963165283\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.052705764770508 | KNN Loss: 5.028006553649902 | BCE Loss: 1.0246989727020264\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.048338890075684 | KNN Loss: 4.999756813049316 | BCE Loss: 1.048581838607788\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.078797340393066 | KNN Loss: 5.028637409210205 | BCE Loss: 1.0501596927642822\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.077756881713867 | KNN Loss: 5.034649848937988 | BCE Loss: 1.043107271194458\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.038630485534668 | KNN Loss: 5.0354838371276855 | BCE Loss: 1.0031466484069824\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.026093482971191 | KNN Loss: 5.004515647888184 | BCE Loss: 1.0215778350830078\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.029285907745361 | KNN Loss: 4.999053478240967 | BCE Loss: 1.0302324295043945\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.023316860198975 | KNN Loss: 4.99356746673584 | BCE Loss: 1.0297493934631348\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.0607218742370605 | KNN Loss: 5.037569522857666 | BCE Loss: 1.0231523513793945\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.055425643920898 | KNN Loss: 5.008934020996094 | BCE Loss: 1.0464913845062256\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.011496543884277 | KNN Loss: 5.001718997955322 | BCE Loss: 1.009777307510376\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.083192348480225 | KNN Loss: 5.006380081176758 | BCE Loss: 1.0768121480941772\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.059396743774414 | KNN Loss: 5.00963020324707 | BCE Loss: 1.0497663021087646\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.116037368774414 | KNN Loss: 5.081905364990234 | BCE Loss: 1.0341320037841797\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.021788120269775 | KNN Loss: 5.022064685821533 | BCE Loss: 0.9997234344482422\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.047275543212891 | KNN Loss: 5.031650543212891 | BCE Loss: 1.0156251192092896\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.036014080047607 | KNN Loss: 4.990574359893799 | BCE Loss: 1.0454397201538086\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.051816940307617 | KNN Loss: 5.006639003753662 | BCE Loss: 1.045177936553955\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.100217819213867 | KNN Loss: 5.019497394561768 | BCE Loss: 1.0807201862335205\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.065338134765625 | KNN Loss: 5.009722709655762 | BCE Loss: 1.0556155443191528\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.053349494934082 | KNN Loss: 5.032649040222168 | BCE Loss: 1.020700454711914\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.072454452514648 | KNN Loss: 5.011804103851318 | BCE Loss: 1.0606505870819092\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.1161417961120605 | KNN Loss: 5.069613933563232 | BCE Loss: 1.0465279817581177\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.074092864990234 | KNN Loss: 5.0383620262146 | BCE Loss: 1.0357310771942139\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.052914142608643 | KNN Loss: 5.003098964691162 | BCE Loss: 1.0498151779174805\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.035198211669922 | KNN Loss: 5.007442951202393 | BCE Loss: 1.0277554988861084\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.102722644805908 | KNN Loss: 5.069350719451904 | BCE Loss: 1.033371925354004\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.056967258453369 | KNN Loss: 5.004188537597656 | BCE Loss: 1.0527786016464233\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.150825023651123 | KNN Loss: 5.0701704025268555 | BCE Loss: 1.0806546211242676\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.032159805297852 | KNN Loss: 5.002520561218262 | BCE Loss: 1.029639482498169\n",
      "Epoch   361: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.027782440185547 | KNN Loss: 5.016481876373291 | BCE Loss: 1.011300802230835\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.0677666664123535 | KNN Loss: 5.037644863128662 | BCE Loss: 1.0301218032836914\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.032588481903076 | KNN Loss: 5.012545585632324 | BCE Loss: 1.0200427770614624\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.041164398193359 | KNN Loss: 5.02102518081665 | BCE Loss: 1.020139217376709\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.0351362228393555 | KNN Loss: 5.00950813293457 | BCE Loss: 1.0256280899047852\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.098946571350098 | KNN Loss: 5.054872512817383 | BCE Loss: 1.0440738201141357\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.029138088226318 | KNN Loss: 5.0073113441467285 | BCE Loss: 1.0218267440795898\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.08316707611084 | KNN Loss: 5.013519287109375 | BCE Loss: 1.0696475505828857\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.057338714599609 | KNN Loss: 5.028550148010254 | BCE Loss: 1.028788685798645\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.0397467613220215 | KNN Loss: 5.027451515197754 | BCE Loss: 1.0122952461242676\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.088677883148193 | KNN Loss: 5.04478645324707 | BCE Loss: 1.043891429901123\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.110418796539307 | KNN Loss: 5.056354999542236 | BCE Loss: 1.0540636777877808\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.084978103637695 | KNN Loss: 5.061522006988525 | BCE Loss: 1.02345609664917\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.062886714935303 | KNN Loss: 5.019455432891846 | BCE Loss: 1.043431282043457\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.058533191680908 | KNN Loss: 5.026803016662598 | BCE Loss: 1.0317301750183105\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.032021522521973 | KNN Loss: 5.010770797729492 | BCE Loss: 1.0212507247924805\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.053636074066162 | KNN Loss: 5.018199920654297 | BCE Loss: 1.0354361534118652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.025862693786621 | KNN Loss: 5.005902290344238 | BCE Loss: 1.0199604034423828\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.053094387054443 | KNN Loss: 5.010839939117432 | BCE Loss: 1.0422545671463013\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.08757209777832 | KNN Loss: 5.0403289794921875 | BCE Loss: 1.0472432374954224\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.008630752563477 | KNN Loss: 4.998585224151611 | BCE Loss: 1.0100452899932861\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.059114933013916 | KNN Loss: 5.005693435668945 | BCE Loss: 1.0534216165542603\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.034046649932861 | KNN Loss: 5.00489616394043 | BCE Loss: 1.0291506052017212\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.055006980895996 | KNN Loss: 4.999687671661377 | BCE Loss: 1.0553191900253296\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.057852745056152 | KNN Loss: 5.0117597579956055 | BCE Loss: 1.0460931062698364\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.043900489807129 | KNN Loss: 5.0034027099609375 | BCE Loss: 1.0404980182647705\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.048505783081055 | KNN Loss: 5.011550426483154 | BCE Loss: 1.0369555950164795\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.043761730194092 | KNN Loss: 5.02761697769165 | BCE Loss: 1.0161447525024414\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.033624649047852 | KNN Loss: 5.0118255615234375 | BCE Loss: 1.021798849105835\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.058326721191406 | KNN Loss: 5.01071834564209 | BCE Loss: 1.0476086139678955\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.0375165939331055 | KNN Loss: 5.0105156898498535 | BCE Loss: 1.027000904083252\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.09428596496582 | KNN Loss: 5.055736064910889 | BCE Loss: 1.0385501384735107\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.059409141540527 | KNN Loss: 5.0155415534973145 | BCE Loss: 1.043867588043213\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.037909984588623 | KNN Loss: 4.998344898223877 | BCE Loss: 1.039565086364746\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.0568952560424805 | KNN Loss: 5.019600868225098 | BCE Loss: 1.037294626235962\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.05754280090332 | KNN Loss: 5.0307297706604 | BCE Loss: 1.0268129110336304\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.017007827758789 | KNN Loss: 4.985493183135986 | BCE Loss: 1.0315147638320923\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.074426651000977 | KNN Loss: 5.036285877227783 | BCE Loss: 1.0381406545639038\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.063761234283447 | KNN Loss: 5.032383918762207 | BCE Loss: 1.0313773155212402\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.062612056732178 | KNN Loss: 5.0341057777404785 | BCE Loss: 1.0285061597824097\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.031717300415039 | KNN Loss: 5.000172138214111 | BCE Loss: 1.0315454006195068\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.0711164474487305 | KNN Loss: 5.033226490020752 | BCE Loss: 1.0378901958465576\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.04036808013916 | KNN Loss: 5.0085039138793945 | BCE Loss: 1.0318644046783447\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.01956033706665 | KNN Loss: 4.990445613861084 | BCE Loss: 1.0291146039962769\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.013587951660156 | KNN Loss: 5.000144004821777 | BCE Loss: 1.0134440660476685\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.014013290405273 | KNN Loss: 5.012688159942627 | BCE Loss: 1.0013248920440674\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.048064708709717 | KNN Loss: 5.015015125274658 | BCE Loss: 1.0330495834350586\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.086069107055664 | KNN Loss: 5.032578468322754 | BCE Loss: 1.0534908771514893\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.04604959487915 | KNN Loss: 5.011268615722656 | BCE Loss: 1.0347809791564941\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.0628533363342285 | KNN Loss: 5.0397257804870605 | BCE Loss: 1.0231274366378784\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.065995693206787 | KNN Loss: 5.043450832366943 | BCE Loss: 1.0225448608398438\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.009104251861572 | KNN Loss: 5.003933429718018 | BCE Loss: 1.0051708221435547\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.057750701904297 | KNN Loss: 5.04190731048584 | BCE Loss: 1.015843152999878\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.095027446746826 | KNN Loss: 5.038846969604492 | BCE Loss: 1.0561805963516235\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.080262184143066 | KNN Loss: 5.028390884399414 | BCE Loss: 1.051871418952942\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.069134712219238 | KNN Loss: 5.015618801116943 | BCE Loss: 1.053516149520874\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.041525840759277 | KNN Loss: 5.0275163650512695 | BCE Loss: 1.0140095949172974\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.05485200881958 | KNN Loss: 5.018611907958984 | BCE Loss: 1.0362401008605957\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.106756210327148 | KNN Loss: 5.05312967300415 | BCE Loss: 1.0536267757415771\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.05686092376709 | KNN Loss: 5.017053604125977 | BCE Loss: 1.0398072004318237\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.044127464294434 | KNN Loss: 5.024640083312988 | BCE Loss: 1.0194876194000244\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.066265106201172 | KNN Loss: 5.02296781539917 | BCE Loss: 1.0432970523834229\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.151812553405762 | KNN Loss: 5.085659503936768 | BCE Loss: 1.0661530494689941\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.068621635437012 | KNN Loss: 5.024775505065918 | BCE Loss: 1.0438458919525146\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.038660049438477 | KNN Loss: 5.00006628036499 | BCE Loss: 1.0385937690734863\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.068058967590332 | KNN Loss: 5.025709629058838 | BCE Loss: 1.0423493385314941\n",
      "Epoch   372: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.1071367263793945 | KNN Loss: 5.034924507141113 | BCE Loss: 1.0722119808197021\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.057517051696777 | KNN Loss: 5.023928165435791 | BCE Loss: 1.0335886478424072\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.053947448730469 | KNN Loss: 5.00618839263916 | BCE Loss: 1.0477588176727295\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.117195129394531 | KNN Loss: 5.072020530700684 | BCE Loss: 1.0451745986938477\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.047065734863281 | KNN Loss: 5.012355327606201 | BCE Loss: 1.034710168838501\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.063072681427002 | KNN Loss: 5.005512714385986 | BCE Loss: 1.0575600862503052\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.034183979034424 | KNN Loss: 5.004650592803955 | BCE Loss: 1.0295333862304688\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.005692958831787 | KNN Loss: 5.006451606750488 | BCE Loss: 0.9992411732673645\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.044304370880127 | KNN Loss: 5.0152268409729 | BCE Loss: 1.0290776491165161\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.129426002502441 | KNN Loss: 5.066217422485352 | BCE Loss: 1.063208818435669\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.062360763549805 | KNN Loss: 5.014618873596191 | BCE Loss: 1.0477418899536133\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.017157077789307 | KNN Loss: 5.025044918060303 | BCE Loss: 0.9921122789382935\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.046393394470215 | KNN Loss: 5.002049446105957 | BCE Loss: 1.044344186782837\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.0455780029296875 | KNN Loss: 5.015745162963867 | BCE Loss: 1.0298329591751099\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.0771074295043945 | KNN Loss: 5.026904106140137 | BCE Loss: 1.0502030849456787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.0482683181762695 | KNN Loss: 5.018329620361328 | BCE Loss: 1.0299386978149414\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.034018039703369 | KNN Loss: 5.0100483894348145 | BCE Loss: 1.0239696502685547\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.033761978149414 | KNN Loss: 5.003570556640625 | BCE Loss: 1.0301913022994995\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.045807838439941 | KNN Loss: 4.99943733215332 | BCE Loss: 1.046370267868042\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.078805923461914 | KNN Loss: 5.029544353485107 | BCE Loss: 1.0492613315582275\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.02915096282959 | KNN Loss: 5.011593341827393 | BCE Loss: 1.0175575017929077\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.038719654083252 | KNN Loss: 5.006389617919922 | BCE Loss: 1.0323301553726196\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.044210433959961 | KNN Loss: 4.995811462402344 | BCE Loss: 1.0483992099761963\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.059535980224609 | KNN Loss: 5.023220062255859 | BCE Loss: 1.036315679550171\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.056474685668945 | KNN Loss: 5.016697883605957 | BCE Loss: 1.0397769212722778\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.068471431732178 | KNN Loss: 5.023624420166016 | BCE Loss: 1.0448471307754517\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.02814245223999 | KNN Loss: 5.014379978179932 | BCE Loss: 1.013762354850769\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.053033828735352 | KNN Loss: 5.015932559967041 | BCE Loss: 1.037101149559021\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.059417724609375 | KNN Loss: 4.997069358825684 | BCE Loss: 1.0623483657836914\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.049363136291504 | KNN Loss: 5.004992961883545 | BCE Loss: 1.044370174407959\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.083133220672607 | KNN Loss: 5.042131423950195 | BCE Loss: 1.0410016775131226\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.065343379974365 | KNN Loss: 5.015926837921143 | BCE Loss: 1.0494165420532227\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.0652055740356445 | KNN Loss: 5.0368242263793945 | BCE Loss: 1.028381109237671\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.032144069671631 | KNN Loss: 5.006333827972412 | BCE Loss: 1.0258102416992188\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.046100616455078 | KNN Loss: 5.007309913635254 | BCE Loss: 1.0387904644012451\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.062963485717773 | KNN Loss: 5.011159420013428 | BCE Loss: 1.0518038272857666\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.03590726852417 | KNN Loss: 4.999791622161865 | BCE Loss: 1.0361157655715942\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.051321029663086 | KNN Loss: 5.01937198638916 | BCE Loss: 1.0319490432739258\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.125039577484131 | KNN Loss: 5.070319175720215 | BCE Loss: 1.054720401763916\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.072898864746094 | KNN Loss: 5.033785820007324 | BCE Loss: 1.0391132831573486\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.010737419128418 | KNN Loss: 5.004971504211426 | BCE Loss: 1.0057661533355713\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.055858612060547 | KNN Loss: 5.003071308135986 | BCE Loss: 1.0527873039245605\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.059894561767578 | KNN Loss: 5.046406269073486 | BCE Loss: 1.0134882926940918\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.05517578125 | KNN Loss: 5.006085395812988 | BCE Loss: 1.0490905046463013\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.08432674407959 | KNN Loss: 5.065502643585205 | BCE Loss: 1.0188238620758057\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.0347185134887695 | KNN Loss: 5.012224197387695 | BCE Loss: 1.0224945545196533\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.127989768981934 | KNN Loss: 5.077923774719238 | BCE Loss: 1.0500661134719849\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.051370620727539 | KNN Loss: 4.997101306915283 | BCE Loss: 1.0542690753936768\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.058389663696289 | KNN Loss: 5.044887542724609 | BCE Loss: 1.0135018825531006\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.041796684265137 | KNN Loss: 5.01386833190918 | BCE Loss: 1.0279282331466675\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.058712959289551 | KNN Loss: 5.011017799377441 | BCE Loss: 1.0476949214935303\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.046316623687744 | KNN Loss: 5.029090404510498 | BCE Loss: 1.0172260999679565\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.047307014465332 | KNN Loss: 5.029329776763916 | BCE Loss: 1.0179774761199951\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.0597052574157715 | KNN Loss: 5.010753631591797 | BCE Loss: 1.0489517450332642\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.051390171051025 | KNN Loss: 5.015726089477539 | BCE Loss: 1.0356640815734863\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.087357997894287 | KNN Loss: 5.035102367401123 | BCE Loss: 1.0522555112838745\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.053214073181152 | KNN Loss: 5.0449604988098145 | BCE Loss: 1.008253574371338\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.045604705810547 | KNN Loss: 5.011573791503906 | BCE Loss: 1.0340311527252197\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.035064220428467 | KNN Loss: 5.012642860412598 | BCE Loss: 1.0224213600158691\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.024909019470215 | KNN Loss: 5.0069074630737305 | BCE Loss: 1.0180015563964844\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.014771938323975 | KNN Loss: 4.990228652954102 | BCE Loss: 1.024543285369873\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.0492167472839355 | KNN Loss: 5.0079851150512695 | BCE Loss: 1.041231632232666\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.098711967468262 | KNN Loss: 5.063853740692139 | BCE Loss: 1.0348581075668335\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.029439926147461 | KNN Loss: 4.99821662902832 | BCE Loss: 1.0312230587005615\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.031922340393066 | KNN Loss: 5.0132155418396 | BCE Loss: 1.018707036972046\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.029534339904785 | KNN Loss: 5.009382247924805 | BCE Loss: 1.02015221118927\n",
      "Epoch   383: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.087864398956299 | KNN Loss: 5.041934967041016 | BCE Loss: 1.0459295511245728\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.036884307861328 | KNN Loss: 5.008741855621338 | BCE Loss: 1.0281426906585693\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.049511432647705 | KNN Loss: 5.017810344696045 | BCE Loss: 1.0317009687423706\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.038052082061768 | KNN Loss: 5.013125419616699 | BCE Loss: 1.0249266624450684\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.0551910400390625 | KNN Loss: 5.015254020690918 | BCE Loss: 1.0399370193481445\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.04658842086792 | KNN Loss: 5.013674259185791 | BCE Loss: 1.0329140424728394\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.045039176940918 | KNN Loss: 5.00917911529541 | BCE Loss: 1.035860300064087\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.0398759841918945 | KNN Loss: 5.031621932983398 | BCE Loss: 1.0082542896270752\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.0267438888549805 | KNN Loss: 5.0116448402404785 | BCE Loss: 1.015099287033081\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.050045013427734 | KNN Loss: 5.00357723236084 | BCE Loss: 1.0464677810668945\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.0291876792907715 | KNN Loss: 5.006286144256592 | BCE Loss: 1.0229015350341797\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.048625469207764 | KNN Loss: 5.015373229980469 | BCE Loss: 1.0332523584365845\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.0223846435546875 | KNN Loss: 5.012651443481445 | BCE Loss: 1.009732961654663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.038246154785156 | KNN Loss: 5.018630027770996 | BCE Loss: 1.0196163654327393\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.063436508178711 | KNN Loss: 5.01180362701416 | BCE Loss: 1.0516328811645508\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.043227672576904 | KNN Loss: 5.009031295776367 | BCE Loss: 1.034196376800537\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.082441329956055 | KNN Loss: 5.039413928985596 | BCE Loss: 1.0430275201797485\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.0567779541015625 | KNN Loss: 5.029412746429443 | BCE Loss: 1.0273652076721191\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.055022716522217 | KNN Loss: 5.007907390594482 | BCE Loss: 1.0471152067184448\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.0579328536987305 | KNN Loss: 5.01771879196167 | BCE Loss: 1.0402143001556396\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.091553211212158 | KNN Loss: 5.040466785430908 | BCE Loss: 1.05108642578125\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.081606864929199 | KNN Loss: 5.044548034667969 | BCE Loss: 1.0370585918426514\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.038636207580566 | KNN Loss: 5.0340576171875 | BCE Loss: 1.0045785903930664\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.051310062408447 | KNN Loss: 5.029106140136719 | BCE Loss: 1.0222039222717285\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.053478240966797 | KNN Loss: 5.017611980438232 | BCE Loss: 1.0358662605285645\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.052798748016357 | KNN Loss: 5.016505718231201 | BCE Loss: 1.0362929105758667\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.059980392456055 | KNN Loss: 5.017939567565918 | BCE Loss: 1.0420410633087158\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.097557544708252 | KNN Loss: 5.069736480712891 | BCE Loss: 1.0278211832046509\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.038834571838379 | KNN Loss: 5.020397186279297 | BCE Loss: 1.0184375047683716\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.056753158569336 | KNN Loss: 5.033899307250977 | BCE Loss: 1.0228538513183594\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.058093070983887 | KNN Loss: 5.008143901824951 | BCE Loss: 1.0499491691589355\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.134818077087402 | KNN Loss: 5.100008010864258 | BCE Loss: 1.034809947013855\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.035499572753906 | KNN Loss: 5.008457183837891 | BCE Loss: 1.0270425081253052\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.063207626342773 | KNN Loss: 5.043181419372559 | BCE Loss: 1.0200259685516357\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.052799224853516 | KNN Loss: 4.9997968673706055 | BCE Loss: 1.0530022382736206\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.033708572387695 | KNN Loss: 5.010364055633545 | BCE Loss: 1.0233447551727295\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.021526336669922 | KNN Loss: 5.014658451080322 | BCE Loss: 1.0068681240081787\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.0522871017456055 | KNN Loss: 4.997550010681152 | BCE Loss: 1.0547370910644531\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.015169143676758 | KNN Loss: 5.015900135040283 | BCE Loss: 0.9992690086364746\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.062525749206543 | KNN Loss: 5.029518127441406 | BCE Loss: 1.0330075025558472\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.036179542541504 | KNN Loss: 5.013853549957275 | BCE Loss: 1.0223259925842285\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.055341720581055 | KNN Loss: 5.025867938995361 | BCE Loss: 1.0294737815856934\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.0366411209106445 | KNN Loss: 5.029965877532959 | BCE Loss: 1.0066754817962646\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.040030479431152 | KNN Loss: 4.994758129119873 | BCE Loss: 1.0452721118927002\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.053823471069336 | KNN Loss: 5.030858516693115 | BCE Loss: 1.0229651927947998\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.057646751403809 | KNN Loss: 5.01535701751709 | BCE Loss: 1.0422897338867188\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.060915470123291 | KNN Loss: 5.0273871421813965 | BCE Loss: 1.0335283279418945\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.066256523132324 | KNN Loss: 5.021635055541992 | BCE Loss: 1.0446215867996216\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.039639472961426 | KNN Loss: 5.007765769958496 | BCE Loss: 1.0318735837936401\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.047858715057373 | KNN Loss: 5.000210762023926 | BCE Loss: 1.0476479530334473\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.029173851013184 | KNN Loss: 5.0307698249816895 | BCE Loss: 0.9984042644500732\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.0367865562438965 | KNN Loss: 5.0197062492370605 | BCE Loss: 1.0170801877975464\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.0287604331970215 | KNN Loss: 5.033109664916992 | BCE Loss: 0.9956506490707397\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.054472923278809 | KNN Loss: 5.013944149017334 | BCE Loss: 1.0405285358428955\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.028800010681152 | KNN Loss: 5.013115406036377 | BCE Loss: 1.0156848430633545\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.119451999664307 | KNN Loss: 5.071706295013428 | BCE Loss: 1.047745704650879\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.011752605438232 | KNN Loss: 5.010233402252197 | BCE Loss: 1.0015193223953247\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.067269325256348 | KNN Loss: 5.025147438049316 | BCE Loss: 1.0421221256256104\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.035412311553955 | KNN Loss: 5.028258800506592 | BCE Loss: 1.0071536302566528\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.048329830169678 | KNN Loss: 5.008264541625977 | BCE Loss: 1.0400652885437012\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.070539951324463 | KNN Loss: 5.01909875869751 | BCE Loss: 1.0514411926269531\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.06704044342041 | KNN Loss: 5.0313568115234375 | BCE Loss: 1.0356836318969727\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.044346809387207 | KNN Loss: 5.016683101654053 | BCE Loss: 1.0276637077331543\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.077186584472656 | KNN Loss: 5.050631523132324 | BCE Loss: 1.026555061340332\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.141437530517578 | KNN Loss: 5.090549468994141 | BCE Loss: 1.0508882999420166\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.042880058288574 | KNN Loss: 5.032584190368652 | BCE Loss: 1.0102959871292114\n",
      "Epoch   394: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.039486885070801 | KNN Loss: 5.004816055297852 | BCE Loss: 1.0346708297729492\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.069241046905518 | KNN Loss: 5.068753242492676 | BCE Loss: 1.0004876852035522\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.102400779724121 | KNN Loss: 5.052468776702881 | BCE Loss: 1.0499322414398193\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.055861473083496 | KNN Loss: 5.0504255294799805 | BCE Loss: 1.0054357051849365\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.032625675201416 | KNN Loss: 5.003726005554199 | BCE Loss: 1.0288997888565063\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.031988620758057 | KNN Loss: 5.009103775024414 | BCE Loss: 1.0228848457336426\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.084664344787598 | KNN Loss: 5.016513824462891 | BCE Loss: 1.068150281906128\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.0377912521362305 | KNN Loss: 5.004881858825684 | BCE Loss: 1.0329093933105469\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.073077201843262 | KNN Loss: 5.036925315856934 | BCE Loss: 1.0361520051956177\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.06414794921875 | KNN Loss: 5.0087361335754395 | BCE Loss: 1.0554119348526\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.0702033042907715 | KNN Loss: 5.004446029663086 | BCE Loss: 1.065757155418396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.059661865234375 | KNN Loss: 5.016458034515381 | BCE Loss: 1.0432038307189941\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.026154041290283 | KNN Loss: 5.010557651519775 | BCE Loss: 1.0155963897705078\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.109240531921387 | KNN Loss: 5.084547996520996 | BCE Loss: 1.0246927738189697\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.033666610717773 | KNN Loss: 5.0179572105407715 | BCE Loss: 1.015709638595581\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.05462646484375 | KNN Loss: 5.015382289886475 | BCE Loss: 1.0392440557479858\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.010775566101074 | KNN Loss: 5.001898288726807 | BCE Loss: 1.0088770389556885\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.0647873878479 | KNN Loss: 5.006306171417236 | BCE Loss: 1.058481216430664\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.055685520172119 | KNN Loss: 5.032402038574219 | BCE Loss: 1.0232834815979004\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.049612045288086 | KNN Loss: 5.00840425491333 | BCE Loss: 1.0412075519561768\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.049683094024658 | KNN Loss: 5.0173563957214355 | BCE Loss: 1.0323268175125122\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.084641456604004 | KNN Loss: 5.0338454246521 | BCE Loss: 1.0507962703704834\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.091433525085449 | KNN Loss: 5.032495498657227 | BCE Loss: 1.058937907218933\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.0506510734558105 | KNN Loss: 5.019128322601318 | BCE Loss: 1.0315226316452026\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.050268173217773 | KNN Loss: 5.037187576293945 | BCE Loss: 1.0130805969238281\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.058610916137695 | KNN Loss: 5.013995170593262 | BCE Loss: 1.0446159839630127\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.030401229858398 | KNN Loss: 5.014761924743652 | BCE Loss: 1.015639305114746\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.071345329284668 | KNN Loss: 5.024387836456299 | BCE Loss: 1.0469577312469482\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.127965927124023 | KNN Loss: 5.040400505065918 | BCE Loss: 1.0875656604766846\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.094686031341553 | KNN Loss: 5.062768459320068 | BCE Loss: 1.031917691230774\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.078824043273926 | KNN Loss: 5.022796630859375 | BCE Loss: 1.0560271739959717\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.057687759399414 | KNN Loss: 5.041037559509277 | BCE Loss: 1.0166499614715576\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.062688827514648 | KNN Loss: 5.0112385749816895 | BCE Loss: 1.051450252532959\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.077063083648682 | KNN Loss: 5.007027626037598 | BCE Loss: 1.0700353384017944\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.0393571853637695 | KNN Loss: 5.008879661560059 | BCE Loss: 1.0304776430130005\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.061534404754639 | KNN Loss: 5.022237777709961 | BCE Loss: 1.0392966270446777\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.139005184173584 | KNN Loss: 5.094992160797119 | BCE Loss: 1.0440129041671753\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.051070213317871 | KNN Loss: 5.0000529289245605 | BCE Loss: 1.0510174036026\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.114305019378662 | KNN Loss: 5.041778087615967 | BCE Loss: 1.0725269317626953\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.050152778625488 | KNN Loss: 5.021445274353027 | BCE Loss: 1.0287072658538818\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.060617923736572 | KNN Loss: 5.05033540725708 | BCE Loss: 1.0102823972702026\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.04409122467041 | KNN Loss: 5.0260725021362305 | BCE Loss: 1.0180189609527588\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.055474281311035 | KNN Loss: 5.011404991149902 | BCE Loss: 1.0440691709518433\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.043332099914551 | KNN Loss: 5.017566680908203 | BCE Loss: 1.0257654190063477\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.018200874328613 | KNN Loss: 4.99747896194458 | BCE Loss: 1.020721673965454\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.093253135681152 | KNN Loss: 5.062775135040283 | BCE Loss: 1.0304781198501587\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.051148414611816 | KNN Loss: 5.0027756690979 | BCE Loss: 1.048372507095337\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.054046630859375 | KNN Loss: 5.010763645172119 | BCE Loss: 1.0432827472686768\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.045377731323242 | KNN Loss: 5.006147861480713 | BCE Loss: 1.0392296314239502\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.020076751708984 | KNN Loss: 5.006814956665039 | BCE Loss: 1.0132615566253662\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.028093338012695 | KNN Loss: 5.040353775024414 | BCE Loss: 0.987739622592926\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.121971130371094 | KNN Loss: 5.081026077270508 | BCE Loss: 1.040945291519165\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.071315765380859 | KNN Loss: 5.028800010681152 | BCE Loss: 1.0425158739089966\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.034683704376221 | KNN Loss: 5.011643409729004 | BCE Loss: 1.0230401754379272\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.057930946350098 | KNN Loss: 5.016341686248779 | BCE Loss: 1.0415890216827393\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.050745487213135 | KNN Loss: 5.009133338928223 | BCE Loss: 1.0416120290756226\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.043371677398682 | KNN Loss: 5.029714584350586 | BCE Loss: 1.0136569738388062\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.147216320037842 | KNN Loss: 5.067904949188232 | BCE Loss: 1.0793113708496094\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.0289201736450195 | KNN Loss: 5.006315231323242 | BCE Loss: 1.0226047039031982\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.064517021179199 | KNN Loss: 5.015923023223877 | BCE Loss: 1.0485939979553223\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.044784069061279 | KNN Loss: 5.008162021636963 | BCE Loss: 1.0366220474243164\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.037524700164795 | KNN Loss: 5.0047149658203125 | BCE Loss: 1.032809853553772\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.067340850830078 | KNN Loss: 5.024382591247559 | BCE Loss: 1.0429584980010986\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.026098728179932 | KNN Loss: 5.008148193359375 | BCE Loss: 1.017950415611267\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.031205177307129 | KNN Loss: 5.006984710693359 | BCE Loss: 1.0242207050323486\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.0167646408081055 | KNN Loss: 5.032410144805908 | BCE Loss: 0.9843544960021973\n",
      "Epoch   405: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.077340126037598 | KNN Loss: 5.0277299880981445 | BCE Loss: 1.0496103763580322\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.037655830383301 | KNN Loss: 5.00808572769165 | BCE Loss: 1.0295703411102295\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.019034385681152 | KNN Loss: 5.002253532409668 | BCE Loss: 1.0167806148529053\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.074038505554199 | KNN Loss: 5.022624492645264 | BCE Loss: 1.0514140129089355\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.103514194488525 | KNN Loss: 5.058682918548584 | BCE Loss: 1.044831395149231\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.021017074584961 | KNN Loss: 5.022013187408447 | BCE Loss: 0.9990040063858032\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.11806058883667 | KNN Loss: 5.036385536193848 | BCE Loss: 1.0816749334335327\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 5.987863540649414 | KNN Loss: 5.003136157989502 | BCE Loss: 0.9847275018692017\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.085301876068115 | KNN Loss: 5.060502052307129 | BCE Loss: 1.0247999429702759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.032354354858398 | KNN Loss: 5.010759353637695 | BCE Loss: 1.0215950012207031\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.065715789794922 | KNN Loss: 5.002964973449707 | BCE Loss: 1.0627508163452148\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.03916072845459 | KNN Loss: 5.018344402313232 | BCE Loss: 1.020816445350647\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.052970886230469 | KNN Loss: 5.029860019683838 | BCE Loss: 1.0231107473373413\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.091856956481934 | KNN Loss: 5.0056610107421875 | BCE Loss: 1.086195945739746\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.0555548667907715 | KNN Loss: 5.015420913696289 | BCE Loss: 1.0401338338851929\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.064697265625 | KNN Loss: 4.999995708465576 | BCE Loss: 1.0647015571594238\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.042248725891113 | KNN Loss: 5.036573886871338 | BCE Loss: 1.0056747198104858\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.049548149108887 | KNN Loss: 5.033308982849121 | BCE Loss: 1.0162392854690552\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.02763032913208 | KNN Loss: 4.9982099533081055 | BCE Loss: 1.0294203758239746\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.053034782409668 | KNN Loss: 5.01622200012207 | BCE Loss: 1.036812663078308\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.041880130767822 | KNN Loss: 5.005311489105225 | BCE Loss: 1.0365686416625977\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.116222858428955 | KNN Loss: 5.086752891540527 | BCE Loss: 1.0294699668884277\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.113850116729736 | KNN Loss: 5.05031681060791 | BCE Loss: 1.0635333061218262\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.054084777832031 | KNN Loss: 5.0242133140563965 | BCE Loss: 1.0298717021942139\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.042928695678711 | KNN Loss: 4.996267318725586 | BCE Loss: 1.046661138534546\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.043269157409668 | KNN Loss: 5.010056495666504 | BCE Loss: 1.0332127809524536\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.048092842102051 | KNN Loss: 5.012535095214844 | BCE Loss: 1.0355579853057861\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.043787479400635 | KNN Loss: 5.005575656890869 | BCE Loss: 1.0382118225097656\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.03673791885376 | KNN Loss: 5.008824348449707 | BCE Loss: 1.0279134511947632\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.036192893981934 | KNN Loss: 5.003838539123535 | BCE Loss: 1.0323545932769775\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.036624431610107 | KNN Loss: 5.012240886688232 | BCE Loss: 1.0243834257125854\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.061366558074951 | KNN Loss: 5.049566745758057 | BCE Loss: 1.011799693107605\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.042873382568359 | KNN Loss: 5.010473728179932 | BCE Loss: 1.0323996543884277\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.03617000579834 | KNN Loss: 5.000643253326416 | BCE Loss: 1.0355266332626343\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.042510509490967 | KNN Loss: 5.013070106506348 | BCE Loss: 1.0294404029846191\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.039027214050293 | KNN Loss: 5.013723850250244 | BCE Loss: 1.0253032445907593\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.041077613830566 | KNN Loss: 5.001114845275879 | BCE Loss: 1.0399630069732666\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.0218400955200195 | KNN Loss: 5.015288829803467 | BCE Loss: 1.0065510272979736\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.042113304138184 | KNN Loss: 5.003297328948975 | BCE Loss: 1.038815975189209\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.053339958190918 | KNN Loss: 5.009334564208984 | BCE Loss: 1.0440056324005127\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.096949577331543 | KNN Loss: 5.054104328155518 | BCE Loss: 1.0428452491760254\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.0859246253967285 | KNN Loss: 5.0616135597229 | BCE Loss: 1.0243109464645386\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.031584739685059 | KNN Loss: 5.0108113288879395 | BCE Loss: 1.0207736492156982\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.0637664794921875 | KNN Loss: 5.007771015167236 | BCE Loss: 1.0559954643249512\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.043988227844238 | KNN Loss: 5.0344157218933105 | BCE Loss: 1.0095726251602173\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.032829284667969 | KNN Loss: 5.003148078918457 | BCE Loss: 1.0296812057495117\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.062682628631592 | KNN Loss: 5.044731616973877 | BCE Loss: 1.0179508924484253\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.074188709259033 | KNN Loss: 5.0194783210754395 | BCE Loss: 1.0547105073928833\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.065094947814941 | KNN Loss: 5.040224552154541 | BCE Loss: 1.0248702764511108\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.046470642089844 | KNN Loss: 5.014484405517578 | BCE Loss: 1.0319863557815552\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.058899402618408 | KNN Loss: 5.016628742218018 | BCE Loss: 1.0422706604003906\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.013434410095215 | KNN Loss: 5.005024433135986 | BCE Loss: 1.0084097385406494\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.0611114501953125 | KNN Loss: 5.033775329589844 | BCE Loss: 1.0273361206054688\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.053144454956055 | KNN Loss: 5.003055095672607 | BCE Loss: 1.0500893592834473\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.065613746643066 | KNN Loss: 5.0176167488098145 | BCE Loss: 1.0479967594146729\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.0179290771484375 | KNN Loss: 4.997933864593506 | BCE Loss: 1.0199954509735107\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.026926040649414 | KNN Loss: 5.013554096221924 | BCE Loss: 1.0133721828460693\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.0601911544799805 | KNN Loss: 4.9981818199157715 | BCE Loss: 1.062009334564209\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.0178608894348145 | KNN Loss: 5.010196208953857 | BCE Loss: 1.007664680480957\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.049698352813721 | KNN Loss: 4.9995574951171875 | BCE Loss: 1.0501407384872437\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.0318756103515625 | KNN Loss: 5.003763198852539 | BCE Loss: 1.0281126499176025\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.070216178894043 | KNN Loss: 5.010521411895752 | BCE Loss: 1.0596950054168701\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.05309534072876 | KNN Loss: 5.017669200897217 | BCE Loss: 1.0354260206222534\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.049213409423828 | KNN Loss: 5.030196189880371 | BCE Loss: 1.019016981124878\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.064216613769531 | KNN Loss: 5.022028923034668 | BCE Loss: 1.0421874523162842\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.09704065322876 | KNN Loss: 5.005910396575928 | BCE Loss: 1.0911301374435425\n",
      "Epoch   416: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.090432167053223 | KNN Loss: 5.041857719421387 | BCE Loss: 1.0485742092132568\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.023519039154053 | KNN Loss: 5.011379718780518 | BCE Loss: 1.0121394395828247\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.032045841217041 | KNN Loss: 5.008545398712158 | BCE Loss: 1.0235005617141724\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.058581352233887 | KNN Loss: 5.011676788330078 | BCE Loss: 1.046904444694519\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.0739264488220215 | KNN Loss: 5.008175849914551 | BCE Loss: 1.0657505989074707\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.068841934204102 | KNN Loss: 5.027825355529785 | BCE Loss: 1.0410168170928955\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.048361778259277 | KNN Loss: 5.007656097412109 | BCE Loss: 1.0407054424285889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.074240207672119 | KNN Loss: 5.030275821685791 | BCE Loss: 1.0439645051956177\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.041131973266602 | KNN Loss: 4.997561931610107 | BCE Loss: 1.043569803237915\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.030750751495361 | KNN Loss: 4.999375820159912 | BCE Loss: 1.0313748121261597\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.072046279907227 | KNN Loss: 5.025304794311523 | BCE Loss: 1.046741247177124\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.048450469970703 | KNN Loss: 5.025455951690674 | BCE Loss: 1.0229946374893188\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.081450462341309 | KNN Loss: 4.997242450714111 | BCE Loss: 1.0842081308364868\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.0456109046936035 | KNN Loss: 5.006505966186523 | BCE Loss: 1.03910493850708\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.02640438079834 | KNN Loss: 5.01438045501709 | BCE Loss: 1.01202392578125\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.00737190246582 | KNN Loss: 4.999690532684326 | BCE Loss: 1.0076814889907837\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.0610246658325195 | KNN Loss: 5.043524265289307 | BCE Loss: 1.017500638961792\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.0708088874816895 | KNN Loss: 5.042065620422363 | BCE Loss: 1.0287433862686157\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.058058261871338 | KNN Loss: 5.019589424133301 | BCE Loss: 1.038468837738037\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.091094017028809 | KNN Loss: 5.041775226593018 | BCE Loss: 1.049318552017212\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.04484748840332 | KNN Loss: 5.010321140289307 | BCE Loss: 1.0345261096954346\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.0191121101379395 | KNN Loss: 5.005067825317383 | BCE Loss: 1.014044165611267\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.065935134887695 | KNN Loss: 5.023745536804199 | BCE Loss: 1.042189598083496\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.072141170501709 | KNN Loss: 5.030538558959961 | BCE Loss: 1.041602611541748\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.020885944366455 | KNN Loss: 5.018796443939209 | BCE Loss: 1.002089500427246\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.015585899353027 | KNN Loss: 5.009677886962891 | BCE Loss: 1.0059081315994263\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.051661491394043 | KNN Loss: 5.021043300628662 | BCE Loss: 1.03061842918396\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.057843208312988 | KNN Loss: 5.01310920715332 | BCE Loss: 1.044734001159668\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.061522483825684 | KNN Loss: 5.011577129364014 | BCE Loss: 1.0499451160430908\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.091629981994629 | KNN Loss: 5.046821594238281 | BCE Loss: 1.0448085069656372\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.055551052093506 | KNN Loss: 5.021225929260254 | BCE Loss: 1.0343250036239624\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.060432434082031 | KNN Loss: 5.009120941162109 | BCE Loss: 1.0513112545013428\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.054832458496094 | KNN Loss: 5.015269756317139 | BCE Loss: 1.039562463760376\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.062270164489746 | KNN Loss: 5.018011093139648 | BCE Loss: 1.0442590713500977\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.019496440887451 | KNN Loss: 5.0140700340271 | BCE Loss: 1.005426287651062\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.1800737380981445 | KNN Loss: 5.11375093460083 | BCE Loss: 1.0663225650787354\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.05401611328125 | KNN Loss: 5.023177623748779 | BCE Loss: 1.0308384895324707\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.0609846115112305 | KNN Loss: 5.023436546325684 | BCE Loss: 1.0375481843948364\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.040628910064697 | KNN Loss: 5.014657020568848 | BCE Loss: 1.02597177028656\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.042749404907227 | KNN Loss: 4.997533321380615 | BCE Loss: 1.0452163219451904\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.047035217285156 | KNN Loss: 5.012375354766846 | BCE Loss: 1.0346596240997314\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.089303016662598 | KNN Loss: 5.023660182952881 | BCE Loss: 1.0656425952911377\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.046623229980469 | KNN Loss: 5.002965927124023 | BCE Loss: 1.0436575412750244\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.066699028015137 | KNN Loss: 5.030017375946045 | BCE Loss: 1.0366816520690918\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.029867172241211 | KNN Loss: 5.018192291259766 | BCE Loss: 1.0116746425628662\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.057977676391602 | KNN Loss: 5.014636516571045 | BCE Loss: 1.0433411598205566\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.030318737030029 | KNN Loss: 5.019285678863525 | BCE Loss: 1.011033058166504\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.04070520401001 | KNN Loss: 5.006629467010498 | BCE Loss: 1.0340756177902222\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.104276657104492 | KNN Loss: 5.0650715827941895 | BCE Loss: 1.0392048358917236\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.010390281677246 | KNN Loss: 5.011266708374023 | BCE Loss: 0.9991236329078674\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.0938334465026855 | KNN Loss: 5.058737754821777 | BCE Loss: 1.0350956916809082\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.064481735229492 | KNN Loss: 5.028148174285889 | BCE Loss: 1.0363335609436035\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.066401481628418 | KNN Loss: 5.055607795715332 | BCE Loss: 1.010793685913086\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.083852767944336 | KNN Loss: 5.02300500869751 | BCE Loss: 1.0608476400375366\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.044497489929199 | KNN Loss: 5.030974864959717 | BCE Loss: 1.0135228633880615\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.045450210571289 | KNN Loss: 5.006923675537109 | BCE Loss: 1.0385265350341797\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.029216766357422 | KNN Loss: 5.025961875915527 | BCE Loss: 1.003254771232605\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.053420543670654 | KNN Loss: 5.028172016143799 | BCE Loss: 1.0252485275268555\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.053103446960449 | KNN Loss: 5.003377914428711 | BCE Loss: 1.0497257709503174\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.013958930969238 | KNN Loss: 5.000583171844482 | BCE Loss: 1.0133757591247559\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.0543293952941895 | KNN Loss: 5.020468711853027 | BCE Loss: 1.033860683441162\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.023582458496094 | KNN Loss: 5.011663436889648 | BCE Loss: 1.0119190216064453\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.026583671569824 | KNN Loss: 5.014535427093506 | BCE Loss: 1.0120484828948975\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.07435941696167 | KNN Loss: 5.015102863311768 | BCE Loss: 1.0592565536499023\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.0370941162109375 | KNN Loss: 5.008565425872803 | BCE Loss: 1.0285284519195557\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.0702714920043945 | KNN Loss: 5.033775329589844 | BCE Loss: 1.0364960432052612\n",
      "Epoch   427: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.0357985496521 | KNN Loss: 5.0027174949646 | BCE Loss: 1.0330810546875\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.047142028808594 | KNN Loss: 5.0038161277771 | BCE Loss: 1.0433259010314941\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.046456336975098 | KNN Loss: 5.017428398132324 | BCE Loss: 1.0290279388427734\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.042659759521484 | KNN Loss: 5.01159143447876 | BCE Loss: 1.0310680866241455\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.06530237197876 | KNN Loss: 5.020758152008057 | BCE Loss: 1.0445442199707031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.038956642150879 | KNN Loss: 5.009698867797852 | BCE Loss: 1.0292580127716064\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.060854911804199 | KNN Loss: 5.0085530281066895 | BCE Loss: 1.0523018836975098\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.032310485839844 | KNN Loss: 5.006734371185303 | BCE Loss: 1.0255763530731201\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.07066011428833 | KNN Loss: 5.064802646636963 | BCE Loss: 1.0058575868606567\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.044219017028809 | KNN Loss: 5.006101608276367 | BCE Loss: 1.0381174087524414\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.09729528427124 | KNN Loss: 5.043058395385742 | BCE Loss: 1.054236888885498\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.025849342346191 | KNN Loss: 5.008677005767822 | BCE Loss: 1.0171725749969482\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.078924179077148 | KNN Loss: 5.033099174499512 | BCE Loss: 1.0458250045776367\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.044610500335693 | KNN Loss: 5.001247406005859 | BCE Loss: 1.0433632135391235\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.039648056030273 | KNN Loss: 5.010204315185547 | BCE Loss: 1.0294437408447266\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.083341598510742 | KNN Loss: 5.003857135772705 | BCE Loss: 1.079484224319458\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.094387054443359 | KNN Loss: 5.044059753417969 | BCE Loss: 1.0503270626068115\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.0660505294799805 | KNN Loss: 5.027185916900635 | BCE Loss: 1.0388643741607666\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.05969762802124 | KNN Loss: 5.005192279815674 | BCE Loss: 1.0545053482055664\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.064115524291992 | KNN Loss: 5.009321689605713 | BCE Loss: 1.0547938346862793\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.03755521774292 | KNN Loss: 5.008210182189941 | BCE Loss: 1.0293450355529785\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.037002086639404 | KNN Loss: 5.002079963684082 | BCE Loss: 1.0349222421646118\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.084591865539551 | KNN Loss: 5.038221836090088 | BCE Loss: 1.0463699102401733\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.0407795906066895 | KNN Loss: 5.003927707672119 | BCE Loss: 1.0368517637252808\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.068349361419678 | KNN Loss: 5.004029750823975 | BCE Loss: 1.0643196105957031\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.027390956878662 | KNN Loss: 5.015523910522461 | BCE Loss: 1.0118670463562012\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.058097839355469 | KNN Loss: 5.020709991455078 | BCE Loss: 1.0373880863189697\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.013454437255859 | KNN Loss: 5.0127177238464355 | BCE Loss: 1.0007367134094238\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.155963897705078 | KNN Loss: 5.067219257354736 | BCE Loss: 1.0887444019317627\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.049691200256348 | KNN Loss: 5.0063910484313965 | BCE Loss: 1.043299913406372\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.049740791320801 | KNN Loss: 5.036473274230957 | BCE Loss: 1.0132677555084229\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.020844459533691 | KNN Loss: 5.002705097198486 | BCE Loss: 1.0181396007537842\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.035717964172363 | KNN Loss: 5.009064674377441 | BCE Loss: 1.0266532897949219\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.037602424621582 | KNN Loss: 5.017988204956055 | BCE Loss: 1.0196142196655273\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.090445518493652 | KNN Loss: 5.046383380889893 | BCE Loss: 1.0440623760223389\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.091555595397949 | KNN Loss: 5.049505233764648 | BCE Loss: 1.0420502424240112\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.009784698486328 | KNN Loss: 5.003413200378418 | BCE Loss: 1.0063714981079102\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.078734397888184 | KNN Loss: 5.032348155975342 | BCE Loss: 1.0463862419128418\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.028549671173096 | KNN Loss: 5.004152297973633 | BCE Loss: 1.024397373199463\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.024030685424805 | KNN Loss: 5.004763603210449 | BCE Loss: 1.0192668437957764\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.073229789733887 | KNN Loss: 5.024552345275879 | BCE Loss: 1.0486772060394287\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.066075325012207 | KNN Loss: 5.013280868530273 | BCE Loss: 1.0527946949005127\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.0490827560424805 | KNN Loss: 5.029167652130127 | BCE Loss: 1.0199151039123535\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.057758331298828 | KNN Loss: 5.0179948806762695 | BCE Loss: 1.0397634506225586\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.067165374755859 | KNN Loss: 5.022021293640137 | BCE Loss: 1.0451443195343018\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.091856002807617 | KNN Loss: 5.0590081214904785 | BCE Loss: 1.0328477621078491\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.0319719314575195 | KNN Loss: 5.003190040588379 | BCE Loss: 1.0287816524505615\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.078624725341797 | KNN Loss: 5.0294189453125 | BCE Loss: 1.0492057800292969\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.056973934173584 | KNN Loss: 5.010406970977783 | BCE Loss: 1.0465668439865112\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.039815425872803 | KNN Loss: 5.011784553527832 | BCE Loss: 1.0280309915542603\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 5.998080253601074 | KNN Loss: 5.000022888183594 | BCE Loss: 0.9980573654174805\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.023871421813965 | KNN Loss: 5.0057053565979 | BCE Loss: 1.0181660652160645\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.032831192016602 | KNN Loss: 5.007305145263672 | BCE Loss: 1.0255258083343506\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.06817102432251 | KNN Loss: 5.0142645835876465 | BCE Loss: 1.0539065599441528\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.066811561584473 | KNN Loss: 5.034420013427734 | BCE Loss: 1.0323914289474487\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.051426410675049 | KNN Loss: 5.030954360961914 | BCE Loss: 1.0204720497131348\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.049069881439209 | KNN Loss: 5.025827407836914 | BCE Loss: 1.023242473602295\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.03928804397583 | KNN Loss: 5.013382911682129 | BCE Loss: 1.0259052515029907\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.050615310668945 | KNN Loss: 5.014268398284912 | BCE Loss: 1.036346673965454\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.050518989562988 | KNN Loss: 5.023105621337891 | BCE Loss: 1.0274133682250977\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.046943664550781 | KNN Loss: 5.008834362030029 | BCE Loss: 1.038109302520752\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.041093826293945 | KNN Loss: 5.01528787612915 | BCE Loss: 1.0258060693740845\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.072195529937744 | KNN Loss: 5.031421661376953 | BCE Loss: 1.040773868560791\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.064815521240234 | KNN Loss: 5.015439987182617 | BCE Loss: 1.0493756532669067\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.071977615356445 | KNN Loss: 5.0054802894592285 | BCE Loss: 1.0664970874786377\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.0743608474731445 | KNN Loss: 5.027828693389893 | BCE Loss: 1.0465319156646729\n",
      "Epoch   438: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.067026138305664 | KNN Loss: 5.036520481109619 | BCE Loss: 1.0305054187774658\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.059175491333008 | KNN Loss: 5.007169723510742 | BCE Loss: 1.0520058870315552\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.054906368255615 | KNN Loss: 5.00808572769165 | BCE Loss: 1.0468205213546753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.029055118560791 | KNN Loss: 5.0061211585998535 | BCE Loss: 1.0229339599609375\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.094236373901367 | KNN Loss: 5.068244457244873 | BCE Loss: 1.0259921550750732\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.056457996368408 | KNN Loss: 5.011569976806641 | BCE Loss: 1.0448880195617676\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.049798011779785 | KNN Loss: 5.009235382080078 | BCE Loss: 1.0405628681182861\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.040761947631836 | KNN Loss: 5.00249719619751 | BCE Loss: 1.0382648706436157\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.032158851623535 | KNN Loss: 5.015150547027588 | BCE Loss: 1.0170080661773682\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.0204758644104 | KNN Loss: 5.023492336273193 | BCE Loss: 0.996983528137207\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.043042182922363 | KNN Loss: 4.9979705810546875 | BCE Loss: 1.0450713634490967\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.074311256408691 | KNN Loss: 5.031733512878418 | BCE Loss: 1.0425776243209839\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.042160511016846 | KNN Loss: 5.020245552062988 | BCE Loss: 1.0219148397445679\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.052895545959473 | KNN Loss: 5.005531311035156 | BCE Loss: 1.0473641157150269\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.018645763397217 | KNN Loss: 5.014303207397461 | BCE Loss: 1.0043424367904663\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.043130874633789 | KNN Loss: 5.012157440185547 | BCE Loss: 1.0309736728668213\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.093304634094238 | KNN Loss: 5.029759407043457 | BCE Loss: 1.0635449886322021\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.025309085845947 | KNN Loss: 4.995394229888916 | BCE Loss: 1.0299148559570312\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.057690143585205 | KNN Loss: 5.0141282081604 | BCE Loss: 1.0435619354248047\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.090892314910889 | KNN Loss: 5.033553600311279 | BCE Loss: 1.0573387145996094\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.06143045425415 | KNN Loss: 5.006007194519043 | BCE Loss: 1.055423378944397\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.121094226837158 | KNN Loss: 5.073503017425537 | BCE Loss: 1.0475913286209106\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.007312774658203 | KNN Loss: 5.00755500793457 | BCE Loss: 0.9997575879096985\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.092926502227783 | KNN Loss: 5.027085304260254 | BCE Loss: 1.0658411979675293\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.029595375061035 | KNN Loss: 5.0080718994140625 | BCE Loss: 1.0215237140655518\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.0797576904296875 | KNN Loss: 5.030073165893555 | BCE Loss: 1.0496842861175537\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.083042144775391 | KNN Loss: 5.046585559844971 | BCE Loss: 1.036456823348999\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.062868118286133 | KNN Loss: 5.008060455322266 | BCE Loss: 1.0548076629638672\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.061553955078125 | KNN Loss: 5.034242630004883 | BCE Loss: 1.027311086654663\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.093928813934326 | KNN Loss: 5.05323600769043 | BCE Loss: 1.0406928062438965\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.057308197021484 | KNN Loss: 5.010824680328369 | BCE Loss: 1.0464837551116943\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.068534851074219 | KNN Loss: 5.0287652015686035 | BCE Loss: 1.0397696495056152\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.055820465087891 | KNN Loss: 5.018927097320557 | BCE Loss: 1.0368934869766235\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.065377712249756 | KNN Loss: 5.038934230804443 | BCE Loss: 1.0264434814453125\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.041165351867676 | KNN Loss: 5.015192031860352 | BCE Loss: 1.0259733200073242\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.033633232116699 | KNN Loss: 4.9984211921691895 | BCE Loss: 1.0352122783660889\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.042346477508545 | KNN Loss: 5.006319046020508 | BCE Loss: 1.0360275506973267\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.027851581573486 | KNN Loss: 5.009013652801514 | BCE Loss: 1.018837809562683\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.078492164611816 | KNN Loss: 5.030439376831055 | BCE Loss: 1.0480525493621826\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.0583343505859375 | KNN Loss: 5.00277042388916 | BCE Loss: 1.0555639266967773\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.065449237823486 | KNN Loss: 5.028482437133789 | BCE Loss: 1.0369669198989868\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.067368507385254 | KNN Loss: 5.0060648918151855 | BCE Loss: 1.0613036155700684\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.089101314544678 | KNN Loss: 5.045214653015137 | BCE Loss: 1.0438867807388306\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.054758071899414 | KNN Loss: 5.006033420562744 | BCE Loss: 1.0487245321273804\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.041924476623535 | KNN Loss: 5.028995037078857 | BCE Loss: 1.0129292011260986\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.070069313049316 | KNN Loss: 5.015200138092041 | BCE Loss: 1.0548691749572754\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.052124500274658 | KNN Loss: 5.021759033203125 | BCE Loss: 1.0303655862808228\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.070687294006348 | KNN Loss: 5.02457857131958 | BCE Loss: 1.0461089611053467\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.024105548858643 | KNN Loss: 5.0029988288879395 | BCE Loss: 1.0211067199707031\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.052581787109375 | KNN Loss: 5.038015365600586 | BCE Loss: 1.0145666599273682\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.051314353942871 | KNN Loss: 5.043994426727295 | BCE Loss: 1.0073198080062866\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.016961097717285 | KNN Loss: 4.9995503425598145 | BCE Loss: 1.0174108743667603\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.054858207702637 | KNN Loss: 5.014225959777832 | BCE Loss: 1.0406324863433838\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.087104320526123 | KNN Loss: 5.024109363555908 | BCE Loss: 1.0629948377609253\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.032855987548828 | KNN Loss: 5.006413459777832 | BCE Loss: 1.026442289352417\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.127978801727295 | KNN Loss: 5.086215972900391 | BCE Loss: 1.0417628288269043\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.067338466644287 | KNN Loss: 5.005401611328125 | BCE Loss: 1.0619367361068726\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.039280891418457 | KNN Loss: 5.000284671783447 | BCE Loss: 1.0389964580535889\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.06519889831543 | KNN Loss: 5.048670768737793 | BCE Loss: 1.0165281295776367\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.061875343322754 | KNN Loss: 5.047381401062012 | BCE Loss: 1.0144939422607422\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.0803046226501465 | KNN Loss: 5.016837120056152 | BCE Loss: 1.0634675025939941\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.080595016479492 | KNN Loss: 5.042736053466797 | BCE Loss: 1.0378587245941162\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.051285743713379 | KNN Loss: 4.9954400062561035 | BCE Loss: 1.0558456182479858\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.092802047729492 | KNN Loss: 5.023929119110107 | BCE Loss: 1.0688726902008057\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.043924331665039 | KNN Loss: 5.021115779876709 | BCE Loss: 1.0228087902069092\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.033815383911133 | KNN Loss: 5.018208980560303 | BCE Loss: 1.015606164932251\n",
      "Epoch   449: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.063269138336182 | KNN Loss: 5.016005992889404 | BCE Loss: 1.0472631454467773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.048320770263672 | KNN Loss: 5.015078544616699 | BCE Loss: 1.033242106437683\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.057239532470703 | KNN Loss: 5.008315563201904 | BCE Loss: 1.0489239692687988\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.038585662841797 | KNN Loss: 5.007270336151123 | BCE Loss: 1.0313154458999634\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.0623016357421875 | KNN Loss: 5.033756256103516 | BCE Loss: 1.028545618057251\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.041375160217285 | KNN Loss: 5.020190715789795 | BCE Loss: 1.0211846828460693\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.0682454109191895 | KNN Loss: 5.044124126434326 | BCE Loss: 1.0241211652755737\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.0270466804504395 | KNN Loss: 5.012785911560059 | BCE Loss: 1.0142606496810913\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.047994136810303 | KNN Loss: 5.017503261566162 | BCE Loss: 1.0304908752441406\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.074334621429443 | KNN Loss: 5.020718574523926 | BCE Loss: 1.053615927696228\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.065285682678223 | KNN Loss: 5.026178359985352 | BCE Loss: 1.039107084274292\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.092691421508789 | KNN Loss: 5.032946586608887 | BCE Loss: 1.0597448348999023\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.038722038269043 | KNN Loss: 5.000330448150635 | BCE Loss: 1.0383918285369873\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.0348286628723145 | KNN Loss: 5.021232604980469 | BCE Loss: 1.0135959386825562\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.032331466674805 | KNN Loss: 5.024305820465088 | BCE Loss: 1.0080254077911377\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.038177490234375 | KNN Loss: 5.0239434242248535 | BCE Loss: 1.0142338275909424\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.053845405578613 | KNN Loss: 5.057997226715088 | BCE Loss: 0.9958482980728149\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.079928398132324 | KNN Loss: 5.023969650268555 | BCE Loss: 1.0559585094451904\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.071172714233398 | KNN Loss: 5.005536079406738 | BCE Loss: 1.0656366348266602\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.083885192871094 | KNN Loss: 5.0326642990112305 | BCE Loss: 1.0512206554412842\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.041378974914551 | KNN Loss: 5.012283802032471 | BCE Loss: 1.02909517288208\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.053535461425781 | KNN Loss: 5.011473178863525 | BCE Loss: 1.0420624017715454\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.07025671005249 | KNN Loss: 5.0028533935546875 | BCE Loss: 1.0674033164978027\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.083503246307373 | KNN Loss: 5.039475917816162 | BCE Loss: 1.044027328491211\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.057280540466309 | KNN Loss: 5.02180814743042 | BCE Loss: 1.0354723930358887\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.056944370269775 | KNN Loss: 5.02261209487915 | BCE Loss: 1.034332275390625\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.0271430015563965 | KNN Loss: 4.995201587677002 | BCE Loss: 1.0319414138793945\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.056365013122559 | KNN Loss: 5.001060962677002 | BCE Loss: 1.0553038120269775\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.02731990814209 | KNN Loss: 5.016409873962402 | BCE Loss: 1.0109097957611084\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.032127380371094 | KNN Loss: 5.0156660079956055 | BCE Loss: 1.0164613723754883\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.053718566894531 | KNN Loss: 5.0079665184021 | BCE Loss: 1.0457520484924316\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.060346603393555 | KNN Loss: 5.051368713378906 | BCE Loss: 1.0089778900146484\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.0401716232299805 | KNN Loss: 4.999972820281982 | BCE Loss: 1.040198564529419\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.036340713500977 | KNN Loss: 5.011875629425049 | BCE Loss: 1.0244648456573486\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.057300090789795 | KNN Loss: 5.023011207580566 | BCE Loss: 1.0342888832092285\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.062046527862549 | KNN Loss: 5.004724502563477 | BCE Loss: 1.0573221445083618\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.060829162597656 | KNN Loss: 5.005502700805664 | BCE Loss: 1.0553264617919922\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.065666198730469 | KNN Loss: 5.049530982971191 | BCE Loss: 1.016135334968567\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.054356575012207 | KNN Loss: 5.0231218338012695 | BCE Loss: 1.031234860420227\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.092541217803955 | KNN Loss: 5.030252456665039 | BCE Loss: 1.0622888803482056\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.043390274047852 | KNN Loss: 5.026244640350342 | BCE Loss: 1.0171456336975098\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.046693801879883 | KNN Loss: 5.032000541687012 | BCE Loss: 1.0146934986114502\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.030487537384033 | KNN Loss: 4.995924472808838 | BCE Loss: 1.0345630645751953\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.0370378494262695 | KNN Loss: 5.002304553985596 | BCE Loss: 1.034733533859253\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.044440269470215 | KNN Loss: 5.002842903137207 | BCE Loss: 1.0415973663330078\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.036905288696289 | KNN Loss: 5.021889686584473 | BCE Loss: 1.0150158405303955\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.0826520919799805 | KNN Loss: 5.014080047607422 | BCE Loss: 1.068571925163269\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.074993133544922 | KNN Loss: 5.02332878112793 | BCE Loss: 1.0516645908355713\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.049707412719727 | KNN Loss: 5.028412342071533 | BCE Loss: 1.0212948322296143\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.024267673492432 | KNN Loss: 5.002742290496826 | BCE Loss: 1.0215253829956055\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.059970855712891 | KNN Loss: 5.018207550048828 | BCE Loss: 1.0417635440826416\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.047906875610352 | KNN Loss: 5.011748790740967 | BCE Loss: 1.0361580848693848\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.018298149108887 | KNN Loss: 5.0236310958862305 | BCE Loss: 0.9946669340133667\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.049687385559082 | KNN Loss: 4.99678897857666 | BCE Loss: 1.0528981685638428\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.02346658706665 | KNN Loss: 5.0004353523254395 | BCE Loss: 1.0230311155319214\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.031846523284912 | KNN Loss: 5.022683620452881 | BCE Loss: 1.0091630220413208\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.079137325286865 | KNN Loss: 5.012863636016846 | BCE Loss: 1.06627357006073\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.030673503875732 | KNN Loss: 4.997532367706299 | BCE Loss: 1.0331411361694336\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.063206672668457 | KNN Loss: 5.0141987800598145 | BCE Loss: 1.0490080118179321\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.035063743591309 | KNN Loss: 5.006450653076172 | BCE Loss: 1.0286133289337158\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.06389045715332 | KNN Loss: 5.002669811248779 | BCE Loss: 1.0612208843231201\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.081643104553223 | KNN Loss: 5.046660900115967 | BCE Loss: 1.0349820852279663\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.12198543548584 | KNN Loss: 5.046727180480957 | BCE Loss: 1.0752583742141724\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.016909122467041 | KNN Loss: 5.029828071594238 | BCE Loss: 0.9870812296867371\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.074823379516602 | KNN Loss: 5.020149230957031 | BCE Loss: 1.0546741485595703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.041540145874023 | KNN Loss: 5.014594078063965 | BCE Loss: 1.0269463062286377\n",
      "Epoch   460: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.061495304107666 | KNN Loss: 5.018918514251709 | BCE Loss: 1.042576789855957\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.020200252532959 | KNN Loss: 5.017342567443848 | BCE Loss: 1.0028576850891113\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.063215255737305 | KNN Loss: 5.003366470336914 | BCE Loss: 1.0598487854003906\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.055025100708008 | KNN Loss: 5.043058395385742 | BCE Loss: 1.0119664669036865\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.0618367195129395 | KNN Loss: 5.034313201904297 | BCE Loss: 1.0275235176086426\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.017180919647217 | KNN Loss: 4.99492883682251 | BCE Loss: 1.022252082824707\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.0490803718566895 | KNN Loss: 5.022019386291504 | BCE Loss: 1.0270609855651855\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.007081031799316 | KNN Loss: 5.00160551071167 | BCE Loss: 1.0054757595062256\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.104789733886719 | KNN Loss: 5.009181976318359 | BCE Loss: 1.095607876777649\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.057031154632568 | KNN Loss: 5.012741565704346 | BCE Loss: 1.0442895889282227\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.047312259674072 | KNN Loss: 5.003746509552002 | BCE Loss: 1.0435657501220703\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.070363998413086 | KNN Loss: 5.022653102874756 | BCE Loss: 1.047710657119751\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.100767612457275 | KNN Loss: 5.053250789642334 | BCE Loss: 1.0475168228149414\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.077364921569824 | KNN Loss: 5.022379398345947 | BCE Loss: 1.054985523223877\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.084081649780273 | KNN Loss: 5.029874801635742 | BCE Loss: 1.0542067289352417\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.061943054199219 | KNN Loss: 5.007853031158447 | BCE Loss: 1.054089903831482\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.046133995056152 | KNN Loss: 5.013735771179199 | BCE Loss: 1.032397985458374\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.098464012145996 | KNN Loss: 5.059311389923096 | BCE Loss: 1.0391528606414795\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.045690536499023 | KNN Loss: 5.014441967010498 | BCE Loss: 1.0312484502792358\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.050522327423096 | KNN Loss: 5.001949787139893 | BCE Loss: 1.0485725402832031\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.020400047302246 | KNN Loss: 5.019577503204346 | BCE Loss: 1.0008225440979004\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.069735527038574 | KNN Loss: 5.027691841125488 | BCE Loss: 1.0420434474945068\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.037134170532227 | KNN Loss: 5.010343074798584 | BCE Loss: 1.0267910957336426\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.063229560852051 | KNN Loss: 5.008840084075928 | BCE Loss: 1.0543897151947021\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.028992176055908 | KNN Loss: 5.009853363037109 | BCE Loss: 1.0191388130187988\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.070948600769043 | KNN Loss: 5.051502704620361 | BCE Loss: 1.0194460153579712\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.06784725189209 | KNN Loss: 5.041903018951416 | BCE Loss: 1.0259442329406738\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.069744110107422 | KNN Loss: 5.005100250244141 | BCE Loss: 1.0646440982818604\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.140363693237305 | KNN Loss: 5.098138332366943 | BCE Loss: 1.0422255992889404\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.041187763214111 | KNN Loss: 5.011049270629883 | BCE Loss: 1.0301384925842285\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.075614929199219 | KNN Loss: 5.046461582183838 | BCE Loss: 1.0291531085968018\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.053640365600586 | KNN Loss: 5.022939205169678 | BCE Loss: 1.030700922012329\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.056680202484131 | KNN Loss: 5.0115461349487305 | BCE Loss: 1.0451339483261108\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.025157451629639 | KNN Loss: 5.002530574798584 | BCE Loss: 1.0226269960403442\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.074071884155273 | KNN Loss: 5.033799171447754 | BCE Loss: 1.040272831916809\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.057443618774414 | KNN Loss: 5.014902591705322 | BCE Loss: 1.042541265487671\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.070686340332031 | KNN Loss: 5.028087139129639 | BCE Loss: 1.0425992012023926\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.075771331787109 | KNN Loss: 5.008194446563721 | BCE Loss: 1.0675768852233887\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.017847061157227 | KNN Loss: 5.005435943603516 | BCE Loss: 1.0124108791351318\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.045673847198486 | KNN Loss: 5.012229919433594 | BCE Loss: 1.0334439277648926\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.05900764465332 | KNN Loss: 5.003346920013428 | BCE Loss: 1.0556607246398926\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.078808307647705 | KNN Loss: 5.000641345977783 | BCE Loss: 1.0781670808792114\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.073349952697754 | KNN Loss: 5.022703647613525 | BCE Loss: 1.0506465435028076\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.052975654602051 | KNN Loss: 4.998000144958496 | BCE Loss: 1.0549753904342651\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.078671455383301 | KNN Loss: 5.03018856048584 | BCE Loss: 1.048482894897461\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.0649919509887695 | KNN Loss: 5.024370193481445 | BCE Loss: 1.0406217575073242\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.061573028564453 | KNN Loss: 5.0028076171875 | BCE Loss: 1.058765172958374\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.087059497833252 | KNN Loss: 5.053158760070801 | BCE Loss: 1.0339008569717407\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.087116718292236 | KNN Loss: 5.008343696594238 | BCE Loss: 1.078773021697998\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.055034637451172 | KNN Loss: 5.013869285583496 | BCE Loss: 1.0411652326583862\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.071012496948242 | KNN Loss: 5.002897262573242 | BCE Loss: 1.068115234375\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.057635307312012 | KNN Loss: 5.002411365509033 | BCE Loss: 1.0552239418029785\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.053633689880371 | KNN Loss: 5.043658256530762 | BCE Loss: 1.0099751949310303\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.047088623046875 | KNN Loss: 5.031880855560303 | BCE Loss: 1.0152075290679932\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.060328483581543 | KNN Loss: 5.032272815704346 | BCE Loss: 1.0280556678771973\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.0676774978637695 | KNN Loss: 5.010744094848633 | BCE Loss: 1.0569336414337158\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.042518615722656 | KNN Loss: 5.042742729187012 | BCE Loss: 0.9997757077217102\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.072439193725586 | KNN Loss: 5.01668643951416 | BCE Loss: 1.0557529926300049\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.091739654541016 | KNN Loss: 5.021896839141846 | BCE Loss: 1.06984281539917\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.032812118530273 | KNN Loss: 5.012256622314453 | BCE Loss: 1.0205557346343994\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.05601692199707 | KNN Loss: 5.010397434234619 | BCE Loss: 1.0456194877624512\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.0331830978393555 | KNN Loss: 5.030944347381592 | BCE Loss: 1.0022387504577637\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.024330139160156 | KNN Loss: 5.011893272399902 | BCE Loss: 1.0124366283416748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.034457206726074 | KNN Loss: 4.995636940002441 | BCE Loss: 1.038820505142212\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.063051700592041 | KNN Loss: 5.032251358032227 | BCE Loss: 1.0308003425598145\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.077188491821289 | KNN Loss: 5.054178237915039 | BCE Loss: 1.023010492324829\n",
      "Epoch   471: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.084349632263184 | KNN Loss: 5.0446295738220215 | BCE Loss: 1.039720058441162\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.040957927703857 | KNN Loss: 5.012941360473633 | BCE Loss: 1.0280165672302246\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.046894550323486 | KNN Loss: 5.032792091369629 | BCE Loss: 1.0141024589538574\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.071711540222168 | KNN Loss: 5.06589412689209 | BCE Loss: 1.0058176517486572\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.045541763305664 | KNN Loss: 5.023704528808594 | BCE Loss: 1.0218369960784912\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.056600093841553 | KNN Loss: 5.018723487854004 | BCE Loss: 1.0378764867782593\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.011735439300537 | KNN Loss: 4.993157863616943 | BCE Loss: 1.0185774564743042\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.051826000213623 | KNN Loss: 5.013508319854736 | BCE Loss: 1.0383176803588867\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.090532302856445 | KNN Loss: 5.020362377166748 | BCE Loss: 1.0701699256896973\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.045357704162598 | KNN Loss: 5.022447109222412 | BCE Loss: 1.0229108333587646\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.016502857208252 | KNN Loss: 5.014279365539551 | BCE Loss: 1.0022233724594116\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.032756328582764 | KNN Loss: 5.005102634429932 | BCE Loss: 1.0276535749435425\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.084140777587891 | KNN Loss: 5.0409064292907715 | BCE Loss: 1.0432344675064087\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.045530796051025 | KNN Loss: 5.0286054611206055 | BCE Loss: 1.0169254541397095\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.0670166015625 | KNN Loss: 5.0307817459106445 | BCE Loss: 1.0362346172332764\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.074072360992432 | KNN Loss: 5.02295446395874 | BCE Loss: 1.0511178970336914\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.043770790100098 | KNN Loss: 5.0068488121032715 | BCE Loss: 1.0369218587875366\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.0503740310668945 | KNN Loss: 5.025181770324707 | BCE Loss: 1.025192379951477\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.042294979095459 | KNN Loss: 5.015442848205566 | BCE Loss: 1.0268522500991821\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.051729679107666 | KNN Loss: 4.999456405639648 | BCE Loss: 1.0522733926773071\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.017287254333496 | KNN Loss: 5.011602401733398 | BCE Loss: 1.0056848526000977\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.086974620819092 | KNN Loss: 5.061431407928467 | BCE Loss: 1.025543212890625\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.087074279785156 | KNN Loss: 5.016119480133057 | BCE Loss: 1.0709550380706787\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.080070495605469 | KNN Loss: 5.0435991287231445 | BCE Loss: 1.0364716053009033\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.043612957000732 | KNN Loss: 5.010919094085693 | BCE Loss: 1.032693862915039\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.0428786277771 | KNN Loss: 5.027104377746582 | BCE Loss: 1.0157742500305176\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.035231590270996 | KNN Loss: 5.024089813232422 | BCE Loss: 1.0111418962478638\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.046238899230957 | KNN Loss: 5.01416540145874 | BCE Loss: 1.0320732593536377\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.054434776306152 | KNN Loss: 5.003234386444092 | BCE Loss: 1.0512006282806396\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.055591106414795 | KNN Loss: 5.034006118774414 | BCE Loss: 1.0215849876403809\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.0545783042907715 | KNN Loss: 5.008162498474121 | BCE Loss: 1.0464156866073608\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.163094520568848 | KNN Loss: 5.093061447143555 | BCE Loss: 1.070033311843872\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.03318452835083 | KNN Loss: 5.004354000091553 | BCE Loss: 1.0288305282592773\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.050188064575195 | KNN Loss: 5.03294038772583 | BCE Loss: 1.0172474384307861\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.03449010848999 | KNN Loss: 5.009087562561035 | BCE Loss: 1.025402545928955\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.072381019592285 | KNN Loss: 5.0164361000061035 | BCE Loss: 1.0559449195861816\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.1092023849487305 | KNN Loss: 5.024281024932861 | BCE Loss: 1.0849215984344482\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.066441535949707 | KNN Loss: 5.02260160446167 | BCE Loss: 1.043839693069458\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.0460662841796875 | KNN Loss: 5.018415451049805 | BCE Loss: 1.0276505947113037\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.014555931091309 | KNN Loss: 5.011571884155273 | BCE Loss: 1.0029842853546143\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.026606559753418 | KNN Loss: 5.009830474853516 | BCE Loss: 1.0167758464813232\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.046144008636475 | KNN Loss: 5.003110408782959 | BCE Loss: 1.043033480644226\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.134147644042969 | KNN Loss: 5.0785441398620605 | BCE Loss: 1.0556035041809082\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.058906555175781 | KNN Loss: 5.013529300689697 | BCE Loss: 1.045377254486084\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.029562950134277 | KNN Loss: 5.025347709655762 | BCE Loss: 1.0042150020599365\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.112112045288086 | KNN Loss: 5.06243371963501 | BCE Loss: 1.049678087234497\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.064422607421875 | KNN Loss: 5.032291889190674 | BCE Loss: 1.0321307182312012\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.034309387207031 | KNN Loss: 5.004914283752441 | BCE Loss: 1.0293951034545898\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.064037322998047 | KNN Loss: 5.030077934265137 | BCE Loss: 1.033959150314331\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.050209999084473 | KNN Loss: 5.020720958709717 | BCE Loss: 1.029489278793335\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.097196578979492 | KNN Loss: 5.039626121520996 | BCE Loss: 1.057570219039917\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.047502517700195 | KNN Loss: 5.0330023765563965 | BCE Loss: 1.0144999027252197\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.021256446838379 | KNN Loss: 5.004695892333984 | BCE Loss: 1.0165605545043945\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.027621746063232 | KNN Loss: 5.026626110076904 | BCE Loss: 1.0009957551956177\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.039846420288086 | KNN Loss: 5.012062072753906 | BCE Loss: 1.0277845859527588\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.079284191131592 | KNN Loss: 5.0278520584106445 | BCE Loss: 1.0514321327209473\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.068554878234863 | KNN Loss: 5.0280961990356445 | BCE Loss: 1.0404584407806396\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.07393741607666 | KNN Loss: 5.029582500457764 | BCE Loss: 1.0443551540374756\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.06125545501709 | KNN Loss: 5.0124287605285645 | BCE Loss: 1.0488265752792358\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.018455505371094 | KNN Loss: 5.019495010375977 | BCE Loss: 0.998960554599762\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.028006553649902 | KNN Loss: 5.006046772003174 | BCE Loss: 1.021959662437439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.049658298492432 | KNN Loss: 5.0222487449646 | BCE Loss: 1.027409553527832\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.035114288330078 | KNN Loss: 5.024486541748047 | BCE Loss: 1.0106276273727417\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.125916481018066 | KNN Loss: 5.033674716949463 | BCE Loss: 1.0922417640686035\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.008894443511963 | KNN Loss: 4.996998310089111 | BCE Loss: 1.0118961334228516\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.024177551269531 | KNN Loss: 5.020304203033447 | BCE Loss: 1.0038731098175049\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.044114112854004 | KNN Loss: 5.003721237182617 | BCE Loss: 1.0403928756713867\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.095734596252441 | KNN Loss: 5.062690734863281 | BCE Loss: 1.0330440998077393\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.028258323669434 | KNN Loss: 5.004518032073975 | BCE Loss: 1.0237400531768799\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.021759986877441 | KNN Loss: 5.000604152679443 | BCE Loss: 1.0211560726165771\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.08563232421875 | KNN Loss: 5.026885032653809 | BCE Loss: 1.0587470531463623\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.043113708496094 | KNN Loss: 5.017726421356201 | BCE Loss: 1.0253870487213135\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.036145210266113 | KNN Loss: 5.004453659057617 | BCE Loss: 1.031691551208496\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.053632736206055 | KNN Loss: 5.006322383880615 | BCE Loss: 1.047310471534729\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.0537800788879395 | KNN Loss: 5.013874530792236 | BCE Loss: 1.0399055480957031\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.0374908447265625 | KNN Loss: 5.0247802734375 | BCE Loss: 1.0127108097076416\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.038612365722656 | KNN Loss: 5.01093864440918 | BCE Loss: 1.0276739597320557\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.040497779846191 | KNN Loss: 5.022220611572266 | BCE Loss: 1.0182769298553467\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.054079055786133 | KNN Loss: 5.016127109527588 | BCE Loss: 1.037952184677124\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.027433395385742 | KNN Loss: 5.014697074890137 | BCE Loss: 1.0127365589141846\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.050304412841797 | KNN Loss: 5.017164707183838 | BCE Loss: 1.033139944076538\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.052585124969482 | KNN Loss: 5.009331703186035 | BCE Loss: 1.0432535409927368\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.0699944496154785 | KNN Loss: 5.0465240478515625 | BCE Loss: 1.023470401763916\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.049289703369141 | KNN Loss: 5.004240036010742 | BCE Loss: 1.0450494289398193\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.050007343292236 | KNN Loss: 5.019463539123535 | BCE Loss: 1.0305439233779907\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.0565643310546875 | KNN Loss: 5.018947601318359 | BCE Loss: 1.0376169681549072\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.050338268280029 | KNN Loss: 5.017074108123779 | BCE Loss: 1.03326416015625\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.033984661102295 | KNN Loss: 4.998776435852051 | BCE Loss: 1.0352081060409546\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.0856428146362305 | KNN Loss: 5.023827075958252 | BCE Loss: 1.0618159770965576\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.077239990234375 | KNN Loss: 5.061939239501953 | BCE Loss: 1.015300989151001\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.071362495422363 | KNN Loss: 5.013347148895264 | BCE Loss: 1.05801522731781\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.020569801330566 | KNN Loss: 5.004110336303711 | BCE Loss: 1.0164597034454346\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.020052909851074 | KNN Loss: 5.002894401550293 | BCE Loss: 1.0171582698822021\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.055301189422607 | KNN Loss: 5.0172529220581055 | BCE Loss: 1.0380483865737915\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.076501846313477 | KNN Loss: 4.999429702758789 | BCE Loss: 1.0770723819732666\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.0640363693237305 | KNN Loss: 5.030235767364502 | BCE Loss: 1.033800482749939\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.049374103546143 | KNN Loss: 4.9988274574279785 | BCE Loss: 1.0505465269088745\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.045697212219238 | KNN Loss: 5.006664276123047 | BCE Loss: 1.0390326976776123\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.0680108070373535 | KNN Loss: 5.03639030456543 | BCE Loss: 1.0316205024719238\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.0369977951049805 | KNN Loss: 5.021820068359375 | BCE Loss: 1.0151777267456055\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.097540855407715 | KNN Loss: 5.059093475341797 | BCE Loss: 1.0384471416473389\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.0293354988098145 | KNN Loss: 5.019068241119385 | BCE Loss: 1.0102671384811401\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.042449951171875 | KNN Loss: 5.007938385009766 | BCE Loss: 1.0345115661621094\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.047317028045654 | KNN Loss: 5.017795085906982 | BCE Loss: 1.0295219421386719\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.076835632324219 | KNN Loss: 5.020493030548096 | BCE Loss: 1.0563424825668335\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.113224983215332 | KNN Loss: 5.066627502441406 | BCE Loss: 1.0465974807739258\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.022472381591797 | KNN Loss: 5.02095365524292 | BCE Loss: 1.0015184879302979\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.091501235961914 | KNN Loss: 5.040850639343262 | BCE Loss: 1.0506503582000732\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.109192848205566 | KNN Loss: 5.098023414611816 | BCE Loss: 1.011169195175171\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.031900405883789 | KNN Loss: 4.99445104598999 | BCE Loss: 1.0374492406845093\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.06447696685791 | KNN Loss: 5.004348278045654 | BCE Loss: 1.0601288080215454\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.103692054748535 | KNN Loss: 5.059077739715576 | BCE Loss: 1.0446144342422485\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.031883239746094 | KNN Loss: 5.017829895019531 | BCE Loss: 1.014053225517273\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.043185234069824 | KNN Loss: 5.012581825256348 | BCE Loss: 1.0306034088134766\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.111140251159668 | KNN Loss: 5.0771331787109375 | BCE Loss: 1.0340070724487305\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.039120197296143 | KNN Loss: 5.02143669128418 | BCE Loss: 1.017683506011963\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.067962646484375 | KNN Loss: 5.025424003601074 | BCE Loss: 1.0425387620925903\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.032060623168945 | KNN Loss: 5.017405033111572 | BCE Loss: 1.014655351638794\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.03003454208374 | KNN Loss: 4.998448371887207 | BCE Loss: 1.0315861701965332\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.049584865570068 | KNN Loss: 5.012572288513184 | BCE Loss: 1.0370125770568848\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.038581848144531 | KNN Loss: 5.013284683227539 | BCE Loss: 1.0252970457077026\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.073528289794922 | KNN Loss: 5.035048007965088 | BCE Loss: 1.038480520248413\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.089955806732178 | KNN Loss: 5.034677028656006 | BCE Loss: 1.0552786588668823\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.105423450469971 | KNN Loss: 5.035335063934326 | BCE Loss: 1.0700883865356445\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.036177635192871 | KNN Loss: 5.01250696182251 | BCE Loss: 1.0236705541610718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.071241855621338 | KNN Loss: 5.025339126586914 | BCE Loss: 1.0459027290344238\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.095396041870117 | KNN Loss: 5.031296253204346 | BCE Loss: 1.0640995502471924\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.084624767303467 | KNN Loss: 5.0562663078308105 | BCE Loss: 1.0283584594726562\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.022395133972168 | KNN Loss: 5.016458511352539 | BCE Loss: 1.005936622619629\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.068243026733398 | KNN Loss: 5.025578498840332 | BCE Loss: 1.0426645278930664\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.061152458190918 | KNN Loss: 5.007332801818848 | BCE Loss: 1.0538197755813599\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.076411247253418 | KNN Loss: 5.056914329528809 | BCE Loss: 1.0194966793060303\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.053962707519531 | KNN Loss: 5.027441024780273 | BCE Loss: 1.0265214443206787\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.109507083892822 | KNN Loss: 5.061745643615723 | BCE Loss: 1.0477614402770996\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.066202640533447 | KNN Loss: 5.01516056060791 | BCE Loss: 1.051042079925537\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.037469387054443 | KNN Loss: 5.025869369506836 | BCE Loss: 1.0116000175476074\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.021146297454834 | KNN Loss: 5.015151023864746 | BCE Loss: 1.0059951543807983\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.046351909637451 | KNN Loss: 5.005565166473389 | BCE Loss: 1.040786862373352\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.055667877197266 | KNN Loss: 5.006592750549316 | BCE Loss: 1.0490748882293701\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.064779758453369 | KNN Loss: 5.0221333503723145 | BCE Loss: 1.0426464080810547\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.005963325500488 | KNN Loss: 5.009206771850586 | BCE Loss: 0.9967567324638367\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.040826797485352 | KNN Loss: 5.010810375213623 | BCE Loss: 1.0300161838531494\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.072650909423828 | KNN Loss: 5.0388641357421875 | BCE Loss: 1.0337868928909302\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.03471565246582 | KNN Loss: 5.020009517669678 | BCE Loss: 1.0147058963775635\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.027603626251221 | KNN Loss: 5.008840560913086 | BCE Loss: 1.0187631845474243\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.107260704040527 | KNN Loss: 5.075190544128418 | BCE Loss: 1.0320699214935303\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.049302577972412 | KNN Loss: 5.008397579193115 | BCE Loss: 1.0409051179885864\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.074760437011719 | KNN Loss: 5.024002552032471 | BCE Loss: 1.050757884979248\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.045249938964844 | KNN Loss: 5.019691467285156 | BCE Loss: 1.0255584716796875\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.046449661254883 | KNN Loss: 5.008236408233643 | BCE Loss: 1.0382130146026611\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.05957555770874 | KNN Loss: 5.023817539215088 | BCE Loss: 1.0357580184936523\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.071351051330566 | KNN Loss: 5.029557228088379 | BCE Loss: 1.0417940616607666\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.046018600463867 | KNN Loss: 5.016940116882324 | BCE Loss: 1.0290783643722534\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.046608924865723 | KNN Loss: 5.04685115814209 | BCE Loss: 0.9997580051422119\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.071889877319336 | KNN Loss: 5.032417297363281 | BCE Loss: 1.0394726991653442\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.030282497406006 | KNN Loss: 5.044624328613281 | BCE Loss: 0.9856580495834351\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.036233425140381 | KNN Loss: 5.006290912628174 | BCE Loss: 1.0299426317214966\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.0716352462768555 | KNN Loss: 5.011454105377197 | BCE Loss: 1.060180902481079\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.015722274780273 | KNN Loss: 4.994333267211914 | BCE Loss: 1.0213887691497803\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.011248588562012 | KNN Loss: 5.013010501861572 | BCE Loss: 0.9982378482818604\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.046037673950195 | KNN Loss: 5.001115322113037 | BCE Loss: 1.044922113418579\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.062375068664551 | KNN Loss: 5.005977630615234 | BCE Loss: 1.0563976764678955\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.075175762176514 | KNN Loss: 5.029105186462402 | BCE Loss: 1.0460705757141113\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.046247482299805 | KNN Loss: 5.021487712860107 | BCE Loss: 1.0247595310211182\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.044004440307617 | KNN Loss: 5.009336471557617 | BCE Loss: 1.03466796875\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.034317970275879 | KNN Loss: 5.009779930114746 | BCE Loss: 1.0245380401611328\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.051900863647461 | KNN Loss: 5.030710220336914 | BCE Loss: 1.0211904048919678\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.050459861755371 | KNN Loss: 5.032415866851807 | BCE Loss: 1.0180442333221436\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.024456024169922 | KNN Loss: 4.994812488555908 | BCE Loss: 1.0296432971954346\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.071234703063965 | KNN Loss: 5.0319695472717285 | BCE Loss: 1.0392651557922363\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.052938938140869 | KNN Loss: 5.029706954956055 | BCE Loss: 1.023232102394104\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.059694290161133 | KNN Loss: 5.026371955871582 | BCE Loss: 1.0333220958709717\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.074676513671875 | KNN Loss: 5.02015495300293 | BCE Loss: 1.0545213222503662\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.0642170906066895 | KNN Loss: 5.052489280700684 | BCE Loss: 1.0117276906967163\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0169,  3.7089,  2.5876,  3.0771,  3.3849,  0.7724,  2.6627,  2.2804,\n",
      "          2.2103,  2.0220,  2.1847,  2.2892,  0.7973,  1.7530,  1.2190,  1.4315,\n",
      "          2.7459,  2.7280,  2.2349,  1.8349,  1.7019,  2.5745,  2.2448,  2.6221,\n",
      "          2.5252,  1.7932,  2.1448,  1.4807,  1.5369,  0.4111, -0.1998,  1.0286,\n",
      "          0.2046,  0.9527,  1.4791,  1.4321,  1.0842,  2.8008,  0.6504,  1.2694,\n",
      "          0.9280, -0.6987, -0.2524,  2.3327,  1.7224,  0.7449, -0.1590,  0.1547,\n",
      "          1.4286,  2.5432,  1.3626,  0.1273,  1.3694,  0.5756, -0.5356,  1.0705,\n",
      "          1.4649,  1.3859,  1.3082,  1.8057,  0.5729,  0.8208,  0.1695,  1.7984,\n",
      "          1.3383,  1.7248, -1.7942,  0.3608,  2.3759,  2.2260,  2.4704,  0.4697,\n",
      "          1.3686,  2.4387,  1.9007,  1.1780,  0.3153,  0.7410,  0.2198,  1.5819,\n",
      "          0.0308,  0.4000,  1.8466, -0.3640,  0.2809, -1.0456, -2.3745, -0.1721,\n",
      "          0.4682, -1.8568,  0.5005, -0.0813, -0.4984, -1.0866,  0.5996,  1.2267,\n",
      "         -0.7037, -0.6617,  0.4098,  1.1726,  0.7338, -1.2361,  0.8947,  1.2089,\n",
      "         -1.2949, -1.0810, -0.0878,  0.0257, -1.0116, -1.7213, -0.4095, -2.6025,\n",
      "         -0.3847,  1.6956,  1.5517, -0.2361, -0.5933,  0.0316,  1.4989, -2.3845,\n",
      "          0.1463, -0.1919,  0.4286, -0.6947,  0.0309, -0.7481, -0.9128,  0.9363,\n",
      "          0.1938, -0.5727,  0.3392, -0.6697, -1.3287, -0.3041, -0.6054,  0.8766,\n",
      "         -0.4781,  0.1359, -1.9809, -0.9170, -1.4249,  0.5491, -1.8502, -0.9435,\n",
      "         -1.1066, -0.5808, -1.5399, -1.0519, -2.3195, -1.0007, -1.3394, -0.3326,\n",
      "         -1.6565,  0.3378, -1.4920, -0.5468, -3.0367,  0.2394, -0.1450, -0.7456,\n",
      "         -2.1422, -1.6758, -1.2190, -1.3407, -2.3334, -2.2532, -3.1514]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.1514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.7089, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c9fe08c4f649de871f4c10fbc938b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 86.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e07cacd48a34c7786c9a1aac8b75f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f882b3fd4f453d9a7b934d02a65603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e63f0aa57254290a02a5522ff98f74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.645 | Reg loss: 0.009 | Tree loss: 9.645 | Accuracy: 0.000000 | 0.291 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.629 | Reg loss: 0.009 | Tree loss: 9.629 | Accuracy: 0.000000 | 0.262 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.620 | Reg loss: 0.008 | Tree loss: 9.620 | Accuracy: 0.000000 | 0.248 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.608 | Reg loss: 0.008 | Tree loss: 9.608 | Accuracy: 0.000000 | 0.242 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.592 | Reg loss: 0.008 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.24 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.586 | Reg loss: 0.008 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.239 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.574 | Reg loss: 0.007 | Tree loss: 9.574 | Accuracy: 0.000000 | 0.238 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.573 | Reg loss: 0.007 | Tree loss: 9.573 | Accuracy: 0.001953 | 0.236 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.546 | Reg loss: 0.007 | Tree loss: 9.546 | Accuracy: 0.005859 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.534 | Reg loss: 0.007 | Tree loss: 9.534 | Accuracy: 0.003906 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.532 | Reg loss: 0.007 | Tree loss: 9.532 | Accuracy: 0.011719 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.518 | Reg loss: 0.008 | Tree loss: 9.518 | Accuracy: 0.019531 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.511 | Reg loss: 0.008 | Tree loss: 9.511 | Accuracy: 0.037109 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.494 | Reg loss: 0.008 | Tree loss: 9.494 | Accuracy: 0.054688 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.487 | Reg loss: 0.008 | Tree loss: 9.487 | Accuracy: 0.083984 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.475 | Reg loss: 0.008 | Tree loss: 9.475 | Accuracy: 0.154297 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.463 | Reg loss: 0.009 | Tree loss: 9.463 | Accuracy: 0.205078 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.455 | Reg loss: 0.009 | Tree loss: 9.455 | Accuracy: 0.205078 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.448 | Reg loss: 0.009 | Tree loss: 9.448 | Accuracy: 0.181641 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.443 | Reg loss: 0.009 | Tree loss: 9.443 | Accuracy: 0.167969 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.425 | Reg loss: 0.010 | Tree loss: 9.425 | Accuracy: 0.191406 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.413 | Reg loss: 0.010 | Tree loss: 9.413 | Accuracy: 0.179688 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.406 | Reg loss: 0.010 | Tree loss: 9.406 | Accuracy: 0.228516 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.394 | Reg loss: 0.011 | Tree loss: 9.394 | Accuracy: 0.232422 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.377 | Reg loss: 0.011 | Tree loss: 9.377 | Accuracy: 0.207031 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.376 | Reg loss: 0.011 | Tree loss: 9.376 | Accuracy: 0.175781 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.366 | Reg loss: 0.011 | Tree loss: 9.366 | Accuracy: 0.185547 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.354 | Reg loss: 0.012 | Tree loss: 9.354 | Accuracy: 0.201172 | 0.235 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.344 | Reg loss: 0.012 | Tree loss: 9.344 | Accuracy: 0.212551 | 0.235 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.462 | Reg loss: 0.004 | Tree loss: 9.462 | Accuracy: 0.224609 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.458 | Reg loss: 0.005 | Tree loss: 9.458 | Accuracy: 0.183594 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.447 | Reg loss: 0.005 | Tree loss: 9.447 | Accuracy: 0.193359 | 0.24 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.435 | Reg loss: 0.005 | Tree loss: 9.435 | Accuracy: 0.183594 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.431 | Reg loss: 0.005 | Tree loss: 9.431 | Accuracy: 0.210938 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.410 | Reg loss: 0.005 | Tree loss: 9.410 | Accuracy: 0.232422 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.404 | Reg loss: 0.006 | Tree loss: 9.404 | Accuracy: 0.210938 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.394 | Reg loss: 0.006 | Tree loss: 9.394 | Accuracy: 0.167969 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.377 | Reg loss: 0.006 | Tree loss: 9.377 | Accuracy: 0.205078 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.378 | Reg loss: 0.007 | Tree loss: 9.378 | Accuracy: 0.179688 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.359 | Reg loss: 0.007 | Tree loss: 9.359 | Accuracy: 0.201172 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.344 | Reg loss: 0.007 | Tree loss: 9.344 | Accuracy: 0.205078 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.342 | Reg loss: 0.008 | Tree loss: 9.342 | Accuracy: 0.195312 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.341 | Reg loss: 0.008 | Tree loss: 9.341 | Accuracy: 0.162109 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.310 | Reg loss: 0.008 | Tree loss: 9.310 | Accuracy: 0.216797 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.309 | Reg loss: 0.009 | Tree loss: 9.309 | Accuracy: 0.216797 | 0.239 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.300 | Reg loss: 0.009 | Tree loss: 9.300 | Accuracy: 0.183594 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.282 | Reg loss: 0.010 | Tree loss: 9.282 | Accuracy: 0.216797 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.278 | Reg loss: 0.010 | Tree loss: 9.278 | Accuracy: 0.193359 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.271 | Reg loss: 0.010 | Tree loss: 9.271 | Accuracy: 0.195312 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.256 | Reg loss: 0.011 | Tree loss: 9.256 | Accuracy: 0.203125 | 0.238 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.256 | Reg loss: 0.011 | Tree loss: 9.256 | Accuracy: 0.208984 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.219 | Reg loss: 0.012 | Tree loss: 9.219 | Accuracy: 0.222656 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.218 | Reg loss: 0.012 | Tree loss: 9.218 | Accuracy: 0.189453 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.199 | Reg loss: 0.012 | Tree loss: 9.199 | Accuracy: 0.193359 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.199 | Reg loss: 0.013 | Tree loss: 9.199 | Accuracy: 0.197266 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.195 | Reg loss: 0.013 | Tree loss: 9.195 | Accuracy: 0.177734 | 0.237 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.169 | Reg loss: 0.013 | Tree loss: 9.169 | Accuracy: 0.203125 | 0.236 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.154 | Reg loss: 0.014 | Tree loss: 9.154 | Accuracy: 0.222672 | 0.236 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.309 | Reg loss: 0.007 | Tree loss: 9.309 | Accuracy: 0.210938 | 0.239 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.300 | Reg loss: 0.007 | Tree loss: 9.300 | Accuracy: 0.210938 | 0.238 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.284 | Reg loss: 0.007 | Tree loss: 9.284 | Accuracy: 0.205078 | 0.238 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.278 | Reg loss: 0.008 | Tree loss: 9.278 | Accuracy: 0.208984 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.271 | Reg loss: 0.008 | Tree loss: 9.271 | Accuracy: 0.177734 | 0.238 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.249 | Reg loss: 0.008 | Tree loss: 9.249 | Accuracy: 0.187500 | 0.238 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.252 | Reg loss: 0.008 | Tree loss: 9.252 | Accuracy: 0.195312 | 0.238 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.249 | Reg loss: 0.009 | Tree loss: 9.249 | Accuracy: 0.191406 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.213 | Reg loss: 0.009 | Tree loss: 9.213 | Accuracy: 0.220703 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.202 | Reg loss: 0.009 | Tree loss: 9.202 | Accuracy: 0.205078 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.199 | Reg loss: 0.010 | Tree loss: 9.199 | Accuracy: 0.199219 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.189 | Reg loss: 0.010 | Tree loss: 9.189 | Accuracy: 0.181641 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.180 | Reg loss: 0.010 | Tree loss: 9.180 | Accuracy: 0.216797 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.161 | Reg loss: 0.011 | Tree loss: 9.161 | Accuracy: 0.183594 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.149 | Reg loss: 0.011 | Tree loss: 9.149 | Accuracy: 0.185547 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.155 | Reg loss: 0.012 | Tree loss: 9.155 | Accuracy: 0.187500 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.122 | Reg loss: 0.012 | Tree loss: 9.122 | Accuracy: 0.216797 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.124 | Reg loss: 0.013 | Tree loss: 9.124 | Accuracy: 0.181641 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.099 | Reg loss: 0.013 | Tree loss: 9.099 | Accuracy: 0.203125 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.098 | Reg loss: 0.014 | Tree loss: 9.098 | Accuracy: 0.195312 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.081 | Reg loss: 0.014 | Tree loss: 9.081 | Accuracy: 0.189453 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 9.073 | Reg loss: 0.014 | Tree loss: 9.073 | Accuracy: 0.208984 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 9.057 | Reg loss: 0.015 | Tree loss: 9.057 | Accuracy: 0.177734 | 0.236 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 9.046 | Reg loss: 0.015 | Tree loss: 9.046 | Accuracy: 0.197266 | 0.237 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 9.023 | Reg loss: 0.016 | Tree loss: 9.023 | Accuracy: 0.210938 | 0.236 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 9.009 | Reg loss: 0.016 | Tree loss: 9.009 | Accuracy: 0.210938 | 0.236 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 8.997 | Reg loss: 0.017 | Tree loss: 8.997 | Accuracy: 0.226562 | 0.236 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 8.983 | Reg loss: 0.017 | Tree loss: 8.983 | Accuracy: 0.208984 | 0.236 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 8.984 | Reg loss: 0.017 | Tree loss: 8.984 | Accuracy: 0.200405 | 0.236 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.155 | Reg loss: 0.010 | Tree loss: 9.155 | Accuracy: 0.216797 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.144 | Reg loss: 0.010 | Tree loss: 9.144 | Accuracy: 0.203125 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.137 | Reg loss: 0.010 | Tree loss: 9.137 | Accuracy: 0.205078 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.118 | Reg loss: 0.010 | Tree loss: 9.118 | Accuracy: 0.208984 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.103 | Reg loss: 0.011 | Tree loss: 9.103 | Accuracy: 0.175781 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.094 | Reg loss: 0.011 | Tree loss: 9.094 | Accuracy: 0.187500 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 9.070 | Reg loss: 0.011 | Tree loss: 9.070 | Accuracy: 0.240234 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.075 | Reg loss: 0.011 | Tree loss: 9.075 | Accuracy: 0.197266 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 9.046 | Reg loss: 0.012 | Tree loss: 9.046 | Accuracy: 0.207031 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 9.048 | Reg loss: 0.012 | Tree loss: 9.048 | Accuracy: 0.193359 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 9.028 | Reg loss: 0.012 | Tree loss: 9.028 | Accuracy: 0.210938 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 9.007 | Reg loss: 0.013 | Tree loss: 9.007 | Accuracy: 0.224609 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 8.996 | Reg loss: 0.013 | Tree loss: 8.996 | Accuracy: 0.203125 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 8.982 | Reg loss: 0.014 | Tree loss: 8.982 | Accuracy: 0.171875 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 8.969 | Reg loss: 0.014 | Tree loss: 8.969 | Accuracy: 0.197266 | 0.237 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 8.977 | Reg loss: 0.015 | Tree loss: 8.977 | Accuracy: 0.183594 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 8.942 | Reg loss: 0.015 | Tree loss: 8.942 | Accuracy: 0.187500 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 8.923 | Reg loss: 0.015 | Tree loss: 8.923 | Accuracy: 0.208984 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 8.917 | Reg loss: 0.016 | Tree loss: 8.917 | Accuracy: 0.197266 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 8.900 | Reg loss: 0.016 | Tree loss: 8.900 | Accuracy: 0.212891 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 8.894 | Reg loss: 0.017 | Tree loss: 8.894 | Accuracy: 0.212891 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 8.878 | Reg loss: 0.017 | Tree loss: 8.878 | Accuracy: 0.191406 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 8.872 | Reg loss: 0.018 | Tree loss: 8.872 | Accuracy: 0.189453 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 8.845 | Reg loss: 0.018 | Tree loss: 8.845 | Accuracy: 0.203125 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 8.840 | Reg loss: 0.019 | Tree loss: 8.840 | Accuracy: 0.224609 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 8.825 | Reg loss: 0.019 | Tree loss: 8.825 | Accuracy: 0.160156 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 8.801 | Reg loss: 0.019 | Tree loss: 8.801 | Accuracy: 0.191406 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 8.784 | Reg loss: 0.020 | Tree loss: 8.784 | Accuracy: 0.193359 | 0.236 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 8.786 | Reg loss: 0.020 | Tree loss: 8.786 | Accuracy: 0.194332 | 0.236 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 8.990 | Reg loss: 0.013 | Tree loss: 8.990 | Accuracy: 0.189453 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 8.978 | Reg loss: 0.013 | Tree loss: 8.978 | Accuracy: 0.193359 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 8.959 | Reg loss: 0.013 | Tree loss: 8.959 | Accuracy: 0.195312 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 8.928 | Reg loss: 0.013 | Tree loss: 8.928 | Accuracy: 0.218750 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 8.936 | Reg loss: 0.013 | Tree loss: 8.936 | Accuracy: 0.208984 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 8.915 | Reg loss: 0.013 | Tree loss: 8.915 | Accuracy: 0.199219 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 8.898 | Reg loss: 0.014 | Tree loss: 8.898 | Accuracy: 0.214844 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 8.884 | Reg loss: 0.014 | Tree loss: 8.884 | Accuracy: 0.207031 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 8.874 | Reg loss: 0.014 | Tree loss: 8.874 | Accuracy: 0.216797 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 8.869 | Reg loss: 0.014 | Tree loss: 8.869 | Accuracy: 0.193359 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 8.850 | Reg loss: 0.015 | Tree loss: 8.850 | Accuracy: 0.191406 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 8.814 | Reg loss: 0.015 | Tree loss: 8.814 | Accuracy: 0.214844 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 8.806 | Reg loss: 0.015 | Tree loss: 8.806 | Accuracy: 0.218750 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 8.788 | Reg loss: 0.016 | Tree loss: 8.788 | Accuracy: 0.191406 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 8.774 | Reg loss: 0.016 | Tree loss: 8.774 | Accuracy: 0.236328 | 0.237 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 8.762 | Reg loss: 0.017 | Tree loss: 8.762 | Accuracy: 0.224609 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 8.756 | Reg loss: 0.017 | Tree loss: 8.756 | Accuracy: 0.246094 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 8.737 | Reg loss: 0.017 | Tree loss: 8.737 | Accuracy: 0.244141 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 8.718 | Reg loss: 0.018 | Tree loss: 8.718 | Accuracy: 0.236328 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 8.706 | Reg loss: 0.018 | Tree loss: 8.706 | Accuracy: 0.222656 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 8.674 | Reg loss: 0.019 | Tree loss: 8.674 | Accuracy: 0.238281 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 8.693 | Reg loss: 0.019 | Tree loss: 8.693 | Accuracy: 0.218750 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 8.661 | Reg loss: 0.020 | Tree loss: 8.661 | Accuracy: 0.220703 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 8.632 | Reg loss: 0.020 | Tree loss: 8.632 | Accuracy: 0.232422 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 8.624 | Reg loss: 0.020 | Tree loss: 8.624 | Accuracy: 0.269531 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.628 | Reg loss: 0.021 | Tree loss: 8.628 | Accuracy: 0.181641 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.588 | Reg loss: 0.021 | Tree loss: 8.588 | Accuracy: 0.265625 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.602 | Reg loss: 0.021 | Tree loss: 8.602 | Accuracy: 0.220703 | 0.236 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.591 | Reg loss: 0.022 | Tree loss: 8.591 | Accuracy: 0.238866 | 0.236 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 8.804 | Reg loss: 0.015 | Tree loss: 8.804 | Accuracy: 0.222656 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 8.785 | Reg loss: 0.015 | Tree loss: 8.785 | Accuracy: 0.228516 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 8.770 | Reg loss: 0.015 | Tree loss: 8.770 | Accuracy: 0.218750 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 8.764 | Reg loss: 0.015 | Tree loss: 8.764 | Accuracy: 0.230469 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 8.736 | Reg loss: 0.015 | Tree loss: 8.736 | Accuracy: 0.203125 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 8.729 | Reg loss: 0.016 | Tree loss: 8.729 | Accuracy: 0.216797 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 8.703 | Reg loss: 0.016 | Tree loss: 8.703 | Accuracy: 0.234375 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 8.680 | Reg loss: 0.016 | Tree loss: 8.680 | Accuracy: 0.222656 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 8.687 | Reg loss: 0.016 | Tree loss: 8.687 | Accuracy: 0.208984 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 8.667 | Reg loss: 0.016 | Tree loss: 8.667 | Accuracy: 0.230469 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 8.650 | Reg loss: 0.017 | Tree loss: 8.650 | Accuracy: 0.246094 | 0.237 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 8.617 | Reg loss: 0.017 | Tree loss: 8.617 | Accuracy: 0.279297 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.614 | Reg loss: 0.017 | Tree loss: 8.614 | Accuracy: 0.248047 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.611 | Reg loss: 0.018 | Tree loss: 8.611 | Accuracy: 0.226562 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.566 | Reg loss: 0.018 | Tree loss: 8.566 | Accuracy: 0.253906 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.560 | Reg loss: 0.018 | Tree loss: 8.560 | Accuracy: 0.277344 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.528 | Reg loss: 0.019 | Tree loss: 8.528 | Accuracy: 0.277344 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.525 | Reg loss: 0.019 | Tree loss: 8.525 | Accuracy: 0.263672 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.493 | Reg loss: 0.020 | Tree loss: 8.493 | Accuracy: 0.259766 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.486 | Reg loss: 0.020 | Tree loss: 8.486 | Accuracy: 0.316406 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.468 | Reg loss: 0.020 | Tree loss: 8.468 | Accuracy: 0.289062 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.453 | Reg loss: 0.021 | Tree loss: 8.453 | Accuracy: 0.298828 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.421 | Reg loss: 0.021 | Tree loss: 8.421 | Accuracy: 0.328125 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.410 | Reg loss: 0.022 | Tree loss: 8.410 | Accuracy: 0.298828 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.391 | Reg loss: 0.022 | Tree loss: 8.391 | Accuracy: 0.308594 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.386 | Reg loss: 0.022 | Tree loss: 8.386 | Accuracy: 0.279297 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.379 | Reg loss: 0.023 | Tree loss: 8.379 | Accuracy: 0.281250 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.368 | Reg loss: 0.023 | Tree loss: 8.368 | Accuracy: 0.273438 | 0.236 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.332 | Reg loss: 0.023 | Tree loss: 8.332 | Accuracy: 0.305668 | 0.236 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 8.591 | Reg loss: 0.017 | Tree loss: 8.591 | Accuracy: 0.250000 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.575 | Reg loss: 0.017 | Tree loss: 8.575 | Accuracy: 0.238281 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.567 | Reg loss: 0.017 | Tree loss: 8.567 | Accuracy: 0.259766 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.549 | Reg loss: 0.017 | Tree loss: 8.549 | Accuracy: 0.220703 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.516 | Reg loss: 0.017 | Tree loss: 8.516 | Accuracy: 0.230469 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.500 | Reg loss: 0.017 | Tree loss: 8.500 | Accuracy: 0.273438 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.493 | Reg loss: 0.018 | Tree loss: 8.493 | Accuracy: 0.261719 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.486 | Reg loss: 0.018 | Tree loss: 8.486 | Accuracy: 0.273438 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.448 | Reg loss: 0.018 | Tree loss: 8.448 | Accuracy: 0.283203 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.464 | Reg loss: 0.018 | Tree loss: 8.464 | Accuracy: 0.265625 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.410 | Reg loss: 0.019 | Tree loss: 8.410 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.393 | Reg loss: 0.019 | Tree loss: 8.393 | Accuracy: 0.291016 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.377 | Reg loss: 0.019 | Tree loss: 8.377 | Accuracy: 0.253906 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.359 | Reg loss: 0.020 | Tree loss: 8.359 | Accuracy: 0.343750 | 0.237 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.319 | Reg loss: 0.020 | Tree loss: 8.319 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.297 | Reg loss: 0.020 | Tree loss: 8.297 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.303 | Reg loss: 0.021 | Tree loss: 8.303 | Accuracy: 0.283203 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.281 | Reg loss: 0.021 | Tree loss: 8.281 | Accuracy: 0.263672 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.255 | Reg loss: 0.022 | Tree loss: 8.255 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.248 | Reg loss: 0.022 | Tree loss: 8.248 | Accuracy: 0.302734 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.234 | Reg loss: 0.022 | Tree loss: 8.234 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.214 | Reg loss: 0.023 | Tree loss: 8.214 | Accuracy: 0.302734 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.175 | Reg loss: 0.023 | Tree loss: 8.175 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.148 | Reg loss: 0.024 | Tree loss: 8.148 | Accuracy: 0.306641 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.150 | Reg loss: 0.024 | Tree loss: 8.150 | Accuracy: 0.292969 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.118 | Reg loss: 0.024 | Tree loss: 8.118 | Accuracy: 0.320312 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.077 | Reg loss: 0.025 | Tree loss: 8.077 | Accuracy: 0.308594 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.059 | Reg loss: 0.025 | Tree loss: 8.059 | Accuracy: 0.294922 | 0.237 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.076 | Reg loss: 0.026 | Tree loss: 8.076 | Accuracy: 0.261134 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.359 | Reg loss: 0.019 | Tree loss: 8.359 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.353 | Reg loss: 0.019 | Tree loss: 8.353 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.325 | Reg loss: 0.019 | Tree loss: 8.325 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.294 | Reg loss: 0.019 | Tree loss: 8.294 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.288 | Reg loss: 0.019 | Tree loss: 8.288 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.264 | Reg loss: 0.019 | Tree loss: 8.264 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.245 | Reg loss: 0.020 | Tree loss: 8.245 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.218 | Reg loss: 0.020 | Tree loss: 8.218 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.191 | Reg loss: 0.020 | Tree loss: 8.191 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.193 | Reg loss: 0.020 | Tree loss: 8.193 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.171 | Reg loss: 0.021 | Tree loss: 8.171 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.116 | Reg loss: 0.021 | Tree loss: 8.116 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.084 | Reg loss: 0.021 | Tree loss: 8.084 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.091 | Reg loss: 0.022 | Tree loss: 8.091 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.055 | Reg loss: 0.022 | Tree loss: 8.055 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.022 | Reg loss: 0.022 | Tree loss: 8.022 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.016 | Reg loss: 0.023 | Tree loss: 8.016 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 7.974 | Reg loss: 0.023 | Tree loss: 7.974 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 7.973 | Reg loss: 0.024 | Tree loss: 7.973 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 7.963 | Reg loss: 0.024 | Tree loss: 7.963 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 7.921 | Reg loss: 0.024 | Tree loss: 7.921 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 7.900 | Reg loss: 0.025 | Tree loss: 7.900 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 7.870 | Reg loss: 0.025 | Tree loss: 7.870 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 7.851 | Reg loss: 0.026 | Tree loss: 7.851 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 7.815 | Reg loss: 0.026 | Tree loss: 7.815 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 7.782 | Reg loss: 0.026 | Tree loss: 7.782 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 7.768 | Reg loss: 0.027 | Tree loss: 7.768 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 7.726 | Reg loss: 0.027 | Tree loss: 7.726 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 7.696 | Reg loss: 0.027 | Tree loss: 7.696 | Accuracy: 0.342105 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.103 | Reg loss: 0.021 | Tree loss: 8.103 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.084 | Reg loss: 0.021 | Tree loss: 8.084 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.053 | Reg loss: 0.021 | Tree loss: 8.053 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.036 | Reg loss: 0.021 | Tree loss: 8.036 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.006 | Reg loss: 0.021 | Tree loss: 8.006 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 7.980 | Reg loss: 0.021 | Tree loss: 7.980 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 7.982 | Reg loss: 0.021 | Tree loss: 7.982 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 7.969 | Reg loss: 0.022 | Tree loss: 7.969 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 7.910 | Reg loss: 0.022 | Tree loss: 7.910 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 7.899 | Reg loss: 0.022 | Tree loss: 7.899 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 7.863 | Reg loss: 0.022 | Tree loss: 7.863 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 7.825 | Reg loss: 0.023 | Tree loss: 7.825 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 7.798 | Reg loss: 0.023 | Tree loss: 7.798 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 7.784 | Reg loss: 0.023 | Tree loss: 7.784 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 7.763 | Reg loss: 0.024 | Tree loss: 7.763 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 7.700 | Reg loss: 0.024 | Tree loss: 7.700 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 7.725 | Reg loss: 0.024 | Tree loss: 7.725 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 7.683 | Reg loss: 0.025 | Tree loss: 7.683 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 7.627 | Reg loss: 0.025 | Tree loss: 7.627 | Accuracy: 0.287109 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 7.618 | Reg loss: 0.025 | Tree loss: 7.618 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 7.597 | Reg loss: 0.026 | Tree loss: 7.597 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 7.554 | Reg loss: 0.026 | Tree loss: 7.554 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 7.569 | Reg loss: 0.026 | Tree loss: 7.569 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 7.570 | Reg loss: 0.027 | Tree loss: 7.570 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.517 | Reg loss: 0.027 | Tree loss: 7.517 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.483 | Reg loss: 0.027 | Tree loss: 7.483 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.507 | Reg loss: 0.028 | Tree loss: 7.507 | Accuracy: 0.242188 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.463 | Reg loss: 0.028 | Tree loss: 7.463 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.434 | Reg loss: 0.028 | Tree loss: 7.434 | Accuracy: 0.287449 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 7.827 | Reg loss: 0.022 | Tree loss: 7.827 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 7.802 | Reg loss: 0.022 | Tree loss: 7.802 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 7.778 | Reg loss: 0.022 | Tree loss: 7.778 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 7.754 | Reg loss: 0.023 | Tree loss: 7.754 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 7.744 | Reg loss: 0.023 | Tree loss: 7.744 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 7.665 | Reg loss: 0.023 | Tree loss: 7.665 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 7.665 | Reg loss: 0.023 | Tree loss: 7.665 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 7.640 | Reg loss: 0.023 | Tree loss: 7.640 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 7.624 | Reg loss: 0.023 | Tree loss: 7.624 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 7.603 | Reg loss: 0.023 | Tree loss: 7.603 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 7.566 | Reg loss: 0.024 | Tree loss: 7.566 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 7.526 | Reg loss: 0.024 | Tree loss: 7.526 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.530 | Reg loss: 0.024 | Tree loss: 7.530 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.523 | Reg loss: 0.024 | Tree loss: 7.523 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.473 | Reg loss: 0.025 | Tree loss: 7.473 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.447 | Reg loss: 0.025 | Tree loss: 7.447 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.419 | Reg loss: 0.025 | Tree loss: 7.419 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.378 | Reg loss: 0.026 | Tree loss: 7.378 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.374 | Reg loss: 0.026 | Tree loss: 7.374 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.342 | Reg loss: 0.026 | Tree loss: 7.342 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.321 | Reg loss: 0.026 | Tree loss: 7.321 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.302 | Reg loss: 0.027 | Tree loss: 7.302 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.281 | Reg loss: 0.027 | Tree loss: 7.281 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.257 | Reg loss: 0.027 | Tree loss: 7.257 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.257 | Reg loss: 0.028 | Tree loss: 7.257 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.218 | Reg loss: 0.028 | Tree loss: 7.218 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.132 | Reg loss: 0.028 | Tree loss: 7.132 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.148 | Reg loss: 0.029 | Tree loss: 7.148 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.118 | Reg loss: 0.029 | Tree loss: 7.118 | Accuracy: 0.299595 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.572 | Reg loss: 0.024 | Tree loss: 7.572 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.518 | Reg loss: 0.024 | Tree loss: 7.518 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.470 | Reg loss: 0.024 | Tree loss: 7.470 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.433 | Reg loss: 0.024 | Tree loss: 7.433 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.448 | Reg loss: 0.024 | Tree loss: 7.448 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.416 | Reg loss: 0.024 | Tree loss: 7.416 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.406 | Reg loss: 0.024 | Tree loss: 7.406 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.375 | Reg loss: 0.024 | Tree loss: 7.375 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.347 | Reg loss: 0.024 | Tree loss: 7.347 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.318 | Reg loss: 0.025 | Tree loss: 7.318 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.273 | Reg loss: 0.025 | Tree loss: 7.273 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.266 | Reg loss: 0.025 | Tree loss: 7.266 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.213 | Reg loss: 0.025 | Tree loss: 7.213 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.211 | Reg loss: 0.025 | Tree loss: 7.211 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.200 | Reg loss: 0.026 | Tree loss: 7.200 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.154 | Reg loss: 0.026 | Tree loss: 7.154 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.146 | Reg loss: 0.026 | Tree loss: 7.146 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.087 | Reg loss: 0.026 | Tree loss: 7.087 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.116 | Reg loss: 0.027 | Tree loss: 7.116 | Accuracy: 0.255859 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.066 | Reg loss: 0.027 | Tree loss: 7.066 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 6.994 | Reg loss: 0.027 | Tree loss: 6.994 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.050 | Reg loss: 0.027 | Tree loss: 7.050 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 7.005 | Reg loss: 0.028 | Tree loss: 7.005 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 6.968 | Reg loss: 0.028 | Tree loss: 6.968 | Accuracy: 0.296875 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 6.979 | Reg loss: 0.028 | Tree loss: 6.979 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 6.923 | Reg loss: 0.028 | Tree loss: 6.923 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 6.890 | Reg loss: 0.029 | Tree loss: 6.890 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 6.862 | Reg loss: 0.029 | Tree loss: 6.862 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 6.856 | Reg loss: 0.029 | Tree loss: 6.856 | Accuracy: 0.297571 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.262 | Reg loss: 0.025 | Tree loss: 7.262 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.233 | Reg loss: 0.025 | Tree loss: 7.233 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.222 | Reg loss: 0.025 | Tree loss: 7.222 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.177 | Reg loss: 0.025 | Tree loss: 7.177 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.110 | Reg loss: 0.025 | Tree loss: 7.110 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.151 | Reg loss: 0.025 | Tree loss: 7.151 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.112 | Reg loss: 0.025 | Tree loss: 7.112 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.067 | Reg loss: 0.025 | Tree loss: 7.067 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.085 | Reg loss: 0.025 | Tree loss: 7.085 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.028 | Reg loss: 0.026 | Tree loss: 7.028 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 7.009 | Reg loss: 0.026 | Tree loss: 7.009 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 6.988 | Reg loss: 0.026 | Tree loss: 6.988 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 6.951 | Reg loss: 0.026 | Tree loss: 6.951 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 6.968 | Reg loss: 0.026 | Tree loss: 6.968 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 6.948 | Reg loss: 0.026 | Tree loss: 6.948 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 6.899 | Reg loss: 0.027 | Tree loss: 6.899 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 6.868 | Reg loss: 0.027 | Tree loss: 6.868 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 6.836 | Reg loss: 0.027 | Tree loss: 6.836 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 6.812 | Reg loss: 0.027 | Tree loss: 6.812 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 6.772 | Reg loss: 0.027 | Tree loss: 6.772 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 6.784 | Reg loss: 0.028 | Tree loss: 6.784 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 6.756 | Reg loss: 0.028 | Tree loss: 6.756 | Accuracy: 0.269531 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 6.751 | Reg loss: 0.028 | Tree loss: 6.751 | Accuracy: 0.296875 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 6.740 | Reg loss: 0.028 | Tree loss: 6.740 | Accuracy: 0.289062 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 6.669 | Reg loss: 0.029 | Tree loss: 6.669 | Accuracy: 0.308594 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 6.664 | Reg loss: 0.029 | Tree loss: 6.664 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 6.641 | Reg loss: 0.029 | Tree loss: 6.641 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 6.625 | Reg loss: 0.029 | Tree loss: 6.625 | Accuracy: 0.294922 | 0.237 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 6.563 | Reg loss: 0.029 | Tree loss: 6.563 | Accuracy: 0.319838 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 6.995 | Reg loss: 0.026 | Tree loss: 6.995 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 6.952 | Reg loss: 0.026 | Tree loss: 6.952 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 6.942 | Reg loss: 0.026 | Tree loss: 6.942 | Accuracy: 0.306641 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 6.923 | Reg loss: 0.026 | Tree loss: 6.923 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 6.897 | Reg loss: 0.026 | Tree loss: 6.897 | Accuracy: 0.310547 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 6.845 | Reg loss: 0.026 | Tree loss: 6.845 | Accuracy: 0.289062 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 6.858 | Reg loss: 0.026 | Tree loss: 6.858 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 6.775 | Reg loss: 0.026 | Tree loss: 6.775 | Accuracy: 0.324219 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 6.768 | Reg loss: 0.026 | Tree loss: 6.768 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 6.757 | Reg loss: 0.026 | Tree loss: 6.757 | Accuracy: 0.332031 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 6.807 | Reg loss: 0.026 | Tree loss: 6.807 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 6.661 | Reg loss: 0.027 | Tree loss: 6.661 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 6.678 | Reg loss: 0.027 | Tree loss: 6.678 | Accuracy: 0.308594 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 6.690 | Reg loss: 0.027 | Tree loss: 6.690 | Accuracy: 0.271484 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 6.657 | Reg loss: 0.027 | Tree loss: 6.657 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 6.599 | Reg loss: 0.027 | Tree loss: 6.599 | Accuracy: 0.310547 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 6.602 | Reg loss: 0.027 | Tree loss: 6.602 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 6.602 | Reg loss: 0.028 | Tree loss: 6.602 | Accuracy: 0.279297 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 6.561 | Reg loss: 0.028 | Tree loss: 6.561 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 6.527 | Reg loss: 0.028 | Tree loss: 6.527 | Accuracy: 0.318359 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 6.523 | Reg loss: 0.028 | Tree loss: 6.523 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 6.504 | Reg loss: 0.028 | Tree loss: 6.504 | Accuracy: 0.277344 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 6.489 | Reg loss: 0.029 | Tree loss: 6.489 | Accuracy: 0.257812 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.473 | Reg loss: 0.029 | Tree loss: 6.473 | Accuracy: 0.296875 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 6.443 | Reg loss: 0.029 | Tree loss: 6.443 | Accuracy: 0.269531 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.443 | Reg loss: 0.029 | Tree loss: 6.443 | Accuracy: 0.257812 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.386 | Reg loss: 0.029 | Tree loss: 6.386 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.382 | Reg loss: 0.029 | Tree loss: 6.382 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.345 | Reg loss: 0.030 | Tree loss: 6.345 | Accuracy: 0.295547 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 6.752 | Reg loss: 0.026 | Tree loss: 6.752 | Accuracy: 0.263672 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 6.678 | Reg loss: 0.026 | Tree loss: 6.678 | Accuracy: 0.308594 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 6.673 | Reg loss: 0.026 | Tree loss: 6.673 | Accuracy: 0.322266 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 6.622 | Reg loss: 0.026 | Tree loss: 6.622 | Accuracy: 0.275391 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 6.611 | Reg loss: 0.026 | Tree loss: 6.611 | Accuracy: 0.302734 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 6.580 | Reg loss: 0.027 | Tree loss: 6.580 | Accuracy: 0.310547 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 6.590 | Reg loss: 0.027 | Tree loss: 6.590 | Accuracy: 0.287109 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 6.549 | Reg loss: 0.027 | Tree loss: 6.549 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 6.536 | Reg loss: 0.027 | Tree loss: 6.536 | Accuracy: 0.269531 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 6.529 | Reg loss: 0.027 | Tree loss: 6.529 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 6.500 | Reg loss: 0.027 | Tree loss: 6.500 | Accuracy: 0.275391 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 6.443 | Reg loss: 0.027 | Tree loss: 6.443 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 6.451 | Reg loss: 0.027 | Tree loss: 6.451 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 6.432 | Reg loss: 0.027 | Tree loss: 6.432 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 6.384 | Reg loss: 0.028 | Tree loss: 6.384 | Accuracy: 0.292969 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 6.367 | Reg loss: 0.028 | Tree loss: 6.367 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.348 | Reg loss: 0.028 | Tree loss: 6.348 | Accuracy: 0.298828 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 6.323 | Reg loss: 0.028 | Tree loss: 6.323 | Accuracy: 0.322266 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.316 | Reg loss: 0.028 | Tree loss: 6.316 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.260 | Reg loss: 0.028 | Tree loss: 6.260 | Accuracy: 0.310547 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.275 | Reg loss: 0.028 | Tree loss: 6.275 | Accuracy: 0.294922 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.258 | Reg loss: 0.029 | Tree loss: 6.258 | Accuracy: 0.296875 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.230 | Reg loss: 0.029 | Tree loss: 6.230 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.246 | Reg loss: 0.029 | Tree loss: 6.246 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.189 | Reg loss: 0.029 | Tree loss: 6.189 | Accuracy: 0.296875 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.185 | Reg loss: 0.029 | Tree loss: 6.185 | Accuracy: 0.292969 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.159 | Reg loss: 0.029 | Tree loss: 6.159 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 6.139 | Reg loss: 0.030 | Tree loss: 6.139 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 6.123 | Reg loss: 0.030 | Tree loss: 6.123 | Accuracy: 0.299595 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 6.490 | Reg loss: 0.027 | Tree loss: 6.490 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 6.429 | Reg loss: 0.027 | Tree loss: 6.429 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 6.394 | Reg loss: 0.027 | Tree loss: 6.394 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 6.390 | Reg loss: 0.027 | Tree loss: 6.390 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 6.337 | Reg loss: 0.027 | Tree loss: 6.337 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 6.339 | Reg loss: 0.027 | Tree loss: 6.339 | Accuracy: 0.294922 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 6.362 | Reg loss: 0.027 | Tree loss: 6.362 | Accuracy: 0.287109 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 6.263 | Reg loss: 0.027 | Tree loss: 6.263 | Accuracy: 0.326172 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.287 | Reg loss: 0.027 | Tree loss: 6.287 | Accuracy: 0.296875 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.248 | Reg loss: 0.027 | Tree loss: 6.248 | Accuracy: 0.302734 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.276 | Reg loss: 0.028 | Tree loss: 6.276 | Accuracy: 0.294922 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.253 | Reg loss: 0.028 | Tree loss: 6.253 | Accuracy: 0.261719 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.209 | Reg loss: 0.028 | Tree loss: 6.209 | Accuracy: 0.269531 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.219 | Reg loss: 0.028 | Tree loss: 6.219 | Accuracy: 0.275391 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 6.145 | Reg loss: 0.028 | Tree loss: 6.145 | Accuracy: 0.306641 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 6.109 | Reg loss: 0.028 | Tree loss: 6.109 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 6.102 | Reg loss: 0.028 | Tree loss: 6.102 | Accuracy: 0.277344 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 6.085 | Reg loss: 0.028 | Tree loss: 6.085 | Accuracy: 0.292969 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 6.076 | Reg loss: 0.029 | Tree loss: 6.076 | Accuracy: 0.289062 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 6.030 | Reg loss: 0.029 | Tree loss: 6.030 | Accuracy: 0.267578 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 6.052 | Reg loss: 0.029 | Tree loss: 6.052 | Accuracy: 0.261719 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 5.978 | Reg loss: 0.029 | Tree loss: 5.978 | Accuracy: 0.367188 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 5.992 | Reg loss: 0.029 | Tree loss: 5.992 | Accuracy: 0.269531 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 5.985 | Reg loss: 0.029 | Tree loss: 5.985 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 5.973 | Reg loss: 0.029 | Tree loss: 5.973 | Accuracy: 0.285156 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 5.964 | Reg loss: 0.030 | Tree loss: 5.964 | Accuracy: 0.287109 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 5.924 | Reg loss: 0.030 | Tree loss: 5.924 | Accuracy: 0.312500 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 5.929 | Reg loss: 0.030 | Tree loss: 5.929 | Accuracy: 0.273438 | 0.237 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 5.874 | Reg loss: 0.030 | Tree loss: 5.874 | Accuracy: 0.309717 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.182 | Reg loss: 0.027 | Tree loss: 6.182 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.173 | Reg loss: 0.027 | Tree loss: 6.173 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 6.150 | Reg loss: 0.027 | Tree loss: 6.150 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.160 | Reg loss: 0.028 | Tree loss: 6.160 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 6.168 | Reg loss: 0.028 | Tree loss: 6.168 | Accuracy: 0.281250 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 6.101 | Reg loss: 0.028 | Tree loss: 6.101 | Accuracy: 0.302734 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 6.094 | Reg loss: 0.028 | Tree loss: 6.094 | Accuracy: 0.287109 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 6.090 | Reg loss: 0.028 | Tree loss: 6.090 | Accuracy: 0.279297 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 6.069 | Reg loss: 0.028 | Tree loss: 6.069 | Accuracy: 0.253906 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 6.042 | Reg loss: 0.028 | Tree loss: 6.042 | Accuracy: 0.287109 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 5.996 | Reg loss: 0.028 | Tree loss: 5.996 | Accuracy: 0.281250 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 5.964 | Reg loss: 0.028 | Tree loss: 5.964 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 5.957 | Reg loss: 0.028 | Tree loss: 5.957 | Accuracy: 0.324219 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 5.968 | Reg loss: 0.028 | Tree loss: 5.968 | Accuracy: 0.271484 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 5.923 | Reg loss: 0.028 | Tree loss: 5.923 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 5.901 | Reg loss: 0.028 | Tree loss: 5.901 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 5.886 | Reg loss: 0.029 | Tree loss: 5.886 | Accuracy: 0.291016 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 5.872 | Reg loss: 0.029 | Tree loss: 5.872 | Accuracy: 0.306641 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 5.838 | Reg loss: 0.029 | Tree loss: 5.838 | Accuracy: 0.312500 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 5.854 | Reg loss: 0.029 | Tree loss: 5.854 | Accuracy: 0.308594 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 5.789 | Reg loss: 0.029 | Tree loss: 5.789 | Accuracy: 0.304688 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 5.754 | Reg loss: 0.029 | Tree loss: 5.754 | Accuracy: 0.314453 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 5.776 | Reg loss: 0.029 | Tree loss: 5.776 | Accuracy: 0.316406 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 5.769 | Reg loss: 0.029 | Tree loss: 5.769 | Accuracy: 0.283203 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 5.734 | Reg loss: 0.030 | Tree loss: 5.734 | Accuracy: 0.289062 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 5.740 | Reg loss: 0.030 | Tree loss: 5.740 | Accuracy: 0.261719 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 5.695 | Reg loss: 0.030 | Tree loss: 5.695 | Accuracy: 0.300781 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 5.676 | Reg loss: 0.030 | Tree loss: 5.676 | Accuracy: 0.283203 | 0.237 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 5.644 | Reg loss: 0.030 | Tree loss: 5.644 | Accuracy: 0.291498 | 0.237 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 5.973 | Reg loss: 0.028 | Tree loss: 5.973 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 5.921 | Reg loss: 0.028 | Tree loss: 5.921 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 5.954 | Reg loss: 0.028 | Tree loss: 5.954 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 5.866 | Reg loss: 0.028 | Tree loss: 5.866 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 5.931 | Reg loss: 0.028 | Tree loss: 5.931 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 5.866 | Reg loss: 0.028 | Tree loss: 5.866 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 5.837 | Reg loss: 0.028 | Tree loss: 5.837 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 5.851 | Reg loss: 0.028 | Tree loss: 5.851 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 5.808 | Reg loss: 0.028 | Tree loss: 5.808 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 5.810 | Reg loss: 0.028 | Tree loss: 5.810 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 5.799 | Reg loss: 0.028 | Tree loss: 5.799 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 5.753 | Reg loss: 0.028 | Tree loss: 5.753 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 5.738 | Reg loss: 0.028 | Tree loss: 5.738 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 5.702 | Reg loss: 0.029 | Tree loss: 5.702 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 5.679 | Reg loss: 0.029 | Tree loss: 5.679 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 5.662 | Reg loss: 0.029 | Tree loss: 5.662 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 5.687 | Reg loss: 0.029 | Tree loss: 5.687 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 5.643 | Reg loss: 0.029 | Tree loss: 5.643 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 5.653 | Reg loss: 0.029 | Tree loss: 5.653 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 5.601 | Reg loss: 0.029 | Tree loss: 5.601 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 5.545 | Reg loss: 0.029 | Tree loss: 5.545 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 5.563 | Reg loss: 0.029 | Tree loss: 5.563 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 5.553 | Reg loss: 0.029 | Tree loss: 5.553 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 5.519 | Reg loss: 0.030 | Tree loss: 5.519 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 5.519 | Reg loss: 0.030 | Tree loss: 5.519 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 5.532 | Reg loss: 0.030 | Tree loss: 5.532 | Accuracy: 0.240234 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 5.510 | Reg loss: 0.030 | Tree loss: 5.510 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 5.482 | Reg loss: 0.030 | Tree loss: 5.482 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 5.486 | Reg loss: 0.030 | Tree loss: 5.486 | Accuracy: 0.273279 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 5.704 | Reg loss: 0.028 | Tree loss: 5.704 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 5.711 | Reg loss: 0.028 | Tree loss: 5.711 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 5.711 | Reg loss: 0.028 | Tree loss: 5.711 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 5.659 | Reg loss: 0.028 | Tree loss: 5.659 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 5.674 | Reg loss: 0.028 | Tree loss: 5.674 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 5.661 | Reg loss: 0.028 | Tree loss: 5.661 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 5.623 | Reg loss: 0.028 | Tree loss: 5.623 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 5.630 | Reg loss: 0.028 | Tree loss: 5.630 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 5.570 | Reg loss: 0.028 | Tree loss: 5.570 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 5.587 | Reg loss: 0.029 | Tree loss: 5.587 | Accuracy: 0.298828 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 5.531 | Reg loss: 0.029 | Tree loss: 5.531 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 5.525 | Reg loss: 0.029 | Tree loss: 5.525 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 5.520 | Reg loss: 0.029 | Tree loss: 5.520 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 5.523 | Reg loss: 0.029 | Tree loss: 5.523 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 5.497 | Reg loss: 0.029 | Tree loss: 5.497 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 5.453 | Reg loss: 0.029 | Tree loss: 5.453 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 5.424 | Reg loss: 0.029 | Tree loss: 5.424 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 5.443 | Reg loss: 0.029 | Tree loss: 5.443 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 5.436 | Reg loss: 0.029 | Tree loss: 5.436 | Accuracy: 0.259766 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 5.421 | Reg loss: 0.029 | Tree loss: 5.421 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 5.378 | Reg loss: 0.029 | Tree loss: 5.378 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 5.384 | Reg loss: 0.030 | Tree loss: 5.384 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 5.337 | Reg loss: 0.030 | Tree loss: 5.337 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 5.365 | Reg loss: 0.030 | Tree loss: 5.365 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 5.307 | Reg loss: 0.030 | Tree loss: 5.307 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 5.299 | Reg loss: 0.030 | Tree loss: 5.299 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 5.275 | Reg loss: 0.030 | Tree loss: 5.275 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 5.267 | Reg loss: 0.030 | Tree loss: 5.267 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 5.241 | Reg loss: 0.030 | Tree loss: 5.241 | Accuracy: 0.325911 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 5.478 | Reg loss: 0.028 | Tree loss: 5.478 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 5.512 | Reg loss: 0.029 | Tree loss: 5.512 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 5.512 | Reg loss: 0.029 | Tree loss: 5.512 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 5.508 | Reg loss: 0.029 | Tree loss: 5.508 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 5.454 | Reg loss: 0.029 | Tree loss: 5.454 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 5.435 | Reg loss: 0.029 | Tree loss: 5.435 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 5.413 | Reg loss: 0.029 | Tree loss: 5.413 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 5.374 | Reg loss: 0.029 | Tree loss: 5.374 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 5.370 | Reg loss: 0.029 | Tree loss: 5.370 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 5.342 | Reg loss: 0.029 | Tree loss: 5.342 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 5.349 | Reg loss: 0.029 | Tree loss: 5.349 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 5.277 | Reg loss: 0.029 | Tree loss: 5.277 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 5.360 | Reg loss: 0.029 | Tree loss: 5.360 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 5.259 | Reg loss: 0.029 | Tree loss: 5.259 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 5.284 | Reg loss: 0.029 | Tree loss: 5.284 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 5.272 | Reg loss: 0.029 | Tree loss: 5.272 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 5.181 | Reg loss: 0.029 | Tree loss: 5.181 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 5.214 | Reg loss: 0.029 | Tree loss: 5.214 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 5.229 | Reg loss: 0.029 | Tree loss: 5.229 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 5.188 | Reg loss: 0.029 | Tree loss: 5.188 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 5.157 | Reg loss: 0.030 | Tree loss: 5.157 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 5.199 | Reg loss: 0.030 | Tree loss: 5.199 | Accuracy: 0.250000 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 5.175 | Reg loss: 0.030 | Tree loss: 5.175 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 5.108 | Reg loss: 0.030 | Tree loss: 5.108 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 5.110 | Reg loss: 0.030 | Tree loss: 5.110 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 5.148 | Reg loss: 0.030 | Tree loss: 5.148 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 5.069 | Reg loss: 0.030 | Tree loss: 5.069 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 5.050 | Reg loss: 0.030 | Tree loss: 5.050 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 5.063 | Reg loss: 0.030 | Tree loss: 5.063 | Accuracy: 0.293522 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 5.305 | Reg loss: 0.029 | Tree loss: 5.305 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 5.277 | Reg loss: 0.029 | Tree loss: 5.277 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 5.319 | Reg loss: 0.029 | Tree loss: 5.319 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 5.250 | Reg loss: 0.029 | Tree loss: 5.250 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 5.237 | Reg loss: 0.029 | Tree loss: 5.237 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 5.243 | Reg loss: 0.029 | Tree loss: 5.243 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 5.193 | Reg loss: 0.029 | Tree loss: 5.193 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 5.173 | Reg loss: 0.029 | Tree loss: 5.173 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 5.184 | Reg loss: 0.029 | Tree loss: 5.184 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 5.148 | Reg loss: 0.029 | Tree loss: 5.148 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 5.166 | Reg loss: 0.029 | Tree loss: 5.166 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 5.103 | Reg loss: 0.029 | Tree loss: 5.103 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 5.079 | Reg loss: 0.029 | Tree loss: 5.079 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 5.105 | Reg loss: 0.029 | Tree loss: 5.105 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 5.036 | Reg loss: 0.029 | Tree loss: 5.036 | Accuracy: 0.291016 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 5.042 | Reg loss: 0.029 | Tree loss: 5.042 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 5.057 | Reg loss: 0.029 | Tree loss: 5.057 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 5.043 | Reg loss: 0.029 | Tree loss: 5.043 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 5.031 | Reg loss: 0.029 | Tree loss: 5.031 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 4.999 | Reg loss: 0.030 | Tree loss: 4.999 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 4.991 | Reg loss: 0.030 | Tree loss: 4.991 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 4.909 | Reg loss: 0.030 | Tree loss: 4.909 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 4.969 | Reg loss: 0.030 | Tree loss: 4.969 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 4.882 | Reg loss: 0.030 | Tree loss: 4.882 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 4.906 | Reg loss: 0.030 | Tree loss: 4.906 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 4.941 | Reg loss: 0.030 | Tree loss: 4.941 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 4.900 | Reg loss: 0.030 | Tree loss: 4.900 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 4.894 | Reg loss: 0.030 | Tree loss: 4.894 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 4.866 | Reg loss: 0.030 | Tree loss: 4.866 | Accuracy: 0.263158 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 5.107 | Reg loss: 0.029 | Tree loss: 5.107 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 5.074 | Reg loss: 0.029 | Tree loss: 5.074 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 5.058 | Reg loss: 0.029 | Tree loss: 5.058 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 5.032 | Reg loss: 0.029 | Tree loss: 5.032 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 5.037 | Reg loss: 0.029 | Tree loss: 5.037 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 5.045 | Reg loss: 0.029 | Tree loss: 5.045 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 5.011 | Reg loss: 0.029 | Tree loss: 5.011 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 4.964 | Reg loss: 0.029 | Tree loss: 4.964 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 5.010 | Reg loss: 0.029 | Tree loss: 5.010 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 4.982 | Reg loss: 0.029 | Tree loss: 4.982 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 4.961 | Reg loss: 0.029 | Tree loss: 4.961 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 4.934 | Reg loss: 0.029 | Tree loss: 4.934 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 4.894 | Reg loss: 0.029 | Tree loss: 4.894 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 4.876 | Reg loss: 0.029 | Tree loss: 4.876 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 4.863 | Reg loss: 0.029 | Tree loss: 4.863 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 4.865 | Reg loss: 0.029 | Tree loss: 4.865 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 4.794 | Reg loss: 0.029 | Tree loss: 4.794 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 4.828 | Reg loss: 0.030 | Tree loss: 4.828 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 4.832 | Reg loss: 0.030 | Tree loss: 4.832 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 4.794 | Reg loss: 0.030 | Tree loss: 4.794 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 4.794 | Reg loss: 0.030 | Tree loss: 4.794 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 4.816 | Reg loss: 0.030 | Tree loss: 4.816 | Accuracy: 0.242188 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 4.746 | Reg loss: 0.030 | Tree loss: 4.746 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 4.732 | Reg loss: 0.030 | Tree loss: 4.732 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 4.768 | Reg loss: 0.030 | Tree loss: 4.768 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 4.695 | Reg loss: 0.030 | Tree loss: 4.695 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 4.717 | Reg loss: 0.030 | Tree loss: 4.717 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 4.689 | Reg loss: 0.030 | Tree loss: 4.689 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 4.651 | Reg loss: 0.030 | Tree loss: 4.651 | Accuracy: 0.297571 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 4.885 | Reg loss: 0.029 | Tree loss: 4.885 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 4.869 | Reg loss: 0.029 | Tree loss: 4.869 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 4.895 | Reg loss: 0.029 | Tree loss: 4.895 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 4.850 | Reg loss: 0.029 | Tree loss: 4.850 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 4.857 | Reg loss: 0.029 | Tree loss: 4.857 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 4.853 | Reg loss: 0.029 | Tree loss: 4.853 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 4.796 | Reg loss: 0.029 | Tree loss: 4.796 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 4.788 | Reg loss: 0.029 | Tree loss: 4.788 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 4.760 | Reg loss: 0.029 | Tree loss: 4.760 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 4.751 | Reg loss: 0.029 | Tree loss: 4.751 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 4.740 | Reg loss: 0.029 | Tree loss: 4.740 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 4.737 | Reg loss: 0.029 | Tree loss: 4.737 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 4.691 | Reg loss: 0.029 | Tree loss: 4.691 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 4.694 | Reg loss: 0.029 | Tree loss: 4.694 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 4.735 | Reg loss: 0.029 | Tree loss: 4.735 | Accuracy: 0.250000 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 4.698 | Reg loss: 0.029 | Tree loss: 4.698 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 4.619 | Reg loss: 0.030 | Tree loss: 4.619 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 4.624 | Reg loss: 0.030 | Tree loss: 4.624 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 4.640 | Reg loss: 0.030 | Tree loss: 4.640 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 4.586 | Reg loss: 0.030 | Tree loss: 4.586 | Accuracy: 0.291016 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 4.617 | Reg loss: 0.030 | Tree loss: 4.617 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 4.575 | Reg loss: 0.030 | Tree loss: 4.575 | Accuracy: 0.259766 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 4.587 | Reg loss: 0.030 | Tree loss: 4.587 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 4.574 | Reg loss: 0.030 | Tree loss: 4.574 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 4.554 | Reg loss: 0.030 | Tree loss: 4.554 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 4.547 | Reg loss: 0.030 | Tree loss: 4.547 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 4.521 | Reg loss: 0.030 | Tree loss: 4.521 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 4.542 | Reg loss: 0.030 | Tree loss: 4.542 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 4.526 | Reg loss: 0.030 | Tree loss: 4.526 | Accuracy: 0.261134 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 4.698 | Reg loss: 0.029 | Tree loss: 4.698 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 4.677 | Reg loss: 0.029 | Tree loss: 4.677 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 4.674 | Reg loss: 0.029 | Tree loss: 4.674 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 4.702 | Reg loss: 0.029 | Tree loss: 4.702 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 4.629 | Reg loss: 0.029 | Tree loss: 4.629 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 4.646 | Reg loss: 0.029 | Tree loss: 4.646 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 4.616 | Reg loss: 0.029 | Tree loss: 4.616 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 4.625 | Reg loss: 0.029 | Tree loss: 4.625 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 4.553 | Reg loss: 0.029 | Tree loss: 4.553 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 4.611 | Reg loss: 0.029 | Tree loss: 4.611 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 4.558 | Reg loss: 0.029 | Tree loss: 4.558 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 4.552 | Reg loss: 0.029 | Tree loss: 4.552 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 4.513 | Reg loss: 0.029 | Tree loss: 4.513 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 4.536 | Reg loss: 0.029 | Tree loss: 4.536 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 4.528 | Reg loss: 0.029 | Tree loss: 4.528 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 4.484 | Reg loss: 0.030 | Tree loss: 4.484 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 4.489 | Reg loss: 0.030 | Tree loss: 4.489 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 4.478 | Reg loss: 0.030 | Tree loss: 4.478 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 4.443 | Reg loss: 0.030 | Tree loss: 4.443 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 4.433 | Reg loss: 0.030 | Tree loss: 4.433 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 4.422 | Reg loss: 0.030 | Tree loss: 4.422 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 4.403 | Reg loss: 0.030 | Tree loss: 4.403 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 4.421 | Reg loss: 0.030 | Tree loss: 4.421 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 4.406 | Reg loss: 0.030 | Tree loss: 4.406 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 4.374 | Reg loss: 0.030 | Tree loss: 4.374 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 4.387 | Reg loss: 0.030 | Tree loss: 4.387 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 4.323 | Reg loss: 0.030 | Tree loss: 4.323 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 4.326 | Reg loss: 0.030 | Tree loss: 4.326 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 4.338 | Reg loss: 0.030 | Tree loss: 4.338 | Accuracy: 0.309717 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 4.564 | Reg loss: 0.029 | Tree loss: 4.564 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 4.531 | Reg loss: 0.029 | Tree loss: 4.531 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 4.531 | Reg loss: 0.029 | Tree loss: 4.531 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 4.478 | Reg loss: 0.029 | Tree loss: 4.478 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 4.471 | Reg loss: 0.029 | Tree loss: 4.471 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 4.407 | Reg loss: 0.029 | Tree loss: 4.407 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 4.463 | Reg loss: 0.029 | Tree loss: 4.463 | Accuracy: 0.238281 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 4.387 | Reg loss: 0.029 | Tree loss: 4.387 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 4.416 | Reg loss: 0.029 | Tree loss: 4.416 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 4.446 | Reg loss: 0.029 | Tree loss: 4.446 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 4.408 | Reg loss: 0.029 | Tree loss: 4.408 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 4.379 | Reg loss: 0.029 | Tree loss: 4.379 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 4.377 | Reg loss: 0.029 | Tree loss: 4.377 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 4.352 | Reg loss: 0.029 | Tree loss: 4.352 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 4.322 | Reg loss: 0.030 | Tree loss: 4.322 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 4.320 | Reg loss: 0.030 | Tree loss: 4.320 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 4.278 | Reg loss: 0.030 | Tree loss: 4.278 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 4.257 | Reg loss: 0.030 | Tree loss: 4.257 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 4.293 | Reg loss: 0.030 | Tree loss: 4.293 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 4.288 | Reg loss: 0.030 | Tree loss: 4.288 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 4.249 | Reg loss: 0.030 | Tree loss: 4.249 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 4.227 | Reg loss: 0.030 | Tree loss: 4.227 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 4.201 | Reg loss: 0.030 | Tree loss: 4.201 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 4.210 | Reg loss: 0.030 | Tree loss: 4.210 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 4.190 | Reg loss: 0.030 | Tree loss: 4.190 | Accuracy: 0.322266 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 4.204 | Reg loss: 0.030 | Tree loss: 4.204 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 4.165 | Reg loss: 0.030 | Tree loss: 4.165 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 4.192 | Reg loss: 0.030 | Tree loss: 4.192 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 4.147 | Reg loss: 0.030 | Tree loss: 4.147 | Accuracy: 0.321862 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 4.352 | Reg loss: 0.029 | Tree loss: 4.352 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 4.328 | Reg loss: 0.029 | Tree loss: 4.328 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 4.328 | Reg loss: 0.029 | Tree loss: 4.328 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 4.304 | Reg loss: 0.029 | Tree loss: 4.304 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 4.292 | Reg loss: 0.029 | Tree loss: 4.292 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 4.306 | Reg loss: 0.029 | Tree loss: 4.306 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 4.257 | Reg loss: 0.029 | Tree loss: 4.257 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 4.246 | Reg loss: 0.029 | Tree loss: 4.246 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 4.247 | Reg loss: 0.029 | Tree loss: 4.247 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 4.239 | Reg loss: 0.029 | Tree loss: 4.239 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 4.225 | Reg loss: 0.029 | Tree loss: 4.225 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 4.176 | Reg loss: 0.029 | Tree loss: 4.176 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 4.211 | Reg loss: 0.030 | Tree loss: 4.211 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 4.152 | Reg loss: 0.030 | Tree loss: 4.152 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 4.174 | Reg loss: 0.030 | Tree loss: 4.174 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 4.143 | Reg loss: 0.030 | Tree loss: 4.143 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 4.159 | Reg loss: 0.030 | Tree loss: 4.159 | Accuracy: 0.265625 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 4.105 | Reg loss: 0.030 | Tree loss: 4.105 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 4.152 | Reg loss: 0.030 | Tree loss: 4.152 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 4.123 | Reg loss: 0.030 | Tree loss: 4.123 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 4.084 | Reg loss: 0.030 | Tree loss: 4.084 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 4.087 | Reg loss: 0.030 | Tree loss: 4.087 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 4.111 | Reg loss: 0.030 | Tree loss: 4.111 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 4.080 | Reg loss: 0.030 | Tree loss: 4.080 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 3.985 | Reg loss: 0.030 | Tree loss: 3.985 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 4.047 | Reg loss: 0.030 | Tree loss: 4.047 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 3.985 | Reg loss: 0.030 | Tree loss: 3.985 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 3.968 | Reg loss: 0.030 | Tree loss: 3.968 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 3.982 | Reg loss: 0.030 | Tree loss: 3.982 | Accuracy: 0.275304 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 4.154 | Reg loss: 0.029 | Tree loss: 4.154 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 4.175 | Reg loss: 0.029 | Tree loss: 4.175 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 4.168 | Reg loss: 0.029 | Tree loss: 4.168 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 4.136 | Reg loss: 0.029 | Tree loss: 4.136 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 4.115 | Reg loss: 0.029 | Tree loss: 4.115 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 4.140 | Reg loss: 0.029 | Tree loss: 4.140 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 4.053 | Reg loss: 0.029 | Tree loss: 4.053 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 4.093 | Reg loss: 0.029 | Tree loss: 4.093 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 4.097 | Reg loss: 0.029 | Tree loss: 4.097 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 4.017 | Reg loss: 0.029 | Tree loss: 4.017 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 4.042 | Reg loss: 0.029 | Tree loss: 4.042 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 4.026 | Reg loss: 0.030 | Tree loss: 4.026 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 4.038 | Reg loss: 0.030 | Tree loss: 4.038 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 4.001 | Reg loss: 0.030 | Tree loss: 4.001 | Accuracy: 0.343750 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 4.000 | Reg loss: 0.030 | Tree loss: 4.000 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 3.982 | Reg loss: 0.030 | Tree loss: 3.982 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 3.968 | Reg loss: 0.030 | Tree loss: 3.968 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 3.964 | Reg loss: 0.030 | Tree loss: 3.964 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 3.963 | Reg loss: 0.030 | Tree loss: 3.963 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 3.948 | Reg loss: 0.030 | Tree loss: 3.948 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 3.939 | Reg loss: 0.030 | Tree loss: 3.939 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 3.925 | Reg loss: 0.030 | Tree loss: 3.925 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 3.921 | Reg loss: 0.030 | Tree loss: 3.921 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 3.901 | Reg loss: 0.030 | Tree loss: 3.901 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 3.930 | Reg loss: 0.030 | Tree loss: 3.930 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 3.900 | Reg loss: 0.030 | Tree loss: 3.900 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 3.852 | Reg loss: 0.030 | Tree loss: 3.852 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 3.819 | Reg loss: 0.030 | Tree loss: 3.819 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 3.862 | Reg loss: 0.030 | Tree loss: 3.862 | Accuracy: 0.297571 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 4.039 | Reg loss: 0.029 | Tree loss: 4.039 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 3.983 | Reg loss: 0.029 | Tree loss: 3.983 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 4.046 | Reg loss: 0.029 | Tree loss: 4.046 | Accuracy: 0.246094 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 3.975 | Reg loss: 0.029 | Tree loss: 3.975 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 4.002 | Reg loss: 0.029 | Tree loss: 4.002 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 3.958 | Reg loss: 0.029 | Tree loss: 3.958 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 3.951 | Reg loss: 0.029 | Tree loss: 3.951 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 3.966 | Reg loss: 0.029 | Tree loss: 3.966 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 3.920 | Reg loss: 0.029 | Tree loss: 3.920 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 3.917 | Reg loss: 0.029 | Tree loss: 3.917 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 3.881 | Reg loss: 0.029 | Tree loss: 3.881 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 3.885 | Reg loss: 0.030 | Tree loss: 3.885 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 3.852 | Reg loss: 0.030 | Tree loss: 3.852 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 3.851 | Reg loss: 0.030 | Tree loss: 3.851 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 3.824 | Reg loss: 0.030 | Tree loss: 3.824 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 3.819 | Reg loss: 0.030 | Tree loss: 3.819 | Accuracy: 0.255859 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 3.823 | Reg loss: 0.030 | Tree loss: 3.823 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 3.853 | Reg loss: 0.030 | Tree loss: 3.853 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 3.782 | Reg loss: 0.030 | Tree loss: 3.782 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 3.735 | Reg loss: 0.030 | Tree loss: 3.735 | Accuracy: 0.347656 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 3.755 | Reg loss: 0.030 | Tree loss: 3.755 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 3.781 | Reg loss: 0.030 | Tree loss: 3.781 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 3.748 | Reg loss: 0.030 | Tree loss: 3.748 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 3.759 | Reg loss: 0.030 | Tree loss: 3.759 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 3.733 | Reg loss: 0.030 | Tree loss: 3.733 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 3.683 | Reg loss: 0.030 | Tree loss: 3.683 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 3.698 | Reg loss: 0.030 | Tree loss: 3.698 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 3.693 | Reg loss: 0.030 | Tree loss: 3.693 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 3.676 | Reg loss: 0.030 | Tree loss: 3.676 | Accuracy: 0.295547 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 3.878 | Reg loss: 0.029 | Tree loss: 3.878 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 3.842 | Reg loss: 0.029 | Tree loss: 3.842 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 3.859 | Reg loss: 0.029 | Tree loss: 3.859 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 3.821 | Reg loss: 0.029 | Tree loss: 3.821 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 3.780 | Reg loss: 0.029 | Tree loss: 3.780 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 3.822 | Reg loss: 0.029 | Tree loss: 3.822 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 3.792 | Reg loss: 0.029 | Tree loss: 3.792 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 3.777 | Reg loss: 0.029 | Tree loss: 3.777 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 3.731 | Reg loss: 0.029 | Tree loss: 3.731 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 3.756 | Reg loss: 0.029 | Tree loss: 3.756 | Accuracy: 0.255859 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 3.725 | Reg loss: 0.029 | Tree loss: 3.725 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 3.704 | Reg loss: 0.029 | Tree loss: 3.704 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 3.719 | Reg loss: 0.030 | Tree loss: 3.719 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 3.709 | Reg loss: 0.030 | Tree loss: 3.709 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 3.710 | Reg loss: 0.030 | Tree loss: 3.710 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 3.715 | Reg loss: 0.030 | Tree loss: 3.715 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 3.690 | Reg loss: 0.030 | Tree loss: 3.690 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 3.651 | Reg loss: 0.030 | Tree loss: 3.651 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 3.674 | Reg loss: 0.030 | Tree loss: 3.674 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 3.660 | Reg loss: 0.030 | Tree loss: 3.660 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 3.593 | Reg loss: 0.030 | Tree loss: 3.593 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 3.576 | Reg loss: 0.030 | Tree loss: 3.576 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 3.599 | Reg loss: 0.030 | Tree loss: 3.599 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 3.614 | Reg loss: 0.030 | Tree loss: 3.614 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 3.605 | Reg loss: 0.030 | Tree loss: 3.605 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 3.549 | Reg loss: 0.030 | Tree loss: 3.549 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 3.563 | Reg loss: 0.030 | Tree loss: 3.563 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 3.599 | Reg loss: 0.030 | Tree loss: 3.599 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 3.530 | Reg loss: 0.030 | Tree loss: 3.530 | Accuracy: 0.311741 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 3.773 | Reg loss: 0.029 | Tree loss: 3.773 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 3.696 | Reg loss: 0.029 | Tree loss: 3.696 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 3.664 | Reg loss: 0.029 | Tree loss: 3.664 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 3.669 | Reg loss: 0.029 | Tree loss: 3.669 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 3.638 | Reg loss: 0.029 | Tree loss: 3.638 | Accuracy: 0.312500 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 3.682 | Reg loss: 0.029 | Tree loss: 3.682 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 3.633 | Reg loss: 0.029 | Tree loss: 3.633 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 3.614 | Reg loss: 0.029 | Tree loss: 3.614 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 3.638 | Reg loss: 0.029 | Tree loss: 3.638 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 3.625 | Reg loss: 0.029 | Tree loss: 3.625 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 3.613 | Reg loss: 0.029 | Tree loss: 3.613 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 3.612 | Reg loss: 0.029 | Tree loss: 3.612 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 3.536 | Reg loss: 0.029 | Tree loss: 3.536 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 3.556 | Reg loss: 0.030 | Tree loss: 3.556 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 3.512 | Reg loss: 0.030 | Tree loss: 3.512 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 3.559 | Reg loss: 0.030 | Tree loss: 3.559 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 3.563 | Reg loss: 0.030 | Tree loss: 3.563 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 3.500 | Reg loss: 0.030 | Tree loss: 3.500 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 3.497 | Reg loss: 0.030 | Tree loss: 3.497 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 3.472 | Reg loss: 0.030 | Tree loss: 3.472 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 3.484 | Reg loss: 0.030 | Tree loss: 3.484 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 3.453 | Reg loss: 0.030 | Tree loss: 3.453 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 3.494 | Reg loss: 0.030 | Tree loss: 3.494 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 3.447 | Reg loss: 0.030 | Tree loss: 3.447 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 3.432 | Reg loss: 0.030 | Tree loss: 3.432 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 3.459 | Reg loss: 0.030 | Tree loss: 3.459 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 3.458 | Reg loss: 0.030 | Tree loss: 3.458 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 3.416 | Reg loss: 0.030 | Tree loss: 3.416 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 3.417 | Reg loss: 0.030 | Tree loss: 3.417 | Accuracy: 0.313765 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 3.570 | Reg loss: 0.029 | Tree loss: 3.570 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 3.600 | Reg loss: 0.029 | Tree loss: 3.600 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 3.552 | Reg loss: 0.029 | Tree loss: 3.552 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 3.506 | Reg loss: 0.029 | Tree loss: 3.506 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 3.507 | Reg loss: 0.029 | Tree loss: 3.507 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 3.545 | Reg loss: 0.029 | Tree loss: 3.545 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 3.515 | Reg loss: 0.029 | Tree loss: 3.515 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 3.502 | Reg loss: 0.029 | Tree loss: 3.502 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 3.497 | Reg loss: 0.029 | Tree loss: 3.497 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 3.472 | Reg loss: 0.029 | Tree loss: 3.472 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 3.467 | Reg loss: 0.029 | Tree loss: 3.467 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 3.444 | Reg loss: 0.029 | Tree loss: 3.444 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 3.434 | Reg loss: 0.029 | Tree loss: 3.434 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 3.392 | Reg loss: 0.029 | Tree loss: 3.392 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 3.385 | Reg loss: 0.029 | Tree loss: 3.385 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 3.410 | Reg loss: 0.030 | Tree loss: 3.410 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 3.407 | Reg loss: 0.030 | Tree loss: 3.407 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 3.388 | Reg loss: 0.030 | Tree loss: 3.388 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 3.379 | Reg loss: 0.030 | Tree loss: 3.379 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 3.344 | Reg loss: 0.030 | Tree loss: 3.344 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 3.372 | Reg loss: 0.030 | Tree loss: 3.372 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 3.343 | Reg loss: 0.030 | Tree loss: 3.343 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 3.310 | Reg loss: 0.030 | Tree loss: 3.310 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 3.335 | Reg loss: 0.030 | Tree loss: 3.335 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 3.326 | Reg loss: 0.030 | Tree loss: 3.326 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 3.315 | Reg loss: 0.030 | Tree loss: 3.315 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 3.313 | Reg loss: 0.030 | Tree loss: 3.313 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 3.275 | Reg loss: 0.030 | Tree loss: 3.275 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 3.259 | Reg loss: 0.030 | Tree loss: 3.259 | Accuracy: 0.309717 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 3.476 | Reg loss: 0.029 | Tree loss: 3.476 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 3.457 | Reg loss: 0.029 | Tree loss: 3.457 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 3.458 | Reg loss: 0.029 | Tree loss: 3.458 | Accuracy: 0.257812 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 3.447 | Reg loss: 0.029 | Tree loss: 3.447 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 3.387 | Reg loss: 0.029 | Tree loss: 3.387 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 3.395 | Reg loss: 0.029 | Tree loss: 3.395 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 3.331 | Reg loss: 0.029 | Tree loss: 3.331 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 3.320 | Reg loss: 0.029 | Tree loss: 3.320 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 3.375 | Reg loss: 0.029 | Tree loss: 3.375 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 3.381 | Reg loss: 0.029 | Tree loss: 3.381 | Accuracy: 0.291016 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 3.349 | Reg loss: 0.029 | Tree loss: 3.349 | Accuracy: 0.251953 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 3.366 | Reg loss: 0.029 | Tree loss: 3.366 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 3.335 | Reg loss: 0.029 | Tree loss: 3.335 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 3.315 | Reg loss: 0.029 | Tree loss: 3.315 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 3.261 | Reg loss: 0.029 | Tree loss: 3.261 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 3.228 | Reg loss: 0.029 | Tree loss: 3.228 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 3.233 | Reg loss: 0.029 | Tree loss: 3.233 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 3.256 | Reg loss: 0.030 | Tree loss: 3.256 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 3.233 | Reg loss: 0.030 | Tree loss: 3.233 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 3.217 | Reg loss: 0.030 | Tree loss: 3.217 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 3.220 | Reg loss: 0.030 | Tree loss: 3.220 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 3.218 | Reg loss: 0.030 | Tree loss: 3.218 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 3.196 | Reg loss: 0.030 | Tree loss: 3.196 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 3.164 | Reg loss: 0.030 | Tree loss: 3.164 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 3.196 | Reg loss: 0.030 | Tree loss: 3.196 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 3.164 | Reg loss: 0.030 | Tree loss: 3.164 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 3.171 | Reg loss: 0.030 | Tree loss: 3.171 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 3.121 | Reg loss: 0.030 | Tree loss: 3.121 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 3.154 | Reg loss: 0.030 | Tree loss: 3.154 | Accuracy: 0.287449 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 3.330 | Reg loss: 0.029 | Tree loss: 3.330 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 3.280 | Reg loss: 0.029 | Tree loss: 3.280 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 3.305 | Reg loss: 0.029 | Tree loss: 3.305 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 3.272 | Reg loss: 0.029 | Tree loss: 3.272 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 3.284 | Reg loss: 0.029 | Tree loss: 3.284 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 3.264 | Reg loss: 0.029 | Tree loss: 3.264 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 3.223 | Reg loss: 0.029 | Tree loss: 3.223 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 3.240 | Reg loss: 0.029 | Tree loss: 3.240 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 3.248 | Reg loss: 0.029 | Tree loss: 3.248 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 3.204 | Reg loss: 0.029 | Tree loss: 3.204 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 3.205 | Reg loss: 0.029 | Tree loss: 3.205 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 3.202 | Reg loss: 0.029 | Tree loss: 3.202 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 3.183 | Reg loss: 0.029 | Tree loss: 3.183 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 3.191 | Reg loss: 0.029 | Tree loss: 3.191 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 3.148 | Reg loss: 0.029 | Tree loss: 3.148 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 3.163 | Reg loss: 0.029 | Tree loss: 3.163 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 3.171 | Reg loss: 0.029 | Tree loss: 3.171 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 3.195 | Reg loss: 0.029 | Tree loss: 3.195 | Accuracy: 0.248047 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 3.122 | Reg loss: 0.029 | Tree loss: 3.122 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 3.126 | Reg loss: 0.030 | Tree loss: 3.126 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 3.027 | Reg loss: 0.030 | Tree loss: 3.027 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 3.124 | Reg loss: 0.030 | Tree loss: 3.124 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 3.083 | Reg loss: 0.030 | Tree loss: 3.083 | Accuracy: 0.236328 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 3.091 | Reg loss: 0.030 | Tree loss: 3.091 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 3.034 | Reg loss: 0.030 | Tree loss: 3.034 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 3.048 | Reg loss: 0.030 | Tree loss: 3.048 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 3.035 | Reg loss: 0.030 | Tree loss: 3.035 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 3.055 | Reg loss: 0.030 | Tree loss: 3.055 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 3.016 | Reg loss: 0.030 | Tree loss: 3.016 | Accuracy: 0.293522 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 3.194 | Reg loss: 0.029 | Tree loss: 3.194 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 3.204 | Reg loss: 0.029 | Tree loss: 3.204 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 3.151 | Reg loss: 0.029 | Tree loss: 3.151 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 3.141 | Reg loss: 0.029 | Tree loss: 3.141 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 3.208 | Reg loss: 0.029 | Tree loss: 3.208 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 3.166 | Reg loss: 0.029 | Tree loss: 3.166 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 3.185 | Reg loss: 0.029 | Tree loss: 3.185 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 3.154 | Reg loss: 0.029 | Tree loss: 3.154 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 3.130 | Reg loss: 0.029 | Tree loss: 3.130 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 3.113 | Reg loss: 0.029 | Tree loss: 3.113 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 3.085 | Reg loss: 0.029 | Tree loss: 3.085 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 3.086 | Reg loss: 0.029 | Tree loss: 3.086 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 3.069 | Reg loss: 0.029 | Tree loss: 3.069 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 3.056 | Reg loss: 0.029 | Tree loss: 3.056 | Accuracy: 0.308594 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 3.010 | Reg loss: 0.029 | Tree loss: 3.010 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 3.017 | Reg loss: 0.029 | Tree loss: 3.017 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 3.012 | Reg loss: 0.029 | Tree loss: 3.012 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 2.992 | Reg loss: 0.029 | Tree loss: 2.992 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 3.000 | Reg loss: 0.029 | Tree loss: 3.000 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 2.940 | Reg loss: 0.030 | Tree loss: 2.940 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 2.997 | Reg loss: 0.030 | Tree loss: 2.997 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 2.928 | Reg loss: 0.030 | Tree loss: 2.928 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 2.928 | Reg loss: 0.030 | Tree loss: 2.928 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 2.898 | Reg loss: 0.030 | Tree loss: 2.898 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 2.933 | Reg loss: 0.030 | Tree loss: 2.933 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 2.902 | Reg loss: 0.030 | Tree loss: 2.902 | Accuracy: 0.299595 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 3.073 | Reg loss: 0.029 | Tree loss: 3.073 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 3.110 | Reg loss: 0.029 | Tree loss: 3.110 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 3.040 | Reg loss: 0.029 | Tree loss: 3.040 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 3.048 | Reg loss: 0.029 | Tree loss: 3.048 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 3.033 | Reg loss: 0.029 | Tree loss: 3.033 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 3.072 | Reg loss: 0.029 | Tree loss: 3.072 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 3.062 | Reg loss: 0.029 | Tree loss: 3.062 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 3.047 | Reg loss: 0.029 | Tree loss: 3.047 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 2.982 | Reg loss: 0.029 | Tree loss: 2.982 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 2.951 | Reg loss: 0.029 | Tree loss: 2.951 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 2.988 | Reg loss: 0.029 | Tree loss: 2.988 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 3.005 | Reg loss: 0.029 | Tree loss: 3.005 | Accuracy: 0.251953 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 2.935 | Reg loss: 0.029 | Tree loss: 2.935 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 2.947 | Reg loss: 0.029 | Tree loss: 2.947 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 2.931 | Reg loss: 0.029 | Tree loss: 2.931 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 2.933 | Reg loss: 0.029 | Tree loss: 2.933 | Accuracy: 0.244141 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 2.908 | Reg loss: 0.029 | Tree loss: 2.908 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 2.911 | Reg loss: 0.029 | Tree loss: 2.911 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 2.834 | Reg loss: 0.029 | Tree loss: 2.834 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 2.842 | Reg loss: 0.029 | Tree loss: 2.842 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 2.841 | Reg loss: 0.029 | Tree loss: 2.841 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 2.888 | Reg loss: 0.029 | Tree loss: 2.888 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 2.831 | Reg loss: 0.029 | Tree loss: 2.831 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 2.846 | Reg loss: 0.030 | Tree loss: 2.846 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 2.822 | Reg loss: 0.030 | Tree loss: 2.822 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 2.796 | Reg loss: 0.030 | Tree loss: 2.796 | Accuracy: 0.313765 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 2.943 | Reg loss: 0.029 | Tree loss: 2.943 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 2.909 | Reg loss: 0.029 | Tree loss: 2.909 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 2.900 | Reg loss: 0.029 | Tree loss: 2.900 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 2.853 | Reg loss: 0.029 | Tree loss: 2.853 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 2.879 | Reg loss: 0.029 | Tree loss: 2.879 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 2.865 | Reg loss: 0.029 | Tree loss: 2.865 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 2.815 | Reg loss: 0.029 | Tree loss: 2.815 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 2.847 | Reg loss: 0.029 | Tree loss: 2.847 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 2.868 | Reg loss: 0.029 | Tree loss: 2.868 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 2.835 | Reg loss: 0.029 | Tree loss: 2.835 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 2.833 | Reg loss: 0.029 | Tree loss: 2.833 | Accuracy: 0.240234 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 2.809 | Reg loss: 0.029 | Tree loss: 2.809 | Accuracy: 0.298828 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 2.745 | Reg loss: 0.029 | Tree loss: 2.745 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 2.798 | Reg loss: 0.029 | Tree loss: 2.798 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 2.757 | Reg loss: 0.029 | Tree loss: 2.757 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 2.777 | Reg loss: 0.029 | Tree loss: 2.777 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 2.762 | Reg loss: 0.029 | Tree loss: 2.762 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 2.701 | Reg loss: 0.029 | Tree loss: 2.701 | Accuracy: 0.343750 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 2.777 | Reg loss: 0.029 | Tree loss: 2.777 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 2.713 | Reg loss: 0.029 | Tree loss: 2.713 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 2.688 | Reg loss: 0.029 | Tree loss: 2.688 | Accuracy: 0.338057 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 2.905 | Reg loss: 0.029 | Tree loss: 2.905 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 2.866 | Reg loss: 0.029 | Tree loss: 2.866 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 2.927 | Reg loss: 0.029 | Tree loss: 2.927 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 2.896 | Reg loss: 0.029 | Tree loss: 2.896 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 2.876 | Reg loss: 0.029 | Tree loss: 2.876 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 2.861 | Reg loss: 0.029 | Tree loss: 2.861 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 2.832 | Reg loss: 0.029 | Tree loss: 2.832 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 2.836 | Reg loss: 0.029 | Tree loss: 2.836 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 2.814 | Reg loss: 0.029 | Tree loss: 2.814 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 2.805 | Reg loss: 0.029 | Tree loss: 2.805 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 2.777 | Reg loss: 0.029 | Tree loss: 2.777 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 2.748 | Reg loss: 0.029 | Tree loss: 2.748 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 2.812 | Reg loss: 0.029 | Tree loss: 2.812 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 2.770 | Reg loss: 0.029 | Tree loss: 2.770 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 2.766 | Reg loss: 0.029 | Tree loss: 2.766 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 2.788 | Reg loss: 0.029 | Tree loss: 2.788 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 2.763 | Reg loss: 0.029 | Tree loss: 2.763 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 2.722 | Reg loss: 0.029 | Tree loss: 2.722 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 2.690 | Reg loss: 0.029 | Tree loss: 2.690 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 2.716 | Reg loss: 0.029 | Tree loss: 2.716 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 2.725 | Reg loss: 0.029 | Tree loss: 2.725 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 2.665 | Reg loss: 0.029 | Tree loss: 2.665 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 2.702 | Reg loss: 0.029 | Tree loss: 2.702 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 2.691 | Reg loss: 0.029 | Tree loss: 2.691 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 2.683 | Reg loss: 0.029 | Tree loss: 2.683 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 2.680 | Reg loss: 0.029 | Tree loss: 2.680 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 2.644 | Reg loss: 0.029 | Tree loss: 2.644 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 2.638 | Reg loss: 0.029 | Tree loss: 2.638 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 2.605 | Reg loss: 0.029 | Tree loss: 2.605 | Accuracy: 0.287449 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 2.806 | Reg loss: 0.029 | Tree loss: 2.806 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 2.801 | Reg loss: 0.029 | Tree loss: 2.801 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 2.803 | Reg loss: 0.029 | Tree loss: 2.803 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 2.781 | Reg loss: 0.029 | Tree loss: 2.781 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 2.776 | Reg loss: 0.029 | Tree loss: 2.776 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 2.760 | Reg loss: 0.029 | Tree loss: 2.760 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 2.755 | Reg loss: 0.029 | Tree loss: 2.755 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 2.709 | Reg loss: 0.029 | Tree loss: 2.709 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 2.706 | Reg loss: 0.029 | Tree loss: 2.706 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 2.713 | Reg loss: 0.029 | Tree loss: 2.713 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 2.735 | Reg loss: 0.029 | Tree loss: 2.735 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 2.669 | Reg loss: 0.029 | Tree loss: 2.669 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 2.708 | Reg loss: 0.029 | Tree loss: 2.708 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 2.661 | Reg loss: 0.029 | Tree loss: 2.661 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 2.704 | Reg loss: 0.029 | Tree loss: 2.704 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 2.709 | Reg loss: 0.029 | Tree loss: 2.709 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 2.648 | Reg loss: 0.029 | Tree loss: 2.648 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 2.593 | Reg loss: 0.029 | Tree loss: 2.593 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 2.666 | Reg loss: 0.029 | Tree loss: 2.666 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 2.638 | Reg loss: 0.029 | Tree loss: 2.638 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 2.639 | Reg loss: 0.029 | Tree loss: 2.639 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 2.630 | Reg loss: 0.029 | Tree loss: 2.630 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 2.600 | Reg loss: 0.029 | Tree loss: 2.600 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 2.584 | Reg loss: 0.029 | Tree loss: 2.584 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 2.606 | Reg loss: 0.029 | Tree loss: 2.606 | Accuracy: 0.279297 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 2.608 | Reg loss: 0.029 | Tree loss: 2.608 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 2.577 | Reg loss: 0.029 | Tree loss: 2.577 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 2.588 | Reg loss: 0.029 | Tree loss: 2.588 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 2.571 | Reg loss: 0.029 | Tree loss: 2.571 | Accuracy: 0.295547 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 2.732 | Reg loss: 0.029 | Tree loss: 2.732 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 2.672 | Reg loss: 0.029 | Tree loss: 2.672 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 2.774 | Reg loss: 0.028 | Tree loss: 2.774 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 2.697 | Reg loss: 0.028 | Tree loss: 2.697 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 2.693 | Reg loss: 0.028 | Tree loss: 2.693 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 2.695 | Reg loss: 0.029 | Tree loss: 2.695 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 2.669 | Reg loss: 0.029 | Tree loss: 2.669 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 2.682 | Reg loss: 0.029 | Tree loss: 2.682 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 2.612 | Reg loss: 0.029 | Tree loss: 2.612 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 2.595 | Reg loss: 0.029 | Tree loss: 2.595 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 2.620 | Reg loss: 0.029 | Tree loss: 2.620 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 2.667 | Reg loss: 0.029 | Tree loss: 2.667 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 2.650 | Reg loss: 0.029 | Tree loss: 2.650 | Accuracy: 0.259766 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 2.596 | Reg loss: 0.029 | Tree loss: 2.596 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 2.577 | Reg loss: 0.029 | Tree loss: 2.577 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 2.581 | Reg loss: 0.029 | Tree loss: 2.581 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 2.596 | Reg loss: 0.029 | Tree loss: 2.596 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 2.580 | Reg loss: 0.029 | Tree loss: 2.580 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 2.602 | Reg loss: 0.029 | Tree loss: 2.602 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 2.563 | Reg loss: 0.029 | Tree loss: 2.563 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 2.501 | Reg loss: 0.029 | Tree loss: 2.501 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 2.509 | Reg loss: 0.029 | Tree loss: 2.509 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 2.532 | Reg loss: 0.029 | Tree loss: 2.532 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 2.555 | Reg loss: 0.029 | Tree loss: 2.555 | Accuracy: 0.257812 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 2.510 | Reg loss: 0.029 | Tree loss: 2.510 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 2.507 | Reg loss: 0.029 | Tree loss: 2.507 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 2.500 | Reg loss: 0.029 | Tree loss: 2.500 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 2.501 | Reg loss: 0.029 | Tree loss: 2.501 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 2.485 | Reg loss: 0.029 | Tree loss: 2.485 | Accuracy: 0.285425 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 2.661 | Reg loss: 0.028 | Tree loss: 2.661 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 2.618 | Reg loss: 0.028 | Tree loss: 2.618 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 2.627 | Reg loss: 0.028 | Tree loss: 2.627 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 2.588 | Reg loss: 0.028 | Tree loss: 2.588 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 2.599 | Reg loss: 0.028 | Tree loss: 2.599 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 2.657 | Reg loss: 0.028 | Tree loss: 2.657 | Accuracy: 0.257812 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 2.587 | Reg loss: 0.028 | Tree loss: 2.587 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 2.567 | Reg loss: 0.028 | Tree loss: 2.567 | Accuracy: 0.351562 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 2.593 | Reg loss: 0.028 | Tree loss: 2.593 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 2.539 | Reg loss: 0.028 | Tree loss: 2.539 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 2.553 | Reg loss: 0.028 | Tree loss: 2.553 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 2.528 | Reg loss: 0.028 | Tree loss: 2.528 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 2.572 | Reg loss: 0.028 | Tree loss: 2.572 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 2.549 | Reg loss: 0.028 | Tree loss: 2.549 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 2.548 | Reg loss: 0.028 | Tree loss: 2.548 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 2.478 | Reg loss: 0.028 | Tree loss: 2.478 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 2.527 | Reg loss: 0.029 | Tree loss: 2.527 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 2.518 | Reg loss: 0.029 | Tree loss: 2.518 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 2.453 | Reg loss: 0.029 | Tree loss: 2.453 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 2.531 | Reg loss: 0.029 | Tree loss: 2.531 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 2.480 | Reg loss: 0.029 | Tree loss: 2.480 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 2.463 | Reg loss: 0.029 | Tree loss: 2.463 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 2.481 | Reg loss: 0.029 | Tree loss: 2.481 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 2.449 | Reg loss: 0.029 | Tree loss: 2.449 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 2.428 | Reg loss: 0.029 | Tree loss: 2.428 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 2.448 | Reg loss: 0.029 | Tree loss: 2.448 | Accuracy: 0.261719 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 2.399 | Reg loss: 0.029 | Tree loss: 2.399 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 2.399 | Reg loss: 0.029 | Tree loss: 2.399 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 2.415 | Reg loss: 0.029 | Tree loss: 2.415 | Accuracy: 0.297571 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 2.555 | Reg loss: 0.028 | Tree loss: 2.555 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 2.568 | Reg loss: 0.028 | Tree loss: 2.568 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 2.511 | Reg loss: 0.028 | Tree loss: 2.511 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 2.526 | Reg loss: 0.028 | Tree loss: 2.526 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 2.538 | Reg loss: 0.028 | Tree loss: 2.538 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 2.530 | Reg loss: 0.028 | Tree loss: 2.530 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 2.527 | Reg loss: 0.028 | Tree loss: 2.527 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 2.505 | Reg loss: 0.028 | Tree loss: 2.505 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 2.509 | Reg loss: 0.028 | Tree loss: 2.509 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 2.509 | Reg loss: 0.028 | Tree loss: 2.509 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 2.454 | Reg loss: 0.028 | Tree loss: 2.454 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 2.437 | Reg loss: 0.028 | Tree loss: 2.437 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 2.483 | Reg loss: 0.028 | Tree loss: 2.483 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 2.442 | Reg loss: 0.028 | Tree loss: 2.442 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 2.492 | Reg loss: 0.028 | Tree loss: 2.492 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 2.441 | Reg loss: 0.028 | Tree loss: 2.441 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 2.448 | Reg loss: 0.028 | Tree loss: 2.448 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 2.416 | Reg loss: 0.028 | Tree loss: 2.416 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 2.464 | Reg loss: 0.028 | Tree loss: 2.464 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 2.430 | Reg loss: 0.028 | Tree loss: 2.430 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 2.409 | Reg loss: 0.028 | Tree loss: 2.409 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 2.415 | Reg loss: 0.028 | Tree loss: 2.415 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 2.363 | Reg loss: 0.028 | Tree loss: 2.363 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 2.420 | Reg loss: 0.029 | Tree loss: 2.420 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 2.365 | Reg loss: 0.029 | Tree loss: 2.365 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 2.360 | Reg loss: 0.029 | Tree loss: 2.360 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 2.348 | Reg loss: 0.029 | Tree loss: 2.348 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 2.331 | Reg loss: 0.029 | Tree loss: 2.331 | Accuracy: 0.325911 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 2.508 | Reg loss: 0.028 | Tree loss: 2.508 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 2.490 | Reg loss: 0.028 | Tree loss: 2.490 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 2.464 | Reg loss: 0.028 | Tree loss: 2.464 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 2.512 | Reg loss: 0.028 | Tree loss: 2.512 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 2.490 | Reg loss: 0.028 | Tree loss: 2.490 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 2.450 | Reg loss: 0.028 | Tree loss: 2.450 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 2.440 | Reg loss: 0.028 | Tree loss: 2.440 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 2.437 | Reg loss: 0.028 | Tree loss: 2.437 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 2.460 | Reg loss: 0.028 | Tree loss: 2.460 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 2.416 | Reg loss: 0.028 | Tree loss: 2.416 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 2.421 | Reg loss: 0.028 | Tree loss: 2.421 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 2.455 | Reg loss: 0.028 | Tree loss: 2.455 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 2.410 | Reg loss: 0.028 | Tree loss: 2.410 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 2.393 | Reg loss: 0.028 | Tree loss: 2.393 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 2.388 | Reg loss: 0.028 | Tree loss: 2.388 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 2.358 | Reg loss: 0.028 | Tree loss: 2.358 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 2.352 | Reg loss: 0.028 | Tree loss: 2.352 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 2.330 | Reg loss: 0.028 | Tree loss: 2.330 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 2.333 | Reg loss: 0.028 | Tree loss: 2.333 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 2.324 | Reg loss: 0.028 | Tree loss: 2.324 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 2.380 | Reg loss: 0.028 | Tree loss: 2.380 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 2.330 | Reg loss: 0.028 | Tree loss: 2.330 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 2.306 | Reg loss: 0.028 | Tree loss: 2.306 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 2.342 | Reg loss: 0.028 | Tree loss: 2.342 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 2.327 | Reg loss: 0.028 | Tree loss: 2.327 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 2.296 | Reg loss: 0.028 | Tree loss: 2.296 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 2.299 | Reg loss: 0.028 | Tree loss: 2.299 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 2.361 | Reg loss: 0.028 | Tree loss: 2.361 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 2.301 | Reg loss: 0.028 | Tree loss: 2.301 | Accuracy: 0.285425 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 2.422 | Reg loss: 0.028 | Tree loss: 2.422 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 2.450 | Reg loss: 0.028 | Tree loss: 2.450 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 2.418 | Reg loss: 0.028 | Tree loss: 2.418 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 2.395 | Reg loss: 0.028 | Tree loss: 2.395 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 2.411 | Reg loss: 0.028 | Tree loss: 2.411 | Accuracy: 0.328125 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 2.414 | Reg loss: 0.028 | Tree loss: 2.414 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 2.409 | Reg loss: 0.028 | Tree loss: 2.409 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 2.414 | Reg loss: 0.028 | Tree loss: 2.414 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 2.420 | Reg loss: 0.028 | Tree loss: 2.420 | Accuracy: 0.248047 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 2.371 | Reg loss: 0.028 | Tree loss: 2.371 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 2.345 | Reg loss: 0.028 | Tree loss: 2.345 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 2.352 | Reg loss: 0.028 | Tree loss: 2.352 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 2.362 | Reg loss: 0.028 | Tree loss: 2.362 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 2.352 | Reg loss: 0.028 | Tree loss: 2.352 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 2.299 | Reg loss: 0.028 | Tree loss: 2.299 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 2.308 | Reg loss: 0.028 | Tree loss: 2.308 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 2.318 | Reg loss: 0.028 | Tree loss: 2.318 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 2.333 | Reg loss: 0.028 | Tree loss: 2.333 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 2.273 | Reg loss: 0.028 | Tree loss: 2.273 | Accuracy: 0.353516 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 2.287 | Reg loss: 0.028 | Tree loss: 2.287 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 2.261 | Reg loss: 0.028 | Tree loss: 2.261 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 2.303 | Reg loss: 0.028 | Tree loss: 2.303 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 2.277 | Reg loss: 0.028 | Tree loss: 2.277 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 2.248 | Reg loss: 0.028 | Tree loss: 2.248 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 2.282 | Reg loss: 0.028 | Tree loss: 2.282 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 2.235 | Reg loss: 0.028 | Tree loss: 2.235 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 2.240 | Reg loss: 0.028 | Tree loss: 2.240 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 2.237 | Reg loss: 0.028 | Tree loss: 2.237 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.299595 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 2.344 | Reg loss: 0.028 | Tree loss: 2.344 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 2.399 | Reg loss: 0.028 | Tree loss: 2.399 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 2.354 | Reg loss: 0.028 | Tree loss: 2.354 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 2.381 | Reg loss: 0.028 | Tree loss: 2.381 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 2.369 | Reg loss: 0.028 | Tree loss: 2.369 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 2.393 | Reg loss: 0.028 | Tree loss: 2.393 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 2.341 | Reg loss: 0.028 | Tree loss: 2.341 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 2.305 | Reg loss: 0.028 | Tree loss: 2.305 | Accuracy: 0.359375 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 2.310 | Reg loss: 0.028 | Tree loss: 2.310 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 2.337 | Reg loss: 0.028 | Tree loss: 2.337 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 2.248 | Reg loss: 0.028 | Tree loss: 2.248 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 2.291 | Reg loss: 0.028 | Tree loss: 2.291 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 2.317 | Reg loss: 0.028 | Tree loss: 2.317 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 2.256 | Reg loss: 0.028 | Tree loss: 2.256 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 2.267 | Reg loss: 0.028 | Tree loss: 2.267 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 2.266 | Reg loss: 0.028 | Tree loss: 2.266 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 2.287 | Reg loss: 0.028 | Tree loss: 2.287 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 2.276 | Reg loss: 0.028 | Tree loss: 2.276 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 2.255 | Reg loss: 0.028 | Tree loss: 2.255 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 2.266 | Reg loss: 0.028 | Tree loss: 2.266 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 2.249 | Reg loss: 0.028 | Tree loss: 2.249 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 2.274 | Reg loss: 0.028 | Tree loss: 2.274 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 2.241 | Reg loss: 0.028 | Tree loss: 2.241 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 2.226 | Reg loss: 0.028 | Tree loss: 2.226 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 2.210 | Reg loss: 0.028 | Tree loss: 2.210 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 2.199 | Reg loss: 0.028 | Tree loss: 2.199 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 2.162 | Reg loss: 0.028 | Tree loss: 2.162 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 2.152 | Reg loss: 0.028 | Tree loss: 2.152 | Accuracy: 0.329960 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 2.302 | Reg loss: 0.028 | Tree loss: 2.302 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 2.355 | Reg loss: 0.028 | Tree loss: 2.355 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 2.294 | Reg loss: 0.028 | Tree loss: 2.294 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 2.277 | Reg loss: 0.028 | Tree loss: 2.277 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 2.289 | Reg loss: 0.028 | Tree loss: 2.289 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 2.317 | Reg loss: 0.028 | Tree loss: 2.317 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 2.336 | Reg loss: 0.028 | Tree loss: 2.336 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 2.285 | Reg loss: 0.028 | Tree loss: 2.285 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 2.256 | Reg loss: 0.028 | Tree loss: 2.256 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 2.245 | Reg loss: 0.028 | Tree loss: 2.245 | Accuracy: 0.285156 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 2.224 | Reg loss: 0.028 | Tree loss: 2.224 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 2.221 | Reg loss: 0.028 | Tree loss: 2.221 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 2.263 | Reg loss: 0.028 | Tree loss: 2.263 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 2.256 | Reg loss: 0.028 | Tree loss: 2.256 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 2.246 | Reg loss: 0.028 | Tree loss: 2.246 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 2.195 | Reg loss: 0.028 | Tree loss: 2.195 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 2.203 | Reg loss: 0.028 | Tree loss: 2.203 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 2.178 | Reg loss: 0.028 | Tree loss: 2.178 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 2.178 | Reg loss: 0.028 | Tree loss: 2.178 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 2.211 | Reg loss: 0.028 | Tree loss: 2.211 | Accuracy: 0.347656 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 2.200 | Reg loss: 0.028 | Tree loss: 2.200 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 2.153 | Reg loss: 0.028 | Tree loss: 2.153 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 2.184 | Reg loss: 0.028 | Tree loss: 2.184 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 2.144 | Reg loss: 0.028 | Tree loss: 2.144 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 2.108 | Reg loss: 0.028 | Tree loss: 2.108 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 2.154 | Reg loss: 0.028 | Tree loss: 2.154 | Accuracy: 0.305668 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 2.294 | Reg loss: 0.028 | Tree loss: 2.294 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 2.240 | Reg loss: 0.028 | Tree loss: 2.240 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 2.240 | Reg loss: 0.028 | Tree loss: 2.240 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 2.264 | Reg loss: 0.028 | Tree loss: 2.264 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 2.224 | Reg loss: 0.028 | Tree loss: 2.224 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 2.263 | Reg loss: 0.028 | Tree loss: 2.263 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 2.232 | Reg loss: 0.028 | Tree loss: 2.232 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 2.231 | Reg loss: 0.028 | Tree loss: 2.231 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 2.198 | Reg loss: 0.028 | Tree loss: 2.198 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 2.215 | Reg loss: 0.028 | Tree loss: 2.215 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 2.236 | Reg loss: 0.028 | Tree loss: 2.236 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 2.193 | Reg loss: 0.028 | Tree loss: 2.193 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 2.205 | Reg loss: 0.028 | Tree loss: 2.205 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 2.206 | Reg loss: 0.028 | Tree loss: 2.206 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 2.142 | Reg loss: 0.028 | Tree loss: 2.142 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 2.195 | Reg loss: 0.028 | Tree loss: 2.195 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 2.134 | Reg loss: 0.028 | Tree loss: 2.134 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 2.104 | Reg loss: 0.028 | Tree loss: 2.104 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 2.162 | Reg loss: 0.028 | Tree loss: 2.162 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 2.145 | Reg loss: 0.028 | Tree loss: 2.145 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 2.141 | Reg loss: 0.028 | Tree loss: 2.141 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 2.127 | Reg loss: 0.028 | Tree loss: 2.127 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 2.134 | Reg loss: 0.028 | Tree loss: 2.134 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 2.150 | Reg loss: 0.028 | Tree loss: 2.150 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 2.088 | Reg loss: 0.028 | Tree loss: 2.088 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 2.112 | Reg loss: 0.028 | Tree loss: 2.112 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 2.091 | Reg loss: 0.028 | Tree loss: 2.091 | Accuracy: 0.321862 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 2.294 | Reg loss: 0.028 | Tree loss: 2.294 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 2.230 | Reg loss: 0.028 | Tree loss: 2.230 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 2.216 | Reg loss: 0.028 | Tree loss: 2.216 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 2.216 | Reg loss: 0.028 | Tree loss: 2.216 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 2.229 | Reg loss: 0.028 | Tree loss: 2.229 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 2.226 | Reg loss: 0.028 | Tree loss: 2.226 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 2.176 | Reg loss: 0.028 | Tree loss: 2.176 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 2.215 | Reg loss: 0.028 | Tree loss: 2.215 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 2.174 | Reg loss: 0.028 | Tree loss: 2.174 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 2.163 | Reg loss: 0.028 | Tree loss: 2.163 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 2.162 | Reg loss: 0.028 | Tree loss: 2.162 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 2.163 | Reg loss: 0.028 | Tree loss: 2.163 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 2.135 | Reg loss: 0.028 | Tree loss: 2.135 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 2.147 | Reg loss: 0.028 | Tree loss: 2.147 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 2.127 | Reg loss: 0.028 | Tree loss: 2.127 | Accuracy: 0.304688 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 2.091 | Reg loss: 0.028 | Tree loss: 2.091 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 2.077 | Reg loss: 0.028 | Tree loss: 2.077 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 2.105 | Reg loss: 0.028 | Tree loss: 2.105 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 2.100 | Reg loss: 0.028 | Tree loss: 2.100 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 2.125 | Reg loss: 0.028 | Tree loss: 2.125 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 2.111 | Reg loss: 0.028 | Tree loss: 2.111 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 2.135 | Reg loss: 0.028 | Tree loss: 2.135 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 2.130 | Reg loss: 0.028 | Tree loss: 2.130 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 2.097 | Reg loss: 0.028 | Tree loss: 2.097 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 2.094 | Reg loss: 0.028 | Tree loss: 2.094 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 2.133 | Reg loss: 0.028 | Tree loss: 2.133 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 2.089 | Reg loss: 0.028 | Tree loss: 2.089 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 2.044 | Reg loss: 0.028 | Tree loss: 2.044 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 2.041 | Reg loss: 0.028 | Tree loss: 2.041 | Accuracy: 0.321862 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 2.162 | Reg loss: 0.028 | Tree loss: 2.162 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 2.171 | Reg loss: 0.028 | Tree loss: 2.171 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 2.161 | Reg loss: 0.028 | Tree loss: 2.161 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 2.158 | Reg loss: 0.028 | Tree loss: 2.158 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 2.139 | Reg loss: 0.028 | Tree loss: 2.139 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 2.168 | Reg loss: 0.028 | Tree loss: 2.168 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 2.163 | Reg loss: 0.028 | Tree loss: 2.163 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 2.172 | Reg loss: 0.028 | Tree loss: 2.172 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 2.147 | Reg loss: 0.028 | Tree loss: 2.147 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 2.105 | Reg loss: 0.028 | Tree loss: 2.105 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 2.112 | Reg loss: 0.028 | Tree loss: 2.112 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 2.139 | Reg loss: 0.028 | Tree loss: 2.139 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 2.088 | Reg loss: 0.028 | Tree loss: 2.088 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 2.106 | Reg loss: 0.028 | Tree loss: 2.106 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 2.122 | Reg loss: 0.028 | Tree loss: 2.122 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 2.100 | Reg loss: 0.028 | Tree loss: 2.100 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 2.095 | Reg loss: 0.028 | Tree loss: 2.095 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 2.051 | Reg loss: 0.028 | Tree loss: 2.051 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 2.107 | Reg loss: 0.028 | Tree loss: 2.107 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 2.109 | Reg loss: 0.028 | Tree loss: 2.109 | Accuracy: 0.347656 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 2.072 | Reg loss: 0.028 | Tree loss: 2.072 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 2.065 | Reg loss: 0.028 | Tree loss: 2.065 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 2.079 | Reg loss: 0.028 | Tree loss: 2.079 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 2.081 | Reg loss: 0.028 | Tree loss: 2.081 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 2.092 | Reg loss: 0.028 | Tree loss: 2.092 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 2.050 | Reg loss: 0.028 | Tree loss: 2.050 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 2.041 | Reg loss: 0.028 | Tree loss: 2.041 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 2.051 | Reg loss: 0.028 | Tree loss: 2.051 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 2.115 | Reg loss: 0.028 | Tree loss: 2.115 | Accuracy: 0.263158 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 2.138 | Reg loss: 0.028 | Tree loss: 2.138 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 2.126 | Reg loss: 0.028 | Tree loss: 2.126 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 2.210 | Reg loss: 0.028 | Tree loss: 2.210 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 2.134 | Reg loss: 0.028 | Tree loss: 2.134 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 2.097 | Reg loss: 0.028 | Tree loss: 2.097 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 2.139 | Reg loss: 0.028 | Tree loss: 2.139 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 2.107 | Reg loss: 0.028 | Tree loss: 2.107 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 2.138 | Reg loss: 0.028 | Tree loss: 2.138 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 2.070 | Reg loss: 0.028 | Tree loss: 2.070 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 2.120 | Reg loss: 0.028 | Tree loss: 2.120 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 2.120 | Reg loss: 0.028 | Tree loss: 2.120 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 2.123 | Reg loss: 0.028 | Tree loss: 2.123 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 2.134 | Reg loss: 0.028 | Tree loss: 2.134 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 2.130 | Reg loss: 0.028 | Tree loss: 2.130 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 2.080 | Reg loss: 0.028 | Tree loss: 2.080 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 2.042 | Reg loss: 0.028 | Tree loss: 2.042 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 2.048 | Reg loss: 0.028 | Tree loss: 2.048 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 2.037 | Reg loss: 0.028 | Tree loss: 2.037 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 2.054 | Reg loss: 0.028 | Tree loss: 2.054 | Accuracy: 0.259766 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 2.037 | Reg loss: 0.028 | Tree loss: 2.037 | Accuracy: 0.330078 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 2.059 | Reg loss: 0.028 | Tree loss: 2.059 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 2.014 | Reg loss: 0.028 | Tree loss: 2.014 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 2.016 | Reg loss: 0.028 | Tree loss: 2.016 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 2.036 | Reg loss: 0.028 | Tree loss: 2.036 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 2.013 | Reg loss: 0.028 | Tree loss: 2.013 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 2.012 | Reg loss: 0.028 | Tree loss: 2.012 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 2.012 | Reg loss: 0.028 | Tree loss: 2.012 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 2.034 | Reg loss: 0.028 | Tree loss: 2.034 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 2.004 | Reg loss: 0.028 | Tree loss: 2.004 | Accuracy: 0.285425 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 2.062 | Reg loss: 0.027 | Tree loss: 2.062 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 2.094 | Reg loss: 0.027 | Tree loss: 2.094 | Accuracy: 0.353516 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 2.131 | Reg loss: 0.027 | Tree loss: 2.131 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 2.095 | Reg loss: 0.027 | Tree loss: 2.095 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 2.079 | Reg loss: 0.027 | Tree loss: 2.079 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 2.068 | Reg loss: 0.027 | Tree loss: 2.068 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 2.073 | Reg loss: 0.027 | Tree loss: 2.073 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 2.077 | Reg loss: 0.027 | Tree loss: 2.077 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 2.084 | Reg loss: 0.027 | Tree loss: 2.084 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 2.066 | Reg loss: 0.027 | Tree loss: 2.066 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 2.094 | Reg loss: 0.027 | Tree loss: 2.094 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 2.080 | Reg loss: 0.027 | Tree loss: 2.080 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 2.052 | Reg loss: 0.027 | Tree loss: 2.052 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 2.049 | Reg loss: 0.027 | Tree loss: 2.049 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 2.030 | Reg loss: 0.027 | Tree loss: 2.030 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 2.060 | Reg loss: 0.027 | Tree loss: 2.060 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 2.019 | Reg loss: 0.027 | Tree loss: 2.019 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 2.036 | Reg loss: 0.028 | Tree loss: 2.036 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 1.987 | Reg loss: 0.028 | Tree loss: 1.987 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 2.026 | Reg loss: 0.028 | Tree loss: 2.026 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 2.025 | Reg loss: 0.028 | Tree loss: 2.025 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 2.052 | Reg loss: 0.028 | Tree loss: 2.052 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 1.989 | Reg loss: 0.028 | Tree loss: 1.989 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 2.012 | Reg loss: 0.028 | Tree loss: 2.012 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 2.024 | Reg loss: 0.028 | Tree loss: 2.024 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 2.013 | Reg loss: 0.028 | Tree loss: 2.013 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 2.042 | Reg loss: 0.028 | Tree loss: 2.042 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 2.060 | Reg loss: 0.028 | Tree loss: 2.060 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 1.987 | Reg loss: 0.028 | Tree loss: 1.987 | Accuracy: 0.315789 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 2.130 | Reg loss: 0.027 | Tree loss: 2.130 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 2.101 | Reg loss: 0.027 | Tree loss: 2.101 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 2.088 | Reg loss: 0.027 | Tree loss: 2.088 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 2.053 | Reg loss: 0.027 | Tree loss: 2.053 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 2.080 | Reg loss: 0.027 | Tree loss: 2.080 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 2.092 | Reg loss: 0.027 | Tree loss: 2.092 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 2.040 | Reg loss: 0.027 | Tree loss: 2.040 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 2.055 | Reg loss: 0.027 | Tree loss: 2.055 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 1.999 | Reg loss: 0.027 | Tree loss: 1.999 | Accuracy: 0.351562 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 2.067 | Reg loss: 0.027 | Tree loss: 2.067 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 2.069 | Reg loss: 0.027 | Tree loss: 2.069 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 2.035 | Reg loss: 0.027 | Tree loss: 2.035 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 2.037 | Reg loss: 0.027 | Tree loss: 2.037 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 2.059 | Reg loss: 0.027 | Tree loss: 2.059 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 2.019 | Reg loss: 0.027 | Tree loss: 2.019 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 1.988 | Reg loss: 0.027 | Tree loss: 1.988 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 2.028 | Reg loss: 0.027 | Tree loss: 2.028 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 2.015 | Reg loss: 0.027 | Tree loss: 2.015 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 2.002 | Reg loss: 0.027 | Tree loss: 2.002 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 2.017 | Reg loss: 0.027 | Tree loss: 2.017 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 2.008 | Reg loss: 0.027 | Tree loss: 2.008 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 1.992 | Reg loss: 0.027 | Tree loss: 1.992 | Accuracy: 0.343750 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 1.957 | Reg loss: 0.027 | Tree loss: 1.957 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 1.979 | Reg loss: 0.027 | Tree loss: 1.979 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 1.962 | Reg loss: 0.027 | Tree loss: 1.962 | Accuracy: 0.296875 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 1.981 | Reg loss: 0.027 | Tree loss: 1.981 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 1.976 | Reg loss: 0.027 | Tree loss: 1.976 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 1.935 | Reg loss: 0.027 | Tree loss: 1.935 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 1.945 | Reg loss: 0.028 | Tree loss: 1.945 | Accuracy: 0.289474 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 2.094 | Reg loss: 0.027 | Tree loss: 2.094 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 2.085 | Reg loss: 0.027 | Tree loss: 2.085 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 2.043 | Reg loss: 0.027 | Tree loss: 2.043 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 2.035 | Reg loss: 0.027 | Tree loss: 2.035 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 2.063 | Reg loss: 0.027 | Tree loss: 2.063 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 2.033 | Reg loss: 0.027 | Tree loss: 2.033 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 2.035 | Reg loss: 0.027 | Tree loss: 2.035 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 2.049 | Reg loss: 0.027 | Tree loss: 2.049 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 2.019 | Reg loss: 0.027 | Tree loss: 2.019 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 1.994 | Reg loss: 0.027 | Tree loss: 1.994 | Accuracy: 0.345703 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 2.042 | Reg loss: 0.027 | Tree loss: 2.042 | Accuracy: 0.265625 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 2.001 | Reg loss: 0.027 | Tree loss: 2.001 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 2.007 | Reg loss: 0.027 | Tree loss: 2.007 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 1.991 | Reg loss: 0.027 | Tree loss: 1.991 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 1.988 | Reg loss: 0.027 | Tree loss: 1.988 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 2.017 | Reg loss: 0.027 | Tree loss: 2.017 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 1.995 | Reg loss: 0.027 | Tree loss: 1.995 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 2.004 | Reg loss: 0.027 | Tree loss: 2.004 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 2.009 | Reg loss: 0.027 | Tree loss: 2.009 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 1.955 | Reg loss: 0.027 | Tree loss: 1.955 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 2.003 | Reg loss: 0.027 | Tree loss: 2.003 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 1.986 | Reg loss: 0.027 | Tree loss: 1.986 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 1.957 | Reg loss: 0.027 | Tree loss: 1.957 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 1.921 | Reg loss: 0.027 | Tree loss: 1.921 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 1.937 | Reg loss: 0.027 | Tree loss: 1.937 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 1.928 | Reg loss: 0.027 | Tree loss: 1.928 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 1.960 | Reg loss: 0.027 | Tree loss: 1.960 | Accuracy: 0.319838 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 2.069 | Reg loss: 0.027 | Tree loss: 2.069 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 2.099 | Reg loss: 0.027 | Tree loss: 2.099 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 2.046 | Reg loss: 0.027 | Tree loss: 2.046 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 2.028 | Reg loss: 0.027 | Tree loss: 2.028 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 2.023 | Reg loss: 0.027 | Tree loss: 2.023 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 1.993 | Reg loss: 0.027 | Tree loss: 1.993 | Accuracy: 0.343750 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 2.028 | Reg loss: 0.027 | Tree loss: 2.028 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 1.958 | Reg loss: 0.027 | Tree loss: 1.958 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 1.988 | Reg loss: 0.027 | Tree loss: 1.988 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 2.042 | Reg loss: 0.027 | Tree loss: 2.042 | Accuracy: 0.267578 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 1.999 | Reg loss: 0.027 | Tree loss: 1.999 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 1.993 | Reg loss: 0.027 | Tree loss: 1.993 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 1.966 | Reg loss: 0.027 | Tree loss: 1.966 | Accuracy: 0.363281 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 1.967 | Reg loss: 0.027 | Tree loss: 1.967 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 1.948 | Reg loss: 0.027 | Tree loss: 1.948 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 1.938 | Reg loss: 0.027 | Tree loss: 1.938 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 1.981 | Reg loss: 0.027 | Tree loss: 1.981 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 1.978 | Reg loss: 0.027 | Tree loss: 1.978 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 1.932 | Reg loss: 0.027 | Tree loss: 1.932 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 1.971 | Reg loss: 0.027 | Tree loss: 1.971 | Accuracy: 0.253906 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 1.968 | Reg loss: 0.027 | Tree loss: 1.968 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 1.965 | Reg loss: 0.027 | Tree loss: 1.965 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 1.904 | Reg loss: 0.027 | Tree loss: 1.904 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 1.942 | Reg loss: 0.027 | Tree loss: 1.942 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 1.958 | Reg loss: 0.027 | Tree loss: 1.958 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 1.949 | Reg loss: 0.027 | Tree loss: 1.949 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 1.898 | Reg loss: 0.027 | Tree loss: 1.898 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 1.973 | Reg loss: 0.027 | Tree loss: 1.973 | Accuracy: 0.283401 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 2.017 | Reg loss: 0.027 | Tree loss: 2.017 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 2.038 | Reg loss: 0.027 | Tree loss: 2.038 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 2.021 | Reg loss: 0.027 | Tree loss: 2.021 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 1.999 | Reg loss: 0.027 | Tree loss: 1.999 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 1.996 | Reg loss: 0.027 | Tree loss: 1.996 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 1.980 | Reg loss: 0.027 | Tree loss: 1.980 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 2.002 | Reg loss: 0.027 | Tree loss: 2.002 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 1.972 | Reg loss: 0.027 | Tree loss: 1.972 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 1.978 | Reg loss: 0.027 | Tree loss: 1.978 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 2.024 | Reg loss: 0.027 | Tree loss: 2.024 | Accuracy: 0.257812 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 1.984 | Reg loss: 0.027 | Tree loss: 1.984 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 1.995 | Reg loss: 0.027 | Tree loss: 1.995 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 2.000 | Reg loss: 0.027 | Tree loss: 2.000 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 1.977 | Reg loss: 0.027 | Tree loss: 1.977 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 1.949 | Reg loss: 0.027 | Tree loss: 1.949 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 1.969 | Reg loss: 0.027 | Tree loss: 1.969 | Accuracy: 0.263672 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 1.942 | Reg loss: 0.027 | Tree loss: 1.942 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 1.991 | Reg loss: 0.027 | Tree loss: 1.991 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 1.899 | Reg loss: 0.027 | Tree loss: 1.899 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 1.929 | Reg loss: 0.027 | Tree loss: 1.929 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 1.948 | Reg loss: 0.027 | Tree loss: 1.948 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 1.925 | Reg loss: 0.027 | Tree loss: 1.925 | Accuracy: 0.345703 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 1.900 | Reg loss: 0.027 | Tree loss: 1.900 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 1.902 | Reg loss: 0.027 | Tree loss: 1.902 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 1.899 | Reg loss: 0.027 | Tree loss: 1.899 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 1.908 | Reg loss: 0.027 | Tree loss: 1.908 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 1.904 | Reg loss: 0.027 | Tree loss: 1.904 | Accuracy: 0.324219 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 1.899 | Reg loss: 0.027 | Tree loss: 1.899 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 1.936 | Reg loss: 0.027 | Tree loss: 1.936 | Accuracy: 0.311741 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 1.984 | Reg loss: 0.027 | Tree loss: 1.984 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 2.007 | Reg loss: 0.027 | Tree loss: 2.007 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 1.980 | Reg loss: 0.027 | Tree loss: 1.980 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 2.016 | Reg loss: 0.027 | Tree loss: 2.016 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 1.980 | Reg loss: 0.027 | Tree loss: 1.980 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 2.011 | Reg loss: 0.027 | Tree loss: 2.011 | Accuracy: 0.271484 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 1.922 | Reg loss: 0.027 | Tree loss: 1.922 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 1.979 | Reg loss: 0.027 | Tree loss: 1.979 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 1.982 | Reg loss: 0.027 | Tree loss: 1.982 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 1.980 | Reg loss: 0.027 | Tree loss: 1.980 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 1.999 | Reg loss: 0.027 | Tree loss: 1.999 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 1.937 | Reg loss: 0.027 | Tree loss: 1.937 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 1.938 | Reg loss: 0.027 | Tree loss: 1.938 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 2.016 | Reg loss: 0.027 | Tree loss: 2.016 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 1.930 | Reg loss: 0.027 | Tree loss: 1.930 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 1.946 | Reg loss: 0.027 | Tree loss: 1.946 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 1.927 | Reg loss: 0.027 | Tree loss: 1.927 | Accuracy: 0.333984 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 1.933 | Reg loss: 0.027 | Tree loss: 1.933 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 1.899 | Reg loss: 0.027 | Tree loss: 1.899 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 1.913 | Reg loss: 0.027 | Tree loss: 1.913 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 1.873 | Reg loss: 0.027 | Tree loss: 1.873 | Accuracy: 0.353516 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 1.886 | Reg loss: 0.027 | Tree loss: 1.886 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 1.930 | Reg loss: 0.027 | Tree loss: 1.930 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 1.869 | Reg loss: 0.027 | Tree loss: 1.869 | Accuracy: 0.338057 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 2.009 | Reg loss: 0.027 | Tree loss: 2.009 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 2.024 | Reg loss: 0.027 | Tree loss: 2.024 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 1.969 | Reg loss: 0.027 | Tree loss: 1.969 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 2.018 | Reg loss: 0.027 | Tree loss: 2.018 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 2.022 | Reg loss: 0.027 | Tree loss: 2.022 | Accuracy: 0.310547 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 1.938 | Reg loss: 0.027 | Tree loss: 1.938 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 1.923 | Reg loss: 0.027 | Tree loss: 1.923 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 1.926 | Reg loss: 0.027 | Tree loss: 1.926 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 1.954 | Reg loss: 0.027 | Tree loss: 1.954 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 1.964 | Reg loss: 0.027 | Tree loss: 1.964 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 1.942 | Reg loss: 0.027 | Tree loss: 1.942 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 1.937 | Reg loss: 0.027 | Tree loss: 1.937 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 1.944 | Reg loss: 0.027 | Tree loss: 1.944 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 1.949 | Reg loss: 0.027 | Tree loss: 1.949 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 1.903 | Reg loss: 0.027 | Tree loss: 1.903 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 1.904 | Reg loss: 0.027 | Tree loss: 1.904 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 1.850 | Reg loss: 0.027 | Tree loss: 1.850 | Accuracy: 0.345703 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 1.906 | Reg loss: 0.027 | Tree loss: 1.906 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 1.897 | Reg loss: 0.027 | Tree loss: 1.897 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 1.900 | Reg loss: 0.027 | Tree loss: 1.900 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 1.846 | Reg loss: 0.027 | Tree loss: 1.846 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 1.870 | Reg loss: 0.027 | Tree loss: 1.870 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 1.879 | Reg loss: 0.027 | Tree loss: 1.879 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 1.864 | Reg loss: 0.027 | Tree loss: 1.864 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 1.854 | Reg loss: 0.027 | Tree loss: 1.854 | Accuracy: 0.293522 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 1.947 | Reg loss: 0.027 | Tree loss: 1.947 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 2.019 | Reg loss: 0.027 | Tree loss: 2.019 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 1.958 | Reg loss: 0.027 | Tree loss: 1.958 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 1.919 | Reg loss: 0.027 | Tree loss: 1.919 | Accuracy: 0.343750 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 1.919 | Reg loss: 0.027 | Tree loss: 1.919 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 1.953 | Reg loss: 0.027 | Tree loss: 1.953 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 1.912 | Reg loss: 0.027 | Tree loss: 1.912 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 1.940 | Reg loss: 0.027 | Tree loss: 1.940 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 1.969 | Reg loss: 0.027 | Tree loss: 1.969 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 1.930 | Reg loss: 0.027 | Tree loss: 1.930 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 1.918 | Reg loss: 0.027 | Tree loss: 1.918 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 1.913 | Reg loss: 0.027 | Tree loss: 1.913 | Accuracy: 0.316406 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 1.892 | Reg loss: 0.027 | Tree loss: 1.892 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 1.909 | Reg loss: 0.027 | Tree loss: 1.909 | Accuracy: 0.281250 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 1.926 | Reg loss: 0.027 | Tree loss: 1.926 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 1.900 | Reg loss: 0.027 | Tree loss: 1.900 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 1.901 | Reg loss: 0.027 | Tree loss: 1.901 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 1.905 | Reg loss: 0.027 | Tree loss: 1.905 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 1.941 | Reg loss: 0.027 | Tree loss: 1.941 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 1.894 | Reg loss: 0.027 | Tree loss: 1.894 | Accuracy: 0.287109 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 1.919 | Reg loss: 0.027 | Tree loss: 1.919 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 1.859 | Reg loss: 0.027 | Tree loss: 1.859 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 1.924 | Reg loss: 0.027 | Tree loss: 1.924 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 1.833 | Reg loss: 0.027 | Tree loss: 1.833 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 1.863 | Reg loss: 0.027 | Tree loss: 1.863 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 1.868 | Reg loss: 0.027 | Tree loss: 1.868 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 1.867 | Reg loss: 0.027 | Tree loss: 1.867 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 1.865 | Reg loss: 0.027 | Tree loss: 1.865 | Accuracy: 0.293522 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 1.944 | Reg loss: 0.027 | Tree loss: 1.944 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 1.945 | Reg loss: 0.027 | Tree loss: 1.945 | Accuracy: 0.285156 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 1.937 | Reg loss: 0.027 | Tree loss: 1.937 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 1.957 | Reg loss: 0.027 | Tree loss: 1.957 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 1.924 | Reg loss: 0.027 | Tree loss: 1.924 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 2.013 | Reg loss: 0.027 | Tree loss: 2.013 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 1.917 | Reg loss: 0.027 | Tree loss: 1.917 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 1.944 | Reg loss: 0.027 | Tree loss: 1.944 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 1.904 | Reg loss: 0.027 | Tree loss: 1.904 | Accuracy: 0.349609 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 1.947 | Reg loss: 0.027 | Tree loss: 1.947 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 1.918 | Reg loss: 0.027 | Tree loss: 1.918 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 1.888 | Reg loss: 0.027 | Tree loss: 1.888 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 1.894 | Reg loss: 0.027 | Tree loss: 1.894 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 1.903 | Reg loss: 0.027 | Tree loss: 1.903 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 1.906 | Reg loss: 0.027 | Tree loss: 1.906 | Accuracy: 0.283203 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 1.917 | Reg loss: 0.027 | Tree loss: 1.917 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 1.890 | Reg loss: 0.027 | Tree loss: 1.890 | Accuracy: 0.277344 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 1.897 | Reg loss: 0.027 | Tree loss: 1.897 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 1.898 | Reg loss: 0.027 | Tree loss: 1.898 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 1.849 | Reg loss: 0.027 | Tree loss: 1.849 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 1.892 | Reg loss: 0.027 | Tree loss: 1.892 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 1.827 | Reg loss: 0.027 | Tree loss: 1.827 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 1.827 | Reg loss: 0.027 | Tree loss: 1.827 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 1.867 | Reg loss: 0.027 | Tree loss: 1.867 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 1.839 | Reg loss: 0.027 | Tree loss: 1.839 | Accuracy: 0.299595 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 1.941 | Reg loss: 0.027 | Tree loss: 1.941 | Accuracy: 0.322266 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 1.914 | Reg loss: 0.027 | Tree loss: 1.914 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 1.922 | Reg loss: 0.027 | Tree loss: 1.922 | Accuracy: 0.289062 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 1.902 | Reg loss: 0.027 | Tree loss: 1.902 | Accuracy: 0.332031 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 1.940 | Reg loss: 0.027 | Tree loss: 1.940 | Accuracy: 0.279297 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 1.951 | Reg loss: 0.027 | Tree loss: 1.951 | Accuracy: 0.291016 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 1.951 | Reg loss: 0.027 | Tree loss: 1.951 | Accuracy: 0.304688 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 1.902 | Reg loss: 0.027 | Tree loss: 1.902 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 1.896 | Reg loss: 0.027 | Tree loss: 1.896 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 1.922 | Reg loss: 0.027 | Tree loss: 1.922 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 1.933 | Reg loss: 0.027 | Tree loss: 1.933 | Accuracy: 0.275391 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 1.901 | Reg loss: 0.027 | Tree loss: 1.901 | Accuracy: 0.341797 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 1.869 | Reg loss: 0.027 | Tree loss: 1.869 | Accuracy: 0.298828 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 1.910 | Reg loss: 0.027 | Tree loss: 1.910 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 1.829 | Reg loss: 0.027 | Tree loss: 1.829 | Accuracy: 0.335938 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 1.866 | Reg loss: 0.027 | Tree loss: 1.866 | Accuracy: 0.257812 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 1.925 | Reg loss: 0.027 | Tree loss: 1.925 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 1.847 | Reg loss: 0.027 | Tree loss: 1.847 | Accuracy: 0.357422 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 1.826 | Reg loss: 0.027 | Tree loss: 1.826 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 1.841 | Reg loss: 0.027 | Tree loss: 1.841 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.328125 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 1.897 | Reg loss: 0.027 | Tree loss: 1.897 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 1.871 | Reg loss: 0.027 | Tree loss: 1.871 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 1.833 | Reg loss: 0.027 | Tree loss: 1.833 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 1.841 | Reg loss: 0.027 | Tree loss: 1.841 | Accuracy: 0.302734 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 1.839 | Reg loss: 0.027 | Tree loss: 1.839 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 1.838 | Reg loss: 0.027 | Tree loss: 1.838 | Accuracy: 0.314453 | 0.238 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 1.833 | Reg loss: 0.027 | Tree loss: 1.833 | Accuracy: 0.271255 | 0.238 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 1.949 | Reg loss: 0.027 | Tree loss: 1.949 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 1.960 | Reg loss: 0.027 | Tree loss: 1.960 | Accuracy: 0.273438 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 1.933 | Reg loss: 0.027 | Tree loss: 1.933 | Accuracy: 0.300781 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 1.924 | Reg loss: 0.027 | Tree loss: 1.924 | Accuracy: 0.349609 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 1.950 | Reg loss: 0.027 | Tree loss: 1.950 | Accuracy: 0.292969 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 1.881 | Reg loss: 0.027 | Tree loss: 1.881 | Accuracy: 0.318359 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 1.897 | Reg loss: 0.027 | Tree loss: 1.897 | Accuracy: 0.326172 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 1.879 | Reg loss: 0.027 | Tree loss: 1.879 | Accuracy: 0.296875 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 1.910 | Reg loss: 0.027 | Tree loss: 1.910 | Accuracy: 0.312500 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 1.913 | Reg loss: 0.027 | Tree loss: 1.913 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.320312 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 1.955 | Reg loss: 0.027 | Tree loss: 1.955 | Accuracy: 0.306641 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.337891 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 1.884 | Reg loss: 0.027 | Tree loss: 1.884 | Accuracy: 0.308594 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 1.906 | Reg loss: 0.027 | Tree loss: 1.906 | Accuracy: 0.283203 | 0.238 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 1.833 | Reg loss: 0.027 | Tree loss: 1.833 | Accuracy: 0.339844 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 1.841 | Reg loss: 0.027 | Tree loss: 1.841 | Accuracy: 0.330078 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.310547 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 1.861 | Reg loss: 0.027 | Tree loss: 1.861 | Accuracy: 0.269531 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 1.797 | Reg loss: 0.027 | Tree loss: 1.797 | Accuracy: 0.294922 | 0.238 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 1.821 | Reg loss: 0.027 | Tree loss: 1.821 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 1.842 | Reg loss: 0.027 | Tree loss: 1.842 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 1.843 | Reg loss: 0.027 | Tree loss: 1.843 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 1.834 | Reg loss: 0.027 | Tree loss: 1.834 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 1.855 | Reg loss: 0.027 | Tree loss: 1.855 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 1.839 | Reg loss: 0.027 | Tree loss: 1.839 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 1.847 | Reg loss: 0.027 | Tree loss: 1.847 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 1.807 | Reg loss: 0.027 | Tree loss: 1.807 | Accuracy: 0.315789 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 1.962 | Reg loss: 0.027 | Tree loss: 1.962 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 1.923 | Reg loss: 0.027 | Tree loss: 1.923 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 1.884 | Reg loss: 0.027 | Tree loss: 1.884 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 1.913 | Reg loss: 0.027 | Tree loss: 1.913 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 1.886 | Reg loss: 0.027 | Tree loss: 1.886 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 1.883 | Reg loss: 0.027 | Tree loss: 1.883 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 1.880 | Reg loss: 0.027 | Tree loss: 1.880 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 1.918 | Reg loss: 0.027 | Tree loss: 1.918 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 1.875 | Reg loss: 0.027 | Tree loss: 1.875 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 1.881 | Reg loss: 0.027 | Tree loss: 1.881 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 1.862 | Reg loss: 0.027 | Tree loss: 1.862 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 1.869 | Reg loss: 0.027 | Tree loss: 1.869 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 1.823 | Reg loss: 0.027 | Tree loss: 1.823 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 1.872 | Reg loss: 0.027 | Tree loss: 1.872 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 1.886 | Reg loss: 0.027 | Tree loss: 1.886 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 1.872 | Reg loss: 0.027 | Tree loss: 1.872 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 1.861 | Reg loss: 0.027 | Tree loss: 1.861 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 1.849 | Reg loss: 0.027 | Tree loss: 1.849 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 1.825 | Reg loss: 0.027 | Tree loss: 1.825 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 1.861 | Reg loss: 0.027 | Tree loss: 1.861 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 1.830 | Reg loss: 0.027 | Tree loss: 1.830 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 1.821 | Reg loss: 0.027 | Tree loss: 1.821 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 1.855 | Reg loss: 0.027 | Tree loss: 1.855 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 1.871 | Reg loss: 0.027 | Tree loss: 1.871 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 1.842 | Reg loss: 0.027 | Tree loss: 1.842 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 1.842 | Reg loss: 0.027 | Tree loss: 1.842 | Accuracy: 0.253906 | 0.239 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 1.790 | Reg loss: 0.027 | Tree loss: 1.790 | Accuracy: 0.287449 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 1.943 | Reg loss: 0.027 | Tree loss: 1.943 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 1.951 | Reg loss: 0.027 | Tree loss: 1.951 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 1.894 | Reg loss: 0.027 | Tree loss: 1.894 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 1.931 | Reg loss: 0.027 | Tree loss: 1.931 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 1.869 | Reg loss: 0.027 | Tree loss: 1.869 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 1.899 | Reg loss: 0.027 | Tree loss: 1.899 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 1.866 | Reg loss: 0.027 | Tree loss: 1.866 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 1.906 | Reg loss: 0.027 | Tree loss: 1.906 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 1.898 | Reg loss: 0.027 | Tree loss: 1.898 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 1.912 | Reg loss: 0.027 | Tree loss: 1.912 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 1.841 | Reg loss: 0.027 | Tree loss: 1.841 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 1.890 | Reg loss: 0.027 | Tree loss: 1.890 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 1.788 | Reg loss: 0.027 | Tree loss: 1.788 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 1.880 | Reg loss: 0.027 | Tree loss: 1.880 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 1.863 | Reg loss: 0.027 | Tree loss: 1.863 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 1.843 | Reg loss: 0.027 | Tree loss: 1.843 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 1.849 | Reg loss: 0.027 | Tree loss: 1.849 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 1.853 | Reg loss: 0.027 | Tree loss: 1.853 | Accuracy: 0.306641 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.263672 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 1.806 | Reg loss: 0.027 | Tree loss: 1.806 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 1.802 | Reg loss: 0.027 | Tree loss: 1.802 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 1.790 | Reg loss: 0.027 | Tree loss: 1.790 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 1.776 | Reg loss: 0.027 | Tree loss: 1.776 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 1.804 | Reg loss: 0.027 | Tree loss: 1.804 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 1.818 | Reg loss: 0.027 | Tree loss: 1.818 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 1.785 | Reg loss: 0.027 | Tree loss: 1.785 | Accuracy: 0.325911 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 1.878 | Reg loss: 0.027 | Tree loss: 1.878 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 1.900 | Reg loss: 0.027 | Tree loss: 1.900 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 1.921 | Reg loss: 0.027 | Tree loss: 1.921 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 1.874 | Reg loss: 0.027 | Tree loss: 1.874 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 1.914 | Reg loss: 0.027 | Tree loss: 1.914 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 1.888 | Reg loss: 0.027 | Tree loss: 1.888 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 1.912 | Reg loss: 0.027 | Tree loss: 1.912 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 1.874 | Reg loss: 0.027 | Tree loss: 1.874 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 1.843 | Reg loss: 0.027 | Tree loss: 1.843 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 1.876 | Reg loss: 0.027 | Tree loss: 1.876 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 1.813 | Reg loss: 0.027 | Tree loss: 1.813 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 1.838 | Reg loss: 0.027 | Tree loss: 1.838 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 1.831 | Reg loss: 0.027 | Tree loss: 1.831 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 1.855 | Reg loss: 0.027 | Tree loss: 1.855 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 1.858 | Reg loss: 0.027 | Tree loss: 1.858 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 1.850 | Reg loss: 0.027 | Tree loss: 1.850 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 1.831 | Reg loss: 0.027 | Tree loss: 1.831 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 1.883 | Reg loss: 0.027 | Tree loss: 1.883 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 1.840 | Reg loss: 0.027 | Tree loss: 1.840 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 1.811 | Reg loss: 0.027 | Tree loss: 1.811 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 1.792 | Reg loss: 0.027 | Tree loss: 1.792 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 1.792 | Reg loss: 0.027 | Tree loss: 1.792 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 1.811 | Reg loss: 0.027 | Tree loss: 1.811 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 1.806 | Reg loss: 0.027 | Tree loss: 1.806 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 1.788 | Reg loss: 0.027 | Tree loss: 1.788 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 1.807 | Reg loss: 0.027 | Tree loss: 1.807 | Accuracy: 0.255859 | 0.239 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 1.825 | Reg loss: 0.027 | Tree loss: 1.825 | Accuracy: 0.287449 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 1.918 | Reg loss: 0.027 | Tree loss: 1.918 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 1.820 | Reg loss: 0.027 | Tree loss: 1.820 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 1.922 | Reg loss: 0.027 | Tree loss: 1.922 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 1.891 | Reg loss: 0.027 | Tree loss: 1.891 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 1.900 | Reg loss: 0.027 | Tree loss: 1.900 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 1.892 | Reg loss: 0.027 | Tree loss: 1.892 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 1.877 | Reg loss: 0.027 | Tree loss: 1.877 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 1.874 | Reg loss: 0.027 | Tree loss: 1.874 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 1.926 | Reg loss: 0.027 | Tree loss: 1.926 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 1.844 | Reg loss: 0.027 | Tree loss: 1.844 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 1.874 | Reg loss: 0.027 | Tree loss: 1.874 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 1.853 | Reg loss: 0.027 | Tree loss: 1.853 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 1.816 | Reg loss: 0.027 | Tree loss: 1.816 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 1.796 | Reg loss: 0.027 | Tree loss: 1.796 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 1.832 | Reg loss: 0.027 | Tree loss: 1.832 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 1.809 | Reg loss: 0.027 | Tree loss: 1.809 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 1.784 | Reg loss: 0.027 | Tree loss: 1.784 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 1.824 | Reg loss: 0.027 | Tree loss: 1.824 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 1.815 | Reg loss: 0.027 | Tree loss: 1.815 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 1.837 | Reg loss: 0.027 | Tree loss: 1.837 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 1.814 | Reg loss: 0.027 | Tree loss: 1.814 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 1.827 | Reg loss: 0.027 | Tree loss: 1.827 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 1.819 | Reg loss: 0.027 | Tree loss: 1.819 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 1.847 | Reg loss: 0.027 | Tree loss: 1.847 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 1.765 | Reg loss: 0.027 | Tree loss: 1.765 | Accuracy: 0.328125 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 1.827 | Reg loss: 0.027 | Tree loss: 1.827 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 1.802 | Reg loss: 0.027 | Tree loss: 1.802 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 1.743 | Reg loss: 0.027 | Tree loss: 1.743 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 1.810 | Reg loss: 0.027 | Tree loss: 1.810 | Accuracy: 0.275304 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 1.848 | Reg loss: 0.026 | Tree loss: 1.848 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 1.844 | Reg loss: 0.026 | Tree loss: 1.844 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 1.868 | Reg loss: 0.026 | Tree loss: 1.868 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 1.889 | Reg loss: 0.026 | Tree loss: 1.889 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 1.884 | Reg loss: 0.026 | Tree loss: 1.884 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 1.876 | Reg loss: 0.026 | Tree loss: 1.876 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 1.897 | Reg loss: 0.026 | Tree loss: 1.897 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 1.892 | Reg loss: 0.026 | Tree loss: 1.892 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 1.871 | Reg loss: 0.026 | Tree loss: 1.871 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.349609 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 1.838 | Reg loss: 0.026 | Tree loss: 1.838 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 1.851 | Reg loss: 0.027 | Tree loss: 1.851 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 1.864 | Reg loss: 0.027 | Tree loss: 1.864 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 1.868 | Reg loss: 0.027 | Tree loss: 1.868 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 1.812 | Reg loss: 0.027 | Tree loss: 1.812 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 1.834 | Reg loss: 0.027 | Tree loss: 1.834 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 1.813 | Reg loss: 0.027 | Tree loss: 1.813 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 1.803 | Reg loss: 0.027 | Tree loss: 1.803 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 1.843 | Reg loss: 0.027 | Tree loss: 1.843 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 1.803 | Reg loss: 0.027 | Tree loss: 1.803 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 1.801 | Reg loss: 0.027 | Tree loss: 1.801 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 1.780 | Reg loss: 0.027 | Tree loss: 1.780 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 1.788 | Reg loss: 0.027 | Tree loss: 1.788 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 1.776 | Reg loss: 0.027 | Tree loss: 1.776 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 1.781 | Reg loss: 0.027 | Tree loss: 1.781 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 1.800 | Reg loss: 0.027 | Tree loss: 1.800 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 1.771 | Reg loss: 0.027 | Tree loss: 1.771 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 1.786 | Reg loss: 0.027 | Tree loss: 1.786 | Accuracy: 0.319838 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 1.851 | Reg loss: 0.026 | Tree loss: 1.851 | Accuracy: 0.371094 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 1.899 | Reg loss: 0.026 | Tree loss: 1.899 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 1.851 | Reg loss: 0.026 | Tree loss: 1.851 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 1.865 | Reg loss: 0.026 | Tree loss: 1.865 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.357422 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 1.867 | Reg loss: 0.026 | Tree loss: 1.867 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 1.871 | Reg loss: 0.026 | Tree loss: 1.871 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 1.845 | Reg loss: 0.026 | Tree loss: 1.845 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 1.835 | Reg loss: 0.026 | Tree loss: 1.835 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 1.807 | Reg loss: 0.026 | Tree loss: 1.807 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 1.817 | Reg loss: 0.026 | Tree loss: 1.817 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 1.878 | Reg loss: 0.026 | Tree loss: 1.878 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 1.828 | Reg loss: 0.027 | Tree loss: 1.828 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 1.823 | Reg loss: 0.027 | Tree loss: 1.823 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 1.835 | Reg loss: 0.027 | Tree loss: 1.835 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 1.828 | Reg loss: 0.027 | Tree loss: 1.828 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 1.810 | Reg loss: 0.027 | Tree loss: 1.810 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 1.766 | Reg loss: 0.027 | Tree loss: 1.766 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 1.790 | Reg loss: 0.027 | Tree loss: 1.790 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 1.811 | Reg loss: 0.027 | Tree loss: 1.811 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 1.729 | Reg loss: 0.027 | Tree loss: 1.729 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 1.774 | Reg loss: 0.027 | Tree loss: 1.774 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 1.787 | Reg loss: 0.027 | Tree loss: 1.787 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 1.793 | Reg loss: 0.027 | Tree loss: 1.793 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 1.787 | Reg loss: 0.027 | Tree loss: 1.787 | Accuracy: 0.293522 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 1.840 | Reg loss: 0.026 | Tree loss: 1.840 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 1.849 | Reg loss: 0.026 | Tree loss: 1.849 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 1.866 | Reg loss: 0.026 | Tree loss: 1.866 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 1.892 | Reg loss: 0.026 | Tree loss: 1.892 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 1.872 | Reg loss: 0.026 | Tree loss: 1.872 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 1.827 | Reg loss: 0.026 | Tree loss: 1.827 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 1.838 | Reg loss: 0.026 | Tree loss: 1.838 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 1.814 | Reg loss: 0.026 | Tree loss: 1.814 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 1.834 | Reg loss: 0.026 | Tree loss: 1.834 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 1.870 | Reg loss: 0.026 | Tree loss: 1.870 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 1.880 | Reg loss: 0.026 | Tree loss: 1.880 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 1.838 | Reg loss: 0.026 | Tree loss: 1.838 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 1.850 | Reg loss: 0.026 | Tree loss: 1.850 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 1.798 | Reg loss: 0.027 | Tree loss: 1.798 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 1.808 | Reg loss: 0.027 | Tree loss: 1.808 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 1.818 | Reg loss: 0.027 | Tree loss: 1.818 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 1.749 | Reg loss: 0.027 | Tree loss: 1.749 | Accuracy: 0.363281 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 1.766 | Reg loss: 0.027 | Tree loss: 1.766 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 1.773 | Reg loss: 0.027 | Tree loss: 1.773 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 1.837 | Reg loss: 0.027 | Tree loss: 1.837 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 1.775 | Reg loss: 0.027 | Tree loss: 1.775 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 1.768 | Reg loss: 0.027 | Tree loss: 1.768 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 1.778 | Reg loss: 0.027 | Tree loss: 1.778 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 1.784 | Reg loss: 0.027 | Tree loss: 1.784 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 1.743 | Reg loss: 0.027 | Tree loss: 1.743 | Accuracy: 0.334008 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 1.885 | Reg loss: 0.026 | Tree loss: 1.885 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 1.869 | Reg loss: 0.026 | Tree loss: 1.869 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 1.851 | Reg loss: 0.026 | Tree loss: 1.851 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 1.826 | Reg loss: 0.026 | Tree loss: 1.826 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 1.850 | Reg loss: 0.026 | Tree loss: 1.850 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.353516 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 1.863 | Reg loss: 0.026 | Tree loss: 1.863 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.267578 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 1.849 | Reg loss: 0.026 | Tree loss: 1.849 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 1.834 | Reg loss: 0.026 | Tree loss: 1.834 | Accuracy: 0.269531 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 1.848 | Reg loss: 0.026 | Tree loss: 1.848 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 1.814 | Reg loss: 0.026 | Tree loss: 1.814 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 1.808 | Reg loss: 0.027 | Tree loss: 1.808 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 1.792 | Reg loss: 0.027 | Tree loss: 1.792 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 1.797 | Reg loss: 0.027 | Tree loss: 1.797 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 1.781 | Reg loss: 0.027 | Tree loss: 1.781 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 1.759 | Reg loss: 0.027 | Tree loss: 1.759 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 1.776 | Reg loss: 0.027 | Tree loss: 1.776 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 1.784 | Reg loss: 0.027 | Tree loss: 1.784 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 1.809 | Reg loss: 0.027 | Tree loss: 1.809 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 1.779 | Reg loss: 0.027 | Tree loss: 1.779 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 1.728 | Reg loss: 0.027 | Tree loss: 1.728 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 1.796 | Reg loss: 0.027 | Tree loss: 1.796 | Accuracy: 0.291498 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 1.895 | Reg loss: 0.026 | Tree loss: 1.895 | Accuracy: 0.273438 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 1.863 | Reg loss: 0.026 | Tree loss: 1.863 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 1.841 | Reg loss: 0.026 | Tree loss: 1.841 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.312500 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 1.849 | Reg loss: 0.026 | Tree loss: 1.849 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 1.811 | Reg loss: 0.026 | Tree loss: 1.811 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 1.831 | Reg loss: 0.026 | Tree loss: 1.831 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 1.841 | Reg loss: 0.026 | Tree loss: 1.841 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 1.825 | Reg loss: 0.026 | Tree loss: 1.825 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 1.798 | Reg loss: 0.026 | Tree loss: 1.798 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 1.834 | Reg loss: 0.026 | Tree loss: 1.834 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 1.796 | Reg loss: 0.027 | Tree loss: 1.796 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 1.787 | Reg loss: 0.027 | Tree loss: 1.787 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 1.735 | Reg loss: 0.027 | Tree loss: 1.735 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 1.794 | Reg loss: 0.027 | Tree loss: 1.794 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 1.783 | Reg loss: 0.027 | Tree loss: 1.783 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 1.785 | Reg loss: 0.027 | Tree loss: 1.785 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 1.775 | Reg loss: 0.027 | Tree loss: 1.775 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 1.782 | Reg loss: 0.027 | Tree loss: 1.782 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 1.716 | Reg loss: 0.027 | Tree loss: 1.716 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 1.711 | Reg loss: 0.027 | Tree loss: 1.711 | Accuracy: 0.291498 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 1.844 | Reg loss: 0.026 | Tree loss: 1.844 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 1.860 | Reg loss: 0.026 | Tree loss: 1.860 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 1.861 | Reg loss: 0.026 | Tree loss: 1.861 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 1.821 | Reg loss: 0.026 | Tree loss: 1.821 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 1.864 | Reg loss: 0.026 | Tree loss: 1.864 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 1.904 | Reg loss: 0.026 | Tree loss: 1.904 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 1.825 | Reg loss: 0.026 | Tree loss: 1.825 | Accuracy: 0.273438 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 1.787 | Reg loss: 0.026 | Tree loss: 1.787 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 1.798 | Reg loss: 0.026 | Tree loss: 1.798 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 1.792 | Reg loss: 0.026 | Tree loss: 1.792 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 1.746 | Reg loss: 0.027 | Tree loss: 1.746 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 1.771 | Reg loss: 0.027 | Tree loss: 1.771 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 1.752 | Reg loss: 0.027 | Tree loss: 1.752 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 1.786 | Reg loss: 0.027 | Tree loss: 1.786 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 1.731 | Reg loss: 0.027 | Tree loss: 1.731 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 1.742 | Reg loss: 0.027 | Tree loss: 1.742 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 1.735 | Reg loss: 0.027 | Tree loss: 1.735 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 1.732 | Reg loss: 0.027 | Tree loss: 1.732 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 1.805 | Reg loss: 0.027 | Tree loss: 1.805 | Accuracy: 0.303644 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 1.831 | Reg loss: 0.026 | Tree loss: 1.831 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 1.847 | Reg loss: 0.026 | Tree loss: 1.847 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 1.872 | Reg loss: 0.026 | Tree loss: 1.872 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 1.780 | Reg loss: 0.026 | Tree loss: 1.780 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 1.782 | Reg loss: 0.026 | Tree loss: 1.782 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 1.835 | Reg loss: 0.026 | Tree loss: 1.835 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.347656 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.330078 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 1.843 | Reg loss: 0.026 | Tree loss: 1.843 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 1.818 | Reg loss: 0.026 | Tree loss: 1.818 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 1.841 | Reg loss: 0.026 | Tree loss: 1.841 | Accuracy: 0.271484 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 1.736 | Reg loss: 0.026 | Tree loss: 1.736 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 1.793 | Reg loss: 0.027 | Tree loss: 1.793 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 1.752 | Reg loss: 0.027 | Tree loss: 1.752 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 1.770 | Reg loss: 0.027 | Tree loss: 1.770 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 1.785 | Reg loss: 0.027 | Tree loss: 1.785 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 1.732 | Reg loss: 0.027 | Tree loss: 1.732 | Accuracy: 0.347656 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 1.750 | Reg loss: 0.027 | Tree loss: 1.750 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 1.801 | Reg loss: 0.027 | Tree loss: 1.801 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 1.783 | Reg loss: 0.027 | Tree loss: 1.783 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 1.758 | Reg loss: 0.027 | Tree loss: 1.758 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 1.709 | Reg loss: 0.027 | Tree loss: 1.709 | Accuracy: 0.309717 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 1.806 | Reg loss: 0.026 | Tree loss: 1.806 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 1.859 | Reg loss: 0.026 | Tree loss: 1.859 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 1.865 | Reg loss: 0.026 | Tree loss: 1.865 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 1.862 | Reg loss: 0.026 | Tree loss: 1.862 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 1.781 | Reg loss: 0.026 | Tree loss: 1.781 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.357422 | 0.24 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 1.845 | Reg loss: 0.026 | Tree loss: 1.845 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 1.845 | Reg loss: 0.026 | Tree loss: 1.845 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 1.811 | Reg loss: 0.026 | Tree loss: 1.811 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 1.787 | Reg loss: 0.026 | Tree loss: 1.787 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 1.780 | Reg loss: 0.026 | Tree loss: 1.780 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 1.790 | Reg loss: 0.027 | Tree loss: 1.790 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 1.764 | Reg loss: 0.027 | Tree loss: 1.764 | Accuracy: 0.355469 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 1.781 | Reg loss: 0.027 | Tree loss: 1.781 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 1.758 | Reg loss: 0.027 | Tree loss: 1.758 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 1.722 | Reg loss: 0.027 | Tree loss: 1.722 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 1.754 | Reg loss: 0.027 | Tree loss: 1.754 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 1.804 | Reg loss: 0.027 | Tree loss: 1.804 | Accuracy: 0.263672 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 1.712 | Reg loss: 0.027 | Tree loss: 1.712 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 1.679 | Reg loss: 0.027 | Tree loss: 1.679 | Accuracy: 0.342105 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 1.858 | Reg loss: 0.026 | Tree loss: 1.858 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 1.831 | Reg loss: 0.026 | Tree loss: 1.831 | Accuracy: 0.271484 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 1.829 | Reg loss: 0.026 | Tree loss: 1.829 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 1.823 | Reg loss: 0.026 | Tree loss: 1.823 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 1.831 | Reg loss: 0.026 | Tree loss: 1.831 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 1.780 | Reg loss: 0.026 | Tree loss: 1.780 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 1.775 | Reg loss: 0.026 | Tree loss: 1.775 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 1.828 | Reg loss: 0.026 | Tree loss: 1.828 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 1.843 | Reg loss: 0.026 | Tree loss: 1.843 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 1.807 | Reg loss: 0.026 | Tree loss: 1.807 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.335938 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 1.727 | Reg loss: 0.026 | Tree loss: 1.727 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 1.747 | Reg loss: 0.026 | Tree loss: 1.747 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 1.781 | Reg loss: 0.026 | Tree loss: 1.781 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 1.746 | Reg loss: 0.027 | Tree loss: 1.746 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 1.735 | Reg loss: 0.027 | Tree loss: 1.735 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 1.739 | Reg loss: 0.027 | Tree loss: 1.739 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 1.731 | Reg loss: 0.027 | Tree loss: 1.731 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 1.750 | Reg loss: 0.027 | Tree loss: 1.750 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 1.722 | Reg loss: 0.027 | Tree loss: 1.722 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 1.785 | Reg loss: 0.027 | Tree loss: 1.785 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 1.808 | Reg loss: 0.027 | Tree loss: 1.808 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 1.746 | Reg loss: 0.027 | Tree loss: 1.746 | Accuracy: 0.321862 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 1.832 | Reg loss: 0.026 | Tree loss: 1.832 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 1.830 | Reg loss: 0.026 | Tree loss: 1.830 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 1.802 | Reg loss: 0.026 | Tree loss: 1.802 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 1.840 | Reg loss: 0.026 | Tree loss: 1.840 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 1.832 | Reg loss: 0.026 | Tree loss: 1.832 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 1.732 | Reg loss: 0.026 | Tree loss: 1.732 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 1.786 | Reg loss: 0.027 | Tree loss: 1.786 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 1.767 | Reg loss: 0.027 | Tree loss: 1.767 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 1.771 | Reg loss: 0.027 | Tree loss: 1.771 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 1.762 | Reg loss: 0.027 | Tree loss: 1.762 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 1.777 | Reg loss: 0.027 | Tree loss: 1.777 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 1.662 | Reg loss: 0.027 | Tree loss: 1.662 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 1.731 | Reg loss: 0.027 | Tree loss: 1.731 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 1.769 | Reg loss: 0.027 | Tree loss: 1.769 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 1.726 | Reg loss: 0.027 | Tree loss: 1.726 | Accuracy: 0.313765 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 1.842 | Reg loss: 0.026 | Tree loss: 1.842 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 1.836 | Reg loss: 0.026 | Tree loss: 1.836 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 1.792 | Reg loss: 0.026 | Tree loss: 1.792 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 1.804 | Reg loss: 0.026 | Tree loss: 1.804 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 1.822 | Reg loss: 0.026 | Tree loss: 1.822 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 1.811 | Reg loss: 0.026 | Tree loss: 1.811 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 1.804 | Reg loss: 0.026 | Tree loss: 1.804 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 1.814 | Reg loss: 0.026 | Tree loss: 1.814 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 1.823 | Reg loss: 0.026 | Tree loss: 1.823 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 1.792 | Reg loss: 0.026 | Tree loss: 1.792 | Accuracy: 0.255859 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 1.775 | Reg loss: 0.026 | Tree loss: 1.775 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.353516 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 1.734 | Reg loss: 0.026 | Tree loss: 1.734 | Accuracy: 0.294922 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 1.730 | Reg loss: 0.027 | Tree loss: 1.730 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 1.772 | Reg loss: 0.027 | Tree loss: 1.772 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 1.750 | Reg loss: 0.027 | Tree loss: 1.750 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 1.774 | Reg loss: 0.027 | Tree loss: 1.774 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 1.685 | Reg loss: 0.027 | Tree loss: 1.685 | Accuracy: 0.353516 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 1.747 | Reg loss: 0.027 | Tree loss: 1.747 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 1.710 | Reg loss: 0.027 | Tree loss: 1.710 | Accuracy: 0.350202 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 1.841 | Reg loss: 0.026 | Tree loss: 1.841 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 1.838 | Reg loss: 0.026 | Tree loss: 1.838 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 1.853 | Reg loss: 0.026 | Tree loss: 1.853 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 1.801 | Reg loss: 0.026 | Tree loss: 1.801 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 1.850 | Reg loss: 0.026 | Tree loss: 1.850 | Accuracy: 0.267578 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 1.822 | Reg loss: 0.026 | Tree loss: 1.822 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 1.762 | Reg loss: 0.026 | Tree loss: 1.762 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 1.821 | Reg loss: 0.026 | Tree loss: 1.821 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 1.748 | Reg loss: 0.026 | Tree loss: 1.748 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 1.736 | Reg loss: 0.026 | Tree loss: 1.736 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 1.747 | Reg loss: 0.027 | Tree loss: 1.747 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 1.691 | Reg loss: 0.027 | Tree loss: 1.691 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 1.772 | Reg loss: 0.027 | Tree loss: 1.772 | Accuracy: 0.248047 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 1.725 | Reg loss: 0.027 | Tree loss: 1.725 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 1.706 | Reg loss: 0.027 | Tree loss: 1.706 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 1.723 | Reg loss: 0.027 | Tree loss: 1.723 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 1.749 | Reg loss: 0.027 | Tree loss: 1.749 | Accuracy: 0.342105 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 1.862 | Reg loss: 0.026 | Tree loss: 1.862 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 1.815 | Reg loss: 0.026 | Tree loss: 1.815 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 1.818 | Reg loss: 0.026 | Tree loss: 1.818 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 1.805 | Reg loss: 0.026 | Tree loss: 1.805 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 1.818 | Reg loss: 0.026 | Tree loss: 1.818 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 1.802 | Reg loss: 0.026 | Tree loss: 1.802 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 1.825 | Reg loss: 0.026 | Tree loss: 1.825 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 1.756 | Reg loss: 0.026 | Tree loss: 1.756 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 1.861 | Reg loss: 0.026 | Tree loss: 1.861 | Accuracy: 0.248047 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 1.740 | Reg loss: 0.026 | Tree loss: 1.740 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 1.748 | Reg loss: 0.026 | Tree loss: 1.748 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 1.714 | Reg loss: 0.027 | Tree loss: 1.714 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 1.722 | Reg loss: 0.027 | Tree loss: 1.722 | Accuracy: 0.343750 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 1.717 | Reg loss: 0.027 | Tree loss: 1.717 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 1.718 | Reg loss: 0.027 | Tree loss: 1.718 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 1.677 | Reg loss: 0.027 | Tree loss: 1.677 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 1.684 | Reg loss: 0.027 | Tree loss: 1.684 | Accuracy: 0.319838 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 1.801 | Reg loss: 0.026 | Tree loss: 1.801 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 1.849 | Reg loss: 0.026 | Tree loss: 1.849 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 1.817 | Reg loss: 0.026 | Tree loss: 1.817 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 1.842 | Reg loss: 0.026 | Tree loss: 1.842 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 1.815 | Reg loss: 0.026 | Tree loss: 1.815 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 1.751 | Reg loss: 0.026 | Tree loss: 1.751 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.277344 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 1.734 | Reg loss: 0.026 | Tree loss: 1.734 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 1.726 | Reg loss: 0.026 | Tree loss: 1.726 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 1.742 | Reg loss: 0.027 | Tree loss: 1.742 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 1.713 | Reg loss: 0.027 | Tree loss: 1.713 | Accuracy: 0.349609 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 1.742 | Reg loss: 0.027 | Tree loss: 1.742 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 1.751 | Reg loss: 0.027 | Tree loss: 1.751 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 1.744 | Reg loss: 0.027 | Tree loss: 1.744 | Accuracy: 0.305668 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 1.845 | Reg loss: 0.026 | Tree loss: 1.845 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 1.821 | Reg loss: 0.026 | Tree loss: 1.821 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.359375 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 1.748 | Reg loss: 0.026 | Tree loss: 1.748 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 1.804 | Reg loss: 0.026 | Tree loss: 1.804 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.273438 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 1.712 | Reg loss: 0.026 | Tree loss: 1.712 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 1.771 | Reg loss: 0.026 | Tree loss: 1.771 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.349609 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 1.680 | Reg loss: 0.027 | Tree loss: 1.680 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 1.736 | Reg loss: 0.027 | Tree loss: 1.736 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 1.710 | Reg loss: 0.027 | Tree loss: 1.710 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 1.722 | Reg loss: 0.027 | Tree loss: 1.722 | Accuracy: 0.317814 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 1.798 | Reg loss: 0.026 | Tree loss: 1.798 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 1.781 | Reg loss: 0.026 | Tree loss: 1.781 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.361328 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 1.805 | Reg loss: 0.026 | Tree loss: 1.805 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 1.751 | Reg loss: 0.026 | Tree loss: 1.751 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 1.831 | Reg loss: 0.026 | Tree loss: 1.831 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.351562 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 1.771 | Reg loss: 0.026 | Tree loss: 1.771 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.257812 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 1.721 | Reg loss: 0.027 | Tree loss: 1.721 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 1.741 | Reg loss: 0.027 | Tree loss: 1.741 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 1.705 | Reg loss: 0.027 | Tree loss: 1.705 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 1.680 | Reg loss: 0.027 | Tree loss: 1.680 | Accuracy: 0.344130 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 1.780 | Reg loss: 0.026 | Tree loss: 1.780 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 1.825 | Reg loss: 0.026 | Tree loss: 1.825 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 1.755 | Reg loss: 0.026 | Tree loss: 1.755 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 1.810 | Reg loss: 0.026 | Tree loss: 1.810 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 1.751 | Reg loss: 0.026 | Tree loss: 1.751 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 1.708 | Reg loss: 0.026 | Tree loss: 1.708 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.349609 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 1.695 | Reg loss: 0.026 | Tree loss: 1.695 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 1.702 | Reg loss: 0.026 | Tree loss: 1.702 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 1.711 | Reg loss: 0.027 | Tree loss: 1.711 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 1.738 | Reg loss: 0.027 | Tree loss: 1.738 | Accuracy: 0.267578 | 0.24 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 1.692 | Reg loss: 0.027 | Tree loss: 1.692 | Accuracy: 0.307692 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 1.838 | Reg loss: 0.026 | Tree loss: 1.838 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 1.775 | Reg loss: 0.026 | Tree loss: 1.775 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 1.849 | Reg loss: 0.026 | Tree loss: 1.849 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 1.799 | Reg loss: 0.026 | Tree loss: 1.799 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 1.780 | Reg loss: 0.026 | Tree loss: 1.780 | Accuracy: 0.326172 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 1.821 | Reg loss: 0.026 | Tree loss: 1.821 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 1.815 | Reg loss: 0.026 | Tree loss: 1.815 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.347656 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 1.734 | Reg loss: 0.026 | Tree loss: 1.734 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 1.754 | Reg loss: 0.026 | Tree loss: 1.754 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 1.732 | Reg loss: 0.026 | Tree loss: 1.732 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 1.716 | Reg loss: 0.026 | Tree loss: 1.716 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 1.721 | Reg loss: 0.026 | Tree loss: 1.721 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 1.713 | Reg loss: 0.027 | Tree loss: 1.713 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 1.665 | Reg loss: 0.027 | Tree loss: 1.665 | Accuracy: 0.305668 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 1.812 | Reg loss: 0.026 | Tree loss: 1.812 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 1.826 | Reg loss: 0.026 | Tree loss: 1.826 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.269531 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 1.687 | Reg loss: 0.026 | Tree loss: 1.687 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 1.777 | Reg loss: 0.026 | Tree loss: 1.777 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 1.689 | Reg loss: 0.026 | Tree loss: 1.689 | Accuracy: 0.371094 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 1.762 | Reg loss: 0.026 | Tree loss: 1.762 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 1.699 | Reg loss: 0.026 | Tree loss: 1.699 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 1.684 | Reg loss: 0.026 | Tree loss: 1.684 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 1.720 | Reg loss: 0.027 | Tree loss: 1.720 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 1.730 | Reg loss: 0.027 | Tree loss: 1.730 | Accuracy: 0.331984 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 1.801 | Reg loss: 0.026 | Tree loss: 1.801 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 1.850 | Reg loss: 0.026 | Tree loss: 1.850 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 1.774 | Reg loss: 0.026 | Tree loss: 1.774 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 1.803 | Reg loss: 0.026 | Tree loss: 1.803 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.296875 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.265625 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 1.714 | Reg loss: 0.026 | Tree loss: 1.714 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.347656 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 1.774 | Reg loss: 0.026 | Tree loss: 1.774 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 1.689 | Reg loss: 0.026 | Tree loss: 1.689 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 1.707 | Reg loss: 0.026 | Tree loss: 1.707 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 1.659 | Reg loss: 0.026 | Tree loss: 1.659 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 1.731 | Reg loss: 0.027 | Tree loss: 1.731 | Accuracy: 0.323887 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 1.827 | Reg loss: 0.026 | Tree loss: 1.827 | Accuracy: 0.355469 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 1.868 | Reg loss: 0.026 | Tree loss: 1.868 | Accuracy: 0.259766 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 1.799 | Reg loss: 0.026 | Tree loss: 1.799 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 1.804 | Reg loss: 0.026 | Tree loss: 1.804 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 1.802 | Reg loss: 0.026 | Tree loss: 1.802 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 1.775 | Reg loss: 0.026 | Tree loss: 1.775 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 1.827 | Reg loss: 0.026 | Tree loss: 1.827 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.279297 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 1.731 | Reg loss: 0.026 | Tree loss: 1.731 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 1.691 | Reg loss: 0.026 | Tree loss: 1.691 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 1.693 | Reg loss: 0.026 | Tree loss: 1.693 | Accuracy: 0.347656 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 1.703 | Reg loss: 0.026 | Tree loss: 1.703 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 1.756 | Reg loss: 0.026 | Tree loss: 1.756 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 1.665 | Reg loss: 0.026 | Tree loss: 1.665 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 1.667 | Reg loss: 0.026 | Tree loss: 1.667 | Accuracy: 0.365234 | 0.24 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 1.663 | Reg loss: 0.027 | Tree loss: 1.663 | Accuracy: 0.334008 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 1.890 | Reg loss: 0.026 | Tree loss: 1.890 | Accuracy: 0.265625 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 1.820 | Reg loss: 0.026 | Tree loss: 1.820 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 1.782 | Reg loss: 0.026 | Tree loss: 1.782 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 1.798 | Reg loss: 0.026 | Tree loss: 1.798 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 1.810 | Reg loss: 0.026 | Tree loss: 1.810 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 1.805 | Reg loss: 0.026 | Tree loss: 1.805 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.279297 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 1.718 | Reg loss: 0.026 | Tree loss: 1.718 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 1.747 | Reg loss: 0.026 | Tree loss: 1.747 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 1.656 | Reg loss: 0.026 | Tree loss: 1.656 | Accuracy: 0.369141 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 1.723 | Reg loss: 0.026 | Tree loss: 1.723 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 1.718 | Reg loss: 0.026 | Tree loss: 1.718 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 1.697 | Reg loss: 0.026 | Tree loss: 1.697 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 1.701 | Reg loss: 0.026 | Tree loss: 1.701 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 1.682 | Reg loss: 0.026 | Tree loss: 1.682 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.267206 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 1.751 | Reg loss: 0.026 | Tree loss: 1.751 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.263672 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 1.756 | Reg loss: 0.026 | Tree loss: 1.756 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 1.766 | Reg loss: 0.026 | Tree loss: 1.766 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 1.762 | Reg loss: 0.026 | Tree loss: 1.762 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 1.721 | Reg loss: 0.026 | Tree loss: 1.721 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 1.727 | Reg loss: 0.026 | Tree loss: 1.727 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 1.687 | Reg loss: 0.026 | Tree loss: 1.687 | Accuracy: 0.361328 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 1.771 | Reg loss: 0.026 | Tree loss: 1.771 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 1.727 | Reg loss: 0.026 | Tree loss: 1.727 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 1.673 | Reg loss: 0.026 | Tree loss: 1.673 | Accuracy: 0.365234 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 1.713 | Reg loss: 0.026 | Tree loss: 1.713 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 1.683 | Reg loss: 0.026 | Tree loss: 1.683 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 1.677 | Reg loss: 0.026 | Tree loss: 1.677 | Accuracy: 0.315789 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 1.806 | Reg loss: 0.026 | Tree loss: 1.806 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 1.818 | Reg loss: 0.026 | Tree loss: 1.818 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 1.824 | Reg loss: 0.026 | Tree loss: 1.824 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 1.806 | Reg loss: 0.026 | Tree loss: 1.806 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 1.755 | Reg loss: 0.026 | Tree loss: 1.755 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 1.720 | Reg loss: 0.026 | Tree loss: 1.720 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.281250 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 1.740 | Reg loss: 0.026 | Tree loss: 1.740 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.324219 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.255859 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 1.689 | Reg loss: 0.026 | Tree loss: 1.689 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 1.699 | Reg loss: 0.026 | Tree loss: 1.699 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.334008 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 1.777 | Reg loss: 0.026 | Tree loss: 1.777 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 1.840 | Reg loss: 0.026 | Tree loss: 1.840 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 1.797 | Reg loss: 0.026 | Tree loss: 1.797 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 1.805 | Reg loss: 0.026 | Tree loss: 1.805 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 1.781 | Reg loss: 0.026 | Tree loss: 1.781 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 1.781 | Reg loss: 0.026 | Tree loss: 1.781 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.273438 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 1.721 | Reg loss: 0.026 | Tree loss: 1.721 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 1.687 | Reg loss: 0.026 | Tree loss: 1.687 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 1.705 | Reg loss: 0.026 | Tree loss: 1.705 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 1.702 | Reg loss: 0.026 | Tree loss: 1.702 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 1.688 | Reg loss: 0.026 | Tree loss: 1.688 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 1.682 | Reg loss: 0.026 | Tree loss: 1.682 | Accuracy: 0.323887 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 1.805 | Reg loss: 0.026 | Tree loss: 1.805 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 1.828 | Reg loss: 0.026 | Tree loss: 1.828 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.314453 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 1.788 | Reg loss: 0.026 | Tree loss: 1.788 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 1.718 | Reg loss: 0.026 | Tree loss: 1.718 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.343750 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 1.774 | Reg loss: 0.026 | Tree loss: 1.774 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 1.705 | Reg loss: 0.026 | Tree loss: 1.705 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 1.707 | Reg loss: 0.026 | Tree loss: 1.707 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 1.716 | Reg loss: 0.026 | Tree loss: 1.716 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.277344 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 1.729 | Reg loss: 0.026 | Tree loss: 1.729 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 1.727 | Reg loss: 0.026 | Tree loss: 1.727 | Accuracy: 0.275391 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 1.691 | Reg loss: 0.026 | Tree loss: 1.691 | Accuracy: 0.281250 | 0.24 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 1.683 | Reg loss: 0.026 | Tree loss: 1.683 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 1.688 | Reg loss: 0.026 | Tree loss: 1.688 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 1.701 | Reg loss: 0.026 | Tree loss: 1.701 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 1.701 | Reg loss: 0.026 | Tree loss: 1.701 | Accuracy: 0.311741 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.351562 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 1.803 | Reg loss: 0.026 | Tree loss: 1.803 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 1.811 | Reg loss: 0.026 | Tree loss: 1.811 | Accuracy: 0.291016 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.318359 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.339844 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 1.787 | Reg loss: 0.026 | Tree loss: 1.787 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.267578 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 1.766 | Reg loss: 0.026 | Tree loss: 1.766 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.285156 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 1.756 | Reg loss: 0.026 | Tree loss: 1.756 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 1.701 | Reg loss: 0.026 | Tree loss: 1.701 | Accuracy: 0.335938 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 1.739 | Reg loss: 0.026 | Tree loss: 1.739 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 1.720 | Reg loss: 0.026 | Tree loss: 1.720 | Accuracy: 0.304688 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 1.687 | Reg loss: 0.026 | Tree loss: 1.687 | Accuracy: 0.357422 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.353516 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 1.691 | Reg loss: 0.026 | Tree loss: 1.691 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 1.660 | Reg loss: 0.026 | Tree loss: 1.660 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 1.668 | Reg loss: 0.026 | Tree loss: 1.668 | Accuracy: 0.287449 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 1.808 | Reg loss: 0.026 | Tree loss: 1.808 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 1.792 | Reg loss: 0.026 | Tree loss: 1.792 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.330078 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 1.808 | Reg loss: 0.026 | Tree loss: 1.808 | Accuracy: 0.306641 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 1.766 | Reg loss: 0.026 | Tree loss: 1.766 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 1.801 | Reg loss: 0.026 | Tree loss: 1.801 | Accuracy: 0.292969 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.345703 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 1.768 | Reg loss: 0.026 | Tree loss: 1.768 | Accuracy: 0.287109 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.283203 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 1.740 | Reg loss: 0.026 | Tree loss: 1.740 | Accuracy: 0.312500 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 1.782 | Reg loss: 0.026 | Tree loss: 1.782 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 1.747 | Reg loss: 0.026 | Tree loss: 1.747 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 1.716 | Reg loss: 0.026 | Tree loss: 1.716 | Accuracy: 0.333984 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.289062 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 1.721 | Reg loss: 0.026 | Tree loss: 1.721 | Accuracy: 0.300781 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 1.681 | Reg loss: 0.026 | Tree loss: 1.681 | Accuracy: 0.332031 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 1.685 | Reg loss: 0.026 | Tree loss: 1.685 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 1.660 | Reg loss: 0.026 | Tree loss: 1.660 | Accuracy: 0.328125 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.302734 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.294922 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 1.716 | Reg loss: 0.026 | Tree loss: 1.716 | Accuracy: 0.341797 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 1.694 | Reg loss: 0.026 | Tree loss: 1.694 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.326172 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 1.707 | Reg loss: 0.026 | Tree loss: 1.707 | Accuracy: 0.308594 | 0.24 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.297571 | 0.24 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 1.791 | Reg loss: 0.026 | Tree loss: 1.791 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.310547 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.324219 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.337891 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 1.814 | Reg loss: 0.026 | Tree loss: 1.814 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 1.800 | Reg loss: 0.026 | Tree loss: 1.800 | Accuracy: 0.298828 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.320312 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.322266 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.296875 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 1.740 | Reg loss: 0.026 | Tree loss: 1.740 | Accuracy: 0.316406 | 0.24 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 1.783 | Reg loss: 0.026 | Tree loss: 1.783 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 1.712 | Reg loss: 0.026 | Tree loss: 1.712 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 1.682 | Reg loss: 0.026 | Tree loss: 1.682 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 1.734 | Reg loss: 0.026 | Tree loss: 1.734 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 1.651 | Reg loss: 0.026 | Tree loss: 1.651 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 1.754 | Reg loss: 0.026 | Tree loss: 1.754 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 1.712 | Reg loss: 0.026 | Tree loss: 1.712 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 1.722 | Reg loss: 0.026 | Tree loss: 1.722 | Accuracy: 0.301619 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 1.858 | Reg loss: 0.026 | Tree loss: 1.858 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 1.778 | Reg loss: 0.026 | Tree loss: 1.778 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 1.784 | Reg loss: 0.026 | Tree loss: 1.784 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 1.830 | Reg loss: 0.026 | Tree loss: 1.830 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 1.754 | Reg loss: 0.026 | Tree loss: 1.754 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 1.761 | Reg loss: 0.026 | Tree loss: 1.761 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 1.729 | Reg loss: 0.026 | Tree loss: 1.729 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 1.732 | Reg loss: 0.026 | Tree loss: 1.732 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 1.766 | Reg loss: 0.026 | Tree loss: 1.766 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 1.713 | Reg loss: 0.026 | Tree loss: 1.713 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 1.686 | Reg loss: 0.026 | Tree loss: 1.686 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 1.699 | Reg loss: 0.026 | Tree loss: 1.699 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 1.713 | Reg loss: 0.026 | Tree loss: 1.713 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 1.726 | Reg loss: 0.026 | Tree loss: 1.726 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 1.661 | Reg loss: 0.026 | Tree loss: 1.661 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 1.662 | Reg loss: 0.026 | Tree loss: 1.662 | Accuracy: 0.340081 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 1.752 | Reg loss: 0.026 | Tree loss: 1.752 | Accuracy: 0.347656 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 1.736 | Reg loss: 0.026 | Tree loss: 1.736 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.339844 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 1.819 | Reg loss: 0.026 | Tree loss: 1.819 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 1.733 | Reg loss: 0.026 | Tree loss: 1.733 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 1.792 | Reg loss: 0.026 | Tree loss: 1.792 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 1.755 | Reg loss: 0.026 | Tree loss: 1.755 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 1.740 | Reg loss: 0.026 | Tree loss: 1.740 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 1.708 | Reg loss: 0.026 | Tree loss: 1.708 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 1.708 | Reg loss: 0.026 | Tree loss: 1.708 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 1.714 | Reg loss: 0.026 | Tree loss: 1.714 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 1.718 | Reg loss: 0.026 | Tree loss: 1.718 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 1.720 | Reg loss: 0.026 | Tree loss: 1.720 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 1.734 | Reg loss: 0.026 | Tree loss: 1.734 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 1.683 | Reg loss: 0.026 | Tree loss: 1.683 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 1.697 | Reg loss: 0.026 | Tree loss: 1.697 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 1.685 | Reg loss: 0.026 | Tree loss: 1.685 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 1.680 | Reg loss: 0.026 | Tree loss: 1.680 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 1.714 | Reg loss: 0.026 | Tree loss: 1.714 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 1.722 | Reg loss: 0.026 | Tree loss: 1.722 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 1.682 | Reg loss: 0.026 | Tree loss: 1.682 | Accuracy: 0.329960 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 1.811 | Reg loss: 0.026 | Tree loss: 1.811 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 1.772 | Reg loss: 0.026 | Tree loss: 1.772 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 1.813 | Reg loss: 0.026 | Tree loss: 1.813 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 1.782 | Reg loss: 0.026 | Tree loss: 1.782 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 1.748 | Reg loss: 0.026 | Tree loss: 1.748 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 1.807 | Reg loss: 0.026 | Tree loss: 1.807 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 1.763 | Reg loss: 0.026 | Tree loss: 1.763 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 1.776 | Reg loss: 0.026 | Tree loss: 1.776 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 1.759 | Reg loss: 0.026 | Tree loss: 1.759 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 1.711 | Reg loss: 0.026 | Tree loss: 1.711 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 1.779 | Reg loss: 0.026 | Tree loss: 1.779 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 1.747 | Reg loss: 0.026 | Tree loss: 1.747 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 1.741 | Reg loss: 0.026 | Tree loss: 1.741 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 1.712 | Reg loss: 0.026 | Tree loss: 1.712 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 1.688 | Reg loss: 0.026 | Tree loss: 1.688 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 1.698 | Reg loss: 0.026 | Tree loss: 1.698 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 1.715 | Reg loss: 0.026 | Tree loss: 1.715 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 1.697 | Reg loss: 0.026 | Tree loss: 1.697 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 1.662 | Reg loss: 0.026 | Tree loss: 1.662 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 1.686 | Reg loss: 0.026 | Tree loss: 1.686 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 1.660 | Reg loss: 0.026 | Tree loss: 1.660 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.299595 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 1.854 | Reg loss: 0.026 | Tree loss: 1.854 | Accuracy: 0.269531 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 1.794 | Reg loss: 0.026 | Tree loss: 1.794 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 1.767 | Reg loss: 0.026 | Tree loss: 1.767 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 1.798 | Reg loss: 0.026 | Tree loss: 1.798 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 1.796 | Reg loss: 0.026 | Tree loss: 1.796 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 1.790 | Reg loss: 0.026 | Tree loss: 1.790 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 1.743 | Reg loss: 0.026 | Tree loss: 1.743 | Accuracy: 0.318359 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 1.693 | Reg loss: 0.026 | Tree loss: 1.693 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 1.722 | Reg loss: 0.026 | Tree loss: 1.722 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.353516 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 1.721 | Reg loss: 0.026 | Tree loss: 1.721 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 1.737 | Reg loss: 0.026 | Tree loss: 1.737 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 1.705 | Reg loss: 0.026 | Tree loss: 1.705 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 1.731 | Reg loss: 0.026 | Tree loss: 1.731 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.345703 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 1.686 | Reg loss: 0.026 | Tree loss: 1.686 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 1.677 | Reg loss: 0.026 | Tree loss: 1.677 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.271484 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 1.670 | Reg loss: 0.026 | Tree loss: 1.670 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 1.664 | Reg loss: 0.026 | Tree loss: 1.664 | Accuracy: 0.325911 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 1.774 | Reg loss: 0.026 | Tree loss: 1.774 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 1.789 | Reg loss: 0.026 | Tree loss: 1.789 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 1.801 | Reg loss: 0.026 | Tree loss: 1.801 | Accuracy: 0.291016 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 1.773 | Reg loss: 0.026 | Tree loss: 1.773 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.333984 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 1.717 | Reg loss: 0.026 | Tree loss: 1.717 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 1.742 | Reg loss: 0.026 | Tree loss: 1.742 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 1.690 | Reg loss: 0.026 | Tree loss: 1.690 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 1.777 | Reg loss: 0.026 | Tree loss: 1.777 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 1.727 | Reg loss: 0.026 | Tree loss: 1.727 | Accuracy: 0.349609 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 1.726 | Reg loss: 0.026 | Tree loss: 1.726 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 1.762 | Reg loss: 0.026 | Tree loss: 1.762 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 1.723 | Reg loss: 0.026 | Tree loss: 1.723 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 1.724 | Reg loss: 0.026 | Tree loss: 1.724 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 1.730 | Reg loss: 0.026 | Tree loss: 1.730 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 1.681 | Reg loss: 0.026 | Tree loss: 1.681 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 1.688 | Reg loss: 0.026 | Tree loss: 1.688 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 1.624 | Reg loss: 0.026 | Tree loss: 1.624 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 1.678 | Reg loss: 0.026 | Tree loss: 1.678 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 1.686 | Reg loss: 0.026 | Tree loss: 1.686 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.297571 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 1.839 | Reg loss: 0.026 | Tree loss: 1.839 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 1.816 | Reg loss: 0.026 | Tree loss: 1.816 | Accuracy: 0.283203 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.326172 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 1.744 | Reg loss: 0.026 | Tree loss: 1.744 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 1.833 | Reg loss: 0.026 | Tree loss: 1.833 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.304688 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 1.765 | Reg loss: 0.026 | Tree loss: 1.765 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 1.770 | Reg loss: 0.026 | Tree loss: 1.770 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 1.716 | Reg loss: 0.026 | Tree loss: 1.716 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 1.736 | Reg loss: 0.026 | Tree loss: 1.736 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.285156 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 1.769 | Reg loss: 0.026 | Tree loss: 1.769 | Accuracy: 0.285156 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 1.693 | Reg loss: 0.026 | Tree loss: 1.693 | Accuracy: 0.328125 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 1.747 | Reg loss: 0.026 | Tree loss: 1.747 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 1.723 | Reg loss: 0.026 | Tree loss: 1.723 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 1.697 | Reg loss: 0.026 | Tree loss: 1.697 | Accuracy: 0.347656 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 1.697 | Reg loss: 0.026 | Tree loss: 1.697 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 1.627 | Reg loss: 0.026 | Tree loss: 1.627 | Accuracy: 0.359375 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 1.644 | Reg loss: 0.026 | Tree loss: 1.644 | Accuracy: 0.337891 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 1.732 | Reg loss: 0.026 | Tree loss: 1.732 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 1.676 | Reg loss: 0.026 | Tree loss: 1.676 | Accuracy: 0.332031 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 1.713 | Reg loss: 0.026 | Tree loss: 1.713 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 1.693 | Reg loss: 0.026 | Tree loss: 1.693 | Accuracy: 0.311741 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 1.771 | Reg loss: 0.026 | Tree loss: 1.771 | Accuracy: 0.369141 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 1.809 | Reg loss: 0.026 | Tree loss: 1.809 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 1.804 | Reg loss: 0.026 | Tree loss: 1.804 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 1.756 | Reg loss: 0.026 | Tree loss: 1.756 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 1.786 | Reg loss: 0.026 | Tree loss: 1.786 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 1.782 | Reg loss: 0.026 | Tree loss: 1.782 | Accuracy: 0.316406 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 1.745 | Reg loss: 0.026 | Tree loss: 1.745 | Accuracy: 0.351562 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 1.758 | Reg loss: 0.026 | Tree loss: 1.758 | Accuracy: 0.292969 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 1.710 | Reg loss: 0.026 | Tree loss: 1.710 | Accuracy: 0.335938 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 1.731 | Reg loss: 0.026 | Tree loss: 1.731 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 1.764 | Reg loss: 0.026 | Tree loss: 1.764 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 1.746 | Reg loss: 0.026 | Tree loss: 1.746 | Accuracy: 0.343750 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 1.720 | Reg loss: 0.026 | Tree loss: 1.720 | Accuracy: 0.277344 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 1.728 | Reg loss: 0.026 | Tree loss: 1.728 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.324219 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 1.720 | Reg loss: 0.026 | Tree loss: 1.720 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.318359 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 1.704 | Reg loss: 0.026 | Tree loss: 1.704 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 1.726 | Reg loss: 0.026 | Tree loss: 1.726 | Accuracy: 0.302734 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 1.706 | Reg loss: 0.026 | Tree loss: 1.706 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 1.702 | Reg loss: 0.026 | Tree loss: 1.702 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 1.691 | Reg loss: 0.026 | Tree loss: 1.691 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 1.757 | Reg loss: 0.026 | Tree loss: 1.757 | Accuracy: 0.263672 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 1.700 | Reg loss: 0.026 | Tree loss: 1.700 | Accuracy: 0.320312 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 1.702 | Reg loss: 0.026 | Tree loss: 1.702 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 1.664 | Reg loss: 0.026 | Tree loss: 1.664 | Accuracy: 0.325911 | 0.239 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 1.826 | Reg loss: 0.026 | Tree loss: 1.826 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 1.785 | Reg loss: 0.026 | Tree loss: 1.785 | Accuracy: 0.300781 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 1.793 | Reg loss: 0.026 | Tree loss: 1.793 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 1.795 | Reg loss: 0.026 | Tree loss: 1.795 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.289062 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 1.753 | Reg loss: 0.026 | Tree loss: 1.753 | Accuracy: 0.347656 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.361328 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 1.738 | Reg loss: 0.026 | Tree loss: 1.738 | Accuracy: 0.308594 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 1.709 | Reg loss: 0.026 | Tree loss: 1.709 | Accuracy: 0.310547 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.341797 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 1.702 | Reg loss: 0.026 | Tree loss: 1.702 | Accuracy: 0.369141 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 1.751 | Reg loss: 0.026 | Tree loss: 1.751 | Accuracy: 0.294922 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 1.760 | Reg loss: 0.026 | Tree loss: 1.760 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 1.762 | Reg loss: 0.026 | Tree loss: 1.762 | Accuracy: 0.275391 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 1.723 | Reg loss: 0.026 | Tree loss: 1.723 | Accuracy: 0.312500 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 1.725 | Reg loss: 0.026 | Tree loss: 1.725 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 1.750 | Reg loss: 0.026 | Tree loss: 1.750 | Accuracy: 0.298828 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 1.749 | Reg loss: 0.026 | Tree loss: 1.749 | Accuracy: 0.306641 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 1.675 | Reg loss: 0.026 | Tree loss: 1.675 | Accuracy: 0.296875 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 1.731 | Reg loss: 0.026 | Tree loss: 1.731 | Accuracy: 0.304688 | 0.239 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 1.726 | Reg loss: 0.026 | Tree loss: 1.726 | Accuracy: 0.279297 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 1.719 | Reg loss: 0.026 | Tree loss: 1.719 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 1.674 | Reg loss: 0.026 | Tree loss: 1.674 | Accuracy: 0.339844 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 1.735 | Reg loss: 0.026 | Tree loss: 1.735 | Accuracy: 0.287109 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 1.682 | Reg loss: 0.026 | Tree loss: 1.682 | Accuracy: 0.330078 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 1.694 | Reg loss: 0.026 | Tree loss: 1.694 | Accuracy: 0.322266 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 1.732 | Reg loss: 0.026 | Tree loss: 1.732 | Accuracy: 0.281250 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 1.698 | Reg loss: 0.026 | Tree loss: 1.698 | Accuracy: 0.314453 | 0.239 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 1.668 | Reg loss: 0.026 | Tree loss: 1.668 | Accuracy: 0.307692 | 0.239 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7696bc4ce344ba78903f63a9c7a4041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987e458ee2b9497094d96a80ba19fd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dfd40cab01492e948bf0e3f56d1855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440f116a59aa4e7380d93a564b31425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.048192771084337\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 83\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "13121\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "Average comprehensibility: 30.771084337349397\n",
      "std comprehensibility: 5.962382682950454\n",
      "var comprehensibility: 35.550007257947456\n",
      "minimum comprehensibility: 18\n",
      "maximum comprehensibility: 40\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
