{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 32\n",
    "tree_depth = 6\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'/mnt/qnap/ekosman/mitbih_train.csv'\n",
    "test_data_path = r'/mnt/qnap/ekosman/mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 4.4404826164245605 | KNN Loss: 3.9451022148132324 | CLS Loss: 0.4953805208206177\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 4.513814926147461 | KNN Loss: 3.9397802352905273 | CLS Loss: 0.5740345120429993\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 4.455218315124512 | KNN Loss: 3.922706365585327 | CLS Loss: 0.5325121283531189\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 4.294148921966553 | KNN Loss: 3.8838322162628174 | CLS Loss: 0.4103168845176697\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 4.294412136077881 | KNN Loss: 3.7883591651916504 | CLS Loss: 0.5060530304908752\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 4.268202304840088 | KNN Loss: 3.8272018432617188 | CLS Loss: 0.4410002529621124\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 4.186133861541748 | KNN Loss: 3.787473440170288 | CLS Loss: 0.39866048097610474\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 4.192091941833496 | KNN Loss: 3.803377866744995 | CLS Loss: 0.38871416449546814\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 4.133146286010742 | KNN Loss: 3.779768705368042 | CLS Loss: 0.35337764024734497\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 4.09360408782959 | KNN Loss: 3.7713685035705566 | CLS Loss: 0.3222358226776123\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 4.103476047515869 | KNN Loss: 3.7801761627197266 | CLS Loss: 0.3233000636100769\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 4.06028413772583 | KNN Loss: 3.7659425735473633 | CLS Loss: 0.29434171319007874\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 4.118402481079102 | KNN Loss: 3.7719626426696777 | CLS Loss: 0.3464396893978119\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 4.114964485168457 | KNN Loss: 3.786250114440918 | CLS Loss: 0.32871460914611816\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 4.088380813598633 | KNN Loss: 3.756133794784546 | CLS Loss: 0.33224692940711975\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 3.9945855140686035 | KNN Loss: 3.731259346008301 | CLS Loss: 0.26332610845565796\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 4.045022010803223 | KNN Loss: 3.7402164936065674 | CLS Loss: 0.3048056662082672\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 4.013031959533691 | KNN Loss: 3.7214622497558594 | CLS Loss: 0.29156970977783203\n",
      "Epoch: 001, Loss: 4.1856, Train: 0.9306, Valid: 0.9294, Best: 0.9294\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 4.038702487945557 | KNN Loss: 3.7674460411071777 | CLS Loss: 0.27125629782676697\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 4.076951503753662 | KNN Loss: 3.7421107292175293 | CLS Loss: 0.33484068512916565\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 3.999112367630005 | KNN Loss: 3.71567702293396 | CLS Loss: 0.2834353744983673\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 3.942164421081543 | KNN Loss: 3.706230640411377 | CLS Loss: 0.23593372106552124\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 4.026676654815674 | KNN Loss: 3.762864112854004 | CLS Loss: 0.26381272077560425\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 3.997796058654785 | KNN Loss: 3.715786933898926 | CLS Loss: 0.2820090353488922\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 3.971510410308838 | KNN Loss: 3.7153327465057373 | CLS Loss: 0.2561775743961334\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 3.9532699584960938 | KNN Loss: 3.7125179767608643 | CLS Loss: 0.24075199663639069\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 3.940197229385376 | KNN Loss: 3.7028119564056396 | CLS Loss: 0.2373853176832199\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 4.041684150695801 | KNN Loss: 3.7179646492004395 | CLS Loss: 0.32371950149536133\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 4.004550457000732 | KNN Loss: 3.731633424758911 | CLS Loss: 0.2729170024394989\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 3.9369659423828125 | KNN Loss: 3.6734251976013184 | CLS Loss: 0.26354077458381653\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 3.8472330570220947 | KNN Loss: 3.657778739929199 | CLS Loss: 0.18945437669754028\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 3.8490030765533447 | KNN Loss: 3.6594161987304688 | CLS Loss: 0.1895867884159088\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 3.9436261653900146 | KNN Loss: 3.6913528442382812 | CLS Loss: 0.2522733211517334\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 3.954785108566284 | KNN Loss: 3.733994960784912 | CLS Loss: 0.22079019248485565\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 3.892028570175171 | KNN Loss: 3.6964657306671143 | CLS Loss: 0.19556280970573425\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 3.871151924133301 | KNN Loss: 3.6796298027038574 | CLS Loss: 0.19152206182479858\n",
      "Epoch: 002, Loss: 3.9484, Train: 0.9428, Valid: 0.9432, Best: 0.9432\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 3.876720666885376 | KNN Loss: 3.67468523979187 | CLS Loss: 0.20203544199466705\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 3.9152212142944336 | KNN Loss: 3.7022511959075928 | CLS Loss: 0.21296991407871246\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 3.899644136428833 | KNN Loss: 3.697580337524414 | CLS Loss: 0.2020639032125473\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 3.970916509628296 | KNN Loss: 3.7414793968200684 | CLS Loss: 0.22943705320358276\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 3.84958815574646 | KNN Loss: 3.652648687362671 | CLS Loss: 0.19693955779075623\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 3.851811647415161 | KNN Loss: 3.653120994567871 | CLS Loss: 0.19869060814380646\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 3.816108226776123 | KNN Loss: 3.6282527446746826 | CLS Loss: 0.18785540759563446\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 3.841869592666626 | KNN Loss: 3.66202974319458 | CLS Loss: 0.17983992397785187\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 3.82251238822937 | KNN Loss: 3.6554272174835205 | CLS Loss: 0.16708523035049438\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 3.8312087059020996 | KNN Loss: 3.656155586242676 | CLS Loss: 0.17505322396755219\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 3.8609540462493896 | KNN Loss: 3.661097764968872 | CLS Loss: 0.19985632598400116\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 3.822465419769287 | KNN Loss: 3.672100305557251 | CLS Loss: 0.15036500990390778\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 3.863219976425171 | KNN Loss: 3.7051711082458496 | CLS Loss: 0.15804894268512726\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 3.8259925842285156 | KNN Loss: 3.6864733695983887 | CLS Loss: 0.13951924443244934\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 3.820751428604126 | KNN Loss: 3.6610193252563477 | CLS Loss: 0.15973211824893951\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 3.8444411754608154 | KNN Loss: 3.6777267456054688 | CLS Loss: 0.16671432554721832\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 3.7796125411987305 | KNN Loss: 3.6623260974884033 | CLS Loss: 0.11728645116090775\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 3.8087000846862793 | KNN Loss: 3.691197633743286 | CLS Loss: 0.11750245839357376\n",
      "Epoch: 003, Loss: 3.8565, Train: 0.9533, Valid: 0.9528, Best: 0.9528\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 3.845776081085205 | KNN Loss: 3.638406991958618 | CLS Loss: 0.2073691487312317\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 3.8124303817749023 | KNN Loss: 3.6283040046691895 | CLS Loss: 0.1841263622045517\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 3.8488364219665527 | KNN Loss: 3.673924446105957 | CLS Loss: 0.17491205036640167\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 3.8167848587036133 | KNN Loss: 3.6472995281219482 | CLS Loss: 0.1694854199886322\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 3.836832284927368 | KNN Loss: 3.69588041305542 | CLS Loss: 0.14095184206962585\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 3.7736411094665527 | KNN Loss: 3.6370391845703125 | CLS Loss: 0.13660185039043427\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 3.833904504776001 | KNN Loss: 3.6445577144622803 | CLS Loss: 0.18934668600559235\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 3.8254897594451904 | KNN Loss: 3.6298747062683105 | CLS Loss: 0.19561496376991272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 3.7460460662841797 | KNN Loss: 3.6439363956451416 | CLS Loss: 0.1021096259355545\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 3.8521170616149902 | KNN Loss: 3.677859306335449 | CLS Loss: 0.17425784468650818\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 3.7913012504577637 | KNN Loss: 3.6585447788238525 | CLS Loss: 0.13275651633739471\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 3.7754223346710205 | KNN Loss: 3.682349681854248 | CLS Loss: 0.09307257831096649\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 3.7439205646514893 | KNN Loss: 3.5736303329467773 | CLS Loss: 0.17029017210006714\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 3.753046751022339 | KNN Loss: 3.6106090545654297 | CLS Loss: 0.14243777096271515\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 3.7714054584503174 | KNN Loss: 3.5768871307373047 | CLS Loss: 0.1945182979106903\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 3.8094074726104736 | KNN Loss: 3.6693241596221924 | CLS Loss: 0.14008338749408722\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 3.7755329608917236 | KNN Loss: 3.6220455169677734 | CLS Loss: 0.15348751842975616\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 3.7789037227630615 | KNN Loss: 3.653719186782837 | CLS Loss: 0.125184565782547\n",
      "Epoch: 004, Loss: 3.8063, Train: 0.9676, Valid: 0.9648, Best: 0.9648\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 3.848031759262085 | KNN Loss: 3.739623785018921 | CLS Loss: 0.10840804874897003\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 3.7708725929260254 | KNN Loss: 3.6240413188934326 | CLS Loss: 0.14683137834072113\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 3.755748987197876 | KNN Loss: 3.5961239337921143 | CLS Loss: 0.1596250981092453\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 3.7257916927337646 | KNN Loss: 3.637136459350586 | CLS Loss: 0.08865512162446976\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 3.764845848083496 | KNN Loss: 3.662389039993286 | CLS Loss: 0.1024567186832428\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 3.7780821323394775 | KNN Loss: 3.66076397895813 | CLS Loss: 0.11731820553541183\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 3.7341864109039307 | KNN Loss: 3.646754264831543 | CLS Loss: 0.08743204921483994\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 3.7724995613098145 | KNN Loss: 3.651864767074585 | CLS Loss: 0.12063482403755188\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 3.809805154800415 | KNN Loss: 3.646871328353882 | CLS Loss: 0.16293393075466156\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 3.8225011825561523 | KNN Loss: 3.661310911178589 | CLS Loss: 0.16119034588336945\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 3.7692008018493652 | KNN Loss: 3.5978212356567383 | CLS Loss: 0.17137961089611053\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 3.7579476833343506 | KNN Loss: 3.647019147872925 | CLS Loss: 0.11092856526374817\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 3.7809982299804688 | KNN Loss: 3.6626198291778564 | CLS Loss: 0.11837846785783768\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 3.7502386569976807 | KNN Loss: 3.667282819747925 | CLS Loss: 0.08295591175556183\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 3.794623851776123 | KNN Loss: 3.6388769149780273 | CLS Loss: 0.1557469218969345\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 3.7310292720794678 | KNN Loss: 3.6320745944976807 | CLS Loss: 0.09895457327365875\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 3.7504231929779053 | KNN Loss: 3.610740900039673 | CLS Loss: 0.13968230783939362\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 3.7892656326293945 | KNN Loss: 3.597266912460327 | CLS Loss: 0.19199861586093903\n",
      "Epoch: 005, Loss: 3.7754, Train: 0.9732, Valid: 0.9707, Best: 0.9707\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 3.7274158000946045 | KNN Loss: 3.631779432296753 | CLS Loss: 0.09563630074262619\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 3.769026041030884 | KNN Loss: 3.660714864730835 | CLS Loss: 0.10831108689308167\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 3.799849510192871 | KNN Loss: 3.675511121749878 | CLS Loss: 0.12433833628892899\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 3.7892954349517822 | KNN Loss: 3.655426502227783 | CLS Loss: 0.13386891782283783\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 3.740203380584717 | KNN Loss: 3.6303484439849854 | CLS Loss: 0.1098548173904419\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 3.7464327812194824 | KNN Loss: 3.6659748554229736 | CLS Loss: 0.08045787364244461\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 3.750758409500122 | KNN Loss: 3.66520619392395 | CLS Loss: 0.08555230498313904\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 3.715120553970337 | KNN Loss: 3.6147758960723877 | CLS Loss: 0.1003446877002716\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 3.795036554336548 | KNN Loss: 3.691601514816284 | CLS Loss: 0.10343492776155472\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 3.7654571533203125 | KNN Loss: 3.6119227409362793 | CLS Loss: 0.15353447198867798\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 3.7185466289520264 | KNN Loss: 3.6506311893463135 | CLS Loss: 0.06791536509990692\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 3.7885398864746094 | KNN Loss: 3.6476027965545654 | CLS Loss: 0.14093704521656036\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 3.7995083332061768 | KNN Loss: 3.6682190895080566 | CLS Loss: 0.1312892735004425\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 3.792008876800537 | KNN Loss: 3.6947896480560303 | CLS Loss: 0.09721933305263519\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 3.7223124504089355 | KNN Loss: 3.5727474689483643 | CLS Loss: 0.1495649218559265\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 3.7615513801574707 | KNN Loss: 3.665569543838501 | CLS Loss: 0.09598187357187271\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 3.7239837646484375 | KNN Loss: 3.6311986446380615 | CLS Loss: 0.09278523176908493\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 3.6922311782836914 | KNN Loss: 3.588982343673706 | CLS Loss: 0.10324893146753311\n",
      "Epoch: 006, Loss: 3.7460, Train: 0.9756, Valid: 0.9719, Best: 0.9719\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 3.768120527267456 | KNN Loss: 3.6970887184143066 | CLS Loss: 0.07103181630373001\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 3.746859550476074 | KNN Loss: 3.6341989040374756 | CLS Loss: 0.1126607283949852\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 3.7654476165771484 | KNN Loss: 3.6683766841888428 | CLS Loss: 0.09707091748714447\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 3.72194504737854 | KNN Loss: 3.6211764812469482 | CLS Loss: 0.10076852887868881\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 3.700648784637451 | KNN Loss: 3.63142991065979 | CLS Loss: 0.06921898573637009\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 3.8383092880249023 | KNN Loss: 3.730559825897217 | CLS Loss: 0.10774952173233032\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 3.7171316146850586 | KNN Loss: 3.6024320125579834 | CLS Loss: 0.11469949781894684\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 3.688260793685913 | KNN Loss: 3.5958492755889893 | CLS Loss: 0.09241149574518204\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 3.685856580734253 | KNN Loss: 3.583463191986084 | CLS Loss: 0.10239329189062119\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 3.6934404373168945 | KNN Loss: 3.6251327991485596 | CLS Loss: 0.06830759346485138\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 3.7189159393310547 | KNN Loss: 3.6274919509887695 | CLS Loss: 0.09142393618822098\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 3.8084118366241455 | KNN Loss: 3.6741139888763428 | CLS Loss: 0.13429774343967438\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 3.7280166149139404 | KNN Loss: 3.652031421661377 | CLS Loss: 0.07598518580198288\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 3.745126724243164 | KNN Loss: 3.6193103790283203 | CLS Loss: 0.12581641972064972\n",
      "Epoch 7 / 200 | iteration 140 / 171 | Total Loss: 3.7455248832702637 | KNN Loss: 3.6248703002929688 | CLS Loss: 0.12065461277961731\n",
      "Epoch 7 / 200 | iteration 150 / 171 | Total Loss: 3.7228121757507324 | KNN Loss: 3.6049654483795166 | CLS Loss: 0.11784677952528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 200 | iteration 160 / 171 | Total Loss: 3.6876981258392334 | KNN Loss: 3.6275582313537598 | CLS Loss: 0.06013982743024826\n",
      "Epoch 7 / 200 | iteration 170 / 171 | Total Loss: 3.697531223297119 | KNN Loss: 3.6259424686431885 | CLS Loss: 0.07158877700567245\n",
      "Epoch: 007, Loss: 3.7277, Train: 0.9772, Valid: 0.9739, Best: 0.9739\n",
      "Epoch 8 / 200 | iteration 0 / 171 | Total Loss: 3.6972131729125977 | KNN Loss: 3.6047801971435547 | CLS Loss: 0.0924328863620758\n",
      "Epoch 8 / 200 | iteration 10 / 171 | Total Loss: 3.7043237686157227 | KNN Loss: 3.6328940391540527 | CLS Loss: 0.07142966985702515\n",
      "Epoch 8 / 200 | iteration 20 / 171 | Total Loss: 3.691478967666626 | KNN Loss: 3.6424553394317627 | CLS Loss: 0.049023743718862534\n",
      "Epoch 8 / 200 | iteration 30 / 171 | Total Loss: 3.7200684547424316 | KNN Loss: 3.6394450664520264 | CLS Loss: 0.08062346279621124\n",
      "Epoch 8 / 200 | iteration 40 / 171 | Total Loss: 3.663825273513794 | KNN Loss: 3.589221715927124 | CLS Loss: 0.07460358738899231\n",
      "Epoch 8 / 200 | iteration 50 / 171 | Total Loss: 3.6970314979553223 | KNN Loss: 3.6268374919891357 | CLS Loss: 0.07019411772489548\n",
      "Epoch 8 / 200 | iteration 60 / 171 | Total Loss: 3.711561679840088 | KNN Loss: 3.6222879886627197 | CLS Loss: 0.08927371352910995\n",
      "Epoch 8 / 200 | iteration 70 / 171 | Total Loss: 3.718851327896118 | KNN Loss: 3.63216495513916 | CLS Loss: 0.08668627589941025\n",
      "Epoch 8 / 200 | iteration 80 / 171 | Total Loss: 3.702362060546875 | KNN Loss: 3.580063819885254 | CLS Loss: 0.12229815125465393\n",
      "Epoch 8 / 200 | iteration 90 / 171 | Total Loss: 3.7351818084716797 | KNN Loss: 3.6457338333129883 | CLS Loss: 0.08944804966449738\n",
      "Epoch 8 / 200 | iteration 100 / 171 | Total Loss: 3.7784159183502197 | KNN Loss: 3.674604892730713 | CLS Loss: 0.10381096601486206\n",
      "Epoch 8 / 200 | iteration 110 / 171 | Total Loss: 3.702692747116089 | KNN Loss: 3.6232199668884277 | CLS Loss: 0.07947275042533875\n",
      "Epoch 8 / 200 | iteration 120 / 171 | Total Loss: 3.7489731311798096 | KNN Loss: 3.6265459060668945 | CLS Loss: 0.12242719531059265\n",
      "Epoch 8 / 200 | iteration 130 / 171 | Total Loss: 3.724731922149658 | KNN Loss: 3.6350595951080322 | CLS Loss: 0.08967241644859314\n",
      "Epoch 8 / 200 | iteration 140 / 171 | Total Loss: 3.6614279747009277 | KNN Loss: 3.5979340076446533 | CLS Loss: 0.06349403411149979\n",
      "Epoch 8 / 200 | iteration 150 / 171 | Total Loss: 3.710524559020996 | KNN Loss: 3.628138780593872 | CLS Loss: 0.08238568902015686\n",
      "Epoch 8 / 200 | iteration 160 / 171 | Total Loss: 3.694969892501831 | KNN Loss: 3.5586981773376465 | CLS Loss: 0.13627171516418457\n",
      "Epoch 8 / 200 | iteration 170 / 171 | Total Loss: 3.706346035003662 | KNN Loss: 3.6112234592437744 | CLS Loss: 0.0951225757598877\n",
      "Epoch: 008, Loss: 3.7201, Train: 0.9790, Valid: 0.9743, Best: 0.9743\n",
      "Epoch 9 / 200 | iteration 0 / 171 | Total Loss: 3.694856882095337 | KNN Loss: 3.6476292610168457 | CLS Loss: 0.04722768813371658\n",
      "Epoch 9 / 200 | iteration 10 / 171 | Total Loss: 3.700887680053711 | KNN Loss: 3.6578733921051025 | CLS Loss: 0.04301425442099571\n",
      "Epoch 9 / 200 | iteration 20 / 171 | Total Loss: 3.6842639446258545 | KNN Loss: 3.6263930797576904 | CLS Loss: 0.05787082016468048\n",
      "Epoch 9 / 200 | iteration 30 / 171 | Total Loss: 3.71639084815979 | KNN Loss: 3.6255321502685547 | CLS Loss: 0.09085863828659058\n",
      "Epoch 9 / 200 | iteration 40 / 171 | Total Loss: 3.725219249725342 | KNN Loss: 3.6315813064575195 | CLS Loss: 0.0936378538608551\n",
      "Epoch 9 / 200 | iteration 50 / 171 | Total Loss: 3.7032642364501953 | KNN Loss: 3.6411800384521484 | CLS Loss: 0.06208417937159538\n",
      "Epoch 9 / 200 | iteration 60 / 171 | Total Loss: 3.7195818424224854 | KNN Loss: 3.613490581512451 | CLS Loss: 0.10609123855829239\n",
      "Epoch 9 / 200 | iteration 70 / 171 | Total Loss: 3.7529592514038086 | KNN Loss: 3.704996109008789 | CLS Loss: 0.047963209450244904\n",
      "Epoch 9 / 200 | iteration 80 / 171 | Total Loss: 3.758826971054077 | KNN Loss: 3.6813836097717285 | CLS Loss: 0.07744336873292923\n",
      "Epoch 9 / 200 | iteration 90 / 171 | Total Loss: 3.7012107372283936 | KNN Loss: 3.6386008262634277 | CLS Loss: 0.06260980665683746\n",
      "Epoch 9 / 200 | iteration 100 / 171 | Total Loss: 3.643249750137329 | KNN Loss: 3.5913851261138916 | CLS Loss: 0.0518646165728569\n",
      "Epoch 9 / 200 | iteration 110 / 171 | Total Loss: 3.732966899871826 | KNN Loss: 3.638369560241699 | CLS Loss: 0.0945972427725792\n",
      "Epoch 9 / 200 | iteration 120 / 171 | Total Loss: 3.712259292602539 | KNN Loss: 3.6123270988464355 | CLS Loss: 0.09993207454681396\n",
      "Epoch 9 / 200 | iteration 130 / 171 | Total Loss: 3.699047565460205 | KNN Loss: 3.649019956588745 | CLS Loss: 0.05002757906913757\n",
      "Epoch 9 / 200 | iteration 140 / 171 | Total Loss: 3.682339906692505 | KNN Loss: 3.6070356369018555 | CLS Loss: 0.07530435919761658\n",
      "Epoch 9 / 200 | iteration 150 / 171 | Total Loss: 3.7234344482421875 | KNN Loss: 3.6632652282714844 | CLS Loss: 0.06016920879483223\n",
      "Epoch 9 / 200 | iteration 160 / 171 | Total Loss: 3.6749722957611084 | KNN Loss: 3.60701060295105 | CLS Loss: 0.06796162575483322\n",
      "Epoch 9 / 200 | iteration 170 / 171 | Total Loss: 3.7074601650238037 | KNN Loss: 3.6184918880462646 | CLS Loss: 0.08896835148334503\n",
      "Epoch: 009, Loss: 3.7160, Train: 0.9796, Valid: 0.9752, Best: 0.9752\n",
      "Epoch 10 / 200 | iteration 0 / 171 | Total Loss: 3.695641279220581 | KNN Loss: 3.5791780948638916 | CLS Loss: 0.11646322160959244\n",
      "Epoch 10 / 200 | iteration 10 / 171 | Total Loss: 3.7525548934936523 | KNN Loss: 3.6689670085906982 | CLS Loss: 0.08358778804540634\n",
      "Epoch 10 / 200 | iteration 20 / 171 | Total Loss: 3.67812442779541 | KNN Loss: 3.6228437423706055 | CLS Loss: 0.05528060346841812\n",
      "Epoch 10 / 200 | iteration 30 / 171 | Total Loss: 3.7398085594177246 | KNN Loss: 3.6363651752471924 | CLS Loss: 0.10344346612691879\n",
      "Epoch 10 / 200 | iteration 40 / 171 | Total Loss: 3.689420700073242 | KNN Loss: 3.597348690032959 | CLS Loss: 0.09207203984260559\n",
      "Epoch 10 / 200 | iteration 50 / 171 | Total Loss: 3.6719987392425537 | KNN Loss: 3.62797474861145 | CLS Loss: 0.044024087488651276\n",
      "Epoch 10 / 200 | iteration 60 / 171 | Total Loss: 3.6880829334259033 | KNN Loss: 3.619389533996582 | CLS Loss: 0.0686933621764183\n",
      "Epoch 10 / 200 | iteration 70 / 171 | Total Loss: 3.7086079120635986 | KNN Loss: 3.6123886108398438 | CLS Loss: 0.09621939808130264\n",
      "Epoch 10 / 200 | iteration 80 / 171 | Total Loss: 3.752469539642334 | KNN Loss: 3.682403087615967 | CLS Loss: 0.07006639987230301\n",
      "Epoch 10 / 200 | iteration 90 / 171 | Total Loss: 3.692075252532959 | KNN Loss: 3.605299234390259 | CLS Loss: 0.08677604049444199\n",
      "Epoch 10 / 200 | iteration 100 / 171 | Total Loss: 3.7285115718841553 | KNN Loss: 3.6224400997161865 | CLS Loss: 0.10607138276100159\n",
      "Epoch 10 / 200 | iteration 110 / 171 | Total Loss: 3.64977765083313 | KNN Loss: 3.5897867679595947 | CLS Loss: 0.05999094992876053\n",
      "Epoch 10 / 200 | iteration 120 / 171 | Total Loss: 3.6892945766448975 | KNN Loss: 3.6277854442596436 | CLS Loss: 0.06150919198989868\n",
      "Epoch 10 / 200 | iteration 130 / 171 | Total Loss: 3.691838264465332 | KNN Loss: 3.6205756664276123 | CLS Loss: 0.0712626725435257\n",
      "Epoch 10 / 200 | iteration 140 / 171 | Total Loss: 3.688406229019165 | KNN Loss: 3.59434175491333 | CLS Loss: 0.09406436234712601\n",
      "Epoch 10 / 200 | iteration 150 / 171 | Total Loss: 3.668193817138672 | KNN Loss: 3.6060359477996826 | CLS Loss: 0.06215794011950493\n",
      "Epoch 10 / 200 | iteration 160 / 171 | Total Loss: 3.7380056381225586 | KNN Loss: 3.6456148624420166 | CLS Loss: 0.09239073842763901\n",
      "Epoch 10 / 200 | iteration 170 / 171 | Total Loss: 3.7560513019561768 | KNN Loss: 3.6550402641296387 | CLS Loss: 0.10101114958524704\n",
      "Epoch: 010, Loss: 3.7009, Train: 0.9794, Valid: 0.9761, Best: 0.9761\n",
      "Epoch 11 / 200 | iteration 0 / 171 | Total Loss: 3.720903158187866 | KNN Loss: 3.6332905292510986 | CLS Loss: 0.08761254698038101\n",
      "Epoch 11 / 200 | iteration 10 / 171 | Total Loss: 3.6951470375061035 | KNN Loss: 3.5937366485595703 | CLS Loss: 0.1014103814959526\n",
      "Epoch 11 / 200 | iteration 20 / 171 | Total Loss: 3.6904051303863525 | KNN Loss: 3.6274895668029785 | CLS Loss: 0.06291545182466507\n",
      "Epoch 11 / 200 | iteration 30 / 171 | Total Loss: 3.678569793701172 | KNN Loss: 3.629302501678467 | CLS Loss: 0.04926726222038269\n",
      "Epoch 11 / 200 | iteration 40 / 171 | Total Loss: 3.6677722930908203 | KNN Loss: 3.6082592010498047 | CLS Loss: 0.05951320752501488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 200 | iteration 50 / 171 | Total Loss: 3.7020018100738525 | KNN Loss: 3.611156940460205 | CLS Loss: 0.09084483981132507\n",
      "Epoch 11 / 200 | iteration 60 / 171 | Total Loss: 3.6834349632263184 | KNN Loss: 3.5963950157165527 | CLS Loss: 0.08704005926847458\n",
      "Epoch 11 / 200 | iteration 70 / 171 | Total Loss: 3.6913721561431885 | KNN Loss: 3.604405164718628 | CLS Loss: 0.08696708083152771\n",
      "Epoch 11 / 200 | iteration 80 / 171 | Total Loss: 3.6948282718658447 | KNN Loss: 3.6119637489318848 | CLS Loss: 0.08286451548337936\n",
      "Epoch 11 / 200 | iteration 90 / 171 | Total Loss: 3.737065315246582 | KNN Loss: 3.6354610919952393 | CLS Loss: 0.10160432755947113\n",
      "Epoch 11 / 200 | iteration 100 / 171 | Total Loss: 3.7306935787200928 | KNN Loss: 3.6641175746917725 | CLS Loss: 0.0665760412812233\n",
      "Epoch 11 / 200 | iteration 110 / 171 | Total Loss: 3.6791985034942627 | KNN Loss: 3.5801010131835938 | CLS Loss: 0.09909740090370178\n",
      "Epoch 11 / 200 | iteration 120 / 171 | Total Loss: 3.7615909576416016 | KNN Loss: 3.6490638256073 | CLS Loss: 0.11252718418836594\n",
      "Epoch 11 / 200 | iteration 130 / 171 | Total Loss: 3.723694324493408 | KNN Loss: 3.584810256958008 | CLS Loss: 0.13888411223888397\n",
      "Epoch 11 / 200 | iteration 140 / 171 | Total Loss: 3.717756509780884 | KNN Loss: 3.647542715072632 | CLS Loss: 0.07021387666463852\n",
      "Epoch 11 / 200 | iteration 150 / 171 | Total Loss: 3.690612554550171 | KNN Loss: 3.600071430206299 | CLS Loss: 0.0905410647392273\n",
      "Epoch 11 / 200 | iteration 160 / 171 | Total Loss: 3.6946301460266113 | KNN Loss: 3.623936176300049 | CLS Loss: 0.07069408148527145\n",
      "Epoch 11 / 200 | iteration 170 / 171 | Total Loss: 3.6421072483062744 | KNN Loss: 3.602019786834717 | CLS Loss: 0.04008737951517105\n",
      "Epoch: 011, Loss: 3.6999, Train: 0.9811, Valid: 0.9763, Best: 0.9763\n",
      "Epoch 12 / 200 | iteration 0 / 171 | Total Loss: 3.6884477138519287 | KNN Loss: 3.6251060962677 | CLS Loss: 0.06334154307842255\n",
      "Epoch 12 / 200 | iteration 10 / 171 | Total Loss: 3.6777777671813965 | KNN Loss: 3.640026569366455 | CLS Loss: 0.03775116056203842\n",
      "Epoch 12 / 200 | iteration 20 / 171 | Total Loss: 3.662853240966797 | KNN Loss: 3.6247217655181885 | CLS Loss: 0.03813139349222183\n",
      "Epoch 12 / 200 | iteration 30 / 171 | Total Loss: 3.680723190307617 | KNN Loss: 3.6093838214874268 | CLS Loss: 0.07133939117193222\n",
      "Epoch 12 / 200 | iteration 40 / 171 | Total Loss: 3.6734395027160645 | KNN Loss: 3.6035940647125244 | CLS Loss: 0.0698455348610878\n",
      "Epoch 12 / 200 | iteration 50 / 171 | Total Loss: 3.736315965652466 | KNN Loss: 3.660456418991089 | CLS Loss: 0.07585962116718292\n",
      "Epoch 12 / 200 | iteration 60 / 171 | Total Loss: 3.6979551315307617 | KNN Loss: 3.578779697418213 | CLS Loss: 0.11917555332183838\n",
      "Epoch 12 / 200 | iteration 70 / 171 | Total Loss: 3.6439754962921143 | KNN Loss: 3.554595708847046 | CLS Loss: 0.08937980234622955\n",
      "Epoch 12 / 200 | iteration 80 / 171 | Total Loss: 3.669816017150879 | KNN Loss: 3.622474431991577 | CLS Loss: 0.047341544181108475\n",
      "Epoch 12 / 200 | iteration 90 / 171 | Total Loss: 3.6994543075561523 | KNN Loss: 3.632429599761963 | CLS Loss: 0.06702481210231781\n",
      "Epoch 12 / 200 | iteration 100 / 171 | Total Loss: 3.6769027709960938 | KNN Loss: 3.6046605110168457 | CLS Loss: 0.07224231958389282\n",
      "Epoch 12 / 200 | iteration 110 / 171 | Total Loss: 3.712409257888794 | KNN Loss: 3.618098735809326 | CLS Loss: 0.09431061148643494\n",
      "Epoch 12 / 200 | iteration 120 / 171 | Total Loss: 3.654550075531006 | KNN Loss: 3.5828893184661865 | CLS Loss: 0.0716608539223671\n",
      "Epoch 12 / 200 | iteration 130 / 171 | Total Loss: 3.6882572174072266 | KNN Loss: 3.610642910003662 | CLS Loss: 0.07761424779891968\n",
      "Epoch 12 / 200 | iteration 140 / 171 | Total Loss: 3.6986448764801025 | KNN Loss: 3.6452174186706543 | CLS Loss: 0.053427353501319885\n",
      "Epoch 12 / 200 | iteration 150 / 171 | Total Loss: 3.6634743213653564 | KNN Loss: 3.593806028366089 | CLS Loss: 0.06966828554868698\n",
      "Epoch 12 / 200 | iteration 160 / 171 | Total Loss: 3.7090706825256348 | KNN Loss: 3.647305488586426 | CLS Loss: 0.06176530942320824\n",
      "Epoch 12 / 200 | iteration 170 / 171 | Total Loss: 3.659210681915283 | KNN Loss: 3.5883350372314453 | CLS Loss: 0.07087565213441849\n",
      "Epoch: 012, Loss: 3.6856, Train: 0.9842, Valid: 0.9798, Best: 0.9798\n",
      "Epoch 13 / 200 | iteration 0 / 171 | Total Loss: 3.666724443435669 | KNN Loss: 3.6187853813171387 | CLS Loss: 0.0479389950633049\n",
      "Epoch 13 / 200 | iteration 10 / 171 | Total Loss: 3.6457979679107666 | KNN Loss: 3.578339099884033 | CLS Loss: 0.06745896488428116\n",
      "Epoch 13 / 200 | iteration 20 / 171 | Total Loss: 3.692216157913208 | KNN Loss: 3.6379261016845703 | CLS Loss: 0.05429014936089516\n",
      "Epoch 13 / 200 | iteration 30 / 171 | Total Loss: 3.6424598693847656 | KNN Loss: 3.5709691047668457 | CLS Loss: 0.07149066030979156\n",
      "Epoch 13 / 200 | iteration 40 / 171 | Total Loss: 3.717623233795166 | KNN Loss: 3.6722331047058105 | CLS Loss: 0.04539002478122711\n",
      "Epoch 13 / 200 | iteration 50 / 171 | Total Loss: 3.6457135677337646 | KNN Loss: 3.5953118801116943 | CLS Loss: 0.050401777029037476\n",
      "Epoch 13 / 200 | iteration 60 / 171 | Total Loss: 3.6976735591888428 | KNN Loss: 3.615216016769409 | CLS Loss: 0.0824575275182724\n",
      "Epoch 13 / 200 | iteration 70 / 171 | Total Loss: 3.70004940032959 | KNN Loss: 3.617112874984741 | CLS Loss: 0.08293649554252625\n",
      "Epoch 13 / 200 | iteration 80 / 171 | Total Loss: 3.66269850730896 | KNN Loss: 3.598694086074829 | CLS Loss: 0.06400448083877563\n",
      "Epoch 13 / 200 | iteration 90 / 171 | Total Loss: 3.6525566577911377 | KNN Loss: 3.610394239425659 | CLS Loss: 0.0421624481678009\n",
      "Epoch 13 / 200 | iteration 100 / 171 | Total Loss: 3.658294677734375 | KNN Loss: 3.618187189102173 | CLS Loss: 0.04010743275284767\n",
      "Epoch 13 / 200 | iteration 110 / 171 | Total Loss: 3.684295177459717 | KNN Loss: 3.5883078575134277 | CLS Loss: 0.09598729759454727\n",
      "Epoch 13 / 200 | iteration 120 / 171 | Total Loss: 3.6652889251708984 | KNN Loss: 3.5868093967437744 | CLS Loss: 0.07847952842712402\n",
      "Epoch 13 / 200 | iteration 130 / 171 | Total Loss: 3.6783742904663086 | KNN Loss: 3.6242520809173584 | CLS Loss: 0.05412217229604721\n",
      "Epoch 13 / 200 | iteration 140 / 171 | Total Loss: 3.6935644149780273 | KNN Loss: 3.623973846435547 | CLS Loss: 0.06959068030118942\n",
      "Epoch 13 / 200 | iteration 150 / 171 | Total Loss: 3.6699676513671875 | KNN Loss: 3.630788803100586 | CLS Loss: 0.039178743958473206\n",
      "Epoch 13 / 200 | iteration 160 / 171 | Total Loss: 3.6474742889404297 | KNN Loss: 3.5862879753112793 | CLS Loss: 0.06118641793727875\n",
      "Epoch 13 / 200 | iteration 170 / 171 | Total Loss: 3.7090392112731934 | KNN Loss: 3.6453142166137695 | CLS Loss: 0.063725046813488\n",
      "Epoch: 013, Loss: 3.6837, Train: 0.9832, Valid: 0.9784, Best: 0.9798\n",
      "Epoch 14 / 200 | iteration 0 / 171 | Total Loss: 3.6897828578948975 | KNN Loss: 3.6280088424682617 | CLS Loss: 0.061773963272571564\n",
      "Epoch 14 / 200 | iteration 10 / 171 | Total Loss: 3.7477526664733887 | KNN Loss: 3.6715521812438965 | CLS Loss: 0.07620038837194443\n",
      "Epoch 14 / 200 | iteration 20 / 171 | Total Loss: 3.625476121902466 | KNN Loss: 3.5709476470947266 | CLS Loss: 0.05452854931354523\n",
      "Epoch 14 / 200 | iteration 30 / 171 | Total Loss: 3.7042288780212402 | KNN Loss: 3.6292335987091064 | CLS Loss: 0.07499538362026215\n",
      "Epoch 14 / 200 | iteration 40 / 171 | Total Loss: 3.6730635166168213 | KNN Loss: 3.602818489074707 | CLS Loss: 0.07024504244327545\n",
      "Epoch 14 / 200 | iteration 50 / 171 | Total Loss: 3.6955411434173584 | KNN Loss: 3.581010341644287 | CLS Loss: 0.11453071981668472\n",
      "Epoch 14 / 200 | iteration 60 / 171 | Total Loss: 3.686208963394165 | KNN Loss: 3.627476453781128 | CLS Loss: 0.05873245373368263\n",
      "Epoch 14 / 200 | iteration 70 / 171 | Total Loss: 3.7102599143981934 | KNN Loss: 3.6263608932495117 | CLS Loss: 0.08389899134635925\n",
      "Epoch 14 / 200 | iteration 80 / 171 | Total Loss: 3.7284531593322754 | KNN Loss: 3.674717426300049 | CLS Loss: 0.05373571440577507\n",
      "Epoch 14 / 200 | iteration 90 / 171 | Total Loss: 3.6906850337982178 | KNN Loss: 3.6563220024108887 | CLS Loss: 0.03436313197016716\n",
      "Epoch 14 / 200 | iteration 100 / 171 | Total Loss: 3.6936025619506836 | KNN Loss: 3.6390373706817627 | CLS Loss: 0.0545651949942112\n",
      "Epoch 14 / 200 | iteration 110 / 171 | Total Loss: 3.63975191116333 | KNN Loss: 3.619982957839966 | CLS Loss: 0.019768932834267616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 200 | iteration 120 / 171 | Total Loss: 3.7090442180633545 | KNN Loss: 3.6246423721313477 | CLS Loss: 0.0844019427895546\n",
      "Epoch 14 / 200 | iteration 130 / 171 | Total Loss: 3.6484458446502686 | KNN Loss: 3.5808098316192627 | CLS Loss: 0.06763608008623123\n",
      "Epoch 14 / 200 | iteration 140 / 171 | Total Loss: 3.7904977798461914 | KNN Loss: 3.726064443588257 | CLS Loss: 0.06443341821432114\n",
      "Epoch 14 / 200 | iteration 150 / 171 | Total Loss: 3.6699464321136475 | KNN Loss: 3.6021406650543213 | CLS Loss: 0.06780566275119781\n",
      "Epoch 14 / 200 | iteration 160 / 171 | Total Loss: 3.6888301372528076 | KNN Loss: 3.6273884773254395 | CLS Loss: 0.061441775411367416\n",
      "Epoch 14 / 200 | iteration 170 / 171 | Total Loss: 3.653918743133545 | KNN Loss: 3.6003856658935547 | CLS Loss: 0.05353296548128128\n",
      "Epoch: 014, Loss: 3.6835, Train: 0.9857, Valid: 0.9805, Best: 0.9805\n",
      "Epoch 15 / 200 | iteration 0 / 171 | Total Loss: 3.6623129844665527 | KNN Loss: 3.624149799346924 | CLS Loss: 0.03816324472427368\n",
      "Epoch 15 / 200 | iteration 10 / 171 | Total Loss: 3.647780656814575 | KNN Loss: 3.6054534912109375 | CLS Loss: 0.04232708737254143\n",
      "Epoch 15 / 200 | iteration 20 / 171 | Total Loss: 3.67909574508667 | KNN Loss: 3.586613655090332 | CLS Loss: 0.09248211979866028\n",
      "Epoch 15 / 200 | iteration 30 / 171 | Total Loss: 3.681647539138794 | KNN Loss: 3.6175756454467773 | CLS Loss: 0.06407195329666138\n",
      "Epoch 15 / 200 | iteration 40 / 171 | Total Loss: 3.644451141357422 | KNN Loss: 3.5896520614624023 | CLS Loss: 0.05479913204908371\n",
      "Epoch 15 / 200 | iteration 50 / 171 | Total Loss: 3.7047219276428223 | KNN Loss: 3.6274707317352295 | CLS Loss: 0.0772511288523674\n",
      "Epoch 15 / 200 | iteration 60 / 171 | Total Loss: 3.6911940574645996 | KNN Loss: 3.618966579437256 | CLS Loss: 0.07222749292850494\n",
      "Epoch 15 / 200 | iteration 70 / 171 | Total Loss: 3.711167335510254 | KNN Loss: 3.6304049491882324 | CLS Loss: 0.08076227456331253\n",
      "Epoch 15 / 200 | iteration 80 / 171 | Total Loss: 3.6803793907165527 | KNN Loss: 3.6212527751922607 | CLS Loss: 0.05912657454609871\n",
      "Epoch 15 / 200 | iteration 90 / 171 | Total Loss: 3.626021385192871 | KNN Loss: 3.5913987159729004 | CLS Loss: 0.03462257608771324\n",
      "Epoch 15 / 200 | iteration 100 / 171 | Total Loss: 3.633253574371338 | KNN Loss: 3.5749974250793457 | CLS Loss: 0.05825619027018547\n",
      "Epoch 15 / 200 | iteration 110 / 171 | Total Loss: 3.657658100128174 | KNN Loss: 3.6023736000061035 | CLS Loss: 0.055284447968006134\n",
      "Epoch 15 / 200 | iteration 120 / 171 | Total Loss: 3.7030208110809326 | KNN Loss: 3.6534526348114014 | CLS Loss: 0.04956820607185364\n",
      "Epoch 15 / 200 | iteration 130 / 171 | Total Loss: 3.6645514965057373 | KNN Loss: 3.6131858825683594 | CLS Loss: 0.051365673542022705\n",
      "Epoch 15 / 200 | iteration 140 / 171 | Total Loss: 3.74703311920166 | KNN Loss: 3.642327070236206 | CLS Loss: 0.10470603406429291\n",
      "Epoch 15 / 200 | iteration 150 / 171 | Total Loss: 3.7046453952789307 | KNN Loss: 3.634430408477783 | CLS Loss: 0.07021494954824448\n",
      "Epoch 15 / 200 | iteration 160 / 171 | Total Loss: 3.6737804412841797 | KNN Loss: 3.6187751293182373 | CLS Loss: 0.05500536412000656\n",
      "Epoch 15 / 200 | iteration 170 / 171 | Total Loss: 3.6456947326660156 | KNN Loss: 3.593050718307495 | CLS Loss: 0.0526440367102623\n",
      "Epoch: 015, Loss: 3.6768, Train: 0.9847, Valid: 0.9809, Best: 0.9809\n",
      "Epoch 16 / 200 | iteration 0 / 171 | Total Loss: 3.700758934020996 | KNN Loss: 3.6398966312408447 | CLS Loss: 0.060862261801958084\n",
      "Epoch 16 / 200 | iteration 10 / 171 | Total Loss: 3.6299736499786377 | KNN Loss: 3.5922319889068604 | CLS Loss: 0.037741683423519135\n",
      "Epoch 16 / 200 | iteration 20 / 171 | Total Loss: 3.644571542739868 | KNN Loss: 3.5902860164642334 | CLS Loss: 0.05428563058376312\n",
      "Epoch 16 / 200 | iteration 30 / 171 | Total Loss: 3.6856298446655273 | KNN Loss: 3.6157145500183105 | CLS Loss: 0.0699153020977974\n",
      "Epoch 16 / 200 | iteration 40 / 171 | Total Loss: 3.6496734619140625 | KNN Loss: 3.5893125534057617 | CLS Loss: 0.0603608675301075\n",
      "Epoch 16 / 200 | iteration 50 / 171 | Total Loss: 3.655029296875 | KNN Loss: 3.5972676277160645 | CLS Loss: 0.057761769741773605\n",
      "Epoch 16 / 200 | iteration 60 / 171 | Total Loss: 3.6350090503692627 | KNN Loss: 3.5501627922058105 | CLS Loss: 0.08484624326229095\n",
      "Epoch 16 / 200 | iteration 70 / 171 | Total Loss: 3.7083864212036133 | KNN Loss: 3.6492271423339844 | CLS Loss: 0.05915925279259682\n",
      "Epoch 16 / 200 | iteration 80 / 171 | Total Loss: 3.6431775093078613 | KNN Loss: 3.615320920944214 | CLS Loss: 0.027856601402163506\n",
      "Epoch 16 / 200 | iteration 90 / 171 | Total Loss: 3.7103395462036133 | KNN Loss: 3.632953405380249 | CLS Loss: 0.07738622277975082\n",
      "Epoch 16 / 200 | iteration 100 / 171 | Total Loss: 3.6408467292785645 | KNN Loss: 3.618147134780884 | CLS Loss: 0.022699544206261635\n",
      "Epoch 16 / 200 | iteration 110 / 171 | Total Loss: 3.6659815311431885 | KNN Loss: 3.6155545711517334 | CLS Loss: 0.05042685940861702\n",
      "Epoch 16 / 200 | iteration 120 / 171 | Total Loss: 3.655521869659424 | KNN Loss: 3.6031181812286377 | CLS Loss: 0.052403610199689865\n",
      "Epoch 16 / 200 | iteration 130 / 171 | Total Loss: 3.6838948726654053 | KNN Loss: 3.615964651107788 | CLS Loss: 0.06793011724948883\n",
      "Epoch 16 / 200 | iteration 140 / 171 | Total Loss: 3.7161171436309814 | KNN Loss: 3.6489980220794678 | CLS Loss: 0.06711922585964203\n",
      "Epoch 16 / 200 | iteration 150 / 171 | Total Loss: 3.6851725578308105 | KNN Loss: 3.6039388179779053 | CLS Loss: 0.08123385906219482\n",
      "Epoch 16 / 200 | iteration 160 / 171 | Total Loss: 3.680013656616211 | KNN Loss: 3.61611008644104 | CLS Loss: 0.06390368938446045\n",
      "Epoch 16 / 200 | iteration 170 / 171 | Total Loss: 3.6797001361846924 | KNN Loss: 3.651792049407959 | CLS Loss: 0.02790803648531437\n",
      "Epoch: 016, Loss: 3.6698, Train: 0.9844, Valid: 0.9784, Best: 0.9809\n",
      "Epoch 17 / 200 | iteration 0 / 171 | Total Loss: 3.691143274307251 | KNN Loss: 3.5775489807128906 | CLS Loss: 0.11359433084726334\n",
      "Epoch 17 / 200 | iteration 10 / 171 | Total Loss: 3.6488003730773926 | KNN Loss: 3.5569069385528564 | CLS Loss: 0.09189344197511673\n",
      "Epoch 17 / 200 | iteration 20 / 171 | Total Loss: 3.69643235206604 | KNN Loss: 3.647571563720703 | CLS Loss: 0.04886076599359512\n",
      "Epoch 17 / 200 | iteration 30 / 171 | Total Loss: 3.663320302963257 | KNN Loss: 3.5889854431152344 | CLS Loss: 0.0743347704410553\n",
      "Epoch 17 / 200 | iteration 40 / 171 | Total Loss: 3.7510263919830322 | KNN Loss: 3.626396894454956 | CLS Loss: 0.12462949752807617\n",
      "Epoch 17 / 200 | iteration 50 / 171 | Total Loss: 3.683885335922241 | KNN Loss: 3.6378297805786133 | CLS Loss: 0.046055566519498825\n",
      "Epoch 17 / 200 | iteration 60 / 171 | Total Loss: 3.6221811771392822 | KNN Loss: 3.5810632705688477 | CLS Loss: 0.041117917746305466\n",
      "Epoch 17 / 200 | iteration 70 / 171 | Total Loss: 3.647771120071411 | KNN Loss: 3.58072829246521 | CLS Loss: 0.06704273074865341\n",
      "Epoch 17 / 200 | iteration 80 / 171 | Total Loss: 3.745408296585083 | KNN Loss: 3.654047727584839 | CLS Loss: 0.09136048704385757\n",
      "Epoch 17 / 200 | iteration 90 / 171 | Total Loss: 3.6800615787506104 | KNN Loss: 3.654811143875122 | CLS Loss: 0.02525036782026291\n",
      "Epoch 17 / 200 | iteration 100 / 171 | Total Loss: 3.658235788345337 | KNN Loss: 3.6183526515960693 | CLS Loss: 0.03988317772746086\n",
      "Epoch 17 / 200 | iteration 110 / 171 | Total Loss: 3.6376588344573975 | KNN Loss: 3.576101779937744 | CLS Loss: 0.06155708059668541\n",
      "Epoch 17 / 200 | iteration 120 / 171 | Total Loss: 3.6329269409179688 | KNN Loss: 3.5819976329803467 | CLS Loss: 0.05092928931117058\n",
      "Epoch 17 / 200 | iteration 130 / 171 | Total Loss: 3.670065402984619 | KNN Loss: 3.6248114109039307 | CLS Loss: 0.045253898948431015\n",
      "Epoch 17 / 200 | iteration 140 / 171 | Total Loss: 3.681323528289795 | KNN Loss: 3.63352632522583 | CLS Loss: 0.047797176986932755\n",
      "Epoch 17 / 200 | iteration 150 / 171 | Total Loss: 3.751315116882324 | KNN Loss: 3.7167534828186035 | CLS Loss: 0.034561701118946075\n",
      "Epoch 17 / 200 | iteration 160 / 171 | Total Loss: 3.7416183948516846 | KNN Loss: 3.6686997413635254 | CLS Loss: 0.07291866838932037\n",
      "Epoch 17 / 200 | iteration 170 / 171 | Total Loss: 3.6516993045806885 | KNN Loss: 3.5660862922668457 | CLS Loss: 0.08561302721500397\n",
      "Epoch: 017, Loss: 3.6637, Train: 0.9857, Valid: 0.9810, Best: 0.9810\n",
      "Epoch 18 / 200 | iteration 0 / 171 | Total Loss: 3.6911065578460693 | KNN Loss: 3.632408380508423 | CLS Loss: 0.05869827792048454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 200 | iteration 10 / 171 | Total Loss: 3.686664342880249 | KNN Loss: 3.6441001892089844 | CLS Loss: 0.042564064264297485\n",
      "Epoch 18 / 200 | iteration 20 / 171 | Total Loss: 3.703777313232422 | KNN Loss: 3.6534957885742188 | CLS Loss: 0.050281405448913574\n",
      "Epoch 18 / 200 | iteration 30 / 171 | Total Loss: 3.650564193725586 | KNN Loss: 3.5808115005493164 | CLS Loss: 0.06975264847278595\n",
      "Epoch 18 / 200 | iteration 40 / 171 | Total Loss: 3.637263774871826 | KNN Loss: 3.5902106761932373 | CLS Loss: 0.04705320671200752\n",
      "Epoch 18 / 200 | iteration 50 / 171 | Total Loss: 3.6588711738586426 | KNN Loss: 3.620802640914917 | CLS Loss: 0.03806859627366066\n",
      "Epoch 18 / 200 | iteration 60 / 171 | Total Loss: 3.666327714920044 | KNN Loss: 3.6336326599121094 | CLS Loss: 0.0326949767768383\n",
      "Epoch 18 / 200 | iteration 70 / 171 | Total Loss: 3.6621155738830566 | KNN Loss: 3.610682249069214 | CLS Loss: 0.05143332481384277\n",
      "Epoch 18 / 200 | iteration 80 / 171 | Total Loss: 3.6864466667175293 | KNN Loss: 3.6320040225982666 | CLS Loss: 0.05444257706403732\n",
      "Epoch 18 / 200 | iteration 90 / 171 | Total Loss: 3.6647191047668457 | KNN Loss: 3.6198761463165283 | CLS Loss: 0.04484289139509201\n",
      "Epoch 18 / 200 | iteration 100 / 171 | Total Loss: 3.686187505722046 | KNN Loss: 3.637176036834717 | CLS Loss: 0.04901135712862015\n",
      "Epoch 18 / 200 | iteration 110 / 171 | Total Loss: 3.6496968269348145 | KNN Loss: 3.5899972915649414 | CLS Loss: 0.05969960615038872\n",
      "Epoch 18 / 200 | iteration 120 / 171 | Total Loss: 3.680600881576538 | KNN Loss: 3.6443912982940674 | CLS Loss: 0.036209601908922195\n",
      "Epoch 18 / 200 | iteration 130 / 171 | Total Loss: 3.6515166759490967 | KNN Loss: 3.5742225646972656 | CLS Loss: 0.07729411125183105\n",
      "Epoch 18 / 200 | iteration 140 / 171 | Total Loss: 3.7441720962524414 | KNN Loss: 3.672988176345825 | CLS Loss: 0.07118388265371323\n",
      "Epoch 18 / 200 | iteration 150 / 171 | Total Loss: 3.658263683319092 | KNN Loss: 3.585951566696167 | CLS Loss: 0.07231199741363525\n",
      "Epoch 18 / 200 | iteration 160 / 171 | Total Loss: 3.6609885692596436 | KNN Loss: 3.6046221256256104 | CLS Loss: 0.05636649951338768\n",
      "Epoch 18 / 200 | iteration 170 / 171 | Total Loss: 3.689100980758667 | KNN Loss: 3.598329544067383 | CLS Loss: 0.09077144414186478\n",
      "Epoch: 018, Loss: 3.6645, Train: 0.9848, Valid: 0.9807, Best: 0.9810\n",
      "Epoch 19 / 200 | iteration 0 / 171 | Total Loss: 3.6241307258605957 | KNN Loss: 3.5995306968688965 | CLS Loss: 0.0246000774204731\n",
      "Epoch 19 / 200 | iteration 10 / 171 | Total Loss: 3.718726873397827 | KNN Loss: 3.647047996520996 | CLS Loss: 0.07167883962392807\n",
      "Epoch 19 / 200 | iteration 20 / 171 | Total Loss: 3.6362388134002686 | KNN Loss: 3.599196195602417 | CLS Loss: 0.03704269230365753\n",
      "Epoch 19 / 200 | iteration 30 / 171 | Total Loss: 3.640582323074341 | KNN Loss: 3.617177724838257 | CLS Loss: 0.023404715582728386\n",
      "Epoch 19 / 200 | iteration 40 / 171 | Total Loss: 3.6361539363861084 | KNN Loss: 3.579914093017578 | CLS Loss: 0.05623980611562729\n",
      "Epoch 19 / 200 | iteration 50 / 171 | Total Loss: 3.66477632522583 | KNN Loss: 3.6201083660125732 | CLS Loss: 0.044667914509773254\n",
      "Epoch 19 / 200 | iteration 60 / 171 | Total Loss: 3.6477298736572266 | KNN Loss: 3.5730695724487305 | CLS Loss: 0.07466036081314087\n",
      "Epoch 19 / 200 | iteration 70 / 171 | Total Loss: 3.6362292766571045 | KNN Loss: 3.587820529937744 | CLS Loss: 0.048408668488264084\n",
      "Epoch 19 / 200 | iteration 80 / 171 | Total Loss: 3.6272144317626953 | KNN Loss: 3.5892508029937744 | CLS Loss: 0.03796360269188881\n",
      "Epoch 19 / 200 | iteration 90 / 171 | Total Loss: 3.6280550956726074 | KNN Loss: 3.570012331008911 | CLS Loss: 0.05804288387298584\n",
      "Epoch 19 / 200 | iteration 100 / 171 | Total Loss: 3.6382224559783936 | KNN Loss: 3.6018521785736084 | CLS Loss: 0.036370325833559036\n",
      "Epoch 19 / 200 | iteration 110 / 171 | Total Loss: 3.6724298000335693 | KNN Loss: 3.620899200439453 | CLS Loss: 0.051530610769987106\n",
      "Epoch 19 / 200 | iteration 120 / 171 | Total Loss: 3.6667964458465576 | KNN Loss: 3.6205294132232666 | CLS Loss: 0.046267133206129074\n",
      "Epoch 19 / 200 | iteration 130 / 171 | Total Loss: 3.7009811401367188 | KNN Loss: 3.638472318649292 | CLS Loss: 0.06250891089439392\n",
      "Epoch 19 / 200 | iteration 140 / 171 | Total Loss: 3.674774408340454 | KNN Loss: 3.6265358924865723 | CLS Loss: 0.04823857918381691\n",
      "Epoch 19 / 200 | iteration 150 / 171 | Total Loss: 3.638066291809082 | KNN Loss: 3.6055452823638916 | CLS Loss: 0.03252100571990013\n",
      "Epoch 19 / 200 | iteration 160 / 171 | Total Loss: 3.674692153930664 | KNN Loss: 3.6107518672943115 | CLS Loss: 0.06394036114215851\n",
      "Epoch 19 / 200 | iteration 170 / 171 | Total Loss: 3.6753313541412354 | KNN Loss: 3.622069835662842 | CLS Loss: 0.053261514753103256\n",
      "Epoch: 019, Loss: 3.6593, Train: 0.9836, Valid: 0.9799, Best: 0.9810\n",
      "Epoch 20 / 200 | iteration 0 / 171 | Total Loss: 3.6454756259918213 | KNN Loss: 3.5880887508392334 | CLS Loss: 0.05738694965839386\n",
      "Epoch 20 / 200 | iteration 10 / 171 | Total Loss: 3.641406774520874 | KNN Loss: 3.5917892456054688 | CLS Loss: 0.04961744695901871\n",
      "Epoch 20 / 200 | iteration 20 / 171 | Total Loss: 3.644167184829712 | KNN Loss: 3.593325614929199 | CLS Loss: 0.05084168538451195\n",
      "Epoch 20 / 200 | iteration 30 / 171 | Total Loss: 3.614051342010498 | KNN Loss: 3.5649852752685547 | CLS Loss: 0.04906599223613739\n",
      "Epoch 20 / 200 | iteration 40 / 171 | Total Loss: 3.6636669635772705 | KNN Loss: 3.5897953510284424 | CLS Loss: 0.07387155294418335\n",
      "Epoch 20 / 200 | iteration 50 / 171 | Total Loss: 3.8014652729034424 | KNN Loss: 3.7049405574798584 | CLS Loss: 0.09652465581893921\n",
      "Epoch 20 / 200 | iteration 60 / 171 | Total Loss: 3.711629867553711 | KNN Loss: 3.657405376434326 | CLS Loss: 0.05422439053654671\n",
      "Epoch 20 / 200 | iteration 70 / 171 | Total Loss: 3.651535987854004 | KNN Loss: 3.6144628524780273 | CLS Loss: 0.03707309067249298\n",
      "Epoch 20 / 200 | iteration 80 / 171 | Total Loss: 3.6377336978912354 | KNN Loss: 3.577410936355591 | CLS Loss: 0.060322877019643784\n",
      "Epoch 20 / 200 | iteration 90 / 171 | Total Loss: 3.6610934734344482 | KNN Loss: 3.6219334602355957 | CLS Loss: 0.039159905165433884\n",
      "Epoch 20 / 200 | iteration 100 / 171 | Total Loss: 3.656099557876587 | KNN Loss: 3.6006813049316406 | CLS Loss: 0.05541819706559181\n",
      "Epoch 20 / 200 | iteration 110 / 171 | Total Loss: 3.6564693450927734 | KNN Loss: 3.6122546195983887 | CLS Loss: 0.04421475529670715\n",
      "Epoch 20 / 200 | iteration 120 / 171 | Total Loss: 3.632976531982422 | KNN Loss: 3.589423179626465 | CLS Loss: 0.043553322553634644\n",
      "Epoch 20 / 200 | iteration 130 / 171 | Total Loss: 3.6319098472595215 | KNN Loss: 3.5969858169555664 | CLS Loss: 0.03492414206266403\n",
      "Epoch 20 / 200 | iteration 140 / 171 | Total Loss: 3.634427309036255 | KNN Loss: 3.5884029865264893 | CLS Loss: 0.046024344861507416\n",
      "Epoch 20 / 200 | iteration 150 / 171 | Total Loss: 3.628230094909668 | KNN Loss: 3.5729947090148926 | CLS Loss: 0.055235277861356735\n",
      "Epoch 20 / 200 | iteration 160 / 171 | Total Loss: 3.717994451522827 | KNN Loss: 3.629685878753662 | CLS Loss: 0.0883086696267128\n",
      "Epoch 20 / 200 | iteration 170 / 171 | Total Loss: 3.6891634464263916 | KNN Loss: 3.61450457572937 | CLS Loss: 0.07465879619121552\n",
      "Epoch: 020, Loss: 3.6542, Train: 0.9854, Valid: 0.9799, Best: 0.9810\n",
      "Epoch 21 / 200 | iteration 0 / 171 | Total Loss: 3.6514320373535156 | KNN Loss: 3.5839457511901855 | CLS Loss: 0.06748625636100769\n",
      "Epoch 21 / 200 | iteration 10 / 171 | Total Loss: 3.707144021987915 | KNN Loss: 3.6361589431762695 | CLS Loss: 0.0709850937128067\n",
      "Epoch 21 / 200 | iteration 20 / 171 | Total Loss: 3.6641416549682617 | KNN Loss: 3.618762254714966 | CLS Loss: 0.04537937790155411\n",
      "Epoch 21 / 200 | iteration 30 / 171 | Total Loss: 3.62337327003479 | KNN Loss: 3.5682339668273926 | CLS Loss: 0.0551392063498497\n",
      "Epoch 21 / 200 | iteration 40 / 171 | Total Loss: 3.6464545726776123 | KNN Loss: 3.574148178100586 | CLS Loss: 0.07230636477470398\n",
      "Epoch 21 / 200 | iteration 50 / 171 | Total Loss: 3.6557421684265137 | KNN Loss: 3.6104326248168945 | CLS Loss: 0.04530956968665123\n",
      "Epoch 21 / 200 | iteration 60 / 171 | Total Loss: 3.624937057495117 | KNN Loss: 3.5642998218536377 | CLS Loss: 0.060637131333351135\n",
      "Epoch 21 / 200 | iteration 70 / 171 | Total Loss: 3.6439614295959473 | KNN Loss: 3.596174478530884 | CLS Loss: 0.0477869026362896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 200 | iteration 80 / 171 | Total Loss: 3.6463847160339355 | KNN Loss: 3.587380886077881 | CLS Loss: 0.0590038038790226\n",
      "Epoch 21 / 200 | iteration 90 / 171 | Total Loss: 3.713458299636841 | KNN Loss: 3.5985424518585205 | CLS Loss: 0.11491577327251434\n",
      "Epoch 21 / 200 | iteration 100 / 171 | Total Loss: 3.639531135559082 | KNN Loss: 3.600400924682617 | CLS Loss: 0.03913012519478798\n",
      "Epoch 21 / 200 | iteration 110 / 171 | Total Loss: 3.652320623397827 | KNN Loss: 3.596884250640869 | CLS Loss: 0.05543634667992592\n",
      "Epoch 21 / 200 | iteration 120 / 171 | Total Loss: 3.642812967300415 | KNN Loss: 3.5828213691711426 | CLS Loss: 0.05999155342578888\n",
      "Epoch 21 / 200 | iteration 130 / 171 | Total Loss: 3.6397979259490967 | KNN Loss: 3.6042985916137695 | CLS Loss: 0.03549940511584282\n",
      "Epoch 21 / 200 | iteration 140 / 171 | Total Loss: 3.6662955284118652 | KNN Loss: 3.62724232673645 | CLS Loss: 0.03905318304896355\n",
      "Epoch 21 / 200 | iteration 150 / 171 | Total Loss: 3.619785785675049 | KNN Loss: 3.5766873359680176 | CLS Loss: 0.04309845343232155\n",
      "Epoch 21 / 200 | iteration 160 / 171 | Total Loss: 3.5920426845550537 | KNN Loss: 3.554276943206787 | CLS Loss: 0.037765808403491974\n",
      "Epoch 21 / 200 | iteration 170 / 171 | Total Loss: 3.618318796157837 | KNN Loss: 3.5810372829437256 | CLS Loss: 0.03728156164288521\n",
      "Epoch: 021, Loss: 3.6567, Train: 0.9864, Valid: 0.9824, Best: 0.9824\n",
      "Epoch 22 / 200 | iteration 0 / 171 | Total Loss: 3.6520214080810547 | KNN Loss: 3.590419292449951 | CLS Loss: 0.061602089554071426\n",
      "Epoch 22 / 200 | iteration 10 / 171 | Total Loss: 3.640627145767212 | KNN Loss: 3.6112473011016846 | CLS Loss: 0.029379835352301598\n",
      "Epoch 22 / 200 | iteration 20 / 171 | Total Loss: 3.6391782760620117 | KNN Loss: 3.624199867248535 | CLS Loss: 0.014978397637605667\n",
      "Epoch 22 / 200 | iteration 30 / 171 | Total Loss: 3.6245830059051514 | KNN Loss: 3.552225351333618 | CLS Loss: 0.07235769927501678\n",
      "Epoch 22 / 200 | iteration 40 / 171 | Total Loss: 3.6369500160217285 | KNN Loss: 3.593898296356201 | CLS Loss: 0.04305180534720421\n",
      "Epoch 22 / 200 | iteration 50 / 171 | Total Loss: 3.6389377117156982 | KNN Loss: 3.584052085876465 | CLS Loss: 0.05488574132323265\n",
      "Epoch 22 / 200 | iteration 60 / 171 | Total Loss: 3.7196671962738037 | KNN Loss: 3.629059076309204 | CLS Loss: 0.0906081423163414\n",
      "Epoch 22 / 200 | iteration 70 / 171 | Total Loss: 3.659513473510742 | KNN Loss: 3.593752145767212 | CLS Loss: 0.06576129794120789\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
