{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 10\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'/mnt/qnap/ekosman/mitbih_train.csv'\n",
    "test_data_path = r'/mnt/qnap/ekosman/mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.475297927856445 | KNN Loss: 5.602973937988281 | CLS Loss: 1.8723238706588745\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 4.45629358291626 | KNN Loss: 3.5741217136383057 | CLS Loss: 0.8821719288825989\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 4.092979431152344 | KNN Loss: 3.3522326946258545 | CLS Loss: 0.7407468557357788\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 3.971721649169922 | KNN Loss: 3.3088138103485107 | CLS Loss: 0.6629079580307007\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 3.879157781600952 | KNN Loss: 3.311636447906494 | CLS Loss: 0.567521333694458\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 3.7825379371643066 | KNN Loss: 3.2636935710906982 | CLS Loss: 0.5188443660736084\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 3.724571704864502 | KNN Loss: 3.206364154815674 | CLS Loss: 0.5182076692581177\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 3.773526906967163 | KNN Loss: 3.2495923042297363 | CLS Loss: 0.523934543132782\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 3.7175166606903076 | KNN Loss: 3.196678638458252 | CLS Loss: 0.5208379626274109\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 3.706416130065918 | KNN Loss: 3.2092151641845703 | CLS Loss: 0.4972008466720581\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 3.6661229133605957 | KNN Loss: 3.2287373542785645 | CLS Loss: 0.43738555908203125\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 3.6028480529785156 | KNN Loss: 3.257509708404541 | CLS Loss: 0.34533828496932983\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 3.565422534942627 | KNN Loss: 3.1994826793670654 | CLS Loss: 0.3659399747848511\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 3.5598301887512207 | KNN Loss: 3.1884710788726807 | CLS Loss: 0.3713591396808624\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 3.5526533126831055 | KNN Loss: 3.196831464767456 | CLS Loss: 0.35582172870635986\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 3.53045654296875 | KNN Loss: 3.2534289360046387 | CLS Loss: 0.27702751755714417\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 3.526139497756958 | KNN Loss: 3.2459235191345215 | CLS Loss: 0.2802160680294037\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 3.5175187587738037 | KNN Loss: 3.2167105674743652 | CLS Loss: 0.30080822110176086\n",
      "Epoch: 001, Loss: 3.8838, Train: 0.9116, Valid: 0.9122, Best: 0.9122\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 3.456042528152466 | KNN Loss: 3.2103071212768555 | CLS Loss: 0.24573533236980438\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 3.492093801498413 | KNN Loss: 3.208705425262451 | CLS Loss: 0.2833884656429291\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 3.4502151012420654 | KNN Loss: 3.2163777351379395 | CLS Loss: 0.23383736610412598\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 3.5486702919006348 | KNN Loss: 3.206139326095581 | CLS Loss: 0.3425309360027313\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 3.5371439456939697 | KNN Loss: 3.17461895942688 | CLS Loss: 0.3625248968601227\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 3.507263660430908 | KNN Loss: 3.191664695739746 | CLS Loss: 0.31559890508651733\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 3.4778499603271484 | KNN Loss: 3.1781275272369385 | CLS Loss: 0.29972249269485474\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 3.4518895149230957 | KNN Loss: 3.2416279315948486 | CLS Loss: 0.21026162803173065\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 3.4114081859588623 | KNN Loss: 3.1737594604492188 | CLS Loss: 0.23764872550964355\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 3.4299402236938477 | KNN Loss: 3.143962860107422 | CLS Loss: 0.28597739338874817\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 3.397181510925293 | KNN Loss: 3.1887283325195312 | CLS Loss: 0.20845310389995575\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 3.4028372764587402 | KNN Loss: 3.180126905441284 | CLS Loss: 0.22271038591861725\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 3.375164031982422 | KNN Loss: 3.1509552001953125 | CLS Loss: 0.2242087721824646\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 3.347882032394409 | KNN Loss: 3.151895523071289 | CLS Loss: 0.19598644971847534\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 3.3450589179992676 | KNN Loss: 3.127594232559204 | CLS Loss: 0.21746467053890228\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 3.434738874435425 | KNN Loss: 3.158439874649048 | CLS Loss: 0.2762989401817322\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 3.335890293121338 | KNN Loss: 3.121619462966919 | CLS Loss: 0.21427075564861298\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 3.3816046714782715 | KNN Loss: 3.140625 | CLS Loss: 0.2409796416759491\n",
      "Epoch: 002, Loss: 3.4296, Train: 0.9395, Valid: 0.9404, Best: 0.9404\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 3.3756749629974365 | KNN Loss: 3.163140296936035 | CLS Loss: 0.21253463625907898\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 3.349526882171631 | KNN Loss: 3.14373517036438 | CLS Loss: 0.20579180121421814\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 3.374702215194702 | KNN Loss: 3.1858184337615967 | CLS Loss: 0.18888381123542786\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 3.3731367588043213 | KNN Loss: 3.1002919673919678 | CLS Loss: 0.27284476161003113\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 3.347788095474243 | KNN Loss: 3.134262800216675 | CLS Loss: 0.21352531015872955\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 3.29695200920105 | KNN Loss: 3.100212812423706 | CLS Loss: 0.19673919677734375\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 3.2721874713897705 | KNN Loss: 3.1276187896728516 | CLS Loss: 0.14456860721111298\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 3.349273204803467 | KNN Loss: 3.09183406829834 | CLS Loss: 0.25743910670280457\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 3.3333003520965576 | KNN Loss: 3.1218461990356445 | CLS Loss: 0.21145407855510712\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 3.3098905086517334 | KNN Loss: 3.1046199798583984 | CLS Loss: 0.20527057349681854\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 3.3157076835632324 | KNN Loss: 3.1142501831054688 | CLS Loss: 0.2014574110507965\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 3.2838730812072754 | KNN Loss: 3.094470977783203 | CLS Loss: 0.18940216302871704\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 3.28086519241333 | KNN Loss: 3.076216697692871 | CLS Loss: 0.20464861392974854\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 3.2648558616638184 | KNN Loss: 3.078406810760498 | CLS Loss: 0.18644894659519196\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 3.2569785118103027 | KNN Loss: 3.117039203643799 | CLS Loss: 0.1399393379688263\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 3.2854297161102295 | KNN Loss: 3.0976290702819824 | CLS Loss: 0.1878005564212799\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 3.314237117767334 | KNN Loss: 3.169670820236206 | CLS Loss: 0.14456623792648315\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 3.3022098541259766 | KNN Loss: 3.1000001430511475 | CLS Loss: 0.20220981538295746\n",
      "Epoch: 003, Loss: 3.3016, Train: 0.9569, Valid: 0.9559, Best: 0.9559\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 3.26027774810791 | KNN Loss: 3.0718464851379395 | CLS Loss: 0.18843118846416473\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 3.2267873287200928 | KNN Loss: 3.083848714828491 | CLS Loss: 0.14293865859508514\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 3.234160900115967 | KNN Loss: 3.0800669193267822 | CLS Loss: 0.15409408509731293\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 3.285806655883789 | KNN Loss: 3.11346173286438 | CLS Loss: 0.17234499752521515\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 3.2557177543640137 | KNN Loss: 3.108572006225586 | CLS Loss: 0.1471458375453949\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 3.244516134262085 | KNN Loss: 3.101250171661377 | CLS Loss: 0.14326594769954681\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 3.2497596740722656 | KNN Loss: 3.104553699493408 | CLS Loss: 0.14520585536956787\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 3.259669303894043 | KNN Loss: 3.0858073234558105 | CLS Loss: 0.17386195063591003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 3.215547800064087 | KNN Loss: 3.0493133068084717 | CLS Loss: 0.16623446345329285\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 3.2456350326538086 | KNN Loss: 3.114419937133789 | CLS Loss: 0.13121512532234192\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 3.1848208904266357 | KNN Loss: 3.06929874420166 | CLS Loss: 0.11552203446626663\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 3.2359862327575684 | KNN Loss: 3.1000452041625977 | CLS Loss: 0.13594093918800354\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 3.1889703273773193 | KNN Loss: 3.0894834995269775 | CLS Loss: 0.09948676079511642\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 3.240623950958252 | KNN Loss: 3.073805093765259 | CLS Loss: 0.16681893169879913\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 3.223127603530884 | KNN Loss: 3.0912883281707764 | CLS Loss: 0.1318393498659134\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 3.1962461471557617 | KNN Loss: 3.1012141704559326 | CLS Loss: 0.09503185749053955\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 3.2055916786193848 | KNN Loss: 3.0726537704467773 | CLS Loss: 0.13293780386447906\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 3.2253153324127197 | KNN Loss: 3.0886919498443604 | CLS Loss: 0.13662342727184296\n",
      "Epoch: 004, Loss: 3.2354, Train: 0.9565, Valid: 0.9548, Best: 0.9559\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 3.210249662399292 | KNN Loss: 3.0830130577087402 | CLS Loss: 0.12723664939403534\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 3.279559850692749 | KNN Loss: 3.1233456134796143 | CLS Loss: 0.1562141627073288\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 3.1797330379486084 | KNN Loss: 3.076120376586914 | CLS Loss: 0.10361272841691971\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 3.2399957180023193 | KNN Loss: 3.072139263153076 | CLS Loss: 0.1678563803434372\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 3.1748087406158447 | KNN Loss: 3.0617923736572266 | CLS Loss: 0.11301642656326294\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 3.2000136375427246 | KNN Loss: 3.0788779258728027 | CLS Loss: 0.12113580852746964\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 3.224146604537964 | KNN Loss: 3.078477144241333 | CLS Loss: 0.14566951990127563\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 3.1809725761413574 | KNN Loss: 3.0521297454833984 | CLS Loss: 0.1288428008556366\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 3.1750855445861816 | KNN Loss: 3.070436716079712 | CLS Loss: 0.10464893281459808\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 3.1678645610809326 | KNN Loss: 3.052157402038574 | CLS Loss: 0.11570709943771362\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 3.207426071166992 | KNN Loss: 3.066194534301758 | CLS Loss: 0.14123156666755676\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 3.2203941345214844 | KNN Loss: 3.0796053409576416 | CLS Loss: 0.14078883826732635\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 3.2100682258605957 | KNN Loss: 3.0763063430786133 | CLS Loss: 0.13376183807849884\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 3.16374135017395 | KNN Loss: 3.0699310302734375 | CLS Loss: 0.09381029009819031\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 3.1616954803466797 | KNN Loss: 3.0607359409332275 | CLS Loss: 0.10095963627099991\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 3.1474204063415527 | KNN Loss: 3.071349859237671 | CLS Loss: 0.07607053220272064\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 3.1689441204071045 | KNN Loss: 3.0684025287628174 | CLS Loss: 0.1005416139960289\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 3.1837172508239746 | KNN Loss: 3.065352439880371 | CLS Loss: 0.1183648481965065\n",
      "Epoch: 005, Loss: 3.1940, Train: 0.9696, Valid: 0.9667, Best: 0.9667\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 3.1932899951934814 | KNN Loss: 3.1010780334472656 | CLS Loss: 0.09221199154853821\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 3.1908578872680664 | KNN Loss: 3.060187339782715 | CLS Loss: 0.13067062199115753\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 3.180537700653076 | KNN Loss: 3.078933000564575 | CLS Loss: 0.10160467773675919\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 3.183788299560547 | KNN Loss: 3.0949907302856445 | CLS Loss: 0.08879746496677399\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 3.1576383113861084 | KNN Loss: 3.0738465785980225 | CLS Loss: 0.08379168063402176\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 3.182018280029297 | KNN Loss: 3.089083194732666 | CLS Loss: 0.09293508529663086\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 3.2058210372924805 | KNN Loss: 3.0764336585998535 | CLS Loss: 0.12938745319843292\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 3.173555374145508 | KNN Loss: 3.0766687393188477 | CLS Loss: 0.09688656032085419\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 3.187068462371826 | KNN Loss: 3.0383260250091553 | CLS Loss: 0.14874231815338135\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 3.1960349082946777 | KNN Loss: 3.053170680999756 | CLS Loss: 0.1428641974925995\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 3.1520473957061768 | KNN Loss: 3.0515284538269043 | CLS Loss: 0.10051903128623962\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 3.1863348484039307 | KNN Loss: 3.066159725189209 | CLS Loss: 0.12017522752285004\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 3.1643013954162598 | KNN Loss: 3.0341296195983887 | CLS Loss: 0.1301717907190323\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 3.128096342086792 | KNN Loss: 3.0407848358154297 | CLS Loss: 0.0873115062713623\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 3.194471836090088 | KNN Loss: 3.0932512283325195 | CLS Loss: 0.10122054070234299\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 3.1669933795928955 | KNN Loss: 3.0654613971710205 | CLS Loss: 0.1015319675207138\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 3.181938648223877 | KNN Loss: 3.0959160327911377 | CLS Loss: 0.08602260053157806\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 3.140519380569458 | KNN Loss: 3.0372438430786133 | CLS Loss: 0.10327544808387756\n",
      "Epoch: 006, Loss: 3.1704, Train: 0.9737, Valid: 0.9712, Best: 0.9712\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 3.1403708457946777 | KNN Loss: 3.0084784030914307 | CLS Loss: 0.1318923979997635\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 3.205228567123413 | KNN Loss: 3.0427134037017822 | CLS Loss: 0.16251523792743683\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 3.150015354156494 | KNN Loss: 3.0708112716674805 | CLS Loss: 0.07920408248901367\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 3.1295394897460938 | KNN Loss: 3.0574584007263184 | CLS Loss: 0.07208117097616196\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 3.108231544494629 | KNN Loss: 3.0369679927825928 | CLS Loss: 0.07126367092132568\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 3.2063283920288086 | KNN Loss: 3.0666308403015137 | CLS Loss: 0.13969752192497253\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 3.1369872093200684 | KNN Loss: 3.0576720237731934 | CLS Loss: 0.07931520789861679\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 3.1451759338378906 | KNN Loss: 3.0492374897003174 | CLS Loss: 0.09593845903873444\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 3.153233528137207 | KNN Loss: 3.0451905727386475 | CLS Loss: 0.10804294049739838\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 3.1305172443389893 | KNN Loss: 3.017951250076294 | CLS Loss: 0.1125660315155983\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 3.123628854751587 | KNN Loss: 3.038015604019165 | CLS Loss: 0.08561332523822784\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 3.214837074279785 | KNN Loss: 3.059922933578491 | CLS Loss: 0.15491409599781036\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 3.1340553760528564 | KNN Loss: 3.027322769165039 | CLS Loss: 0.10673271119594574\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 3.159815549850464 | KNN Loss: 3.099930763244629 | CLS Loss: 0.05988474190235138\n",
      "Epoch 7 / 200 | iteration 140 / 171 | Total Loss: 3.17854380607605 | KNN Loss: 3.070723533630371 | CLS Loss: 0.10782038420438766\n",
      "Epoch 7 / 200 | iteration 150 / 171 | Total Loss: 3.143226385116577 | KNN Loss: 3.0433242321014404 | CLS Loss: 0.09990216046571732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 200 | iteration 160 / 171 | Total Loss: 3.1254847049713135 | KNN Loss: 3.018068313598633 | CLS Loss: 0.10741640627384186\n",
      "Epoch 7 / 200 | iteration 170 / 171 | Total Loss: 3.1957969665527344 | KNN Loss: 3.050910472869873 | CLS Loss: 0.1448865830898285\n",
      "Epoch: 007, Loss: 3.1540, Train: 0.9721, Valid: 0.9689, Best: 0.9712\n",
      "Epoch 8 / 200 | iteration 0 / 171 | Total Loss: 3.1672019958496094 | KNN Loss: 3.0664424896240234 | CLS Loss: 0.1007595956325531\n",
      "Epoch 8 / 200 | iteration 10 / 171 | Total Loss: 3.163222074508667 | KNN Loss: 3.0432236194610596 | CLS Loss: 0.11999840289354324\n",
      "Epoch 8 / 200 | iteration 20 / 171 | Total Loss: 3.1639440059661865 | KNN Loss: 3.0616989135742188 | CLS Loss: 0.10224498063325882\n",
      "Epoch 8 / 200 | iteration 30 / 171 | Total Loss: 3.184519052505493 | KNN Loss: 3.0765140056610107 | CLS Loss: 0.10800512880086899\n",
      "Epoch 8 / 200 | iteration 40 / 171 | Total Loss: 3.1195497512817383 | KNN Loss: 3.0272727012634277 | CLS Loss: 0.09227711707353592\n",
      "Epoch 8 / 200 | iteration 50 / 171 | Total Loss: 3.100050210952759 | KNN Loss: 3.041415214538574 | CLS Loss: 0.05863499268889427\n",
      "Epoch 8 / 200 | iteration 60 / 171 | Total Loss: 3.1452016830444336 | KNN Loss: 3.0565216541290283 | CLS Loss: 0.08868002891540527\n",
      "Epoch 8 / 200 | iteration 70 / 171 | Total Loss: 3.1247613430023193 | KNN Loss: 3.03348445892334 | CLS Loss: 0.0912768617272377\n",
      "Epoch 8 / 200 | iteration 80 / 171 | Total Loss: 3.1620543003082275 | KNN Loss: 3.076340913772583 | CLS Loss: 0.08571349084377289\n",
      "Epoch 8 / 200 | iteration 90 / 171 | Total Loss: 3.1651854515075684 | KNN Loss: 3.073423385620117 | CLS Loss: 0.09176213294267654\n",
      "Epoch 8 / 200 | iteration 100 / 171 | Total Loss: 3.1408703327178955 | KNN Loss: 3.032083749771118 | CLS Loss: 0.10878666490316391\n",
      "Epoch 8 / 200 | iteration 110 / 171 | Total Loss: 3.129865884780884 | KNN Loss: 3.0417468547821045 | CLS Loss: 0.08811912685632706\n",
      "Epoch 8 / 200 | iteration 120 / 171 | Total Loss: 3.19977068901062 | KNN Loss: 3.0832395553588867 | CLS Loss: 0.11653105914592743\n",
      "Epoch 8 / 200 | iteration 130 / 171 | Total Loss: 3.1551413536071777 | KNN Loss: 3.0196752548217773 | CLS Loss: 0.13546615839004517\n",
      "Epoch 8 / 200 | iteration 140 / 171 | Total Loss: 3.1169650554656982 | KNN Loss: 3.0400631427764893 | CLS Loss: 0.07690192013978958\n",
      "Epoch 8 / 200 | iteration 150 / 171 | Total Loss: 3.18259596824646 | KNN Loss: 3.0907795429229736 | CLS Loss: 0.09181646257638931\n",
      "Epoch 8 / 200 | iteration 160 / 171 | Total Loss: 3.1082093715667725 | KNN Loss: 3.0295231342315674 | CLS Loss: 0.07868615537881851\n",
      "Epoch 8 / 200 | iteration 170 / 171 | Total Loss: 3.1480987071990967 | KNN Loss: 3.050917148590088 | CLS Loss: 0.09718167036771774\n",
      "Epoch: 008, Loss: 3.1480, Train: 0.9748, Valid: 0.9714, Best: 0.9714\n",
      "Epoch 9 / 200 | iteration 0 / 171 | Total Loss: 3.1268715858459473 | KNN Loss: 3.0128257274627686 | CLS Loss: 0.11404581367969513\n",
      "Epoch 9 / 200 | iteration 10 / 171 | Total Loss: 3.1548261642456055 | KNN Loss: 3.077617645263672 | CLS Loss: 0.07720859348773956\n",
      "Epoch 9 / 200 | iteration 20 / 171 | Total Loss: 3.153510093688965 | KNN Loss: 3.062917470932007 | CLS Loss: 0.0905926302075386\n",
      "Epoch 9 / 200 | iteration 30 / 171 | Total Loss: 3.1588892936706543 | KNN Loss: 3.0531532764434814 | CLS Loss: 0.10573603212833405\n",
      "Epoch 9 / 200 | iteration 40 / 171 | Total Loss: 3.1049375534057617 | KNN Loss: 3.026275873184204 | CLS Loss: 0.07866156101226807\n",
      "Epoch 9 / 200 | iteration 50 / 171 | Total Loss: 3.139221429824829 | KNN Loss: 3.0721678733825684 | CLS Loss: 0.06705356389284134\n",
      "Epoch 9 / 200 | iteration 60 / 171 | Total Loss: 3.176499605178833 | KNN Loss: 3.0878961086273193 | CLS Loss: 0.08860338479280472\n",
      "Epoch 9 / 200 | iteration 70 / 171 | Total Loss: 3.1338562965393066 | KNN Loss: 3.097198009490967 | CLS Loss: 0.03665832430124283\n",
      "Epoch 9 / 200 | iteration 80 / 171 | Total Loss: 3.1381609439849854 | KNN Loss: 3.074517250061035 | CLS Loss: 0.0636436939239502\n",
      "Epoch 9 / 200 | iteration 90 / 171 | Total Loss: 3.1515071392059326 | KNN Loss: 3.041450262069702 | CLS Loss: 0.11005682498216629\n",
      "Epoch 9 / 200 | iteration 100 / 171 | Total Loss: 3.132843494415283 | KNN Loss: 3.044678211212158 | CLS Loss: 0.08816523104906082\n",
      "Epoch 9 / 200 | iteration 110 / 171 | Total Loss: 3.1380226612091064 | KNN Loss: 3.032590627670288 | CLS Loss: 0.10543202608823776\n",
      "Epoch 9 / 200 | iteration 120 / 171 | Total Loss: 3.0892722606658936 | KNN Loss: 3.0034189224243164 | CLS Loss: 0.08585326373577118\n",
      "Epoch 9 / 200 | iteration 130 / 171 | Total Loss: 3.100888729095459 | KNN Loss: 3.025691032409668 | CLS Loss: 0.0751977190375328\n",
      "Epoch 9 / 200 | iteration 140 / 171 | Total Loss: 3.133107900619507 | KNN Loss: 3.015187978744507 | CLS Loss: 0.11791988462209702\n",
      "Epoch 9 / 200 | iteration 150 / 171 | Total Loss: 3.1226842403411865 | KNN Loss: 3.009852647781372 | CLS Loss: 0.11283164471387863\n",
      "Epoch 9 / 200 | iteration 160 / 171 | Total Loss: 3.1198952198028564 | KNN Loss: 3.050908327102661 | CLS Loss: 0.0689869299530983\n",
      "Epoch 9 / 200 | iteration 170 / 171 | Total Loss: 3.128856658935547 | KNN Loss: 3.0114033222198486 | CLS Loss: 0.11745330691337585\n",
      "Epoch: 009, Loss: 3.1326, Train: 0.9785, Valid: 0.9746, Best: 0.9746\n",
      "Epoch 10 / 200 | iteration 0 / 171 | Total Loss: 3.0574562549591064 | KNN Loss: 3.001166820526123 | CLS Loss: 0.05628952756524086\n",
      "Epoch 10 / 200 | iteration 10 / 171 | Total Loss: 3.105119466781616 | KNN Loss: 3.0264627933502197 | CLS Loss: 0.07865674048662186\n",
      "Epoch 10 / 200 | iteration 20 / 171 | Total Loss: 3.115532398223877 | KNN Loss: 3.0635886192321777 | CLS Loss: 0.05194371938705444\n",
      "Epoch 10 / 200 | iteration 30 / 171 | Total Loss: 3.168436050415039 | KNN Loss: 3.0497288703918457 | CLS Loss: 0.11870725452899933\n",
      "Epoch 10 / 200 | iteration 40 / 171 | Total Loss: 3.118500232696533 | KNN Loss: 3.0314393043518066 | CLS Loss: 0.08706089854240417\n",
      "Epoch 10 / 200 | iteration 50 / 171 | Total Loss: 3.132593870162964 | KNN Loss: 3.0571954250335693 | CLS Loss: 0.07539840042591095\n",
      "Epoch 10 / 200 | iteration 60 / 171 | Total Loss: 3.138882875442505 | KNN Loss: 3.0482866764068604 | CLS Loss: 0.09059613943099976\n",
      "Epoch 10 / 200 | iteration 70 / 171 | Total Loss: 3.1139628887176514 | KNN Loss: 3.0742311477661133 | CLS Loss: 0.039731696248054504\n",
      "Epoch 10 / 200 | iteration 80 / 171 | Total Loss: 3.1306891441345215 | KNN Loss: 3.041290521621704 | CLS Loss: 0.08939863741397858\n",
      "Epoch 10 / 200 | iteration 90 / 171 | Total Loss: 3.1127026081085205 | KNN Loss: 3.021822690963745 | CLS Loss: 0.09087985008955002\n",
      "Epoch 10 / 200 | iteration 100 / 171 | Total Loss: 3.0988008975982666 | KNN Loss: 3.049736261367798 | CLS Loss: 0.04906473681330681\n",
      "Epoch 10 / 200 | iteration 110 / 171 | Total Loss: 3.1044323444366455 | KNN Loss: 3.056387424468994 | CLS Loss: 0.0480448454618454\n",
      "Epoch 10 / 200 | iteration 120 / 171 | Total Loss: 3.1046831607818604 | KNN Loss: 3.0279741287231445 | CLS Loss: 0.07670899480581284\n",
      "Epoch 10 / 200 | iteration 130 / 171 | Total Loss: 3.0847361087799072 | KNN Loss: 3.0216355323791504 | CLS Loss: 0.063100665807724\n",
      "Epoch 10 / 200 | iteration 140 / 171 | Total Loss: 3.08463454246521 | KNN Loss: 3.032572031021118 | CLS Loss: 0.05206252634525299\n",
      "Epoch 10 / 200 | iteration 150 / 171 | Total Loss: 3.113323211669922 | KNN Loss: 3.04858136177063 | CLS Loss: 0.06474190205335617\n",
      "Epoch 10 / 200 | iteration 160 / 171 | Total Loss: 3.182638645172119 | KNN Loss: 3.059976100921631 | CLS Loss: 0.12266255170106888\n",
      "Epoch 10 / 200 | iteration 170 / 171 | Total Loss: 3.1440486907958984 | KNN Loss: 3.0374107360839844 | CLS Loss: 0.10663792490959167\n",
      "Epoch: 010, Loss: 3.1217, Train: 0.9777, Valid: 0.9740, Best: 0.9746\n",
      "Epoch 11 / 200 | iteration 0 / 171 | Total Loss: 3.1079819202423096 | KNN Loss: 3.0250980854034424 | CLS Loss: 0.08288387954235077\n",
      "Epoch 11 / 200 | iteration 10 / 171 | Total Loss: 3.1192972660064697 | KNN Loss: 3.0386624336242676 | CLS Loss: 0.08063479512929916\n",
      "Epoch 11 / 200 | iteration 20 / 171 | Total Loss: 3.1427714824676514 | KNN Loss: 3.0832793712615967 | CLS Loss: 0.059492163360118866\n",
      "Epoch 11 / 200 | iteration 30 / 171 | Total Loss: 3.133437395095825 | KNN Loss: 3.060734510421753 | CLS Loss: 0.07270297408103943\n",
      "Epoch 11 / 200 | iteration 40 / 171 | Total Loss: 3.1008460521698 | KNN Loss: 3.0278079509735107 | CLS Loss: 0.0730380043387413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 200 | iteration 50 / 171 | Total Loss: 3.091186046600342 | KNN Loss: 3.039804697036743 | CLS Loss: 0.05138140916824341\n",
      "Epoch 11 / 200 | iteration 60 / 171 | Total Loss: 3.107962131500244 | KNN Loss: 3.018915891647339 | CLS Loss: 0.08904634416103363\n",
      "Epoch 11 / 200 | iteration 70 / 171 | Total Loss: 3.0603342056274414 | KNN Loss: 3.002293586730957 | CLS Loss: 0.05804058536887169\n",
      "Epoch 11 / 200 | iteration 80 / 171 | Total Loss: 3.1454226970672607 | KNN Loss: 3.0462677478790283 | CLS Loss: 0.09915491193532944\n",
      "Epoch 11 / 200 | iteration 90 / 171 | Total Loss: 3.116126298904419 | KNN Loss: 3.0472683906555176 | CLS Loss: 0.06885800510644913\n",
      "Epoch 11 / 200 | iteration 100 / 171 | Total Loss: 3.162749767303467 | KNN Loss: 3.047560691833496 | CLS Loss: 0.11518895626068115\n",
      "Epoch 11 / 200 | iteration 110 / 171 | Total Loss: 3.1130826473236084 | KNN Loss: 2.9956142902374268 | CLS Loss: 0.11746841669082642\n",
      "Epoch 11 / 200 | iteration 120 / 171 | Total Loss: 3.0878806114196777 | KNN Loss: 3.024498224258423 | CLS Loss: 0.06338242441415787\n",
      "Epoch 11 / 200 | iteration 130 / 171 | Total Loss: 3.079934597015381 | KNN Loss: 3.024876117706299 | CLS Loss: 0.055058397352695465\n",
      "Epoch 11 / 200 | iteration 140 / 171 | Total Loss: 3.1092846393585205 | KNN Loss: 3.0404245853424072 | CLS Loss: 0.0688600242137909\n",
      "Epoch 11 / 200 | iteration 150 / 171 | Total Loss: 3.1192970275878906 | KNN Loss: 3.004302501678467 | CLS Loss: 0.11499447375535965\n",
      "Epoch 11 / 200 | iteration 160 / 171 | Total Loss: 3.086671829223633 | KNN Loss: 3.0418004989624023 | CLS Loss: 0.04487139359116554\n",
      "Epoch 11 / 200 | iteration 170 / 171 | Total Loss: 3.2284317016601562 | KNN Loss: 3.0455751419067383 | CLS Loss: 0.18285663425922394\n",
      "Epoch: 011, Loss: 3.1165, Train: 0.9754, Valid: 0.9723, Best: 0.9746\n",
      "Epoch 12 / 200 | iteration 0 / 171 | Total Loss: 3.184217929840088 | KNN Loss: 3.0768418312072754 | CLS Loss: 0.1073760986328125\n",
      "Epoch 12 / 200 | iteration 10 / 171 | Total Loss: 3.1087465286254883 | KNN Loss: 3.025968074798584 | CLS Loss: 0.0827784538269043\n",
      "Epoch 12 / 200 | iteration 20 / 171 | Total Loss: 3.11195969581604 | KNN Loss: 3.037447929382324 | CLS Loss: 0.07451167702674866\n",
      "Epoch 12 / 200 | iteration 30 / 171 | Total Loss: 3.0731377601623535 | KNN Loss: 3.0242679119110107 | CLS Loss: 0.04886975139379501\n",
      "Epoch 12 / 200 | iteration 40 / 171 | Total Loss: 3.076199531555176 | KNN Loss: 3.0414626598358154 | CLS Loss: 0.034736938774585724\n",
      "Epoch 12 / 200 | iteration 50 / 171 | Total Loss: 3.1439337730407715 | KNN Loss: 3.0583748817443848 | CLS Loss: 0.08555878698825836\n",
      "Epoch 12 / 200 | iteration 60 / 171 | Total Loss: 3.1063098907470703 | KNN Loss: 3.030521869659424 | CLS Loss: 0.07578804343938828\n",
      "Epoch 12 / 200 | iteration 70 / 171 | Total Loss: 3.1358795166015625 | KNN Loss: 3.0571398735046387 | CLS Loss: 0.07873963564634323\n",
      "Epoch 12 / 200 | iteration 80 / 171 | Total Loss: 3.1365737915039062 | KNN Loss: 3.076003313064575 | CLS Loss: 0.06057056039571762\n",
      "Epoch 12 / 200 | iteration 90 / 171 | Total Loss: 3.087860584259033 | KNN Loss: 3.02250599861145 | CLS Loss: 0.06535452604293823\n",
      "Epoch 12 / 200 | iteration 100 / 171 | Total Loss: 3.099090576171875 | KNN Loss: 3.0292325019836426 | CLS Loss: 0.06985802948474884\n",
      "Epoch 12 / 200 | iteration 110 / 171 | Total Loss: 3.095604181289673 | KNN Loss: 3.0370826721191406 | CLS Loss: 0.0585215799510479\n",
      "Epoch 12 / 200 | iteration 120 / 171 | Total Loss: 3.0949296951293945 | KNN Loss: 3.034040927886963 | CLS Loss: 0.06088874489068985\n",
      "Epoch 12 / 200 | iteration 130 / 171 | Total Loss: 3.0729446411132812 | KNN Loss: 3.0165250301361084 | CLS Loss: 0.05641956627368927\n",
      "Epoch 12 / 200 | iteration 140 / 171 | Total Loss: 3.080390214920044 | KNN Loss: 3.0296742916107178 | CLS Loss: 0.05071593448519707\n",
      "Epoch 12 / 200 | iteration 150 / 171 | Total Loss: 3.0544395446777344 | KNN Loss: 2.995620012283325 | CLS Loss: 0.05881962180137634\n",
      "Epoch 12 / 200 | iteration 160 / 171 | Total Loss: 3.0925614833831787 | KNN Loss: 3.0248358249664307 | CLS Loss: 0.0677257627248764\n",
      "Epoch 12 / 200 | iteration 170 / 171 | Total Loss: 3.0634710788726807 | KNN Loss: 3.0156733989715576 | CLS Loss: 0.04779769480228424\n",
      "Epoch: 012, Loss: 3.1108, Train: 0.9809, Valid: 0.9780, Best: 0.9780\n",
      "Epoch 13 / 200 | iteration 0 / 171 | Total Loss: 3.1053998470306396 | KNN Loss: 3.0071849822998047 | CLS Loss: 0.09821482747793198\n",
      "Epoch 13 / 200 | iteration 10 / 171 | Total Loss: 3.112126588821411 | KNN Loss: 3.0373125076293945 | CLS Loss: 0.07481401413679123\n",
      "Epoch 13 / 200 | iteration 20 / 171 | Total Loss: 3.101466655731201 | KNN Loss: 3.02339506149292 | CLS Loss: 0.0780716985464096\n",
      "Epoch 13 / 200 | iteration 30 / 171 | Total Loss: 3.0767195224761963 | KNN Loss: 3.0104732513427734 | CLS Loss: 0.06624627113342285\n",
      "Epoch 13 / 200 | iteration 40 / 171 | Total Loss: 3.1256935596466064 | KNN Loss: 3.0341618061065674 | CLS Loss: 0.09153175354003906\n",
      "Epoch 13 / 200 | iteration 50 / 171 | Total Loss: 3.1301283836364746 | KNN Loss: 3.079181432723999 | CLS Loss: 0.05094685032963753\n",
      "Epoch 13 / 200 | iteration 60 / 171 | Total Loss: 3.1126859188079834 | KNN Loss: 3.0440807342529297 | CLS Loss: 0.0686052069067955\n",
      "Epoch 13 / 200 | iteration 70 / 171 | Total Loss: 3.0856313705444336 | KNN Loss: 3.0243985652923584 | CLS Loss: 0.06123284995555878\n",
      "Epoch 13 / 200 | iteration 80 / 171 | Total Loss: 3.110438585281372 | KNN Loss: 3.043455123901367 | CLS Loss: 0.06698355078697205\n",
      "Epoch 13 / 200 | iteration 90 / 171 | Total Loss: 3.086101770401001 | KNN Loss: 3.0217599868774414 | CLS Loss: 0.06434183567762375\n",
      "Epoch 13 / 200 | iteration 100 / 171 | Total Loss: 3.0858407020568848 | KNN Loss: 3.0115931034088135 | CLS Loss: 0.07424765080213547\n",
      "Epoch 13 / 200 | iteration 110 / 171 | Total Loss: 3.078688859939575 | KNN Loss: 3.0386860370635986 | CLS Loss: 0.04000287130475044\n",
      "Epoch 13 / 200 | iteration 120 / 171 | Total Loss: 3.0960516929626465 | KNN Loss: 3.0374972820281982 | CLS Loss: 0.058554474264383316\n",
      "Epoch 13 / 200 | iteration 130 / 171 | Total Loss: 3.1449170112609863 | KNN Loss: 3.066220760345459 | CLS Loss: 0.07869613170623779\n",
      "Epoch 13 / 200 | iteration 140 / 171 | Total Loss: 3.13616681098938 | KNN Loss: 3.0464978218078613 | CLS Loss: 0.08966909348964691\n",
      "Epoch 13 / 200 | iteration 150 / 171 | Total Loss: 3.1530802249908447 | KNN Loss: 3.0735011100769043 | CLS Loss: 0.07957900315523148\n",
      "Epoch 13 / 200 | iteration 160 / 171 | Total Loss: 3.127671957015991 | KNN Loss: 3.0400006771087646 | CLS Loss: 0.08767129480838776\n",
      "Epoch 13 / 200 | iteration 170 / 171 | Total Loss: 3.1046485900878906 | KNN Loss: 3.059330940246582 | CLS Loss: 0.04531753808259964\n",
      "Epoch: 013, Loss: 3.0992, Train: 0.9812, Valid: 0.9776, Best: 0.9780\n",
      "Epoch 14 / 200 | iteration 0 / 171 | Total Loss: 3.0851173400878906 | KNN Loss: 3.048543930053711 | CLS Loss: 0.03657345473766327\n",
      "Epoch 14 / 200 | iteration 10 / 171 | Total Loss: 3.1383955478668213 | KNN Loss: 3.0637717247009277 | CLS Loss: 0.07462386041879654\n",
      "Epoch 14 / 200 | iteration 20 / 171 | Total Loss: 3.156179904937744 | KNN Loss: 3.099489450454712 | CLS Loss: 0.05669039487838745\n",
      "Epoch 14 / 200 | iteration 30 / 171 | Total Loss: 3.034487724304199 | KNN Loss: 2.9944303035736084 | CLS Loss: 0.04005737975239754\n",
      "Epoch 14 / 200 | iteration 40 / 171 | Total Loss: 3.1159543991088867 | KNN Loss: 3.037297248840332 | CLS Loss: 0.0786571130156517\n",
      "Epoch 14 / 200 | iteration 50 / 171 | Total Loss: 3.1028854846954346 | KNN Loss: 3.010690450668335 | CLS Loss: 0.09219500422477722\n",
      "Epoch 14 / 200 | iteration 60 / 171 | Total Loss: 3.085172176361084 | KNN Loss: 3.0293595790863037 | CLS Loss: 0.05581265315413475\n",
      "Epoch 14 / 200 | iteration 70 / 171 | Total Loss: 3.0581040382385254 | KNN Loss: 2.9938080310821533 | CLS Loss: 0.06429603695869446\n",
      "Epoch 14 / 200 | iteration 80 / 171 | Total Loss: 3.096406936645508 | KNN Loss: 2.9828498363494873 | CLS Loss: 0.11355709284543991\n",
      "Epoch 14 / 200 | iteration 90 / 171 | Total Loss: 3.130833148956299 | KNN Loss: 3.045177698135376 | CLS Loss: 0.0856553390622139\n",
      "Epoch 14 / 200 | iteration 100 / 171 | Total Loss: 3.100515365600586 | KNN Loss: 3.012568712234497 | CLS Loss: 0.08794662356376648\n",
      "Epoch 14 / 200 | iteration 110 / 171 | Total Loss: 3.0709047317504883 | KNN Loss: 3.0304853916168213 | CLS Loss: 0.04041939601302147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 200 | iteration 120 / 171 | Total Loss: 3.0825815200805664 | KNN Loss: 3.002673387527466 | CLS Loss: 0.079908087849617\n",
      "Epoch 14 / 200 | iteration 130 / 171 | Total Loss: 3.0879812240600586 | KNN Loss: 3.0278236865997314 | CLS Loss: 0.06015755236148834\n",
      "Epoch 14 / 200 | iteration 140 / 171 | Total Loss: 3.106348991394043 | KNN Loss: 3.0556702613830566 | CLS Loss: 0.05067875236272812\n",
      "Epoch 14 / 200 | iteration 150 / 171 | Total Loss: 3.0995595455169678 | KNN Loss: 3.0306074619293213 | CLS Loss: 0.06895215064287186\n",
      "Epoch 14 / 200 | iteration 160 / 171 | Total Loss: 3.1364939212799072 | KNN Loss: 3.057478904724121 | CLS Loss: 0.07901498675346375\n",
      "Epoch 14 / 200 | iteration 170 / 171 | Total Loss: 3.1579861640930176 | KNN Loss: 3.043156385421753 | CLS Loss: 0.1148298904299736\n",
      "Epoch: 014, Loss: 3.0993, Train: 0.9814, Valid: 0.9778, Best: 0.9780\n",
      "Epoch 15 / 200 | iteration 0 / 171 | Total Loss: 3.1010653972625732 | KNN Loss: 3.048903226852417 | CLS Loss: 0.05216224864125252\n",
      "Epoch 15 / 200 | iteration 10 / 171 | Total Loss: 3.0714752674102783 | KNN Loss: 2.989926338195801 | CLS Loss: 0.08154893666505814\n",
      "Epoch 15 / 200 | iteration 20 / 171 | Total Loss: 3.0940427780151367 | KNN Loss: 3.0083484649658203 | CLS Loss: 0.08569442480802536\n",
      "Epoch 15 / 200 | iteration 30 / 171 | Total Loss: 3.086637258529663 | KNN Loss: 3.029421806335449 | CLS Loss: 0.05721538886427879\n",
      "Epoch 15 / 200 | iteration 40 / 171 | Total Loss: 3.1369030475616455 | KNN Loss: 3.0399880409240723 | CLS Loss: 0.09691495448350906\n",
      "Epoch 15 / 200 | iteration 50 / 171 | Total Loss: 3.0911409854888916 | KNN Loss: 3.0299532413482666 | CLS Loss: 0.0611877404153347\n",
      "Epoch 15 / 200 | iteration 60 / 171 | Total Loss: 3.1013920307159424 | KNN Loss: 3.0238125324249268 | CLS Loss: 0.07757958024740219\n",
      "Epoch 15 / 200 | iteration 70 / 171 | Total Loss: 3.082535982131958 | KNN Loss: 2.9977948665618896 | CLS Loss: 0.08474111557006836\n",
      "Epoch 15 / 200 | iteration 80 / 171 | Total Loss: 3.0967061519622803 | KNN Loss: 3.0301029682159424 | CLS Loss: 0.06660307198762894\n",
      "Epoch 15 / 200 | iteration 90 / 171 | Total Loss: 3.1270744800567627 | KNN Loss: 3.0587620735168457 | CLS Loss: 0.06831233203411102\n",
      "Epoch 15 / 200 | iteration 100 / 171 | Total Loss: 3.1103131771087646 | KNN Loss: 3.039651870727539 | CLS Loss: 0.0706612765789032\n",
      "Epoch 15 / 200 | iteration 110 / 171 | Total Loss: 3.09470272064209 | KNN Loss: 3.0237019062042236 | CLS Loss: 0.07100075483322144\n",
      "Epoch 15 / 200 | iteration 120 / 171 | Total Loss: 3.0981619358062744 | KNN Loss: 3.050938844680786 | CLS Loss: 0.047223050147295\n",
      "Epoch 15 / 200 | iteration 130 / 171 | Total Loss: 3.057431697845459 | KNN Loss: 2.9911928176879883 | CLS Loss: 0.06623883545398712\n",
      "Epoch 15 / 200 | iteration 140 / 171 | Total Loss: 3.083770275115967 | KNN Loss: 3.0264875888824463 | CLS Loss: 0.057282574474811554\n",
      "Epoch 15 / 200 | iteration 150 / 171 | Total Loss: 3.07578706741333 | KNN Loss: 3.0148186683654785 | CLS Loss: 0.06096840649843216\n",
      "Epoch 15 / 200 | iteration 160 / 171 | Total Loss: 3.0966053009033203 | KNN Loss: 3.0488977432250977 | CLS Loss: 0.04770752787590027\n",
      "Epoch 15 / 200 | iteration 170 / 171 | Total Loss: 3.1293203830718994 | KNN Loss: 3.0651602745056152 | CLS Loss: 0.06416010856628418\n",
      "Epoch: 015, Loss: 3.0920, Train: 0.9838, Valid: 0.9798, Best: 0.9798\n",
      "Epoch 16 / 200 | iteration 0 / 171 | Total Loss: 3.05879545211792 | KNN Loss: 3.013712167739868 | CLS Loss: 0.045083336532115936\n",
      "Epoch 16 / 200 | iteration 10 / 171 | Total Loss: 3.073685884475708 | KNN Loss: 3.0280721187591553 | CLS Loss: 0.04561366140842438\n",
      "Epoch 16 / 200 | iteration 20 / 171 | Total Loss: 3.100165843963623 | KNN Loss: 3.047743797302246 | CLS Loss: 0.05242204666137695\n",
      "Epoch 16 / 200 | iteration 30 / 171 | Total Loss: 3.11930251121521 | KNN Loss: 3.0181262493133545 | CLS Loss: 0.1011761724948883\n",
      "Epoch 16 / 200 | iteration 40 / 171 | Total Loss: 3.0687782764434814 | KNN Loss: 3.0088844299316406 | CLS Loss: 0.05989382788538933\n",
      "Epoch 16 / 200 | iteration 50 / 171 | Total Loss: 3.071990489959717 | KNN Loss: 3.0366692543029785 | CLS Loss: 0.035321298986673355\n",
      "Epoch 16 / 200 | iteration 60 / 171 | Total Loss: 3.06500506401062 | KNN Loss: 3.0312910079956055 | CLS Loss: 0.03371395543217659\n",
      "Epoch 16 / 200 | iteration 70 / 171 | Total Loss: 3.087397575378418 | KNN Loss: 3.0591304302215576 | CLS Loss: 0.028267143294215202\n",
      "Epoch 16 / 200 | iteration 80 / 171 | Total Loss: 3.1153037548065186 | KNN Loss: 3.050278425216675 | CLS Loss: 0.0650254413485527\n",
      "Epoch 16 / 200 | iteration 90 / 171 | Total Loss: 3.1201205253601074 | KNN Loss: 3.0428757667541504 | CLS Loss: 0.07724487036466599\n",
      "Epoch 16 / 200 | iteration 100 / 171 | Total Loss: 3.0872714519500732 | KNN Loss: 3.042109966278076 | CLS Loss: 0.04516160115599632\n",
      "Epoch 16 / 200 | iteration 110 / 171 | Total Loss: 3.10821795463562 | KNN Loss: 2.994688034057617 | CLS Loss: 0.11352992057800293\n",
      "Epoch 16 / 200 | iteration 120 / 171 | Total Loss: 3.092042922973633 | KNN Loss: 3.021850824356079 | CLS Loss: 0.07019201666116714\n",
      "Epoch 16 / 200 | iteration 130 / 171 | Total Loss: 3.1128313541412354 | KNN Loss: 3.041971445083618 | CLS Loss: 0.07085999101400375\n",
      "Epoch 16 / 200 | iteration 140 / 171 | Total Loss: 3.0848031044006348 | KNN Loss: 3.0412204265594482 | CLS Loss: 0.043582648038864136\n",
      "Epoch 16 / 200 | iteration 150 / 171 | Total Loss: 3.1044089794158936 | KNN Loss: 3.043501377105713 | CLS Loss: 0.060907624661922455\n",
      "Epoch 16 / 200 | iteration 160 / 171 | Total Loss: 3.0563876628875732 | KNN Loss: 2.997570037841797 | CLS Loss: 0.05881766229867935\n",
      "Epoch 16 / 200 | iteration 170 / 171 | Total Loss: 3.047913074493408 | KNN Loss: 2.9868738651275635 | CLS Loss: 0.06103913113474846\n",
      "Epoch: 016, Loss: 3.0896, Train: 0.9833, Valid: 0.9793, Best: 0.9798\n",
      "Epoch 17 / 200 | iteration 0 / 171 | Total Loss: 3.055781364440918 | KNN Loss: 3.001753807067871 | CLS Loss: 0.05402747541666031\n",
      "Epoch 17 / 200 | iteration 10 / 171 | Total Loss: 3.0666005611419678 | KNN Loss: 3.0196692943573 | CLS Loss: 0.0469311885535717\n",
      "Epoch 17 / 200 | iteration 20 / 171 | Total Loss: 3.0901525020599365 | KNN Loss: 3.040316343307495 | CLS Loss: 0.049836110323667526\n",
      "Epoch 17 / 200 | iteration 30 / 171 | Total Loss: 3.107495069503784 | KNN Loss: 3.0352187156677246 | CLS Loss: 0.07227640599012375\n",
      "Epoch 17 / 200 | iteration 40 / 171 | Total Loss: 3.075308322906494 | KNN Loss: 3.044135093688965 | CLS Loss: 0.031173299998044968\n",
      "Epoch 17 / 200 | iteration 50 / 171 | Total Loss: 3.0803472995758057 | KNN Loss: 3.005769968032837 | CLS Loss: 0.07457736879587173\n",
      "Epoch 17 / 200 | iteration 60 / 171 | Total Loss: 3.1046316623687744 | KNN Loss: 2.992201328277588 | CLS Loss: 0.1124303787946701\n",
      "Epoch 17 / 200 | iteration 70 / 171 | Total Loss: 3.093973159790039 | KNN Loss: 3.0362465381622314 | CLS Loss: 0.05772652104496956\n",
      "Epoch 17 / 200 | iteration 80 / 171 | Total Loss: 3.039353609085083 | KNN Loss: 3.0087521076202393 | CLS Loss: 0.030601581558585167\n",
      "Epoch 17 / 200 | iteration 90 / 171 | Total Loss: 3.0343568325042725 | KNN Loss: 2.988835573196411 | CLS Loss: 0.045521270483732224\n",
      "Epoch 17 / 200 | iteration 100 / 171 | Total Loss: 3.1090641021728516 | KNN Loss: 3.0280637741088867 | CLS Loss: 0.08100029081106186\n",
      "Epoch 17 / 200 | iteration 110 / 171 | Total Loss: 3.048187732696533 | KNN Loss: 3.0053906440734863 | CLS Loss: 0.04279707372188568\n",
      "Epoch 17 / 200 | iteration 120 / 171 | Total Loss: 3.070096492767334 | KNN Loss: 3.0083045959472656 | CLS Loss: 0.0617917962372303\n",
      "Epoch 17 / 200 | iteration 130 / 171 | Total Loss: 3.061129331588745 | KNN Loss: 2.9997828006744385 | CLS Loss: 0.061346635222435\n",
      "Epoch 17 / 200 | iteration 140 / 171 | Total Loss: 3.07535719871521 | KNN Loss: 3.026108741760254 | CLS Loss: 0.04924852028489113\n",
      "Epoch 17 / 200 | iteration 150 / 171 | Total Loss: 3.071521520614624 | KNN Loss: 3.0412604808807373 | CLS Loss: 0.03026098571717739\n",
      "Epoch 17 / 200 | iteration 160 / 171 | Total Loss: 3.050091028213501 | KNN Loss: 3.0109829902648926 | CLS Loss: 0.03910805657505989\n",
      "Epoch 17 / 200 | iteration 170 / 171 | Total Loss: 3.0749192237854004 | KNN Loss: 3.0187036991119385 | CLS Loss: 0.05621545761823654\n",
      "Epoch: 017, Loss: 3.0778, Train: 0.9849, Valid: 0.9810, Best: 0.9810\n",
      "Epoch 18 / 200 | iteration 0 / 171 | Total Loss: 3.042341709136963 | KNN Loss: 3.0104615688323975 | CLS Loss: 0.03188011795282364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 200 | iteration 10 / 171 | Total Loss: 3.1107540130615234 | KNN Loss: 3.0310940742492676 | CLS Loss: 0.07966001331806183\n",
      "Epoch 18 / 200 | iteration 20 / 171 | Total Loss: 3.1205809116363525 | KNN Loss: 3.0421838760375977 | CLS Loss: 0.07839705049991608\n",
      "Epoch 18 / 200 | iteration 30 / 171 | Total Loss: 3.087979316711426 | KNN Loss: 3.042618751525879 | CLS Loss: 0.04536059871315956\n",
      "Epoch 18 / 200 | iteration 40 / 171 | Total Loss: 3.0854392051696777 | KNN Loss: 3.048264503479004 | CLS Loss: 0.03717474266886711\n",
      "Epoch 18 / 200 | iteration 50 / 171 | Total Loss: 3.046215534210205 | KNN Loss: 3.00057315826416 | CLS Loss: 0.04564239829778671\n",
      "Epoch 18 / 200 | iteration 60 / 171 | Total Loss: 3.084365129470825 | KNN Loss: 3.030977725982666 | CLS Loss: 0.05338743329048157\n",
      "Epoch 18 / 200 | iteration 70 / 171 | Total Loss: 3.0990936756134033 | KNN Loss: 3.0539746284484863 | CLS Loss: 0.045119039714336395\n",
      "Epoch 18 / 200 | iteration 80 / 171 | Total Loss: 3.0946459770202637 | KNN Loss: 2.995950222015381 | CLS Loss: 0.0986957997083664\n",
      "Epoch 18 / 200 | iteration 90 / 171 | Total Loss: 3.046755075454712 | KNN Loss: 2.984052896499634 | CLS Loss: 0.0627022236585617\n",
      "Epoch 18 / 200 | iteration 100 / 171 | Total Loss: 3.076754331588745 | KNN Loss: 3.0317301750183105 | CLS Loss: 0.04502421244978905\n",
      "Epoch 18 / 200 | iteration 110 / 171 | Total Loss: 3.0894417762756348 | KNN Loss: 3.0365536212921143 | CLS Loss: 0.052888158708810806\n",
      "Epoch 18 / 200 | iteration 120 / 171 | Total Loss: 3.1235668659210205 | KNN Loss: 3.060210943222046 | CLS Loss: 0.06335586309432983\n",
      "Epoch 18 / 200 | iteration 130 / 171 | Total Loss: 3.0747568607330322 | KNN Loss: 3.0093557834625244 | CLS Loss: 0.06540100276470184\n",
      "Epoch 18 / 200 | iteration 140 / 171 | Total Loss: 3.0613625049591064 | KNN Loss: 3.044822931289673 | CLS Loss: 0.016539493575692177\n",
      "Epoch 18 / 200 | iteration 150 / 171 | Total Loss: 3.112839698791504 | KNN Loss: 3.0367374420166016 | CLS Loss: 0.07610227167606354\n",
      "Epoch 18 / 200 | iteration 160 / 171 | Total Loss: 3.0394341945648193 | KNN Loss: 3.0064239501953125 | CLS Loss: 0.03301013633608818\n",
      "Epoch 18 / 200 | iteration 170 / 171 | Total Loss: 3.0465197563171387 | KNN Loss: 2.9683780670166016 | CLS Loss: 0.07814168184995651\n",
      "Epoch: 018, Loss: 3.0775, Train: 0.9851, Valid: 0.9803, Best: 0.9810\n",
      "Epoch 19 / 200 | iteration 0 / 171 | Total Loss: 3.0286357402801514 | KNN Loss: 2.9977540969848633 | CLS Loss: 0.030881555750966072\n",
      "Epoch 19 / 200 | iteration 10 / 171 | Total Loss: 3.1175689697265625 | KNN Loss: 3.03732967376709 | CLS Loss: 0.08023940026760101\n",
      "Epoch 19 / 200 | iteration 20 / 171 | Total Loss: 3.084505081176758 | KNN Loss: 3.0211753845214844 | CLS Loss: 0.06332959234714508\n",
      "Epoch 19 / 200 | iteration 30 / 171 | Total Loss: 3.1088099479675293 | KNN Loss: 3.0080811977386475 | CLS Loss: 0.10072870552539825\n",
      "Epoch 19 / 200 | iteration 40 / 171 | Total Loss: 3.0747244358062744 | KNN Loss: 3.033635377883911 | CLS Loss: 0.041089124977588654\n",
      "Epoch 19 / 200 | iteration 50 / 171 | Total Loss: 3.107147216796875 | KNN Loss: 3.0754330158233643 | CLS Loss: 0.031714219599962234\n",
      "Epoch 19 / 200 | iteration 60 / 171 | Total Loss: 3.0451090335845947 | KNN Loss: 2.9961140155792236 | CLS Loss: 0.04899490252137184\n",
      "Epoch 19 / 200 | iteration 70 / 171 | Total Loss: 3.0436203479766846 | KNN Loss: 2.996501922607422 | CLS Loss: 0.04711850732564926\n",
      "Epoch 19 / 200 | iteration 80 / 171 | Total Loss: 3.0412778854370117 | KNN Loss: 2.993710517883301 | CLS Loss: 0.04756733775138855\n",
      "Epoch 19 / 200 | iteration 90 / 171 | Total Loss: 3.0395846366882324 | KNN Loss: 2.9994547367095947 | CLS Loss: 0.04012995958328247\n",
      "Epoch 19 / 200 | iteration 100 / 171 | Total Loss: 3.0779709815979004 | KNN Loss: 3.0133564472198486 | CLS Loss: 0.06461451202630997\n",
      "Epoch 19 / 200 | iteration 110 / 171 | Total Loss: 3.1102797985076904 | KNN Loss: 3.0134458541870117 | CLS Loss: 0.09683386236429214\n",
      "Epoch 19 / 200 | iteration 120 / 171 | Total Loss: 3.0399675369262695 | KNN Loss: 3.005518913269043 | CLS Loss: 0.03444856405258179\n",
      "Epoch 19 / 200 | iteration 130 / 171 | Total Loss: 3.039182662963867 | KNN Loss: 3.006513833999634 | CLS Loss: 0.03266885131597519\n",
      "Epoch 19 / 200 | iteration 140 / 171 | Total Loss: 3.1121420860290527 | KNN Loss: 3.0529773235321045 | CLS Loss: 0.05916473641991615\n",
      "Epoch 19 / 200 | iteration 150 / 171 | Total Loss: 3.1027939319610596 | KNN Loss: 3.0466458797454834 | CLS Loss: 0.05614806339144707\n",
      "Epoch 19 / 200 | iteration 160 / 171 | Total Loss: 3.080089569091797 | KNN Loss: 3.014662742614746 | CLS Loss: 0.0654267817735672\n",
      "Epoch 19 / 200 | iteration 170 / 171 | Total Loss: 3.1005544662475586 | KNN Loss: 3.0597712993621826 | CLS Loss: 0.040783144533634186\n",
      "Epoch: 019, Loss: 3.0733, Train: 0.9856, Valid: 0.9806, Best: 0.9810\n",
      "Epoch 20 / 200 | iteration 0 / 171 | Total Loss: 3.0358386039733887 | KNN Loss: 3.0003015995025635 | CLS Loss: 0.035537078976631165\n",
      "Epoch 20 / 200 | iteration 10 / 171 | Total Loss: 3.0442123413085938 | KNN Loss: 3.024374485015869 | CLS Loss: 0.019837738946080208\n",
      "Epoch 20 / 200 | iteration 20 / 171 | Total Loss: 3.0615131855010986 | KNN Loss: 3.001814842224121 | CLS Loss: 0.059698283672332764\n",
      "Epoch 20 / 200 | iteration 30 / 171 | Total Loss: 3.038724660873413 | KNN Loss: 3.0036723613739014 | CLS Loss: 0.03505227714776993\n",
      "Epoch 20 / 200 | iteration 40 / 171 | Total Loss: 3.0979533195495605 | KNN Loss: 3.024106502532959 | CLS Loss: 0.0738467425107956\n",
      "Epoch 20 / 200 | iteration 50 / 171 | Total Loss: 3.0907485485076904 | KNN Loss: 3.0488195419311523 | CLS Loss: 0.041928987950086594\n",
      "Epoch 20 / 200 | iteration 60 / 171 | Total Loss: 3.071544885635376 | KNN Loss: 3.042311429977417 | CLS Loss: 0.029233351349830627\n",
      "Epoch 20 / 200 | iteration 70 / 171 | Total Loss: 3.0490520000457764 | KNN Loss: 3.004842519760132 | CLS Loss: 0.04420952871441841\n",
      "Epoch 20 / 200 | iteration 80 / 171 | Total Loss: 3.0484371185302734 | KNN Loss: 3.001084566116333 | CLS Loss: 0.04735266789793968\n",
      "Epoch 20 / 200 | iteration 90 / 171 | Total Loss: 3.0698792934417725 | KNN Loss: 3.0166702270507812 | CLS Loss: 0.05320916324853897\n",
      "Epoch 20 / 200 | iteration 100 / 171 | Total Loss: 3.0432021617889404 | KNN Loss: 2.9993698596954346 | CLS Loss: 0.043832190334796906\n",
      "Epoch 20 / 200 | iteration 110 / 171 | Total Loss: 3.082587480545044 | KNN Loss: 3.0405547618865967 | CLS Loss: 0.04203276336193085\n",
      "Epoch 20 / 200 | iteration 120 / 171 | Total Loss: 3.0488014221191406 | KNN Loss: 2.9925520420074463 | CLS Loss: 0.056249361485242844\n",
      "Epoch 20 / 200 | iteration 130 / 171 | Total Loss: 3.114938259124756 | KNN Loss: 3.0387561321258545 | CLS Loss: 0.07618208974599838\n",
      "Epoch 20 / 200 | iteration 140 / 171 | Total Loss: 3.025648593902588 | KNN Loss: 2.966322660446167 | CLS Loss: 0.05932581424713135\n",
      "Epoch 20 / 200 | iteration 150 / 171 | Total Loss: 3.084655523300171 | KNN Loss: 2.997082471847534 | CLS Loss: 0.08757314085960388\n",
      "Epoch 20 / 200 | iteration 160 / 171 | Total Loss: 3.073549509048462 | KNN Loss: 3.0256505012512207 | CLS Loss: 0.0478990413248539\n",
      "Epoch 20 / 200 | iteration 170 / 171 | Total Loss: 3.0718908309936523 | KNN Loss: 3.0365707874298096 | CLS Loss: 0.035320162773132324\n",
      "Epoch: 020, Loss: 3.0711, Train: 0.9870, Valid: 0.9820, Best: 0.9820\n",
      "Epoch 21 / 200 | iteration 0 / 171 | Total Loss: 3.0714452266693115 | KNN Loss: 3.048539638519287 | CLS Loss: 0.022905634716153145\n",
      "Epoch 21 / 200 | iteration 10 / 171 | Total Loss: 3.070394277572632 | KNN Loss: 3.017390012741089 | CLS Loss: 0.053004190325737\n",
      "Epoch 21 / 200 | iteration 20 / 171 | Total Loss: 3.0478501319885254 | KNN Loss: 3.0272650718688965 | CLS Loss: 0.020585140213370323\n",
      "Epoch 21 / 200 | iteration 30 / 171 | Total Loss: 3.0274574756622314 | KNN Loss: 2.9672560691833496 | CLS Loss: 0.060201313346624374\n",
      "Epoch 21 / 200 | iteration 40 / 171 | Total Loss: 3.0880653858184814 | KNN Loss: 3.0291497707366943 | CLS Loss: 0.058915551751852036\n",
      "Epoch 21 / 200 | iteration 50 / 171 | Total Loss: 3.085606813430786 | KNN Loss: 3.0386393070220947 | CLS Loss: 0.046967461705207825\n",
      "Epoch 21 / 200 | iteration 60 / 171 | Total Loss: 3.052555561065674 | KNN Loss: 3.0026960372924805 | CLS Loss: 0.04985950142145157\n",
      "Epoch 21 / 200 | iteration 70 / 171 | Total Loss: 3.0369865894317627 | KNN Loss: 3.0024197101593018 | CLS Loss: 0.03456681966781616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 200 | iteration 80 / 171 | Total Loss: 3.0480027198791504 | KNN Loss: 3.0246052742004395 | CLS Loss: 0.023397427052259445\n",
      "Epoch 21 / 200 | iteration 90 / 171 | Total Loss: 3.0563135147094727 | KNN Loss: 3.0131585597991943 | CLS Loss: 0.043155066668987274\n",
      "Epoch 21 / 200 | iteration 100 / 171 | Total Loss: 3.068023204803467 | KNN Loss: 3.0137362480163574 | CLS Loss: 0.05428692325949669\n",
      "Epoch 21 / 200 | iteration 110 / 171 | Total Loss: 3.0750768184661865 | KNN Loss: 3.0397398471832275 | CLS Loss: 0.035336896777153015\n",
      "Epoch 21 / 200 | iteration 120 / 171 | Total Loss: 3.083533525466919 | KNN Loss: 3.0370914936065674 | CLS Loss: 0.046442076563835144\n",
      "Epoch 21 / 200 | iteration 130 / 171 | Total Loss: 3.0372166633605957 | KNN Loss: 2.9934444427490234 | CLS Loss: 0.04377210885286331\n",
      "Epoch 21 / 200 | iteration 140 / 171 | Total Loss: 3.061859369277954 | KNN Loss: 3.006035327911377 | CLS Loss: 0.055824097245931625\n",
      "Epoch 21 / 200 | iteration 150 / 171 | Total Loss: 3.085111618041992 | KNN Loss: 3.006405830383301 | CLS Loss: 0.07870570570230484\n",
      "Epoch 21 / 200 | iteration 160 / 171 | Total Loss: 3.102527379989624 | KNN Loss: 3.0473134517669678 | CLS Loss: 0.055213894695043564\n",
      "Epoch 21 / 200 | iteration 170 / 171 | Total Loss: 3.002115249633789 | KNN Loss: 2.9596104621887207 | CLS Loss: 0.04250485450029373\n",
      "Epoch: 021, Loss: 3.0644, Train: 0.9864, Valid: 0.9808, Best: 0.9820\n",
      "Epoch 22 / 200 | iteration 0 / 171 | Total Loss: 3.06054425239563 | KNN Loss: 3.0002923011779785 | CLS Loss: 0.060251928865909576\n",
      "Epoch 22 / 200 | iteration 10 / 171 | Total Loss: 3.082686424255371 | KNN Loss: 3.013017416000366 | CLS Loss: 0.06966900080442429\n",
      "Epoch 22 / 200 | iteration 20 / 171 | Total Loss: 3.0712056159973145 | KNN Loss: 3.036698818206787 | CLS Loss: 0.034506916999816895\n",
      "Epoch 22 / 200 | iteration 30 / 171 | Total Loss: 3.0219931602478027 | KNN Loss: 2.995662212371826 | CLS Loss: 0.026330841705203056\n",
      "Epoch 22 / 200 | iteration 40 / 171 | Total Loss: 3.0560035705566406 | KNN Loss: 3.0249552726745605 | CLS Loss: 0.031048400327563286\n",
      "Epoch 22 / 200 | iteration 50 / 171 | Total Loss: 3.0591933727264404 | KNN Loss: 3.023040533065796 | CLS Loss: 0.036152783781290054\n",
      "Epoch 22 / 200 | iteration 60 / 171 | Total Loss: 3.093491315841675 | KNN Loss: 3.019371747970581 | CLS Loss: 0.07411959022283554\n",
      "Epoch 22 / 200 | iteration 70 / 171 | Total Loss: 3.046809673309326 | KNN Loss: 2.9856443405151367 | CLS Loss: 0.06116533651947975\n",
      "Epoch 22 / 200 | iteration 80 / 171 | Total Loss: 3.0621285438537598 | KNN Loss: 3.031928300857544 | CLS Loss: 0.03020019829273224\n",
      "Epoch 22 / 200 | iteration 90 / 171 | Total Loss: 3.0444893836975098 | KNN Loss: 2.993377923965454 | CLS Loss: 0.05111156031489372\n",
      "Epoch 22 / 200 | iteration 100 / 171 | Total Loss: 3.0762670040130615 | KNN Loss: 3.003502130508423 | CLS Loss: 0.07276494055986404\n",
      "Epoch 22 / 200 | iteration 110 / 171 | Total Loss: 3.076969861984253 | KNN Loss: 3.0006282329559326 | CLS Loss: 0.07634154707193375\n",
      "Epoch 22 / 200 | iteration 120 / 171 | Total Loss: 3.122819185256958 | KNN Loss: 3.057131052017212 | CLS Loss: 0.06568807363510132\n",
      "Epoch 22 / 200 | iteration 130 / 171 | Total Loss: 3.041067600250244 | KNN Loss: 3.0090925693511963 | CLS Loss: 0.03197510540485382\n",
      "Epoch 22 / 200 | iteration 140 / 171 | Total Loss: 3.079850912094116 | KNN Loss: 3.023191452026367 | CLS Loss: 0.05665938928723335\n",
      "Epoch 22 / 200 | iteration 150 / 171 | Total Loss: 3.067929983139038 | KNN Loss: 3.014291524887085 | CLS Loss: 0.05363835394382477\n",
      "Epoch 22 / 200 | iteration 160 / 171 | Total Loss: 3.054117202758789 | KNN Loss: 3.011831521987915 | CLS Loss: 0.04228557273745537\n",
      "Epoch 22 / 200 | iteration 170 / 171 | Total Loss: 3.061087131500244 | KNN Loss: 3.016364336013794 | CLS Loss: 0.04472281411290169\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
